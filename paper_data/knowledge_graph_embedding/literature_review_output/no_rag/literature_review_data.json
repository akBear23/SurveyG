{
  "title": "A Comprehensive Literature Review with Self-Reflection",
  "papers_processed": 377,
  "paper_list": [
    "d899e434a7f2eecf33a90053df84cf32842fbca9.pdf",
    "83d58bc46b7adb92d8750da52313f060b10f201d.pdf",
    "10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf",
    "b1d807fc6b184d757ebdea67acd81132d8298ff6.pdf",
    "abea782b5d0bdb4cd90ec42f672711613e71e43e.pdf",
    "658702b2fa647ae7eaf1255058105da9eefe6f52.pdf",
    "29eb99518d16ccf8ac306d92f4a6377ae109d9be.pdf",
    "58e1b93b18370433633152cb8825917edc2f16a6.pdf",
    "d4220644ef94fa4c2e5138a619cfcd86508d2ea1.pdf",
    "15710515bae025372f298570267d234d4a3141cb.pdf",
    "354fb91810c6d3756600c99ad84d2e6ef4136021.pdf",
    "67cab3bafc8fa9e1ae3ff89791ad43c81441d271.pdf",
    "405a7a7464cfe175333d6f04703ac272e00a85b4.pdf",
    "8b717c4dfb309638307fcc7d2c798b1c20927a3e.pdf",
    "29052ddd048acb1afa2c42613068b63bb7428a34.pdf",
    "23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff.pdf",
    "af051c87cecca64c2de4ad9110608f7579766653.pdf",
    "85064a4b1b96863af4fccff9ad34ce484945ad7b.pdf",
    "06315f8b2633a54b087c6094cdb281f01dd06482.pdf",
    "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0.pdf",
    "3ac716ac5d47d4420010678fda766ebb5b882ba9.pdf",
    "933cb8bf1cd50d6d5833a627683327b15db28836.pdf",
    "bb3e135757bfb82c4de202c807c9e381caecb623.pdf",
    "398978c84ca8dab093d0b7fa73c6d380f5fa914c.pdf",
    "b594b21557395c6a8fa8356249373f8e318c2df2.pdf",
    "3e3a84bbceba79843ca1105939b2eb438c149e9e.pdf",
    "b3f0cdc217a3d192d2671e44913542903c94105b.pdf",
    "52eb7f27cdfbf359096b8b5ef56b2c2826beb660.pdf",
    "ecb80d1e5507e163be4a6757b00c8809a2de4863.pdf",
    "33d469c6d9fc09b59522d91b7696b15dc60a9a93.pdf",
    "4801db5c5cb24a9069f2d264252fa26986ceefa9.pdf",
    "a166957ec488cd20e61360d630568b3b81af3397.pdf",
    "bcffbb40e7922d2a34e752f8faaa4fe99649e21a.pdf",
    "7029ecb5d5fc04f54e1e25e739db2e993fb147c8.pdf",
    "990334cf76845e2da64d3baa10b0a671e433d4b6.pdf",
    "0367603c0197ab48eeba29aa6af391584a5077c0.pdf",
    "7572aefcd241ec76341addcb2e2e417587cb2e4c.pdf",
    "c2c6edc5750a438bddd1217481832d38df6336de.pdf",
    "a6a735f8e218f772e5b9dac411fa4abea87fdb9c.pdf",
    "f2b924e69735fb7fd6fd95c6a032954480862029.pdf",
    "e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a.pdf",
    "63836e669416668744c3676a831060e8de3f58a1.pdf",
    "11e402c699bcb54d57da1a5fdbc57076d7255baf.pdf",
    "191815e4109ee392b9120b61642c0e859fb662a1.pdf",
    "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b.pdf",
    "c64433657869ecdaaa7988a029eabfe774d3ac47.pdf",
    "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4.pdf",
    "68f34ed64fdf07bb1325097c93576658e061231e.pdf",
    "efea0197c956e981e98c4d2532fa720c58954492.pdf",
    "f470e11faa6200026cf39e248510070c078e509a.pdf",
    "5dc88d795cbcd01e6e99ba673e91e9024f0c3318.pdf",
    "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf",
    "33f3f53c957c4a8832b1dcb095a4ac967bd89897.pdf",
    "2e925a02db26a60ee1cc022f3923e09f3fae7b39.pdf",
    "040fe47af8f4870bf681f34861c42b3ea46d76cf.pdf",
    "c762e198b0239313ee50476021b1939390c4ef9d.pdf",
    "1f20378d2820fdf1c1bb09ce22f739ab77b14e82.pdf",
    "991b64748dfeecf026a27030c16fe1743aa20167.pdf",
    "6a2f26cece133b0aa52843be0f149a65e78374f7.pdf",
    "2a3f862199883ceff5e3c74126f0c80770653e05.pdf",
    "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3.pdf",
    "acc855d74431537b98de5185e065e4eacbab7b26.pdf",
    "2a25540e3ce0baba56ee71da7ca938f0264f790d.pdf",
    "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4.pdf",
    "d7ef14459674b75807cd9be549f1e12d53849ead.pdf",
    "3f170af3566f055e758fa3bdf2bfd3a0e8787e58.pdf",
    "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408.pdf",
    "f4e39a4f8fd8f8453372b74fda17047b9860d870.pdf",
    "6a86594566fc9fa2e92afb6f0229d63a45fe25e6.pdf",
    "1620a20881b572b5ffc6f9cb3cf39f6090cee19f.pdf",
    "83a46afaeb520abcd9b0138507a253f6d4d8bff7.pdf",
    "f44ee7932aacd054101b00f37d4c26c27630c557.pdf",
    "44ce738296c3148c6593324773706cdc228614d4.pdf",
    "bcdb8914550df02bfe1f69348c9830d775f6590a.pdf",
    "77dc07c92c37586f94a6f5ac3de103b218931578.pdf",
    "d1a525c16a53b94200029df1037f2c9c7c244d7b.pdf",
    "8f096071a09701012c9c279aee2a88143a295935.pdf",
    "18bd7cd489874ed9976b4f87a6a558f9533316e0.pdf",
    "0364e17da01358e2705524cd781ef8cc928256f5.pdf",
    "fda63b289d4c0c332f88975994114fb61b514ced.pdf",
    "3f0d5aa7a637d2c0bb3d768c99cc203430b4481e.pdf",
    "2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83.pdf",
    "84aa127dc5ca3080385439cb10edc50b5d2c04e4.pdf",
    "727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf",
    "19a672bdf29367b7509586a4be27c6843af903b1.pdf",
    "ecc04e9285f016090697a1a8f9e96ce01e94e742.pdf",
    "beade097ff41c62a8d8d29065be0e1339be39f30.pdf",
    "bbb89d88ad5b8279709ff089d3c00cd2750cd26b.pdf",
    "d605a7628b2a7ff8ce04fc27111626e2d734cab4.pdf",
    "322aa32b2a409d2e135dbb14736d9aeb497f1c52.pdf",
    "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67.pdf",
    "ce7291c5cd919a97ced6369ca697db9849848688.pdf",
    "780bc77fac1aaf460ba191daa218f3c111119092.pdf",
    "6205f75cb6db1503c94386441ca68c63c9cbd456.pdf",
    "e379f7c85441df5d8ddc1565cabf4b4290c22f1f.pdf",
    "c180564160d0788a82df203f9e5f61380d9846aa.pdf",
    "69418ff5d4eac106c72130e152b807004e2b979c.pdf",
    "552bfaca30af29647c083993fbe406867fc70d4c.pdf",
    "33a7b7abf006d22de24c1471e6f6c93842a497b6.pdf",
    "86ac98157da100a529ca65fe6e1da064b0a651e8.pdf",
    "52b167a90a10cde25309e40d7f6e6b5e14ec3261.pdf",
    "145fa4ea1567a6b9d981fdea0e183140d99aeb97.pdf",
    "e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c.pdf",
    "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596.pdf",
    "4e52607397a96fb2104a99c570c9cec29c9ca519.pdf",
    "eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf",
    "7e5f318bf5b9c986ca82d2d97e11f50d58ee6680.pdf",
    "8c93f3cecf79bd9f8d021f589d095305e281dd2f.pdf",
    "cab5194d13c1ce89a96322adaac754b2cb630d87.pdf",
    "95c3d25b40f963eb248136555bd9b9e35817cc09.pdf",
    "12cc4b65644a84a16ef7dfe7bdd70172cd38cffd.pdf",
    "40479fd70115e545d21c01853aad56e6922280ac.pdf",
    "5515fd5d14ac7b19806294119560a8c74f7fa4b2.pdf",
    "e5c851867af5587466f7cd9c22f8b2c84f8c6b63.pdf",
    "eb14b24b329a6cc80747644616e15491ef49596f.pdf",
    "9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf",
    "d9802a67b326fe89bbd761c261937ee1e4d4d674.pdf",
    "b307e96f59fde63567cd0beb30c9e36d968fad8e.pdf",
    "e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d.pdf",
    "c075a84356b529464df2e06a02bf9b524a815152.pdf",
    "b30481dd5467a187b7e1a5a2dd326d97cafd95ac.pdf",
    "2930168f3be575781939a57f4bb92e6b29c33b08.pdf",
    "da60d33d007681743d939861ae24f4cdac15667e.pdf",
    "bb65c0898647c57c87a72e80d97a53576e3034ca.pdf",
    "c03965d00865074ae66d0324c7145bf59aec73e6.pdf",
    "4b0e3d0721ea9324e9950b3bb98d917da8acb222.pdf",
    "8df10fa4eca07dbb5fe2fe2ecc1e546cb8a8c947.pdf",
    "d6cc2a58df29d3e3fe4c55902880908dde32ee60.pdf",
    "a57af41c3845a6d15ffbe5bd278e971ca9b8124a.pdf",
    "8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf",
    "23ae48cdb8b7985e5a32fc79b6aae0de3230fe4f.pdf",
    "87ccb0d6c3e9f6367cd753538f4e906838cea8c2.pdf",
    "0dddf37145689e5f2899f8081d9971882e6ff1e9.pdf",
    "4be29e1cd866ab31f83f03723e2f307cdc1faab0.pdf",
    "2a81032e5bb4b29f6e1423b6083b9a04bb54b605.pdf",
    "c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf",
    "d97ec8a07cea1a18edf0a20981aad7e3dfe351e6.pdf",
    "389935511c395526817cf4ae62dae8913845ebdf.pdf",
    "ba524aa0ae24971b56eef6e92491de07d097a233.pdf",
    "a264af122f9f2ea5df46c030beb8ec0c25d6e907.pdf",
    "90450fe686c0fa645a1954950adffc5b2401e4b7.pdf",
    "2257eb642e9ecae24f455a58dc807ee2a843081f.pdf",
    "d77de3a4ddfa62f8105c0591fd41e549edcfd95f.pdf",
    "52457f574780c53c68ad645fcdc86e2492b5074a.pdf",
    "ac79b551ca16f98c1c3a5592c22d8093a492c4f3.pdf",
    "0abee37fe165b86753b306ffcc59a77e89de0599.pdf",
    "512177d6b1e643b49b1d5ab1ad389666750144a9.pdf",
    "60347869db7d1940958ee465b3010b3a612bf791.pdf",
    "9f7731d72e2aa251d2994eb1729c22aa78d0f718.pdf",
    "c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a.pdf",
    "4ac5f7ad786fbee89b04023383a4fbe095ccc779.pdf",
    "9fc2fd3d53a04d082edc80bafa470a66acdebb14.pdf",
    "747dff7b9cd0d6feb16c340b684b1923034e8777.pdf",
    "3e76e90180fc8300ecdeb5b543015cc68e0fd249.pdf",
    "547dfe2a9d6a1bb1023f2208fb31f3a0671bf9ca.pdf",
    "39eb51ae87c168ad4339214de6b91e2e2fdcfaa1.pdf",
    "fee5ac3604ccdefee2b65275fed47503234099e2.pdf",
    "154fac5040865b4d74cf5a2cad39381c134a8b7d.pdf",
    "543497b1e551ad6473ddb9aa46697db28bccd3f5.pdf",
    "6cc55dec26f5c078c6872d612c1561b1646d459a.pdf",
    "ee5ceab9fa5f3bad231469923a03ad16184b51b9.pdf",
    "3705cfe0d7dab8881518cb932f2465ca432d3f24.pdf",
    "882d6fe22a093ff95a8106a215bca37603ada710.pdf",
    "92ef8ff6715733697ca915c65cb18b160a764da6.pdf",
    "a0ca7d39296d8d31dbbf300f58e7e375fb879492.pdf",
    "9155e1340e9263cf042d144681acccfc0c9d194b.pdf",
    "b5167990eda7d48f1a70a1fcb900ed5d46c40985.pdf",
    "0a8faa6c0e6dc9f743e96f276239d02d8839aca2.pdf",
    "71245f9d9ba0317f78151698dc1ddba7583a3afd.pdf",
    "f0499c2123e17106039e8e772878aad073ccf916.pdf",
    "2bdb9985208a7c7805676029300e3ba648125bd1.pdf",
    "7ccb05062f9ea7179532fd3355cf984b0102cfc5.pdf",
    "c8214cac9c841f7b295a78c5bf71b6ed37c40eec.pdf",
    "dab87bce4ac8c6033f5836f575b57c4a665b4f49.pdf",
    "7ae22798887ff4e19033a8028007e1780b53ba8c.pdf",
    "01c1e7830031b25410ed70965d239ac439a6fb68.pdf",
    "021cbcd59c0438ac8a50c511be7634b0c00a1b89.pdf",
    "f211a2123e28d60cd8cdc05449c3cb7da2610b0a.pdf",
    "3646e2947827c0a9314443e5cbb15575fafaf4ba.pdf",
    "67c03d7a477059dc20faa02e3b45ca7055433615.pdf",
    "91d8e1339eddee3217a6897cebdeb526b4bb1f72.pdf",
    "b1464e3f0c82e21e23dfd9bc28e423856754b3d6.pdf",
    "57a7804d4e4e57de9a5c096ce7ea3e50d2c86f0f.pdf",
    "678dacdf029becac1116f345520f8e4afff5a873.pdf",
    "1a25c8afacb6d36d4d8635eb9e3f8b8cf2e2122c.pdf",
    "60ad3ce0492a004020ff55653a51d6bfc457f12d.pdf",
    "434b32d34b5d21071fc78a081741757f263c14ae.pdf",
    "4a96636d1fc92221f2232d2d74be6e303cd0642a.pdf",
    "9c17d3f1837ae9f10f57c0b07c8288137d84026b.pdf",
    "e740a9aa753fcc926857ef4b90c1f91dd086e08d.pdf",
    "315b239040f73063076014fdfabcc621b2719d83.pdf",
    "96b1f6fb6e904a674aef5cd32efee3edfa1c8ee2.pdf",
    "5d6b4c5e48ec0585facea96a746bcbf7225d424c.pdf",
    "441f124d48662d6bd4f8e3190633371aa1b034eb.pdf",
    "5f9ea28be0d3bb9a73d62512190a772b10e92db0.pdf",
    "836d1d1c94f0fd0713c77b86ce136fffd059dbc0.pdf",
    "0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf",
    "00529345e4a604674477f8a1dc1333114883b8d9.pdf",
    "f0d5351c76448e28626177ece5ce97715087a0f9.pdf",
    "9866a21c0ada20b62b28b3722c975595be819e24.pdf",
    "50e7017c7768b7b2f5215a35539db1490ddc37ab.pdf",
    "95a501bfe4b09323e6e178edd64dc24a6935c23f.pdf",
    "46b5198a535dfcaf1cc7d57d471ad9ec050e46cf.pdf",
    "cda7a1bdce2bfa77c2d463b631ff84b69ce3c9ed.pdf",
    "f76a6e8f059820667af53edbd42d33fc4bca85fd.pdf",
    "40667a731593a44d4e2f9391f1d14f368321b751.pdf",
    "6bf53a97f5a3f5b0375f4702cbec28d8e9ab61c0.pdf",
    "4ae2631fb5e99cb64ff7d6e7ed3a1e6b0bedd269.pdf",
    "d76b3bf29366b4f0902ea145a3f7c020a35f084f.pdf",
    "151c9bb547306d66ba252be7c20e35f711e9f330.pdf",
    "c0827be29366be4b8cfa0dfbef4ead3f7b08f562.pdf",
    "2d38cdaf2e232b5d1cb1dce388aa0fe75babcf29.pdf",
    "d6508e8825a6a1281ad415de47a2f108d98df87d.pdf",
    "18101998fb57704b79eb4c4c37891144ede8f8b9.pdf",
    "23830bb104b25103162ec9f9f463624d9a434194.pdf",
    "77e23cd2437c6afb16082793badbb02842442e13.pdf",
    "92351a799555df8d49465c2d4959118030339cc0.pdf",
    "6de535eb1b0024887227f7987e6eb22478af2a95.pdf",
    "be7b102315ce70a7e01eb87c1140dd6850148e8b.pdf",
    "5b6a24ea3ffdccb14ce0267a815845c62ef026c9.pdf",
    "75f7e3459e53fa0775c941cb703f049797851ef0.pdf",
    "3ea066e35fdd45162a7fa94e995abe0beeceb532.pdf",
    "c7a630751e45e3a74691bd0fc0880b4bf87be101.pdf",
    "a2a7f85d2ba28750725c4956eb14d53f6a90f003.pdf",
    "bb0613ea0d39e35901aa0018de40deaf35cbbd5d.pdf",
    "509fa029989e89a4b82dd01ab75734aed937d684.pdf",
    "4f2cc26b689cdac36ceb2037338eac65e7e5a193.pdf",
    "7bb4cd36de648ca44cc390fe886ee70a4b2ad1ac.pdf",
    "93db6077c12cc83ea165a0d8851efb69b0055f3a.pdf",
    "2f700be8a387101411a84199adfe30636e331752.pdf",
    "2dba03d338170bde4e965909230256086bafa9f8.pdf",
    "c2648a294ef2fc299e1dd959bc1f92973f9c9ebc.pdf",
    "62c50e300ee87b185401ce27323bbb3f5262fdff.pdf",
    "66f19b09f644578f808e69f38d3e76f8b972f813.pdf",
    "9b68475f787be0999e7d09456003e664d37c2155.pdf",
    "f0ac0c2f82886700dc7e7a178d597d33deebfc88.pdf",
    "a5aeca7ef265b27ff6d9ea08873c9499632b6439.pdf",
    "8412cc4dd7c8d309d7a84573637d4daaad8d33b5.pdf",
    "8be21591c29d68d99e89a71fc7755f09f5eed3a1.pdf",
    "6493e6d563282fcb65029162a71cd2cb8168765b.pdf",
    "d5eabc89e2346411134569a603e63a143d1d6552.pdf",
    "89cf9719b97e69f5bb7d715d5a16609676c14e86.pdf",
    "1c1b5fd282d3a1fe03a671f7d13092d49cb31139.pdf",
    "7f7137d3e1de7e0e801c27d5e8b963dfd6d94eb4.pdf",
    "49899fd94cd272914f7d1e81b0915058c25bb665.pdf",
    "e64557514ab856d22ddbb34bc23ffb7085d5d6b0.pdf",
    "7eece37709dceba5086f48dc43ac1a69d0427486.pdf",
    "83424a4fea2e75311632059914bf358bc045435f.pdf",
    "3f8b13ede9f4d3a770ec8b4771b6036b9f603bfa.pdf",
    "ac0c9afa9c19f0700d903e00a92e83e41587add3.pdf",
    "f42d060fb530a11daecd90695211c01a5c264f8d.pdf",
    "7aca91d068d20d3389b28b8277ebc3d488be459f.pdf",
    "fa07384402f5c9d5b789edf7667bbcc555f381e3.pdf",
    "48c2e0d87b84efca7f11462bbdac1be1177e2433.pdf",
    "51c18009b2c566d7cddc934b2cf9a1bca813f58f.pdf",
    "5cbf9bc26b3d0471cb37c3f4a931990b1260d82d.pdf",
    "4383242be5bdfb30ffa84e58cc252acfb58d4878.pdf",
    "f26d45a806d1f1319f37eb41b8aa87d768a1d656.pdf",
    "7b569aecc97f5fe57ce19ca0670a6b1bc62c7f7c.pdf",
    "8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf",
    "e83b693a44ec32ddfb084d13138e8d7ebc85a7c3.pdf",
    "f284977aa917be0ff15b835b538294b827135d19.pdf",
    "f3fa1ef467c996b30242124a298b5b9d031e9ed5.pdf",
    "61ef322fba87ccfd36c004afc875542a290fe879.pdf",
    "5bef4d28d12dd578ce8a971d88d2779ec01c7ec5.pdf",
    "c441b2833db8bd96b4ad133679a68f79d464ef59.pdf",
    "edfbe0b62b9f628858d05b64bd830cf9b0a1ab74.pdf",
    "88e700e9fd6c14f3aa4502176a60512ca4020e35.pdf",
    "942541df1b97a9d1e46059c7c2d11503adc51c4c.pdf",
    "abc424e17642df01e0e056427250526bc624f762.pdf",
    "825d7339eadadd2baf962f7d3c8fe7dc0cdc9819.pdf",
    "b6839f89a59132f0e62011a218ec229a27ffff6b.pdf",
    "59116a07dbdb3cdeebb20085fdfde8b899de8f6a.pdf",
    "3cab78074e79122fd28cd76f37fd8805e8e4fc31.pdf",
    "ed21098804490b98899bcb7195084983ce69ed6c.pdf",
    "354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf",
    "c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf",
    "dc949e502e35307753a1acbcdf937f0cd866e63b.pdf",
    "a64167fcaa7a487575c6479510e57795afc9974e.pdf",
    "f9a575349133b2d4bf512cfb7754fca6d13b0a81.pdf",
    "5f850f1f522f959e2d3dcad263d05b0fdbb187c3.pdf",
    "4c68ee32d3db73d4d05803c1b3f2f4b929a88b78.pdf",
    "2ac47be80b02a3ff1b87c46cf2b8c27e739c2873.pdf",
    "b5aedc0464d04aa3fed7e59a141d9be7ee18c217.pdf",
    "463c7e28be13eb02620ad7e29b562bf6e5014ba2.pdf",
    "7009fd9eb533df6882644a1c8e1019dc034b9cc5.pdf",
    "e186e5000174ea70729c90d465e60279c5f88646.pdf",
    "70dc4c1ec4cda0a7c88751fb9a6b0c648e48e11f.pdf",
    "5a8c6890e524b708dc262d3f456c985e8a46d7d1.pdf",
    "86631a005e1a88a66926ac0c364ed0101a02b7e7.pdf",
    "92b9aeabaaac0f20f66c5a68fbb4fc268c5eaae5.pdf",
    "ce494973ceefe5ac011f7e9879843530395fa9db.pdf",
    "25edfb99d3b8377a11433cf7be2bcd9f8bfbdb87.pdf",
    "709a128e752414c973613814ddc2509f2abe092f.pdf",
    "18fd8982051fc1de652a9882c2c52db11bca646b.pdf",
    "a7f0b4776d3df11cf0d0e72785c3035cc744726c.pdf",
    "e2783f8aa4c61443760a8754cd6d88165d50b213.pdf",
    "77fedfa533871c6c4218285493f725d5df4e74e5.pdf",
    "695ef4cf57b4fd0c7ec17a6e10dffade51f38179.pdf",
    "90d5e74b18d03f733c6086418bfe9b20bb6a0a69.pdf",
    "c495b2780accfbb53a932181e3c9fd957d16895d.pdf",
    "85bfec413860c072529ab8399676ab4b072f2e34.pdf",
    "a89f61021e5382912aaeb3f69a6d8a6265787af4.pdf",
    "b3cbbc1f34a20c22853f3dd347fd635b2e414fd5.pdf",
    "df7265b4652b21bc690497b3967a708d811ddd23.pdf",
    "f6182d5c14c6047d197f1af842862653a13238f2.pdf",
    "082856e9b36fac60b9b9400abffaff0e74552fe1.pdf",
    "b25744d3c5d93e49b1906991dc8b5426ea2cf51d.pdf",
    "18bcad2521cbe8df9d84b1adff1dd57c72c68a9d.pdf",
    "bdd6c1a6695e3d201b70f4a913ffc758b74216e7.pdf",
    "e93565f447a42b158df27ba75385f5e2fc30dde7.pdf",
    "cf436f34ca6aabe1971c3531d465ecaa3d480d68.pdf",
    "76016197d7d4f2213a4ace29988c93285793e154.pdf",
    "9730f484b84074c1d61c154211ea06cc6ff20940.pdf",
    "10c388fa25dd6f07707a414946e5b7a674e7155b.pdf",
    "7e6a50b70223dc00c712a17537fb7e23f8fd5ad4.pdf",
    "ae58ebc99f67eed0de7f4ba2ca6f7ceb9ab056fb.pdf",
    "6ad02ad36e7a2c7d72d1a0b15ffc61dae2be1d7a.pdf",
    "75ba0b92bcf095e7cd1544425f1818fed195f83f.pdf",
    "905d27e361c50da406439bdac25807dd38258fd8.pdf",
    "b2646d9ee88c3dd6822b039a38c9604932aaaf47.pdf",
    "c7666fbaa49da21c465dbfabcf5fdd768b8c7b9e.pdf",
    "f1b7682df472a88fbaac3e6049f638ecec6937e7.pdf",
    "d66622beef468f7b934a5bf601cb8a3fcefe78f3.pdf",
    "20486c2fb358730ee99ae39b5e0a88d7b39ca720.pdf",
    "b49f6029d681ac286ab929238f5aef5f352767c8.pdf",
    "c5a19440511a741edd1581d41d37d3e9b7088186.pdf",
    "822ad7c33316202a2511d300c6b8a263b758ad1a.pdf",
    "ba61c59abb560ff47a8dd780c8ccffb0af5e14c2.pdf",
    "b3c340aa22bcd183c41836ef7265d656f741911f.pdf",
    "7c82aa0ae4b4e027a2df8afe9bbeccf88368c62f.pdf",
    "0d9a788260e3abff4794d79f72b2b5ab2fb5abe5.pdf",
    "6cba788eea4fdb3bd0d1db4ecdd8a70040b81e62.pdf",
    "6c195ec2d5a491ffca9ab893968c4d44a6d0ce7d.pdf",
    "37b274eb6fa68dede9f4aaad6dec1e2ea56095ce.pdf",
    "9be88067bd7351b36bb0c698f5559ced3918a1d5.pdf",
    "e0d17f8b2fffff6c5eaf3f13bc45126196ddd128.pdf",
    "a4b6e13efa80bedf8e588ac69f91fdaecc8e5077.pdf",
    "ccb6674576de48f8cfd99374c3b737a94dc3cb98.pdf",
    "75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf",
    "8ff387296878f23632a588076823b160673866ab.pdf",
    "6a66b459955959c4b8a67bd298ed291506923b7a.pdf",
    "6b69c8848a1cc50ed8775beb483c71cfc314c66b.pdf",
    "d57e01d80c7f0f86b5e3f096b193ab9210e9095f.pdf",
    "a9bfb9ab236553768782f2b90a69c5625f033186.pdf",
    "6903aea3553a449257388580028e0bddf119d021.pdf",
    "767d56fe80f7681b97943a8bff39f0b580e4acd8.pdf",
    "9e7799ef313143aa9c0669a7d1918fcfd5d21359.pdf",
    "563b3d57927b688e59322dbbfc973e5f1b269584.pdf",
    "984c18fa61b10b6d1c34affc98f27ca8344d4224.pdf",
    "4a0048f1942a68e7c39adac43588d1604af26fc7.pdf",
    "49dfd47177fa3aeab8a6bea82a77ec8bdb93bf1e.pdf",
    "2a5c888b2df4fd8c49aef46ee065422b00b178c0.pdf",
    "48c07506022634f332b410fb59dca9f61f89b032.pdf",
    "575af1587dea578d48eb27f45f008203565d9170.pdf",
    "7bd50842503e23e6479447b98912ac482ef43adc.pdf",
    "4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf",
    "c3861a930a65e8d9ee7ab9f0a6ee71e0e59df7ed.pdf",
    "217a4712feae7d7590d813d23e88f5fbb4f2c37f.pdf",
    "cf696a919b8476a4d74b8b726e919812a2f05779.pdf",
    "91d5aa3d43237ec60266563ec6e8079f86532cfa.pdf",
    "58480444670ff933fe644563f7e2948a79503442.pdf",
    "9b836b4764d4f6947ac684fd4ba3e8c3597d95bd.pdf",
    "bd0e8d6db97111686d02b51134f87439f8f1acfa.pdf",
    "bea79d59ab3d203d06c88ebf67ac47cb34adeaa7.pdf",
    "241904795d94dcb1946ad46c9184c59899783af1.pdf",
    "55dab161c25d1dd04fbeecdeca085274bfe8463f.pdf",
    "3ff6b617cd839c9d85cb7b58aa6ad56e95b6cf69.pdf",
    "9560ca767022020ccf414a2a8514f25b89f78cb3.pdf",
    "d5c8dcc8f5c87c269780c7011a355b9202858847.pdf",
    "a77b3c5f532e61af63a9d95e671ce02d8065ee24.pdf",
    "2d12d1cec23e1c26c65de52100db70d91ca90035.pdf",
    "4b1d0cf2b99aec85cdedceaef88c3a074de79832.pdf",
    "0845cea58467d372eb296fa1f184ecabe02be18b.pdf",
    "6a9caace1919b0e7bb247f0ecb585068c1ec4ff8.pdf",
    "30321b036607a7936221235ea8ec7cf7c1627100.pdf",
    "e03b8e02ddda86eafb54cafc5c44d231992be95a.pdf"
  ],
  "citations_map": {
    "d899e434a7f2eecf33a90053df84cf32842fbca9.pdf": "sun2018",
    "83d58bc46b7adb92d8750da52313f060b10f201d.pdf": "dasgupta2018",
    "10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf": "chen2023",
    "b1d807fc6b184d757ebdea67acd81132d8298ff6.pdf": "yang2023",
    "abea782b5d0bdb4cd90ec42f672711613e71e43e.pdf": "jia2015",
    "658702b2fa647ae7eaf1255058105da9eefe6f52.pdf": "lloyd2022",
    "29eb99518d16ccf8ac306d92f4a6377ae109d9be.pdf": "wu2021",
    "58e1b93b18370433633152cb8825917edc2f16a6.pdf": "xu2019",
    "d4220644ef94fa4c2e5138a619cfcd86508d2ea1.pdf": "shan2018",
    "15710515bae025372f298570267d234d4a3141cb.pdf": "zheng2024",
    "354fb91810c6d3756600c99ad84d2e6ef4136021.pdf": "he2023",
    "67cab3bafc8fa9e1ae3ff89791ad43c81441d271.pdf": "xiao2015",
    "405a7a7464cfe175333d6f04703ac272e00a85b4.pdf": "guo2017",
    "8b717c4dfb309638307fcc7d2c798b1c20927a3e.pdf": "chen2021",
    "29052ddd048acb1afa2c42613068b63bb7428a34.pdf": "li2023",
    "23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff.pdf": "zhou2023",
    "af051c87cecca64c2de4ad9110608f7579766653.pdf": "xiang2021",
    "85064a4b1b96863af4fccff9ad34ce484945ad7b.pdf": "cao2022",
    "06315f8b2633a54b087c6094cdb281f01dd06482.pdf": "wang2021",
    "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0.pdf": "guo2020",
    "3ac716ac5d47d4420010678fda766ebb5b882ba9.pdf": "zhang2024",
    "933cb8bf1cd50d6d5833a627683327b15db28836.pdf": "shen2022",
    "bb3e135757bfb82c4de202c807c9e381caecb623.pdf": "hu2024",
    "398978c84ca8dab093d0b7fa73c6d380f5fa914c.pdf": "liu2024",
    "b594b21557395c6a8fa8356249373f8e318c2df2.pdf": "zhang2019",
    "3e3a84bbceba79843ca1105939b2eb438c149e9e.pdf": "yang2019",
    "b3f0cdc217a3d192d2671e44913542903c94105b.pdf": "xie2023",
    "52eb7f27cdfbf359096b8b5ef56b2c2826beb660.pdf": "wang2024",
    "ecb80d1e5507e163be4a6757b00c8809a2de4863.pdf": "xiao2019",
    "33d469c6d9fc09b59522d91b7696b15dc60a9a93.pdf": "sachan2020",
    "4801db5c5cb24a9069f2d264252fa26986ceefa9.pdf": "madushanka2024",
    "a166957ec488cd20e61360d630568b3b81af3397.pdf": "zhu2022",
    "bcffbb40e7922d2a34e752f8faaa4fe99649e21a.pdf": "liang2024",
    "7029ecb5d5fc04f54e1e25e739db2e993fb147c8.pdf": "li2024",
    "990334cf76845e2da64d3baa10b0a671e433d4b6.pdf": "ebisu2017",
    "0367603c0197ab48eeba29aa6af391584a5077c0.pdf": "zhang2021",
    "7572aefcd241ec76341addcb2e2e417587cb2e4c.pdf": "huang2019",
    "c2c6edc5750a438bddd1217481832d38df6336de.pdf": "tang2019",
    "a6a735f8e218f772e5b9dac411fa4abea87fdb9c.pdf": "sun2018",
    "f2b924e69735fb7fd6fd95c6a032954480862029.pdf": "ge2023",
    "e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a.pdf": "wang2020",
    "63836e669416668744c3676a831060e8de3f58a1.pdf": "li2022",
    "11e402c699bcb54d57da1a5fdbc57076d7255baf.pdf": "zhang2019",
    "191815e4109ee392b9120b61642c0e859fb662a1.pdf": "tang2022",
    "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b.pdf": "lv2018",
    "c64433657869ecdaaa7988a029eabfe774d3ac47.pdf": "chen2025",
    "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4.pdf": "qian2021",
    "68f34ed64fdf07bb1325097c93576658e061231e.pdf": "dai2020",
    "efea0197c956e981e98c4d2532fa720c58954492.pdf": "ji2024",
    "f470e11faa6200026cf39e248510070c078e509a.pdf": "yan2022",
    "5dc88d795cbcd01e6e99ba673e91e9024f0c3318.pdf": "zhang2023",
    "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf": "li2021",
    "33f3f53c957c4a8832b1dcb095a4ac967bd89897.pdf": "yang2025",
    "2e925a02db26a60ee1cc022f3923e09f3fae7b39.pdf": "wang2019",
    "040fe47af8f4870bf681f34861c42b3ea46d76cf.pdf": "di2023",
    "c762e198b0239313ee50476021b1939390c4ef9d.pdf": "jia2017",
    "1f20378d2820fdf1c1bb09ce22f739ab77b14e82.pdf": "choudhary2021",
    "991b64748dfeecf026a27030c16fe1743aa20167.pdf": "xiao2015",
    "6a2f26cece133b0aa52843be0f149a65e78374f7.pdf": "hu2024",
    "2a3f862199883ceff5e3c74126f0c80770653e05.pdf": "wang2014",
    "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3.pdf": "zhu2020",
    "acc855d74431537b98de5185e065e4eacbab7b26.pdf": "ali2020",
    "2a25540e3ce0baba56ee71da7ca938f0264f790d.pdf": "mohamed2020",
    "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4.pdf": "gao2020",
    "d7ef14459674b75807cd9be549f1e12d53849ead.pdf": "peng2021",
    "3f170af3566f055e758fa3bdf2bfd3a0e8787e58.pdf": "shi2025",
    "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408.pdf": "zhang2024",
    "f4e39a4f8fd8f8453372b74fda17047b9860d870.pdf": "rosso2020",
    "6a86594566fc9fa2e92afb6f0229d63a45fe25e6.pdf": "zhou2024",
    "1620a20881b572b5ffc6f9cb3cf39f6090cee19f.pdf": "xie2020",
    "83a46afaeb520abcd9b0138507a253f6d4d8bff7.pdf": "song2021",
    "f44ee7932aacd054101b00f37d4c26c27630c557.pdf": "zhang2020",
    "44ce738296c3148c6593324773706cdc228614d4.pdf": "ge2022",
    "bcdb8914550df02bfe1f69348c9830d775f6590a.pdf": "ren2020",
    "77dc07c92c37586f94a6f5ac3de103b218931578.pdf": "yuan2019",
    "d1a525c16a53b94200029df1037f2c9c7c244d7b.pdf": "xiao2015",
    "8f096071a09701012c9c279aee2a88143a295935.pdf": "sun2018",
    "18bd7cd489874ed9976b4f87a6a558f9533316e0.pdf": "ji2015",
    "0364e17da01358e2705524cd781ef8cc928256f5.pdf": "lin2020",
    "fda63b289d4c0c332f88975994114fb61b514ced.pdf": "islam2023",
    "3f0d5aa7a637d2c0bb3d768c99cc203430b4481e.pdf": "wang2021",
    "2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83.pdf": "broscheit2020",
    "84aa127dc5ca3080385439cb10edc50b5d2c04e4.pdf": "fanourakis2022",
    "727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf": "wang2018",
    "19a672bdf29367b7509586a4be27c6843af903b1.pdf": "tabacof2019",
    "ecc04e9285f016090697a1a8f9e96ce01e94e742.pdf": "pei2019",
    "beade097ff41c62a8d8d29065be0e1339be39f30.pdf": "zhang2018",
    "bbb89d88ad5b8279709ff089d3c00cd2750cd26b.pdf": "li2021",
    "d605a7628b2a7ff8ce04fc27111626e2d734cab4.pdf": "li2022",
    "322aa32b2a409d2e135dbb14736d9aeb497f1c52.pdf": "ding2018",
    "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67.pdf": "zhang2022",
    "ce7291c5cd919a97ced6369ca697db9849848688.pdf": "sun2024",
    "780bc77fac1aaf460ba191daa218f3c111119092.pdf": "wang2024",
    "6205f75cb6db1503c94386441ca68c63c9cbd456.pdf": "modak2024",
    "e379f7c85441df5d8ddc1565cabf4b4290c22f1f.pdf": "xiao2016",
    "c180564160d0788a82df203f9e5f61380d9846aa.pdf": "zhang2023",
    "69418ff5d4eac106c72130e152b807004e2b979c.pdf": "guo2015",
    "552bfaca30af29647c083993fbe406867fc70d4c.pdf": "xu2020",
    "33a7b7abf006d22de24c1471e6f6c93842a497b6.pdf": "zheng2024",
    "86ac98157da100a529ca65fe6e1da064b0a651e8.pdf": "zhang2018",
    "52b167a90a10cde25309e40d7f6e6b5e14ec3261.pdf": "zhu2024",
    "145fa4ea1567a6b9d981fdea0e183140d99aeb97.pdf": "liu2023",
    "e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c.pdf": "choi2020",
    "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596.pdf": "ge2023",
    "4e52607397a96fb2104a99c570c9cec29c9ca519.pdf": "sadeghian2021",
    "eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf": "liu2024",
    "7e5f318bf5b9c986ca82d2d97e11f50d58ee6680.pdf": "li2022",
    "8c93f3cecf79bd9f8d021f589d095305e281dd2f.pdf": "rossi2020",
    "cab5194d13c1ce89a96322adaac754b2cb630d87.pdf": "li2023",
    "95c3d25b40f963eb248136555bd9b9e35817cc09.pdf": "peng2020",
    "12cc4b65644a84a16ef7dfe7bdd70172cd38cffd.pdf": "ji2024",
    "40479fd70115e545d21c01853aad56e6922280ac.pdf": "zhang2024",
    "5515fd5d14ac7b19806294119560a8c74f7fa4b2.pdf": "kochsiek2021",
    "e5c851867af5587466f7cd9c22f8b2c84f8c6b63.pdf": "yang2021",
    "eb14b24b329a6cc80747644616e15491ef49596f.pdf": "shang2024",
    "9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf": "asmara2023",
    "d9802a67b326fe89bbd761c261937ee1e4d4d674.pdf": "gregucci2023",
    "b307e96f59fde63567cd0beb30c9e36d968fad8e.pdf": "pan2021",
    "e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d.pdf": "yoon2016",
    "c075a84356b529464df2e06a02bf9b524a815152.pdf": "li2024",
    "b30481dd5467a187b7e1a5a2dd326d97cafd95ac.pdf": "xiong2017zqu",
    "2930168f3be575781939a57f4bb92e6b29c33b08.pdf": "gong2020b2k",
    "da60d33d007681743d939861ae24f4cdac15667e.pdf": "zhou2022ehi",
    "bb65c0898647c57c87a72e80d97a53576e3034ca.pdf": "le2022ji8",
    "c03965d00865074ae66d0324c7145bf59aec73e6.pdf": "zhou2022vgb",
    "4b0e3d0721ea9324e9950b3bb98d917da8acb222.pdf": "xu2019t6b",
    "8df10fa4eca07dbb5fe2fe2ecc1e546cb8a8c947.pdf": "mezni20218ml",
    "d6cc2a58df29d3e3fe4c55902880908dde32ee60.pdf": "do2021mw0",
    "a57af41c3845a6d15ffbe5bd278e971ca9b8124a.pdf": "mai2020ei3",
    "8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf": "zhang2022eab",
    "23ae48cdb8b7985e5a32fc79b6aae0de3230fe4f.pdf": "sosa2019ih0",
    "87ccb0d6c3e9f6367cd753538f4e906838cea8c2.pdf": "guan2019pr4",
    "0dddf37145689e5f2899f8081d9971882e6ff1e9.pdf": "fan2014g7s",
    "4be29e1cd866ab31f83f03723e2f307cdc1faab0.pdf": "zhang20190zu",
    "2a81032e5bb4b29f6e1423b6083b9a04bb54b605.pdf": "chen2022mxn",
    "c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf": "wang2022hwx",
    "d97ec8a07cea1a18edf0a20981aad7e3dfe351e6.pdf": "chen20226e4",
    "389935511c395526817cf4ae62dae8913845ebdf.pdf": "abusalih2020gdu",
    "ba524aa0ae24971b56eef6e92491de07d097a233.pdf": "fang2022wp6",
    "a264af122f9f2ea5df46c030beb8ec0c25d6e907.pdf": "elebi2019bzc",
    "90450fe686c0fa645a1954950adffc5b2401e4b7.pdf": "sha2019i3a",
    "2257eb642e9ecae24f455a58dc807ee2a843081f.pdf": "li2021ro5",
    "d77de3a4ddfa62f8105c0591fd41e549edcfd95f.pdf": "xiao20151fj",
    "52457f574780c53c68ad645fcdc86e2492b5074a.pdf": "zhang2021wg7",
    "ac79b551ca16f98c1c3a5592c22d8093a492c4f3.pdf": "wang20186zs",
    "0abee37fe165b86753b306ffcc59a77e89de0599.pdf": "li2021x10",
    "512177d6b1e643b49b1d5ab1ad389666750144a9.pdf": "wang202110w",
    "60347869db7d1940958ee465b3010b3a612bf791.pdf": "gutirrezbasulto2018oi0",
    "9f7731d72e2aa251d2994eb1729c22aa78d0f718.pdf": "portisch20221rd",
    "c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a.pdf": "zhang2022muu",
    "4ac5f7ad786fbee89b04023383a4fbe095ccc779.pdf": "feng2016dp7",
    "9fc2fd3d53a04d082edc80bafa470a66acdebb14.pdf": "liu2021wqa",
    "747dff7b9cd0d6feb16c340b684b1923034e8777.pdf": "sang2019gjl",
    "3e76e90180fc8300ecdeb5b543015cc68e0fd249.pdf": "wang2017yjq",
    "547dfe2a9d6a1bb1023f2208fb31f3a0671bf9ca.pdf": "jiang20219xl",
    "39eb51ae87c168ad4339214de6b91e2e2fdcfaa1.pdf": "liu2022fu5",
    "fee5ac3604ccdefee2b65275fed47503234099e2.pdf": "khan202236g",
    "154fac5040865b4d74cf5a2cad39381c134a8b7d.pdf": "mezni2021ezn",
    "543497b1e551ad6473ddb9aa46697db28bccd3f5.pdf": "zhang2021wix",
    "6cc55dec26f5c078c6872d612c1561b1646d459a.pdf": "huang2021u42",
    "ee5ceab9fa5f3bad231469923a03ad16184b51b9.pdf": "pavlovic2022qte",
    "3705cfe0d7dab8881518cb932f2465ca432d3f24.pdf": "wang20213kg",
    "882d6fe22a093ff95a8106a215bca37603ada710.pdf": "zhang2019rlm",
    "92ef8ff6715733697ca915c65cb18b160a764da6.pdf": "mai20195rp",
    "a0ca7d39296d8d31dbbf300f58e7e375fb879492.pdf": "han2018tzc",
    "9155e1340e9263cf042d144681acccfc0c9d194b.pdf": "wang2022fvx",
    "b5167990eda7d48f1a70a1fcb900ed5d46c40985.pdf": "ferrari2022r82",
    "0a8faa6c0e6dc9f743e96f276239d02d8839aca2.pdf": "fu2022df2",
    "71245f9d9ba0317f78151698dc1ddba7583a3afd.pdf": "wu2018c4b",
    "f0499c2123e17106039e8e772878aad073ccf916.pdf": "zhang202121t",
    "2bdb9985208a7c7805676029300e3ba648125bd1.pdf": "mohamed2019meq",
    "7ccb05062f9ea7179532fd3355cf984b0102cfc5.pdf": "xin2022dam",
    "c8214cac9c841f7b295a78c5bf71b6ed37c40eec.pdf": "nie20195gc",
    "dab87bce4ac8c6033f5836f575b57c4a665b4f49.pdf": "liu2018kvd",
    "7ae22798887ff4e19033a8028007e1780b53ba8c.pdf": "ni2020ruj",
    "01c1e7830031b25410ed70965d239ac439a6fb68.pdf": "li20215pu",
    "021cbcd59c0438ac8a50c511be7634b0c00a1b89.pdf": "yu2019qgs",
    "f211a2123e28d60cd8cdc05449c3cb7da2610b0a.pdf": "fatemi2018e6v",
    "3646e2947827c0a9314443e5cbb15575fafaf4ba.pdf": "chen2021i5t",
    "67c03d7a477059dc20faa02e3b45ca7055433615.pdf": "dong2022c6z",
    "91d8e1339eddee3217a6897cebdeb526b4bb1f72.pdf": "lu20206x1",
    "b1464e3f0c82e21e23dfd9bc28e423856754b3d6.pdf": "li2022nr8",
    "57a7804d4e4e57de9a5c096ce7ea3e50d2c86f0f.pdf": "luo2015df2",
    "678dacdf029becac1116f345520f8e4afff5a873.pdf": "zhou20216m0",
    "1a25c8afacb6d36d4d8635eb9e3f8b8cf2e2122c.pdf": "zhao202095o",
    "60ad3ce0492a004020ff55653a51d6bfc457f12d.pdf": "jia201870f",
    "434b32d34b5d21071fc78a081741757f263c14ae.pdf": "mai2018u0h",
    "4a96636d1fc92221f2232d2d74be6e303cd0642a.pdf": "li201949n",
    "9c17d3f1837ae9f10f57c0b07c8288137d84026b.pdf": "tang2020ufr",
    "e740a9aa753fcc926857ef4b90c1f91dd086e08d.pdf": "guo2022qtv",
    "315b239040f73063076014fdfabcc621b2719d83.pdf": "jiang202235y",
    "96b1f6fb6e904a674aef5cd32efee3edfa1c8ee2.pdf": "liu201918i",
    "5d6b4c5e48ec0585facea96a746bcbf7225d424c.pdf": "zhang2020s4x",
    "441f124d48662d6bd4f8e3190633371aa1b034eb.pdf": "chang20179yf",
    "5f9ea28be0d3bb9a73d62512190a772b10e92db0.pdf": "lee2022hr9",
    "836d1d1c94f0fd0713c77b86ce136fffd059dbc0.pdf": "zhang2022fpm",
    "0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf": "liu2019e1u",
    "00529345e4a604674477f8a1dc1333114883b8d9.pdf": "song2021fnl",
    "f0d5351c76448e28626177ece5ce97715087a0f9.pdf": "gradgyenge2017xdy",
    "9866a21c0ada20b62b28b3722c975595be819e24.pdf": "zhou20218bt",
    "50e7017c7768b7b2f5215a35539db1490ddc37ab.pdf": "chen20210ah",
    "95a501bfe4b09323e6e178edd64dc24a6935c23f.pdf": "zhang2020i7j",
    "46b5198a535dfcaf1cc7d57d471ad9ec050e46cf.pdf": "boschin2020ki4",
    "cda7a1bdce2bfa77c2d463b631ff84b69ce3c9ed.pdf": "wang20199fe",
    "f76a6e8f059820667af53edbd42d33fc4bca85fd.pdf": "myklebust201941l",
    "40667a731593a44d4e2f9391f1d14f368321b751.pdf": "kartheek2021aj7",
    "6bf53a97f5a3f5b0375f4702cbec28d8e9ab61c0.pdf": "sha2019plw",
    "4ae2631fb5e99cb64ff7d6e7ed3a1e6b0bedd269.pdf": "lu2020x6y",
    "d76b3bf29366b4f0902ea145a3f7c020a35f084f.pdf": "zhang2020c15",
    "151c9bb547306d66ba252be7c20e35f711e9f330.pdf": "li2020ek4",
    "c0827be29366be4b8cfa0dfbef4ead3f7b08f562.pdf": "li2020he5",
    "2d38cdaf2e232b5d1cb1dce388aa0fe75babcf29.pdf": "kim2020zu3",
    "d6508e8825a6a1281ad415de47a2f108d98df87d.pdf": "zhu2018l0u",
    "18101998fb57704b79eb4c4c37891144ede8f8b9.pdf": "do20184o2",
    "23830bb104b25103162ec9f9f463624d9a434194.pdf": "ma20194ua",
    "77e23cd2437c6afb16082793badbb02842442e13.pdf": "zhang2020wou",
    "92351a799555df8d49465c2d4959118030339cc0.pdf": "zhang2019hs5",
    "6de535eb1b0024887227f7987e6eb22478af2a95.pdf": "wang20198d2",
    "be7b102315ce70a7e01eb87c1140dd6850148e8b.pdf": "tran20195x3",
    "5b6a24ea3ffdccb14ce0267a815845c62ef026c9.pdf": "xiong2018fof",
    "75f7e3459e53fa0775c941cb703f049797851ef0.pdf": "radstok2021yup",
    "3ea066e35fdd45162a7fa94e995abe0beeceb532.pdf": "zhao2020o6z",
    "c7a630751e45e3a74691bd0fc0880b4bf87be101.pdf": "zhang20182ey",
    "a2a7f85d2ba28750725c4956eb14d53f6a90f003.pdf": "jia20207dd",
    "bb0613ea0d39e35901aa0018de40deaf35cbbd5d.pdf": "zhu2019ir6",
    "509fa029989e89a4b82dd01ab75734aed937d684.pdf": "wang2021dgy",
    "4f2cc26b689cdac36ceb2037338eac65e7e5a193.pdf": "ning20219et",
    "7bb4cd36de648ca44cc390fe886ee70a4b2ad1ac.pdf": "sheikh20213qq",
    "93db6077c12cc83ea165a0d8851efb69b0055f3a.pdf": "rim2021s9a",
    "2f700be8a387101411a84199adfe30636e331752.pdf": "zhang20179i2",
    "2dba03d338170bde4e965909230256086bafa9f8.pdf": "elebi20182bd",
    "c2648a294ef2fc299e1dd959bc1f92973f9c9ebc.pdf": "garofalo20185g9",
    "62c50e300ee87b185401ce27323bbb3f5262fdff.pdf": "wang201825m",
    "66f19b09f644578f808e69f38d3e76f8b972f813.pdf": "chung2021u2l",
    "9b68475f787be0999e7d09456003e664d37c2155.pdf": "tran2019j42",
    "f0ac0c2f82886700dc7e7a178d597d33deebfc88.pdf": "shi2017m2h",
    "a5aeca7ef265b27ff6d9ea08873c9499632b6439.pdf": "zhang2017ixt",
    "8412cc4dd7c8d309d7a84573637d4daaad8d33b5.pdf": "zhu20196p1",
    "8be21591c29d68d99e89a71fc7755f09f5eed3a1.pdf": "kertkeidkachorn2019dkn",
    "6493e6d563282fcb65029162a71cd2cb8168765b.pdf": "zhu2019zqy",
    "d5eabc89e2346411134569a603e63a143d1d6552.pdf": "zhang20193g2",
    "89cf9719b97e69f5bb7d715d5a16609676c14e86.pdf": "liu2019fcs",
    "1c1b5fd282d3a1fe03a671f7d13092d49cb31139.pdf": "kanojia20171in",
    "7f7137d3e1de7e0e801c27d5e8b963dfd6d94eb4.pdf": "gao2018di0",
    "49899fd94cd272914f7d1e81b0915058c25bb665.pdf": "mai2018egi",
    "e64557514ab856d22ddbb34bc23ffb7085d5d6b0.pdf": "xiao2016bb9",
    "7eece37709dceba5086f48dc43ac1a69d0427486.pdf": "liu2024q3q",
    "83424a4fea2e75311632059914bf358bc045435f.pdf": "zhang2024cjl",
    "3f8b13ede9f4d3a770ec8b4771b6036b9f603bfa.pdf": "su2023v6e",
    "ac0c9afa9c19f0700d903e00a92e83e41587add3.pdf": "zhu2023bfj",
    "f42d060fb530a11daecd90695211c01a5c264f8d.pdf": "liu2024to0",
    "7aca91d068d20d3389b28b8277ebc3d488be459f.pdf": "wang2024vgj",
    "fa07384402f5c9d5b789edf7667bbcc555f381e3.pdf": "li2024920",
    "48c2e0d87b84efca7f11462bbdac1be1177e2433.pdf": "lee202380l",
    "51c18009b2c566d7cddc934b2cf9a1bca813f58f.pdf": "shokrzadeh2023twj",
    "5cbf9bc26b3d0471cb37c3f4a931990b1260d82d.pdf": "gao2023086",
    "4383242be5bdfb30ffa84e58cc252acfb58d4878.pdf": "li2024sgp",
    "f26d45a806d1f1319f37eb41b8aa87d768a1d656.pdf": "xue2023qi7",
    "7b569aecc97f5fe57ce19ca0670a6b1bc62c7f7c.pdf": "duan2024d3f",
    "8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf": "chen20246rm",
    "e83b693a44ec32ddfb084d13138e8d7ebc85a7c3.pdf": "zhu2022o32",
    "f284977aa917be0ff15b835b538294b827135d19.pdf": "mitropoulou20235t0",
    "f3fa1ef467c996b30242124a298b5b9d031e9ed5.pdf": "shomer2023imo",
    "61ef322fba87ccfd36c004afc875542a290fe879.pdf": "wang202490m",
    "5bef4d28d12dd578ce8a971d88d2779ec01c7ec5.pdf": "li2024bl5",
    "c441b2833db8bd96b4ad133679a68f79d464ef59.pdf": "li2024y2a",
    "edfbe0b62b9f628858d05b64bd830cf9b0a1ab74.pdf": "jia2023krv",
    "88e700e9fd6c14f3aa4502176a60512ca4020e35.pdf": "huang2023grx",
    "942541df1b97a9d1e46059c7c2d11503adc51c4c.pdf": "wang2023s70",
    "abc424e17642df01e0e056427250526bc624f762.pdf": "hou20237gt",
    "825d7339eadadd2baf962f7d3c8fe7dc0cdc9819.pdf": "jiang2023opm",
    "b6839f89a59132f0e62011a218ec229a27ffff6b.pdf": "lu2022bwo",
    "59116a07dbdb3cdeebb20085fdfde8b899de8f6a.pdf": "djeddi2023g71",
    "3cab78074e79122fd28cd76f37fd8805e8e4fc31.pdf": "zhang20243iw",
    "ed21098804490b98899bcb7195084983ce69ed6c.pdf": "le2023hjy",
    "354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf": "yao2023y12",
    "c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf": "li2023y5q",
    "dc949e502e35307753a1acbcdf937f0cd866e63b.pdf": "yang2022j7z",
    "a64167fcaa7a487575c6479510e57795afc9974e.pdf": "banerjee2023fdi",
    "f9a575349133b2d4bf512cfb7754fca6d13b0a81.pdf": "hu20230kr",
    "5f850f1f522f959e2d3dcad263d05b0fdbb187c3.pdf": "li2023wgg",
    "4c68ee32d3db73d4d05803c1b3f2f4b929a88b78.pdf": "hao2022cl4",
    "2ac47be80b02a3ff1b87c46cf2b8c27e739c2873.pdf": "khan20222j1",
    "b5aedc0464d04aa3fed7e59a141d9be7ee18c217.pdf": "le2022ybl",
    "463c7e28be13eb02620ad7e29b562bf6e5014ba2.pdf": "liang202338l",
    "7009fd9eb533df6882644a1c8e1019dc034b9cc5.pdf": "khan2022ipv",
    "e186e5000174ea70729c90d465e60279c5f88646.pdf": "he2022e37",
    "70dc4c1ec4cda0a7c88751fb9a6b0c648e48e11f.pdf": "shen2022d5j",
    "5a8c6890e524b708dc262d3f456c985e8a46d7d1.pdf": "di20210ib",
    "86631a005e1a88a66926ac0c364ed0101a02b7e7.pdf": "niu2020uyy",
    "92b9aeabaaac0f20f66c5a68fbb4fc268c5eaae5.pdf": "nie2023ejz",
    "ce494973ceefe5ac011f7e9879843530395fa9db.pdf": "li2022du0",
    "25edfb99d3b8377a11433cf7be2bcd9f8bfbdb87.pdf": "daruna2022dmk",
    "709a128e752414c973613814ddc2509f2abe092f.pdf": "zhou20210ma",
    "18fd8982051fc1de652a9882c2c52db11bca646b.pdf": "kun202384f",
    "a7f0b4776d3df11cf0d0e72785c3035cc744726c.pdf": "dong2022taz",
    "e2783f8aa4c61443760a8754cd6d88165d50b213.pdf": "kamigaito20218jz",
    "77fedfa533871c6c4218285493f725d5df4e74e5.pdf": "krause2022th0",
    "695ef4cf57b4fd0c7ec17a6e10dffade51f38179.pdf": "zhang20213h6",
    "90d5e74b18d03f733c6086418bfe9b20bb6a0a69.pdf": "li2021tm6",
    "c495b2780accfbb53a932181e3c9fd957d16895d.pdf": "wang2020au0",
    "85bfec413860c072529ab8399676ab4b072f2e34.pdf": "wei20215a7",
    "a89f61021e5382912aaeb3f69a6d8a6265787af4.pdf": "zhang2021rjh",
    "b3cbbc1f34a20c22853f3dd347fd635b2e414fd5.pdf": "sheikh202245c",
    "df7265b4652b21bc690497b3967a708d811ddd23.pdf": "ren2021muc",
    "f6182d5c14c6047d197f1af842862653a13238f2.pdf": "eyharabide2021wx4",
    "082856e9b36fac60b9b9400abffaff0e74552fe1.pdf": "hong2020hyg",
    "b25744d3c5d93e49b1906991dc8b5426ea2cf51d.pdf": "huang2020sqc",
    "18bcad2521cbe8df9d84b1adff1dd57c72c68a9d.pdf": "kurokawa2021f4f",
    "bdd6c1a6695e3d201b70f4a913ffc758b74216e7.pdf": "mohamed2021dwg",
    "e93565f447a42b158df27ba75385f5e2fc30dde7.pdf": "gebhart2021gtp",
    "cf436f34ca6aabe1971c3531d465ecaa3d480d68.pdf": "deng2024643",
    "76016197d7d4f2213a4ace29988c93285793e154.pdf": "liu2024zr9",
    "9730f484b84074c1d61c154211ea06cc6ff20940.pdf": "zhang2024zmq",
    "10c388fa25dd6f07707a414946e5b7a674e7155b.pdf": "he2024vks",
    "7e6a50b70223dc00c712a17537fb7e23f8fd5ad4.pdf": "zhang2024fy0",
    "ae58ebc99f67eed0de7f4ba2ca6f7ceb9ab056fb.pdf": "zhang2024ivc",
    "6ad02ad36e7a2c7d72d1a0b15ffc61dae2be1d7a.pdf": "jing2024nxw",
    "75ba0b92bcf095e7cd1544425f1818fed195f83f.pdf": "jiang2024zlc",
    "905d27e361c50da406439bdac25807dd38258fd8.pdf": "han2024u0t",
    "b2646d9ee88c3dd6822b039a38c9604932aaaf47.pdf": "quan2024o2a",
    "c7666fbaa49da21c465dbfabcf5fdd768b8c7b9e.pdf": "liu2024tc2",
    "f1b7682df472a88fbaac3e6049f638ecec6937e7.pdf": "hello2024hgf",
    "d66622beef468f7b934a5bf601cb8a3fcefe78f3.pdf": "li2024z0e",
    "20486c2fb358730ee99ae39b5e0a88d7b39ca720.pdf": "yan2024joa",
    "b49f6029d681ac286ab929238f5aef5f352767c8.pdf": "liu2024tn0",
    "c5a19440511a741edd1581d41d37d3e9b7088186.pdf": "wang20245dw",
    "822ad7c33316202a2511d300c6b8a263b758ad1a.pdf": "long2024soi",
    "ba61c59abb560ff47a8dd780c8ccffb0af5e14c2.pdf": "zhou2024ayq",
    "b3c340aa22bcd183c41836ef7265d656f741911f.pdf": "huang2024t19",
    "7c82aa0ae4b4e027a2df8afe9bbeccf88368c62f.pdf": "lu2024fsd",
    "0d9a788260e3abff4794d79f72b2b5ab2fb5abe5.pdf": "liu2024yar",
    "6cba788eea4fdb3bd0d1db4ecdd8a70040b81e62.pdf": "khan20242y2",
    "6c195ec2d5a491ffca9ab893968c4d44a6d0ce7d.pdf": "xue2025ee8",
    "37b274eb6fa68dede9f4aaad6dec1e2ea56095ce.pdf": "long20248vt",
    "9be88067bd7351b36bb0c698f5559ced3918a1d5.pdf": "huang20240su",
    "e0d17f8b2fffff6c5eaf3f13bc45126196ddd128.pdf": "wang2024nej",
    "a4b6e13efa80bedf8e588ac69f91fdaecc8e5077.pdf": "wang2024c8z",
    "ccb6674576de48f8cfd99374c3b737a94dc3cb98.pdf": "liu2024x0k",
    "75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf": "li2024uio",
    "8ff387296878f23632a588076823b160673866ab.pdf": "zhang2024z78",
    "6a66b459955959c4b8a67bd298ed291506923b7a.pdf": "wang2024534",
    "6b69c8848a1cc50ed8775beb483c71cfc314c66b.pdf": "ni202438q",
    "d57e01d80c7f0f86b5e3f096b193ab9210e9095f.pdf": "nie202499i",
    "a9bfb9ab236553768782f2b90a69c5625f033186.pdf": "wang2024d52",
    "6903aea3553a449257388580028e0bddf119d021.pdf": "mao2024v2s",
    "767d56fe80f7681b97943a8bff39f0b580e4acd8.pdf": "jafarzadeh202468v",
    "9e7799ef313143aa9c0669a7d1918fcfd5d21359.pdf": "wang2024dea",
    "563b3d57927b688e59322dbbfc973e5f1b269584.pdf": "lu202436n",
    "984c18fa61b10b6d1c34affc98f27ca8344d4224.pdf": "han2024gaq",
    "4a0048f1942a68e7c39adac43588d1604af26fc7.pdf": "liu2024jz8",
    "49dfd47177fa3aeab8a6bea82a77ec8bdb93bf1e.pdf": "he2024y6o",
    "2a5c888b2df4fd8c49aef46ee065422b00b178c0.pdf": "fang20243a4",
    "48c07506022634f332b410fb59dca9f61f89b032.pdf": "zhang2024h9k",
    "575af1587dea578d48eb27f45f008203565d9170.pdf": "li2024wyh",
    "7bd50842503e23e6479447b98912ac482ef43adc.pdf": "dong2024ijo",
    "4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf": "wang20246c7",
    "c3861a930a65e8d9ee7ab9f0a6ee71e0e59df7ed.pdf": "zhang2024yjo",
    "217a4712feae7d7590d813d23e88f5fbb4f2c37f.pdf": "liang20247wv",
    "cf696a919b8476a4d74b8b726e919812a2f05779.pdf": "liu2024t05",
    "91d5aa3d43237ec60266563ec6e8079f86532cfa.pdf": "pham20243mh",
    "58480444670ff933fe644563f7e2948a79503442.pdf": "li2024gar",
    "9b836b4764d4f6947ac684fd4ba3e8c3597d95bd.pdf": "li2024nje",
    "bd0e8d6db97111686d02b51134f87439f8f1acfa.pdf": "bao20249xp",
    "bea79d59ab3d203d06c88ebf67ac47cb34adeaa7.pdf": "xu2024fto",
    "241904795d94dcb1946ad46c9184c59899783af1.pdf": "liang2024z0q",
    "55dab161c25d1dd04fbeecdeca085274bfe8463f.pdf": "liu2024ixy",
    "3ff6b617cd839c9d85cb7b58aa6ad56e95b6cf69.pdf": "dong2025l9k",
    "9560ca767022020ccf414a2a8514f25b89f78cb3.pdf": "zhang2025ebv",
    "d5c8dcc8f5c87c269780c7011a355b9202858847.pdf": "liu20242zm",
    "a77b3c5f532e61af63a9d95e671ce02d8065ee24.pdf": "yang2024lwa",
    "2d12d1cec23e1c26c65de52100db70d91ca90035.pdf": "li20246qx",
    "4b1d0cf2b99aec85cdedceaef88c3a074de79832.pdf": "liu2024mji",
    "0845cea58467d372eb296fa1f184ecabe02be18b.pdf": "chen2024efo",
    "6a9caace1919b0e7bb247f0ecb585068c1ec4ff8.pdf": "chen2024uld",
    "30321b036607a7936221235ea8ec7cf7c1627100.pdf": "wang2017zm5",
    "e03b8e02ddda86eafb54cafc5c44d231992be95a.pdf": "li2021qr0"
  },
  "sections": {
    "Introduction": "\\section{Introduction}\n\\label{sec:introduction}\n\nThe landscape of artificial intelligence has been profoundly shaped by advancements in knowledge representation, evolving from early symbolic systems to the sophisticated structures of modern knowledge graphs (KGs). These KGs, which organize world knowledge into networks of entities and relations, have become indispensable for various AI tasks, yet their inherent symbolic nature presents significant challenges. Issues such as data sparsity, computational inefficiency in large-scale reasoning, and difficulty in capturing nuanced semantic similarities often hinder their full potential \\cite{68f34ed64fdf07bb1325097c93576658e061231e}. This introductory section establishes the foundational context for understanding Knowledge Graph Embeddings (KGEs), a paradigm-shifting approach that addresses these limitations by transforming discrete symbolic knowledge into continuous, low-dimensional vector spaces.\n\nKGE methods have emerged as a central pillar in modern knowledge graph research, enabling machines to understand, reason with, and leverage complex relational data more effectively. By converting entities and relations into dense embeddings, KGEs facilitate seamless integration with advanced machine learning models, thereby enhancing capabilities in diverse AI applications such as link prediction, entity alignment, question answering, and recommender systems. This section begins by providing a comprehensive background on knowledge graphs, tracing their evolution and highlighting their structural characteristics and inherent challenges. Subsequently, it delves into the core motivations behind the development of knowledge graph embedding techniques, explaining how they overcome the limitations of traditional symbolic representations. Finally, this section delineates the scope and organizational structure of the entire literature review, offering readers a roadmap through the intricate landscape of KGE research. By laying this essential groundwork, we aim to underscore the significance and trajectory of KGEs in transforming symbolic knowledge into actionable, machine-understandable formats, thereby advancing the broader field of artificial intelligence.\n\n\\subsection{Background: Knowledge Graphs}\n\\label{sec:1_1_background:_knowledge_graphs}\n\n\nKnowledge Graphs (KGs) represent a fundamental paradigm for organizing and representing world knowledge in a structured, machine-readable format. At their core, KGs are directed graphs composed of entities (nodes) and relations (edges), forming a collection of factual triplets in the form of (head entity, relation, tail entity) \\cite{ge2023, dai2020}. For instance, the triplet (Barack Obama, bornIn, Hawaii) explicitly states a factual relationship between two entities. This structured representation allows for explicit semantic connections, enabling machines to understand and process information in a manner closer to human cognition.\n\nThe historical trajectory of knowledge representation has seen a significant evolution, from early semantic networks and expert systems in artificial intelligence to the more formalized ontologies and the vision of the Semantic Web. These foundational efforts aimed to capture human knowledge in a symbolic form, providing a basis for logical reasoning and inference. With the advent of the internet and the explosion of digital information, the need for large-scale, interconnected knowledge bases became paramount. This led to the development of modern, expansive KGs such as Freebase (now largely integrated into Wikidata), DBpedia, and Wikidata itself \\cite{wang2014, lv2018, zhang2018}. These prominent examples serve as crucial repositories, aggregating and organizing vast amounts of world knowledge from diverse sources like Wikipedia, enabling a wide array of intelligent systems, from search engines and question-answering systems to recommender platforms \\cite{huang2019, sun2018}.\n\nDespite their immense utility and structured nature, symbolic KGs inherently face several significant challenges that limit their scalability, efficiency, and ability to handle real-world complexities. Firstly, reasoning with symbolic KGs, particularly when involving complex logical rules or multi-hop inference, can be computationally inefficient and resource-intensive, often exhibiting exponential complexity \\cite{ge2023, dai2020}. This makes real-time inference on large-scale KGs a formidable task. Secondly, KGs are almost always incomplete; real-world knowledge is vast and constantly evolving, making it practically impossible to explicitly represent every single fact. Symbolic methods struggle profoundly with this incompleteness, as they typically require explicit rules or complete data to infer missing links, leading to brittle and often inaccurate predictions in sparse environments. The \"data sparsity\" problem is a recurring theme, where many entities and relations have limited connections, hindering comprehensive analysis \\cite{ge2023, dai2020}.\n\nFurthermore, symbolic representations treat entities and relations as discrete, atomic tokens, which inherently limits their capacity to capture nuanced semantic similarities or implicit relationships. For example, while \"car\" and \"automobile\" are semantically very close, a purely symbolic KG would treat them as distinct, unrelated entities unless explicitly linked by a relation. This lack of inherent semantic fluidity makes it difficult to generalize knowledge or discover novel patterns based on underlying similarities. These limitationscomputational inefficiency, difficulty in managing growing data, challenges in handling incompleteness, and the inability to capture implicit semantic similaritiescollectively underscore the necessity for more advanced representation techniques. As highlighted in the broader context of knowledge graph embedding research, these issues motivate the fundamental shift towards embedding entities and relations into continuous, low-dimensional vector spaces, thereby transforming complex symbolic problems into more efficient vector operations and laying the \"bedrock for representing complex relational data in a machine-understandable format\" \\cite{cao2022}. This transition to embedding techniques is crucial for unlocking the full potential of KGs in modern AI applications, providing a robust and scalable foundation for knowledge inference and fusion \\cite{ge2023, dai2020}.\n\\subsection{Motivation for Knowledge Graph Embedding}\n\\label{sec:1_2_motivation_for_knowledge_graph_embedding}\n\nKnowledge Graphs (KGs) serve as powerful repositories of structured world knowledge, representing entities and their relationships in a symbolic, triple-based format (e.g., (subject, predicate, object)). While invaluable for many AI applications, traditional symbolic KGs inherently suffer from several critical limitations that impede their scalability, flexibility, and integration with modern machine learning paradigms. These limitations form the core motivation for the development of Knowledge Graph Embedding (KGE) techniques.\n\nFirstly, symbolic representations are inherently **sparse and discrete**, making it challenging to capture nuanced semantic similarities between entities and relations \\cite{dai2020, cao2022}. For instance, while a symbolic KG might state (``Paris'', ``locatedIn'', ``France'') and (``Berlin'', ``locatedIn'', ``Germany''), it struggles to infer that ``Paris'' and ``Berlin'' are both capital cities or that ``France'' and ``Germany'' are both European countries without explicit rules or additional facts. This sparsity also makes it difficult to generalize to unseen entities or relations, as there is no inherent notion of proximity or relatedness in the discrete symbolic space.\n\nSecondly, reasoning over large-scale symbolic KGs is often **computationally inefficient**. As KGs grow exponentially in size, performing complex queries or multi-hop reasoning becomes computationally expensive, often requiring graph traversal algorithms that do not scale well \\cite{dai2020}. Managing and processing vast amounts of discrete data poses significant challenges, leading to bottlenecks in real-world applications.\n\nThirdly, KGs are almost always **incomplete**, a pervasive issue that limits their utility. Many real-world facts are missing, and traditional symbolic methods struggle to infer these missing links without explicit, hand-crafted rules. This incompleteness directly impacts the performance of downstream tasks that rely on comprehensive knowledge.\n\nTo overcome these limitations, Knowledge Graph Embedding (KGE) emerged as a transformative approach, converting sparse, symbolic entities and relations into continuous, low-dimensional vector representations (embeddings) in a latent space \\cite{dai2020, cao2022}. This fundamental shift from symbolic to vector-based representation offers several profound advantages:\n\n\\begin{itemize}\n    \\item \\textbf{Scalability and Efficiency:} By representing entities and relations as dense vectors, KGE models transform complex symbolic problems into efficient vector operations, such as distance calculations or dot products. This significantly enhances computational efficiency and scalability, enabling KGs with millions of entities and relations to be processed and reasoned over more effectively. The \"Efficiency, Compression, and System Optimization\" subgroup highlights this, with works like \\cite{zhu2020} demonstrating how knowledge distillation can reduce embedding parameters by 7-15x and increase inference speed, and \\cite{wang2021} proposing lightweight frameworks for efficient storage and inference. More recently, system-level optimizations like \\cite{zheng2024} aim for general and efficient KGE learning systems, achieving significant speedups.\n    \n    \\item \\textbf{Capturing Nuanced Semantic Similarities:} In the continuous embedding space, semantically similar entities or relations are mapped to proximate vectors. This allows KGE models to inherently capture nuanced semantic relationships that are difficult to express symbolically. For instance, translational models like TransH \\cite{wang2014} and rotational models like RotatE \\cite{sun2018} learn to represent relations as transformations (translations on hyperplanes or rotations in complex space) that connect head and tail entities, thereby capturing patterns like symmetry, antisymmetry, inversion, and composition. This capability is central to the \"Core KGE Model Architectures and Expressiveness\" subgroup.\n    \n    \\item \\textbf{Handling Incompleteness and Enabling Link Prediction:} KGE models are particularly adept at addressing KG incompleteness through link prediction. By learning the underlying patterns of existing facts, they can infer the plausibility of unobserved triples. This is achieved by scoring candidate triples based on the learned embeddings, effectively transforming the problem of finding missing links into an efficient vector similarity search \\cite{rossi2020}. This capability is a cornerstone of KGE applications, as detailed in the \"Link Prediction and Knowledge Graph Completion\" section.\n    \n    \\item \\textbf{Seamless Integration with Modern Machine Learning Pipelines:} KGEs provide a powerful bridge between structured knowledge and modern machine learning (ML) techniques, especially deep learning. The continuous vector representations can be seamlessly integrated as features or pre-trained components into various ML models for tasks like natural language processing, computer vision, and recommendation systems. This allows KGs to enrich data-driven models with structured background knowledge, enhancing their performance and interpretability. The \"KGE for Downstream Applications and Explainability\" subgroup exemplifies this, with models like RKGE \\cite{sun2018} and CKGE \\cite{yang2023} leveraging embeddings for explainable recommendation, and systems like Marie and BERT \\cite{zhou2023} integrating KGEs for chemistry-specific question answering.\n    \n    \\item \\textbf{Facilitating Diverse AI Tasks:} Beyond link prediction, KGEs enable a wide array of AI tasks by converting complex symbolic problems into efficient vector operations. These include:\n    \\begin{itemize}\n        \\item \\textbf{Entity Alignment:} Identifying equivalent entities across different KGs by comparing their embeddings \\cite{sun2018, zhang2019}.\n        \\item \\textbf{Question Answering (QA):} Matching natural language questions to relevant facts in the KG by embedding both questions and KG elements into a common space \\cite{huang2019, zhou2023}.\n        \\item \\textbf{Recommendation Systems:} Modeling user-item interactions and preferences by embedding items and users within a KG context \\cite{sun2018, yang2023}.\n    \\end{itemize}\n\\end{itemize}\n\nWhile the conversion to vector space offers immense benefits, it also introduces challenges. Early KGE models, though efficient, often struggled to capture all complex relational patterns, leading to a continuous evolution towards more expressive geometric and deep learning models, as evidenced by the \"Core KGE Model Architectures\" and \"Geometric KGE\" subgroups. Furthermore, while embeddings make KGs more actionable for AI, the inherent interpretability of symbolic logic can sometimes be lost in dense vector spaces, spurring research into explainable KGEs \\cite{yang2023}. This strategic shift from explicit symbolic to implicit dense vector representations, as highlighted in the overall perspective, represents a fundamental progression in knowledge representation, making KGs more accessible, scalable, and powerful for a wide spectrum of AI applications.\n\\subsection{Scope and Structure of the Review}\n\\label{sec:1_3_scope__and__structure_of_the_review}\n\nThis literature review offers a comprehensive exploration of Knowledge Graph Embedding (KGE) research, meticulously tracing its evolution from foundational theoretical models to cutting-edge architectural advancements, critical practical considerations, and diverse real-world applications. The scope is designed to provide a pedagogical progression, beginning with core concepts and gradually building towards more sophisticated and specialized developments, thereby ensuring a coherent narrative that captures the field's dynamic trajectory. We aim to synthesize the vast landscape of KGE, offering a structured roadmap for understanding its complexities and future directions.\n\nThe review commences with an \\textbf{Introduction} (Section 1), which sets the stage by outlining the fundamental role of knowledge graphs in AI and elucidating the core motivations behind embedding them into continuous vector spaces. This initial section establishes why KGE has become indispensable for overcoming the limitations of symbolic representations, such as scalability and the inability to capture nuanced semantic similarities.\n\nFollowing this, \\textbf{Foundational KGE Models and Geometric Paradigms} (Section 2) delves into the bedrock of KGE research. This section examines early and influential models, primarily those based on geometric and algebraic principles. It discusses how relations are conceptualized as transformations within embedding spaces, detailing the progression from simple translational models like TransH \\cite{wang2014} and TransD \\cite{ji2015} to more complex rotational approaches such as RotatE \\cite{sun2018}. These foundational works, categorized in the thematic taxonomy as \"Core Translational Models and Their Extensions\" and \"Advanced Geometric Models,\" are critically analyzed for their ability to capture diverse relational patterns, including symmetry, antisymmetry, and composition, while balancing model capacity and computational efficiency. The limitations of earlier models in handling complex relation patterns often necessitated the development of more expressive geometric operations.\n\nThe review then transitions to \\textbf{Deep Learning Architectures for Knowledge Graph Embedding} (Section 3), reflecting a significant paradigm shift in the field. This section explores how Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformer models have been adapted to learn more expressive and context-aware representations. This progression highlights the move from predefined geometric transformations to data-driven feature extraction, enabling the capture of intricate structural patterns and non-linear relationships that were challenging for simpler models.\n\nBuilding upon these architectural advancements, \\textbf{Enriching KGE: Auxiliary Information, Rules, and Multi-modality} (Section 4) investigates methods that transcend purely structural information. This section details the integration of auxiliary data (e.g., entity types, attributes), explicit logical rules, and multi-modal information (e.g., text, images) to enhance embedding quality. This critical area addresses the inherent incompleteness and sparsity of KGs, demonstrating how external knowledge can lead to more robust, semantically rich, and interpretable embeddings.\n\nRecognizing the dynamic nature of real-world knowledge, \\textbf{Dynamic, Inductive, and Distributed KGE} (Section 5) focuses on models capable of handling temporal changes, learning embeddings for unseen entities, and operating in privacy-preserving, distributed environments. Models like HyTE \\cite{dasgupta2018} are crucial here, explicitly incorporating time to enable temporally aware inference. This section underscores the field's evolution towards more adaptable and scalable solutions, moving beyond static and centralized assumptions to meet the demands of evolving knowledge bases.\n\n\\textbf{Practical Considerations: Efficiency, Robustness, and Evaluation} (Section 6) addresses the critical challenges in deploying and evaluating KGE models. It covers strategies for improving computational efficiency, enhancing robustness against noisy data, and optimizing training processes. This section also critically examines the importance of rigorous evaluation, benchmarking, and reproducibility, drawing insights from comprehensive comparative analyses of state-of-the-art methods \\cite{rossi2020}. The discussion here highlights how experimental setups and reporting practices can significantly affect generalizability, emphasizing the need for standardized benchmarks and transparent methodologies to ensure reliable scientific progress. Survey papers like \\cite{dai2020} and \\cite{cao2022} further underscore the importance of systematic classification and comparison of KGE techniques based on their underlying representation spaces and performance across various benchmarks.\n\nThe review culminates with \\textbf{Applications and Real-World Impact of KGE} (Section 7), showcasing the diverse utility of KGE across various AI tasks. This includes core applications like link prediction and knowledge graph completion, as well as more complex tasks such as entity alignment \\cite{sun2018, zhang2019}, question answering \\cite{huang2019}, and recommender systems \\cite{sun2018}. This section demonstrates how KGE bridges the gap between structured knowledge and practical AI problems, providing tangible benefits in various domains. The inclusion of application-specific KGE frameworks from the taxonomy, such as KEQA \\cite{huang2019} and RKGE \\cite{sun2018}, illustrates the versatility and real-world impact of these embedding techniques.\n\nFinally, the \\textbf{Conclusion and Future Directions} (Section 8) synthesizes the key developments, identifies persistent open challenges, theoretical gaps, and practical limitations, and outlines emerging trends and ethical considerations. This forward-looking perspective aims to inspire new research and guide the responsible advancement of KGE technologies, providing a comprehensive roadmap for navigating the complex and rapidly evolving landscape of knowledge graph embedding research.\n",
    "Foundational KGE Models and Geometric Paradigms": "\\label{sec:foundational_kge_models__and__geometric_paradigms}\n\n\\section{Foundational KGE Models and Geometric Paradigms}\n\\label{sec:foundational_kge_models_and_geometric_paradigms}\n\nBuilding upon the motivation to overcome the limitations of sparse symbolic knowledge graphs, this section delves into the pioneering efforts that established the bedrock of knowledge graph embedding research. It explores the early and influential models that first translated entities and relations into continuous vector spaces, laying the theoretical and practical groundwork for subsequent advancements. The primary focus here is on geometric and algebraic paradigms, which conceptualize relations not merely as static links, but as dynamic transformations or interactions within these learned embedding spaces. This fundamental shift enabled the capture of implicit semantic similarities and relational patterns that were previously inaccessible.\n\nThe evolution began with simple yet powerful translational models, such as TransE and its extensions like TransH \\cite{wang2014} and TransD \\cite{ji2015}. These models represent relations as direct translations from head to tail entities, offering an efficient way to capture basic relational patterns. However, their inherent limitations in modeling complex properties like symmetry, antisymmetry, and composition spurred the development of more sophisticated approaches. This led to the emergence of rotational models, exemplified by RotatE \\cite{sun2018}, which leverage rotations in complex or higher-dimensional spaces to capture richer and more diverse relational semantics. Further innovations explored other geometric and algebraic structures, including embeddings on Lie groups or using quaternions, to enhance expressiveness and address specific challenges. Collectively, these foundational models were instrumental in demonstrating how geometric operations could effectively capture intricate relational patterns, significantly improving the expressiveness and utility of KGEs and forming the essential basis for the field's rapid expansion into more advanced architectures.\n\n\\subsection{Core Translational Models and Extensions}\n\\label{sec:2_1_core_translational_models__and__extensions}\n\nThe advent of knowledge graph embedding (KGE) marked a significant paradigm shift from purely symbolic knowledge representation to continuous vector spaces, offering enhanced efficiency and expressiveness for tasks such as link prediction and knowledge graph completion. At the forefront of this transformation were the translational models, which posited that a relation could be represented as a translation operation in an embedding space, moving a head entity vector closer to a tail entity vector. This foundational idea was first popularized by TransE, a pioneering model for its simplicity and computational efficiency \\cite{wang2014}. TransE models a triple $(h, r, t)$ by enforcing the constraint $\\mathbf{h} + \\mathbf{r} \\approx \\mathbf{t}$, where $\\mathbf{h}, \\mathbf{r}, \\mathbf{t}$ are the embeddings of the head entity, relation, and tail entity, respectively. While remarkably effective for its time, TransE exhibited limitations in handling complex relational patterns, particularly one-to-many, many-to-one, and reflexive relations, where a single relation vector could not adequately distinguish between multiple valid tail entities for a given head, or vice-versa.\n\nTo address these inherent limitations, subsequent models extended the translational paradigm by introducing more sophisticated mechanisms. TransH emerged as a notable improvement, proposing to model relations as translations on relation-specific hyperplanes rather than directly in the entity embedding space \\cite{wang2014}. Specifically, for a triple $(h, r, t)$, TransH projects the head and tail entity embeddings ($\\mathbf{h}, \\mathbf{t}$) onto a hyperplane defined by the relation $\\mathbf{r}$'s normal vector $\\mathbf{w}_r$, resulting in projected entities $\\mathbf{h}_{\\perp}$ and $\\mathbf{t}_{\\perp}$. The translational assumption then applies to these projected vectors: $\\mathbf{h}_{\\perp} + \\mathbf{d}_r \\approx \\mathbf{t}_{\\perp}$, where $\\mathbf{d}_r$ is the relation-specific translation vector on the hyperplane. This mechanism allows TransH to better distinguish entities involved in one-to-many or many-to-one relations, as different entity pairs can be projected onto different points on the hyperplane while sharing the same relation vector. For instance, if a person has multiple children, TransH can project the person and each child onto the 'has\\_child' hyperplane, allowing distinct representations for each child while maintaining the 'has\\_child' relation. This approach offered a crucial trade-off, significantly improving expressiveness for complex relation types with almost the same model complexity as TransE, thereby maintaining scalability \\cite{wang2014}. The recent review by \\cite{asmara2023} further underscores TransH's importance in addressing these early challenges.\n\nBuilding upon the concept of relation-specific transformations, TransD further refined the translational approach by introducing dynamic mapping matrices for entities and relations \\cite{ji2015}. Unlike TransH, which uses a single hyperplane per relation, TransD employs two vectors for each entity and relation: one representing its meaning and another for constructing a dynamic mapping matrix. This allows for more fine-grained, entity-specific projections, where the projection matrix for a relation is dynamically constructed based on both the entity and relation vectors. The core idea is that different entities might interact with a relation in different ways, and a static projection (as in TransH) might not capture this diversity. TransD's dynamic mapping matrices provide a more adaptive mechanism to project entities into relation-specific spaces, thereby accounting for the diversity of both relations and entities. A significant advantage of TransD over its predecessors like TransR/CTransR (which used static, larger projection matrices) is its reduced parameter count and avoidance of computationally intensive matrix-vector multiplication operations, making it more scalable for large knowledge graphs \\cite{ji2015}. This efficiency gain, while increasing expressiveness, was a critical step in making KGE models practical for real-world applications.\n\nThese core translational models and their extensions collectively established a fundamental paradigm for KGE. They demonstrated that representing symbolic knowledge in continuous vector spaces could not only be efficient but also expressive enough to capture intricate relational semantics. While TransH and TransD significantly improved upon TransE's ability to model one-to-many/many-to-one relations, they still operated within the limitations of a Euclidean embedding space and relatively simple geometric transformations. This inherent simplicity, while beneficial for efficiency, meant they struggled with more complex logical patterns such as symmetry, antisymmetry, inversion, and composition, which later models like RotatE would address more elegantly through rotational transformations in complex spaces \\cite{sun2018}. Nevertheless, the foundational work of TransE, TransH, and TransD laid the essential groundwork, proving the viability of the embedding approach and setting the stage for the diverse array of KGE models that continue to influence modern research, as highlighted in various surveys \\cite{dai2020, cao2022}. Their emphasis on balancing model capacity with computational efficiency remains a crucial design principle in the field.\n\\subsection{Rotational and Complex Space Embeddings}\n\\label{sec:2_2_rotational__and__complex_space_embeddings}\n\nWhile foundational translational models like TransE and TransH \\cite{wang2014} offered a significant step forward in knowledge graph embedding (KGE) by modeling relations as vector translations, they often struggled with capturing the full spectrum of complex relational semantics, such as symmetry, antisymmetry, inversion, and composition \\cite{rossi2020}. This limitation spurred the development of models that leverage rotations in complex or higher-dimensional spaces, offering more nuanced and powerful transformations to represent these intricate logical patterns. This represents a key methodological shift within the \"Core KGE Model Architectures and Expressiveness\" and \"Geometric and Algebraic KGE Models for Complex Relations\" subgroups, moving beyond simpler linear operations to richer algebraic structures.\n\nA seminal contribution in this direction is RotatE \\cite{sun2018}, which defines each relation as a rotation from the head entity to the tail entity in a complex vector space. By representing entities as vectors and relations as Hadamard products with complex-valued relation vectors (which correspond to rotations), RotatE inherently captures symmetry (rotation by $\\pi$), antisymmetry (rotation by non-$\\pi$ angles), inversion (rotation by negative angle), and composition (sequential rotations). This elegant formulation proved highly effective for modeling complex patterns and significantly outperformed existing state-of-the-art models for link prediction on benchmark datasets \\cite{sun2018}. The success of RotatE highlighted the expressive power of complex space embeddings, demonstrating how algebraic structures could directly encode logical properties.\n\nBuilding upon this rotational paradigm, researchers explored extensions to higher-dimensional Euclidean and non-Euclidean spaces. Rotate3D \\cite{gao2020} extends the concept of relations as rotations to a three-dimensional Euclidean space. A key motivation for Rotate3D was to capture non-commutative composition patterns, which are essential for multi-hop reasoning and are naturally supported by rotations in 3D space. While RotatE primarily operates in a 2D complex plane for each dimension, Rotate3D generalizes this to a full 3D rotation, allowing for a richer set of transformations. Similarly, Orthogonal Relation Transforms \\cite{tang2019} further generalize this idea by employing high-dimensional orthogonal transforms, which encompass rotations and reflections, to model relations. This approach aims to retain the benefits of rotational models (symmetry, inversion, composition) while enhancing modeling capacity for complex relations like N-to-1 and 1-to-N by integrating graph context.\n\nThe pursuit of even more expressive geometric transformations led to models like HousE \\cite{li2022}, which introduces Householder parameterization. Householder transformations, a type of reflection, can be generalized to represent rotations and projections in high-dimensional spaces. HousE aims to simultaneously capture crucial relation patterns and mapping properties, theoretically generalizing existing rotation-based models while extending rotations to higher dimensions. This exemplifies the continuous effort to find more powerful mathematical tools to encode relational semantics.\n\nFurther enhancing the complexity of transformations, CompoundE \\cite{ge2022} and its 3D extension, CompoundE3D \\cite{ge2023}, propose using compound geometric operations, including translation, rotation, and scaling. These models treat relations not as a single operation but as a cascade of multiple transformations, suggesting that a richer set of combined operations can lead to better modeling capacity. CompoundE, by framing itself within group theory, demonstrates that several existing KGE models are special cases of its generalized framework, highlighting a trend towards unifying diverse geometric approaches. While these compound operations offer increased expressiveness, they also introduce greater model complexity and potentially higher computational costs, a common trade-off in KGE research \\cite{cao2022}.\n\nA significant recent development is the use of quaternions, an extension of complex numbers to four dimensions, to represent relations. ConQuatE \\cite{chen2025} leverages quaternion rotations to address the challenge of polysemy in knowledge graphs, where entities can exhibit different semantic characteristics depending on the relation. By incorporating contextual cues from various connected relations through efficient vector transformations in quaternion space, ConQuatE aims to capture diverse relational contexts without requiring extra information beyond original triples. This approach offers a novel way to handle the nuanced semantic variations that simpler rotational models might overlook, particularly for link prediction and multihop reasoning.\n\nThe \"Geometric and Algebraic KGE Models for Complex Relations\" subgroup analysis highlights that while these models achieve state-of-the-art performance, a common limitation is the potential for increased computational cost and parameter count, which can affect scalability to extremely large KGs. Moreover, the empirical validation often relies heavily on standard link prediction metrics, which may not fully capture the nuances of all the complex patterns these models aim to capture, especially for tasks like set retrieval or complex logical reasoning. For instance, MQuinE \\cite{liu2024} identifies and addresses a theoretical deficiency, termed the \"Z-paradox,\" in some popular KGE models, demonstrating that even advanced models can suffer from subtle expressiveness issues that degrade performance on challenging test samples. This underscores that merely introducing complex operations is insufficient; theoretical soundness and complete expressiveness are paramount.\n\nThe intellectual trajectory in this area, as noted in the \"Core KGE Model Architectures and Expressiveness\" subgroup, shows a clear progression from specific rotational models to more theoretically grounded and generalized orthogonal transformations. HolmE \\cite{zheng2024} introduces a KGE model whose relation embedding space is \"closed under composition,\" a crucial property for inherently modeling under-represented (long-tail) composition patterns and extrapolating to unseen relations. This addresses a theoretical gap where prior KGEs often considered relations compositional only if well-represented in training data. Similarly, GoldE \\cite{li2024} proposes a universal orthogonal parameterization based on a generalized Householder reflection, aiming to unify dimensional extension and geometric unification with theoretical guarantees, thereby capturing both logical patterns and topological heterogeneity. SpherE \\cite{li2024} further extends rotational embeddings by representing entities as spheres instead of vectors, specifically targeting the challenging problem of set retrieval and many-to-many relations, while maintaining interpretability. These advancements demonstrate a continuous drive to enhance the fundamental expressiveness of KGE models, ensuring they can effectively handle the complexities and imperfections of real-world knowledge graphs.\n\\subsection{Other Geometric and Algebraic Innovations}\n\\label{sec:2_3_other_geometric__and__algebraic_innovations}\n\nBeyond the foundational translational and rotational paradigms, Knowledge Graph Embedding (KGE) research has continuously sought to refine its mathematical underpinnings by exploring a broader spectrum of geometric spaces and algebraic transformations. This pursuit is driven by the need for more expressive, theoretically sound, and robust representations capable of capturing the intricate nuances of real-world knowledge graphs.\n\nOne significant direction involves embedding entities and relations within non-Euclidean spaces, particularly Lie groups, to circumvent inherent limitations of standard vector spaces. \\cite{ebisu2017} introduced \\textbf{TorusE}, a pioneering model that embeds entities on a Lie group, specifically a torus. The primary motivation for TorusE was to address the regularization problems encountered by models like TransE, where forcing entity embeddings onto a sphere in Euclidean space could warp representations and adversely affect link prediction accuracy. By leveraging the compact nature of a torus, TorusE naturally avoids the need for explicit regularization, as the space itself is bounded. While innovative in its choice of embedding space, TorusE still adheres to a translation-like principle, defining relations as translations within the Lie group. However, its geometric complexity, while elegant, might not inherently capture all forms of complex relation patterns, such as compositionality, as effectively as models designed with specific algebraic operations for such patterns.\n\nAnother critical area of innovation lies in scrutinizing and redefining the metric used within the embedding space. \\cite{yang2021} presented \\textbf{CyclE}, which critically examines the implications of the widely adopted Minkowski metric in KGE. The authors argue that the choice of metric significantly influences the expressiveness of the embedding space and propose a novel \"Cycle metric\" based on the oscillation property of periodic functions. Their quantitative analysis suggests that a smaller function period in the Cycle metric leads to superior expressive ability. CyclE, by combining this new metric with popular KGE models, demonstrated enhanced performance. This work highlights a fundamental aspect of geometric KGE: the distance function itself is a crucial design choice. However, focusing solely on the metric, while foundational, may not inherently provide the rich *transformations* required to model complex logical patterns like transitivity or hierarchy without further architectural or operational enhancements.\n\nA more direct approach to enhancing modeling capacity involves introducing advanced algebraic transformations. \\cite{li2022} proposed \\textbf{HousE}, a powerful KGE framework that leverages Householder parameterization. HousE employs two types of Householder transformations: Householder rotations to achieve superior capacity for modeling relation patterns and Householder projections to handle sophisticated relation mapping properties (e.g., 1-to-N, N-to-1). Theoretically, HousE is capable of simultaneously modeling crucial relation patterns and mapping properties, and it generalizes existing rotation-based models by extending rotations to high-dimensional spaces. Empirically, HousE achieved state-of-the-art performance on several benchmarks, indicating its enhanced expressiveness compared to simpler rotation-based models like \\cite{gao2020} Rotate3D or \\cite{tang2019} Orthogonal Relation Transforms. The strength of HousE lies in its mathematically robust and versatile transformations, offering a richer set of operations than basic rotations.\n\nBuilding upon the idea of combining multiple operations, \\cite{ge2022} introduced \\textbf{CompoundE}, which integrates translation, rotation, and scaling operations into a cascaded compound transformation. This model views relations as complex geometric manipulations, demonstrating that a synergy of these operations can lead to highly expressive embeddings. Further extending this concept, \\cite{ge2023} developed \\textbf{CompoundE3D}, which leverages 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear. CompoundE3D offers multiple design variants, allowing for flexibility to match the rich underlying characteristics of diverse knowledge graphs. These compound operation models represent a significant evolutionary step, as they generalize many existing scoring-function-based KGE models as special cases, effectively encompassing the strengths of both translational and rotational approaches while adding further dimensions of transformation. This contrasts with models like \\cite{yang2019} TransMS, which focuses on multidirectional semantics within a translation framework, or \\cite{peng2020} LineaRE, which models relations as simple linear functions.\n\nOther notable algebraic innovations include \\cite{song2021} \\textbf{Rot-Pro}, which combines projection and relational rotation to specifically model transitivity, a common but challenging relation pattern. \\cite{zhang2022} \\textbf{TranS} introduces synthetic relation representations within transition-based frameworks to better handle complex scenarios where the same entity pair might have different relations. More recently, \\cite{liu2024} proposed \\textbf{MQuinE}, which directly addresses and cures a theoretical deficiency termed the \"Z-paradox\" in some popular KGE models, thereby ensuring stronger expressiveness and theoretical soundness. The use of advanced algebraic structures is further exemplified by \\cite{chen2025} \\textbf{ConQuatE}, which leverages quaternion rotations to capture diverse relational contexts and address the polysemy issue, where entities exhibit different semantic characteristics depending on the relation.\n\nCollectively, these innovations highlight a continuous intellectual trajectory in KGE research: moving from simpler, single-operation models to more complex, multi-operation, multi-dimensional, and non-Euclidean spaces. This evolution is driven by the relentless quest to capture increasingly complex and nuanced relational patterns, thereby enhancing model expressiveness and theoretical rigor. While these models achieve state-of-the-art performance on various benchmarks, they often introduce increased mathematical complexity and computational costs, which can impact scalability, especially for extremely large knowledge graphs. Furthermore, the theoretical elegance of these geometric and algebraic models, while appealing, sometimes comes at the cost of interpretability, making it challenging to fully understand *why* certain transformations are optimal for specific relation patterns. The choice of the \"best\" geometric space or transformation remains highly dependent on the specific characteristics of the knowledge graph and the types of relations it contains.\n",
    "Deep Learning Architectures for Knowledge Graph Embedding": "\\label{sec:deep_learning_architectures_for_knowledge_graph_embedding}\n\n\\section{Deep Learning Architectures for Knowledge Graph Embedding}\n\\label{sec:deep_learning_architectures_for_knowledge_graph_embedding}\n\nBuilding upon the foundational geometric and algebraic paradigms discussed in Section \\ref{sec:foundational_kge_models_and_geometric_paradigms}, which established the initial framework for representing knowledge in continuous vector spaces, this section explores a significant paradigm shift in Knowledge Graph Embedding (KGE) research. While earlier geometric models, such as translational and rotational approaches, provided valuable insights into capturing specific relational patterns, their reliance on predefined transformations often limited their capacity to model the highly intricate, hierarchical, and non-linear relationships pervasive in real-world knowledge graphs \\cite{rossi2020, cao2022}. The advent of deep learning has revolutionized this landscape, enabling the development of more expressive and context-aware KGE models by allowing them to automatically extract features and learn complex interactions directly from data.\n\nThis section delves into how advanced deep learning architectures have been adapted to overcome these limitations, pushing the boundaries of KGE performance. We will detail the application of Convolutional Neural Networks (CNNs), which are leveraged to capture local features and intricate structural patterns within triplets and their neighborhoods. Subsequently, we examine Graph Neural Networks (GNNs), including various attention mechanisms, which inherently excel at encoding rich structural information and neighborhood context through message passing, thereby learning more robust, context-dependent embeddings. Finally, we explore the emergence of Transformer models in KGE, demonstrating how their powerful self-attention mechanisms capture long-range dependencies and contextualized representations, enabling the modeling of both global and local semantic relationships. This architectural evolution marks a crucial advancement, facilitating the learning of nuanced, non-linear relationships and paving the way for significantly enhanced performance across a spectrum of downstream AI tasks \\cite{dai2020, cao2022}.\n\n\\subsection{Convolutional Neural Networks (CNNs) for KGE}\n\\label{sec:3_1_convolutional_neural_networks_(cnns)_for_kge}\n\nConvolutional Neural Networks (CNNs) have emerged as a powerful paradigm in Knowledge Graph Embedding (KGE), offering a distinct advantage over traditional geometric models by automatically extracting local features and modeling intricate, non-linear interactions between entity and relation embeddings. Unlike models that rely on predefined geometric transformations, CNNs learn complex patterns directly from the data, enabling them to capture nuanced relational semantics that are often challenging for simpler approaches \\cite{cao2022}. This shift represents a significant evolution in KGE, moving towards more expressive and data-driven architectures.\n\nEarly applications of CNNs in KGE, such as AcrE \\cite{ren2020} and M-DCN \\cite{zhang2020}, demonstrated their capability to enhance link prediction. AcrE, for instance, introduced atrous convolutions and residual learning to effectively increase feature interactions while maintaining a simpler structure and higher parameter efficiency. This approach addressed the limitation of conventional models in capturing diverse relation patterns by allowing for a broader receptive field without increasing the number of parameters. M-DCN further advanced this by proposing a multi-scale dynamic convolutional network, utilizing dynamic filters to extract richer and more expressive feature embeddings. M-DCN was particularly designed to handle complex relation patterns like 1-to-N, N-to-1, and N-to-N, which often pose significant challenges for translation-based or simple semantic matching models \\cite{ge2023}. The dynamic nature of its filters, tailored to each relation, allowed for a more adaptive modeling of these complex interactions.\n\nThe integration of attention mechanisms further refined CNN-based KGE models. ReInceptionE \\cite{xie2020} exemplified this by combining an Inception network with a relation-aware attention mechanism. The Inception network was employed to increase interactions between head and relation embeddings, while the attention mechanism enriched these embeddings with joint local and global structural information. This allowed ReInceptionE to adaptively utilize neighborhood context, a capability that purely convolutional models might partially miss, thereby bridging the gap between local feature extraction and broader graph topology awareness. This approach highlights an evolutionary trend within the \"Deep Learning Architectures for KGE\" subgroup, where models increasingly seek to combine the strengths of different neural components to capture a more comprehensive view of the knowledge graph.\n\nMore recent works continue to refine CNN-based techniques for KGE. CNN-ECFA \\cite{hu2024} introduced a Convolutional Neural Network-based Entity-specific Common Feature Aggregation strategy, aiming to improve knowledge graph representation learning by leveraging common features that are specific to entities. This model demonstrates that by aggregating entity-specific features, CNNs can learn more effective representations, outperforming state-of-the-art feature projection strategies. Similarly, SEConv \\cite{yang2025} proposed a semantic-enhanced KGE model, incorporating a less resource-consuming self-attention mechanism alongside a multi-layer CNN. The multi-layer CNN in SEConv is specifically designed to learn deeper structural features from triplets, while self-attention generates more expressive embedding representations. This model, with its application focus on healthcare prediction, underscores the practical utility of CNNs in learning discriminative features for specialized domains.\n\nA key strength of CNN-based KGE models lies in their ability to automatically discover intricate, non-linear feature interactions, which contrasts with the hand-crafted transformations of geometric models (e.g., TransD \\cite{ji2015} or TorusE \\cite{ebisu2017}). This automatic feature learning often leads to superior performance in link prediction tasks, achieving state-of-the-art results on various benchmarks. However, this expressiveness comes with trade-offs. CNN models typically involve higher computational complexity and a larger number of parameters compared to simpler geometric models, potentially impacting scalability for extremely large knowledge graphs. Furthermore, while they excel at capturing local patterns, their ability to model long-range dependencies or global graph structures might be less direct than Graph Neural Networks (GNNs) or Transformer-based models, which are inherently designed for such tasks. The development trajectory of CNNs in KGE shows a clear progression from basic convolutional operations to more sophisticated designs incorporating multi-scale processing, dynamic filters, and attention, continually pushing the boundaries of what can be learned from entity-relation interactions.\n\\subsection{Graph Neural Networks (GNNs) and Attention Mechanisms}\n\\label{sec:3_2_graph_neural_networks_(gnns)__and__attention_mechanisms}\n\n\nThe integration of Graph Neural Networks (GNNs) and attention mechanisms represents a significant advancement in Knowledge Graph Embedding (KGE), moving beyond simple triplet-based interactions to leverage the rich topological and relational context of knowledge graphs. GNNs, through their inherent message passing and aggregation mechanisms, are uniquely suited to capture structural information and neighborhood context, which is crucial for understanding complex relational patterns and inferring missing links. This paradigm shift enables KGE models to learn richer, context-dependent embeddings by explicitly modeling multi-hop relational paths and local graph structures.\n\nEarly explorations into inductive capabilities for KGE, a key advantage of GNNs, were demonstrated by models like Logic Attention-based Neighborhood Aggregation (LAN) \\cite{wang2018}. LAN addressed the challenges of unordered and unequal neighbors by introducing a novel aggregator that uses both rule- and network-based attention weights. This allowed for the inductive embedding of new entities by aggregating information from their existing neighbors, a crucial step towards handling the dynamic nature of real-world knowledge graphs where new entities frequently emerge. However, while LAN provided a foundational approach to inductive learning, its attention mechanism was relatively simple and might not fully capture the nuanced importance of different relational paths.\n\nBuilding upon the strengths of GNNs, Graph Attenuated Attention Networks (GAATs) \\cite{wang2020} further refined the use of attention. GAATs incorporated an attenuated attention mechanism to assign varying weights to different relation paths within the knowledge graph, thereby acquiring more informative features from neighbor nodes. This approach recognized that not all paths or neighbors contribute equally to an entity's representation, and by attenuating less relevant information, GAATs could learn more discriminative embeddings. This marked an improvement over uniform aggregation strategies, allowing entities and relations to be learned within any neighborhood context, enriching the feature extraction process.\n\nA more sophisticated approach to GNN-based KGE is seen in DisenKGAT \\cite{wu2021}, which introduced a novel Disentangled Graph Attention Network. DisenKGAT leverages both micro-disentanglement and macro-disentanglement to learn diverse and independent component representations. Micro-disentanglement is achieved through a relation-aware aggregation mechanism that generates varied component representations, while macro-disentanglement uses mutual information as a regularization to enhance the independence of these components. This disentangled approach allows the model to generate adaptive representations based on the given scenario, thereby capturing more diverse and nuanced semantics behind complex relations. DisenKGAT's ability to produce adaptive and explainable representations showcases a significant strength, addressing the limitation of single, static representations in traditional KGE models.\n\nThe ongoing research in GNNs for KGE also includes efforts to optimize their design and understand their generalization capabilities. For instance, \\cite{di2023} proposed a Message Function Search for KGE, aiming to automatically discover suitable GNN message functions for various KG forms (e.g., n-ary, hyper-relational data). This highlights a meta-level approach to GNN design, seeking to overcome the limitations of fixed GNN architectures by adapting them to specific data characteristics. Similarly, \\cite{li2021} delved into understanding *how* KGE models, particularly GNN-based ones, extrapolate to unseen data, proposing \"Semantic Evidences\" and introducing SE-GNN to explicitly model and merge these evidences for improved inductive capabilities.\n\nWhile GNNs and attention mechanisms significantly enhance KGE by capturing complex structural and contextual information, they are not without limitations. A primary concern is their computational complexity and scalability, especially for very large knowledge graphs, as message passing can become resource-intensive. Furthermore, deep GNNs can suffer from over-smoothing, where entity representations become indistinguishable after many layers of aggregation, diminishing their discriminative power. The effectiveness of these models also heavily relies on the quality and density of local neighborhood information; sparse neighborhoods can limit their ability to learn rich contextual embeddings. Despite these challenges, the continuous development of more efficient GNN architectures, such as those explored in message function search \\cite{di2023}, and a deeper understanding of their generalization properties \\cite{li2021}, indicates a strong future for GNNs and attention mechanisms in KGE, pushing towards more robust and context-aware knowledge representation.\n\\subsection{Transformer-based KGE Models}\n\\label{sec:3_3_transformer-based_kge_models}\n\nThe emergence of Transformer architectures, initially lauded for their unparalleled success in natural language processing, has profoundly influenced the landscape of knowledge graph embedding (KGE). These models, characterized by their self-attention mechanisms, offer a powerful paradigm for capturing long-range dependencies and generating highly contextualized representations, capabilities that were often limited in earlier KGE approaches. The adaptation of Transformers to knowledge graphs represents a significant methodological evolution within the \"Deep Learning Architectures for KGE\" subgroup, pushing the boundaries of expressiveness beyond traditional geometric or simpler neural network models.\n\nEarly adaptations of Transformers to KGE primarily treated knowledge graphs as sequences of entities and relations. A pioneering example is CoKE (Contextualized Knowledge Graph Embedding) \\cite{wang2019}, which frames edges and paths within a KG as sequences. By feeding these sequences into a Transformer encoder, CoKE learns dynamic, flexible, and fully contextualized embeddings for entities and relations. This approach marked a critical shift from static, context-independent embeddings, allowing the model to capture varying properties of entities and relations based on their surrounding graph context. While CoKE demonstrated superior performance in link prediction and path query answering, its sequence-centric view inherently grappled with the graph's non-sequential nature, potentially overlooking intricate topological structures that are not easily linearized. This limitation highlights a theoretical gap: how to reconcile the order-invariance of self-attention with the directed, ordered nature of relational facts in KGs.\n\nSubsequent research has focused on explicitly integrating graph structures into Transformer frameworks, moving beyond simple sequence linearization. Knowformer \\cite{li2023} directly addresses the challenge of order invariance inherent in the self-attention mechanism, which struggles to distinguish between a valid triplet (subject, relation, object) and its shuffled variants. Knowformer innovatively incorporates relational compositions into entity representations, explicitly injecting semantics and capturing the role of an entity based on its position (subject or object) within a relation triplet. This design choice allows the Transformer to correctly capture relational semantics by distinguishing entity roles, a crucial advancement for modeling complex relational patterns that simpler translation-based models like TransD \\cite{ji2015} or even rotational models like RotatE \\cite{sun2018} might struggle to fully express without explicit positional encoding. Knowformer's ability to integrate positional awareness and relational semantics directly into the self-attention mechanism significantly enhances its expressiveness, particularly for modeling complex contextual information.\n\nThe latest advancements, such as TGformer \\cite{shi2025}, further refine the integration of Transformer architectures with graph structures, proposing a general graph Transformer framework for KGE. TGformer is notable for being the first to explicitly leverage a graph Transformer to build knowledge embeddings that incorporate both triplet-level and graph-level structural features. This comprehensive approach addresses a critical limitation of previous methods: triplet-based models often ignore the broader graph structure, while some graph-based methods (e.g., certain GNNs like DisenKGAT \\cite{wu2021}) might overlook the specific contextual information of individual nodes within a triplet. By constructing context-level subgraphs for each predicted triplet and employing a Knowledge Graph Transformer Network (KGTN), TGformer fully explores multi-structural features, boosting the model's understanding of entities and relations in diverse contexts. Furthermore, TGformer extends its capabilities to temporal knowledge graphs, a significant step towards handling the dynamic nature of real-world knowledge, aligning with the broader development direction of building \"more powerful and comprehensive models that leverage advanced neural architectures to capture increasingly complex structural and contextual information, including temporal dynamics.\"\n\nThe primary strength of Transformer-based KGE models lies in their ability to capture global and local semantic relationships through self-attention, leading to highly contextualized representations. This contrasts with CNN-based KGE models like AcrE \\cite{ren2020} or ReInceptionE \\cite{xie2020}, which excel at extracting local features and interactions but may require additional mechanisms to capture long-range dependencies effectively. While CNN-ECFA \\cite{hu2024} and SEConv \\cite{yang2025} demonstrate the continued refinement of CNNs for feature aggregation, Transformers offer a more inherent capability for global context. Compared to foundational models like TorusE \\cite{ebisu2017} or CyclE \\cite{yang2021} that focus on refining the embedding space's geometry, Transformers provide a data-driven approach to learn complex, non-linear transformations and interactions, often achieving state-of-the-art performance in link prediction.\n\nHowever, Transformer-based models also present trade-offs. Their computational complexity and high parameter count can pose scalability challenges for extremely large knowledge graphs, a practical constraint that needs careful consideration. While models like Knowformer address the initial order-invariance issue, the fundamental assumption of treating graph elements as sequences, even with sophisticated positional encodings, can sometimes be an oversimplification of the rich, multi-relational graph topology. The experimental setups for these models typically involve standard benchmark datasets, and while results are often superior, the generalizability to highly sparse or domain-specific KGs with limited data might still be a concern, requiring extensive pre-training or specialized fine-tuning. Despite these challenges, the innovative application of Transformers to graph structures, particularly in integrating multi-structural features and handling temporal dynamics, signifies a robust and adaptive direction for KGE research, continually pushing the state-of-the-art in capturing the intricate semantics of knowledge graphs.\n",
    "Enriching KGE: Auxiliary Information, Rules, and Multi-modality": "\\label{sec:enriching_kge:_auxiliary_information,_rules,__and__multi-modality}\n\n\\section{Enriching KGE: Auxiliary Information, Rules, and Multi-modality}\n\\label{sec:enriching_kge_auxiliary_information_rules_and_multi_modality}\n\nBuilding upon the advancements in deep learning architectures for Knowledge Graph Embedding (KGE) discussed in Section \\ref{sec:deep_learning_architectures_for_knowledge_graph_embedding}, which primarily focused on leveraging structural patterns within the graph, this section explores a crucial paradigm shift: enriching KGE models by integrating diverse external knowledge sources and logical constraints. While deep learning models like GNNs and Transformers have significantly enhanced the capture of intricate structural and contextual relationships, purely structural information often proves insufficient in addressing challenges such as data sparsity, ambiguity, and the need for more robust reasoning capabilities in complex, real-world scenarios \\cite{general_kge_review_1}.\n\nThis section delves into advanced KGE approaches that move beyond the confines of graph topology alone, aiming to provide a more comprehensive and nuanced representation of knowledge. We will first examine how auxiliary information, such as entity types and attributes, can be seamlessly incorporated to provide semantic guidance, thereby improving embedding quality and robustness, particularly for incomplete or noisy knowledge graphs. Subsequently, the discussion will shift to the integration of explicit logical rules and constraints, which inject prior knowledge into the embedding process, enhancing reasoning capabilities and model interpretability by ensuring learned representations adhere to logical patterns. Finally, we explore the burgeoning field of multi-modal KGE, detailing how information from diverse modalities, including textual descriptions and visual features, can be leveraged to overcome data sparsity and enrich semantic understanding, enabling more holistic knowledge representation \\cite{general_kge_review_2}. By exploring these complementary dimensions, this section highlights how KGE models can achieve superior performance, interpretability, and applicability in complex AI tasks.\n\n\\subsection{Incorporating Auxiliary Information (Types, Attributes)}\n\\label{sec:4_1_incorporating_auxiliary_information_(types,_attributes)}\n\nThe effectiveness of Knowledge Graph Embedding (KGE) models, while primarily driven by structural information, can be significantly enhanced by integrating auxiliary semantic information such as entity types and attributes. This approach moves beyond the simplistic triplet structure, grounding embeddings in a richer context to yield more semantic, discriminative, and robust representations, particularly crucial when dealing with incomplete or noisy knowledge graphs (KGs). The intellectual trajectory in this area reflects a growing recognition that external, well-structured knowledge can bridge gaps that purely structural models cannot, contributing to the development of more inherently capable KGE models.\n\nEarly efforts to incorporate auxiliary information often focused on entity types. \\cite{wang2021} proposed \\textit{TransET}, a novel KGE model that leverages entity types to learn more semantic features. By utilizing circle convolution based on entity and entity type embeddings, TransET maps head and tail entities to type-specific representations, which are then used in a translation-based scoring function. This method demonstrated that even a relatively straightforward integration of type information could lead to improved performance in link prediction and triple classification tasks. Building on this, \\cite{he2023} introduced \\textit{TaKE}, a more universal \\textit{Type-augmented Knowledge graph Embedding framework}. TaKE distinguishes itself by automatically capturing type features without explicit supervision and learning relation-specific type representations, allowing for a nuanced understanding of how entity types interact with different relations. Furthermore, TaKE incorporates a type-constrained negative sampling strategy, which is critical for constructing more effective negative samples during training, a fundamental aspect for KGE robustness \\cite{sachan2020}. While TransET provided a specific model, TaKE offers a generalizable framework that can enhance various traditional KGE models, showcasing a methodological evolution towards broader applicability.\n\nThe utility of type information extends to domain-specific applications. \\cite{hu2024} presented \\textit{SR-KGE}, a \\textit{GeoEntity-type constrained knowledge graph embedding} framework designed for predicting natural-language spatial relations. This approach integrates geoentity types as a constraint, combining graph structures with semantic attributes to capture spatial and semantic relations more accurately. While TaKE provides a universal type integration, SR-KGE exemplifies how tailored auxiliary information, when applied to a specific domain, can yield superior results for specialized tasks. The strength of these type-augmented methods lies in their ability to provide semantic guidance, making embeddings more discriminative by enforcing type consistency and enriching the relational context. However, a common limitation is their reliance on the availability and quality of type information; if types are sparse, noisy, or inconsistently defined, the benefits may diminish, and the complexity of integrating diverse type hierarchies can be substantial.\n\nBeyond explicit types, entity attributes offer a richer, instance-specific form of auxiliary information. \\cite{zhang2024} addressed the critical problem of erroneous triples in KGs by proposing \\textit{AEKE}, a framework for \\textit{Attributed Error-aware Knowledge Embedding}. AEKE leverages entity attributes to guide the KGE model in learning against the impact of erroneous triples. It designs triple-level hypergraphs to model both KG topological structures and attribute structures, jointly calculating confidence scores for each triple based on self-contradiction, structural consistency, and attribute homogeneity. These confidence scores then adaptively weigh contributions during multi-view graph learning and margin loss calculation, ensuring that potentially erroneous triples have minimal impact. AEKE represents a significant step towards enhancing KGE robustness, moving beyond merely completing KGs to making them more reliable. While type-based methods provide broad semantic categories, attribute-based approaches like AEKE offer fine-grained details that can be crucial for identifying and mitigating data quality issues.\n\nThe scope of auxiliary information also extends to hyper-relational facts, moving \"beyond triplets\" to capture richer contextual data. \\cite{rosso2020} introduced \\textit{HINGE}, a \\textit{hyper-relational Knowledge Graph Embedding model} that directly learns from hyper-relational facts, where each fact includes a base triplet (\\textit{h, r, t}) and associated key-value pairs (\\textit{k, v}). HINGE captures not only the primary structural information of the KG but also the correlation between each triplet and its associated key-value pairs. This is a crucial distinction from type or attribute integration, as HINGE directly models additional structured facts that are part of the knowledge base, rather than meta-information about entities. This allows for a more comprehensive understanding of complex data semantics, outperforming models that rely solely on triplets or transform hyper-relational facts into less structured n-ary representations.\n\nIn synthesis, the integration of auxiliary information, whether through entity types \\cite{wang2021, he2023, hu2024}, attributes \\cite{zhang2024}, or hyper-relational facts \\cite{rosso2020}, represents a vital direction in KGE research. These approaches collectively address the limitations of purely structural KGEs by providing richer semantic context, making embeddings more discriminative and robust to noise and incompleteness. The evolution from specific type integration (TransET) to universal frameworks (TaKE) and domain-specific applications (SR-KGE) highlights a growing sophistication. Furthermore, the focus on error-aware learning through attributes (AEKE) and the direct modeling of hyper-relational facts (HINGE) underscore the field's commitment to developing KGE models that can handle the complexities and imperfections of real-world knowledge graphs. A key trade-off, however, is the increased reliance on the availability and quality of this auxiliary data, which may not always be consistent across diverse KGs. Nevertheless, these advancements are crucial for pushing KGE towards greater practical utility and theoretical soundness, enabling more intelligent and reliable AI applications.\n\\subsection{Rule-based and Constraint-driven KGE}\n\\label{sec:4_2_rule-based__and__constraint-driven_kge}\n\nWhile purely data-driven knowledge graph embedding (KGE) models excel at capturing statistical patterns from observed triples, they often struggle with ensuring logical consistency, facilitating explicit reasoning, and providing interpretability aligned with human understanding. This subsection delves into approaches that integrate logical rules and explicit constraints directly into the KGE learning process, thereby injecting prior knowledge to address these limitations.\n\nOne foundational approach to injecting semantic consistency is exemplified by \"Semantically Smooth Knowledge Graph Embedding\" (SSE) \\cite{guo2015}. This method enforces a \"semantically smooth\" embedding space, where entities belonging to the same semantic category are encouraged to lie close to each other. By employing manifold learning techniques, such as Laplacian Eigenmaps and Locally Linear Embedding, as regularization terms, SSE guides the embedding process to discover intrinsic geometric structures that reflect categorical semantics. While effective in promoting semantic coherence, SSE primarily relies on entity categories as a form of soft constraint, which is less explicit than logical rules and might not directly enhance complex reasoning capabilities. Its strength lies in its generality, as the smoothness assumption can be applied to various embedding models and constructed from diverse information beyond just entity categories.\n\nA significant advancement in this domain is the explicit incorporation of logical rules. Early rule-based methods often relied on hard rules, which are rigid and require extensive manual curation. However, real-world knowledge graphs are often noisy and incomplete, making hard rules brittle. \"Knowledge Graph Embedding with Iterative Guidance from Soft Rules\" (RUGE) \\cite{guo2017} introduced a novel paradigm to iteratively integrate soft rules (rules associated with confidence levels) into the embedding learning process. RUGE simultaneously learns from observed triples, unlabeled triples (whose labels are iteratively predicted), and automatically extracted soft rules. This iterative guidance allows the knowledge embodied in rules to be progressively transferred into the learned embeddings, leading to more robust representations. The key advantage of RUGE is its ability to leverage abundant, albeit uncertain, automatically extracted rules, moving beyond the limitations of manually curated hard rules. This iterative feedback loop between rule inference and embedding updates represents a crucial step towards deeply intertwining symbolic logic with subsymbolic representations.\n\nComplementing complex rule integration, simpler structural constraints can also significantly enhance KGE models. \"Improving Knowledge Graph Embedding Using Simple Constraints\" \\cite{ding2018} demonstrated that even straightforward constraints can yield substantial improvements in interpretability and structure without adding significant computational overhead. Specifically, this work explored non-negativity constraints on entity representations, which help learn compact and interpretable features, and approximate entailment constraints on relation representations. These entailment constraints encode regularities of logical entailment between relations, structuring the embedding space to reflect hierarchical or inferential relationships. While these constraints are less expressive than full first-order logic rules, their simplicity makes them highly efficient and broadly applicable, offering a practical trade-off between model complexity and the benefits of injected prior knowledge.\n\nMore recent contributions have further refined the integration of soft rules. \"Knowledge Graph Embedding Preserving Soft Logical Regularity\" \\cite{guo2020} focused on imposing soft rule constraints directly on relation representations. By representing relations as bilinear forms and mapping entity representations into a non-negative and bounded space, the method derives a rule-based regularization that enforces relation representations to satisfy rule-introduced constraints. A notable strength of this approach is its improved scalability, as the complexity of rule learning becomes independent of the entity set size, making it more feasible for large-scale KGs. This direct regularization of relations ensures that logical patterns are preserved in the relational space, which is crucial for consistent reasoning.\n\nBuilding on these foundations, RulE (\"Rule Embedding\") \\cite{tang2022} presents a principled framework that learns rule embeddings jointly with entity and relation embeddings within a unified vector space. Unlike previous methods that might treat rules as external regularization, RulE explicitly represents rules as vectors, allowing for soft logical inference directly within the embedding space. This deep integration enables rule embeddings to regularize and enrich entity/relation embeddings, leading to more coherent and reasoning-capable representations. RulE's ability to calculate a confidence score for each rule based on its consistency with observed triples further refines the \"softness\" of logical inference, alleviating the brittleness often associated with strict logic. This joint learning paradigm represents a sophisticated approach to intertwining symbolic knowledge with subsymbolic representations, pushing the boundaries of reasoning capabilities within KGEs.\n\nCollectively, these rule-based and constraint-driven methods highlight a critical evolution in KGE research. They move beyond purely data-driven models (like those in the \"Core KGE Model Architectures\" subgroup) by leveraging explicit logical knowledge to enhance robustness, improve reasoning, and increase interpretability. The progression from general semantic smoothness \\cite{guo2015} to iterative soft rule guidance \\cite{guo2017}, the application of simple yet effective structural constraints \\cite{ding2018}, and finally to the joint embedding of rules themselves \\cite{tang2022} demonstrates a continuous effort to bridge the gap between symbolic logic and continuous vector spaces.\n\nHowever, several challenges persist. A primary limitation across many rule-based approaches is the reliance on the availability and quality of logical rules. While methods like RUGE can leverage automatically extracted soft rules, the efficiency and accuracy of such rule extraction remain a practical hurdle. Furthermore, balancing the strict adherence to logical rules with the flexibility to capture exceptions or nuanced patterns not covered by rules is a delicate trade-off. Over-constraining the embedding space with rules might inadvertently reduce its expressiveness or ability to generalize to unseen, complex scenarios. The scalability of managing and applying very large and complex rule sets, particularly for higher-order logic, also poses a significant challenge, despite efforts like \\cite{guo2020} to optimize rule learning complexity. Future research needs to explore more robust and automated rule discovery mechanisms, develop adaptive frameworks that can dynamically weigh rule adherence against data-driven insights, and investigate more principled theoretical foundations for combining probabilistic logic with continuous embeddings.\n\\subsection{Multi-modal and Cross-domain KGE}\n\\label{sec:4_3_multi-modal__and__cross-domain_kge}\n\nThe limitations of relying solely on structural information in knowledge graphs, particularly data sparsity and the inability to capture rich semantic nuances, have driven significant research into multi-modal and cross-domain Knowledge Graph Embedding (KGE). These approaches integrate diverse information sources, such as textual descriptions, visual features, or data from multiple domains, to enrich representations and enable more comprehensive knowledge understanding. The core motivation is to leverage complementary information, thereby enhancing the expressiveness and robustness of KGE models.\n\nEarly efforts in multi-modal KGE primarily focused on integrating textual descriptions to augment structural embeddings. For instance, the Semantic Space Projection (SSP) model \\cite{xiao2016} proposed a method that jointly learns from symbolic triples and textual descriptions. SSP projects textual information into a semantic space, using text to discover semantic relevance and provide more precise embeddings. This was a crucial step in addressing the \"weak-semantic\" nature of purely geometric models, which often struggle to differentiate entities with similar structural positions but distinct semantic meanings. While SSP offered a foundational approach, its text integration was relatively simple, often relying on bag-of-words or basic word embeddings.\n\nMore recent advancements have leveraged the power of pre-trained language models (PLMs) for deeper semantic understanding. The \"Joint Language Semantic and Structure Embedding for Knowledge Graph Completion\" model \\cite{shen2022} exemplifies this by fine-tuning PLMs with a probabilistic structured loss. This method effectively captures semantics from natural language descriptions while simultaneously reconstructing structural information, demonstrating state-of-the-art performance, particularly in low-resource settings where semantic cues are invaluable. This approach highlights a significant evolutionary trend, moving from simple text projection to sophisticated joint learning frameworks that deeply intertwine language semantics with graph structure. A key strength of such models is their ability to infer relations and entity properties even when structural data is sparse, a common challenge in many real-world KGs. However, the computational cost of fine-tuning large PLMs and the availability of high-quality textual descriptions for all entities remain practical constraints.\n\nBeyond textual data, multi-modal KGE has expanded to incorporate other modalities and domain-specific knowledge. For instance, in the biomedical domain, \"Multimodal reasoning based on knowledge graph embedding for specific diseases\" \\cite{zhu2022} constructs Specific Disease Knowledge Graphs (SDKGs) and implements multimodal reasoning using reverse-hyperplane projection. This model integrates structural, category, and description embeddings to discover new, reliable knowledge, showcasing how combining different modalities can lead to enhanced insights in specialized contexts. This demonstrates the power of multimodal integration in addressing the unique complexities and data characteristics of domain-specific KGs. However, the generalizability of such highly specialized models to other domains without significant re-engineering remains an open question. Furthermore, the challenge of designing effective negative sampling strategies becomes more pronounced in multi-modal settings, as highlighted by \\cite{zhang2023}, which proposes Modality-Aware Negative Sampling (MANS) to align structural and visual embeddings, underscoring that training optimization must adapt to the complexity of heterogeneous data.\n\nCross-domain KGE extends this concept by enabling knowledge transfer and interaction across distinct knowledge graphs or domains. This is particularly vital for applications like recommender systems, where user preferences and item characteristics often span multiple categories or platforms. \"Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation\" \\cite{liu2023} addresses the critical problems of cross-domain cold start and multi-domain recommendations. This approach proposes a \"binding rule\" to efficiently interact items across multiple domains, allowing for both homo-domain and hetero-domain item embeddings. By modeling associations and interactions between items across diverse domains, this method significantly improves multi-domain item-item recommendations, outperforming traditional recommender systems that struggle with data sparsity in new domains. The strength lies in its ability to leverage shared entities or relations to bridge information gaps, enriching representations for items even in domains with limited data. A limitation, however, is the reliance on explicit links or shared entities between domains, which may not always be readily available or accurately reflect complex cross-domain relationships.\n\nIn summary, multi-modal and cross-domain KGE represent a crucial evolutionary trajectory in knowledge representation. They move beyond the limitations of purely structural models by integrating diverse, complementary information sources. While models like SSP \\cite{xiao2016} laid the groundwork for textual integration, the field has progressed to sophisticated joint learning frameworks leveraging pre-trained language models \\cite{shen2022} and domain-specific multimodal reasoning \\cite{zhu2022}. Concurrently, cross-domain approaches \\cite{liu2023} tackle challenges like data sparsity and cold start in complex applications such as recommendation. The collective contribution of these methods is the creation of richer, more semantically grounded, and robust embeddings, which are essential for comprehensive knowledge understanding and practical applicability in diverse AI tasks. However, challenges persist in effectively fusing heterogeneous information, managing increased computational complexity, and designing robust training strategies like modality-aware negative sampling \\cite{zhang2023}.\n",
    "Dynamic, Inductive, and Distributed KGE": "\\section{Dynamic, Inductive, and Distributed KGE}\n\\label{sec:dynamic,_inductive,__and__distributed_kge}\n\n\\label{sec:dynamic_inductive_and_distributed_kge}\n\\section{Dynamic, Inductive, and Distributed KGE}\n\nBuilding upon the enriched KGE models discussed in Section \\ref{sec:enriching_kge_auxiliary_information_rules_and_multi_modality}, which focused on leveraging diverse auxiliary information and logical constraints to deepen semantic understanding, this section shifts attention to the critical operational challenges of Knowledge Graph Embeddings in real-world environments. Traditional KGE models often assume static, complete, and centrally managed knowledge graphs, a paradigm increasingly at odds with the dynamic, evolving, and distributed nature of modern knowledge bases. This section addresses these fundamental limitations by exploring advanced KGE methodologies designed for adaptability, scalability, and security in complex, real-world operational settings, moving beyond static and centralized assumptions to meet the demands of modern knowledge management systems.\n\nWe delve into three interconnected areas. First, \\textit{Temporal Knowledge Graph Embedding (TKGE)} tackles the inherent dynamism of real-world facts, where entities and relations change over time. These methods move beyond static representations to capture the fluidity and evolution of knowledge, crucial for tasks requiring reasoning over time. Second, \\textit{Inductive and Continual KGE} addresses the challenge of unseen entities and the need for continuous model updates. This area explores how KGE models can efficiently learn embeddings for new entities without full retraining and adapt to a constant influx of new facts, mitigating catastrophic forgetting. Finally, \\textit{Federated and Privacy-Preserving KGE} investigates collaborative learning paradigms for distributed knowledge graphs. This crucial direction enables multiple parties to jointly train robust KGE models while safeguarding sensitive data, addressing the growing demand for privacy-aware AI systems. Together, these advancements are pivotal for transitioning KGE from theoretical constructs to robust, secure, and continuously operational components within complex, real-world knowledge management systems.\n\n\\subsection{Temporal Knowledge Graph Embedding (TKGE)}\n\\label{sec:5_1_temporal_knowledge_graph_embedding_(tkge)}\n\nThe inherent dynamism of real-world knowledge necessitates models capable of capturing the temporal evolution of facts within knowledge graphs (KGs). Traditional Knowledge Graph Embedding (KGE) models, primarily designed for static KGs, fall short in tasks requiring reasoning over time or understanding the fluidity of information. Temporal Knowledge Graph Embedding (TKGE) addresses this by explicitly integrating time into the embedding process, moving beyond static representations to model evolving entities and relations \\cite{dai2020}.\n\nEarly approaches to TKGE focused on explicitly structuring and modeling time itself. \\cite{dasgupta2018} introduced \\textit{HyTE}, a hyperplane-based method that associates each timestamp with a corresponding hyperplane in the entity-relation space. This allows for temporally guided KG inference and prediction of temporal scopes for facts, marking a significant step towards dynamic KGEs. While intuitive, HyTE's reliance on simple hyperplanes might struggle with highly complex, non-linear temporal dependencies. Another prominent method involves treating the entire fact set as a higher-order tensor, typically a fourth-order tensor (head, relation, tail, time), and applying tensor decomposition to learn dense, low-dimensional temporal embeddings \\cite{lin2020}. This provides a robust mathematical framework for integrating time as a distinct dimension. Complementing this, \\textit{ATiSE} models temporal evolution using additive time series decomposition, mapping representations into multi-dimensional Gaussian distributions where covariance captures temporal uncertainty, offering a probabilistic view of temporal dynamics \\cite{xu2019}. More recently, \\textit{TeAST} innovatively structures time by mapping relations onto an Archimedean spiral timeline, transforming the quadruple completion problem into a 3rd-order tensor completion task. This approach aims to ensure relations evolve orderly over time with a spiral regularizer, offering a degree of interpretability regarding temporal patterns \\cite{li2023}. A common limitation across these tensor and time series methods is the computational cost associated with higher-order operations or complex decompositions, especially for very dense temporal data.\n\nA significant advancement in TKGE involves leveraging geometric transformations to model temporal dynamics. \\textit{TeRo} defines the temporal evolution of entity embeddings as a rotation from an initial time to the current time in a complex vector space, representing relations for time intervals with dual complex embeddings \\cite{xu2020}. Building upon this, \\textit{ChronoR} extends the concept by employing k-dimensional rotation transformations, parametrized by relation and time, to transform a head entity to fall near its tail entity. This effectively captures rich interactions between temporal and multi-relational characteristics \\cite{sadeghian2021}. While powerful, the interpretability of complex rotations in high-dimensional spaces can be challenging, and the computational complexity associated with learning these transformations can be substantial, particularly for very large KGs or highly granular temporal data.\n\nMore advanced methods, particularly emerging in recent years, address the complexities of dynamic, spatiotemporal, and even fuzzy knowledge by moving beyond a single Euclidean space. \\textit{MADE} and \\textit{IME}, both published in 2024, represent a cutting-edge shift towards modeling TKGs in multi-curvature spaces, including Euclidean, hyperbolic, and hyperspherical geometries \\cite{wang2024, wang2024a}. The rationale is that TKGs often contain interwoven complex geometric structures (e.g., hierarchical, ring, chain) that no single curvature space can optimally capture. MADE introduces an adaptive weighting mechanism to assign different weights to these spaces in a data-driven manner, along with a quadruplet distributor and temporal regularization for timestamp smoothness \\cite{wang2024}. IME, on the other hand, incorporates \"space-shared\" properties to learn commonalities across spaces and alleviate spatial gaps, and \"space-specific\" properties to capture characteristic features, complemented by an Adjustable Multi-curvature Pooling (AMP) approach \\cite{wang2024a}. While both achieve state-of-the-art results, MADE's adaptive weighting offers a more flexible approach to handling diverse geometric structures without requiring explicit design of shared/specific properties. The primary limitation for both is the increased complexity of optimizing embeddings across multiple, potentially disparate, geometric spaces, and the computational overhead.\n\nFurther extending these geometric transformations, recent works tackle fuzzy and spatiotemporal dimensions. \\textit{FSTRE} uses projection and rotation in a complex vector space to embed spatial and temporal information, introducing fine-grained fuzziness through modal lengths of anisotropic vectors \\cite{ji2024}. This addresses the insufficiency of prior KGE models for uncertain and dynamic knowledge. Building on this, \\cite{ji2024a} leverages quaternion embeddings to jointly embed spatiotemporal entities, representing relations as rotations and exploiting the non-commutative compositional pattern of quaternions for multihop path reasoning and uncertainty modeling. This approach is particularly powerful for complex tasks like multihop querying on incomplete fuzzy spatiotemporal KGs, where previous methods overlooked uncertainty and spatiotemporal sensitivity during reasoning. These advanced models, while powerful, introduce additional complexity (fuzziness, spatiotemporal, quaternions) which can increase model intricacy and training demands.\n\nFinally, \\textit{TARGAT} offers an alternative paradigm by employing a time-aware relational graph attention model based on Graph Neural Networks (GNNs) \\cite{xie2023}. It addresses the limitation of previous GNN-based models that struggle to directly capture multi-fact interactions at different timestamps by dynamically generating time-aware relational message transformation matrices. This GNN-based approach provides a unified way to process the entire graph of multi-facts over time. However, GNNs can face scalability challenges with extremely large and dense TKGs due to the computational intensity of message passing.\n\nIn summary, TKGE research has evolved from explicit temporal integration using hyperplanes or tensors to sophisticated geometric transformations (rotations, multi-curvature spaces) and advanced algebraic structures (quaternions) to handle the multifaceted nature of dynamic, spatiotemporal, and fuzzy knowledge. The trade-off between model expressiveness and computational complexity remains a persistent challenge, with recent models pushing the boundaries of what can be captured, often at the cost of increased model intricacy and optimization demands.\n\\subsection{Inductive and Continual KGE}\n\\label{sec:5_2_inductive__and__continual_kge}\n\nReal-world knowledge graphs (KGs) are inherently dynamic, with new entities, relations, and facts constantly emerging. Traditional Knowledge Graph Embedding (KGE) models are often transductive, meaning they can only generate embeddings for entities seen during training, necessitating expensive full retraining when new information arrives. This limitation has spurred significant research into inductive and continual KGE, aiming to adapt models to evolving KGs by handling unseen entities and efficiently updating knowledge without catastrophic forgetting \\cite{liu2024, sun2024}. These methods are crucial for maintaining the scalability and relevance of KGE models in dynamic environments.\n\nEarly efforts in inductive KGE focused on neighborhood aggregation techniques. \\cite{wang2018} introduced the Logic Attention Network (LAN), an aggregator that learns to embed new entities by combining the embeddings of their existing neighbors. LAN addresses the unordered and unequal nature of an entity's neighbors by employing both rules- and network-based attention weights. While innovative for its time, aggregation-based methods like LAN inherently rely on the presence of existing neighbors for new entities. This poses a limitation when truly novel, isolated entities with sparse connections emerge, as their representations might be less robust or even impossible to generate. The generalizability of such methods is also constrained by the quality and density of the local neighborhood information.\n\nTo overcome the limitations of direct entity embedding and enhance transferability, meta-learning has emerged as a powerful paradigm for inductive KGE. \\cite{chen2021} proposed MorsE, a model that does not learn explicit entity embeddings but instead learns transferable meta-knowledge. This meta-knowledge, modeled by entity-independent modules and learned through meta-learning, can then be used to produce embeddings for new entities in an inductive setting. This approach offers a more generalized inductive capability compared to simple aggregation, as it aims to capture underlying patterns that are independent of specific entities. Building on this, \\cite{sun2024} applied meta-learning to dynamic KGE in evolving service ecosystems with MetaHG. This model incorporates both local (via a GNN layer) and potential global (via a hypergraph neural network, HGNN, layer) structural information from current KG snapshots to enhance the representation of emerging entities. MetaHG's hybrid GNN framework and meta-learning strategy aim to mitigate issues like spatial deformation and improve the quality of embeddings for new entities. A critical comparison reveals that while meta-learning offers a more robust inductive framework, its complexity in training and the need for sufficient meta-training tasks can be significant. Furthermore, the effectiveness of meta-knowledge transfer can still be influenced by the similarity between the meta-training and target domains.\n\nBeyond inductive learning, continual KGE addresses the challenge of efficiently acquiring new knowledge while simultaneously preserving previously learned information, a problem often plagued by catastrophic forgetting. \\cite{liu2024} introduced FastKGE, a framework incorporating an incremental low-rank adapter (IncLoRA) mechanism. FastKGE tackles both efficient new knowledge acquisition and catastrophic forgetting by isolating and allocating new knowledge to specific layers based on the fine-grained influence between old and new KGs. The IncLoRA mechanism then embeds these specific layers into low-rank adapters, significantly reducing the number of trainable parameters during fine-tuning. This approach also features adaptive rank allocation, making the LoRA aware of entity importance. Experimental results demonstrate that FastKGE can reduce training time by 34-49\\% on public datasets while maintaining competitive performance, and even greater savings (51-68\\%) on larger, newly constructed datasets, alongside performance improvements. This parameter-efficient adaptation is a crucial advancement for large-scale KGE models, balancing the acquisition of new knowledge with the retention of old, a key trade-off in continual learning. However, the efficacy of LoRA-based methods can depend on the intrinsic rank of the updates and the architecture of the base KGE model.\n\nThe overarching goal across these inductive and continual KGE methods is to balance the acquisition of new knowledge with the retention of previously learned information, mitigating catastrophic forgetting and ensuring scalability. While neighborhood aggregation provides a straightforward, albeit limited, inductive capability, meta-learning offers a more generalized approach by learning transferable knowledge. Parameter-efficient adaptation techniques like IncLoRA represent a practical solution for continual learning, particularly for large models, by enabling efficient updates without full retraining. A common theoretical gap preventing a complete solution to these problems lies in developing truly universal inductive mechanisms that are robust to completely novel, isolated entities and can perform continuous, long-term updates without any degradation in performance or significant increase in computational cost. The experimental setups for these models often require specialized dynamic datasets, which can be less standardized than static link prediction benchmarks, making direct comparisons challenging and potentially affecting generalizability. The field continues to seek methods that can seamlessly integrate new information while preserving the integrity and expressiveness of the entire knowledge graph, a critical step towards truly adaptive and intelligent AI systems.\n\\subsection{Federated and Privacy-Preserving KGE}\n\\label{sec:5_3_federated__and__privacy-preserving_kge}\n\nThe increasing concerns over data privacy and the proliferation of distributed knowledge sources have propelled Federated Learning (FL) as a crucial paradigm for Knowledge Graph Embedding (KGE). Federated KGE (FKGE) enables collaborative model training across multiple clients, each holding a local knowledge graph (KG), without centralizing sensitive data. This approach is vital for leveraging decentralized knowledge in privacy-sensitive domains, addressing the growing need for privacy-aware AI systems. However, FKGE introduces unique challenges, primarily related to communication efficiency, personalization for diverse client data, and security vulnerabilities.\n\nA significant challenge in FKGE is the high communication cost stemming from the large size of KGE parameters and the extensive number of communication rounds required for convergence. Traditional FL methods often focus on reducing communication rounds by increasing local training epochs, but they frequently overlook the size of parameters transmitted in each round. To address this, \\cite{zhang2024} (Communication-Efficient FKGE) proposes FedS, a bidirectional communication-efficient framework based on Entity-Wise Top-K Sparsification. This method allows clients to dynamically identify and upload only the Top-K entity embeddings with the most significant changes to the server. Similarly, the server transmits only the Top-K aggregated embeddings to each client after performing personalized aggregation. This approach, coupled with an Intermittent Synchronization Mechanism, aims to mitigate the negative effects of embedding inconsistency caused by client heterogeneity. While FedS significantly enhances communication efficiency, a critical analysis reveals a potential trade-off: universal reduction in embedding precision, as noted by the authors, can impede convergence speed. The challenge lies in precisely identifying the \"most significant\" changes without losing crucial information, especially for less frequently updated entities or relations. This aligns with the broader \"Efficiency, Compression, and System Optimization\" subgroup's goal of reducing resource consumption while maintaining performance, but within the added constraint of distributed, privacy-preserving learning.\n\nBeyond communication efficiency, the semantic disparities among clients pose a substantial hurdle for FKGE. Existing FKGE methods often rely on a global consensus model, typically using the arithmetic mean of entity embeddings as global supplementary knowledge \\cite{zhang2024}. This \"one-size-fits-all\" approach, however, neglects the inherent semantic heterogeneity across diverse client KGs, leading to a global model that might be inundated with noise when tailored to a specific client. To overcome this, \\cite{zhang2024} (Personalized Federated KGE) introduces PFedEG, a novel approach that employs a client-wise relation graph to learn personalized embeddings. PFedEG discerns the semantic relevance of embeddings from other clients, allowing each client to learn personalized supplementary knowledge by amalgamating entity embeddings from its \"neighboring\" clients based on their affinity on this graph. This personalized approach addresses the \"Personalized Federated KGE\" challenge, moving beyond a universal global model to improve embedding quality for individual clients. The strength of PFedEG lies in its ability to adapt to diverse data distributions, a critical aspect highlighted in the \"Dynamic, Inductive, and Continual KGE\" subgroup, which emphasizes adaptability to evolving and heterogeneous knowledge. However, the construction and maintenance of such a client-wise relation graph introduce additional computational complexity and potential privacy leakage risks if the \"affinity\" metrics are not carefully designed.\n\nWhile FL is designed to preserve data privacy, it is not inherently immune to security vulnerabilities. \\cite{zhou2024} (Poisoning Attack on Federated KGE) systematically explores the risks of poisoning attacks in FKGE, highlighting a critical security challenge. This pioneering work develops a novel framework that forces victim clients to predict specific false facts, demonstrating that privacy-preserving distributed training does not automatically equate to security. Unlike centralized KGEs, where attackers might directly inject poisoned data, FKGE's local data maintenance necessitates indirect attack vectors. The proposed attack framework involves inferring targeted relations in the victim's local KG via a \"KG component inference attack\" and then using an optimized dynamic poisoning scheme to generate progressive poisoned updates through FKGE aggregation. The experimental results demonstrate remarkable success rates (e.g., 100\\% on TransE with WN18RR) with minimal impact on the original task's performance, exposing a significant vulnerability. This research, while adversarial, is crucial for informing the design of robust FKGE systems, aligning with the \"Robustness and Training Optimization\" subgroup's focus on mitigating data imperfections and ensuring model integrity. The theoretical gap here is the lack of a direct defense mechanism proposed by the authors, which remains an open challenge for future research.\n\nIn synthesis, the emerging field of Federated and Privacy-Preserving KGE is rapidly addressing the practical demands of distributed and privacy-sensitive environments. The works by \\cite{zhang2024} (Communication-Efficient FKGE) and \\cite{zhang2024} (Personalized Federated KGE) represent constructive efforts to optimize FKGE by tackling communication bottlenecks and semantic heterogeneity, respectively. These solutions are critical for making FKGE scalable and effective in real-world deployments. However, the findings of \\cite{zhou2024} (Poisoning Attack on Federated KGE) serve as a stark reminder that privacy and security are distinct concerns, and the distributed nature of FL introduces new attack surfaces. The trade-offs are evident: aggressive communication sparsification might impact model convergence, personalization adds complexity and potential for noise, and robust security measures could introduce computational overhead. The rapid publication of these papers in 2024 underscores the contemporary and pressing nature of these challenges, reflecting the field's accelerated evolution towards practical, secure, and adaptable KGE solutions for decentralized knowledge.\n",
    "Practical Considerations: Efficiency, Robustness, and Evaluation": "\\label{sec:practical_considerations:_efficiency,_robustness,__and__evaluation}\n\n\\section{Practical Considerations: Efficiency, Robustness, and Evaluation}\n\\label{sec:practical_considerations_efficiency_robustness_and_evaluation}\n\nBuilding upon the advancements in dynamic, inductive, and distributed knowledge graph embedding (KGE) models discussed in Section \\ref{sec:dynamic_inductive_and_distributed_kge}, which focused on adapting KGEs to evolving and decentralized knowledge, this section shifts its focus to the critical operational challenges of deploying and evaluating these models in real-world scenarios. While previous sections explored theoretical foundations and architectural innovations, the true utility of KGEs hinges on their practical viability, encompassing computational efficiency, resilience to imperfect data, and the trustworthiness of their empirical validation. This section bridges the gap between theoretical progress and reliable real-world application, addressing the fundamental requirements for KGE models to move from research prototypes to robust, scalable, and trustworthy components of intelligent systems.\n\nWe delve into three interconnected areas vital for practical KGE deployment. First, we examine strategies for enhancing \\textit{efficiency, compression, and scalability}, which are paramount for handling the immense size and complexity of modern knowledge graphs \\cite{community_1, community_6}. This involves techniques to reduce computational cost, minimize memory footprint, and optimize training and inference processes, making KGE models viable for resource-constrained environments and large-scale applications. Second, we explore methods for improving \\textit{robustness and training optimization}, crucial for mitigating the impact of noisy, incomplete, or imbalanced data inherent in real-world KGs \\cite{community_2, community_3, 2a3f862199883ceff5e3c74126f0c80770653e05}. This includes advanced negative sampling techniques and noise filtering mechanisms that ensure models learn accurate and reliable representations. Finally, a significant portion is dedicated to the importance of rigorous \\textit{evaluation, benchmarking, and reproducibility} \\cite{community_0, community_6}. This area underscores the necessity for standardized metrics, fair comparisons, and transparent research practices to ensure that KGE advancements are scientifically sound and lead to reliable, trustworthy applications. Together, these practical considerations are indispensable for translating the rich theoretical landscape of KGE into impactful and dependable real-world solutions.\n\n\\subsection{Efficiency, Compression, and Scalability}\n\\label{sec:6_1_efficiency,_compression,__and__scalability}\n\nThe practical deployment of Knowledge Graph Embedding (KGE) models for massive knowledge graphs (KGs) is often hampered by significant computational costs, extensive training times, and prohibitive memory footprints. Addressing these bottlenecks is crucial for transitioning KGEs from academic benchmarks to real-world applications, especially in resource-constrained environments. This area of research focuses on techniques spanning knowledge distillation, embedding compression, parameter-efficient learning, optimized system designs, and novel algorithms.\n\nOne prominent strategy for reducing the computational burden and memory footprint is \\textit{knowledge distillation}. \\cite{zhu2020} introduced DualDE, a method that distills knowledge from a high-dimensional, high-performing teacher KGE model into a low-dimensional student model. This approach significantly reduces embedding parameters (by 7-15x) and increases inference speed (by 2-6x) while retaining competitive performance. DualDE's generality allows its application across various KGE architectures, making it a versatile tool for efficiency. However, a common trade-off in distillation is a minor, albeit often acceptable, loss in performance, which is inherent to compressing information.\n\nComplementary to distillation are direct \\textit{embedding compression} techniques. \\cite{sachan2020} proposed representing entities with discrete codes, achieving remarkable compression ratios (50-1000x) of the embedding layer with only a minor performance drop. Building on this, LightKG \\cite{wang2021} introduced a lightweight framework that stores only a few codebooks and indices, drastically reducing storage and boosting inference efficiency through quick look-ups. LightKG also incorporates a dynamic negative sampling strategy, which further enhances performance. While these methods offer substantial gains in storage and inference speed, they rely on approximating the original embeddings, which might introduce subtle inaccuracies compared to full-precision representations.\n\n\\textit{Parameter-efficient learning} offers another avenue for scalability. Entity-Agnostic Representation Learning (EARL) \\cite{chen2023} tackles the escalating parameter storage costs by learning embeddings only for a small set of \"reserved entities.\" Embeddings for other entities are then derived from their context (e.g., connected relations, k-nearest reserved entities, multi-hop neighbors) using universal, entity-agnostic encoders. This approach results in a static and significantly lower parameter count, making it particularly beneficial for large and continuously growing KGs. A potential limitation of EARL is its reliance on contextual information; entities with sparse connections or those truly \"unseen\" without sufficient neighbors might have less expressive representations compared to directly learned embeddings.\n\nBeyond model-specific optimizations, \\textit{optimized system designs} are critical for large-scale KGE training. GE2 \\cite{zheng2024} proposes a general and efficient KGE learning system that addresses long CPU times and high CPU-GPU communication overhead, especially in multi-GPU setups. GE2 offloads operations to GPUs and introduces COVER, a novel algorithm for managing data swap between CPU and multiple GPUs, achieving speedups of 2x to 7.5x across various models and datasets. This system-level innovation provides a foundational improvement for KGE research, enabling faster experimentation and deployment of complex models. While GE2 focuses on training efficiency, it doesn't directly reduce the size of the *final* embedding model, which is where distillation and compression techniques become relevant.\n\nNovel algorithmic approaches also contribute significantly to efficiency. \"Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis\" \\cite{peng2021} introduces a fundamentally different paradigm by proposing a closed-form solution using Orthogonal Procrustes Analysis (OPA). This enables full-batch learning and non-negative sampling, reducing training time and carbon footprint by orders of magnitude while yielding competitive performance. The closed-form nature bypasses iterative optimization, which is a major source of computational cost in traditional KGEs. However, the inherent mathematical constraints of OPA might limit its expressiveness compared to more flexible, iteratively optimized models for certain complex relational patterns.\n\nFor Graph Neural Network (GNN)-based KGEs, which are known for their computational intensity, \\textit{graph partitioning strategies} are essential. CPa-WAC \\cite{modak2024} employs modularity maximization-based constellation partitioning to break down KGs into subgraphs. This allows for separate processing, reducing memory and training time for GNNs while aiming to retain prediction accuracy. CPa-WAC demonstrates up to a five-fold speedup, highlighting the effectiveness of distributed processing. Nevertheless, partitioning a graph can sometimes hinder the capture of global dependencies that span across different subgraphs, potentially impacting performance on tasks that require broader structural understanding.\n\nFinally, a comprehensive understanding of \\textit{parallelization techniques} is vital. \\cite{kochsiek2021} provided a critical meta-study, re-implementing and comparing various parallelization strategies for KGE training. Their work revealed that naive parallelization can degrade embedding quality and proposed effective mitigations, such as a variation of the stratification technique. This study underscores that simply distributing computation does not guarantee efficiency or quality, emphasizing the need for careful technique selection.\n\nIn summary, the pursuit of efficiency and scalability in KGE involves a multi-faceted approach. Techniques like DualDE \\cite{zhu2020}, \\cite{sachan2020}, and LightKG \\cite{wang2021} focus on compressing the model itself for storage and inference. EARL \\cite{chen2023} addresses parameter growth for evolving KGs. Meanwhile, GE2 \\cite{zheng2024}, OPA-based learning \\cite{peng2021}, CPa-WAC \\cite{modak2024}, and parallelization studies \\cite{kochsiek2021} target the efficiency of the training process. These innovations collectively aim to overcome the practical bottlenecks of KGE, making them deployable in resource-constrained environments and capable of handling the ever-growing scale of real-world knowledge bases. The ongoing challenge lies in balancing the gains in efficiency and scalability with the preservation of model expressiveness and predictive accuracy.\n\\subsection{Robustness and Training Optimization}\n\\label{sec:6_2_robustness__and__training_optimization}\n\nThe efficacy of Knowledge Graph Embedding (KGE) models in real-world applications is profoundly dependent on their robustness against data imperfections and the optimization of their training processes. Knowledge graphs are inherently noisy, incomplete, and often suffer from imbalanced data distributions, necessitating sophisticated techniques to ensure that learned representations are accurate, reliable, and generalize well. This subsection delves into methods designed to enhance KGE model robustness and refine their training, particularly focusing on negative sampling strategies.\n\nA critical aspect of model reliability is the trustworthiness of its predictions. \\cite{tabacof2019} highlight that many popular KGE models, despite achieving high accuracy, produce uncalibrated probability estimates, meaning their predicted scores do not directly correspond to true probabilities. They propose post-hoc calibration methods like Platt scaling and isotonic regression, which are particularly valuable when ground truth negatives are scarce, a common scenario in KGs. While these methods offer a general solution applicable to various KGE models, their effectiveness relies on the quality of the calibration data and may introduce additional computational overhead during inference. This emphasizes that model performance metrics alone are insufficient; the reliability of output probabilities is equally crucial for practical deployment.\n\nTo address the pervasive issue of noisy data within KGs, which often arises from automatic knowledge construction, \\cite{zhang2021} introduce a multi-task reinforcement learning framework. This innovative approach actively filters out noisy triples during training, allowing the KGE model to learn from a cleaner, more reliable subset of facts. By exploiting correlations among semantically similar relations through multi-task learning, their method aims to learn more robust representations. This proactive noise-filtering mechanism is a significant advancement over passive error handling, as it directly impacts the quality of the input data for embedding. However, the complexity introduced by a reinforcement learning agent within the training loop can increase computational cost and require careful hyperparameter tuning, posing scalability challenges for extremely large KGs.\n\nAnother common imperfection in KGs is data imbalance, where entities and relations follow a long-tail distribution, with a few occurring frequently and many appearing rarely. Traditional KGE methods often assign equal weights during training, leading to unreliable representations for infrequent (long-tail) entities and relations. To counteract this, \\cite{zhang2023} propose WeightE, a weighted KGE model that employs a bilevel optimization scheme to assign differential weights. WeightE dynamically endows lower weights to frequent elements and higher weights to infrequent ones, ensuring that long-tail entities and relations receive adequate training attention. This flexible weighting technique can be applied to various existing KGE models, offering a practical solution to a widespread problem. While effective, the bilevel optimization adds a layer of complexity to the training process, which might require more computational resources or careful convergence monitoring compared to standard training.\n\nBeyond handling data imperfections, optimizing the training process itself, particularly through effective negative sampling, is paramount. KGE models typically rely on contrastive learning, requiring both positive (observed) and negative (false) triples. The quality of generated negative samples profoundly impacts model performance \\cite{qian2021, madushanka2024}. Early approaches to negative sampling often struggled in noisy environments. \\cite{shan2018} address this with a confidence-aware negative sampling method for noisy KGE, introducing the concept of negative triple confidence to improve training stability and prevent issues like zero loss or false detection. This method acknowledges that not all negative samples are equally informative, especially in the presence of noise. Building on the idea of identifying \"hard\" negatives, \\cite{zhang2018} propose NSCaching, an efficient caching strategy that tracks and samples challenging negative triplets. This approach, inspired by generative adversarial networks (GANs) but simpler, aims to distill the benefits of complex adversarial sampling into a more computationally efficient framework, demonstrating a trade-off between model complexity and training efficiency.\n\nA more radical departure from traditional negative sampling is presented by \\cite{li2021} with their Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE). This method proposes to avoid negative sampling entirely by considering all negative instances. While this theoretically removes the uncertainty and potential instability inherent in sampling, it dramatically increases computational complexity. NS-KGE addresses this by leveraging mathematical derivations to reduce the complexity of the non-sampling loss function, aiming for both better efficiency and accuracy. This non-sampling paradigm offers a compelling alternative, particularly for models whose loss functions can be efficiently reformulated, but its generalizability across all KGE architectures and its practical scalability to extremely dense KGs remain areas of active research.\n\nThe increasing complexity of KGE models and the integration of diverse data modalities further complicate negative sampling. \\cite{zhang2023} (Modality-Aware Negative Sampling) address this by proposing MANS, a method specifically designed for multi-modal KGE. MANS aligns structural and visual embeddings for entities, demonstrating that negative sampling strategies must adapt to the unique characteristics of multi-modal information to learn meaningful embeddings. This highlights a critical development direction where training optimization must evolve in tandem with model architectural advancements.\n\nOverall, the advancements in robustness and training optimization reflect a maturing KGE field that is moving beyond purely theoretical model expressiveness towards practical utility. The systematic reviews of negative sampling \\cite{qian2021, madushanka2024} underscore its foundational importance, even as models like \\cite{chen2025} (ConQuatE) introduce sophisticated quaternion-based embeddings to handle polysemy. The continued focus on refining training mechanisms, such as negative sampling, alongside the development of advanced architectures, indicates a holistic approach where the effectiveness of complex KGE models is deeply intertwined with robust and efficient training methodologies. While significant progress has been made in handling noise, imbalance, and sampling, the challenge of efficiently and accurately identifying the \"true\" negative distribution in KGs, especially in dynamic and multi-modal settings, remains a theoretical gap and an active area of research.\n\\subsection{Evaluation, Benchmarking, and Reproducibility}\n\\label{sec:6_3_evaluation,_benchmarking,__and__reproducibility}\n\n\nThe rapid proliferation of Knowledge Graph Embedding (KGE) models has underscored the critical importance of rigorous evaluation, standardized benchmarking, and robust reproducibility practices. Without these, fair comparisons between models become challenging, scientific progress can be hindered by unreliable results, and the trustworthy deployment of KGE models in real-world applications is compromised. The field has increasingly recognized the need to move towards higher standards of empirical validation and transparency to ensure the reliability and generalizability of research findings.\n\nA significant step towards addressing these challenges has been the development of unified frameworks and libraries. \\cite{broscheit2020} introduced \\texttt{LibKGE}, an open-source PyTorch-based library designed to foster reproducible research. Its key strengths lie in its high configurability, decoupled components that allow for flexible mixing and matching, and comprehensive logging, making it an invaluable tool for conducting systematic experimental studies and analyzing the contributions of individual model components. While \\texttt{LibKGE} provides the infrastructure for reproducible experimentation, large-scale comparative studies have simultaneously exposed the widespread issues plaguing KGE research.\n\n\\cite{ali2020}'s seminal work, \"Bringing Light Into the Dark,\" provided a stark revelation of reproducibility failures within the KGE community. By re-implementing and evaluating 21 models within a unified framework, \\texttt{PyKEEN}, the authors found that many published results could not be reproduced with their reported hyperparameters, and some not at all. This highlights a significant methodological limitation: the heterogeneity in implementations, training procedures, and evaluation protocols across different research groups often leads to incomparable results and inflated performance claims. The study emphasized that model performance is not solely determined by architecture but by a complex interplay of architecture, training approach, loss function, and the explicit modeling of inverse relations. This suggests that many experimental setups in prior work lacked the necessary standardization to ensure generalizability.\n\nFurther compounding these issues, \\cite{rossi2020} conducted a comprehensive comparison of 18 state-of-the-art KGE methods for link prediction, critically examining the effect of design choices and exposing biases in standard evaluation practices. They highlighted that the common practice of aggregating accuracy over a large number of test facts, where some entities are vastly more represented than others, allows models to achieve good results by focusing on these high-frequency entities, thereby ignoring the majority of the knowledge graph. This inherent bias in benchmark datasets can lead to an overestimation of a model's true generalization capabilities, as its performance might be artificially boosted by exploiting statistical artifacts rather than genuinely learning complex relational patterns. This reveals a critical assumption made in many KGE evaluationsthat benchmark datasets provide a uniformly representative test bedwhich is often unrealistic.\n\nBeyond evaluation biases, the impact of hyperparameter tuning on KGE quality has also been rigorously investigated. \\cite{lloyd2022} employed Sobol sensitivity analysis to quantify the importance of various hyperparameters, revealing substantial variability in their sensitivities across different knowledge graphs. This implies that optimal hyperparameter configurations are often dataset-specific, making universal recommendations difficult and further complicating reproducibility. A particularly concerning finding was the identification of data leakage in the widely used UMLS-43 benchmark due to inverse relations, which could lead to artificially inflated performance metrics. This directly challenges the integrity of a common experimental setup and underscores the need for meticulous data curation and validation.\n\nCollectively, these studies reveal that the KGE field, while innovative in model development, has historically suffered from a lack of meta-scientific rigor. The methodological limitations include inconsistent implementations, biased evaluation metrics, and an underestimation of hyperparameter sensitivity. These issues directly impact the generalizability of findings, as models might be overfit to specific experimental conditions or benchmark quirks. The theoretical gap isn't necessarily in the KGE models themselves, but in the overarching framework for their empirical validation. Addressing these challenges requires a concerted community effort towards adopting unified frameworks, conducting transparent and reproducible experiments, and developing more robust, unbiased evaluation metrics that truly reflect a model's understanding of the knowledge graph. This move towards higher standards is crucial for fostering reliable scientific progress and ensuring the trustworthy deployment of KGE models in critical applications.\n",
    "Applications and Real-World Impact of KGE": "\\label{sec:applications__and__real-world_impact_of_kge}\n\n\\section{Applications and Real-World Impact of KGE}\n\\label{sec:applications_and_real_world_impact_of_kge}\n\nFollowing the discussion on practical considerations for KGE models in Section \\ref{sec:practical_considerations_efficiency_robustness_and_evaluation}, which focused on ensuring their efficiency, robustness, and rigorous evaluation, this section shifts its attention to the tangible outcomes and widespread utility of these advancements. Here, we move beyond theoretical developments to showcase the diverse and significant real-world impact of knowledge graph embedding across various artificial intelligence applications. KGE models are no longer confined to academic benchmarks; they are actively leveraged to address complex problems across numerous domains, demonstrating their transformative potential in modern AI systems.\n\nThe subsequent subsections will delve into how KGE underpins fundamental tasks such as \\textit{link prediction and knowledge graph completion}, which are crucial for enhancing the completeness and inferential capabilities of knowledge bases \\cite{community_0, community_1}. We will then explore its vital role in \\textit{entity alignment}, enabling the seamless integration of heterogeneous knowledge sources by identifying equivalent entities across disparate graphs \\cite{community_5}. Furthermore, this section highlights the application of KGE in enhancing user-facing AI systems, specifically in improving the intelligence and personalization of \\textit{question answering} and \\textit{recommender systems} \\cite{community_1, community_3}. Finally, we will examine various \\textit{domain-specific applications}, from biological systems to patent analysis, emphasizing how KGE models are tailored to solve industry-specific challenges and often incorporate principles of \\textit{explainability} to build trust and provide actionable insights \\cite{community_2, community_6}. This comprehensive overview illustrates the practical utility, tangible benefits, and broad applicability of knowledge graph embedding techniques, underscoring their indispensable contribution to the evolution of intelligent systems that can effectively understand, reason with, and leverage vast amounts of structured knowledge.\n\n\\subsection{Link Prediction and Knowledge Graph Completion}\n\\label{sec:7_1_link_prediction__and__knowledge_graph_completion}\n\nLink prediction (LP) and knowledge graph completion (KGC) represent the fundamental applications of knowledge graph embedding (KGE), aiming to infer missing facts and enhance the completeness of knowledge graphs. These tasks are crucial for making KGs more robust and informative for downstream AI systems by automatically inferring unobserved facts within the graph structure. The evolution of KGE models for LP/KGC reflects a continuous effort to improve accuracy, handle complex relational patterns, and address practical challenges.\n\nEarly KGE models primarily leveraged geometric transformations to represent entities and relations. Translational models, such as TransH \\cite{wang2014} and TransD \\cite{ji2015}, extended the foundational TransE by modeling relations as translations on hyperplanes or through dynamic mapping matrices, respectively. TransH notably improved the handling of one-to-many and many-to-one relations by allowing entity projections, while TransD further refined this by considering the diversity of both entities and relations. However, these models, while efficient, often struggled to capture more intricate logical patterns like symmetry, antisymmetry, inversion, and composition. This limitation spurred the development of rotational models, with RotatE \\cite{sun2018} defining relations as rotations in complex vector spaces. RotatE demonstrated superior expressiveness for these complex patterns, significantly outperforming its translational predecessors in link prediction tasks. Further geometric innovations include embedding entities on Lie groups (e.g., TorusE \\cite{ebisu2017}) to address regularization issues, exploring alternative metrics like the Cycle metric \\cite{yang2021} for enhanced expressiveness, and introducing powerful transformations such as Householder parameterization (HousE \\cite{li2022}) or compound operations (CompoundE \\cite{ge2022}, CompoundE3D \\cite{ge2023}) to capture a broader spectrum of relational semantics. More recently, models like HolmE \\cite{zheng2024} have focused on ensuring closure under composition, a theoretical property vital for modeling under-represented compositional patterns, while MQuinE \\cite{liu2024} addresses specific theoretical deficiencies (\"Z-paradox\") in existing models to enhance expressiveness. While these geometric models offer mathematical elegance and interpretability, their ability to capture highly complex, non-linear interactions can be limited compared to deep learning approaches.\n\nThe advent of deep learning architectures marked a significant paradigm shift in KGE for LP/KGC. Convolutional Neural Networks (CNNs) have been widely adopted to extract local features and model interactions between entity and relation embeddings. Models like AcrE \\cite{ren2020} and ReInceptionE \\cite{xie2020} utilize various convolutional layers and attention mechanisms to capture complex relation patterns and aggregate entity-specific features, achieving state-of-the-art results. More recent CNN-based methods, such as CNN-ECFA \\cite{hu2024} and SEConv \\cite{yang2025}, continue to refine feature aggregation and interaction. Graph Neural Networks (GNNs) and attention mechanisms further leverage the graph's topology, with models like DisenKGAT \\cite{wu2021} employing disentangled graph attention networks for diverse representations and GAATs \\cite{wang2020} incorporating graph attenuated attention to capture rich neighborhood information. Transformer-based architectures, including CoKE \\cite{wang2019}, Knowformer \\cite{li2023}, and TGformer \\cite{shi2025}, treat KGs as sequences or integrate graph structures into Transformer frameworks, leveraging self-attention to capture long-range dependencies and contextualized representations. These deep learning models excel at learning intricate, non-linear, and context-dependent features, often surpassing purely geometric approaches in accuracy, but typically incur higher computational costs and can be less interpretable.\n\nBeyond structural information, KGE models for LP/KGC have been enriched by incorporating auxiliary information, logical rules, and multi-modal data. Integrating entity types, as seen in TransET \\cite{wang2021} and TaKE \\cite{he2023}, provides semantic guidance that significantly improves KG completion, especially in low-resource settings. Hyper-relational KGE models like HINGE \\cite{rosso2020} move beyond simple triplets to incorporate associated key-value pairs, capturing richer data semantics. Rule-based KGE approaches, such as RUGE \\cite{guo2017} and RulE \\cite{tang2022}, iteratively guide embedding learning with soft logical rules, enhancing reasoning capabilities and ensuring semantic consistency, though rule extraction and balancing rule adherence with flexibility remain challenges. Furthermore, multi-modal KGEs, exemplified by SSP \\cite{xiao2016} which projects text descriptions into semantic space, and Joint Language Semantic and Structure Embedding \\cite{shen2022} which integrates pre-trained language models, enrich embeddings with external semantic context, crucial for overcoming data sparsity and improving understanding.\n\nThe practical effectiveness of KGE for LP/KGC also heavily relies on training optimizations and robustness. Negative sampling strategies, such as Confidence-Aware Negative Sampling \\cite{shan2018} and NSCaching \\cite{zhang2018}, are critical for efficient and effective training, though the \"true\" negative distribution remains a theoretical gap. Some approaches, like Efficient Non-Sampling KGE \\cite{li2021}, even attempt to avoid negative sampling entirely to achieve more stable performance, albeit with increased computational complexity. Robustness against noisy data is addressed by methods like those using multi-task reinforcement learning \\cite{zhang2021} to filter out erroneous triples or weighted training schemes (e.g., WeightE \\cite{zhang2023}) to handle data imbalance. The continuous development across these diverse methodologies underscores the central role of link prediction and knowledge graph completion in advancing the utility and reliability of knowledge graphs for a wide array of AI applications \\cite{dai2020, cao2022, ge2023, rossi2020}.\n\\subsection{Entity Alignment}\n\\label{sec:7_2_entity_alignment}\n\nEntity Alignment (EA) is a critical task in knowledge graph integration, aiming to identify equivalent entities across different, often heterogeneous, knowledge graphs (KGs). The proliferation of KGs from diverse sources necessitates robust methods for their integration, which is fundamental for building comprehensive knowledge bases and enabling sophisticated cross-KG reasoning \\cite{dai2020, choudhary2021}. Knowledge Graph Embeddings (KGEs) have emerged as a powerful, data-driven approach to tackle this challenge, transforming symbolic entities and relations into low-dimensional vector spaces where semantic correspondences can be identified through similarity measures \\cite{yan2022, cao2022}. This approach leverages the ability of KGEs to capture intricate structural and semantic patterns, making them highly suitable for finding equivalences between disparate knowledge structures.\n\nA significant hurdle in embedding-based entity alignment is the scarcity of labeled training data, which can limit the accuracy and generalizability of models. To address this, bootstrapping methods have been developed. For instance, \\cite{sun2018} proposed an iterative bootstrapping approach that progressively labels likely entity alignments to augment the training data for learning alignment-oriented KG embeddings. This method strategically employs an alignment editing technique to mitigate the accumulation of errors during the iterative process, which is a common pitfall in self-training schemes. While effective in leveraging unlabeled data, the performance of such bootstrapping approaches can be sensitive to the quality of the initial seed alignments and the robustness of the error reduction mechanism, as false positives in early iterations can propagate and degrade overall accuracy.\n\nExtending beyond purely bootstrapping, semi-supervised learning frameworks have been introduced to more effectively utilize both limited labeled data and abundant unlabeled information. \\cite{pei2019} presented a semi-supervised entity alignment method (SEA) that not only leverages unlabeled entities but also incorporates an awareness of entity degree differences. This is crucial because entities with vastly different degrees (i.e., number of connections) can lead to biased embeddings, making alignment challenging, particularly between high-frequency and low-frequency entities. By employing adversarial training, SEA aims to learn more robust embeddings that are less affected by these structural disparities. This approach addresses a practical limitation of many KGE models, where embedding quality can be disproportionately influenced by highly connected entities, thereby improving alignment accuracy across the entire spectrum of entities. However, the complexity of adversarial training can introduce challenges in model stability and hyperparameter tuning.\n\nFurther enhancing the robustness and accuracy of EA, multi-view frameworks integrate diverse types of entity information. \\cite{zhang2019} proposed a novel multi-view KGE framework that unifies entity names, relational structures, and attributes to learn more comprehensive embeddings for alignment. Traditional KGE methods often focus predominantly on relational structures, overlooking other rich features that can provide complementary semantic cues. By combining these multiple views with various strategies and designing cross-KG inference methods, this approach significantly improves alignment performance. The strength of multi-view learning lies in its ability to capture a broader spectrum of semantic information, making the embeddings more discriminative. Nevertheless, the challenge lies in effectively weighting and integrating potentially conflicting signals from different views, and the computational overhead increases with the number of views considered.\n\nMore recently, the integration of ontological information has provided another powerful dimension for entity alignment. \\cite{xiang2021} introduced OntoEA, an ontology-guided entity alignment method that jointly embeds both KGs and their associated ontologies. This approach explicitly utilizes critical meta-information, such as class hierarchies and class disjointness constraints, which are often ignored by purely structural or attribute-based methods. By enforcing these ontological constraints during the embedding process, OntoEA can prevent false mappings and guide the alignment towards semantically coherent equivalences. This addresses a theoretical gap where KGEs, while powerful, often lack an explicit mechanism to incorporate higher-level schema knowledge. The effectiveness of OntoEA, however, is contingent on the availability and quality of consistent ontological information across the KGs being aligned, which may not always be present in real-world scenarios.\n\nThe collective advancements in these KGE-based EA methods underscore a clear evolution in the field. Early approaches focused on leveraging structural similarity, while subsequent methods have progressively integrated more semantic context, auxiliary information, and robust learning paradigms to overcome limitations like data scarcity and feature incompleteness. Comprehensive surveys and experimental reviews, such as those by \\cite{zhu2024} and \\cite{fanourakis2022}, further highlight the strengths and weaknesses of various KGE-based EA techniques. They emphasize the need for more robust noise filtering strategies, better utilization of additional information, and rigorous comparative analyses across diverse datasets to ensure generalizability. These meta-analyses confirm that KGEs provide a versatile and powerful foundation for integrating heterogeneous knowledge sources, enabling the construction of more comprehensive knowledge bases and facilitating complex cross-KG reasoning, which is crucial for advancing knowledge-driven AI applications.\n\\subsection{Question Answering and Recommendation Systems}\n\\label{sec:7_3_question_answering__and__recommendation_systems}\n\nKnowledge Graph Embeddings (KGEs) have emerged as a pivotal technology for bridging the semantic gap between natural language and structured knowledge, enabling more intelligent and interpretable interactions in diverse applications such as Question Answering (QA) and Recommender Systems. These applications leverage KGEs to transform complex symbolic reasoning into efficient vector space operations, thereby enhancing performance and user experience \\cite{dai2020, cao2022}.\n\nIn the realm of Question Answering over Knowledge Graphs (QA-KG), KGEs facilitate the understanding of natural language queries by mapping them to entities and relations within the underlying knowledge graph. Early frameworks, such as Knowledge Embedding based Question Answering (KEQA), demonstrated the utility of KGEs by jointly recovering head entity, predicate, and tail entity representations in the embedding space to answer simple natural language questions \\cite{huang2019}. KEQA's strength lies in its ability to address predicate variability and entity ambiguity by leveraging the semantic proximity captured by embeddings. However, its focus on \"simple questions\" highlights a limitation in handling more complex, multi-hop, or nuanced queries, which often require deeper integration with natural language processing (NLP) capabilities.\n\nMore advanced QA systems have evolved into hybrid architectures that seamlessly integrate KGEs with sophisticated NLP models. A prime example is the Marie and BERT system for chemistry, which showcases a comprehensive approach to domain-specific QA \\cite{zhou2023}. This system employs hybrid KGEs, leveraging multiple embedding spaces to capture diverse relational patterns, and integrates a BERT-based entity-linking model to enhance robustness and accuracy in identifying entities from natural language queries. Furthermore, Marie and BERT addresses the complexities of deep ontologies by deriving implicit multi-hop relations and incorporates mechanisms for numerical filtering, demonstrating a significant leap in handling intricate, fact-oriented information retrieval in specialized domains. While such hybrid systems offer superior performance in complex scenarios, their domain specificity and the inherent complexity of integrating heterogeneous components (KGEs, BERT, semantic agents) can limit generalizability and increase development overhead. The methodological challenge lies in effectively harmonizing the continuous vector representations from KGEs with the discrete, symbolic reasoning often required for precise QA.\n\nFor recommender systems, KGEs provide a powerful mechanism to model user preferences and item characteristics by representing them as entities in a knowledge graph, thereby enabling more personalized and transparent suggestions. Recurrent Knowledge Graph Embedding (RKGE) was an early and influential approach that utilized a recurrent network to automatically learn semantic representations of paths between entities \\cite{sun2018}. By fusing these path semantics into the recommendation process, RKGE not only improved recommendation accuracy but also offered meaningful explanations based on path saliency, a crucial step towards interpretable recommendations. The recurrent network architecture allowed RKGE to discriminate the saliency of different paths in characterizing user preferences, moving beyond traditional feature engineering that often requires extensive domain knowledge.\n\nBuilding upon this foundation, recent research has pushed towards more contextualized and explainable approaches. Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation (CKGE) represents a significant advancement in this direction \\cite{yang2023}. CKGE constructs \"meta-graphs\" for talent-course pairs, incorporating contextualized neighbor semantics and high-order connections as \"motivation-aware information.\" It then employs a novel KG-based Transformer, equipped with relational attention and structural encoding, to model the global dependencies of KG structured data. A key innovation in CKGE is its \"local path mask prediction,\" which effectively reveals the importance of different paths, thereby offering precise and explainable recommendations that can discriminate the saliencies of meta-paths in characterizing corresponding preferences \\cite{yang2023}.\n\nComparing RKGE and CKGE, the evolution highlights a shift from recurrent networks for path modeling to more sophisticated Transformer architectures that capture richer contextual information. While RKGE provided explanations based on path saliency, CKGE offers a more granular and motivation-aware interpretability by integrating contextualization and high-order connections. This progression, as noted in the development directions, reflects an acceleration in research driven by the demand for more sophisticated, robust, and interpretable recommendation solutions. However, the increased complexity of models like CKGE, with their meta-graph construction and Transformer integration, introduces trade-offs in computational cost and the interpretability of the underlying embedding space. The theoretical gap remains in developing truly intuitive and actionable explanations that are universally understandable to human users, rather than merely technical artifacts of the model.\n\nIn summary, KGEs have profoundly impacted QA and recommender systems by providing a robust framework to bridge natural language and structured knowledge. From foundational KEQA to hybrid systems like Marie and BERT, and from recurrent RKGEs to contextualized CKGEs, these advancements demonstrate KGE's utility in enabling more intelligent, personalized, and interpretable user interactions. The ongoing challenge lies in balancing model expressiveness with computational efficiency and ensuring that the generated explanations are genuinely transparent and actionable across diverse application contexts.\n\\subsection{Domain-Specific Applications and Explainability}\n\\label{sec:7_4_domain-specific_applications__and__explainability}\n\nThe utility of Knowledge Graph Embedding (KGE) models extends significantly into specialized, high-stakes domains, where their ability to transform symbolic knowledge into actionable insights is paramount. This section highlights the application of KGE in fields such as biological systems, patent metadata analysis, and drug repurposing, emphasizing how these models are tailored, validated, and increasingly scrutinized for explainability to build trust and deliver verifiable solutions.\n\nIn biological and biomedical systems, KGE models offer powerful tools for discovery and analysis. \\cite{mohamed2020} provides a comprehensive review of KGE applications in this domain, showcasing their predictive and analytical capabilities for tasks like drug-target interactions and polypharmacy side effects. The authors argue that KGEs are a natural fit for representing complex biological knowledge, overcoming the scalability limitations of traditional graph exploratory approaches. Building on this, \\cite{zhu2022} demonstrates multimodal reasoning based on KGE for specific diseases. They construct Specific Disease Knowledge Graphs (SDKGs) and integrate structural, category, and description embeddings using reverse-hyperplane projection. This multimodal approach enhances the discovery of new, reliable knowledge, underscoring the value of tailoring KGEs to domain-specific knowledge structures and leveraging diverse data modalities to improve reasoning.\n\nA particularly compelling example of KGE application in a critical domain is drug repurposing for diseases like COVID-19. \\cite{islam2023} proposes an innovative approach that utilizes ensemble KGEs to generate robust latent representations, which are then fed into a deep neural network for identifying potential drug candidates. Crucially, this work moves beyond generic KGE evaluation metrics by incorporating \\textit{molecular docking} to validate predictions, a domain-specific and verifiable method for assessing drug-target interactions. This integration of molecular-level evaluation is a significant step towards delivering transparent solutions, as it provides concrete, scientific validation for the abstract KGE predictions. Furthermore, \\cite{islam2023} addresses the paramount need for explainability by providing explanations through rules extracted from the knowledge graph and instantiated by explanatory paths. This allows medical professionals and researchers to understand *why* a particular drug is predicted, fostering trust and enabling actionable insights in a field where decisions have direct human impact.\n\nBeyond biomedicine, KGEs are also applied to analyze complex structured data like patent metadata. \\cite{li2022} operationalizes knowledge proximity within the US Patent Database by training KGE models on a \"PatNet\" knowledge graph constructed from patent citations, inventors, assignees, and domain classifications. By using cosine similarity between learned embeddings, they measure knowledge proximity between homogeneous (e.g., patent-patent) and heterogeneous (e.g., inventor-assignee) entities. This application demonstrates how KGEs can be tailored to specific industry problems, providing quantitative measures for abstract concepts like \"knowledge proximity\" and enabling the analysis of domain expansion profiles for inventors and assignees. While this work primarily focuses on predictive performance, the inherent interpretability of proximity measures in the embedding space can offer insights into innovation landscapes.\n\nThe growing demand for interpretable KGE models in these high-stakes fields is a significant evolutionary trend. The explicit focus on explainability in \\cite{islam2023} highlights a shift from merely achieving high performance on abstract metrics to delivering verifiable and transparent solutions. This aligns with broader research efforts in making KGEs more understandable. For instance, methods that integrate logical rules and constraints into the embedding process, such as \\cite{guo2017}'s iterative guidance from soft rules (RUGE) or \\cite{tang2022}'s RulE framework, which learns rule embeddings jointly with entity and relation embeddings, inherently contribute to explainability. By aligning embeddings with human-understandable logical patterns, these approaches can provide a basis for explaining model predictions. Similarly, \\cite{ding2018} showed that even simple constraints like non-negativity on entity representations can improve model interpretability by structuring the embedding space.\n\nHowever, achieving robust explainability in complex KGE models, especially those leveraging deep learning architectures, remains a challenge. The trade-off often lies between the high expressiveness and predictive power of complex models and the inherent difficulty in extracting clear, human-understandable explanations from their latent spaces. While \\cite{islam2023} successfully combines ensemble KGEs with molecular docking and rule-based explanations, the generalizability of such multi-faceted explanation strategies across all domain-specific KGE applications requires further investigation. The theoretical gap in universally interpretable embedding spaces, particularly for highly non-linear models, prevents a straightforward solution to providing transparent insights for every prediction. Nevertheless, the explicit integration of domain-specific validation and explanation mechanisms, as exemplified by these works, marks a crucial step towards building trust and enabling the responsible deployment of KGE technologies in critical real-world scenarios.\n",
    "Conclusion and Future Directions": "\\section{Conclusion and Future Directions}\n\\label{sec:conclusion__and__future_directions}\n\n\\label{sec:conclusion__and__future_directions}\n\\section{Conclusion and Future Directions}\n\\label{sec:conclusion_and_future_directions}\n\nHaving explored the diverse applications and real-world impact of Knowledge Graph Embedding (KGE) models in Section \\ref{sec:applications_and_real_world_impact_of_kge}, this concluding section offers a comprehensive synthesis of the field's intellectual trajectory and charts a course for its future. We reflect on the remarkable progression of KGE research, which has evolved from foundational geometric and algebraic models to sophisticated deep learning architectures and advanced application-driven solutions. This journey has seen continuous efforts to enhance model expressiveness, efficiency, and robustness, leading to KGEs becoming indispensable tools for tasks ranging from link prediction and entity alignment to complex question answering and recommendation systems \\cite{community_0, community_1, community_3}.\n\nDespite these significant advancements, the field of KGE still grapples with persistent open challenges and theoretical gaps. These include the intricate balance between model complexity and interpretability, the efficient integration of high-quality logical rules, and the development of truly scalable and generalizable inductive models capable of handling dynamic, ever-evolving knowledge graphs \\cite{community_1, community_6}. Addressing these limitations is crucial for unlocking the full potential of KGE technologies. Looking ahead, this section will outline several emerging trends that are poised to redefine the landscape of KGE, notably the increasing integration with large language models for richer semantic understanding, advancements in adaptive multi-curvature embeddings, and the critical development of federated and privacy-preserving KGE methods \\cite{85064a4b1b96863af4fccff9ad34ce484945ad7b, community_1}. Finally, we will delve into the paramount ethical considerations surrounding KGE, including potential biases in learned representations and the imperative for transparent and responsible deployment in sensitive applications. This forward-looking perspective aims to inspire new research directions and guide the responsible advancement of KGE technologies towards a more intelligent and ethically sound future.\n\n\\subsection{Summary of Key Developments}\n\\label{sec:8_1_summary_of_key_developments}\n\n\nThe field of Knowledge Graph Embedding (KGE) has undergone a remarkable evolution, transitioning from foundational geometric and algebraic models to highly sophisticated deep learning architectures, driven by a continuous pursuit of enhanced expressiveness, efficiency, and robustness. Early advancements were rooted in the geometric paradigm, where relations were conceptualized as transformations within continuous vector spaces. Pioneering models like TransE and its successors, such as TransH \\cite{wang2014} and TransD \\cite{ji2015}, established the translation-based approach, modeling relations as vector translations from head to tail entities. These early efforts focused on refining the embedding space to better capture diverse relational patterns, with TransD introducing dynamic mapping matrices for finer-grained distinctions between entities and relations, thereby improving expressiveness while managing parameter complexity \\cite{ji2015}. Further geometric innovations explored non-Euclidean spaces, such as TorusE \\cite{ebisu2017} embedding on Lie groups to address regularization challenges, and CyclE \\cite{yang2021} investigating the impact of metric choices (e.g., Cycle vs. Minkowski) on expressiveness. These models underscored the importance of the underlying geometry in accurately representing complex knowledge.\n\nA significant paradigm shift occurred with the integration of deep learning architectures, which enabled KGE models to automatically learn intricate features and structural patterns. Convolutional Neural Networks (CNNs) were adapted to KGE, with models like AcrE \\cite{ren2020} leveraging atrous convolutions and residual learning for efficient feature interactions, and ReInceptionE \\cite{xie2020} employing inception networks and attention for joint local-global structural information. More recent CNN-based approaches, such as CNN-ECFA \\cite{hu2024} and SEConv \\cite{yang2025}, continue to refine feature aggregation for improved performance. Graph Neural Networks (GNNs) further enhanced KGE by capturing neighborhood context and structural information through message passing, exemplified by DisenKGAT \\cite{wu2021}, which introduced disentangled graph attention networks for more diverse and independent component representations. The emergence of Transformer architectures, as seen in CoKE \\cite{wang2019}, Knowformer \\cite{li2023}, and TGformer \\cite{shi2025}, marked another leap, enabling contextualized embeddings by treating KGs as sequences or integrating graph structures into self-attention mechanisms, thereby capturing long-range dependencies and multi-structural features.\n\nBeyond core architectural advancements, the field has continuously sought to enrich KGE models with auxiliary information, logical rules, and multi-modal data to overcome inherent limitations. Approaches like TransET \\cite{wang2021} and TaKE \\cite{he2023} demonstrated the value of incorporating entity type information to provide semantic guidance and improve knowledge graph completion. Similarly, models like HINGE \\cite{rosso2020} moved \"beyond triplets\" to directly learn from hyper-relational facts, capturing richer data semantics. The integration of logical rules, as in RUGE \\cite{guo2017} and RulE \\cite{tang2022}, allowed KGE models to inject prior knowledge and enforce semantic consistency, moving towards more robust and interpretable reasoning. Furthermore, multi-modal KGE, exemplified by SSP \\cite{xiao2016} integrating text descriptions and recent works leveraging pre-trained language models \\cite{shen2022}, has addressed data sparsity and enhanced semantic understanding by fusing diverse information sources.\n\nThe practical deployment of KGE models has driven significant research into efficiency, robustness, and adaptability. Efforts to enhance efficiency include knowledge distillation (DualDE \\cite{zhu2020}), embedding compression \\cite{sachan2020}, parameter-efficient learning (EARL \\cite{chen2023}), and optimized training systems like GE2 \\cite{zheng2024}. Robustness has been improved through techniques like confidence-aware negative sampling \\cite{shan2018}, reinforcement learning-based noise filtering \\cite{zhang2021}, and weighted training for imbalanced data \\cite{zhang2023}. The challenge of dynamic KGs has led to inductive KGE models leveraging neighborhood aggregation \\cite{wang2018} and meta-learning \\cite{chen2021, sun2024}, as well as continual learning approaches like incremental LoRA for efficient updates \\cite{liu2024}. Moreover, the rise of federated learning has spurred research into privacy-preserving KGE, addressing communication efficiency \\cite{zhang2024} and personalization \\cite{zhang2024_personalized}, while also acknowledging security vulnerabilities like poisoning attacks \\cite{zhou2024}.\n\nIn summary, the KGE landscape has evolved from simple geometric models to complex deep learning architectures, continuously pushing the boundaries of expressiveness and efficiency. The field's progression is marked by a holistic approach: enhancing core models, enriching them with diverse contextual and logical information, and ensuring their practical utility through robust, scalable, and adaptable designs. These advancements underscore the significant strides made in transforming symbolic knowledge into actionable insights, making KGE a cornerstone for intelligent AI systems across various applications, from link prediction and question answering \\cite{huang2019, zhou2023} to recommendation systems \\cite{sun2018, yang2023}.\n\\subsection{Open Challenges and Theoretical Gaps}\n\\label{sec:8_2_open_challenges__and__theoretical_gaps}\n\nDespite significant advancements in Knowledge Graph Embedding (KGE) research, several critical open challenges and theoretical gaps persist, representing fertile ground for future investigation. These issues often stem from inherent trade-offs, the complexity of real-world knowledge graphs, and the limitations of current theoretical understandings.\n\nOne pervasive challenge lies in balancing model expressiveness with computational complexity. While models like RotatE \\cite{sun2018} and the composition-closed HolmE \\cite{zheng2024} have pushed the boundaries of capturing intricate relational patterns, their increased expressiveness often comes at the cost of higher computational demands for training and inference (Subgroup 1, Overall Perspective). Efforts in efficiency and compression, such as DualDE's distillation \\cite{zhu2020} and LightKG's codebook-based storage \\cite{wang2021}, aim to mitigate this. However, as noted by \\cite{sachan2020}, these often entail a \"minor loss in performance.\" The theoretical gap here is to devise architectures that are *inherently* expressive yet computationally lean, perhaps through novel mathematical formulations like the Orthogonal Procrustes Analysis in \\cite{peng2021}, which offers a closed-form solution for efficiency, rather than relying on post-hoc compression or distillation.\n\nEnsuring the interpretability of complex deep learning KGE models is another significant hurdle. As KGE increasingly leverages Graph Neural Networks (GNNs) and Transformer architectures (Subgroup 6, Advanced Model Design), their black-box nature becomes a concern, especially in high-stakes applications. While some models like SpherE \\cite{li2024} claim interpretability through their geometric properties, this is often specific to the model's design and does not generalize to the complex reasoning paths learned by deep networks. Application-focused works, such as the explainable drug repurposing by \\cite{islam2023} and contextualized recommendation by \\cite{yang2023}, demonstrate the *need* for explanations, but typically rely on post-hoc rule extraction or path saliency. A theoretical gap exists in developing intrinsically interpretable deep KGE models that can transparently reveal their reasoning processes without sacrificing predictive power.\n\nThe efficient extraction and integration of high-quality logical rules remain a bottleneck. While methods like RUGE \\cite{guo2017} and RulE \\cite{tang2022} have shown the value of incorporating soft rules to enhance reasoning and consistency (Subgroup 3, Rule-based & Constraint-driven KGE), the process of obtaining these rules and balancing their adherence with the flexibility to capture exceptions is challenging. Automatically extracted rules often carry \"uncertainties\" \\cite{guo2017}, and the scalability of integrating complex rule sets can be problematic \\cite{guo2020}. The theoretical challenge lies in developing robust, automated rule induction systems that can generate high-fidelity rules from noisy KGs and seamlessly integrate them into embedding models without introducing significant computational overhead or compromising the data-driven learning of nuanced patterns.\n\nFurthermore, resolving issues related to the 'true' negative distribution in training is fundamental. KGE models rely heavily on negative sampling for contrastive learning, yet the \"true\" negative distribution is inherently unknown (Subgroup 3, Negative Sampling & Training Optimization). While approaches like NSCaching \\cite{zhang2018} and confidence-aware sampling \\cite{shan2018} improve efficiency and robustness to noise, they are still heuristic approximations. The \"Efficient Non-Sampling Knowledge Graph Embedding\" \\cite{li2021} attempts to bypass sampling entirely, but requires complex mathematical derivations to manage computational complexity. As highlighted by comprehensive reviews \\cite{qian2021, madushanka2024}, this remains a persistent challenge, particularly when extending to multi-modal KGE \\cite{zhang2023}. A theoretical breakthrough is needed to either accurately model the true negative distribution or develop training paradigms that are robust to its uncertainty without prohibitive computational costs.\n\nThe field also critically needs more robust and unbiased evaluation metrics. As revealed by the \"Evaluation, Benchmarking, and Reproducibility\" subgroup, standard metrics often suffer from biases, such as the over-representation of certain entities \\cite{rossi2020}, and reported results can be sensitive to hyperparameter choices or even data leakage \\cite{lloyd2022}. The lack of standardized frameworks and reproducibility issues \\cite{ali2020, broscheit2020} further complicate fair comparisons. The theoretical gap is in developing evaluation protocols that are robust to dataset characteristics, sensitive to diverse relational patterns (e.g., long-tail, compositional), and truly reflect real-world application performance, moving beyond simple link prediction accuracy.\n\nThe challenges of scalability for extremely large and dynamic knowledge graphs are paramount. While progress has been made in efficiency \\cite{peng2021, zheng2024} and compression \\cite{sachan2020, wang2021}, handling KGs with billions of triples and continuous, real-time updates remains difficult. Dynamic KGE methods like FastKGE \\cite{liu2024} and MetaHG \\cite{sun2024} focus on efficient updates and mitigating catastrophic forgetting, but the sheer volume and velocity of changes in real-world KGs can still overwhelm these systems. Federated KGE introduces additional complexities related to communication efficiency \\cite{zhang2024} and personalized aggregation for diverse client data \\cite{zhang2024_personalized}. The theoretical challenge is to design truly elastic and adaptive KGE architectures that can scale horizontally, handle continuous streams of new information, and maintain global consistency and performance in highly distributed and dynamic environments without prohibitive computational or communication costs.\n\nFinally, the development of truly generalizable inductive models remains an open problem. While neighborhood aggregation \\cite{wang2018} and meta-learning approaches \\cite{chen2021, sun2024} have enabled inductive capabilities for new entities (Subgroup 4, Dynamic, Inductive, and Continual KGE), they often rely on existing neighbors or transferable meta-knowledge. Truly novel entities with entirely new semantic or structural patterns, or isolated entities with sparse connections, still pose a significant challenge. A deeper theoretical understanding of \"semantic evidence\" for extrapolation \\cite{li2021} is required to build models that can robustly infer representations for such unseen entities or entirely new relational types, pushing beyond mere interpolation to genuine generalization. These challenges collectively underscore the need for continued fundamental and applied research to unlock the full potential of KGE.\n\\subsection{Emerging Trends and Ethical Considerations}\n\\label{sec:8_3_emerging_trends__and__ethical_considerations}\n\nThe landscape of Knowledge Graph Embedding (KGE) is continuously evolving, driven by both technological advancements and a growing awareness of societal impact. This section delves into key emerging trends that are shaping the future of KGE research, alongside crucial ethical considerations that must guide its development. These discussions are informed by the field's methodological evolution, knowledge progression, and the increasing demand for robust, responsible AI systems.\n\nOne of the most significant emerging trends is the deeper integration of KGE with pre-trained language models (PLMs) for richer semantic understanding. Traditional KGE models, as discussed in the \"Core KGE Model Architectures and Expressiveness\" subgroup, primarily learn representations from the structural patterns of knowledge graphs \\cite{wang2014, sun2018, zheng2024}. While effective, these models often struggle with data sparsity and entities lacking sufficient structural connections, a limitation highlighted in the \"Knowledge Progression\" of KGE research. PLMs, on the other hand, excel at capturing rich contextual semantics from vast amounts of text. Hybrid approaches, such as those that leverage BERT-based models for entity linking and contextual understanding within KGE systems \\cite{zhou2023}, represent a powerful synergy. This integration allows KGEs to infer meaning from textual descriptions, thereby enhancing embedding quality and addressing the cold-start problem for new entities. Surveys like \\cite{dai2020} and \\cite{cao2022} have begun to acknowledge the potential of incorporating textual information, but the current trend moves towards more sophisticated, joint learning frameworks that align and fuse representations from both modalities. A key challenge here lies in effectively bridging the gap between the discrete, symbolic nature of KGs and the continuous, contextualized space of PLMs, while managing the increased computational complexity.\n\nAnother prominent trend involves the development of more adaptive multi-curvature embeddings. As explored in the \"Geometric KGE for Hierarchical and Complex Structures\" subgroup, hyperbolic spaces have demonstrated superior capabilities for modeling hierarchical structures due to their negative curvature \\cite{pan2021, liang2024}. However, real-world knowledge graphs often exhibit a mixture of structural patternshierarchical, cyclic, and Euclidean-like. The emerging direction is to move beyond single-geometry embeddings towards models that can adaptively utilize different curvatures (e.g., hyperbolic, spherical, Euclidean) for different parts of a knowledge graph or for different types of relations. Approaches like \\cite{shang2024}, which propose mixed geometry message functions and scoring functions, exemplify this trend. By integrating information from multiple geometric spaces, these models aim to capture diverse local structures with higher fidelity and fewer dimensions, offering a more expressive and compact representation than any single geometry could provide. The methodological challenge lies in designing robust mechanisms for dynamically selecting or combining appropriate geometries, ensuring stable training, and avoiding an explosion in model complexity. Furthermore, models like \\cite{li2024} which embed entities as spheres, extend rotational embeddings to better capture many-to-many relations and enable set retrieval, showcasing the continuous innovation in geometric KGE.\n\nAdvancements in federated and privacy-preserving KGE also constitute a critical emerging trend, directly addressing the practical and ethical concerns of data distribution and privacy. The \"Federated KGE, Privacy, and Security\" subgroup highlights the growing interest in collaboratively training KGE models across distributed knowledge graphs without centralizing sensitive data. This is crucial for applications where data privacy regulations (e.g., GDPR) are paramount. Recent works focus on improving communication efficiency, such as \\cite{zhang2024} which proposes entity-wise Top-K sparsification to reduce transmitted parameters. Furthermore, addressing data heterogeneity among clients is vital, leading to personalized federated KGE approaches that learn client-specific supplementary knowledge \\cite{zhang2024}. However, this distributed paradigm introduces new security vulnerabilities, as demonstrated by poisoning attacks that can manipulate model outcomes by injecting malicious data indirectly through aggregation \\cite{zhou2024}. The trade-off between privacy guarantees, communication efficiency, personalization, and robustness against adversarial attacks remains a central challenge, requiring sophisticated cryptographic techniques and robust aggregation mechanisms.\n\nBeyond these technological advancements, crucial ethical considerations are increasingly guiding KGE research. Foremost among these is the issue of potential biases in learned representations. KGE models learn from existing knowledge graphs, which are often constructed from diverse sources reflecting historical, societal, or cultural biases. If the training data contains skewed representations (e.g., gender stereotypes, racial biases, under-representation of certain groups), the KGE model will inevitably learn and potentially amplify these biases. This can lead to discriminatory outcomes in downstream applications, such as biased recommendations or unfair decision-making in sensitive domains. While the \"Evaluation, Benchmarking, and Reproducibility\" subgroup has focused on hyperparameter effects \\cite{lloyd2022} and evaluation biases \\cite{rossi2020}, there is a growing imperative to explicitly detect, measure, and mitigate these semantic biases within the embedding space.\n\nThe responsible use of KGE in sensitive applications is another paramount ethical concern. As KGE models are increasingly deployed in high-stakes domains like healthcare (e.g., drug repurposing for COVID-19 \\cite{islam2023}), finance, and legal systems, the consequences of erroneous or biased predictions can be severe. The \"Domain-Specific Application and Explainability\" subgroup underscores the need for rigorous, domain-specific validation and explainability in such contexts. For instance, in drug repurposing, molecular evaluation is integrated to verify predictions \\cite{islam2023}. This highlights a critical shift from solely optimizing for accuracy on standard benchmarks to ensuring real-world safety, fairness, and accountability. The assumptions made during model design and the generalizability of experimental setups must be critically scrutinized, especially when findings from one domain are applied to another.\n\nFinally, the imperative for transparent and explainable AI systems is gaining traction. Complex KGE models, particularly those leveraging deep learning architectures like GNNs or Transformers, often operate as \"black boxes,\" making it difficult to understand *why* a particular prediction or recommendation was made. The \"KGE for Downstream Applications and Explainability\" subgroup directly addresses this, with models like CKGE \\cite{yang2023} and RKGE \\cite{sun2018} aiming to provide explainable recommendations through path saliency or contextualized neighbor semantics. For sensitive applications, mere performance is insufficient; users and stakeholders need to trust and verify the model's reasoning. This necessitates the development of KGE models that can provide human-understandable explanations, whether through extracting logical rules, highlighting influential paths, or visualizing attention mechanisms. The theoretical gap often lies in balancing the high expressiveness of complex models with the inherent simplicity required for genuine interpretability, presenting a continuous trade-off that future research must navigate. These emerging trends and ethical considerations collectively define the next frontier for KGE research, demanding not only technological ingenuity but also a strong commitment to societal responsibility.\n"
  },
  "subsections": {
    "Background: Knowledge Graphs": "\\subsection*{Background: Knowledge Graphs}\n\nKnowledge Graphs (KGs) represent a fundamental paradigm for organizing and representing world knowledge in a structured, machine-readable format. At their core, KGs are directed graphs composed of entities (nodes) and relations (edges), forming a collection of factual triplets in the form of (head entity, relation, tail entity) \\cite{ge2023, dai2020}. For instance, the triplet (Barack Obama, bornIn, Hawaii) explicitly states a factual relationship between two entities. This structured representation allows for explicit semantic connections, enabling machines to understand and process information in a manner closer to human cognition.\n\nThe historical trajectory of knowledge representation has seen a significant evolution, from early semantic networks and expert systems in artificial intelligence to the more formalized ontologies and the vision of the Semantic Web. These foundational efforts aimed to capture human knowledge in a symbolic form, providing a basis for logical reasoning and inference. With the advent of the internet and the explosion of digital information, the need for large-scale, interconnected knowledge bases became paramount. This led to the development of modern, expansive KGs such as Freebase (now largely integrated into Wikidata), DBpedia, and Wikidata itself \\cite{wang2014, lv2018, zhang2018}. These prominent examples serve as crucial repositories, aggregating and organizing vast amounts of world knowledge from diverse sources like Wikipedia, enabling a wide array of intelligent systems, from search engines and question-answering systems to recommender platforms \\cite{huang2019, sun2018}.\n\nDespite their immense utility and structured nature, symbolic KGs inherently face several significant challenges that limit their scalability, efficiency, and ability to handle real-world complexities. Firstly, reasoning with symbolic KGs, particularly when involving complex logical rules or multi-hop inference, can be computationally inefficient and resource-intensive, often exhibiting exponential complexity \\cite{ge2023, dai2020}. This makes real-time inference on large-scale KGs a formidable task. Secondly, KGs are almost always incomplete; real-world knowledge is vast and constantly evolving, making it practically impossible to explicitly represent every single fact. Symbolic methods struggle profoundly with this incompleteness, as they typically require explicit rules or complete data to infer missing links, leading to brittle and often inaccurate predictions in sparse environments. The \"data sparsity\" problem is a recurring theme, where many entities and relations have limited connections, hindering comprehensive analysis \\cite{ge2023, dai2020}.\n\nFurthermore, symbolic representations treat entities and relations as discrete, atomic tokens, which inherently limits their capacity to capture nuanced semantic similarities or implicit relationships. For example, while \"car\" and \"automobile\" are semantically very close, a purely symbolic KG would treat them as distinct, unrelated entities unless explicitly linked by a relation. This lack of inherent semantic fluidity makes it difficult to generalize knowledge or discover novel patterns based on underlying similarities. These limitationscomputational inefficiency, difficulty in managing growing data, challenges in handling incompleteness, and the inability to capture implicit semantic similaritiescollectively underscore the necessity for more advanced representation techniques. As highlighted in the broader context of knowledge graph embedding research, these issues motivate the fundamental shift towards embedding entities and relations into continuous, low-dimensional vector spaces, thereby transforming complex symbolic problems into more efficient vector operations and laying the \"bedrock for representing complex relational data in a machine-understandable format\" \\cite{cao2022}. This transition to embedding techniques is crucial for unlocking the full potential of KGs in modern AI applications, providing a robust and scalable foundation for knowledge inference and fusion \\cite{ge2023, dai2020}.",
    "Motivation for Knowledge Graph Embedding": "\\subsection{Motivation for Knowledge Graph Embedding}\nKnowledge Graphs (KGs) serve as powerful repositories of structured world knowledge, representing entities and their relationships in a symbolic, triple-based format (e.g., (subject, predicate, object)). While invaluable for many AI applications, traditional symbolic KGs inherently suffer from several critical limitations that impede their scalability, flexibility, and integration with modern machine learning paradigms. These limitations form the core motivation for the development of Knowledge Graph Embedding (KGE) techniques.\n\nFirstly, symbolic representations are inherently **sparse and discrete**, making it challenging to capture nuanced semantic similarities between entities and relations \\cite{dai2020, cao2022}. For instance, while a symbolic KG might state (``Paris'', ``locatedIn'', ``France'') and (``Berlin'', ``locatedIn'', ``Germany''), it struggles to infer that ``Paris'' and ``Berlin'' are both capital cities or that ``France'' and ``Germany'' are both European countries without explicit rules or additional facts. This sparsity also makes it difficult to generalize to unseen entities or relations, as there is no inherent notion of proximity or relatedness in the discrete symbolic space.\n\nSecondly, reasoning over large-scale symbolic KGs is often **computationally inefficient**. As KGs grow exponentially in size, performing complex queries or multi-hop reasoning becomes computationally expensive, often requiring graph traversal algorithms that do not scale well \\cite{dai2020}. Managing and processing vast amounts of discrete data poses significant challenges, leading to bottlenecks in real-world applications.\n\nThirdly, KGs are almost always **incomplete**, a pervasive issue that limits their utility. Many real-world facts are missing, and traditional symbolic methods struggle to infer these missing links without explicit, hand-crafted rules. This incompleteness directly impacts the performance of downstream tasks that rely on comprehensive knowledge.\n\nTo overcome these limitations, Knowledge Graph Embedding (KGE) emerged as a transformative approach, converting sparse, symbolic entities and relations into continuous, low-dimensional vector representations (embeddings) in a latent space \\cite{dai2020, cao2022}. This fundamental shift from symbolic to vector-based representation offers several profound advantages:\n\n\\begin{itemize}\n    \\item \\textbf{Scalability and Efficiency:} By representing entities and relations as dense vectors, KGE models transform complex symbolic problems into efficient vector operations, such as distance calculations or dot products. This significantly enhances computational efficiency and scalability, enabling KGs with millions of entities and relations to be processed and reasoned over more effectively. The \"Efficiency, Compression, and System Optimization\" subgroup highlights this, with works like \\cite{zhu2020} demonstrating how knowledge distillation can reduce embedding parameters by 7-15x and increase inference speed, and \\cite{wang2021} proposing lightweight frameworks for efficient storage and inference. More recently, system-level optimizations like \\cite{zheng2024} aim for general and efficient KGE learning systems, achieving significant speedups.\n    \n    \\item \\textbf{Capturing Nuanced Semantic Similarities:} In the continuous embedding space, semantically similar entities or relations are mapped to proximate vectors. This allows KGE models to inherently capture nuanced semantic relationships that are difficult to express symbolically. For instance, translational models like TransH \\cite{wang2014} and rotational models like RotatE \\cite{sun2018} learn to represent relations as transformations (translations on hyperplanes or rotations in complex space) that connect head and tail entities, thereby capturing patterns like symmetry, antisymmetry, inversion, and composition. This capability is central to the \"Core KGE Model Architectures and Expressiveness\" subgroup.\n    \n    \\item \\textbf{Handling Incompleteness and Enabling Link Prediction:} KGE models are particularly adept at addressing KG incompleteness through link prediction. By learning the underlying patterns of existing facts, they can infer the plausibility of unobserved triples. This is achieved by scoring candidate triples based on the learned embeddings, effectively transforming the problem of finding missing links into an efficient vector similarity search \\cite{rossi2020}. This capability is a cornerstone of KGE applications, as detailed in the \"Link Prediction and Knowledge Graph Completion\" section.\n    \n    \\item \\textbf{Seamless Integration with Modern Machine Learning Pipelines:} KGEs provide a powerful bridge between structured knowledge and modern machine learning (ML) techniques, especially deep learning. The continuous vector representations can be seamlessly integrated as features or pre-trained components into various ML models for tasks like natural language processing, computer vision, and recommendation systems. This allows KGs to enrich data-driven models with structured background knowledge, enhancing their performance and interpretability. The \"KGE for Downstream Applications and Explainability\" subgroup exemplifies this, with models like RKGE \\cite{sun2018} and CKGE \\cite{yang2023} leveraging embeddings for explainable recommendation, and systems like Marie and BERT \\cite{zhou2023} integrating KGEs for chemistry-specific question answering.\n    \n    \\item \\textbf{Facilitating Diverse AI Tasks:} Beyond link prediction, KGEs enable a wide array of AI tasks by converting complex symbolic problems into efficient vector operations. These include:\n    \\begin{itemize}\n        \\item \\textbf{Entity Alignment:} Identifying equivalent entities across different KGs by comparing their embeddings \\cite{sun2018, zhang2019}.\n        \\item \\textbf{Question Answering (QA):} Matching natural language questions to relevant facts in the KG by embedding both questions and KG elements into a common space \\cite{huang2019, zhou2023}.\n        \\item \\textbf{Recommendation Systems:} Modeling user-item interactions and preferences by embedding items and users within a KG context \\cite{sun2018, yang2023}.\n    \\end{itemize}\n\\end{itemize}\n\nWhile the conversion to vector space offers immense benefits, it also introduces challenges. Early KGE models, though efficient, often struggled to capture all complex relational patterns, leading to a continuous evolution towards more expressive geometric and deep learning models, as evidenced by the \"Core KGE Model Architectures\" and \"Geometric KGE\" subgroups. Furthermore, while embeddings make KGs more actionable for AI, the inherent interpretability of symbolic logic can sometimes be lost in dense vector spaces, spurring research into explainable KGEs \\cite{yang2023}. This strategic shift from explicit symbolic to implicit dense vector representations, as highlighted in the overall perspective, represents a fundamental progression in knowledge representation, making KGs more accessible, scalable, and powerful for a wide spectrum of AI applications.",
    "Scope and Structure of the Review": "\\subsection{Scope and Structure of the Review}\nThis literature review offers a comprehensive exploration of Knowledge Graph Embedding (KGE) research, meticulously tracing its evolution from foundational theoretical models to cutting-edge architectural advancements, critical practical considerations, and diverse real-world applications. The scope is designed to provide a pedagogical progression, beginning with core concepts and gradually building towards more sophisticated and specialized developments, thereby ensuring a coherent narrative that captures the field's dynamic trajectory. We aim to synthesize the vast landscape of KGE, offering a structured roadmap for understanding its complexities and future directions.\n\nThe review commences with an \\textbf{Introduction} (Section 1), which sets the stage by outlining the fundamental role of knowledge graphs in AI and elucidating the core motivations behind embedding them into continuous vector spaces. This initial section establishes why KGE has become indispensable for overcoming the limitations of symbolic representations, such as scalability and the inability to capture nuanced semantic similarities.\n\nFollowing this, \\textbf{Foundational KGE Models and Geometric Paradigms} (Section 2) delves into the bedrock of KGE research. This section examines early and influential models, primarily those based on geometric and algebraic principles. It discusses how relations are conceptualized as transformations within embedding spaces, detailing the progression from simple translational models like TransH \\cite{wang2014} and TransD \\cite{ji2015} to more complex rotational approaches such as RotatE \\cite{sun2018}. These foundational works, categorized in the thematic taxonomy as \"Core Translational Models and Their Extensions\" and \"Advanced Geometric Models,\" are critically analyzed for their ability to capture diverse relational patterns, including symmetry, antisymmetry, and composition, while balancing model capacity and computational efficiency. The limitations of earlier models in handling complex relation patterns often necessitated the development of more expressive geometric operations.\n\nThe review then transitions to \\textbf{Deep Learning Architectures for Knowledge Graph Embedding} (Section 3), reflecting a significant paradigm shift in the field. This section explores how Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformer models have been adapted to learn more expressive and context-aware representations. This progression highlights the move from predefined geometric transformations to data-driven feature extraction, enabling the capture of intricate structural patterns and non-linear relationships that were challenging for simpler models.\n\nBuilding upon these architectural advancements, \\textbf{Enriching KGE: Auxiliary Information, Rules, and Multi-modality} (Section 4) investigates methods that transcend purely structural information. This section details the integration of auxiliary data (e.g., entity types, attributes), explicit logical rules, and multi-modal information (e.g., text, images) to enhance embedding quality. This critical area addresses the inherent incompleteness and sparsity of KGs, demonstrating how external knowledge can lead to more robust, semantically rich, and interpretable embeddings.\n\nRecognizing the dynamic nature of real-world knowledge, \\textbf{Dynamic, Inductive, and Distributed KGE} (Section 5) focuses on models capable of handling temporal changes, learning embeddings for unseen entities, and operating in privacy-preserving, distributed environments. Models like HyTE \\cite{dasgupta2018} are crucial here, explicitly incorporating time to enable temporally aware inference. This section underscores the field's evolution towards more adaptable and scalable solutions, moving beyond static and centralized assumptions to meet the demands of evolving knowledge bases.\n\n\\textbf{Practical Considerations: Efficiency, Robustness, and Evaluation} (Section 6) addresses the critical challenges in deploying and evaluating KGE models. It covers strategies for improving computational efficiency, enhancing robustness against noisy data, and optimizing training processes. This section also critically examines the importance of rigorous evaluation, benchmarking, and reproducibility, drawing insights from comprehensive comparative analyses of state-of-the-art methods \\cite{rossi2020}. The discussion here highlights how experimental setups and reporting practices can significantly affect generalizability, emphasizing the need for standardized benchmarks and transparent methodologies to ensure reliable scientific progress. Survey papers like \\cite{dai2020} and \\cite{cao2022} further underscore the importance of systematic classification and comparison of KGE techniques based on their underlying representation spaces and performance across various benchmarks.\n\nThe review culminates with \\textbf{Applications and Real-World Impact of KGE} (Section 7), showcasing the diverse utility of KGE across various AI tasks. This includes core applications like link prediction and knowledge graph completion, as well as more complex tasks such as entity alignment \\cite{sun2018, zhang2019}, question answering \\cite{huang2019}, and recommender systems \\cite{sun2018}. This section demonstrates how KGE bridges the gap between structured knowledge and practical AI problems, providing tangible benefits in various domains. The inclusion of application-specific KGE frameworks from the taxonomy, such as KEQA \\cite{huang2019} and RKGE \\cite{sun2018}, illustrates the versatility and real-world impact of these embedding techniques.\n\nFinally, the \\textbf{Conclusion and Future Directions} (Section 8) synthesizes the key developments, identifies persistent open challenges, theoretical gaps, and practical limitations, and outlines emerging trends and ethical considerations. This forward-looking perspective aims to inspire new research and guide the responsible advancement of KGE technologies, providing a comprehensive roadmap for navigating the complex and rapidly evolving landscape of knowledge graph embedding research.",
    "Core Translational Models and Extensions": "\\subsection{Core Translational Models and Extensions}\nThe advent of knowledge graph embedding (KGE) marked a significant paradigm shift from purely symbolic knowledge representation to continuous vector spaces, offering enhanced efficiency and expressiveness for tasks such as link prediction and knowledge graph completion. At the forefront of this transformation were the translational models, which posited that a relation could be represented as a translation operation in an embedding space, moving a head entity vector closer to a tail entity vector. This foundational idea was first popularized by TransE, a pioneering model for its simplicity and computational efficiency \\cite{wang2014}. TransE models a triple $(h, r, t)$ by enforcing the constraint $\\mathbf{h} + \\mathbf{r} \\approx \\mathbf{t}$, where $\\mathbf{h}, \\mathbf{r}, \\mathbf{t}$ are the embeddings of the head entity, relation, and tail entity, respectively. While remarkably effective for its time, TransE exhibited limitations in handling complex relational patterns, particularly one-to-many, many-to-one, and reflexive relations, where a single relation vector could not adequately distinguish between multiple valid tail entities for a given head, or vice-versa.\n\nTo address these inherent limitations, subsequent models extended the translational paradigm by introducing more sophisticated mechanisms. TransH emerged as a notable improvement, proposing to model relations as translations on relation-specific hyperplanes rather than directly in the entity embedding space \\cite{wang2014}. Specifically, for a triple $(h, r, t)$, TransH projects the head and tail entity embeddings ($\\mathbf{h}, \\mathbf{t}$) onto a hyperplane defined by the relation $\\mathbf{r}$'s normal vector $\\mathbf{w}_r$, resulting in projected entities $\\mathbf{h}_{\\perp}$ and $\\mathbf{t}_{\\perp}$. The translational assumption then applies to these projected vectors: $\\mathbf{h}_{\\perp} + \\mathbf{d}_r \\approx \\mathbf{t}_{\\perp}$, where $\\mathbf{d}_r$ is the relation-specific translation vector on the hyperplane. This mechanism allows TransH to better distinguish entities involved in one-to-many or many-to-one relations, as different entity pairs can be projected onto different points on the hyperplane while sharing the same relation vector. For instance, if a person has multiple children, TransH can project the person and each child onto the 'has\\_child' hyperplane, allowing distinct representations for each child while maintaining the 'has\\_child' relation. This approach offered a crucial trade-off, significantly improving expressiveness for complex relation types with almost the same model complexity as TransE, thereby maintaining scalability \\cite{wang2014}. The recent review by \\cite{asmara2023} further underscores TransH's importance in addressing these early challenges.\n\nBuilding upon the concept of relation-specific transformations, TransD further refined the translational approach by introducing dynamic mapping matrices for entities and relations \\cite{ji2015}. Unlike TransH, which uses a single hyperplane per relation, TransD employs two vectors for each entity and relation: one representing its meaning and another for constructing a dynamic mapping matrix. This allows for more fine-grained, entity-specific projections, where the projection matrix for a relation is dynamically constructed based on both the entity and relation vectors. The core idea is that different entities might interact with a relation in different ways, and a static projection (as in TransH) might not capture this diversity. TransD's dynamic mapping matrices provide a more adaptive mechanism to project entities into relation-specific spaces, thereby accounting for the diversity of both relations and entities. A significant advantage of TransD over its predecessors like TransR/CTransR (which used static, larger projection matrices) is its reduced parameter count and avoidance of computationally intensive matrix-vector multiplication operations, making it more scalable for large knowledge graphs \\cite{ji2015}. This efficiency gain, while increasing expressiveness, was a critical step in making KGE models practical for real-world applications.\n\nThese core translational models and their extensions collectively established a fundamental paradigm for KGE. They demonstrated that representing symbolic knowledge in continuous vector spaces could not only be efficient but also expressive enough to capture intricate relational semantics. While TransH and TransD significantly improved upon TransE's ability to model one-to-many/many-to-one relations, they still operated within the limitations of a Euclidean embedding space and relatively simple geometric transformations. This inherent simplicity, while beneficial for efficiency, meant they struggled with more complex logical patterns such as symmetry, antisymmetry, inversion, and composition, which later models like RotatE would address more elegantly through rotational transformations in complex spaces \\cite{sun2018}. Nevertheless, the foundational work of TransE, TransH, and TransD laid the essential groundwork, proving the viability of the embedding approach and setting the stage for the diverse array of KGE models that continue to influence modern research, as highlighted in various surveys \\cite{dai2020, cao2022}. Their emphasis on balancing model capacity with computational efficiency remains a crucial design principle in the field.",
    "Rotational and Complex Space Embeddings": "\\subsection{Rotational and Complex Space Embeddings}\nWhile foundational translational models like TransE and TransH \\cite{wang2014} offered a significant step forward in knowledge graph embedding (KGE) by modeling relations as vector translations, they often struggled with capturing the full spectrum of complex relational semantics, such as symmetry, antisymmetry, inversion, and composition \\cite{rossi2020}. This limitation spurred the development of models that leverage rotations in complex or higher-dimensional spaces, offering more nuanced and powerful transformations to represent these intricate logical patterns. This represents a key methodological shift within the \"Core KGE Model Architectures and Expressiveness\" and \"Geometric and Algebraic KGE Models for Complex Relations\" subgroups, moving beyond simpler linear operations to richer algebraic structures.\n\nA seminal contribution in this direction is RotatE \\cite{sun2018}, which defines each relation as a rotation from the head entity to the tail entity in a complex vector space. By representing entities as vectors and relations as Hadamard products with complex-valued relation vectors (which correspond to rotations), RotatE inherently captures symmetry (rotation by $\\pi$), antisymmetry (rotation by non-$\\pi$ angles), inversion (rotation by negative angle), and composition (sequential rotations). This elegant formulation proved highly effective for modeling complex patterns and significantly outperformed existing state-of-the-art models for link prediction on benchmark datasets \\cite{sun2018}. The success of RotatE highlighted the expressive power of complex space embeddings, demonstrating how algebraic structures could directly encode logical properties.\n\nBuilding upon this rotational paradigm, researchers explored extensions to higher-dimensional Euclidean and non-Euclidean spaces. Rotate3D \\cite{gao2020} extends the concept of relations as rotations to a three-dimensional Euclidean space. A key motivation for Rotate3D was to capture non-commutative composition patterns, which are essential for multi-hop reasoning and are naturally supported by rotations in 3D space. While RotatE primarily operates in a 2D complex plane for each dimension, Rotate3D generalizes this to a full 3D rotation, allowing for a richer set of transformations. Similarly, Orthogonal Relation Transforms \\cite{tang2019} further generalize this idea by employing high-dimensional orthogonal transforms, which encompass rotations and reflections, to model relations. This approach aims to retain the benefits of rotational models (symmetry, inversion, composition) while enhancing modeling capacity for complex relations like N-to-1 and 1-to-N by integrating graph context.\n\nThe pursuit of even more expressive geometric transformations led to models like HousE \\cite{li2022}, which introduces Householder parameterization. Householder transformations, a type of reflection, can be generalized to represent rotations and projections in high-dimensional spaces. HousE aims to simultaneously capture crucial relation patterns and mapping properties, theoretically generalizing existing rotation-based models while extending rotations to higher dimensions. This exemplifies the continuous effort to find more powerful mathematical tools to encode relational semantics.\n\nFurther enhancing the complexity of transformations, CompoundE \\cite{ge2022} and its 3D extension, CompoundE3D \\cite{ge2023}, propose using compound geometric operations, including translation, rotation, and scaling. These models treat relations not as a single operation but as a cascade of multiple transformations, suggesting that a richer set of combined operations can lead to better modeling capacity. CompoundE, by framing itself within group theory, demonstrates that several existing KGE models are special cases of its generalized framework, highlighting a trend towards unifying diverse geometric approaches. While these compound operations offer increased expressiveness, they also introduce greater model complexity and potentially higher computational costs, a common trade-off in KGE research \\cite{cao2022}.\n\nA significant recent development is the use of quaternions, an extension of complex numbers to four dimensions, to represent relations. ConQuatE \\cite{chen2025} leverages quaternion rotations to address the challenge of polysemy in knowledge graphs, where entities can exhibit different semantic characteristics depending on the relation. By incorporating contextual cues from various connected relations through efficient vector transformations in quaternion space, ConQuatE aims to capture diverse relational contexts without requiring extra information beyond original triples. This approach offers a novel way to handle the nuanced semantic variations that simpler rotational models might overlook, particularly for link prediction and multihop reasoning.\n\nThe \"Geometric and Algebraic KGE Models for Complex Relations\" subgroup analysis highlights that while these models achieve state-of-the-art performance, a common limitation is the potential for increased computational cost and parameter count, which can affect scalability to extremely large KGs. Moreover, the empirical validation often relies heavily on standard link prediction metrics, which may not fully capture the nuances of all the complex patterns these models aim to capture, especially for tasks like set retrieval or complex logical reasoning. For instance, MQuinE \\cite{liu2024} identifies and addresses a theoretical deficiency, termed the \"Z-paradox,\" in some popular KGE models, demonstrating that even advanced models can suffer from subtle expressiveness issues that degrade performance on challenging test samples. This underscores that merely introducing complex operations is insufficient; theoretical soundness and complete expressiveness are paramount.\n\nThe intellectual trajectory in this area, as noted in the \"Core KGE Model Architectures and Expressiveness\" subgroup, shows a clear progression from specific rotational models to more theoretically grounded and generalized orthogonal transformations. HolmE \\cite{zheng2024} introduces a KGE model whose relation embedding space is \"closed under composition,\" a crucial property for inherently modeling under-represented (long-tail) composition patterns and extrapolating to unseen relations. This addresses a theoretical gap where prior KGEs often considered relations compositional only if well-represented in training data. Similarly, GoldE \\cite{li2024} proposes a universal orthogonal parameterization based on a generalized Householder reflection, aiming to unify dimensional extension and geometric unification with theoretical guarantees, thereby capturing both logical patterns and topological heterogeneity. SpherE \\cite{li2024} further extends rotational embeddings by representing entities as spheres instead of vectors, specifically targeting the challenging problem of set retrieval and many-to-many relations, while maintaining interpretability. These advancements demonstrate a continuous drive to enhance the fundamental expressiveness of KGE models, ensuring they can effectively handle the complexities and imperfections of real-world knowledge graphs.",
    "Other Geometric and Algebraic Innovations": "\\subsection*{Other Geometric and Algebraic Innovations}\nBeyond the foundational translational and rotational paradigms, Knowledge Graph Embedding (KGE) research has continuously sought to refine its mathematical underpinnings by exploring a broader spectrum of geometric spaces and algebraic transformations. This pursuit is driven by the need for more expressive, theoretically sound, and robust representations capable of capturing the intricate nuances of real-world knowledge graphs.\n\nOne significant direction involves embedding entities and relations within non-Euclidean spaces, particularly Lie groups, to circumvent inherent limitations of standard vector spaces. \\cite{ebisu2017} introduced \\textbf{TorusE}, a pioneering model that embeds entities on a Lie group, specifically a torus. The primary motivation for TorusE was to address the regularization problems encountered by models like TransE, where forcing entity embeddings onto a sphere in Euclidean space could warp representations and adversely affect link prediction accuracy. By leveraging the compact nature of a torus, TorusE naturally avoids the need for explicit regularization, as the space itself is bounded. While innovative in its choice of embedding space, TorusE still adheres to a translation-like principle, defining relations as translations within the Lie group. However, its geometric complexity, while elegant, might not inherently capture all forms of complex relation patterns, such as compositionality, as effectively as models designed with specific algebraic operations for such patterns.\n\nAnother critical area of innovation lies in scrutinizing and redefining the metric used within the embedding space. \\cite{yang2021} presented \\textbf{CyclE}, which critically examines the implications of the widely adopted Minkowski metric in KGE. The authors argue that the choice of metric significantly influences the expressiveness of the embedding space and propose a novel \"Cycle metric\" based on the oscillation property of periodic functions. Their quantitative analysis suggests that a smaller function period in the Cycle metric leads to superior expressive ability. CyclE, by combining this new metric with popular KGE models, demonstrated enhanced performance. This work highlights a fundamental aspect of geometric KGE: the distance function itself is a crucial design choice. However, focusing solely on the metric, while foundational, may not inherently provide the rich *transformations* required to model complex logical patterns like transitivity or hierarchy without further architectural or operational enhancements.\n\nA more direct approach to enhancing modeling capacity involves introducing advanced algebraic transformations. \\cite{li2022} proposed \\textbf{HousE}, a powerful KGE framework that leverages Householder parameterization. HousE employs two types of Householder transformations: Householder rotations to achieve superior capacity for modeling relation patterns and Householder projections to handle sophisticated relation mapping properties (e.g., 1-to-N, N-to-1). Theoretically, HousE is capable of simultaneously modeling crucial relation patterns and mapping properties, and it generalizes existing rotation-based models by extending rotations to high-dimensional spaces. Empirically, HousE achieved state-of-the-art performance on several benchmarks, indicating its enhanced expressiveness compared to simpler rotation-based models like \\cite{gao2020} Rotate3D or \\cite{tang2019} Orthogonal Relation Transforms. The strength of HousE lies in its mathematically robust and versatile transformations, offering a richer set of operations than basic rotations.\n\nBuilding upon the idea of combining multiple operations, \\cite{ge2022} introduced \\textbf{CompoundE}, which integrates translation, rotation, and scaling operations into a cascaded compound transformation. This model views relations as complex geometric manipulations, demonstrating that a synergy of these operations can lead to highly expressive embeddings. Further extending this concept, \\cite{ge2023} developed \\textbf{CompoundE3D}, which leverages 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear. CompoundE3D offers multiple design variants, allowing for flexibility to match the rich underlying characteristics of diverse knowledge graphs. These compound operation models represent a significant evolutionary step, as they generalize many existing scoring-function-based KGE models as special cases, effectively encompassing the strengths of both translational and rotational approaches while adding further dimensions of transformation. This contrasts with models like \\cite{yang2019} TransMS, which focuses on multidirectional semantics within a translation framework, or \\cite{peng2020} LineaRE, which models relations as simple linear functions.\n\nOther notable algebraic innovations include \\cite{song2021} \\textbf{Rot-Pro}, which combines projection and relational rotation to specifically model transitivity, a common but challenging relation pattern. \\cite{zhang2022} \\textbf{TranS} introduces synthetic relation representations within transition-based frameworks to better handle complex scenarios where the same entity pair might have different relations. More recently, \\cite{liu2024} proposed \\textbf{MQuinE}, which directly addresses and cures a theoretical deficiency termed the \"Z-paradox\" in some popular KGE models, thereby ensuring stronger expressiveness and theoretical soundness. The use of advanced algebraic structures is further exemplified by \\cite{chen2025} \\textbf{ConQuatE}, which leverages quaternion rotations to capture diverse relational contexts and address the polysemy issue, where entities exhibit different semantic characteristics depending on the relation.\n\nCollectively, these innovations highlight a continuous intellectual trajectory in KGE research: moving from simpler, single-operation models to more complex, multi-operation, multi-dimensional, and non-Euclidean spaces. This evolution is driven by the relentless quest to capture increasingly complex and nuanced relational patterns, thereby enhancing model expressiveness and theoretical rigor. While these models achieve state-of-the-art performance on various benchmarks, they often introduce increased mathematical complexity and computational costs, which can impact scalability, especially for extremely large knowledge graphs. Furthermore, the theoretical elegance of these geometric and algebraic models, while appealing, sometimes comes at the cost of interpretability, making it challenging to fully understand *why* certain transformations are optimal for specific relation patterns. The choice of the \"best\" geometric space or transformation remains highly dependent on the specific characteristics of the knowledge graph and the types of relations it contains.",
    "Convolutional Neural Networks (CNNs) for KGE": "\\subsection{Convolutional Neural Networks (CNNs) for KGE}\nConvolutional Neural Networks (CNNs) have emerged as a powerful paradigm in Knowledge Graph Embedding (KGE), offering a distinct advantage over traditional geometric models by automatically extracting local features and modeling intricate, non-linear interactions between entity and relation embeddings. Unlike models that rely on predefined geometric transformations, CNNs learn complex patterns directly from the data, enabling them to capture nuanced relational semantics that are often challenging for simpler approaches \\cite{cao2022}. This shift represents a significant evolution in KGE, moving towards more expressive and data-driven architectures.\n\nEarly applications of CNNs in KGE, such as AcrE \\cite{ren2020} and M-DCN \\cite{zhang2020}, demonstrated their capability to enhance link prediction. AcrE, for instance, introduced atrous convolutions and residual learning to effectively increase feature interactions while maintaining a simpler structure and higher parameter efficiency. This approach addressed the limitation of conventional models in capturing diverse relation patterns by allowing for a broader receptive field without increasing the number of parameters. M-DCN further advanced this by proposing a multi-scale dynamic convolutional network, utilizing dynamic filters to extract richer and more expressive feature embeddings. M-DCN was particularly designed to handle complex relation patterns like 1-to-N, N-to-1, and N-to-N, which often pose significant challenges for translation-based or simple semantic matching models \\cite{ge2023}. The dynamic nature of its filters, tailored to each relation, allowed for a more adaptive modeling of these complex interactions.\n\nThe integration of attention mechanisms further refined CNN-based KGE models. ReInceptionE \\cite{xie2020} exemplified this by combining an Inception network with a relation-aware attention mechanism. The Inception network was employed to increase interactions between head and relation embeddings, while the attention mechanism enriched these embeddings with joint local and global structural information. This allowed ReInceptionE to adaptively utilize neighborhood context, a capability that purely convolutional models might partially miss, thereby bridging the gap between local feature extraction and broader graph topology awareness. This approach highlights an evolutionary trend within the \"Deep Learning Architectures for KGE\" subgroup, where models increasingly seek to combine the strengths of different neural components to capture a more comprehensive view of the knowledge graph.\n\nMore recent works continue to refine CNN-based techniques for KGE. CNN-ECFA \\cite{hu2024} introduced a Convolutional Neural Network-based Entity-specific Common Feature Aggregation strategy, aiming to improve knowledge graph representation learning by leveraging common features that are specific to entities. This model demonstrates that by aggregating entity-specific features, CNNs can learn more effective representations, outperforming state-of-the-art feature projection strategies. Similarly, SEConv \\cite{yang2025} proposed a semantic-enhanced KGE model, incorporating a less resource-consuming self-attention mechanism alongside a multi-layer CNN. The multi-layer CNN in SEConv is specifically designed to learn deeper structural features from triplets, while self-attention generates more expressive embedding representations. This model, with its application focus on healthcare prediction, underscores the practical utility of CNNs in learning discriminative features for specialized domains.\n\nA key strength of CNN-based KGE models lies in their ability to automatically discover intricate, non-linear feature interactions, which contrasts with the hand-crafted transformations of geometric models (e.g., TransD \\cite{ji2015} or TorusE \\cite{ebisu2017}). This automatic feature learning often leads to superior performance in link prediction tasks, achieving state-of-the-art results on various benchmarks. However, this expressiveness comes with trade-offs. CNN models typically involve higher computational complexity and a larger number of parameters compared to simpler geometric models, potentially impacting scalability for extremely large knowledge graphs. Furthermore, while they excel at capturing local patterns, their ability to model long-range dependencies or global graph structures might be less direct than Graph Neural Networks (GNNs) or Transformer-based models, which are inherently designed for such tasks. The development trajectory of CNNs in KGE shows a clear progression from basic convolutional operations to more sophisticated designs incorporating multi-scale processing, dynamic filters, and attention, continually pushing the boundaries of what can be learned from entity-relation interactions.",
    "Graph Neural Networks (GNNs) and Attention Mechanisms": "\\subsection{Graph Neural Networks (GNNs) and Attention Mechanisms}\n\nThe integration of Graph Neural Networks (GNNs) and attention mechanisms represents a significant advancement in Knowledge Graph Embedding (KGE), moving beyond simple triplet-based interactions to leverage the rich topological and relational context of knowledge graphs. GNNs, through their inherent message passing and aggregation mechanisms, are uniquely suited to capture structural information and neighborhood context, which is crucial for understanding complex relational patterns and inferring missing links. This paradigm shift enables KGE models to learn richer, context-dependent embeddings by explicitly modeling multi-hop relational paths and local graph structures.\n\nEarly explorations into inductive capabilities for KGE, a key advantage of GNNs, were demonstrated by models like Logic Attention-based Neighborhood Aggregation (LAN) \\cite{wang2018}. LAN addressed the challenges of unordered and unequal neighbors by introducing a novel aggregator that uses both rule- and network-based attention weights. This allowed for the inductive embedding of new entities by aggregating information from their existing neighbors, a crucial step towards handling the dynamic nature of real-world knowledge graphs where new entities frequently emerge. However, while LAN provided a foundational approach to inductive learning, its attention mechanism was relatively simple and might not fully capture the nuanced importance of different relational paths.\n\nBuilding upon the strengths of GNNs, Graph Attenuated Attention Networks (GAATs) \\cite{wang2020} further refined the use of attention. GAATs incorporated an attenuated attention mechanism to assign varying weights to different relation paths within the knowledge graph, thereby acquiring more informative features from neighbor nodes. This approach recognized that not all paths or neighbors contribute equally to an entity's representation, and by attenuating less relevant information, GAATs could learn more discriminative embeddings. This marked an improvement over uniform aggregation strategies, allowing entities and relations to be learned within any neighborhood context, enriching the feature extraction process.\n\nA more sophisticated approach to GNN-based KGE is seen in DisenKGAT \\cite{wu2021}, which introduced a novel Disentangled Graph Attention Network. DisenKGAT leverages both micro-disentanglement and macro-disentanglement to learn diverse and independent component representations. Micro-disentanglement is achieved through a relation-aware aggregation mechanism that generates varied component representations, while macro-disentanglement uses mutual information as a regularization to enhance the independence of these components. This disentangled approach allows the model to generate adaptive representations based on the given scenario, thereby capturing more diverse and nuanced semantics behind complex relations. DisenKGAT's ability to produce adaptive and explainable representations showcases a significant strength, addressing the limitation of single, static representations in traditional KGE models.\n\nThe ongoing research in GNNs for KGE also includes efforts to optimize their design and understand their generalization capabilities. For instance, \\cite{di2023} proposed a Message Function Search for KGE, aiming to automatically discover suitable GNN message functions for various KG forms (e.g., n-ary, hyper-relational data). This highlights a meta-level approach to GNN design, seeking to overcome the limitations of fixed GNN architectures by adapting them to specific data characteristics. Similarly, \\cite{li2021} delved into understanding *how* KGE models, particularly GNN-based ones, extrapolate to unseen data, proposing \"Semantic Evidences\" and introducing SE-GNN to explicitly model and merge these evidences for improved inductive capabilities.\n\nWhile GNNs and attention mechanisms significantly enhance KGE by capturing complex structural and contextual information, they are not without limitations. A primary concern is their computational complexity and scalability, especially for very large knowledge graphs, as message passing can become resource-intensive. Furthermore, deep GNNs can suffer from over-smoothing, where entity representations become indistinguishable after many layers of aggregation, diminishing their discriminative power. The effectiveness of these models also heavily relies on the quality and density of local neighborhood information; sparse neighborhoods can limit their ability to learn rich contextual embeddings. Despite these challenges, the continuous development of more efficient GNN architectures, such as those explored in message function search \\cite{di2023}, and a deeper understanding of their generalization properties \\cite{li2021}, indicates a strong future for GNNs and attention mechanisms in KGE, pushing towards more robust and context-aware knowledge representation.",
    "Transformer-based KGE Models": "\\subsection*{Transformer-based KGE Models}\nThe emergence of Transformer architectures, initially lauded for their unparalleled success in natural language processing, has profoundly influenced the landscape of knowledge graph embedding (KGE). These models, characterized by their self-attention mechanisms, offer a powerful paradigm for capturing long-range dependencies and generating highly contextualized representations, capabilities that were often limited in earlier KGE approaches. The adaptation of Transformers to knowledge graphs represents a significant methodological evolution within the \"Deep Learning Architectures for KGE\" subgroup, pushing the boundaries of expressiveness beyond traditional geometric or simpler neural network models.\n\nEarly adaptations of Transformers to KGE primarily treated knowledge graphs as sequences of entities and relations. A pioneering example is CoKE (Contextualized Knowledge Graph Embedding) \\cite{wang2019}, which frames edges and paths within a KG as sequences. By feeding these sequences into a Transformer encoder, CoKE learns dynamic, flexible, and fully contextualized embeddings for entities and relations. This approach marked a critical shift from static, context-independent embeddings, allowing the model to capture varying properties of entities and relations based on their surrounding graph context. While CoKE demonstrated superior performance in link prediction and path query answering, its sequence-centric view inherently grappled with the graph's non-sequential nature, potentially overlooking intricate topological structures that are not easily linearized. This limitation highlights a theoretical gap: how to reconcile the order-invariance of self-attention with the directed, ordered nature of relational facts in KGs.\n\nSubsequent research has focused on explicitly integrating graph structures into Transformer frameworks, moving beyond simple sequence linearization. Knowformer \\cite{li2023} directly addresses the challenge of order invariance inherent in the self-attention mechanism, which struggles to distinguish between a valid triplet (subject, relation, object) and its shuffled variants. Knowformer innovatively incorporates relational compositions into entity representations, explicitly injecting semantics and capturing the role of an entity based on its position (subject or object) within a relation triplet. This design choice allows the Transformer to correctly capture relational semantics by distinguishing entity roles, a crucial advancement for modeling complex relational patterns that simpler translation-based models like TransD \\cite{ji2015} or even rotational models like RotatE \\cite{sun2018} might struggle to fully express without explicit positional encoding. Knowformer's ability to integrate positional awareness and relational semantics directly into the self-attention mechanism significantly enhances its expressiveness, particularly for modeling complex contextual information.\n\nThe latest advancements, such as TGformer \\cite{shi2025}, further refine the integration of Transformer architectures with graph structures, proposing a general graph Transformer framework for KGE. TGformer is notable for being the first to explicitly leverage a graph Transformer to build knowledge embeddings that incorporate both triplet-level and graph-level structural features. This comprehensive approach addresses a critical limitation of previous methods: triplet-based models often ignore the broader graph structure, while some graph-based methods (e.g., certain GNNs like DisenKGAT \\cite{wu2021}) might overlook the specific contextual information of individual nodes within a triplet. By constructing context-level subgraphs for each predicted triplet and employing a Knowledge Graph Transformer Network (KGTN), TGformer fully explores multi-structural features, boosting the model's understanding of entities and relations in diverse contexts. Furthermore, TGformer extends its capabilities to temporal knowledge graphs, a significant step towards handling the dynamic nature of real-world knowledge, aligning with the broader development direction of building \"more powerful and comprehensive models that leverage advanced neural architectures to capture increasingly complex structural and contextual information, including temporal dynamics.\"\n\nThe primary strength of Transformer-based KGE models lies in their ability to capture global and local semantic relationships through self-attention, leading to highly contextualized representations. This contrasts with CNN-based KGE models like AcrE \\cite{ren2020} or ReInceptionE \\cite{xie2020}, which excel at extracting local features and interactions but may require additional mechanisms to capture long-range dependencies effectively. While CNN-ECFA \\cite{hu2024} and SEConv \\cite{yang2025} demonstrate the continued refinement of CNNs for feature aggregation, Transformers offer a more inherent capability for global context. Compared to foundational models like TorusE \\cite{ebisu2017} or CyclE \\cite{yang2021} that focus on refining the embedding space's geometry, Transformers provide a data-driven approach to learn complex, non-linear transformations and interactions, often achieving state-of-the-art performance in link prediction.\n\nHowever, Transformer-based models also present trade-offs. Their computational complexity and high parameter count can pose scalability challenges for extremely large knowledge graphs, a practical constraint that needs careful consideration. While models like Knowformer address the initial order-invariance issue, the fundamental assumption of treating graph elements as sequences, even with sophisticated positional encodings, can sometimes be an oversimplification of the rich, multi-relational graph topology. The experimental setups for these models typically involve standard benchmark datasets, and while results are often superior, the generalizability to highly sparse or domain-specific KGs with limited data might still be a concern, requiring extensive pre-training or specialized fine-tuning. Despite these challenges, the innovative application of Transformers to graph structures, particularly in integrating multi-structural features and handling temporal dynamics, signifies a robust and adaptive direction for KGE research, continually pushing the state-of-the-art in capturing the intricate semantics of knowledge graphs.",
    "Incorporating Auxiliary Information (Types, Attributes)": "\\subsection{Incorporating Auxiliary Information (Types, Attributes)}\nThe effectiveness of Knowledge Graph Embedding (KGE) models, while primarily driven by structural information, can be significantly enhanced by integrating auxiliary semantic information such as entity types and attributes. This approach moves beyond the simplistic triplet structure, grounding embeddings in a richer context to yield more semantic, discriminative, and robust representations, particularly crucial when dealing with incomplete or noisy knowledge graphs (KGs). The intellectual trajectory in this area reflects a growing recognition that external, well-structured knowledge can bridge gaps that purely structural models cannot, contributing to the development of more inherently capable KGE models.\n\nEarly efforts to incorporate auxiliary information often focused on entity types. \\cite{wang2021} proposed \\textit{TransET}, a novel KGE model that leverages entity types to learn more semantic features. By utilizing circle convolution based on entity and entity type embeddings, TransET maps head and tail entities to type-specific representations, which are then used in a translation-based scoring function. This method demonstrated that even a relatively straightforward integration of type information could lead to improved performance in link prediction and triple classification tasks. Building on this, \\cite{he2023} introduced \\textit{TaKE}, a more universal \\textit{Type-augmented Knowledge graph Embedding framework}. TaKE distinguishes itself by automatically capturing type features without explicit supervision and learning relation-specific type representations, allowing for a nuanced understanding of how entity types interact with different relations. Furthermore, TaKE incorporates a type-constrained negative sampling strategy, which is critical for constructing more effective negative samples during training, a fundamental aspect for KGE robustness \\cite{sachan2020}. While TransET provided a specific model, TaKE offers a generalizable framework that can enhance various traditional KGE models, showcasing a methodological evolution towards broader applicability.\n\nThe utility of type information extends to domain-specific applications. \\cite{hu2024} presented \\textit{SR-KGE}, a \\textit{GeoEntity-type constrained knowledge graph embedding} framework designed for predicting natural-language spatial relations. This approach integrates geoentity types as a constraint, combining graph structures with semantic attributes to capture spatial and semantic relations more accurately. While TaKE provides a universal type integration, SR-KGE exemplifies how tailored auxiliary information, when applied to a specific domain, can yield superior results for specialized tasks. The strength of these type-augmented methods lies in their ability to provide semantic guidance, making embeddings more discriminative by enforcing type consistency and enriching the relational context. However, a common limitation is their reliance on the availability and quality of type information; if types are sparse, noisy, or inconsistently defined, the benefits may diminish, and the complexity of integrating diverse type hierarchies can be substantial.\n\nBeyond explicit types, entity attributes offer a richer, instance-specific form of auxiliary information. \\cite{zhang2024} addressed the critical problem of erroneous triples in KGs by proposing \\textit{AEKE}, a framework for \\textit{Attributed Error-aware Knowledge Embedding}. AEKE leverages entity attributes to guide the KGE model in learning against the impact of erroneous triples. It designs triple-level hypergraphs to model both KG topological structures and attribute structures, jointly calculating confidence scores for each triple based on self-contradiction, structural consistency, and attribute homogeneity. These confidence scores then adaptively weigh contributions during multi-view graph learning and margin loss calculation, ensuring that potentially erroneous triples have minimal impact. AEKE represents a significant step towards enhancing KGE robustness, moving beyond merely completing KGs to making them more reliable. While type-based methods provide broad semantic categories, attribute-based approaches like AEKE offer fine-grained details that can be crucial for identifying and mitigating data quality issues.\n\nThe scope of auxiliary information also extends to hyper-relational facts, moving \"beyond triplets\" to capture richer contextual data. \\cite{rosso2020} introduced \\textit{HINGE}, a \\textit{hyper-relational Knowledge Graph Embedding model} that directly learns from hyper-relational facts, where each fact includes a base triplet (\\textit{h, r, t}) and associated key-value pairs (\\textit{k, v}). HINGE captures not only the primary structural information of the KG but also the correlation between each triplet and its associated key-value pairs. This is a crucial distinction from type or attribute integration, as HINGE directly models additional structured facts that are part of the knowledge base, rather than meta-information about entities. This allows for a more comprehensive understanding of complex data semantics, outperforming models that rely solely on triplets or transform hyper-relational facts into less structured n-ary representations.\n\nIn synthesis, the integration of auxiliary information, whether through entity types \\cite{wang2021, he2023, hu2024}, attributes \\cite{zhang2024}, or hyper-relational facts \\cite{rosso2020}, represents a vital direction in KGE research. These approaches collectively address the limitations of purely structural KGEs by providing richer semantic context, making embeddings more discriminative and robust to noise and incompleteness. The evolution from specific type integration (TransET) to universal frameworks (TaKE) and domain-specific applications (SR-KGE) highlights a growing sophistication. Furthermore, the focus on error-aware learning through attributes (AEKE) and the direct modeling of hyper-relational facts (HINGE) underscore the field's commitment to developing KGE models that can handle the complexities and imperfections of real-world knowledge graphs. A key trade-off, however, is the increased reliance on the availability and quality of this auxiliary data, which may not always be consistent across diverse KGs. Nevertheless, these advancements are crucial for pushing KGE towards greater practical utility and theoretical soundness, enabling more intelligent and reliable AI applications.",
    "Rule-based and Constraint-driven KGE": "\\subsection{Rule-based and Constraint-driven KGE}\nWhile purely data-driven knowledge graph embedding (KGE) models excel at capturing statistical patterns from observed triples, they often struggle with ensuring logical consistency, facilitating explicit reasoning, and providing interpretability aligned with human understanding. This subsection delves into approaches that integrate logical rules and explicit constraints directly into the KGE learning process, thereby injecting prior knowledge to address these limitations.\n\nOne foundational approach to injecting semantic consistency is exemplified by \"Semantically Smooth Knowledge Graph Embedding\" (SSE) \\cite{guo2015}. This method enforces a \"semantically smooth\" embedding space, where entities belonging to the same semantic category are encouraged to lie close to each other. By employing manifold learning techniques, such as Laplacian Eigenmaps and Locally Linear Embedding, as regularization terms, SSE guides the embedding process to discover intrinsic geometric structures that reflect categorical semantics. While effective in promoting semantic coherence, SSE primarily relies on entity categories as a form of soft constraint, which is less explicit than logical rules and might not directly enhance complex reasoning capabilities. Its strength lies in its generality, as the smoothness assumption can be applied to various embedding models and constructed from diverse information beyond just entity categories.\n\nA significant advancement in this domain is the explicit incorporation of logical rules. Early rule-based methods often relied on hard rules, which are rigid and require extensive manual curation. However, real-world knowledge graphs are often noisy and incomplete, making hard rules brittle. \"Knowledge Graph Embedding with Iterative Guidance from Soft Rules\" (RUGE) \\cite{guo2017} introduced a novel paradigm to iteratively integrate soft rules (rules associated with confidence levels) into the embedding learning process. RUGE simultaneously learns from observed triples, unlabeled triples (whose labels are iteratively predicted), and automatically extracted soft rules. This iterative guidance allows the knowledge embodied in rules to be progressively transferred into the learned embeddings, leading to more robust representations. The key advantage of RUGE is its ability to leverage abundant, albeit uncertain, automatically extracted rules, moving beyond the limitations of manually curated hard rules. This iterative feedback loop between rule inference and embedding updates represents a crucial step towards deeply intertwining symbolic logic with subsymbolic representations.\n\nComplementing complex rule integration, simpler structural constraints can also significantly enhance KGE models. \"Improving Knowledge Graph Embedding Using Simple Constraints\" \\cite{ding2018} demonstrated that even straightforward constraints can yield substantial improvements in interpretability and structure without adding significant computational overhead. Specifically, this work explored non-negativity constraints on entity representations, which help learn compact and interpretable features, and approximate entailment constraints on relation representations. These entailment constraints encode regularities of logical entailment between relations, structuring the embedding space to reflect hierarchical or inferential relationships. While these constraints are less expressive than full first-order logic rules, their simplicity makes them highly efficient and broadly applicable, offering a practical trade-off between model complexity and the benefits of injected prior knowledge.\n\nMore recent contributions have further refined the integration of soft rules. \"Knowledge Graph Embedding Preserving Soft Logical Regularity\" \\cite{guo2020} focused on imposing soft rule constraints directly on relation representations. By representing relations as bilinear forms and mapping entity representations into a non-negative and bounded space, the method derives a rule-based regularization that enforces relation representations to satisfy rule-introduced constraints. A notable strength of this approach is its improved scalability, as the complexity of rule learning becomes independent of the entity set size, making it more feasible for large-scale KGs. This direct regularization of relations ensures that logical patterns are preserved in the relational space, which is crucial for consistent reasoning.\n\nBuilding on these foundations, RulE (\"Rule Embedding\") \\cite{tang2022} presents a principled framework that learns rule embeddings jointly with entity and relation embeddings within a unified vector space. Unlike previous methods that might treat rules as external regularization, RulE explicitly represents rules as vectors, allowing for soft logical inference directly within the embedding space. This deep integration enables rule embeddings to regularize and enrich entity/relation embeddings, leading to more coherent and reasoning-capable representations. RulE's ability to calculate a confidence score for each rule based on its consistency with observed triples further refines the \"softness\" of logical inference, alleviating the brittleness often associated with strict logic. This joint learning paradigm represents a sophisticated approach to intertwining symbolic knowledge with subsymbolic representations, pushing the boundaries of reasoning capabilities within KGEs.\n\nCollectively, these rule-based and constraint-driven methods highlight a critical evolution in KGE research. They move beyond purely data-driven models (like those in the \"Core KGE Model Architectures\" subgroup) by leveraging explicit logical knowledge to enhance robustness, improve reasoning, and increase interpretability. The progression from general semantic smoothness \\cite{guo2015} to iterative soft rule guidance \\cite{guo2017}, the application of simple yet effective structural constraints \\cite{ding2018}, and finally to the joint embedding of rules themselves \\cite{tang2022} demonstrates a continuous effort to bridge the gap between symbolic logic and continuous vector spaces.\n\nHowever, several challenges persist. A primary limitation across many rule-based approaches is the reliance on the availability and quality of logical rules. While methods like RUGE can leverage automatically extracted soft rules, the efficiency and accuracy of such rule extraction remain a practical hurdle. Furthermore, balancing the strict adherence to logical rules with the flexibility to capture exceptions or nuanced patterns not covered by rules is a delicate trade-off. Over-constraining the embedding space with rules might inadvertently reduce its expressiveness or ability to generalize to unseen, complex scenarios. The scalability of managing and applying very large and complex rule sets, particularly for higher-order logic, also poses a significant challenge, despite efforts like \\cite{guo2020} to optimize rule learning complexity. Future research needs to explore more robust and automated rule discovery mechanisms, develop adaptive frameworks that can dynamically weigh rule adherence against data-driven insights, and investigate more principled theoretical foundations for combining probabilistic logic with continuous embeddings.",
    "Multi-modal and Cross-domain KGE": "\\subsection{Multi-modal and Cross-domain KGE}\nThe limitations of relying solely on structural information in knowledge graphs, particularly data sparsity and the inability to capture rich semantic nuances, have driven significant research into multi-modal and cross-domain Knowledge Graph Embedding (KGE). These approaches integrate diverse information sources, such as textual descriptions, visual features, or data from multiple domains, to enrich representations and enable more comprehensive knowledge understanding. The core motivation is to leverage complementary information, thereby enhancing the expressiveness and robustness of KGE models.\n\nEarly efforts in multi-modal KGE primarily focused on integrating textual descriptions to augment structural embeddings. For instance, the Semantic Space Projection (SSP) model \\cite{xiao2016} proposed a method that jointly learns from symbolic triples and textual descriptions. SSP projects textual information into a semantic space, using text to discover semantic relevance and provide more precise embeddings. This was a crucial step in addressing the \"weak-semantic\" nature of purely geometric models, which often struggle to differentiate entities with similar structural positions but distinct semantic meanings. While SSP offered a foundational approach, its text integration was relatively simple, often relying on bag-of-words or basic word embeddings.\n\nMore recent advancements have leveraged the power of pre-trained language models (PLMs) for deeper semantic understanding. The \"Joint Language Semantic and Structure Embedding for Knowledge Graph Completion\" model \\cite{shen2022} exemplifies this by fine-tuning PLMs with a probabilistic structured loss. This method effectively captures semantics from natural language descriptions while simultaneously reconstructing structural information, demonstrating state-of-the-art performance, particularly in low-resource settings where semantic cues are invaluable. This approach highlights a significant evolutionary trend, moving from simple text projection to sophisticated joint learning frameworks that deeply intertwine language semantics with graph structure. A key strength of such models is their ability to infer relations and entity properties even when structural data is sparse, a common challenge in many real-world KGs. However, the computational cost of fine-tuning large PLMs and the availability of high-quality textual descriptions for all entities remain practical constraints.\n\nBeyond textual data, multi-modal KGE has expanded to incorporate other modalities and domain-specific knowledge. For instance, in the biomedical domain, \"Multimodal reasoning based on knowledge graph embedding for specific diseases\" \\cite{zhu2022} constructs Specific Disease Knowledge Graphs (SDKGs) and implements multimodal reasoning using reverse-hyperplane projection. This model integrates structural, category, and description embeddings to discover new, reliable knowledge, showcasing how combining different modalities can lead to enhanced insights in specialized contexts. This demonstrates the power of multimodal integration in addressing the unique complexities and data characteristics of domain-specific KGs. However, the generalizability of such highly specialized models to other domains without significant re-engineering remains an open question. Furthermore, the challenge of designing effective negative sampling strategies becomes more pronounced in multi-modal settings, as highlighted by \\cite{zhang2023}, which proposes Modality-Aware Negative Sampling (MANS) to align structural and visual embeddings, underscoring that training optimization must adapt to the complexity of heterogeneous data.\n\nCross-domain KGE extends this concept by enabling knowledge transfer and interaction across distinct knowledge graphs or domains. This is particularly vital for applications like recommender systems, where user preferences and item characteristics often span multiple categories or platforms. \"Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation\" \\cite{liu2023} addresses the critical problems of cross-domain cold start and multi-domain recommendations. This approach proposes a \"binding rule\" to efficiently interact items across multiple domains, allowing for both homo-domain and hetero-domain item embeddings. By modeling associations and interactions between items across diverse domains, this method significantly improves multi-domain item-item recommendations, outperforming traditional recommender systems that struggle with data sparsity in new domains. The strength lies in its ability to leverage shared entities or relations to bridge information gaps, enriching representations for items even in domains with limited data. A limitation, however, is the reliance on explicit links or shared entities between domains, which may not always be readily available or accurately reflect complex cross-domain relationships.\n\nIn summary, multi-modal and cross-domain KGE represent a crucial evolutionary trajectory in knowledge representation. They move beyond the limitations of purely structural models by integrating diverse, complementary information sources. While models like SSP \\cite{xiao2016} laid the groundwork for textual integration, the field has progressed to sophisticated joint learning frameworks leveraging pre-trained language models \\cite{shen2022} and domain-specific multimodal reasoning \\cite{zhu2022}. Concurrently, cross-domain approaches \\cite{liu2023} tackle challenges like data sparsity and cold start in complex applications such as recommendation. The collective contribution of these methods is the creation of richer, more semantically grounded, and robust embeddings, which are essential for comprehensive knowledge understanding and practical applicability in diverse AI tasks. However, challenges persist in effectively fusing heterogeneous information, managing increased computational complexity, and designing robust training strategies like modality-aware negative sampling \\cite{zhang2023}.",
    "Temporal Knowledge Graph Embedding (TKGE)": "\\subsection{Temporal Knowledge Graph Embedding (TKGE)}\nThe inherent dynamism of real-world knowledge necessitates models capable of capturing the temporal evolution of facts within knowledge graphs (KGs). Traditional Knowledge Graph Embedding (KGE) models, primarily designed for static KGs, fall short in tasks requiring reasoning over time or understanding the fluidity of information. Temporal Knowledge Graph Embedding (TKGE) addresses this by explicitly integrating time into the embedding process, moving beyond static representations to model evolving entities and relations \\cite{dai2020}.\n\nEarly approaches to TKGE focused on explicitly structuring and modeling time itself. \\cite{dasgupta2018} introduced \\textit{HyTE}, a hyperplane-based method that associates each timestamp with a corresponding hyperplane in the entity-relation space. This allows for temporally guided KG inference and prediction of temporal scopes for facts, marking a significant step towards dynamic KGEs. While intuitive, HyTE's reliance on simple hyperplanes might struggle with highly complex, non-linear temporal dependencies. Another prominent method involves treating the entire fact set as a higher-order tensor, typically a fourth-order tensor (head, relation, tail, time), and applying tensor decomposition to learn dense, low-dimensional temporal embeddings \\cite{lin2020}. This provides a robust mathematical framework for integrating time as a distinct dimension. Complementing this, \\textit{ATiSE} models temporal evolution using additive time series decomposition, mapping representations into multi-dimensional Gaussian distributions where covariance captures temporal uncertainty, offering a probabilistic view of temporal dynamics \\cite{xu2019}. More recently, \\textit{TeAST} innovatively structures time by mapping relations onto an Archimedean spiral timeline, transforming the quadruple completion problem into a 3rd-order tensor completion task. This approach aims to ensure relations evolve orderly over time with a spiral regularizer, offering a degree of interpretability regarding temporal patterns \\cite{li2023}. A common limitation across these tensor and time series methods is the computational cost associated with higher-order operations or complex decompositions, especially for very dense temporal data.\n\nA significant advancement in TKGE involves leveraging geometric transformations to model temporal dynamics. \\textit{TeRo} defines the temporal evolution of entity embeddings as a rotation from an initial time to the current time in a complex vector space, representing relations for time intervals with dual complex embeddings \\cite{xu2020}. Building upon this, \\textit{ChronoR} extends the concept by employing k-dimensional rotation transformations, parametrized by relation and time, to transform a head entity to fall near its tail entity. This effectively captures rich interactions between temporal and multi-relational characteristics \\cite{sadeghian2021}. While powerful, the interpretability of complex rotations in high-dimensional spaces can be challenging, and the computational complexity associated with learning these transformations can be substantial, particularly for very large KGs or highly granular temporal data.\n\nMore advanced methods, particularly emerging in recent years, address the complexities of dynamic, spatiotemporal, and even fuzzy knowledge by moving beyond a single Euclidean space. \\textit{MADE} and \\textit{IME}, both published in 2024, represent a cutting-edge shift towards modeling TKGs in multi-curvature spaces, including Euclidean, hyperbolic, and hyperspherical geometries \\cite{wang2024, wang2024a}. The rationale is that TKGs often contain interwoven complex geometric structures (e.g., hierarchical, ring, chain) that no single curvature space can optimally capture. MADE introduces an adaptive weighting mechanism to assign different weights to these spaces in a data-driven manner, along with a quadruplet distributor and temporal regularization for timestamp smoothness \\cite{wang2024}. IME, on the other hand, incorporates \"space-shared\" properties to learn commonalities across spaces and alleviate spatial gaps, and \"space-specific\" properties to capture characteristic features, complemented by an Adjustable Multi-curvature Pooling (AMP) approach \\cite{wang2024a}. While both achieve state-of-the-art results, MADE's adaptive weighting offers a more flexible approach to handling diverse geometric structures without requiring explicit design of shared/specific properties. The primary limitation for both is the increased complexity of optimizing embeddings across multiple, potentially disparate, geometric spaces, and the computational overhead.\n\nFurther extending these geometric transformations, recent works tackle fuzzy and spatiotemporal dimensions. \\textit{FSTRE} uses projection and rotation in a complex vector space to embed spatial and temporal information, introducing fine-grained fuzziness through modal lengths of anisotropic vectors \\cite{ji2024}. This addresses the insufficiency of prior KGE models for uncertain and dynamic knowledge. Building on this, \\cite{ji2024a} leverages quaternion embeddings to jointly embed spatiotemporal entities, representing relations as rotations and exploiting the non-commutative compositional pattern of quaternions for multihop path reasoning and uncertainty modeling. This approach is particularly powerful for complex tasks like multihop querying on incomplete fuzzy spatiotemporal KGs, where previous methods overlooked uncertainty and spatiotemporal sensitivity during reasoning. These advanced models, while powerful, introduce additional complexity (fuzziness, spatiotemporal, quaternions) which can increase model intricacy and training demands.\n\nFinally, \\textit{TARGAT} offers an alternative paradigm by employing a time-aware relational graph attention model based on Graph Neural Networks (GNNs) \\cite{xie2023}. It addresses the limitation of previous GNN-based models that struggle to directly capture multi-fact interactions at different timestamps by dynamically generating time-aware relational message transformation matrices. This GNN-based approach provides a unified way to process the entire graph of multi-facts over time. However, GNNs can face scalability challenges with extremely large and dense TKGs due to the computational intensity of message passing.\n\nIn summary, TKGE research has evolved from explicit temporal integration using hyperplanes or tensors to sophisticated geometric transformations (rotations, multi-curvature spaces) and advanced algebraic structures (quaternions) to handle the multifaceted nature of dynamic, spatiotemporal, and fuzzy knowledge. The trade-off between model expressiveness and computational complexity remains a persistent challenge, with recent models pushing the boundaries of what can be captured, often at the cost of increased model intricacy and optimization demands.",
    "Inductive and Continual KGE": "\\subsection{Inductive and Continual KGE}\nReal-world knowledge graphs (KGs) are inherently dynamic, with new entities, relations, and facts constantly emerging. Traditional Knowledge Graph Embedding (KGE) models are often transductive, meaning they can only generate embeddings for entities seen during training, necessitating expensive full retraining when new information arrives. This limitation has spurred significant research into inductive and continual KGE, aiming to adapt models to evolving KGs by handling unseen entities and efficiently updating knowledge without catastrophic forgetting \\cite{liu2024, sun2024}. These methods are crucial for maintaining the scalability and relevance of KGE models in dynamic environments.\n\nEarly efforts in inductive KGE focused on neighborhood aggregation techniques. \\cite{wang2018} introduced the Logic Attention Network (LAN), an aggregator that learns to embed new entities by combining the embeddings of their existing neighbors. LAN addresses the unordered and unequal nature of an entity's neighbors by employing both rules- and network-based attention weights. While innovative for its time, aggregation-based methods like LAN inherently rely on the presence of existing neighbors for new entities. This poses a limitation when truly novel, isolated entities with sparse connections emerge, as their representations might be less robust or even impossible to generate. The generalizability of such methods is also constrained by the quality and density of the local neighborhood information.\n\nTo overcome the limitations of direct entity embedding and enhance transferability, meta-learning has emerged as a powerful paradigm for inductive KGE. \\cite{chen2021} proposed MorsE, a model that does not learn explicit entity embeddings but instead learns transferable meta-knowledge. This meta-knowledge, modeled by entity-independent modules and learned through meta-learning, can then be used to produce embeddings for new entities in an inductive setting. This approach offers a more generalized inductive capability compared to simple aggregation, as it aims to capture underlying patterns that are independent of specific entities. Building on this, \\cite{sun2024} applied meta-learning to dynamic KGE in evolving service ecosystems with MetaHG. This model incorporates both local (via a GNN layer) and potential global (via a hypergraph neural network, HGNN, layer) structural information from current KG snapshots to enhance the representation of emerging entities. MetaHG's hybrid GNN framework and meta-learning strategy aim to mitigate issues like spatial deformation and improve the quality of embeddings for new entities. A critical comparison reveals that while meta-learning offers a more robust inductive framework, its complexity in training and the need for sufficient meta-training tasks can be significant. Furthermore, the effectiveness of meta-knowledge transfer can still be influenced by the similarity between the meta-training and target domains.\n\nBeyond inductive learning, continual KGE addresses the challenge of efficiently acquiring new knowledge while simultaneously preserving previously learned information, a problem often plagued by catastrophic forgetting. \\cite{liu2024} introduced FastKGE, a framework incorporating an incremental low-rank adapter (IncLoRA) mechanism. FastKGE tackles both efficient new knowledge acquisition and catastrophic forgetting by isolating and allocating new knowledge to specific layers based on the fine-grained influence between old and new KGs. The IncLoRA mechanism then embeds these specific layers into low-rank adapters, significantly reducing the number of trainable parameters during fine-tuning. This approach also features adaptive rank allocation, making the LoRA aware of entity importance. Experimental results demonstrate that FastKGE can reduce training time by 34-49\\% on public datasets while maintaining competitive performance, and even greater savings (51-68\\%) on larger, newly constructed datasets, alongside performance improvements. This parameter-efficient adaptation is a crucial advancement for large-scale KGE models, balancing the acquisition of new knowledge with the retention of old, a key trade-off in continual learning. However, the efficacy of LoRA-based methods can depend on the intrinsic rank of the updates and the architecture of the base KGE model.\n\nThe overarching goal across these inductive and continual KGE methods is to balance the acquisition of new knowledge with the retention of previously learned information, mitigating catastrophic forgetting and ensuring scalability. While neighborhood aggregation provides a straightforward, albeit limited, inductive capability, meta-learning offers a more generalized approach by learning transferable knowledge. Parameter-efficient adaptation techniques like IncLoRA represent a practical solution for continual learning, particularly for large models, by enabling efficient updates without full retraining. A common theoretical gap preventing a complete solution to these problems lies in developing truly universal inductive mechanisms that are robust to completely novel, isolated entities and can perform continuous, long-term updates without any degradation in performance or significant increase in computational cost. The experimental setups for these models often require specialized dynamic datasets, which can be less standardized than static link prediction benchmarks, making direct comparisons challenging and potentially affecting generalizability. The field continues to seek methods that can seamlessly integrate new information while preserving the integrity and expressiveness of the entire knowledge graph, a critical step towards truly adaptive and intelligent AI systems.",
    "Federated and Privacy-Preserving KGE": "\\subsection{Federated and Privacy-Preserving KGE}\nThe increasing concerns over data privacy and the proliferation of distributed knowledge sources have propelled Federated Learning (FL) as a crucial paradigm for Knowledge Graph Embedding (KGE). Federated KGE (FKGE) enables collaborative model training across multiple clients, each holding a local knowledge graph (KG), without centralizing sensitive data. This approach is vital for leveraging decentralized knowledge in privacy-sensitive domains, addressing the growing need for privacy-aware AI systems. However, FKGE introduces unique challenges, primarily related to communication efficiency, personalization for diverse client data, and security vulnerabilities.\n\nA significant challenge in FKGE is the high communication cost stemming from the large size of KGE parameters and the extensive number of communication rounds required for convergence. Traditional FL methods often focus on reducing communication rounds by increasing local training epochs, but they frequently overlook the size of parameters transmitted in each round. To address this, \\cite{zhang2024} (Communication-Efficient FKGE) proposes FedS, a bidirectional communication-efficient framework based on Entity-Wise Top-K Sparsification. This method allows clients to dynamically identify and upload only the Top-K entity embeddings with the most significant changes to the server. Similarly, the server transmits only the Top-K aggregated embeddings to each client after performing personalized aggregation. This approach, coupled with an Intermittent Synchronization Mechanism, aims to mitigate the negative effects of embedding inconsistency caused by client heterogeneity. While FedS significantly enhances communication efficiency, a critical analysis reveals a potential trade-off: universal reduction in embedding precision, as noted by the authors, can impede convergence speed. The challenge lies in precisely identifying the \"most significant\" changes without losing crucial information, especially for less frequently updated entities or relations. This aligns with the broader \"Efficiency, Compression, and System Optimization\" subgroup's goal of reducing resource consumption while maintaining performance, but within the added constraint of distributed, privacy-preserving learning.\n\nBeyond communication efficiency, the semantic disparities among clients pose a substantial hurdle for FKGE. Existing FKGE methods often rely on a global consensus model, typically using the arithmetic mean of entity embeddings as global supplementary knowledge \\cite{zhang2024}. This \"one-size-fits-all\" approach, however, neglects the inherent semantic heterogeneity across diverse client KGs, leading to a global model that might be inundated with noise when tailored to a specific client. To overcome this, \\cite{zhang2024} (Personalized Federated KGE) introduces PFedEG, a novel approach that employs a client-wise relation graph to learn personalized embeddings. PFedEG discerns the semantic relevance of embeddings from other clients, allowing each client to learn personalized supplementary knowledge by amalgamating entity embeddings from its \"neighboring\" clients based on their affinity on this graph. This personalized approach addresses the \"Personalized Federated KGE\" challenge, moving beyond a universal global model to improve embedding quality for individual clients. The strength of PFedEG lies in its ability to adapt to diverse data distributions, a critical aspect highlighted in the \"Dynamic, Inductive, and Continual KGE\" subgroup, which emphasizes adaptability to evolving and heterogeneous knowledge. However, the construction and maintenance of such a client-wise relation graph introduce additional computational complexity and potential privacy leakage risks if the \"affinity\" metrics are not carefully designed.\n\nWhile FL is designed to preserve data privacy, it is not inherently immune to security vulnerabilities. \\cite{zhou2024} (Poisoning Attack on Federated KGE) systematically explores the risks of poisoning attacks in FKGE, highlighting a critical security challenge. This pioneering work develops a novel framework that forces victim clients to predict specific false facts, demonstrating that privacy-preserving distributed training does not automatically equate to security. Unlike centralized KGEs, where attackers might directly inject poisoned data, FKGE's local data maintenance necessitates indirect attack vectors. The proposed attack framework involves inferring targeted relations in the victim's local KG via a \"KG component inference attack\" and then using an optimized dynamic poisoning scheme to generate progressive poisoned updates through FKGE aggregation. The experimental results demonstrate remarkable success rates (e.g., 100\\% on TransE with WN18RR) with minimal impact on the original task's performance, exposing a significant vulnerability. This research, while adversarial, is crucial for informing the design of robust FKGE systems, aligning with the \"Robustness and Training Optimization\" subgroup's focus on mitigating data imperfections and ensuring model integrity. The theoretical gap here is the lack of a direct defense mechanism proposed by the authors, which remains an open challenge for future research.\n\nIn synthesis, the emerging field of Federated and Privacy-Preserving KGE is rapidly addressing the practical demands of distributed and privacy-sensitive environments. The works by \\cite{zhang2024} (Communication-Efficient FKGE) and \\cite{zhang2024} (Personalized Federated KGE) represent constructive efforts to optimize FKGE by tackling communication bottlenecks and semantic heterogeneity, respectively. These solutions are critical for making FKGE scalable and effective in real-world deployments. However, the findings of \\cite{zhou2024} (Poisoning Attack on Federated KGE) serve as a stark reminder that privacy and security are distinct concerns, and the distributed nature of FL introduces new attack surfaces. The trade-offs are evident: aggressive communication sparsification might impact model convergence, personalization adds complexity and potential for noise, and robust security measures could introduce computational overhead. The rapid publication of these papers in 2024 underscores the contemporary and pressing nature of these challenges, reflecting the field's accelerated evolution towards practical, secure, and adaptable KGE solutions for decentralized knowledge.",
    "Efficiency, Compression, and Scalability": "\\subsection{Efficiency, Compression, and Scalability}\nThe practical deployment of Knowledge Graph Embedding (KGE) models for massive knowledge graphs (KGs) is often hampered by significant computational costs, extensive training times, and prohibitive memory footprints. Addressing these bottlenecks is crucial for transitioning KGEs from academic benchmarks to real-world applications, especially in resource-constrained environments. This area of research focuses on techniques spanning knowledge distillation, embedding compression, parameter-efficient learning, optimized system designs, and novel algorithms.\n\nOne prominent strategy for reducing the computational burden and memory footprint is \\textit{knowledge distillation}. \\cite{zhu2020} introduced DualDE, a method that distills knowledge from a high-dimensional, high-performing teacher KGE model into a low-dimensional student model. This approach significantly reduces embedding parameters (by 7-15x) and increases inference speed (by 2-6x) while retaining competitive performance. DualDE's generality allows its application across various KGE architectures, making it a versatile tool for efficiency. However, a common trade-off in distillation is a minor, albeit often acceptable, loss in performance, which is inherent to compressing information.\n\nComplementary to distillation are direct \\textit{embedding compression} techniques. \\cite{sachan2020} proposed representing entities with discrete codes, achieving remarkable compression ratios (50-1000x) of the embedding layer with only a minor performance drop. Building on this, LightKG \\cite{wang2021} introduced a lightweight framework that stores only a few codebooks and indices, drastically reducing storage and boosting inference efficiency through quick look-ups. LightKG also incorporates a dynamic negative sampling strategy, which further enhances performance. While these methods offer substantial gains in storage and inference speed, they rely on approximating the original embeddings, which might introduce subtle inaccuracies compared to full-precision representations.\n\n\\textit{Parameter-efficient learning} offers another avenue for scalability. Entity-Agnostic Representation Learning (EARL) \\cite{chen2023} tackles the escalating parameter storage costs by learning embeddings only for a small set of \"reserved entities.\" Embeddings for other entities are then derived from their context (e.g., connected relations, k-nearest reserved entities, multi-hop neighbors) using universal, entity-agnostic encoders. This approach results in a static and significantly lower parameter count, making it particularly beneficial for large and continuously growing KGs. A potential limitation of EARL is its reliance on contextual information; entities with sparse connections or those truly \"unseen\" without sufficient neighbors might have less expressive representations compared to directly learned embeddings.\n\nBeyond model-specific optimizations, \\textit{optimized system designs} are critical for large-scale KGE training. GE2 \\cite{zheng2024} proposes a general and efficient KGE learning system that addresses long CPU times and high CPU-GPU communication overhead, especially in multi-GPU setups. GE2 offloads operations to GPUs and introduces COVER, a novel algorithm for managing data swap between CPU and multiple GPUs, achieving speedups of 2x to 7.5x across various models and datasets. This system-level innovation provides a foundational improvement for KGE research, enabling faster experimentation and deployment of complex models. While GE2 focuses on training efficiency, it doesn't directly reduce the size of the *final* embedding model, which is where distillation and compression techniques become relevant.\n\nNovel algorithmic approaches also contribute significantly to efficiency. \"Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis\" \\cite{peng2021} introduces a fundamentally different paradigm by proposing a closed-form solution using Orthogonal Procrustes Analysis (OPA). This enables full-batch learning and non-negative sampling, reducing training time and carbon footprint by orders of magnitude while yielding competitive performance. The closed-form nature bypasses iterative optimization, which is a major source of computational cost in traditional KGEs. However, the inherent mathematical constraints of OPA might limit its expressiveness compared to more flexible, iteratively optimized models for certain complex relational patterns.\n\nFor Graph Neural Network (GNN)-based KGEs, which are known for their computational intensity, \\textit{graph partitioning strategies} are essential. CPa-WAC \\cite{modak2024} employs modularity maximization-based constellation partitioning to break down KGs into subgraphs. This allows for separate processing, reducing memory and training time for GNNs while aiming to retain prediction accuracy. CPa-WAC demonstrates up to a five-fold speedup, highlighting the effectiveness of distributed processing. Nevertheless, partitioning a graph can sometimes hinder the capture of global dependencies that span across different subgraphs, potentially impacting performance on tasks that require broader structural understanding.\n\nFinally, a comprehensive understanding of \\textit{parallelization techniques} is vital. \\cite{kochsiek2021} provided a critical meta-study, re-implementing and comparing various parallelization strategies for KGE training. Their work revealed that naive parallelization can degrade embedding quality and proposed effective mitigations, such as a variation of the stratification technique. This study underscores that simply distributing computation does not guarantee efficiency or quality, emphasizing the need for careful technique selection.\n\nIn summary, the pursuit of efficiency and scalability in KGE involves a multi-faceted approach. Techniques like DualDE \\cite{zhu2020}, \\cite{sachan2020}, and LightKG \\cite{wang2021} focus on compressing the model itself for storage and inference. EARL \\cite{chen2023} addresses parameter growth for evolving KGs. Meanwhile, GE2 \\cite{zheng2024}, OPA-based learning \\cite{peng2021}, CPa-WAC \\cite{modak2024}, and parallelization studies \\cite{kochsiek2021} target the efficiency of the training process. These innovations collectively aim to overcome the practical bottlenecks of KGE, making them deployable in resource-constrained environments and capable of handling the ever-growing scale of real-world knowledge bases. The ongoing challenge lies in balancing the gains in efficiency and scalability with the preservation of model expressiveness and predictive accuracy.",
    "Robustness and Training Optimization": "\\subsection*{Robustness and Training Optimization}\nThe efficacy of Knowledge Graph Embedding (KGE) models in real-world applications is profoundly dependent on their robustness against data imperfections and the optimization of their training processes. Knowledge graphs are inherently noisy, incomplete, and often suffer from imbalanced data distributions, necessitating sophisticated techniques to ensure that learned representations are accurate, reliable, and generalize well. This subsection delves into methods designed to enhance KGE model robustness and refine their training, particularly focusing on negative sampling strategies.\n\nA critical aspect of model reliability is the trustworthiness of its predictions. \\cite{tabacof2019} highlight that many popular KGE models, despite achieving high accuracy, produce uncalibrated probability estimates, meaning their predicted scores do not directly correspond to true probabilities. They propose post-hoc calibration methods like Platt scaling and isotonic regression, which are particularly valuable when ground truth negatives are scarce, a common scenario in KGs. While these methods offer a general solution applicable to various KGE models, their effectiveness relies on the quality of the calibration data and may introduce additional computational overhead during inference. This emphasizes that model performance metrics alone are insufficient; the reliability of output probabilities is equally crucial for practical deployment.\n\nTo address the pervasive issue of noisy data within KGs, which often arises from automatic knowledge construction, \\cite{zhang2021} introduce a multi-task reinforcement learning framework. This innovative approach actively filters out noisy triples during training, allowing the KGE model to learn from a cleaner, more reliable subset of facts. By exploiting correlations among semantically similar relations through multi-task learning, their method aims to learn more robust representations. This proactive noise-filtering mechanism is a significant advancement over passive error handling, as it directly impacts the quality of the input data for embedding. However, the complexity introduced by a reinforcement learning agent within the training loop can increase computational cost and require careful hyperparameter tuning, posing scalability challenges for extremely large KGs.\n\nAnother common imperfection in KGs is data imbalance, where entities and relations follow a long-tail distribution, with a few occurring frequently and many appearing rarely. Traditional KGE methods often assign equal weights during training, leading to unreliable representations for infrequent (long-tail) entities and relations. To counteract this, \\cite{zhang2023} propose WeightE, a weighted KGE model that employs a bilevel optimization scheme to assign differential weights. WeightE dynamically endows lower weights to frequent elements and higher weights to infrequent ones, ensuring that long-tail entities and relations receive adequate training attention. This flexible weighting technique can be applied to various existing KGE models, offering a practical solution to a widespread problem. While effective, the bilevel optimization adds a layer of complexity to the training process, which might require more computational resources or careful convergence monitoring compared to standard training.\n\nBeyond handling data imperfections, optimizing the training process itself, particularly through effective negative sampling, is paramount. KGE models typically rely on contrastive learning, requiring both positive (observed) and negative (false) triples. The quality of generated negative samples profoundly impacts model performance \\cite{qian2021, madushanka2024}. Early approaches to negative sampling often struggled in noisy environments. \\cite{shan2018} address this with a confidence-aware negative sampling method for noisy KGE, introducing the concept of negative triple confidence to improve training stability and prevent issues like zero loss or false detection. This method acknowledges that not all negative samples are equally informative, especially in the presence of noise. Building on the idea of identifying \"hard\" negatives, \\cite{zhang2018} propose NSCaching, an efficient caching strategy that tracks and samples challenging negative triplets. This approach, inspired by generative adversarial networks (GANs) but simpler, aims to distill the benefits of complex adversarial sampling into a more computationally efficient framework, demonstrating a trade-off between model complexity and training efficiency.\n\nA more radical departure from traditional negative sampling is presented by \\cite{li2021} with their Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE). This method proposes to avoid negative sampling entirely by considering all negative instances. While this theoretically removes the uncertainty and potential instability inherent in sampling, it dramatically increases computational complexity. NS-KGE addresses this by leveraging mathematical derivations to reduce the complexity of the non-sampling loss function, aiming for both better efficiency and accuracy. This non-sampling paradigm offers a compelling alternative, particularly for models whose loss functions can be efficiently reformulated, but its generalizability across all KGE architectures and its practical scalability to extremely dense KGs remain areas of active research.\n\nThe increasing complexity of KGE models and the integration of diverse data modalities further complicate negative sampling. \\cite{zhang2023} (Modality-Aware Negative Sampling) address this by proposing MANS, a method specifically designed for multi-modal KGE. MANS aligns structural and visual embeddings for entities, demonstrating that negative sampling strategies must adapt to the unique characteristics of multi-modal information to learn meaningful embeddings. This highlights a critical development direction where training optimization must evolve in tandem with model architectural advancements.\n\nOverall, the advancements in robustness and training optimization reflect a maturing KGE field that is moving beyond purely theoretical model expressiveness towards practical utility. The systematic reviews of negative sampling \\cite{qian2021, madushanka2024} underscore its foundational importance, even as models like \\cite{chen2025} (ConQuatE) introduce sophisticated quaternion-based embeddings to handle polysemy. The continued focus on refining training mechanisms, such as negative sampling, alongside the development of advanced architectures, indicates a holistic approach where the effectiveness of complex KGE models is deeply intertwined with robust and efficient training methodologies. While significant progress has been made in handling noise, imbalance, and sampling, the challenge of efficiently and accurately identifying the \"true\" negative distribution in KGs, especially in dynamic and multi-modal settings, remains a theoretical gap and an active area of research.",
    "Evaluation, Benchmarking, and Reproducibility": "\\subsection{Evaluation, Benchmarking, and Reproducibility}\n\nThe rapid proliferation of Knowledge Graph Embedding (KGE) models has underscored the critical importance of rigorous evaluation, standardized benchmarking, and robust reproducibility practices. Without these, fair comparisons between models become challenging, scientific progress can be hindered by unreliable results, and the trustworthy deployment of KGE models in real-world applications is compromised. The field has increasingly recognized the need to move towards higher standards of empirical validation and transparency to ensure the reliability and generalizability of research findings.\n\nA significant step towards addressing these challenges has been the development of unified frameworks and libraries. \\cite{broscheit2020} introduced \\texttt{LibKGE}, an open-source PyTorch-based library designed to foster reproducible research. Its key strengths lie in its high configurability, decoupled components that allow for flexible mixing and matching, and comprehensive logging, making it an invaluable tool for conducting systematic experimental studies and analyzing the contributions of individual model components. While \\texttt{LibKGE} provides the infrastructure for reproducible experimentation, large-scale comparative studies have simultaneously exposed the widespread issues plaguing KGE research.\n\n\\cite{ali2020}'s seminal work, \"Bringing Light Into the Dark,\" provided a stark revelation of reproducibility failures within the KGE community. By re-implementing and evaluating 21 models within a unified framework, \\texttt{PyKEEN}, the authors found that many published results could not be reproduced with their reported hyperparameters, and some not at all. This highlights a significant methodological limitation: the heterogeneity in implementations, training procedures, and evaluation protocols across different research groups often leads to incomparable results and inflated performance claims. The study emphasized that model performance is not solely determined by architecture but by a complex interplay of architecture, training approach, loss function, and the explicit modeling of inverse relations. This suggests that many experimental setups in prior work lacked the necessary standardization to ensure generalizability.\n\nFurther compounding these issues, \\cite{rossi2020} conducted a comprehensive comparison of 18 state-of-the-art KGE methods for link prediction, critically examining the effect of design choices and exposing biases in standard evaluation practices. They highlighted that the common practice of aggregating accuracy over a large number of test facts, where some entities are vastly more represented than others, allows models to achieve good results by focusing on these high-frequency entities, thereby ignoring the majority of the knowledge graph. This inherent bias in benchmark datasets can lead to an overestimation of a model's true generalization capabilities, as its performance might be artificially boosted by exploiting statistical artifacts rather than genuinely learning complex relational patterns. This reveals a critical assumption made in many KGE evaluationsthat benchmark datasets provide a uniformly representative test bedwhich is often unrealistic.\n\nBeyond evaluation biases, the impact of hyperparameter tuning on KGE quality has also been rigorously investigated. \\cite{lloyd2022} employed Sobol sensitivity analysis to quantify the importance of various hyperparameters, revealing substantial variability in their sensitivities across different knowledge graphs. This implies that optimal hyperparameter configurations are often dataset-specific, making universal recommendations difficult and further complicating reproducibility. A particularly concerning finding was the identification of data leakage in the widely used UMLS-43 benchmark due to inverse relations, which could lead to artificially inflated performance metrics. This directly challenges the integrity of a common experimental setup and underscores the need for meticulous data curation and validation.\n\nCollectively, these studies reveal that the KGE field, while innovative in model development, has historically suffered from a lack of meta-scientific rigor. The methodological limitations include inconsistent implementations, biased evaluation metrics, and an underestimation of hyperparameter sensitivity. These issues directly impact the generalizability of findings, as models might be overfit to specific experimental conditions or benchmark quirks. The theoretical gap isn't necessarily in the KGE models themselves, but in the overarching framework for their empirical validation. Addressing these challenges requires a concerted community effort towards adopting unified frameworks, conducting transparent and reproducible experiments, and developing more robust, unbiased evaluation metrics that truly reflect a model's understanding of the knowledge graph. This move towards higher standards is crucial for fostering reliable scientific progress and ensuring the trustworthy deployment of KGE models in critical applications.",
    "Link Prediction and Knowledge Graph Completion": "\\subsection{Link Prediction and Knowledge Graph Completion}\nLink prediction (LP) and knowledge graph completion (KGC) represent the fundamental applications of knowledge graph embedding (KGE), aiming to infer missing facts and enhance the completeness of knowledge graphs. These tasks are crucial for making KGs more robust and informative for downstream AI systems by automatically inferring unobserved facts within the graph structure. The evolution of KGE models for LP/KGC reflects a continuous effort to improve accuracy, handle complex relational patterns, and address practical challenges.\n\nEarly KGE models primarily leveraged geometric transformations to represent entities and relations. Translational models, such as TransH \\cite{wang2014} and TransD \\cite{ji2015}, extended the foundational TransE by modeling relations as translations on hyperplanes or through dynamic mapping matrices, respectively. TransH notably improved the handling of one-to-many and many-to-one relations by allowing entity projections, while TransD further refined this by considering the diversity of both entities and relations. However, these models, while efficient, often struggled to capture more intricate logical patterns like symmetry, antisymmetry, inversion, and composition. This limitation spurred the development of rotational models, with RotatE \\cite{sun2018} defining relations as rotations in complex vector spaces. RotatE demonstrated superior expressiveness for these complex patterns, significantly outperforming its translational predecessors in link prediction tasks. Further geometric innovations include embedding entities on Lie groups (e.g., TorusE \\cite{ebisu2017}) to address regularization issues, exploring alternative metrics like the Cycle metric \\cite{yang2021} for enhanced expressiveness, and introducing powerful transformations such as Householder parameterization (HousE \\cite{li2022}) or compound operations (CompoundE \\cite{ge2022}, CompoundE3D \\cite{ge2023}) to capture a broader spectrum of relational semantics. More recently, models like HolmE \\cite{zheng2024} have focused on ensuring closure under composition, a theoretical property vital for modeling under-represented compositional patterns, while MQuinE \\cite{liu2024} addresses specific theoretical deficiencies (\"Z-paradox\") in existing models to enhance expressiveness. While these geometric models offer mathematical elegance and interpretability, their ability to capture highly complex, non-linear interactions can be limited compared to deep learning approaches.\n\nThe advent of deep learning architectures marked a significant paradigm shift in KGE for LP/KGC. Convolutional Neural Networks (CNNs) have been widely adopted to extract local features and model interactions between entity and relation embeddings. Models like AcrE \\cite{ren2020} and ReInceptionE \\cite{xie2020} utilize various convolutional layers and attention mechanisms to capture complex relation patterns and aggregate entity-specific features, achieving state-of-the-art results. More recent CNN-based methods, such as CNN-ECFA \\cite{hu2024} and SEConv \\cite{yang2025}, continue to refine feature aggregation and interaction. Graph Neural Networks (GNNs) and attention mechanisms further leverage the graph's topology, with models like DisenKGAT \\cite{wu2021} employing disentangled graph attention networks for diverse representations and GAATs \\cite{wang2020} incorporating graph attenuated attention to capture rich neighborhood information. Transformer-based architectures, including CoKE \\cite{wang2019}, Knowformer \\cite{li2023}, and TGformer \\cite{shi2025}, treat KGs as sequences or integrate graph structures into Transformer frameworks, leveraging self-attention to capture long-range dependencies and contextualized representations. These deep learning models excel at learning intricate, non-linear, and context-dependent features, often surpassing purely geometric approaches in accuracy, but typically incur higher computational costs and can be less interpretable.\n\nBeyond structural information, KGE models for LP/KGC have been enriched by incorporating auxiliary information, logical rules, and multi-modal data. Integrating entity types, as seen in TransET \\cite{wang2021} and TaKE \\cite{he2023}, provides semantic guidance that significantly improves KG completion, especially in low-resource settings. Hyper-relational KGE models like HINGE \\cite{rosso2020} move beyond simple triplets to incorporate associated key-value pairs, capturing richer data semantics. Rule-based KGE approaches, such as RUGE \\cite{guo2017} and RulE \\cite{tang2022}, iteratively guide embedding learning with soft logical rules, enhancing reasoning capabilities and ensuring semantic consistency, though rule extraction and balancing rule adherence with flexibility remain challenges. Furthermore, multi-modal KGEs, exemplified by SSP \\cite{xiao2016} which projects text descriptions into semantic space, and Joint Language Semantic and Structure Embedding \\cite{shen2022} which integrates pre-trained language models, enrich embeddings with external semantic context, crucial for overcoming data sparsity and improving understanding.\n\nThe practical effectiveness of KGE for LP/KGC also heavily relies on training optimizations and robustness. Negative sampling strategies, such as Confidence-Aware Negative Sampling \\cite{shan2018} and NSCaching \\cite{zhang2018}, are critical for efficient and effective training, though the \"true\" negative distribution remains a theoretical gap. Some approaches, like Efficient Non-Sampling KGE \\cite{li2021}, even attempt to avoid negative sampling entirely to achieve more stable performance, albeit with increased computational complexity. Robustness against noisy data is addressed by methods like those using multi-task reinforcement learning \\cite{zhang2021} to filter out erroneous triples or weighted training schemes (e.g., WeightE \\cite{zhang2023}) to handle data imbalance. The continuous development across these diverse methodologies underscores the central role of link prediction and knowledge graph completion in advancing the utility and reliability of knowledge graphs for a wide array of AI applications \\cite{dai2020, cao2022, ge2023, rossi2020}.",
    "Entity Alignment": "\\subsection{Entity Alignment}\nEntity Alignment (EA) is a critical task in knowledge graph integration, aiming to identify equivalent entities across different, often heterogeneous, knowledge graphs (KGs). The proliferation of KGs from diverse sources necessitates robust methods for their integration, which is fundamental for building comprehensive knowledge bases and enabling sophisticated cross-KG reasoning \\cite{dai2020, choudhary2021}. Knowledge Graph Embeddings (KGEs) have emerged as a powerful, data-driven approach to tackle this challenge, transforming symbolic entities and relations into low-dimensional vector spaces where semantic correspondences can be identified through similarity measures \\cite{yan2022, cao2022}. This approach leverages the ability of KGEs to capture intricate structural and semantic patterns, making them highly suitable for finding equivalences between disparate knowledge structures.\n\nA significant hurdle in embedding-based entity alignment is the scarcity of labeled training data, which can limit the accuracy and generalizability of models. To address this, bootstrapping methods have been developed. For instance, \\cite{sun2018} proposed an iterative bootstrapping approach that progressively labels likely entity alignments to augment the training data for learning alignment-oriented KG embeddings. This method strategically employs an alignment editing technique to mitigate the accumulation of errors during the iterative process, which is a common pitfall in self-training schemes. While effective in leveraging unlabeled data, the performance of such bootstrapping approaches can be sensitive to the quality of the initial seed alignments and the robustness of the error reduction mechanism, as false positives in early iterations can propagate and degrade overall accuracy.\n\nExtending beyond purely bootstrapping, semi-supervised learning frameworks have been introduced to more effectively utilize both limited labeled data and abundant unlabeled information. \\cite{pei2019} presented a semi-supervised entity alignment method (SEA) that not only leverages unlabeled entities but also incorporates an awareness of entity degree differences. This is crucial because entities with vastly different degrees (i.e., number of connections) can lead to biased embeddings, making alignment challenging, particularly between high-frequency and low-frequency entities. By employing adversarial training, SEA aims to learn more robust embeddings that are less affected by these structural disparities. This approach addresses a practical limitation of many KGE models, where embedding quality can be disproportionately influenced by highly connected entities, thereby improving alignment accuracy across the entire spectrum of entities. However, the complexity of adversarial training can introduce challenges in model stability and hyperparameter tuning.\n\nFurther enhancing the robustness and accuracy of EA, multi-view frameworks integrate diverse types of entity information. \\cite{zhang2019} proposed a novel multi-view KGE framework that unifies entity names, relational structures, and attributes to learn more comprehensive embeddings for alignment. Traditional KGE methods often focus predominantly on relational structures, overlooking other rich features that can provide complementary semantic cues. By combining these multiple views with various strategies and designing cross-KG inference methods, this approach significantly improves alignment performance. The strength of multi-view learning lies in its ability to capture a broader spectrum of semantic information, making the embeddings more discriminative. Nevertheless, the challenge lies in effectively weighting and integrating potentially conflicting signals from different views, and the computational overhead increases with the number of views considered.\n\nMore recently, the integration of ontological information has provided another powerful dimension for entity alignment. \\cite{xiang2021} introduced OntoEA, an ontology-guided entity alignment method that jointly embeds both KGs and their associated ontologies. This approach explicitly utilizes critical meta-information, such as class hierarchies and class disjointness constraints, which are often ignored by purely structural or attribute-based methods. By enforcing these ontological constraints during the embedding process, OntoEA can prevent false mappings and guide the alignment towards semantically coherent equivalences. This addresses a theoretical gap where KGEs, while powerful, often lack an explicit mechanism to incorporate higher-level schema knowledge. The effectiveness of OntoEA, however, is contingent on the availability and quality of consistent ontological information across the KGs being aligned, which may not always be present in real-world scenarios.\n\nThe collective advancements in these KGE-based EA methods underscore a clear evolution in the field. Early approaches focused on leveraging structural similarity, while subsequent methods have progressively integrated more semantic context, auxiliary information, and robust learning paradigms to overcome limitations like data scarcity and feature incompleteness. Comprehensive surveys and experimental reviews, such as those by \\cite{zhu2024} and \\cite{fanourakis2022}, further highlight the strengths and weaknesses of various KGE-based EA techniques. They emphasize the need for more robust noise filtering strategies, better utilization of additional information, and rigorous comparative analyses across diverse datasets to ensure generalizability. These meta-analyses confirm that KGEs provide a versatile and powerful foundation for integrating heterogeneous knowledge sources, enabling the construction of more comprehensive knowledge bases and facilitating complex cross-KG reasoning, which is crucial for advancing knowledge-driven AI applications.",
    "Question Answering and Recommendation Systems": "\\subsection*{Question Answering and Recommendation Systems}\nKnowledge Graph Embeddings (KGEs) have emerged as a pivotal technology for bridging the semantic gap between natural language and structured knowledge, enabling more intelligent and interpretable interactions in diverse applications such as Question Answering (QA) and Recommender Systems. These applications leverage KGEs to transform complex symbolic reasoning into efficient vector space operations, thereby enhancing performance and user experience \\cite{dai2020, cao2022}.\n\nIn the realm of Question Answering over Knowledge Graphs (QA-KG), KGEs facilitate the understanding of natural language queries by mapping them to entities and relations within the underlying knowledge graph. Early frameworks, such as Knowledge Embedding based Question Answering (KEQA), demonstrated the utility of KGEs by jointly recovering head entity, predicate, and tail entity representations in the embedding space to answer simple natural language questions \\cite{huang2019}. KEQA's strength lies in its ability to address predicate variability and entity ambiguity by leveraging the semantic proximity captured by embeddings. However, its focus on \"simple questions\" highlights a limitation in handling more complex, multi-hop, or nuanced queries, which often require deeper integration with natural language processing (NLP) capabilities.\n\nMore advanced QA systems have evolved into hybrid architectures that seamlessly integrate KGEs with sophisticated NLP models. A prime example is the Marie and BERT system for chemistry, which showcases a comprehensive approach to domain-specific QA \\cite{zhou2023}. This system employs hybrid KGEs, leveraging multiple embedding spaces to capture diverse relational patterns, and integrates a BERT-based entity-linking model to enhance robustness and accuracy in identifying entities from natural language queries. Furthermore, Marie and BERT addresses the complexities of deep ontologies by deriving implicit multi-hop relations and incorporates mechanisms for numerical filtering, demonstrating a significant leap in handling intricate, fact-oriented information retrieval in specialized domains. While such hybrid systems offer superior performance in complex scenarios, their domain specificity and the inherent complexity of integrating heterogeneous components (KGEs, BERT, semantic agents) can limit generalizability and increase development overhead. The methodological challenge lies in effectively harmonizing the continuous vector representations from KGEs with the discrete, symbolic reasoning often required for precise QA.\n\nFor recommender systems, KGEs provide a powerful mechanism to model user preferences and item characteristics by representing them as entities in a knowledge graph, thereby enabling more personalized and transparent suggestions. Recurrent Knowledge Graph Embedding (RKGE) was an early and influential approach that utilized a recurrent network to automatically learn semantic representations of paths between entities \\cite{sun2018}. By fusing these path semantics into the recommendation process, RKGE not only improved recommendation accuracy but also offered meaningful explanations based on path saliency, a crucial step towards interpretable recommendations. The recurrent network architecture allowed RKGE to discriminate the saliency of different paths in characterizing user preferences, moving beyond traditional feature engineering that often requires extensive domain knowledge.\n\nBuilding upon this foundation, recent research has pushed towards more contextualized and explainable approaches. Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation (CKGE) represents a significant advancement in this direction \\cite{yang2023}. CKGE constructs \"meta-graphs\" for talent-course pairs, incorporating contextualized neighbor semantics and high-order connections as \"motivation-aware information.\" It then employs a novel KG-based Transformer, equipped with relational attention and structural encoding, to model the global dependencies of KG structured data. A key innovation in CKGE is its \"local path mask prediction,\" which effectively reveals the importance of different paths, thereby offering precise and explainable recommendations that can discriminate the saliencies of meta-paths in characterizing corresponding preferences \\cite{yang2023}.\n\nComparing RKGE and CKGE, the evolution highlights a shift from recurrent networks for path modeling to more sophisticated Transformer architectures that capture richer contextual information. While RKGE provided explanations based on path saliency, CKGE offers a more granular and motivation-aware interpretability by integrating contextualization and high-order connections. This progression, as noted in the development directions, reflects an acceleration in research driven by the demand for more sophisticated, robust, and interpretable recommendation solutions. However, the increased complexity of models like CKGE, with their meta-graph construction and Transformer integration, introduces trade-offs in computational cost and the interpretability of the underlying embedding space. The theoretical gap remains in developing truly intuitive and actionable explanations that are universally understandable to human users, rather than merely technical artifacts of the model.\n\nIn summary, KGEs have profoundly impacted QA and recommender systems by providing a robust framework to bridge natural language and structured knowledge. From foundational KEQA to hybrid systems like Marie and BERT, and from recurrent RKGEs to contextualized CKGEs, these advancements demonstrate KGE's utility in enabling more intelligent, personalized, and interpretable user interactions. The ongoing challenge lies in balancing model expressiveness with computational efficiency and ensuring that the generated explanations are genuinely transparent and actionable across diverse application contexts.",
    "Domain-Specific Applications and Explainability": "\\subsection{Domain-Specific Applications and Explainability}\nThe utility of Knowledge Graph Embedding (KGE) models extends significantly into specialized, high-stakes domains, where their ability to transform symbolic knowledge into actionable insights is paramount. This section highlights the application of KGE in fields such as biological systems, patent metadata analysis, and drug repurposing, emphasizing how these models are tailored, validated, and increasingly scrutinized for explainability to build trust and deliver verifiable solutions.\n\nIn biological and biomedical systems, KGE models offer powerful tools for discovery and analysis. \\cite{mohamed2020} provides a comprehensive review of KGE applications in this domain, showcasing their predictive and analytical capabilities for tasks like drug-target interactions and polypharmacy side effects. The authors argue that KGEs are a natural fit for representing complex biological knowledge, overcoming the scalability limitations of traditional graph exploratory approaches. Building on this, \\cite{zhu2022} demonstrates multimodal reasoning based on KGE for specific diseases. They construct Specific Disease Knowledge Graphs (SDKGs) and integrate structural, category, and description embeddings using reverse-hyperplane projection. This multimodal approach enhances the discovery of new, reliable knowledge, underscoring the value of tailoring KGEs to domain-specific knowledge structures and leveraging diverse data modalities to improve reasoning.\n\nA particularly compelling example of KGE application in a critical domain is drug repurposing for diseases like COVID-19. \\cite{islam2023} proposes an innovative approach that utilizes ensemble KGEs to generate robust latent representations, which are then fed into a deep neural network for identifying potential drug candidates. Crucially, this work moves beyond generic KGE evaluation metrics by incorporating \\textit{molecular docking} to validate predictions, a domain-specific and verifiable method for assessing drug-target interactions. This integration of molecular-level evaluation is a significant step towards delivering transparent solutions, as it provides concrete, scientific validation for the abstract KGE predictions. Furthermore, \\cite{islam2023} addresses the paramount need for explainability by providing explanations through rules extracted from the knowledge graph and instantiated by explanatory paths. This allows medical professionals and researchers to understand *why* a particular drug is predicted, fostering trust and enabling actionable insights in a field where decisions have direct human impact.\n\nBeyond biomedicine, KGEs are also applied to analyze complex structured data like patent metadata. \\cite{li2022} operationalizes knowledge proximity within the US Patent Database by training KGE models on a \"PatNet\" knowledge graph constructed from patent citations, inventors, assignees, and domain classifications. By using cosine similarity between learned embeddings, they measure knowledge proximity between homogeneous (e.g., patent-patent) and heterogeneous (e.g., inventor-assignee) entities. This application demonstrates how KGEs can be tailored to specific industry problems, providing quantitative measures for abstract concepts like \"knowledge proximity\" and enabling the analysis of domain expansion profiles for inventors and assignees. While this work primarily focuses on predictive performance, the inherent interpretability of proximity measures in the embedding space can offer insights into innovation landscapes.\n\nThe growing demand for interpretable KGE models in these high-stakes fields is a significant evolutionary trend. The explicit focus on explainability in \\cite{islam2023} highlights a shift from merely achieving high performance on abstract metrics to delivering verifiable and transparent solutions. This aligns with broader research efforts in making KGEs more understandable. For instance, methods that integrate logical rules and constraints into the embedding process, such as \\cite{guo2017}'s iterative guidance from soft rules (RUGE) or \\cite{tang2022}'s RulE framework, which learns rule embeddings jointly with entity and relation embeddings, inherently contribute to explainability. By aligning embeddings with human-understandable logical patterns, these approaches can provide a basis for explaining model predictions. Similarly, \\cite{ding2018} showed that even simple constraints like non-negativity on entity representations can improve model interpretability by structuring the embedding space.\n\nHowever, achieving robust explainability in complex KGE models, especially those leveraging deep learning architectures, remains a challenge. The trade-off often lies between the high expressiveness and predictive power of complex models and the inherent difficulty in extracting clear, human-understandable explanations from their latent spaces. While \\cite{islam2023} successfully combines ensemble KGEs with molecular docking and rule-based explanations, the generalizability of such multi-faceted explanation strategies across all domain-specific KGE applications requires further investigation. The theoretical gap in universally interpretable embedding spaces, particularly for highly non-linear models, prevents a straightforward solution to providing transparent insights for every prediction. Nevertheless, the explicit integration of domain-specific validation and explanation mechanisms, as exemplified by these works, marks a crucial step towards building trust and enabling the responsible deployment of KGE technologies in critical real-world scenarios.",
    "Summary of Key Developments": "\\subsection*{Summary of Key Developments}\n\nThe field of Knowledge Graph Embedding (KGE) has undergone a remarkable evolution, transitioning from foundational geometric and algebraic models to highly sophisticated deep learning architectures, driven by a continuous pursuit of enhanced expressiveness, efficiency, and robustness. Early advancements were rooted in the geometric paradigm, where relations were conceptualized as transformations within continuous vector spaces. Pioneering models like TransE and its successors, such as TransH \\cite{wang2014} and TransD \\cite{ji2015}, established the translation-based approach, modeling relations as vector translations from head to tail entities. These early efforts focused on refining the embedding space to better capture diverse relational patterns, with TransD introducing dynamic mapping matrices for finer-grained distinctions between entities and relations, thereby improving expressiveness while managing parameter complexity \\cite{ji2015}. Further geometric innovations explored non-Euclidean spaces, such as TorusE \\cite{ebisu2017} embedding on Lie groups to address regularization challenges, and CyclE \\cite{yang2021} investigating the impact of metric choices (e.g., Cycle vs. Minkowski) on expressiveness. These models underscored the importance of the underlying geometry in accurately representing complex knowledge.\n\nA significant paradigm shift occurred with the integration of deep learning architectures, which enabled KGE models to automatically learn intricate features and structural patterns. Convolutional Neural Networks (CNNs) were adapted to KGE, with models like AcrE \\cite{ren2020} leveraging atrous convolutions and residual learning for efficient feature interactions, and ReInceptionE \\cite{xie2020} employing inception networks and attention for joint local-global structural information. More recent CNN-based approaches, such as CNN-ECFA \\cite{hu2024} and SEConv \\cite{yang2025}, continue to refine feature aggregation for improved performance. Graph Neural Networks (GNNs) further enhanced KGE by capturing neighborhood context and structural information through message passing, exemplified by DisenKGAT \\cite{wu2021}, which introduced disentangled graph attention networks for more diverse and independent component representations. The emergence of Transformer architectures, as seen in CoKE \\cite{wang2019}, Knowformer \\cite{li2023}, and TGformer \\cite{shi2025}, marked another leap, enabling contextualized embeddings by treating KGs as sequences or integrating graph structures into self-attention mechanisms, thereby capturing long-range dependencies and multi-structural features.\n\nBeyond core architectural advancements, the field has continuously sought to enrich KGE models with auxiliary information, logical rules, and multi-modal data to overcome inherent limitations. Approaches like TransET \\cite{wang2021} and TaKE \\cite{he2023} demonstrated the value of incorporating entity type information to provide semantic guidance and improve knowledge graph completion. Similarly, models like HINGE \\cite{rosso2020} moved \"beyond triplets\" to directly learn from hyper-relational facts, capturing richer data semantics. The integration of logical rules, as in RUGE \\cite{guo2017} and RulE \\cite{tang2022}, allowed KGE models to inject prior knowledge and enforce semantic consistency, moving towards more robust and interpretable reasoning. Furthermore, multi-modal KGE, exemplified by SSP \\cite{xiao2016} integrating text descriptions and recent works leveraging pre-trained language models \\cite{shen2022}, has addressed data sparsity and enhanced semantic understanding by fusing diverse information sources.\n\nThe practical deployment of KGE models has driven significant research into efficiency, robustness, and adaptability. Efforts to enhance efficiency include knowledge distillation (DualDE \\cite{zhu2020}), embedding compression \\cite{sachan2020}, parameter-efficient learning (EARL \\cite{chen2023}), and optimized training systems like GE2 \\cite{zheng2024}. Robustness has been improved through techniques like confidence-aware negative sampling \\cite{shan2018}, reinforcement learning-based noise filtering \\cite{zhang2021}, and weighted training for imbalanced data \\cite{zhang2023}. The challenge of dynamic KGs has led to inductive KGE models leveraging neighborhood aggregation \\cite{wang2018} and meta-learning \\cite{chen2021, sun2024}, as well as continual learning approaches like incremental LoRA for efficient updates \\cite{liu2024}. Moreover, the rise of federated learning has spurred research into privacy-preserving KGE, addressing communication efficiency \\cite{zhang2024} and personalization \\cite{zhang2024_personalized}, while also acknowledging security vulnerabilities like poisoning attacks \\cite{zhou2024}.\n\nIn summary, the KGE landscape has evolved from simple geometric models to complex deep learning architectures, continuously pushing the boundaries of expressiveness and efficiency. The field's progression is marked by a holistic approach: enhancing core models, enriching them with diverse contextual and logical information, and ensuring their practical utility through robust, scalable, and adaptable designs. These advancements underscore the significant strides made in transforming symbolic knowledge into actionable insights, making KGE a cornerstone for intelligent AI systems across various applications, from link prediction and question answering \\cite{huang2019, zhou2023} to recommendation systems \\cite{sun2018, yang2023}.",
    "Open Challenges and Theoretical Gaps": "\\subsection{Open Challenges and Theoretical Gaps}\nDespite significant advancements in Knowledge Graph Embedding (KGE) research, several critical open challenges and theoretical gaps persist, representing fertile ground for future investigation. These issues often stem from inherent trade-offs, the complexity of real-world knowledge graphs, and the limitations of current theoretical understandings.\n\nOne pervasive challenge lies in balancing model expressiveness with computational complexity. While models like RotatE \\cite{sun2018} and the composition-closed HolmE \\cite{zheng2024} have pushed the boundaries of capturing intricate relational patterns, their increased expressiveness often comes at the cost of higher computational demands for training and inference (Subgroup 1, Overall Perspective). Efforts in efficiency and compression, such as DualDE's distillation \\cite{zhu2020} and LightKG's codebook-based storage \\cite{wang2021}, aim to mitigate this. However, as noted by \\cite{sachan2020}, these often entail a \"minor loss in performance.\" The theoretical gap here is to devise architectures that are *inherently* expressive yet computationally lean, perhaps through novel mathematical formulations like the Orthogonal Procrustes Analysis in \\cite{peng2021}, which offers a closed-form solution for efficiency, rather than relying on post-hoc compression or distillation.\n\nEnsuring the interpretability of complex deep learning KGE models is another significant hurdle. As KGE increasingly leverages Graph Neural Networks (GNNs) and Transformer architectures (Subgroup 6, Advanced Model Design), their black-box nature becomes a concern, especially in high-stakes applications. While some models like SpherE \\cite{li2024} claim interpretability through their geometric properties, this is often specific to the model's design and does not generalize to the complex reasoning paths learned by deep networks. Application-focused works, such as the explainable drug repurposing by \\cite{islam2023} and contextualized recommendation by \\cite{yang2023}, demonstrate the *need* for explanations, but typically rely on post-hoc rule extraction or path saliency. A theoretical gap exists in developing intrinsically interpretable deep KGE models that can transparently reveal their reasoning processes without sacrificing predictive power.\n\nThe efficient extraction and integration of high-quality logical rules remain a bottleneck. While methods like RUGE \\cite{guo2017} and RulE \\cite{tang2022} have shown the value of incorporating soft rules to enhance reasoning and consistency (Subgroup 3, Rule-based & Constraint-driven KGE), the process of obtaining these rules and balancing their adherence with the flexibility to capture exceptions is challenging. Automatically extracted rules often carry \"uncertainties\" \\cite{guo2017}, and the scalability of integrating complex rule sets can be problematic \\cite{guo2020}. The theoretical challenge lies in developing robust, automated rule induction systems that can generate high-fidelity rules from noisy KGs and seamlessly integrate them into embedding models without introducing significant computational overhead or compromising the data-driven learning of nuanced patterns.\n\nFurthermore, resolving issues related to the 'true' negative distribution in training is fundamental. KGE models rely heavily on negative sampling for contrastive learning, yet the \"true\" negative distribution is inherently unknown (Subgroup 3, Negative Sampling & Training Optimization). While approaches like NSCaching \\cite{zhang2018} and confidence-aware sampling \\cite{shan2018} improve efficiency and robustness to noise, they are still heuristic approximations. The \"Efficient Non-Sampling Knowledge Graph Embedding\" \\cite{li2021} attempts to bypass sampling entirely, but requires complex mathematical derivations to manage computational complexity. As highlighted by comprehensive reviews \\cite{qian2021, madushanka2024}, this remains a persistent challenge, particularly when extending to multi-modal KGE \\cite{zhang2023}. A theoretical breakthrough is needed to either accurately model the true negative distribution or develop training paradigms that are robust to its uncertainty without prohibitive computational costs.\n\nThe field also critically needs more robust and unbiased evaluation metrics. As revealed by the \"Evaluation, Benchmarking, and Reproducibility\" subgroup, standard metrics often suffer from biases, such as the over-representation of certain entities \\cite{rossi2020}, and reported results can be sensitive to hyperparameter choices or even data leakage \\cite{lloyd2022}. The lack of standardized frameworks and reproducibility issues \\cite{ali2020, broscheit2020} further complicate fair comparisons. The theoretical gap is in developing evaluation protocols that are robust to dataset characteristics, sensitive to diverse relational patterns (e.g., long-tail, compositional), and truly reflect real-world application performance, moving beyond simple link prediction accuracy.\n\nThe challenges of scalability for extremely large and dynamic knowledge graphs are paramount. While progress has been made in efficiency \\cite{peng2021, zheng2024} and compression \\cite{sachan2020, wang2021}, handling KGs with billions of triples and continuous, real-time updates remains difficult. Dynamic KGE methods like FastKGE \\cite{liu2024} and MetaHG \\cite{sun2024} focus on efficient updates and mitigating catastrophic forgetting, but the sheer volume and velocity of changes in real-world KGs can still overwhelm these systems. Federated KGE introduces additional complexities related to communication efficiency \\cite{zhang2024} and personalized aggregation for diverse client data \\cite{zhang2024_personalized}. The theoretical challenge is to design truly elastic and adaptive KGE architectures that can scale horizontally, handle continuous streams of new information, and maintain global consistency and performance in highly distributed and dynamic environments without prohibitive computational or communication costs.\n\nFinally, the development of truly generalizable inductive models remains an open problem. While neighborhood aggregation \\cite{wang2018} and meta-learning approaches \\cite{chen2021, sun2024} have enabled inductive capabilities for new entities (Subgroup 4, Dynamic, Inductive, and Continual KGE), they often rely on existing neighbors or transferable meta-knowledge. Truly novel entities with entirely new semantic or structural patterns, or isolated entities with sparse connections, still pose a significant challenge. A deeper theoretical understanding of \"semantic evidence\" for extrapolation \\cite{li2021} is required to build models that can robustly infer representations for such unseen entities or entirely new relational types, pushing beyond mere interpolation to genuine generalization. These challenges collectively underscore the need for continued fundamental and applied research to unlock the full potential of KGE.",
    "Emerging Trends and Ethical Considerations": "\\subsection*{Emerging Trends and Ethical Considerations}\nThe landscape of Knowledge Graph Embedding (KGE) is continuously evolving, driven by both technological advancements and a growing awareness of societal impact. This section delves into key emerging trends that are shaping the future of KGE research, alongside crucial ethical considerations that must guide its development. These discussions are informed by the field's methodological evolution, knowledge progression, and the increasing demand for robust, responsible AI systems.\n\nOne of the most significant emerging trends is the deeper integration of KGE with pre-trained language models (PLMs) for richer semantic understanding. Traditional KGE models, as discussed in the \"Core KGE Model Architectures and Expressiveness\" subgroup, primarily learn representations from the structural patterns of knowledge graphs \\cite{wang2014, sun2018, zheng2024}. While effective, these models often struggle with data sparsity and entities lacking sufficient structural connections, a limitation highlighted in the \"Knowledge Progression\" of KGE research. PLMs, on the other hand, excel at capturing rich contextual semantics from vast amounts of text. Hybrid approaches, such as those that leverage BERT-based models for entity linking and contextual understanding within KGE systems \\cite{zhou2023}, represent a powerful synergy. This integration allows KGEs to infer meaning from textual descriptions, thereby enhancing embedding quality and addressing the cold-start problem for new entities. Surveys like \\cite{dai2020} and \\cite{cao2022} have begun to acknowledge the potential of incorporating textual information, but the current trend moves towards more sophisticated, joint learning frameworks that align and fuse representations from both modalities. A key challenge here lies in effectively bridging the gap between the discrete, symbolic nature of KGs and the continuous, contextualized space of PLMs, while managing the increased computational complexity.\n\nAnother prominent trend involves the development of more adaptive multi-curvature embeddings. As explored in the \"Geometric KGE for Hierarchical and Complex Structures\" subgroup, hyperbolic spaces have demonstrated superior capabilities for modeling hierarchical structures due to their negative curvature \\cite{pan2021, liang2024}. However, real-world knowledge graphs often exhibit a mixture of structural patternshierarchical, cyclic, and Euclidean-like. The emerging direction is to move beyond single-geometry embeddings towards models that can adaptively utilize different curvatures (e.g., hyperbolic, spherical, Euclidean) for different parts of a knowledge graph or for different types of relations. Approaches like \\cite{shang2024}, which propose mixed geometry message functions and scoring functions, exemplify this trend. By integrating information from multiple geometric spaces, these models aim to capture diverse local structures with higher fidelity and fewer dimensions, offering a more expressive and compact representation than any single geometry could provide. The methodological challenge lies in designing robust mechanisms for dynamically selecting or combining appropriate geometries, ensuring stable training, and avoiding an explosion in model complexity. Furthermore, models like \\cite{li2024} which embed entities as spheres, extend rotational embeddings to better capture many-to-many relations and enable set retrieval, showcasing the continuous innovation in geometric KGE.\n\nAdvancements in federated and privacy-preserving KGE also constitute a critical emerging trend, directly addressing the practical and ethical concerns of data distribution and privacy. The \"Federated KGE, Privacy, and Security\" subgroup highlights the growing interest in collaboratively training KGE models across distributed knowledge graphs without centralizing sensitive data. This is crucial for applications where data privacy regulations (e.g., GDPR) are paramount. Recent works focus on improving communication efficiency, such as \\cite{zhang2024} which proposes entity-wise Top-K sparsification to reduce transmitted parameters. Furthermore, addressing data heterogeneity among clients is vital, leading to personalized federated KGE approaches that learn client-specific supplementary knowledge \\cite{zhang2024}. However, this distributed paradigm introduces new security vulnerabilities, as demonstrated by poisoning attacks that can manipulate model outcomes by injecting malicious data indirectly through aggregation \\cite{zhou2024}. The trade-off between privacy guarantees, communication efficiency, personalization, and robustness against adversarial attacks remains a central challenge, requiring sophisticated cryptographic techniques and robust aggregation mechanisms.\n\nBeyond these technological advancements, crucial ethical considerations are increasingly guiding KGE research. Foremost among these is the issue of potential biases in learned representations. KGE models learn from existing knowledge graphs, which are often constructed from diverse sources reflecting historical, societal, or cultural biases. If the training data contains skewed representations (e.g., gender stereotypes, racial biases, under-representation of certain groups), the KGE model will inevitably learn and potentially amplify these biases. This can lead to discriminatory outcomes in downstream applications, such as biased recommendations or unfair decision-making in sensitive domains. While the \"Evaluation, Benchmarking, and Reproducibility\" subgroup has focused on hyperparameter effects \\cite{lloyd2022} and evaluation biases \\cite{rossi2020}, there is a growing imperative to explicitly detect, measure, and mitigate these semantic biases within the embedding space.\n\nThe responsible use of KGE in sensitive applications is another paramount ethical concern. As KGE models are increasingly deployed in high-stakes domains like healthcare (e.g., drug repurposing for COVID-19 \\cite{islam2023}), finance, and legal systems, the consequences of erroneous or biased predictions can be severe. The \"Domain-Specific Application and Explainability\" subgroup underscores the need for rigorous, domain-specific validation and explainability in such contexts. For instance, in drug repurposing, molecular evaluation is integrated to verify predictions \\cite{islam2023}. This highlights a critical shift from solely optimizing for accuracy on standard benchmarks to ensuring real-world safety, fairness, and accountability. The assumptions made during model design and the generalizability of experimental setups must be critically scrutinized, especially when findings from one domain are applied to another.\n\nFinally, the imperative for transparent and explainable AI systems is gaining traction. Complex KGE models, particularly those leveraging deep learning architectures like GNNs or Transformers, often operate as \"black boxes,\" making it difficult to understand *why* a particular prediction or recommendation was made. The \"KGE for Downstream Applications and Explainability\" subgroup directly addresses this, with models like CKGE \\cite{yang2023} and RKGE \\cite{sun2018} aiming to provide explainable recommendations through path saliency or contextualized neighbor semantics. For sensitive applications, mere performance is insufficient; users and stakeholders need to trust and verify the model's reasoning. This necessitates the development of KGE models that can provide human-understandable explanations, whether through extracting logical rules, highlighting influential paths, or visualizing attention mechanisms. The theoretical gap often lies in balancing the high expressiveness of complex models with the inherent simplicity required for genuine interpretability, presenting a continuous trade-off that future research must navigate. These emerging trends and ethical considerations collectively define the next frontier for KGE research, demanding not only technological ingenuity but also a strong commitment to societal responsibility."
  },
  "latex_document": "\\documentclass[12pt,a4paper]{article}\n    \\usepackage[utf8]{inputenc}\n    \\usepackage[T1]{fontenc}\n    \\usepackage{amsmath,amsfonts,amssymb}\n    \\usepackage{graphicx}\n    \\usepackage[margin=2.5cm]{geometry}\n    \\usepackage{setspace}\n    \\usepackage{natbib}\n    \\usepackage{url}\n    \\usepackage{hyperref}\n    \\usepackage{booktabs}\n    \\usepackage{longtable}\n    \\usepackage{array}\n    \\usepackage{multirow}\n    \\usepackage{wrapfig}\n    \\usepackage{float}\n    \\usepackage{colortbl}\n    \\usepackage{pdflscape}\n    \\usepackage{tabu}\n    \\usepackage{threeparttable}\n    \\usepackage{threeparttablex}\n    \\usepackage[normalem]{ulem}\n    \\usepackage{makecell}\n    \\usepackage{xcolor}\n\n    % Set line spacing\n    \\doublespacing\n\n    % Configure hyperref\n    \\hypersetup{\n        colorlinks=true,\n        linkcolor=blue,\n        filecolor=magenta,      \n        urlcolor=cyan,\n        citecolor=red,\n    }\n\n    % Title and author information\n    \\title{A Comprehensive Literature Review with Self-Reflection}\n    \\author{Literature Review}\n    \\date{\\today}\n\n    \\begin{document}\n\n    \\maketitle\n\n    % Abstract (optional)\n    \\begin{abstract}\n    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 377 research papers, identifying key themes, methodological approaches, and future research directions.\n    \\end{abstract}\n\n    \\newpage\n    \\tableofcontents\n    \\newpage\n\n    \\label{sec:introduction}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nThe landscape of artificial intelligence has been profoundly shaped by advancements in knowledge representation, evolving from early symbolic systems to the sophisticated structures of modern knowledge graphs (KGs). These KGs, which organize world knowledge into networks of entities and relations, have become indispensable for various AI tasks, yet their inherent symbolic nature presents significant challenges. Issues such as data sparsity, computational inefficiency in large-scale reasoning, and difficulty in capturing nuanced semantic similarities often hinder their full potential \\cite{68f34ed64fdf07bb1325097c93576658e061231e}. This introductory section establishes the foundational context for understanding Knowledge Graph Embeddings (KGEs), a paradigm-shifting approach that addresses these limitations by transforming discrete symbolic knowledge into continuous, low-dimensional vector spaces.\n\nKGE methods have emerged as a central pillar in modern knowledge graph research, enabling machines to understand, reason with, and leverage complex relational data more effectively. By converting entities and relations into dense embeddings, KGEs facilitate seamless integration with advanced machine learning models, thereby enhancing capabilities in diverse AI applications such as link prediction, entity alignment, question answering, and recommender systems. This section begins by providing a comprehensive background on knowledge graphs, tracing their evolution and highlighting their structural characteristics and inherent challenges. Subsequently, it delves into the core motivations behind the development of knowledge graph embedding techniques, explaining how they overcome the limitations of traditional symbolic representations. Finally, this section delineates the scope and organizational structure of the entire literature review, offering readers a roadmap through the intricate landscape of KGE research. By laying this essential groundwork, we aim to underscore the significance and trajectory of KGEs in transforming symbolic knowledge into actionable, machine-understandable formats, thereby advancing the broader field of artificial intelligence.\n\n\\subsection{Background: Knowledge Graphs}\n\\label{sec:1\\_1\\_background:\\_knowledge\\_graphs}\n\nKnowledge Graphs (KGs) represent a fundamental paradigm for organizing and representing world knowledge in a structured, machine-readable format. At their core, KGs are directed graphs composed of entities (nodes) and relations (edges), forming a collection of factual triplets in the form of (head entity, relation, tail entity) \\cite{ge2023, dai2020}. For instance, the triplet (Barack Obama, bornIn, Hawaii) explicitly states a factual relationship between two entities. This structured representation allows for explicit semantic connections, enabling machines to understand and process information in a manner closer to human cognition.\n\nThe historical trajectory of knowledge representation has seen a significant evolution, from early semantic networks and expert systems in artificial intelligence to the more formalized ontologies and the vision of the Semantic Web. These foundational efforts aimed to capture human knowledge in a symbolic form, providing a basis for logical reasoning and inference. With the advent of the internet and the explosion of digital information, the need for large-scale, interconnected knowledge bases became paramount. This led to the development of modern, expansive KGs such as Freebase (now largely integrated into Wikidata), DBpedia, and Wikidata itself \\cite{wang2014, lv2018, zhang2018}. These prominent examples serve as crucial repositories, aggregating and organizing vast amounts of world knowledge from diverse sources like Wikipedia, enabling a wide array of intelligent systems, from search engines and question-answering systems to recommender platforms \\cite{huang2019, sun2018}.\n\nDespite their immense utility and structured nature, symbolic KGs inherently face several significant challenges that limit their scalability, efficiency, and ability to handle real-world complexities. Firstly, reasoning with symbolic KGs, particularly when involving complex logical rules or multi-hop inference, can be computationally inefficient and resource-intensive, often exhibiting exponential complexity \\cite{ge2023, dai2020}. This makes real-time inference on large-scale KGs a formidable task. Secondly, KGs are almost always incomplete; real-world knowledge is vast and constantly evolving, making it practically impossible to explicitly represent every single fact. Symbolic methods struggle profoundly with this incompleteness, as they typically require explicit rules or complete data to infer missing links, leading to brittle and often inaccurate predictions in sparse environments. The \"data sparsity\" problem is a recurring theme, where many entities and relations have limited connections, hindering comprehensive analysis \\cite{ge2023, dai2020}.\n\nFurthermore, symbolic representations treat entities and relations as discrete, atomic tokens, which inherently limits their capacity to capture nuanced semantic similarities or implicit relationships. For example, while \"car\" and \"automobile\" are semantically very close, a purely symbolic KG would treat them as distinct, unrelated entities unless explicitly linked by a relation. This lack of inherent semantic fluidity makes it difficult to generalize knowledge or discover novel patterns based on underlying similarities. These limitationscomputational inefficiency, difficulty in managing growing data, challenges in handling incompleteness, and the inability to capture implicit semantic similaritiescollectively underscore the necessity for more advanced representation techniques. As highlighted in the broader context of knowledge graph embedding research, these issues motivate the fundamental shift towards embedding entities and relations into continuous, low-dimensional vector spaces, thereby transforming complex symbolic problems into more efficient vector operations and laying the \"bedrock for representing complex relational data in a machine-understandable format\" \\cite{cao2022}. This transition to embedding techniques is crucial for unlocking the full potential of KGs in modern AI applications, providing a robust and scalable foundation for knowledge inference and fusion \\cite{ge2023, dai2020}.\n\\subsection{Motivation for Knowledge Graph Embedding}\n\\label{sec:1\\_2\\_motivation\\_for\\_knowledge\\_graph\\_embedding}\n\nKnowledge Graphs (KGs) serve as powerful repositories of structured world knowledge, representing entities and their relationships in a symbolic, triple-based format (e.g., (subject, predicate, object)). While invaluable for many AI applications, traditional symbolic KGs inherently suffer from several critical limitations that impede their scalability, flexibility, and integration with modern machine learning paradigms. These limitations form the core motivation for the development of Knowledge Graph Embedding (KGE) techniques.\n\nFirstly, symbolic representations are inherently \\textbf{sparse and discrete}, making it challenging to capture nuanced semantic similarities between entities and relations \\cite{dai2020, cao2022}. For instance, while a symbolic KG might state (`\\texttt{Paris'', }\\texttt{locatedIn'', }\\texttt{France'') and (}\\texttt{Berlin'', }\\texttt{locatedIn'', }\\texttt{Germany''), it struggles to infer that }\\texttt{Paris'' and }\\texttt{Berlin'' are both capital cities or that }\\texttt{France'' and }`Germany'' are both European countries without explicit rules or additional facts. This sparsity also makes it difficult to generalize to unseen entities or relations, as there is no inherent notion of proximity or relatedness in the discrete symbolic space.\n\nSecondly, reasoning over large-scale symbolic KGs is often \\textbf{computationally inefficient}. As KGs grow exponentially in size, performing complex queries or multi-hop reasoning becomes computationally expensive, often requiring graph traversal algorithms that do not scale well \\cite{dai2020}. Managing and processing vast amounts of discrete data poses significant challenges, leading to bottlenecks in real-world applications.\n\nThirdly, KGs are almost always \\textbf{incomplete}, a pervasive issue that limits their utility. Many real-world facts are missing, and traditional symbolic methods struggle to infer these missing links without explicit, hand-crafted rules. This incompleteness directly impacts the performance of downstream tasks that rely on comprehensive knowledge.\n\nTo overcome these limitations, Knowledge Graph Embedding (KGE) emerged as a transformative approach, converting sparse, symbolic entities and relations into continuous, low-dimensional vector representations (embeddings) in a latent space \\cite{dai2020, cao2022}. This fundamental shift from symbolic to vector-based representation offers several profound advantages:\n\n\\begin{itemize}\n    \\item \\textbf{Scalability and Efficiency:} By representing entities and relations as dense vectors, KGE models transform complex symbolic problems into efficient vector operations, such as distance calculations or dot products. This significantly enhances computational efficiency and scalability, enabling KGs with millions of entities and relations to be processed and reasoned over more effectively. The \"Efficiency, Compression, and System Optimization\" subgroup highlights this, with works like \\cite{zhu2020} demonstrating how knowledge distillation can reduce embedding parameters by 7-15x and increase inference speed, and \\cite{wang2021} proposing lightweight frameworks for efficient storage and inference. More recently, system-level optimizations like \\cite{zheng2024} aim for general and efficient KGE learning systems, achieving significant speedups.\n    \n    \\item \\textbf{Capturing Nuanced Semantic Similarities:} In the continuous embedding space, semantically similar entities or relations are mapped to proximate vectors. This allows KGE models to inherently capture nuanced semantic relationships that are difficult to express symbolically. For instance, translational models like TransH \\cite{wang2014} and rotational models like RotatE \\cite{sun2018} learn to represent relations as transformations (translations on hyperplanes or rotations in complex space) that connect head and tail entities, thereby capturing patterns like symmetry, antisymmetry, inversion, and composition. This capability is central to the \"Core KGE Model Architectures and Expressiveness\" subgroup.\n    \n    \\item \\textbf{Handling Incompleteness and Enabling Link Prediction:} KGE models are particularly adept at addressing KG incompleteness through link prediction. By learning the underlying patterns of existing facts, they can infer the plausibility of unobserved triples. This is achieved by scoring candidate triples based on the learned embeddings, effectively transforming the problem of finding missing links into an efficient vector similarity search \\cite{rossi2020}. This capability is a cornerstone of KGE applications, as detailed in the \"Link Prediction and Knowledge Graph Completion\" section.\n    \n    \\item \\textbf{Seamless Integration with Modern Machine Learning Pipelines:} KGEs provide a powerful bridge between structured knowledge and modern machine learning (ML) techniques, especially deep learning. The continuous vector representations can be seamlessly integrated as features or pre-trained components into various ML models for tasks like natural language processing, computer vision, and recommendation systems. This allows KGs to enrich data-driven models with structured background knowledge, enhancing their performance and interpretability. The \"KGE for Downstream Applications and Explainability\" subgroup exemplifies this, with models like RKGE \\cite{sun2018} and CKGE \\cite{yang2023} leveraging embeddings for explainable recommendation, and systems like Marie and BERT \\cite{zhou2023} integrating KGEs for chemistry-specific question answering.\n    \n    \\item \\textbf{Facilitating Diverse AI Tasks:} Beyond link prediction, KGEs enable a wide array of AI tasks by converting complex symbolic problems into efficient vector operations. These include:\n    \\begin{itemize}\n        \\item \\textbf{Entity Alignment:} Identifying equivalent entities across different KGs by comparing their embeddings \\cite{sun2018, zhang2019}.\n        \\item \\textbf{Question Answering (QA):} Matching natural language questions to relevant facts in the KG by embedding both questions and KG elements into a common space \\cite{huang2019, zhou2023}.\n        \\item \\textbf{Recommendation Systems:} Modeling user-item interactions and preferences by embedding items and users within a KG context \\cite{sun2018, yang2023}.\n    \\end{itemize}\n\\end{itemize}\n\nWhile the conversion to vector space offers immense benefits, it also introduces challenges. Early KGE models, though efficient, often struggled to capture all complex relational patterns, leading to a continuous evolution towards more expressive geometric and deep learning models, as evidenced by the \"Core KGE Model Architectures\" and \"Geometric KGE\" subgroups. Furthermore, while embeddings make KGs more actionable for AI, the inherent interpretability of symbolic logic can sometimes be lost in dense vector spaces, spurring research into explainable KGEs \\cite{yang2023}. This strategic shift from explicit symbolic to implicit dense vector representations, as highlighted in the overall perspective, represents a fundamental progression in knowledge representation, making KGs more accessible, scalable, and powerful for a wide spectrum of AI applications.\n\\subsection{Scope and Structure of the Review}\n\\label{sec:1\\_3\\_scope\\_\\_and\\_\\_structure\\_of\\_the\\_review}\n\nThis literature review offers a comprehensive exploration of Knowledge Graph Embedding (KGE) research, meticulously tracing its evolution from foundational theoretical models to cutting-edge architectural advancements, critical practical considerations, and diverse real-world applications. The scope is designed to provide a pedagogical progression, beginning with core concepts and gradually building towards more sophisticated and specialized developments, thereby ensuring a coherent narrative that captures the field's dynamic trajectory. We aim to synthesize the vast landscape of KGE, offering a structured roadmap for understanding its complexities and future directions.\n\nThe review commences with an \\textbf{Introduction} (Section 1), which sets the stage by outlining the fundamental role of knowledge graphs in AI and elucidating the core motivations behind embedding them into continuous vector spaces. This initial section establishes why KGE has become indispensable for overcoming the limitations of symbolic representations, such as scalability and the inability to capture nuanced semantic similarities.\n\nFollowing this, \\textbf{Foundational KGE Models and Geometric Paradigms} (Section 2) delves into the bedrock of KGE research. This section examines early and influential models, primarily those based on geometric and algebraic principles. It discusses how relations are conceptualized as transformations within embedding spaces, detailing the progression from simple translational models like TransH \\cite{wang2014} and TransD \\cite{ji2015} to more complex rotational approaches such as RotatE \\cite{sun2018}. These foundational works, categorized in the thematic taxonomy as \"Core Translational Models and Their Extensions\" and \"Advanced Geometric Models,\" are critically analyzed for their ability to capture diverse relational patterns, including symmetry, antisymmetry, and composition, while balancing model capacity and computational efficiency. The limitations of earlier models in handling complex relation patterns often necessitated the development of more expressive geometric operations.\n\nThe review then transitions to \\textbf{Deep Learning Architectures for Knowledge Graph Embedding} (Section 3), reflecting a significant paradigm shift in the field. This section explores how Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformer models have been adapted to learn more expressive and context-aware representations. This progression highlights the move from predefined geometric transformations to data-driven feature extraction, enabling the capture of intricate structural patterns and non-linear relationships that were challenging for simpler models.\n\nBuilding upon these architectural advancements, \\textbf{Enriching KGE: Auxiliary Information, Rules, and Multi-modality} (Section 4) investigates methods that transcend purely structural information. This section details the integration of auxiliary data (e.g., entity types, attributes), explicit logical rules, and multi-modal information (e.g., text, images) to enhance embedding quality. This critical area addresses the inherent incompleteness and sparsity of KGs, demonstrating how external knowledge can lead to more robust, semantically rich, and interpretable embeddings.\n\nRecognizing the dynamic nature of real-world knowledge, \\textbf{Dynamic, Inductive, and Distributed KGE} (Section 5) focuses on models capable of handling temporal changes, learning embeddings for unseen entities, and operating in privacy-preserving, distributed environments. Models like HyTE \\cite{dasgupta2018} are crucial here, explicitly incorporating time to enable temporally aware inference. This section underscores the field's evolution towards more adaptable and scalable solutions, moving beyond static and centralized assumptions to meet the demands of evolving knowledge bases.\n\n\\textbf{Practical Considerations: Efficiency, Robustness, and Evaluation} (Section 6) addresses the critical challenges in deploying and evaluating KGE models. It covers strategies for improving computational efficiency, enhancing robustness against noisy data, and optimizing training processes. This section also critically examines the importance of rigorous evaluation, benchmarking, and reproducibility, drawing insights from comprehensive comparative analyses of state-of-the-art methods \\cite{rossi2020}. The discussion here highlights how experimental setups and reporting practices can significantly affect generalizability, emphasizing the need for standardized benchmarks and transparent methodologies to ensure reliable scientific progress. Survey papers like \\cite{dai2020} and \\cite{cao2022} further underscore the importance of systematic classification and comparison of KGE techniques based on their underlying representation spaces and performance across various benchmarks.\n\nThe review culminates with \\textbf{Applications and Real-World Impact of KGE} (Section 7), showcasing the diverse utility of KGE across various AI tasks. This includes core applications like link prediction and knowledge graph completion, as well as more complex tasks such as entity alignment \\cite{sun2018, zhang2019}, question answering \\cite{huang2019}, and recommender systems \\cite{sun2018}. This section demonstrates how KGE bridges the gap between structured knowledge and practical AI problems, providing tangible benefits in various domains. The inclusion of application-specific KGE frameworks from the taxonomy, such as KEQA \\cite{huang2019} and RKGE \\cite{sun2018}, illustrates the versatility and real-world impact of these embedding techniques.\n\nFinally, the \\textbf{Conclusion and Future Directions} (Section 8) synthesizes the key developments, identifies persistent open challenges, theoretical gaps, and practical limitations, and outlines emerging trends and ethical considerations. This forward-looking perspective aims to inspire new research and guide the responsible advancement of KGE technologies, providing a comprehensive roadmap for navigating the complex and rapidly evolving landscape of knowledge graph embedding research.\n\n\n\\label{sec:foundational_kge_models_and_geometric_paradigms}\n\n\\label{sec:foundational\\_kge\\_models\\_\\_and\\_\\_geometric\\_paradigms}\n\n\\section{Foundational KGE Models and Geometric Paradigms}\n\\label{sec:foundational\\_kge\\_models\\_and\\_geometric\\_paradigms}\n\nBuilding upon the motivation to overcome the limitations of sparse symbolic knowledge graphs, this section delves into the pioneering efforts that established the bedrock of knowledge graph embedding research. It explores the early and influential models that first translated entities and relations into continuous vector spaces, laying the theoretical and practical groundwork for subsequent advancements. The primary focus here is on geometric and algebraic paradigms, which conceptualize relations not merely as static links, but as dynamic transformations or interactions within these learned embedding spaces. This fundamental shift enabled the capture of implicit semantic similarities and relational patterns that were previously inaccessible.\n\nThe evolution began with simple yet powerful translational models, such as TransE and its extensions like TransH \\cite{wang2014} and TransD \\cite{ji2015}. These models represent relations as direct translations from head to tail entities, offering an efficient way to capture basic relational patterns. However, their inherent limitations in modeling complex properties like symmetry, antisymmetry, and composition spurred the development of more sophisticated approaches. This led to the emergence of rotational models, exemplified by RotatE \\cite{sun2018}, which leverage rotations in complex or higher-dimensional spaces to capture richer and more diverse relational semantics. Further innovations explored other geometric and algebraic structures, including embeddings on Lie groups or using quaternions, to enhance expressiveness and address specific challenges. Collectively, these foundational models were instrumental in demonstrating how geometric operations could effectively capture intricate relational patterns, significantly improving the expressiveness and utility of KGEs and forming the essential basis for the field's rapid expansion into more advanced architectures.\n\n\\subsection{Core Translational Models and Extensions}\n\\label{sec:2\\_1\\_core\\_translational\\_models\\_\\_and\\_\\_extensions}\n\nThe advent of knowledge graph embedding (KGE) marked a significant paradigm shift from purely symbolic knowledge representation to continuous vector spaces, offering enhanced efficiency and expressiveness for tasks such as link prediction and knowledge graph completion. At the forefront of this transformation were the translational models, which posited that a relation could be represented as a translation operation in an embedding space, moving a head entity vector closer to a tail entity vector. This foundational idea was first popularized by TransE, a pioneering model for its simplicity and computational efficiency \\cite{wang2014}. TransE models a triple $(h, r, t)$ by enforcing the constraint $\\mathbf{h} + \\mathbf{r} \\approx \\mathbf{t}$, where $\\mathbf{h}, \\mathbf{r}, \\mathbf{t}$ are the embeddings of the head entity, relation, and tail entity, respectively. While remarkably effective for its time, TransE exhibited limitations in handling complex relational patterns, particularly one-to-many, many-to-one, and reflexive relations, where a single relation vector could not adequately distinguish between multiple valid tail entities for a given head, or vice-versa.\n\nTo address these inherent limitations, subsequent models extended the translational paradigm by introducing more sophisticated mechanisms. TransH emerged as a notable improvement, proposing to model relations as translations on relation-specific hyperplanes rather than directly in the entity embedding space \\cite{wang2014}. Specifically, for a triple $(h, r, t)$, TransH projects the head and tail entity embeddings ($\\mathbf{h}, \\mathbf{t}$) onto a hyperplane defined by the relation $\\mathbf{r}$'s normal vector $\\mathbf{w}\\_r$, resulting in projected entities $\\mathbf{h}\\_{\\perp}$ and $\\mathbf{t}\\_{\\perp}$. The translational assumption then applies to these projected vectors: $\\mathbf{h}\\_{\\perp} + \\mathbf{d}\\_r \\approx \\mathbf{t}\\_{\\perp}$, where $\\mathbf{d}\\_r$ is the relation-specific translation vector on the hyperplane. This mechanism allows TransH to better distinguish entities involved in one-to-many or many-to-one relations, as different entity pairs can be projected onto different points on the hyperplane while sharing the same relation vector. For instance, if a person has multiple children, TransH can project the person and each child onto the 'has\\\\_child' hyperplane, allowing distinct representations for each child while maintaining the 'has\\\\_child' relation. This approach offered a crucial trade-off, significantly improving expressiveness for complex relation types with almost the same model complexity as TransE, thereby maintaining scalability \\cite{wang2014}. The recent review by \\cite{asmara2023} further underscores TransH's importance in addressing these early challenges.\n\nBuilding upon the concept of relation-specific transformations, TransD further refined the translational approach by introducing dynamic mapping matrices for entities and relations \\cite{ji2015}. Unlike TransH, which uses a single hyperplane per relation, TransD employs two vectors for each entity and relation: one representing its meaning and another for constructing a dynamic mapping matrix. This allows for more fine-grained, entity-specific projections, where the projection matrix for a relation is dynamically constructed based on both the entity and relation vectors. The core idea is that different entities might interact with a relation in different ways, and a static projection (as in TransH) might not capture this diversity. TransD's dynamic mapping matrices provide a more adaptive mechanism to project entities into relation-specific spaces, thereby accounting for the diversity of both relations and entities. A significant advantage of TransD over its predecessors like TransR/CTransR (which used static, larger projection matrices) is its reduced parameter count and avoidance of computationally intensive matrix-vector multiplication operations, making it more scalable for large knowledge graphs \\cite{ji2015}. This efficiency gain, while increasing expressiveness, was a critical step in making KGE models practical for real-world applications.\n\nThese core translational models and their extensions collectively established a fundamental paradigm for KGE. They demonstrated that representing symbolic knowledge in continuous vector spaces could not only be efficient but also expressive enough to capture intricate relational semantics. While TransH and TransD significantly improved upon TransE's ability to model one-to-many/many-to-one relations, they still operated within the limitations of a Euclidean embedding space and relatively simple geometric transformations. This inherent simplicity, while beneficial for efficiency, meant they struggled with more complex logical patterns such as symmetry, antisymmetry, inversion, and composition, which later models like RotatE would address more elegantly through rotational transformations in complex spaces \\cite{sun2018}. Nevertheless, the foundational work of TransE, TransH, and TransD laid the essential groundwork, proving the viability of the embedding approach and setting the stage for the diverse array of KGE models that continue to influence modern research, as highlighted in various surveys \\cite{dai2020, cao2022}. Their emphasis on balancing model capacity with computational efficiency remains a crucial design principle in the field.\n\\subsection{Rotational and Complex Space Embeddings}\n\\label{sec:2\\_2\\_rotational\\_\\_and\\_\\_complex\\_space\\_embeddings}\n\nWhile foundational translational models like TransE and TransH \\cite{wang2014} offered a significant step forward in knowledge graph embedding (KGE) by modeling relations as vector translations, they often struggled with capturing the full spectrum of complex relational semantics, such as symmetry, antisymmetry, inversion, and composition \\cite{rossi2020}. This limitation spurred the development of models that leverage rotations in complex or higher-dimensional spaces, offering more nuanced and powerful transformations to represent these intricate logical patterns. This represents a key methodological shift within the \"Core KGE Model Architectures and Expressiveness\" and \"Geometric and Algebraic KGE Models for Complex Relations\" subgroups, moving beyond simpler linear operations to richer algebraic structures.\n\nA seminal contribution in this direction is RotatE \\cite{sun2018}, which defines each relation as a rotation from the head entity to the tail entity in a complex vector space. By representing entities as vectors and relations as Hadamard products with complex-valued relation vectors (which correspond to rotations), RotatE inherently captures symmetry (rotation by $\\pi$), antisymmetry (rotation by non-$\\pi$ angles), inversion (rotation by negative angle), and composition (sequential rotations). This elegant formulation proved highly effective for modeling complex patterns and significantly outperformed existing state-of-the-art models for link prediction on benchmark datasets \\cite{sun2018}. The success of RotatE highlighted the expressive power of complex space embeddings, demonstrating how algebraic structures could directly encode logical properties.\n\nBuilding upon this rotational paradigm, researchers explored extensions to higher-dimensional Euclidean and non-Euclidean spaces. Rotate3D \\cite{gao2020} extends the concept of relations as rotations to a three-dimensional Euclidean space. A key motivation for Rotate3D was to capture non-commutative composition patterns, which are essential for multi-hop reasoning and are naturally supported by rotations in 3D space. While RotatE primarily operates in a 2D complex plane for each dimension, Rotate3D generalizes this to a full 3D rotation, allowing for a richer set of transformations. Similarly, Orthogonal Relation Transforms \\cite{tang2019} further generalize this idea by employing high-dimensional orthogonal transforms, which encompass rotations and reflections, to model relations. This approach aims to retain the benefits of rotational models (symmetry, inversion, composition) while enhancing modeling capacity for complex relations like N-to-1 and 1-to-N by integrating graph context.\n\nThe pursuit of even more expressive geometric transformations led to models like HousE \\cite{li2022}, which introduces Householder parameterization. Householder transformations, a type of reflection, can be generalized to represent rotations and projections in high-dimensional spaces. HousE aims to simultaneously capture crucial relation patterns and mapping properties, theoretically generalizing existing rotation-based models while extending rotations to higher dimensions. This exemplifies the continuous effort to find more powerful mathematical tools to encode relational semantics.\n\nFurther enhancing the complexity of transformations, CompoundE \\cite{ge2022} and its 3D extension, CompoundE3D \\cite{ge2023}, propose using compound geometric operations, including translation, rotation, and scaling. These models treat relations not as a single operation but as a cascade of multiple transformations, suggesting that a richer set of combined operations can lead to better modeling capacity. CompoundE, by framing itself within group theory, demonstrates that several existing KGE models are special cases of its generalized framework, highlighting a trend towards unifying diverse geometric approaches. While these compound operations offer increased expressiveness, they also introduce greater model complexity and potentially higher computational costs, a common trade-off in KGE research \\cite{cao2022}.\n\nA significant recent development is the use of quaternions, an extension of complex numbers to four dimensions, to represent relations. ConQuatE \\cite{chen2025} leverages quaternion rotations to address the challenge of polysemy in knowledge graphs, where entities can exhibit different semantic characteristics depending on the relation. By incorporating contextual cues from various connected relations through efficient vector transformations in quaternion space, ConQuatE aims to capture diverse relational contexts without requiring extra information beyond original triples. This approach offers a novel way to handle the nuanced semantic variations that simpler rotational models might overlook, particularly for link prediction and multihop reasoning.\n\nThe \"Geometric and Algebraic KGE Models for Complex Relations\" subgroup analysis highlights that while these models achieve state-of-the-art performance, a common limitation is the potential for increased computational cost and parameter count, which can affect scalability to extremely large KGs. Moreover, the empirical validation often relies heavily on standard link prediction metrics, which may not fully capture the nuances of all the complex patterns these models aim to capture, especially for tasks like set retrieval or complex logical reasoning. For instance, MQuinE \\cite{liu2024} identifies and addresses a theoretical deficiency, termed the \"Z-paradox,\" in some popular KGE models, demonstrating that even advanced models can suffer from subtle expressiveness issues that degrade performance on challenging test samples. This underscores that merely introducing complex operations is insufficient; theoretical soundness and complete expressiveness are paramount.\n\nThe intellectual trajectory in this area, as noted in the \"Core KGE Model Architectures and Expressiveness\" subgroup, shows a clear progression from specific rotational models to more theoretically grounded and generalized orthogonal transformations. HolmE \\cite{zheng2024} introduces a KGE model whose relation embedding space is \"closed under composition,\" a crucial property for inherently modeling under-represented (long-tail) composition patterns and extrapolating to unseen relations. This addresses a theoretical gap where prior KGEs often considered relations compositional only if well-represented in training data. Similarly, GoldE \\cite{li2024} proposes a universal orthogonal parameterization based on a generalized Householder reflection, aiming to unify dimensional extension and geometric unification with theoretical guarantees, thereby capturing both logical patterns and topological heterogeneity. SpherE \\cite{li2024} further extends rotational embeddings by representing entities as spheres instead of vectors, specifically targeting the challenging problem of set retrieval and many-to-many relations, while maintaining interpretability. These advancements demonstrate a continuous drive to enhance the fundamental expressiveness of KGE models, ensuring they can effectively handle the complexities and imperfections of real-world knowledge graphs.\n\\subsection{Other Geometric and Algebraic Innovations}\n\\label{sec:2\\_3\\_other\\_geometric\\_\\_and\\_\\_algebraic\\_innovations}\n\nBeyond the foundational translational and rotational paradigms, Knowledge Graph Embedding (KGE) research has continuously sought to refine its mathematical underpinnings by exploring a broader spectrum of geometric spaces and algebraic transformations. This pursuit is driven by the need for more expressive, theoretically sound, and robust representations capable of capturing the intricate nuances of real-world knowledge graphs.\n\nOne significant direction involves embedding entities and relations within non-Euclidean spaces, particularly Lie groups, to circumvent inherent limitations of standard vector spaces. \\cite{ebisu2017} introduced \\textbf{TorusE}, a pioneering model that embeds entities on a Lie group, specifically a torus. The primary motivation for TorusE was to address the regularization problems encountered by models like TransE, where forcing entity embeddings onto a sphere in Euclidean space could warp representations and adversely affect link prediction accuracy. By leveraging the compact nature of a torus, TorusE naturally avoids the need for explicit regularization, as the space itself is bounded. While innovative in its choice of embedding space, TorusE still adheres to a translation-like principle, defining relations as translations within the Lie group. However, its geometric complexity, while elegant, might not inherently capture all forms of complex relation patterns, such as compositionality, as effectively as models designed with specific algebraic operations for such patterns.\n\nAnother critical area of innovation lies in scrutinizing and redefining the metric used within the embedding space. \\cite{yang2021} presented \\textbf{CyclE}, which critically examines the implications of the widely adopted Minkowski metric in KGE. The authors argue that the choice of metric significantly influences the expressiveness of the embedding space and propose a novel \"Cycle metric\" based on the oscillation property of periodic functions. Their quantitative analysis suggests that a smaller function period in the Cycle metric leads to superior expressive ability. CyclE, by combining this new metric with popular KGE models, demonstrated enhanced performance. This work highlights a fundamental aspect of geometric KGE: the distance function itself is a crucial design choice. However, focusing solely on the metric, while foundational, may not inherently provide the rich \\textit{transformations} required to model complex logical patterns like transitivity or hierarchy without further architectural or operational enhancements.\n\nA more direct approach to enhancing modeling capacity involves introducing advanced algebraic transformations. \\cite{li2022} proposed \\textbf{HousE}, a powerful KGE framework that leverages Householder parameterization. HousE employs two types of Householder transformations: Householder rotations to achieve superior capacity for modeling relation patterns and Householder projections to handle sophisticated relation mapping properties (e.g., 1-to-N, N-to-1). Theoretically, HousE is capable of simultaneously modeling crucial relation patterns and mapping properties, and it generalizes existing rotation-based models by extending rotations to high-dimensional spaces. Empirically, HousE achieved state-of-the-art performance on several benchmarks, indicating its enhanced expressiveness compared to simpler rotation-based models like \\cite{gao2020} Rotate3D or \\cite{tang2019} Orthogonal Relation Transforms. The strength of HousE lies in its mathematically robust and versatile transformations, offering a richer set of operations than basic rotations.\n\nBuilding upon the idea of combining multiple operations, \\cite{ge2022} introduced \\textbf{CompoundE}, which integrates translation, rotation, and scaling operations into a cascaded compound transformation. This model views relations as complex geometric manipulations, demonstrating that a synergy of these operations can lead to highly expressive embeddings. Further extending this concept, \\cite{ge2023} developed \\textbf{CompoundE3D}, which leverages 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear. CompoundE3D offers multiple design variants, allowing for flexibility to match the rich underlying characteristics of diverse knowledge graphs. These compound operation models represent a significant evolutionary step, as they generalize many existing scoring-function-based KGE models as special cases, effectively encompassing the strengths of both translational and rotational approaches while adding further dimensions of transformation. This contrasts with models like \\cite{yang2019} TransMS, which focuses on multidirectional semantics within a translation framework, or \\cite{peng2020} LineaRE, which models relations as simple linear functions.\n\nOther notable algebraic innovations include \\cite{song2021} \\textbf{Rot-Pro}, which combines projection and relational rotation to specifically model transitivity, a common but challenging relation pattern. \\cite{zhang2022} \\textbf{TranS} introduces synthetic relation representations within transition-based frameworks to better handle complex scenarios where the same entity pair might have different relations. More recently, \\cite{liu2024} proposed \\textbf{MQuinE}, which directly addresses and cures a theoretical deficiency termed the \"Z-paradox\" in some popular KGE models, thereby ensuring stronger expressiveness and theoretical soundness. The use of advanced algebraic structures is further exemplified by \\cite{chen2025} \\textbf{ConQuatE}, which leverages quaternion rotations to capture diverse relational contexts and address the polysemy issue, where entities exhibit different semantic characteristics depending on the relation.\n\nCollectively, these innovations highlight a continuous intellectual trajectory in KGE research: moving from simpler, single-operation models to more complex, multi-operation, multi-dimensional, and non-Euclidean spaces. This evolution is driven by the relentless quest to capture increasingly complex and nuanced relational patterns, thereby enhancing model expressiveness and theoretical rigor. While these models achieve state-of-the-art performance on various benchmarks, they often introduce increased mathematical complexity and computational costs, which can impact scalability, especially for extremely large knowledge graphs. Furthermore, the theoretical elegance of these geometric and algebraic models, while appealing, sometimes comes at the cost of interpretability, making it challenging to fully understand \\textit{why} certain transformations are optimal for specific relation patterns. The choice of the \"best\" geometric space or transformation remains highly dependent on the specific characteristics of the knowledge graph and the types of relations it contains.\n\n\n\\label{sec:deep_learning_architectures_for_knowledge_graph_embedding}\n\n\\label{sec:deep\\_learning\\_architectures\\_for\\_knowledge\\_graph\\_embedding}\n\n\\section{Deep Learning Architectures for Knowledge Graph Embedding}\n\\label{sec:deep\\_learning\\_architectures\\_for\\_knowledge\\_graph\\_embedding}\n\nBuilding upon the foundational geometric and algebraic paradigms discussed in Section \\ref{sec:foundational\\_kge\\_models\\_and\\_geometric\\_paradigms}, which established the initial framework for representing knowledge in continuous vector spaces, this section explores a significant paradigm shift in Knowledge Graph Embedding (KGE) research. While earlier geometric models, such as translational and rotational approaches, provided valuable insights into capturing specific relational patterns, their reliance on predefined transformations often limited their capacity to model the highly intricate, hierarchical, and non-linear relationships pervasive in real-world knowledge graphs \\cite{rossi2020, cao2022}. The advent of deep learning has revolutionized this landscape, enabling the development of more expressive and context-aware KGE models by allowing them to automatically extract features and learn complex interactions directly from data.\n\nThis section delves into how advanced deep learning architectures have been adapted to overcome these limitations, pushing the boundaries of KGE performance. We will detail the application of Convolutional Neural Networks (CNNs), which are leveraged to capture local features and intricate structural patterns within triplets and their neighborhoods. Subsequently, we examine Graph Neural Networks (GNNs), including various attention mechanisms, which inherently excel at encoding rich structural information and neighborhood context through message passing, thereby learning more robust, context-dependent embeddings. Finally, we explore the emergence of Transformer models in KGE, demonstrating how their powerful self-attention mechanisms capture long-range dependencies and contextualized representations, enabling the modeling of both global and local semantic relationships. This architectural evolution marks a crucial advancement, facilitating the learning of nuanced, non-linear relationships and paving the way for significantly enhanced performance across a spectrum of downstream AI tasks \\cite{dai2020, cao2022}.\n\n\\subsection{Convolutional Neural Networks (CNNs) for KGE}\n\\label{sec:3\\_1\\_convolutional\\_neural\\_networks\\_(cnns)\\_for\\_kge}\n\nConvolutional Neural Networks (CNNs) have emerged as a powerful paradigm in Knowledge Graph Embedding (KGE), offering a distinct advantage over traditional geometric models by automatically extracting local features and modeling intricate, non-linear interactions between entity and relation embeddings. Unlike models that rely on predefined geometric transformations, CNNs learn complex patterns directly from the data, enabling them to capture nuanced relational semantics that are often challenging for simpler approaches \\cite{cao2022}. This shift represents a significant evolution in KGE, moving towards more expressive and data-driven architectures.\n\nEarly applications of CNNs in KGE, such as AcrE \\cite{ren2020} and M-DCN \\cite{zhang2020}, demonstrated their capability to enhance link prediction. AcrE, for instance, introduced atrous convolutions and residual learning to effectively increase feature interactions while maintaining a simpler structure and higher parameter efficiency. This approach addressed the limitation of conventional models in capturing diverse relation patterns by allowing for a broader receptive field without increasing the number of parameters. M-DCN further advanced this by proposing a multi-scale dynamic convolutional network, utilizing dynamic filters to extract richer and more expressive feature embeddings. M-DCN was particularly designed to handle complex relation patterns like 1-to-N, N-to-1, and N-to-N, which often pose significant challenges for translation-based or simple semantic matching models \\cite{ge2023}. The dynamic nature of its filters, tailored to each relation, allowed for a more adaptive modeling of these complex interactions.\n\nThe integration of attention mechanisms further refined CNN-based KGE models. ReInceptionE \\cite{xie2020} exemplified this by combining an Inception network with a relation-aware attention mechanism. The Inception network was employed to increase interactions between head and relation embeddings, while the attention mechanism enriched these embeddings with joint local and global structural information. This allowed ReInceptionE to adaptively utilize neighborhood context, a capability that purely convolutional models might partially miss, thereby bridging the gap between local feature extraction and broader graph topology awareness. This approach highlights an evolutionary trend within the \"Deep Learning Architectures for KGE\" subgroup, where models increasingly seek to combine the strengths of different neural components to capture a more comprehensive view of the knowledge graph.\n\nMore recent works continue to refine CNN-based techniques for KGE. CNN-ECFA \\cite{hu2024} introduced a Convolutional Neural Network-based Entity-specific Common Feature Aggregation strategy, aiming to improve knowledge graph representation learning by leveraging common features that are specific to entities. This model demonstrates that by aggregating entity-specific features, CNNs can learn more effective representations, outperforming state-of-the-art feature projection strategies. Similarly, SEConv \\cite{yang2025} proposed a semantic-enhanced KGE model, incorporating a less resource-consuming self-attention mechanism alongside a multi-layer CNN. The multi-layer CNN in SEConv is specifically designed to learn deeper structural features from triplets, while self-attention generates more expressive embedding representations. This model, with its application focus on healthcare prediction, underscores the practical utility of CNNs in learning discriminative features for specialized domains.\n\nA key strength of CNN-based KGE models lies in their ability to automatically discover intricate, non-linear feature interactions, which contrasts with the hand-crafted transformations of geometric models (e.g., TransD \\cite{ji2015} or TorusE \\cite{ebisu2017}). This automatic feature learning often leads to superior performance in link prediction tasks, achieving state-of-the-art results on various benchmarks. However, this expressiveness comes with trade-offs. CNN models typically involve higher computational complexity and a larger number of parameters compared to simpler geometric models, potentially impacting scalability for extremely large knowledge graphs. Furthermore, while they excel at capturing local patterns, their ability to model long-range dependencies or global graph structures might be less direct than Graph Neural Networks (GNNs) or Transformer-based models, which are inherently designed for such tasks. The development trajectory of CNNs in KGE shows a clear progression from basic convolutional operations to more sophisticated designs incorporating multi-scale processing, dynamic filters, and attention, continually pushing the boundaries of what can be learned from entity-relation interactions.\n\\subsection{Graph Neural Networks (GNNs) and Attention Mechanisms}\n\\label{sec:3\\_2\\_graph\\_neural\\_networks\\_(gnns)\\_\\_and\\_\\_attention\\_mechanisms}\n\nThe integration of Graph Neural Networks (GNNs) and attention mechanisms represents a significant advancement in Knowledge Graph Embedding (KGE), moving beyond simple triplet-based interactions to leverage the rich topological and relational context of knowledge graphs. GNNs, through their inherent message passing and aggregation mechanisms, are uniquely suited to capture structural information and neighborhood context, which is crucial for understanding complex relational patterns and inferring missing links. This paradigm shift enables KGE models to learn richer, context-dependent embeddings by explicitly modeling multi-hop relational paths and local graph structures.\n\nEarly explorations into inductive capabilities for KGE, a key advantage of GNNs, were demonstrated by models like Logic Attention-based Neighborhood Aggregation (LAN) \\cite{wang2018}. LAN addressed the challenges of unordered and unequal neighbors by introducing a novel aggregator that uses both rule- and network-based attention weights. This allowed for the inductive embedding of new entities by aggregating information from their existing neighbors, a crucial step towards handling the dynamic nature of real-world knowledge graphs where new entities frequently emerge. However, while LAN provided a foundational approach to inductive learning, its attention mechanism was relatively simple and might not fully capture the nuanced importance of different relational paths.\n\nBuilding upon the strengths of GNNs, Graph Attenuated Attention Networks (GAATs) \\cite{wang2020} further refined the use of attention. GAATs incorporated an attenuated attention mechanism to assign varying weights to different relation paths within the knowledge graph, thereby acquiring more informative features from neighbor nodes. This approach recognized that not all paths or neighbors contribute equally to an entity's representation, and by attenuating less relevant information, GAATs could learn more discriminative embeddings. This marked an improvement over uniform aggregation strategies, allowing entities and relations to be learned within any neighborhood context, enriching the feature extraction process.\n\nA more sophisticated approach to GNN-based KGE is seen in DisenKGAT \\cite{wu2021}, which introduced a novel Disentangled Graph Attention Network. DisenKGAT leverages both micro-disentanglement and macro-disentanglement to learn diverse and independent component representations. Micro-disentanglement is achieved through a relation-aware aggregation mechanism that generates varied component representations, while macro-disentanglement uses mutual information as a regularization to enhance the independence of these components. This disentangled approach allows the model to generate adaptive representations based on the given scenario, thereby capturing more diverse and nuanced semantics behind complex relations. DisenKGAT's ability to produce adaptive and explainable representations showcases a significant strength, addressing the limitation of single, static representations in traditional KGE models.\n\nThe ongoing research in GNNs for KGE also includes efforts to optimize their design and understand their generalization capabilities. For instance, \\cite{di2023} proposed a Message Function Search for KGE, aiming to automatically discover suitable GNN message functions for various KG forms (e.g., n-ary, hyper-relational data). This highlights a meta-level approach to GNN design, seeking to overcome the limitations of fixed GNN architectures by adapting them to specific data characteristics. Similarly, \\cite{li2021} delved into understanding \\textit{how} KGE models, particularly GNN-based ones, extrapolate to unseen data, proposing \"Semantic Evidences\" and introducing SE-GNN to explicitly model and merge these evidences for improved inductive capabilities.\n\nWhile GNNs and attention mechanisms significantly enhance KGE by capturing complex structural and contextual information, they are not without limitations. A primary concern is their computational complexity and scalability, especially for very large knowledge graphs, as message passing can become resource-intensive. Furthermore, deep GNNs can suffer from over-smoothing, where entity representations become indistinguishable after many layers of aggregation, diminishing their discriminative power. The effectiveness of these models also heavily relies on the quality and density of local neighborhood information; sparse neighborhoods can limit their ability to learn rich contextual embeddings. Despite these challenges, the continuous development of more efficient GNN architectures, such as those explored in message function search \\cite{di2023}, and a deeper understanding of their generalization properties \\cite{li2021}, indicates a strong future for GNNs and attention mechanisms in KGE, pushing towards more robust and context-aware knowledge representation.\n\\subsection{Transformer-based KGE Models}\n\\label{sec:3\\_3\\_transformer-based\\_kge\\_models}\n\nThe emergence of Transformer architectures, initially lauded for their unparalleled success in natural language processing, has profoundly influenced the landscape of knowledge graph embedding (KGE). These models, characterized by their self-attention mechanisms, offer a powerful paradigm for capturing long-range dependencies and generating highly contextualized representations, capabilities that were often limited in earlier KGE approaches. The adaptation of Transformers to knowledge graphs represents a significant methodological evolution within the \"Deep Learning Architectures for KGE\" subgroup, pushing the boundaries of expressiveness beyond traditional geometric or simpler neural network models.\n\nEarly adaptations of Transformers to KGE primarily treated knowledge graphs as sequences of entities and relations. A pioneering example is CoKE (Contextualized Knowledge Graph Embedding) \\cite{wang2019}, which frames edges and paths within a KG as sequences. By feeding these sequences into a Transformer encoder, CoKE learns dynamic, flexible, and fully contextualized embeddings for entities and relations. This approach marked a critical shift from static, context-independent embeddings, allowing the model to capture varying properties of entities and relations based on their surrounding graph context. While CoKE demonstrated superior performance in link prediction and path query answering, its sequence-centric view inherently grappled with the graph's non-sequential nature, potentially overlooking intricate topological structures that are not easily linearized. This limitation highlights a theoretical gap: how to reconcile the order-invariance of self-attention with the directed, ordered nature of relational facts in KGs.\n\nSubsequent research has focused on explicitly integrating graph structures into Transformer frameworks, moving beyond simple sequence linearization. Knowformer \\cite{li2023} directly addresses the challenge of order invariance inherent in the self-attention mechanism, which struggles to distinguish between a valid triplet (subject, relation, object) and its shuffled variants. Knowformer innovatively incorporates relational compositions into entity representations, explicitly injecting semantics and capturing the role of an entity based on its position (subject or object) within a relation triplet. This design choice allows the Transformer to correctly capture relational semantics by distinguishing entity roles, a crucial advancement for modeling complex relational patterns that simpler translation-based models like TransD \\cite{ji2015} or even rotational models like RotatE \\cite{sun2018} might struggle to fully express without explicit positional encoding. Knowformer's ability to integrate positional awareness and relational semantics directly into the self-attention mechanism significantly enhances its expressiveness, particularly for modeling complex contextual information.\n\nThe latest advancements, such as TGformer \\cite{shi2025}, further refine the integration of Transformer architectures with graph structures, proposing a general graph Transformer framework for KGE. TGformer is notable for being the first to explicitly leverage a graph Transformer to build knowledge embeddings that incorporate both triplet-level and graph-level structural features. This comprehensive approach addresses a critical limitation of previous methods: triplet-based models often ignore the broader graph structure, while some graph-based methods (e.g., certain GNNs like DisenKGAT \\cite{wu2021}) might overlook the specific contextual information of individual nodes within a triplet. By constructing context-level subgraphs for each predicted triplet and employing a Knowledge Graph Transformer Network (KGTN), TGformer fully explores multi-structural features, boosting the model's understanding of entities and relations in diverse contexts. Furthermore, TGformer extends its capabilities to temporal knowledge graphs, a significant step towards handling the dynamic nature of real-world knowledge, aligning with the broader development direction of building \"more powerful and comprehensive models that leverage advanced neural architectures to capture increasingly complex structural and contextual information, including temporal dynamics.\"\n\nThe primary strength of Transformer-based KGE models lies in their ability to capture global and local semantic relationships through self-attention, leading to highly contextualized representations. This contrasts with CNN-based KGE models like AcrE \\cite{ren2020} or ReInceptionE \\cite{xie2020}, which excel at extracting local features and interactions but may require additional mechanisms to capture long-range dependencies effectively. While CNN-ECFA \\cite{hu2024} and SEConv \\cite{yang2025} demonstrate the continued refinement of CNNs for feature aggregation, Transformers offer a more inherent capability for global context. Compared to foundational models like TorusE \\cite{ebisu2017} or CyclE \\cite{yang2021} that focus on refining the embedding space's geometry, Transformers provide a data-driven approach to learn complex, non-linear transformations and interactions, often achieving state-of-the-art performance in link prediction.\n\nHowever, Transformer-based models also present trade-offs. Their computational complexity and high parameter count can pose scalability challenges for extremely large knowledge graphs, a practical constraint that needs careful consideration. While models like Knowformer address the initial order-invariance issue, the fundamental assumption of treating graph elements as sequences, even with sophisticated positional encodings, can sometimes be an oversimplification of the rich, multi-relational graph topology. The experimental setups for these models typically involve standard benchmark datasets, and while results are often superior, the generalizability to highly sparse or domain-specific KGs with limited data might still be a concern, requiring extensive pre-training or specialized fine-tuning. Despite these challenges, the innovative application of Transformers to graph structures, particularly in integrating multi-structural features and handling temporal dynamics, signifies a robust and adaptive direction for KGE research, continually pushing the state-of-the-art in capturing the intricate semantics of knowledge graphs.\n\n\n\\label{sec:enriching_kge:_auxiliary_information,_rules,_and_multi-modality}\n\n\\label{sec:enriching\\_kge:\\_auxiliary\\_information,\\_rules,\\_\\_and\\_\\_multi-modality}\n\n\\section{Enriching KGE: Auxiliary Information, Rules, and Multi-modality}\n\\label{sec:enriching\\_kge\\_auxiliary\\_information\\_rules\\_and\\_multi\\_modality}\n\nBuilding upon the advancements in deep learning architectures for Knowledge Graph Embedding (KGE) discussed in Section \\ref{sec:deep\\_learning\\_architectures\\_for\\_knowledge\\_graph\\_embedding}, which primarily focused on leveraging structural patterns within the graph, this section explores a crucial paradigm shift: enriching KGE models by integrating diverse external knowledge sources and logical constraints. While deep learning models like GNNs and Transformers have significantly enhanced the capture of intricate structural and contextual relationships, purely structural information often proves insufficient in addressing challenges such as data sparsity, ambiguity, and the need for more robust reasoning capabilities in complex, real-world scenarios \\cite{general\\_kge\\_review\\_1}.\n\nThis section delves into advanced KGE approaches that move beyond the confines of graph topology alone, aiming to provide a more comprehensive and nuanced representation of knowledge. We will first examine how auxiliary information, such as entity types and attributes, can be seamlessly incorporated to provide semantic guidance, thereby improving embedding quality and robustness, particularly for incomplete or noisy knowledge graphs. Subsequently, the discussion will shift to the integration of explicit logical rules and constraints, which inject prior knowledge into the embedding process, enhancing reasoning capabilities and model interpretability by ensuring learned representations adhere to logical patterns. Finally, we explore the burgeoning field of multi-modal KGE, detailing how information from diverse modalities, including textual descriptions and visual features, can be leveraged to overcome data sparsity and enrich semantic understanding, enabling more holistic knowledge representation \\cite{general\\_kge\\_review\\_2}. By exploring these complementary dimensions, this section highlights how KGE models can achieve superior performance, interpretability, and applicability in complex AI tasks.\n\n\\subsection{Incorporating Auxiliary Information (Types, Attributes)}\n\\label{sec:4\\_1\\_incorporating\\_auxiliary\\_information\\_(types,\\_attributes)}\n\nThe effectiveness of Knowledge Graph Embedding (KGE) models, while primarily driven by structural information, can be significantly enhanced by integrating auxiliary semantic information such as entity types and attributes. This approach moves beyond the simplistic triplet structure, grounding embeddings in a richer context to yield more semantic, discriminative, and robust representations, particularly crucial when dealing with incomplete or noisy knowledge graphs (KGs). The intellectual trajectory in this area reflects a growing recognition that external, well-structured knowledge can bridge gaps that purely structural models cannot, contributing to the development of more inherently capable KGE models.\n\nEarly efforts to incorporate auxiliary information often focused on entity types. \\cite{wang2021} proposed \\textit{TransET}, a novel KGE model that leverages entity types to learn more semantic features. By utilizing circle convolution based on entity and entity type embeddings, TransET maps head and tail entities to type-specific representations, which are then used in a translation-based scoring function. This method demonstrated that even a relatively straightforward integration of type information could lead to improved performance in link prediction and triple classification tasks. Building on this, \\cite{he2023} introduced \\textit{TaKE}, a more universal \\textit{Type-augmented Knowledge graph Embedding framework}. TaKE distinguishes itself by automatically capturing type features without explicit supervision and learning relation-specific type representations, allowing for a nuanced understanding of how entity types interact with different relations. Furthermore, TaKE incorporates a type-constrained negative sampling strategy, which is critical for constructing more effective negative samples during training, a fundamental aspect for KGE robustness \\cite{sachan2020}. While TransET provided a specific model, TaKE offers a generalizable framework that can enhance various traditional KGE models, showcasing a methodological evolution towards broader applicability.\n\nThe utility of type information extends to domain-specific applications. \\cite{hu2024} presented \\textit{SR-KGE}, a \\textit{GeoEntity-type constrained knowledge graph embedding} framework designed for predicting natural-language spatial relations. This approach integrates geoentity types as a constraint, combining graph structures with semantic attributes to capture spatial and semantic relations more accurately. While TaKE provides a universal type integration, SR-KGE exemplifies how tailored auxiliary information, when applied to a specific domain, can yield superior results for specialized tasks. The strength of these type-augmented methods lies in their ability to provide semantic guidance, making embeddings more discriminative by enforcing type consistency and enriching the relational context. However, a common limitation is their reliance on the availability and quality of type information; if types are sparse, noisy, or inconsistently defined, the benefits may diminish, and the complexity of integrating diverse type hierarchies can be substantial.\n\nBeyond explicit types, entity attributes offer a richer, instance-specific form of auxiliary information. \\cite{zhang2024} addressed the critical problem of erroneous triples in KGs by proposing \\textit{AEKE}, a framework for \\textit{Attributed Error-aware Knowledge Embedding}. AEKE leverages entity attributes to guide the KGE model in learning against the impact of erroneous triples. It designs triple-level hypergraphs to model both KG topological structures and attribute structures, jointly calculating confidence scores for each triple based on self-contradiction, structural consistency, and attribute homogeneity. These confidence scores then adaptively weigh contributions during multi-view graph learning and margin loss calculation, ensuring that potentially erroneous triples have minimal impact. AEKE represents a significant step towards enhancing KGE robustness, moving beyond merely completing KGs to making them more reliable. While type-based methods provide broad semantic categories, attribute-based approaches like AEKE offer fine-grained details that can be crucial for identifying and mitigating data quality issues.\n\nThe scope of auxiliary information also extends to hyper-relational facts, moving \"beyond triplets\" to capture richer contextual data. \\cite{rosso2020} introduced \\textit{HINGE}, a \\textit{hyper-relational Knowledge Graph Embedding model} that directly learns from hyper-relational facts, where each fact includes a base triplet (\\textit{h, r, t}) and associated key-value pairs (\\textit{k, v}). HINGE captures not only the primary structural information of the KG but also the correlation between each triplet and its associated key-value pairs. This is a crucial distinction from type or attribute integration, as HINGE directly models additional structured facts that are part of the knowledge base, rather than meta-information about entities. This allows for a more comprehensive understanding of complex data semantics, outperforming models that rely solely on triplets or transform hyper-relational facts into less structured n-ary representations.\n\nIn synthesis, the integration of auxiliary information, whether through entity types \\cite{wang2021, he2023, hu2024}, attributes \\cite{zhang2024}, or hyper-relational facts \\cite{rosso2020}, represents a vital direction in KGE research. These approaches collectively address the limitations of purely structural KGEs by providing richer semantic context, making embeddings more discriminative and robust to noise and incompleteness. The evolution from specific type integration (TransET) to universal frameworks (TaKE) and domain-specific applications (SR-KGE) highlights a growing sophistication. Furthermore, the focus on error-aware learning through attributes (AEKE) and the direct modeling of hyper-relational facts (HINGE) underscore the field's commitment to developing KGE models that can handle the complexities and imperfections of real-world knowledge graphs. A key trade-off, however, is the increased reliance on the availability and quality of this auxiliary data, which may not always be consistent across diverse KGs. Nevertheless, these advancements are crucial for pushing KGE towards greater practical utility and theoretical soundness, enabling more intelligent and reliable AI applications.\n\\subsection{Rule-based and Constraint-driven KGE}\n\\label{sec:4\\_2\\_rule-based\\_\\_and\\_\\_constraint-driven\\_kge}\n\nWhile purely data-driven knowledge graph embedding (KGE) models excel at capturing statistical patterns from observed triples, they often struggle with ensuring logical consistency, facilitating explicit reasoning, and providing interpretability aligned with human understanding. This subsection delves into approaches that integrate logical rules and explicit constraints directly into the KGE learning process, thereby injecting prior knowledge to address these limitations.\n\nOne foundational approach to injecting semantic consistency is exemplified by \"Semantically Smooth Knowledge Graph Embedding\" (SSE) \\cite{guo2015}. This method enforces a \"semantically smooth\" embedding space, where entities belonging to the same semantic category are encouraged to lie close to each other. By employing manifold learning techniques, such as Laplacian Eigenmaps and Locally Linear Embedding, as regularization terms, SSE guides the embedding process to discover intrinsic geometric structures that reflect categorical semantics. While effective in promoting semantic coherence, SSE primarily relies on entity categories as a form of soft constraint, which is less explicit than logical rules and might not directly enhance complex reasoning capabilities. Its strength lies in its generality, as the smoothness assumption can be applied to various embedding models and constructed from diverse information beyond just entity categories.\n\nA significant advancement in this domain is the explicit incorporation of logical rules. Early rule-based methods often relied on hard rules, which are rigid and require extensive manual curation. However, real-world knowledge graphs are often noisy and incomplete, making hard rules brittle. \"Knowledge Graph Embedding with Iterative Guidance from Soft Rules\" (RUGE) \\cite{guo2017} introduced a novel paradigm to iteratively integrate soft rules (rules associated with confidence levels) into the embedding learning process. RUGE simultaneously learns from observed triples, unlabeled triples (whose labels are iteratively predicted), and automatically extracted soft rules. This iterative guidance allows the knowledge embodied in rules to be progressively transferred into the learned embeddings, leading to more robust representations. The key advantage of RUGE is its ability to leverage abundant, albeit uncertain, automatically extracted rules, moving beyond the limitations of manually curated hard rules. This iterative feedback loop between rule inference and embedding updates represents a crucial step towards deeply intertwining symbolic logic with subsymbolic representations.\n\nComplementing complex rule integration, simpler structural constraints can also significantly enhance KGE models. \"Improving Knowledge Graph Embedding Using Simple Constraints\" \\cite{ding2018} demonstrated that even straightforward constraints can yield substantial improvements in interpretability and structure without adding significant computational overhead. Specifically, this work explored non-negativity constraints on entity representations, which help learn compact and interpretable features, and approximate entailment constraints on relation representations. These entailment constraints encode regularities of logical entailment between relations, structuring the embedding space to reflect hierarchical or inferential relationships. While these constraints are less expressive than full first-order logic rules, their simplicity makes them highly efficient and broadly applicable, offering a practical trade-off between model complexity and the benefits of injected prior knowledge.\n\nMore recent contributions have further refined the integration of soft rules. \"Knowledge Graph Embedding Preserving Soft Logical Regularity\" \\cite{guo2020} focused on imposing soft rule constraints directly on relation representations. By representing relations as bilinear forms and mapping entity representations into a non-negative and bounded space, the method derives a rule-based regularization that enforces relation representations to satisfy rule-introduced constraints. A notable strength of this approach is its improved scalability, as the complexity of rule learning becomes independent of the entity set size, making it more feasible for large-scale KGs. This direct regularization of relations ensures that logical patterns are preserved in the relational space, which is crucial for consistent reasoning.\n\nBuilding on these foundations, RulE (\"Rule Embedding\") \\cite{tang2022} presents a principled framework that learns rule embeddings jointly with entity and relation embeddings within a unified vector space. Unlike previous methods that might treat rules as external regularization, RulE explicitly represents rules as vectors, allowing for soft logical inference directly within the embedding space. This deep integration enables rule embeddings to regularize and enrich entity/relation embeddings, leading to more coherent and reasoning-capable representations. RulE's ability to calculate a confidence score for each rule based on its consistency with observed triples further refines the \"softness\" of logical inference, alleviating the brittleness often associated with strict logic. This joint learning paradigm represents a sophisticated approach to intertwining symbolic knowledge with subsymbolic representations, pushing the boundaries of reasoning capabilities within KGEs.\n\nCollectively, these rule-based and constraint-driven methods highlight a critical evolution in KGE research. They move beyond purely data-driven models (like those in the \"Core KGE Model Architectures\" subgroup) by leveraging explicit logical knowledge to enhance robustness, improve reasoning, and increase interpretability. The progression from general semantic smoothness \\cite{guo2015} to iterative soft rule guidance \\cite{guo2017}, the application of simple yet effective structural constraints \\cite{ding2018}, and finally to the joint embedding of rules themselves \\cite{tang2022} demonstrates a continuous effort to bridge the gap between symbolic logic and continuous vector spaces.\n\nHowever, several challenges persist. A primary limitation across many rule-based approaches is the reliance on the availability and quality of logical rules. While methods like RUGE can leverage automatically extracted soft rules, the efficiency and accuracy of such rule extraction remain a practical hurdle. Furthermore, balancing the strict adherence to logical rules with the flexibility to capture exceptions or nuanced patterns not covered by rules is a delicate trade-off. Over-constraining the embedding space with rules might inadvertently reduce its expressiveness or ability to generalize to unseen, complex scenarios. The scalability of managing and applying very large and complex rule sets, particularly for higher-order logic, also poses a significant challenge, despite efforts like \\cite{guo2020} to optimize rule learning complexity. Future research needs to explore more robust and automated rule discovery mechanisms, develop adaptive frameworks that can dynamically weigh rule adherence against data-driven insights, and investigate more principled theoretical foundations for combining probabilistic logic with continuous embeddings.\n\\subsection{Multi-modal and Cross-domain KGE}\n\\label{sec:4\\_3\\_multi-modal\\_\\_and\\_\\_cross-domain\\_kge}\n\nThe limitations of relying solely on structural information in knowledge graphs, particularly data sparsity and the inability to capture rich semantic nuances, have driven significant research into multi-modal and cross-domain Knowledge Graph Embedding (KGE). These approaches integrate diverse information sources, such as textual descriptions, visual features, or data from multiple domains, to enrich representations and enable more comprehensive knowledge understanding. The core motivation is to leverage complementary information, thereby enhancing the expressiveness and robustness of KGE models.\n\nEarly efforts in multi-modal KGE primarily focused on integrating textual descriptions to augment structural embeddings. For instance, the Semantic Space Projection (SSP) model \\cite{xiao2016} proposed a method that jointly learns from symbolic triples and textual descriptions. SSP projects textual information into a semantic space, using text to discover semantic relevance and provide more precise embeddings. This was a crucial step in addressing the \"weak-semantic\" nature of purely geometric models, which often struggle to differentiate entities with similar structural positions but distinct semantic meanings. While SSP offered a foundational approach, its text integration was relatively simple, often relying on bag-of-words or basic word embeddings.\n\nMore recent advancements have leveraged the power of pre-trained language models (PLMs) for deeper semantic understanding. The \"Joint Language Semantic and Structure Embedding for Knowledge Graph Completion\" model \\cite{shen2022} exemplifies this by fine-tuning PLMs with a probabilistic structured loss. This method effectively captures semantics from natural language descriptions while simultaneously reconstructing structural information, demonstrating state-of-the-art performance, particularly in low-resource settings where semantic cues are invaluable. This approach highlights a significant evolutionary trend, moving from simple text projection to sophisticated joint learning frameworks that deeply intertwine language semantics with graph structure. A key strength of such models is their ability to infer relations and entity properties even when structural data is sparse, a common challenge in many real-world KGs. However, the computational cost of fine-tuning large PLMs and the availability of high-quality textual descriptions for all entities remain practical constraints.\n\nBeyond textual data, multi-modal KGE has expanded to incorporate other modalities and domain-specific knowledge. For instance, in the biomedical domain, \"Multimodal reasoning based on knowledge graph embedding for specific diseases\" \\cite{zhu2022} constructs Specific Disease Knowledge Graphs (SDKGs) and implements multimodal reasoning using reverse-hyperplane projection. This model integrates structural, category, and description embeddings to discover new, reliable knowledge, showcasing how combining different modalities can lead to enhanced insights in specialized contexts. This demonstrates the power of multimodal integration in addressing the unique complexities and data characteristics of domain-specific KGs. However, the generalizability of such highly specialized models to other domains without significant re-engineering remains an open question. Furthermore, the challenge of designing effective negative sampling strategies becomes more pronounced in multi-modal settings, as highlighted by \\cite{zhang2023}, which proposes Modality-Aware Negative Sampling (MANS) to align structural and visual embeddings, underscoring that training optimization must adapt to the complexity of heterogeneous data.\n\nCross-domain KGE extends this concept by enabling knowledge transfer and interaction across distinct knowledge graphs or domains. This is particularly vital for applications like recommender systems, where user preferences and item characteristics often span multiple categories or platforms. \"Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation\" \\cite{liu2023} addresses the critical problems of cross-domain cold start and multi-domain recommendations. This approach proposes a \"binding rule\" to efficiently interact items across multiple domains, allowing for both homo-domain and hetero-domain item embeddings. By modeling associations and interactions between items across diverse domains, this method significantly improves multi-domain item-item recommendations, outperforming traditional recommender systems that struggle with data sparsity in new domains. The strength lies in its ability to leverage shared entities or relations to bridge information gaps, enriching representations for items even in domains with limited data. A limitation, however, is the reliance on explicit links or shared entities between domains, which may not always be readily available or accurately reflect complex cross-domain relationships.\n\nIn summary, multi-modal and cross-domain KGE represent a crucial evolutionary trajectory in knowledge representation. They move beyond the limitations of purely structural models by integrating diverse, complementary information sources. While models like SSP \\cite{xiao2016} laid the groundwork for textual integration, the field has progressed to sophisticated joint learning frameworks leveraging pre-trained language models \\cite{shen2022} and domain-specific multimodal reasoning \\cite{zhu2022}. Concurrently, cross-domain approaches \\cite{liu2023} tackle challenges like data sparsity and cold start in complex applications such as recommendation. The collective contribution of these methods is the creation of richer, more semantically grounded, and robust embeddings, which are essential for comprehensive knowledge understanding and practical applicability in diverse AI tasks. However, challenges persist in effectively fusing heterogeneous information, managing increased computational complexity, and designing robust training strategies like modality-aware negative sampling \\cite{zhang2023}.\n\n\n\\label{sec:dynamic,_inductive,_and_distributed_kge}\n\n\\section{Dynamic, Inductive, and Distributed KGE}\n\\label{sec:dynamic,\\_inductive,\\_\\_and\\_\\_distributed\\_kge}\n\n\\label{sec:dynamic\\_inductive\\_and\\_distributed\\_kge}\n\\section{Dynamic, Inductive, and Distributed KGE}\n\nBuilding upon the enriched KGE models discussed in Section \\ref{sec:enriching\\_kge\\_auxiliary\\_information\\_rules\\_and\\_multi\\_modality}, which focused on leveraging diverse auxiliary information and logical constraints to deepen semantic understanding, this section shifts attention to the critical operational challenges of Knowledge Graph Embeddings in real-world environments. Traditional KGE models often assume static, complete, and centrally managed knowledge graphs, a paradigm increasingly at odds with the dynamic, evolving, and distributed nature of modern knowledge bases. This section addresses these fundamental limitations by exploring advanced KGE methodologies designed for adaptability, scalability, and security in complex, real-world operational settings, moving beyond static and centralized assumptions to meet the demands of modern knowledge management systems.\n\nWe delve into three interconnected areas. First, \\textit{Temporal Knowledge Graph Embedding (TKGE)} tackles the inherent dynamism of real-world facts, where entities and relations change over time. These methods move beyond static representations to capture the fluidity and evolution of knowledge, crucial for tasks requiring reasoning over time. Second, \\textit{Inductive and Continual KGE} addresses the challenge of unseen entities and the need for continuous model updates. This area explores how KGE models can efficiently learn embeddings for new entities without full retraining and adapt to a constant influx of new facts, mitigating catastrophic forgetting. Finally, \\textit{Federated and Privacy-Preserving KGE} investigates collaborative learning paradigms for distributed knowledge graphs. This crucial direction enables multiple parties to jointly train robust KGE models while safeguarding sensitive data, addressing the growing demand for privacy-aware AI systems. Together, these advancements are pivotal for transitioning KGE from theoretical constructs to robust, secure, and continuously operational components within complex, real-world knowledge management systems.\n\n\\subsection{Temporal Knowledge Graph Embedding (TKGE)}\n\\label{sec:5\\_1\\_temporal\\_knowledge\\_graph\\_embedding\\_(tkge)}\n\nThe inherent dynamism of real-world knowledge necessitates models capable of capturing the temporal evolution of facts within knowledge graphs (KGs). Traditional Knowledge Graph Embedding (KGE) models, primarily designed for static KGs, fall short in tasks requiring reasoning over time or understanding the fluidity of information. Temporal Knowledge Graph Embedding (TKGE) addresses this by explicitly integrating time into the embedding process, moving beyond static representations to model evolving entities and relations \\cite{dai2020}.\n\nEarly approaches to TKGE focused on explicitly structuring and modeling time itself. \\cite{dasgupta2018} introduced \\textit{HyTE}, a hyperplane-based method that associates each timestamp with a corresponding hyperplane in the entity-relation space. This allows for temporally guided KG inference and prediction of temporal scopes for facts, marking a significant step towards dynamic KGEs. While intuitive, HyTE's reliance on simple hyperplanes might struggle with highly complex, non-linear temporal dependencies. Another prominent method involves treating the entire fact set as a higher-order tensor, typically a fourth-order tensor (head, relation, tail, time), and applying tensor decomposition to learn dense, low-dimensional temporal embeddings \\cite{lin2020}. This provides a robust mathematical framework for integrating time as a distinct dimension. Complementing this, \\textit{ATiSE} models temporal evolution using additive time series decomposition, mapping representations into multi-dimensional Gaussian distributions where covariance captures temporal uncertainty, offering a probabilistic view of temporal dynamics \\cite{xu2019}. More recently, \\textit{TeAST} innovatively structures time by mapping relations onto an Archimedean spiral timeline, transforming the quadruple completion problem into a 3rd-order tensor completion task. This approach aims to ensure relations evolve orderly over time with a spiral regularizer, offering a degree of interpretability regarding temporal patterns \\cite{li2023}. A common limitation across these tensor and time series methods is the computational cost associated with higher-order operations or complex decompositions, especially for very dense temporal data.\n\nA significant advancement in TKGE involves leveraging geometric transformations to model temporal dynamics. \\textit{TeRo} defines the temporal evolution of entity embeddings as a rotation from an initial time to the current time in a complex vector space, representing relations for time intervals with dual complex embeddings \\cite{xu2020}. Building upon this, \\textit{ChronoR} extends the concept by employing k-dimensional rotation transformations, parametrized by relation and time, to transform a head entity to fall near its tail entity. This effectively captures rich interactions between temporal and multi-relational characteristics \\cite{sadeghian2021}. While powerful, the interpretability of complex rotations in high-dimensional spaces can be challenging, and the computational complexity associated with learning these transformations can be substantial, particularly for very large KGs or highly granular temporal data.\n\nMore advanced methods, particularly emerging in recent years, address the complexities of dynamic, spatiotemporal, and even fuzzy knowledge by moving beyond a single Euclidean space. \\textit{MADE} and \\textit{IME}, both published in 2024, represent a cutting-edge shift towards modeling TKGs in multi-curvature spaces, including Euclidean, hyperbolic, and hyperspherical geometries \\cite{wang2024, wang2024a}. The rationale is that TKGs often contain interwoven complex geometric structures (e.g., hierarchical, ring, chain) that no single curvature space can optimally capture. MADE introduces an adaptive weighting mechanism to assign different weights to these spaces in a data-driven manner, along with a quadruplet distributor and temporal regularization for timestamp smoothness \\cite{wang2024}. IME, on the other hand, incorporates \"space-shared\" properties to learn commonalities across spaces and alleviate spatial gaps, and \"space-specific\" properties to capture characteristic features, complemented by an Adjustable Multi-curvature Pooling (AMP) approach \\cite{wang2024a}. While both achieve state-of-the-art results, MADE's adaptive weighting offers a more flexible approach to handling diverse geometric structures without requiring explicit design of shared/specific properties. The primary limitation for both is the increased complexity of optimizing embeddings across multiple, potentially disparate, geometric spaces, and the computational overhead.\n\nFurther extending these geometric transformations, recent works tackle fuzzy and spatiotemporal dimensions. \\textit{FSTRE} uses projection and rotation in a complex vector space to embed spatial and temporal information, introducing fine-grained fuzziness through modal lengths of anisotropic vectors \\cite{ji2024}. This addresses the insufficiency of prior KGE models for uncertain and dynamic knowledge. Building on this, \\cite{ji2024a} leverages quaternion embeddings to jointly embed spatiotemporal entities, representing relations as rotations and exploiting the non-commutative compositional pattern of quaternions for multihop path reasoning and uncertainty modeling. This approach is particularly powerful for complex tasks like multihop querying on incomplete fuzzy spatiotemporal KGs, where previous methods overlooked uncertainty and spatiotemporal sensitivity during reasoning. These advanced models, while powerful, introduce additional complexity (fuzziness, spatiotemporal, quaternions) which can increase model intricacy and training demands.\n\nFinally, \\textit{TARGAT} offers an alternative paradigm by employing a time-aware relational graph attention model based on Graph Neural Networks (GNNs) \\cite{xie2023}. It addresses the limitation of previous GNN-based models that struggle to directly capture multi-fact interactions at different timestamps by dynamically generating time-aware relational message transformation matrices. This GNN-based approach provides a unified way to process the entire graph of multi-facts over time. However, GNNs can face scalability challenges with extremely large and dense TKGs due to the computational intensity of message passing.\n\nIn summary, TKGE research has evolved from explicit temporal integration using hyperplanes or tensors to sophisticated geometric transformations (rotations, multi-curvature spaces) and advanced algebraic structures (quaternions) to handle the multifaceted nature of dynamic, spatiotemporal, and fuzzy knowledge. The trade-off between model expressiveness and computational complexity remains a persistent challenge, with recent models pushing the boundaries of what can be captured, often at the cost of increased model intricacy and optimization demands.\n\\subsection{Inductive and Continual KGE}\n\\label{sec:5\\_2\\_inductive\\_\\_and\\_\\_continual\\_kge}\n\nReal-world knowledge graphs (KGs) are inherently dynamic, with new entities, relations, and facts constantly emerging. Traditional Knowledge Graph Embedding (KGE) models are often transductive, meaning they can only generate embeddings for entities seen during training, necessitating expensive full retraining when new information arrives. This limitation has spurred significant research into inductive and continual KGE, aiming to adapt models to evolving KGs by handling unseen entities and efficiently updating knowledge without catastrophic forgetting \\cite{liu2024, sun2024}. These methods are crucial for maintaining the scalability and relevance of KGE models in dynamic environments.\n\nEarly efforts in inductive KGE focused on neighborhood aggregation techniques. \\cite{wang2018} introduced the Logic Attention Network (LAN), an aggregator that learns to embed new entities by combining the embeddings of their existing neighbors. LAN addresses the unordered and unequal nature of an entity's neighbors by employing both rules- and network-based attention weights. While innovative for its time, aggregation-based methods like LAN inherently rely on the presence of existing neighbors for new entities. This poses a limitation when truly novel, isolated entities with sparse connections emerge, as their representations might be less robust or even impossible to generate. The generalizability of such methods is also constrained by the quality and density of the local neighborhood information.\n\nTo overcome the limitations of direct entity embedding and enhance transferability, meta-learning has emerged as a powerful paradigm for inductive KGE. \\cite{chen2021} proposed MorsE, a model that does not learn explicit entity embeddings but instead learns transferable meta-knowledge. This meta-knowledge, modeled by entity-independent modules and learned through meta-learning, can then be used to produce embeddings for new entities in an inductive setting. This approach offers a more generalized inductive capability compared to simple aggregation, as it aims to capture underlying patterns that are independent of specific entities. Building on this, \\cite{sun2024} applied meta-learning to dynamic KGE in evolving service ecosystems with MetaHG. This model incorporates both local (via a GNN layer) and potential global (via a hypergraph neural network, HGNN, layer) structural information from current KG snapshots to enhance the representation of emerging entities. MetaHG's hybrid GNN framework and meta-learning strategy aim to mitigate issues like spatial deformation and improve the quality of embeddings for new entities. A critical comparison reveals that while meta-learning offers a more robust inductive framework, its complexity in training and the need for sufficient meta-training tasks can be significant. Furthermore, the effectiveness of meta-knowledge transfer can still be influenced by the similarity between the meta-training and target domains.\n\nBeyond inductive learning, continual KGE addresses the challenge of efficiently acquiring new knowledge while simultaneously preserving previously learned information, a problem often plagued by catastrophic forgetting. \\cite{liu2024} introduced FastKGE, a framework incorporating an incremental low-rank adapter (IncLoRA) mechanism. FastKGE tackles both efficient new knowledge acquisition and catastrophic forgetting by isolating and allocating new knowledge to specific layers based on the fine-grained influence between old and new KGs. The IncLoRA mechanism then embeds these specific layers into low-rank adapters, significantly reducing the number of trainable parameters during fine-tuning. This approach also features adaptive rank allocation, making the LoRA aware of entity importance. Experimental results demonstrate that FastKGE can reduce training time by 34-49\\\\% on public datasets while maintaining competitive performance, and even greater savings (51-68\\\\%) on larger, newly constructed datasets, alongside performance improvements. This parameter-efficient adaptation is a crucial advancement for large-scale KGE models, balancing the acquisition of new knowledge with the retention of old, a key trade-off in continual learning. However, the efficacy of LoRA-based methods can depend on the intrinsic rank of the updates and the architecture of the base KGE model.\n\nThe overarching goal across these inductive and continual KGE methods is to balance the acquisition of new knowledge with the retention of previously learned information, mitigating catastrophic forgetting and ensuring scalability. While neighborhood aggregation provides a straightforward, albeit limited, inductive capability, meta-learning offers a more generalized approach by learning transferable knowledge. Parameter-efficient adaptation techniques like IncLoRA represent a practical solution for continual learning, particularly for large models, by enabling efficient updates without full retraining. A common theoretical gap preventing a complete solution to these problems lies in developing truly universal inductive mechanisms that are robust to completely novel, isolated entities and can perform continuous, long-term updates without any degradation in performance or significant increase in computational cost. The experimental setups for these models often require specialized dynamic datasets, which can be less standardized than static link prediction benchmarks, making direct comparisons challenging and potentially affecting generalizability. The field continues to seek methods that can seamlessly integrate new information while preserving the integrity and expressiveness of the entire knowledge graph, a critical step towards truly adaptive and intelligent AI systems.\n\\subsection{Federated and Privacy-Preserving KGE}\n\\label{sec:5\\_3\\_federated\\_\\_and\\_\\_privacy-preserving\\_kge}\n\nThe increasing concerns over data privacy and the proliferation of distributed knowledge sources have propelled Federated Learning (FL) as a crucial paradigm for Knowledge Graph Embedding (KGE). Federated KGE (FKGE) enables collaborative model training across multiple clients, each holding a local knowledge graph (KG), without centralizing sensitive data. This approach is vital for leveraging decentralized knowledge in privacy-sensitive domains, addressing the growing need for privacy-aware AI systems. However, FKGE introduces unique challenges, primarily related to communication efficiency, personalization for diverse client data, and security vulnerabilities.\n\nA significant challenge in FKGE is the high communication cost stemming from the large size of KGE parameters and the extensive number of communication rounds required for convergence. Traditional FL methods often focus on reducing communication rounds by increasing local training epochs, but they frequently overlook the size of parameters transmitted in each round. To address this, \\cite{zhang2024} (Communication-Efficient FKGE) proposes FedS, a bidirectional communication-efficient framework based on Entity-Wise Top-K Sparsification. This method allows clients to dynamically identify and upload only the Top-K entity embeddings with the most significant changes to the server. Similarly, the server transmits only the Top-K aggregated embeddings to each client after performing personalized aggregation. This approach, coupled with an Intermittent Synchronization Mechanism, aims to mitigate the negative effects of embedding inconsistency caused by client heterogeneity. While FedS significantly enhances communication efficiency, a critical analysis reveals a potential trade-off: universal reduction in embedding precision, as noted by the authors, can impede convergence speed. The challenge lies in precisely identifying the \"most significant\" changes without losing crucial information, especially for less frequently updated entities or relations. This aligns with the broader \"Efficiency, Compression, and System Optimization\" subgroup's goal of reducing resource consumption while maintaining performance, but within the added constraint of distributed, privacy-preserving learning.\n\nBeyond communication efficiency, the semantic disparities among clients pose a substantial hurdle for FKGE. Existing FKGE methods often rely on a global consensus model, typically using the arithmetic mean of entity embeddings as global supplementary knowledge \\cite{zhang2024}. This \"one-size-fits-all\" approach, however, neglects the inherent semantic heterogeneity across diverse client KGs, leading to a global model that might be inundated with noise when tailored to a specific client. To overcome this, \\cite{zhang2024} (Personalized Federated KGE) introduces PFedEG, a novel approach that employs a client-wise relation graph to learn personalized embeddings. PFedEG discerns the semantic relevance of embeddings from other clients, allowing each client to learn personalized supplementary knowledge by amalgamating entity embeddings from its \"neighboring\" clients based on their affinity on this graph. This personalized approach addresses the \"Personalized Federated KGE\" challenge, moving beyond a universal global model to improve embedding quality for individual clients. The strength of PFedEG lies in its ability to adapt to diverse data distributions, a critical aspect highlighted in the \"Dynamic, Inductive, and Continual KGE\" subgroup, which emphasizes adaptability to evolving and heterogeneous knowledge. However, the construction and maintenance of such a client-wise relation graph introduce additional computational complexity and potential privacy leakage risks if the \"affinity\" metrics are not carefully designed.\n\nWhile FL is designed to preserve data privacy, it is not inherently immune to security vulnerabilities. \\cite{zhou2024} (Poisoning Attack on Federated KGE) systematically explores the risks of poisoning attacks in FKGE, highlighting a critical security challenge. This pioneering work develops a novel framework that forces victim clients to predict specific false facts, demonstrating that privacy-preserving distributed training does not automatically equate to security. Unlike centralized KGEs, where attackers might directly inject poisoned data, FKGE's local data maintenance necessitates indirect attack vectors. The proposed attack framework involves inferring targeted relations in the victim's local KG via a \"KG component inference attack\" and then using an optimized dynamic poisoning scheme to generate progressive poisoned updates through FKGE aggregation. The experimental results demonstrate remarkable success rates (e.g., 100\\\\% on TransE with WN18RR) with minimal impact on the original task's performance, exposing a significant vulnerability. This research, while adversarial, is crucial for informing the design of robust FKGE systems, aligning with the \"Robustness and Training Optimization\" subgroup's focus on mitigating data imperfections and ensuring model integrity. The theoretical gap here is the lack of a direct defense mechanism proposed by the authors, which remains an open challenge for future research.\n\nIn synthesis, the emerging field of Federated and Privacy-Preserving KGE is rapidly addressing the practical demands of distributed and privacy-sensitive environments. The works by \\cite{zhang2024} (Communication-Efficient FKGE) and \\cite{zhang2024} (Personalized Federated KGE) represent constructive efforts to optimize FKGE by tackling communication bottlenecks and semantic heterogeneity, respectively. These solutions are critical for making FKGE scalable and effective in real-world deployments. However, the findings of \\cite{zhou2024} (Poisoning Attack on Federated KGE) serve as a stark reminder that privacy and security are distinct concerns, and the distributed nature of FL introduces new attack surfaces. The trade-offs are evident: aggressive communication sparsification might impact model convergence, personalization adds complexity and potential for noise, and robust security measures could introduce computational overhead. The rapid publication of these papers in 2024 underscores the contemporary and pressing nature of these challenges, reflecting the field's accelerated evolution towards practical, secure, and adaptable KGE solutions for decentralized knowledge.\n\n\n\\label{sec:practical_considerations:_efficiency,_robustness,_and_evaluation}\n\n\\label{sec:practical\\_considerations:\\_efficiency,\\_robustness,\\_\\_and\\_\\_evaluation}\n\n\\section{Practical Considerations: Efficiency, Robustness, and Evaluation}\n\\label{sec:practical\\_considerations\\_efficiency\\_robustness\\_and\\_evaluation}\n\nBuilding upon the advancements in dynamic, inductive, and distributed knowledge graph embedding (KGE) models discussed in Section \\ref{sec:dynamic\\_inductive\\_and\\_distributed\\_kge}, which focused on adapting KGEs to evolving and decentralized knowledge, this section shifts its focus to the critical operational challenges of deploying and evaluating these models in real-world scenarios. While previous sections explored theoretical foundations and architectural innovations, the true utility of KGEs hinges on their practical viability, encompassing computational efficiency, resilience to imperfect data, and the trustworthiness of their empirical validation. This section bridges the gap between theoretical progress and reliable real-world application, addressing the fundamental requirements for KGE models to move from research prototypes to robust, scalable, and trustworthy components of intelligent systems.\n\nWe delve into three interconnected areas vital for practical KGE deployment. First, we examine strategies for enhancing \\textit{efficiency, compression, and scalability}, which are paramount for handling the immense size and complexity of modern knowledge graphs \\cite{community\\_1, community\\_6}. This involves techniques to reduce computational cost, minimize memory footprint, and optimize training and inference processes, making KGE models viable for resource-constrained environments and large-scale applications. Second, we explore methods for improving \\textit{robustness and training optimization}, crucial for mitigating the impact of noisy, incomplete, or imbalanced data inherent in real-world KGs \\cite{community\\_2, community\\_3, 2a3f862199883ceff5e3c74126f0c80770653e05}. This includes advanced negative sampling techniques and noise filtering mechanisms that ensure models learn accurate and reliable representations. Finally, a significant portion is dedicated to the importance of rigorous \\textit{evaluation, benchmarking, and reproducibility} \\cite{community\\_0, community\\_6}. This area underscores the necessity for standardized metrics, fair comparisons, and transparent research practices to ensure that KGE advancements are scientifically sound and lead to reliable, trustworthy applications. Together, these practical considerations are indispensable for translating the rich theoretical landscape of KGE into impactful and dependable real-world solutions.\n\n\\subsection{Efficiency, Compression, and Scalability}\n\\label{sec:6\\_1\\_efficiency,\\_compression,\\_\\_and\\_\\_scalability}\n\nThe practical deployment of Knowledge Graph Embedding (KGE) models for massive knowledge graphs (KGs) is often hampered by significant computational costs, extensive training times, and prohibitive memory footprints. Addressing these bottlenecks is crucial for transitioning KGEs from academic benchmarks to real-world applications, especially in resource-constrained environments. This area of research focuses on techniques spanning knowledge distillation, embedding compression, parameter-efficient learning, optimized system designs, and novel algorithms.\n\nOne prominent strategy for reducing the computational burden and memory footprint is \\textit{knowledge distillation}. \\cite{zhu2020} introduced DualDE, a method that distills knowledge from a high-dimensional, high-performing teacher KGE model into a low-dimensional student model. This approach significantly reduces embedding parameters (by 7-15x) and increases inference speed (by 2-6x) while retaining competitive performance. DualDE's generality allows its application across various KGE architectures, making it a versatile tool for efficiency. However, a common trade-off in distillation is a minor, albeit often acceptable, loss in performance, which is inherent to compressing information.\n\nComplementary to distillation are direct \\textit{embedding compression} techniques. \\cite{sachan2020} proposed representing entities with discrete codes, achieving remarkable compression ratios (50-1000x) of the embedding layer with only a minor performance drop. Building on this, LightKG \\cite{wang2021} introduced a lightweight framework that stores only a few codebooks and indices, drastically reducing storage and boosting inference efficiency through quick look-ups. LightKG also incorporates a dynamic negative sampling strategy, which further enhances performance. While these methods offer substantial gains in storage and inference speed, they rely on approximating the original embeddings, which might introduce subtle inaccuracies compared to full-precision representations.\n\n\\textit{Parameter-efficient learning} offers another avenue for scalability. Entity-Agnostic Representation Learning (EARL) \\cite{chen2023} tackles the escalating parameter storage costs by learning embeddings only for a small set of \"reserved entities.\" Embeddings for other entities are then derived from their context (e.g., connected relations, k-nearest reserved entities, multi-hop neighbors) using universal, entity-agnostic encoders. This approach results in a static and significantly lower parameter count, making it particularly beneficial for large and continuously growing KGs. A potential limitation of EARL is its reliance on contextual information; entities with sparse connections or those truly \"unseen\" without sufficient neighbors might have less expressive representations compared to directly learned embeddings.\n\nBeyond model-specific optimizations, \\textit{optimized system designs} are critical for large-scale KGE training. GE2 \\cite{zheng2024} proposes a general and efficient KGE learning system that addresses long CPU times and high CPU-GPU communication overhead, especially in multi-GPU setups. GE2 offloads operations to GPUs and introduces COVER, a novel algorithm for managing data swap between CPU and multiple GPUs, achieving speedups of 2x to 7.5x across various models and datasets. This system-level innovation provides a foundational improvement for KGE research, enabling faster experimentation and deployment of complex models. While GE2 focuses on training efficiency, it doesn't directly reduce the size of the \\textit{final} embedding model, which is where distillation and compression techniques become relevant.\n\nNovel algorithmic approaches also contribute significantly to efficiency. \"Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis\" \\cite{peng2021} introduces a fundamentally different paradigm by proposing a closed-form solution using Orthogonal Procrustes Analysis (OPA). This enables full-batch learning and non-negative sampling, reducing training time and carbon footprint by orders of magnitude while yielding competitive performance. The closed-form nature bypasses iterative optimization, which is a major source of computational cost in traditional KGEs. However, the inherent mathematical constraints of OPA might limit its expressiveness compared to more flexible, iteratively optimized models for certain complex relational patterns.\n\nFor Graph Neural Network (GNN)-based KGEs, which are known for their computational intensity, \\textit{graph partitioning strategies} are essential. CPa-WAC \\cite{modak2024} employs modularity maximization-based constellation partitioning to break down KGs into subgraphs. This allows for separate processing, reducing memory and training time for GNNs while aiming to retain prediction accuracy. CPa-WAC demonstrates up to a five-fold speedup, highlighting the effectiveness of distributed processing. Nevertheless, partitioning a graph can sometimes hinder the capture of global dependencies that span across different subgraphs, potentially impacting performance on tasks that require broader structural understanding.\n\nFinally, a comprehensive understanding of \\textit{parallelization techniques} is vital. \\cite{kochsiek2021} provided a critical meta-study, re-implementing and comparing various parallelization strategies for KGE training. Their work revealed that naive parallelization can degrade embedding quality and proposed effective mitigations, such as a variation of the stratification technique. This study underscores that simply distributing computation does not guarantee efficiency or quality, emphasizing the need for careful technique selection.\n\nIn summary, the pursuit of efficiency and scalability in KGE involves a multi-faceted approach. Techniques like DualDE \\cite{zhu2020}, \\cite{sachan2020}, and LightKG \\cite{wang2021} focus on compressing the model itself for storage and inference. EARL \\cite{chen2023} addresses parameter growth for evolving KGs. Meanwhile, GE2 \\cite{zheng2024}, OPA-based learning \\cite{peng2021}, CPa-WAC \\cite{modak2024}, and parallelization studies \\cite{kochsiek2021} target the efficiency of the training process. These innovations collectively aim to overcome the practical bottlenecks of KGE, making them deployable in resource-constrained environments and capable of handling the ever-growing scale of real-world knowledge bases. The ongoing challenge lies in balancing the gains in efficiency and scalability with the preservation of model expressiveness and predictive accuracy.\n\\subsection{Robustness and Training Optimization}\n\\label{sec:6\\_2\\_robustness\\_\\_and\\_\\_training\\_optimization}\n\nThe efficacy of Knowledge Graph Embedding (KGE) models in real-world applications is profoundly dependent on their robustness against data imperfections and the optimization of their training processes. Knowledge graphs are inherently noisy, incomplete, and often suffer from imbalanced data distributions, necessitating sophisticated techniques to ensure that learned representations are accurate, reliable, and generalize well. This subsection delves into methods designed to enhance KGE model robustness and refine their training, particularly focusing on negative sampling strategies.\n\nA critical aspect of model reliability is the trustworthiness of its predictions. \\cite{tabacof2019} highlight that many popular KGE models, despite achieving high accuracy, produce uncalibrated probability estimates, meaning their predicted scores do not directly correspond to true probabilities. They propose post-hoc calibration methods like Platt scaling and isotonic regression, which are particularly valuable when ground truth negatives are scarce, a common scenario in KGs. While these methods offer a general solution applicable to various KGE models, their effectiveness relies on the quality of the calibration data and may introduce additional computational overhead during inference. This emphasizes that model performance metrics alone are insufficient; the reliability of output probabilities is equally crucial for practical deployment.\n\nTo address the pervasive issue of noisy data within KGs, which often arises from automatic knowledge construction, \\cite{zhang2021} introduce a multi-task reinforcement learning framework. This innovative approach actively filters out noisy triples during training, allowing the KGE model to learn from a cleaner, more reliable subset of facts. By exploiting correlations among semantically similar relations through multi-task learning, their method aims to learn more robust representations. This proactive noise-filtering mechanism is a significant advancement over passive error handling, as it directly impacts the quality of the input data for embedding. However, the complexity introduced by a reinforcement learning agent within the training loop can increase computational cost and require careful hyperparameter tuning, posing scalability challenges for extremely large KGs.\n\nAnother common imperfection in KGs is data imbalance, where entities and relations follow a long-tail distribution, with a few occurring frequently and many appearing rarely. Traditional KGE methods often assign equal weights during training, leading to unreliable representations for infrequent (long-tail) entities and relations. To counteract this, \\cite{zhang2023} propose WeightE, a weighted KGE model that employs a bilevel optimization scheme to assign differential weights. WeightE dynamically endows lower weights to frequent elements and higher weights to infrequent ones, ensuring that long-tail entities and relations receive adequate training attention. This flexible weighting technique can be applied to various existing KGE models, offering a practical solution to a widespread problem. While effective, the bilevel optimization adds a layer of complexity to the training process, which might require more computational resources or careful convergence monitoring compared to standard training.\n\nBeyond handling data imperfections, optimizing the training process itself, particularly through effective negative sampling, is paramount. KGE models typically rely on contrastive learning, requiring both positive (observed) and negative (false) triples. The quality of generated negative samples profoundly impacts model performance \\cite{qian2021, madushanka2024}. Early approaches to negative sampling often struggled in noisy environments. \\cite{shan2018} address this with a confidence-aware negative sampling method for noisy KGE, introducing the concept of negative triple confidence to improve training stability and prevent issues like zero loss or false detection. This method acknowledges that not all negative samples are equally informative, especially in the presence of noise. Building on the idea of identifying \"hard\" negatives, \\cite{zhang2018} propose NSCaching, an efficient caching strategy that tracks and samples challenging negative triplets. This approach, inspired by generative adversarial networks (GANs) but simpler, aims to distill the benefits of complex adversarial sampling into a more computationally efficient framework, demonstrating a trade-off between model complexity and training efficiency.\n\nA more radical departure from traditional negative sampling is presented by \\cite{li2021} with their Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE). This method proposes to avoid negative sampling entirely by considering all negative instances. While this theoretically removes the uncertainty and potential instability inherent in sampling, it dramatically increases computational complexity. NS-KGE addresses this by leveraging mathematical derivations to reduce the complexity of the non-sampling loss function, aiming for both better efficiency and accuracy. This non-sampling paradigm offers a compelling alternative, particularly for models whose loss functions can be efficiently reformulated, but its generalizability across all KGE architectures and its practical scalability to extremely dense KGs remain areas of active research.\n\nThe increasing complexity of KGE models and the integration of diverse data modalities further complicate negative sampling. \\cite{zhang2023} (Modality-Aware Negative Sampling) address this by proposing MANS, a method specifically designed for multi-modal KGE. MANS aligns structural and visual embeddings for entities, demonstrating that negative sampling strategies must adapt to the unique characteristics of multi-modal information to learn meaningful embeddings. This highlights a critical development direction where training optimization must evolve in tandem with model architectural advancements.\n\nOverall, the advancements in robustness and training optimization reflect a maturing KGE field that is moving beyond purely theoretical model expressiveness towards practical utility. The systematic reviews of negative sampling \\cite{qian2021, madushanka2024} underscore its foundational importance, even as models like \\cite{chen2025} (ConQuatE) introduce sophisticated quaternion-based embeddings to handle polysemy. The continued focus on refining training mechanisms, such as negative sampling, alongside the development of advanced architectures, indicates a holistic approach where the effectiveness of complex KGE models is deeply intertwined with robust and efficient training methodologies. While significant progress has been made in handling noise, imbalance, and sampling, the challenge of efficiently and accurately identifying the \"true\" negative distribution in KGs, especially in dynamic and multi-modal settings, remains a theoretical gap and an active area of research.\n\\subsection{Evaluation, Benchmarking, and Reproducibility}\n\\label{sec:6\\_3\\_evaluation,\\_benchmarking,\\_\\_and\\_\\_reproducibility}\n\nThe rapid proliferation of Knowledge Graph Embedding (KGE) models has underscored the critical importance of rigorous evaluation, standardized benchmarking, and robust reproducibility practices. Without these, fair comparisons between models become challenging, scientific progress can be hindered by unreliable results, and the trustworthy deployment of KGE models in real-world applications is compromised. The field has increasingly recognized the need to move towards higher standards of empirical validation and transparency to ensure the reliability and generalizability of research findings.\n\nA significant step towards addressing these challenges has been the development of unified frameworks and libraries. \\cite{broscheit2020} introduced \\texttt{LibKGE}, an open-source PyTorch-based library designed to foster reproducible research. Its key strengths lie in its high configurability, decoupled components that allow for flexible mixing and matching, and comprehensive logging, making it an invaluable tool for conducting systematic experimental studies and analyzing the contributions of individual model components. While \\texttt{LibKGE} provides the infrastructure for reproducible experimentation, large-scale comparative studies have simultaneously exposed the widespread issues plaguing KGE research.\n\n\\cite{ali2020}'s seminal work, \"Bringing Light Into the Dark,\" provided a stark revelation of reproducibility failures within the KGE community. By re-implementing and evaluating 21 models within a unified framework, \\texttt{PyKEEN}, the authors found that many published results could not be reproduced with their reported hyperparameters, and some not at all. This highlights a significant methodological limitation: the heterogeneity in implementations, training procedures, and evaluation protocols across different research groups often leads to incomparable results and inflated performance claims. The study emphasized that model performance is not solely determined by architecture but by a complex interplay of architecture, training approach, loss function, and the explicit modeling of inverse relations. This suggests that many experimental setups in prior work lacked the necessary standardization to ensure generalizability.\n\nFurther compounding these issues, \\cite{rossi2020} conducted a comprehensive comparison of 18 state-of-the-art KGE methods for link prediction, critically examining the effect of design choices and exposing biases in standard evaluation practices. They highlighted that the common practice of aggregating accuracy over a large number of test facts, where some entities are vastly more represented than others, allows models to achieve good results by focusing on these high-frequency entities, thereby ignoring the majority of the knowledge graph. This inherent bias in benchmark datasets can lead to an overestimation of a model's true generalization capabilities, as its performance might be artificially boosted by exploiting statistical artifacts rather than genuinely learning complex relational patterns. This reveals a critical assumption made in many KGE evaluationsthat benchmark datasets provide a uniformly representative test bedwhich is often unrealistic.\n\nBeyond evaluation biases, the impact of hyperparameter tuning on KGE quality has also been rigorously investigated. \\cite{lloyd2022} employed Sobol sensitivity analysis to quantify the importance of various hyperparameters, revealing substantial variability in their sensitivities across different knowledge graphs. This implies that optimal hyperparameter configurations are often dataset-specific, making universal recommendations difficult and further complicating reproducibility. A particularly concerning finding was the identification of data leakage in the widely used UMLS-43 benchmark due to inverse relations, which could lead to artificially inflated performance metrics. This directly challenges the integrity of a common experimental setup and underscores the need for meticulous data curation and validation.\n\nCollectively, these studies reveal that the KGE field, while innovative in model development, has historically suffered from a lack of meta-scientific rigor. The methodological limitations include inconsistent implementations, biased evaluation metrics, and an underestimation of hyperparameter sensitivity. These issues directly impact the generalizability of findings, as models might be overfit to specific experimental conditions or benchmark quirks. The theoretical gap isn't necessarily in the KGE models themselves, but in the overarching framework for their empirical validation. Addressing these challenges requires a concerted community effort towards adopting unified frameworks, conducting transparent and reproducible experiments, and developing more robust, unbiased evaluation metrics that truly reflect a model's understanding of the knowledge graph. This move towards higher standards is crucial for fostering reliable scientific progress and ensuring the trustworthy deployment of KGE models in critical applications.\n\n\n\\label{sec:applications_and_real-world_impact_of_kge}\n\n\\label{sec:applications\\_\\_and\\_\\_real-world\\_impact\\_of\\_kge}\n\n\\section{Applications and Real-World Impact of KGE}\n\\label{sec:applications\\_and\\_real\\_world\\_impact\\_of\\_kge}\n\nFollowing the discussion on practical considerations for KGE models in Section \\ref{sec:practical\\_considerations\\_efficiency\\_robustness\\_and\\_evaluation}, which focused on ensuring their efficiency, robustness, and rigorous evaluation, this section shifts its attention to the tangible outcomes and widespread utility of these advancements. Here, we move beyond theoretical developments to showcase the diverse and significant real-world impact of knowledge graph embedding across various artificial intelligence applications. KGE models are no longer confined to academic benchmarks; they are actively leveraged to address complex problems across numerous domains, demonstrating their transformative potential in modern AI systems.\n\nThe subsequent subsections will delve into how KGE underpins fundamental tasks such as \\textit{link prediction and knowledge graph completion}, which are crucial for enhancing the completeness and inferential capabilities of knowledge bases \\cite{community\\_0, community\\_1}. We will then explore its vital role in \\textit{entity alignment}, enabling the seamless integration of heterogeneous knowledge sources by identifying equivalent entities across disparate graphs \\cite{community\\_5}. Furthermore, this section highlights the application of KGE in enhancing user-facing AI systems, specifically in improving the intelligence and personalization of \\textit{question answering} and \\textit{recommender systems} \\cite{community\\_1, community\\_3}. Finally, we will examine various \\textit{domain-specific applications}, from biological systems to patent analysis, emphasizing how KGE models are tailored to solve industry-specific challenges and often incorporate principles of \\textit{explainability} to build trust and provide actionable insights \\cite{community\\_2, community\\_6}. This comprehensive overview illustrates the practical utility, tangible benefits, and broad applicability of knowledge graph embedding techniques, underscoring their indispensable contribution to the evolution of intelligent systems that can effectively understand, reason with, and leverage vast amounts of structured knowledge.\n\n\\subsection{Link Prediction and Knowledge Graph Completion}\n\\label{sec:7\\_1\\_link\\_prediction\\_\\_and\\_\\_knowledge\\_graph\\_completion}\n\nLink prediction (LP) and knowledge graph completion (KGC) represent the fundamental applications of knowledge graph embedding (KGE), aiming to infer missing facts and enhance the completeness of knowledge graphs. These tasks are crucial for making KGs more robust and informative for downstream AI systems by automatically inferring unobserved facts within the graph structure. The evolution of KGE models for LP/KGC reflects a continuous effort to improve accuracy, handle complex relational patterns, and address practical challenges.\n\nEarly KGE models primarily leveraged geometric transformations to represent entities and relations. Translational models, such as TransH \\cite{wang2014} and TransD \\cite{ji2015}, extended the foundational TransE by modeling relations as translations on hyperplanes or through dynamic mapping matrices, respectively. TransH notably improved the handling of one-to-many and many-to-one relations by allowing entity projections, while TransD further refined this by considering the diversity of both entities and relations. However, these models, while efficient, often struggled to capture more intricate logical patterns like symmetry, antisymmetry, inversion, and composition. This limitation spurred the development of rotational models, with RotatE \\cite{sun2018} defining relations as rotations in complex vector spaces. RotatE demonstrated superior expressiveness for these complex patterns, significantly outperforming its translational predecessors in link prediction tasks. Further geometric innovations include embedding entities on Lie groups (e.g., TorusE \\cite{ebisu2017}) to address regularization issues, exploring alternative metrics like the Cycle metric \\cite{yang2021} for enhanced expressiveness, and introducing powerful transformations such as Householder parameterization (HousE \\cite{li2022}) or compound operations (CompoundE \\cite{ge2022}, CompoundE3D \\cite{ge2023}) to capture a broader spectrum of relational semantics. More recently, models like HolmE \\cite{zheng2024} have focused on ensuring closure under composition, a theoretical property vital for modeling under-represented compositional patterns, while MQuinE \\cite{liu2024} addresses specific theoretical deficiencies (\"Z-paradox\") in existing models to enhance expressiveness. While these geometric models offer mathematical elegance and interpretability, their ability to capture highly complex, non-linear interactions can be limited compared to deep learning approaches.\n\nThe advent of deep learning architectures marked a significant paradigm shift in KGE for LP/KGC. Convolutional Neural Networks (CNNs) have been widely adopted to extract local features and model interactions between entity and relation embeddings. Models like AcrE \\cite{ren2020} and ReInceptionE \\cite{xie2020} utilize various convolutional layers and attention mechanisms to capture complex relation patterns and aggregate entity-specific features, achieving state-of-the-art results. More recent CNN-based methods, such as CNN-ECFA \\cite{hu2024} and SEConv \\cite{yang2025}, continue to refine feature aggregation and interaction. Graph Neural Networks (GNNs) and attention mechanisms further leverage the graph's topology, with models like DisenKGAT \\cite{wu2021} employing disentangled graph attention networks for diverse representations and GAATs \\cite{wang2020} incorporating graph attenuated attention to capture rich neighborhood information. Transformer-based architectures, including CoKE \\cite{wang2019}, Knowformer \\cite{li2023}, and TGformer \\cite{shi2025}, treat KGs as sequences or integrate graph structures into Transformer frameworks, leveraging self-attention to capture long-range dependencies and contextualized representations. These deep learning models excel at learning intricate, non-linear, and context-dependent features, often surpassing purely geometric approaches in accuracy, but typically incur higher computational costs and can be less interpretable.\n\nBeyond structural information, KGE models for LP/KGC have been enriched by incorporating auxiliary information, logical rules, and multi-modal data. Integrating entity types, as seen in TransET \\cite{wang2021} and TaKE \\cite{he2023}, provides semantic guidance that significantly improves KG completion, especially in low-resource settings. Hyper-relational KGE models like HINGE \\cite{rosso2020} move beyond simple triplets to incorporate associated key-value pairs, capturing richer data semantics. Rule-based KGE approaches, such as RUGE \\cite{guo2017} and RulE \\cite{tang2022}, iteratively guide embedding learning with soft logical rules, enhancing reasoning capabilities and ensuring semantic consistency, though rule extraction and balancing rule adherence with flexibility remain challenges. Furthermore, multi-modal KGEs, exemplified by SSP \\cite{xiao2016} which projects text descriptions into semantic space, and Joint Language Semantic and Structure Embedding \\cite{shen2022} which integrates pre-trained language models, enrich embeddings with external semantic context, crucial for overcoming data sparsity and improving understanding.\n\nThe practical effectiveness of KGE for LP/KGC also heavily relies on training optimizations and robustness. Negative sampling strategies, such as Confidence-Aware Negative Sampling \\cite{shan2018} and NSCaching \\cite{zhang2018}, are critical for efficient and effective training, though the \"true\" negative distribution remains a theoretical gap. Some approaches, like Efficient Non-Sampling KGE \\cite{li2021}, even attempt to avoid negative sampling entirely to achieve more stable performance, albeit with increased computational complexity. Robustness against noisy data is addressed by methods like those using multi-task reinforcement learning \\cite{zhang2021} to filter out erroneous triples or weighted training schemes (e.g., WeightE \\cite{zhang2023}) to handle data imbalance. The continuous development across these diverse methodologies underscores the central role of link prediction and knowledge graph completion in advancing the utility and reliability of knowledge graphs for a wide array of AI applications \\cite{dai2020, cao2022, ge2023, rossi2020}.\n\\subsection{Entity Alignment}\n\\label{sec:7\\_2\\_entity\\_alignment}\n\nEntity Alignment (EA) is a critical task in knowledge graph integration, aiming to identify equivalent entities across different, often heterogeneous, knowledge graphs (KGs). The proliferation of KGs from diverse sources necessitates robust methods for their integration, which is fundamental for building comprehensive knowledge bases and enabling sophisticated cross-KG reasoning \\cite{dai2020, choudhary2021}. Knowledge Graph Embeddings (KGEs) have emerged as a powerful, data-driven approach to tackle this challenge, transforming symbolic entities and relations into low-dimensional vector spaces where semantic correspondences can be identified through similarity measures \\cite{yan2022, cao2022}. This approach leverages the ability of KGEs to capture intricate structural and semantic patterns, making them highly suitable for finding equivalences between disparate knowledge structures.\n\nA significant hurdle in embedding-based entity alignment is the scarcity of labeled training data, which can limit the accuracy and generalizability of models. To address this, bootstrapping methods have been developed. For instance, \\cite{sun2018} proposed an iterative bootstrapping approach that progressively labels likely entity alignments to augment the training data for learning alignment-oriented KG embeddings. This method strategically employs an alignment editing technique to mitigate the accumulation of errors during the iterative process, which is a common pitfall in self-training schemes. While effective in leveraging unlabeled data, the performance of such bootstrapping approaches can be sensitive to the quality of the initial seed alignments and the robustness of the error reduction mechanism, as false positives in early iterations can propagate and degrade overall accuracy.\n\nExtending beyond purely bootstrapping, semi-supervised learning frameworks have been introduced to more effectively utilize both limited labeled data and abundant unlabeled information. \\cite{pei2019} presented a semi-supervised entity alignment method (SEA) that not only leverages unlabeled entities but also incorporates an awareness of entity degree differences. This is crucial because entities with vastly different degrees (i.e., number of connections) can lead to biased embeddings, making alignment challenging, particularly between high-frequency and low-frequency entities. By employing adversarial training, SEA aims to learn more robust embeddings that are less affected by these structural disparities. This approach addresses a practical limitation of many KGE models, where embedding quality can be disproportionately influenced by highly connected entities, thereby improving alignment accuracy across the entire spectrum of entities. However, the complexity of adversarial training can introduce challenges in model stability and hyperparameter tuning.\n\nFurther enhancing the robustness and accuracy of EA, multi-view frameworks integrate diverse types of entity information. \\cite{zhang2019} proposed a novel multi-view KGE framework that unifies entity names, relational structures, and attributes to learn more comprehensive embeddings for alignment. Traditional KGE methods often focus predominantly on relational structures, overlooking other rich features that can provide complementary semantic cues. By combining these multiple views with various strategies and designing cross-KG inference methods, this approach significantly improves alignment performance. The strength of multi-view learning lies in its ability to capture a broader spectrum of semantic information, making the embeddings more discriminative. Nevertheless, the challenge lies in effectively weighting and integrating potentially conflicting signals from different views, and the computational overhead increases with the number of views considered.\n\nMore recently, the integration of ontological information has provided another powerful dimension for entity alignment. \\cite{xiang2021} introduced OntoEA, an ontology-guided entity alignment method that jointly embeds both KGs and their associated ontologies. This approach explicitly utilizes critical meta-information, such as class hierarchies and class disjointness constraints, which are often ignored by purely structural or attribute-based methods. By enforcing these ontological constraints during the embedding process, OntoEA can prevent false mappings and guide the alignment towards semantically coherent equivalences. This addresses a theoretical gap where KGEs, while powerful, often lack an explicit mechanism to incorporate higher-level schema knowledge. The effectiveness of OntoEA, however, is contingent on the availability and quality of consistent ontological information across the KGs being aligned, which may not always be present in real-world scenarios.\n\nThe collective advancements in these KGE-based EA methods underscore a clear evolution in the field. Early approaches focused on leveraging structural similarity, while subsequent methods have progressively integrated more semantic context, auxiliary information, and robust learning paradigms to overcome limitations like data scarcity and feature incompleteness. Comprehensive surveys and experimental reviews, such as those by \\cite{zhu2024} and \\cite{fanourakis2022}, further highlight the strengths and weaknesses of various KGE-based EA techniques. They emphasize the need for more robust noise filtering strategies, better utilization of additional information, and rigorous comparative analyses across diverse datasets to ensure generalizability. These meta-analyses confirm that KGEs provide a versatile and powerful foundation for integrating heterogeneous knowledge sources, enabling the construction of more comprehensive knowledge bases and facilitating complex cross-KG reasoning, which is crucial for advancing knowledge-driven AI applications.\n\\subsection{Question Answering and Recommendation Systems}\n\\label{sec:7\\_3\\_question\\_answering\\_\\_and\\_\\_recommendation\\_systems}\n\nKnowledge Graph Embeddings (KGEs) have emerged as a pivotal technology for bridging the semantic gap between natural language and structured knowledge, enabling more intelligent and interpretable interactions in diverse applications such as Question Answering (QA) and Recommender Systems. These applications leverage KGEs to transform complex symbolic reasoning into efficient vector space operations, thereby enhancing performance and user experience \\cite{dai2020, cao2022}.\n\nIn the realm of Question Answering over Knowledge Graphs (QA-KG), KGEs facilitate the understanding of natural language queries by mapping them to entities and relations within the underlying knowledge graph. Early frameworks, such as Knowledge Embedding based Question Answering (KEQA), demonstrated the utility of KGEs by jointly recovering head entity, predicate, and tail entity representations in the embedding space to answer simple natural language questions \\cite{huang2019}. KEQA's strength lies in its ability to address predicate variability and entity ambiguity by leveraging the semantic proximity captured by embeddings. However, its focus on \"simple questions\" highlights a limitation in handling more complex, multi-hop, or nuanced queries, which often require deeper integration with natural language processing (NLP) capabilities.\n\nMore advanced QA systems have evolved into hybrid architectures that seamlessly integrate KGEs with sophisticated NLP models. A prime example is the Marie and BERT system for chemistry, which showcases a comprehensive approach to domain-specific QA \\cite{zhou2023}. This system employs hybrid KGEs, leveraging multiple embedding spaces to capture diverse relational patterns, and integrates a BERT-based entity-linking model to enhance robustness and accuracy in identifying entities from natural language queries. Furthermore, Marie and BERT addresses the complexities of deep ontologies by deriving implicit multi-hop relations and incorporates mechanisms for numerical filtering, demonstrating a significant leap in handling intricate, fact-oriented information retrieval in specialized domains. While such hybrid systems offer superior performance in complex scenarios, their domain specificity and the inherent complexity of integrating heterogeneous components (KGEs, BERT, semantic agents) can limit generalizability and increase development overhead. The methodological challenge lies in effectively harmonizing the continuous vector representations from KGEs with the discrete, symbolic reasoning often required for precise QA.\n\nFor recommender systems, KGEs provide a powerful mechanism to model user preferences and item characteristics by representing them as entities in a knowledge graph, thereby enabling more personalized and transparent suggestions. Recurrent Knowledge Graph Embedding (RKGE) was an early and influential approach that utilized a recurrent network to automatically learn semantic representations of paths between entities \\cite{sun2018}. By fusing these path semantics into the recommendation process, RKGE not only improved recommendation accuracy but also offered meaningful explanations based on path saliency, a crucial step towards interpretable recommendations. The recurrent network architecture allowed RKGE to discriminate the saliency of different paths in characterizing user preferences, moving beyond traditional feature engineering that often requires extensive domain knowledge.\n\nBuilding upon this foundation, recent research has pushed towards more contextualized and explainable approaches. Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation (CKGE) represents a significant advancement in this direction \\cite{yang2023}. CKGE constructs \"meta-graphs\" for talent-course pairs, incorporating contextualized neighbor semantics and high-order connections as \"motivation-aware information.\" It then employs a novel KG-based Transformer, equipped with relational attention and structural encoding, to model the global dependencies of KG structured data. A key innovation in CKGE is its \"local path mask prediction,\" which effectively reveals the importance of different paths, thereby offering precise and explainable recommendations that can discriminate the saliencies of meta-paths in characterizing corresponding preferences \\cite{yang2023}.\n\nComparing RKGE and CKGE, the evolution highlights a shift from recurrent networks for path modeling to more sophisticated Transformer architectures that capture richer contextual information. While RKGE provided explanations based on path saliency, CKGE offers a more granular and motivation-aware interpretability by integrating contextualization and high-order connections. This progression, as noted in the development directions, reflects an acceleration in research driven by the demand for more sophisticated, robust, and interpretable recommendation solutions. However, the increased complexity of models like CKGE, with their meta-graph construction and Transformer integration, introduces trade-offs in computational cost and the interpretability of the underlying embedding space. The theoretical gap remains in developing truly intuitive and actionable explanations that are universally understandable to human users, rather than merely technical artifacts of the model.\n\nIn summary, KGEs have profoundly impacted QA and recommender systems by providing a robust framework to bridge natural language and structured knowledge. From foundational KEQA to hybrid systems like Marie and BERT, and from recurrent RKGEs to contextualized CKGEs, these advancements demonstrate KGE's utility in enabling more intelligent, personalized, and interpretable user interactions. The ongoing challenge lies in balancing model expressiveness with computational efficiency and ensuring that the generated explanations are genuinely transparent and actionable across diverse application contexts.\n\\subsection{Domain-Specific Applications and Explainability}\n\\label{sec:7\\_4\\_domain-specific\\_applications\\_\\_and\\_\\_explainability}\n\nThe utility of Knowledge Graph Embedding (KGE) models extends significantly into specialized, high-stakes domains, where their ability to transform symbolic knowledge into actionable insights is paramount. This section highlights the application of KGE in fields such as biological systems, patent metadata analysis, and drug repurposing, emphasizing how these models are tailored, validated, and increasingly scrutinized for explainability to build trust and deliver verifiable solutions.\n\nIn biological and biomedical systems, KGE models offer powerful tools for discovery and analysis. \\cite{mohamed2020} provides a comprehensive review of KGE applications in this domain, showcasing their predictive and analytical capabilities for tasks like drug-target interactions and polypharmacy side effects. The authors argue that KGEs are a natural fit for representing complex biological knowledge, overcoming the scalability limitations of traditional graph exploratory approaches. Building on this, \\cite{zhu2022} demonstrates multimodal reasoning based on KGE for specific diseases. They construct Specific Disease Knowledge Graphs (SDKGs) and integrate structural, category, and description embeddings using reverse-hyperplane projection. This multimodal approach enhances the discovery of new, reliable knowledge, underscoring the value of tailoring KGEs to domain-specific knowledge structures and leveraging diverse data modalities to improve reasoning.\n\nA particularly compelling example of KGE application in a critical domain is drug repurposing for diseases like COVID-19. \\cite{islam2023} proposes an innovative approach that utilizes ensemble KGEs to generate robust latent representations, which are then fed into a deep neural network for identifying potential drug candidates. Crucially, this work moves beyond generic KGE evaluation metrics by incorporating \\textit{molecular docking} to validate predictions, a domain-specific and verifiable method for assessing drug-target interactions. This integration of molecular-level evaluation is a significant step towards delivering transparent solutions, as it provides concrete, scientific validation for the abstract KGE predictions. Furthermore, \\cite{islam2023} addresses the paramount need for explainability by providing explanations through rules extracted from the knowledge graph and instantiated by explanatory paths. This allows medical professionals and researchers to understand \\textit{why} a particular drug is predicted, fostering trust and enabling actionable insights in a field where decisions have direct human impact.\n\nBeyond biomedicine, KGEs are also applied to analyze complex structured data like patent metadata. \\cite{li2022} operationalizes knowledge proximity within the US Patent Database by training KGE models on a \"PatNet\" knowledge graph constructed from patent citations, inventors, assignees, and domain classifications. By using cosine similarity between learned embeddings, they measure knowledge proximity between homogeneous (e.g., patent-patent) and heterogeneous (e.g., inventor-assignee) entities. This application demonstrates how KGEs can be tailored to specific industry problems, providing quantitative measures for abstract concepts like \"knowledge proximity\" and enabling the analysis of domain expansion profiles for inventors and assignees. While this work primarily focuses on predictive performance, the inherent interpretability of proximity measures in the embedding space can offer insights into innovation landscapes.\n\nThe growing demand for interpretable KGE models in these high-stakes fields is a significant evolutionary trend. The explicit focus on explainability in \\cite{islam2023} highlights a shift from merely achieving high performance on abstract metrics to delivering verifiable and transparent solutions. This aligns with broader research efforts in making KGEs more understandable. For instance, methods that integrate logical rules and constraints into the embedding process, such as \\cite{guo2017}'s iterative guidance from soft rules (RUGE) or \\cite{tang2022}'s RulE framework, which learns rule embeddings jointly with entity and relation embeddings, inherently contribute to explainability. By aligning embeddings with human-understandable logical patterns, these approaches can provide a basis for explaining model predictions. Similarly, \\cite{ding2018} showed that even simple constraints like non-negativity on entity representations can improve model interpretability by structuring the embedding space.\n\nHowever, achieving robust explainability in complex KGE models, especially those leveraging deep learning architectures, remains a challenge. The trade-off often lies between the high expressiveness and predictive power of complex models and the inherent difficulty in extracting clear, human-understandable explanations from their latent spaces. While \\cite{islam2023} successfully combines ensemble KGEs with molecular docking and rule-based explanations, the generalizability of such multi-faceted explanation strategies across all domain-specific KGE applications requires further investigation. The theoretical gap in universally interpretable embedding spaces, particularly for highly non-linear models, prevents a straightforward solution to providing transparent insights for every prediction. Nevertheless, the explicit integration of domain-specific validation and explanation mechanisms, as exemplified by these works, marks a crucial step towards building trust and enabling the responsible deployment of KGE technologies in critical real-world scenarios.\n\n\n\\label{sec:conclusion_and_future_directions}\n\n\\section{Conclusion and Future Directions}\n\\label{sec:conclusion\\_\\_and\\_\\_future\\_directions}\n\n\\label{sec:conclusion\\_\\_and\\_\\_future\\_directions}\n\\section{Conclusion and Future Directions}\n\\label{sec:conclusion\\_and\\_future\\_directions}\n\nHaving explored the diverse applications and real-world impact of Knowledge Graph Embedding (KGE) models in Section \\ref{sec:applications\\_and\\_real\\_world\\_impact\\_of\\_kge}, this concluding section offers a comprehensive synthesis of the field's intellectual trajectory and charts a course for its future. We reflect on the remarkable progression of KGE research, which has evolved from foundational geometric and algebraic models to sophisticated deep learning architectures and advanced application-driven solutions. This journey has seen continuous efforts to enhance model expressiveness, efficiency, and robustness, leading to KGEs becoming indispensable tools for tasks ranging from link prediction and entity alignment to complex question answering and recommendation systems \\cite{community\\_0, community\\_1, community\\_3}.\n\nDespite these significant advancements, the field of KGE still grapples with persistent open challenges and theoretical gaps. These include the intricate balance between model complexity and interpretability, the efficient integration of high-quality logical rules, and the development of truly scalable and generalizable inductive models capable of handling dynamic, ever-evolving knowledge graphs \\cite{community\\_1, community\\_6}. Addressing these limitations is crucial for unlocking the full potential of KGE technologies. Looking ahead, this section will outline several emerging trends that are poised to redefine the landscape of KGE, notably the increasing integration with large language models for richer semantic understanding, advancements in adaptive multi-curvature embeddings, and the critical development of federated and privacy-preserving KGE methods \\cite{85064a4b1b96863af4fccff9ad34ce484945ad7b, community\\_1}. Finally, we will delve into the paramount ethical considerations surrounding KGE, including potential biases in learned representations and the imperative for transparent and responsible deployment in sensitive applications. This forward-looking perspective aims to inspire new research directions and guide the responsible advancement of KGE technologies towards a more intelligent and ethically sound future.\n\n\\subsection{Summary of Key Developments}\n\\label{sec:8\\_1\\_summary\\_of\\_key\\_developments}\n\nThe field of Knowledge Graph Embedding (KGE) has undergone a remarkable evolution, transitioning from foundational geometric and algebraic models to highly sophisticated deep learning architectures, driven by a continuous pursuit of enhanced expressiveness, efficiency, and robustness. Early advancements were rooted in the geometric paradigm, where relations were conceptualized as transformations within continuous vector spaces. Pioneering models like TransE and its successors, such as TransH \\cite{wang2014} and TransD \\cite{ji2015}, established the translation-based approach, modeling relations as vector translations from head to tail entities. These early efforts focused on refining the embedding space to better capture diverse relational patterns, with TransD introducing dynamic mapping matrices for finer-grained distinctions between entities and relations, thereby improving expressiveness while managing parameter complexity \\cite{ji2015}. Further geometric innovations explored non-Euclidean spaces, such as TorusE \\cite{ebisu2017} embedding on Lie groups to address regularization challenges, and CyclE \\cite{yang2021} investigating the impact of metric choices (e.g., Cycle vs. Minkowski) on expressiveness. These models underscored the importance of the underlying geometry in accurately representing complex knowledge.\n\nA significant paradigm shift occurred with the integration of deep learning architectures, which enabled KGE models to automatically learn intricate features and structural patterns. Convolutional Neural Networks (CNNs) were adapted to KGE, with models like AcrE \\cite{ren2020} leveraging atrous convolutions and residual learning for efficient feature interactions, and ReInceptionE \\cite{xie2020} employing inception networks and attention for joint local-global structural information. More recent CNN-based approaches, such as CNN-ECFA \\cite{hu2024} and SEConv \\cite{yang2025}, continue to refine feature aggregation for improved performance. Graph Neural Networks (GNNs) further enhanced KGE by capturing neighborhood context and structural information through message passing, exemplified by DisenKGAT \\cite{wu2021}, which introduced disentangled graph attention networks for more diverse and independent component representations. The emergence of Transformer architectures, as seen in CoKE \\cite{wang2019}, Knowformer \\cite{li2023}, and TGformer \\cite{shi2025}, marked another leap, enabling contextualized embeddings by treating KGs as sequences or integrating graph structures into self-attention mechanisms, thereby capturing long-range dependencies and multi-structural features.\n\nBeyond core architectural advancements, the field has continuously sought to enrich KGE models with auxiliary information, logical rules, and multi-modal data to overcome inherent limitations. Approaches like TransET \\cite{wang2021} and TaKE \\cite{he2023} demonstrated the value of incorporating entity type information to provide semantic guidance and improve knowledge graph completion. Similarly, models like HINGE \\cite{rosso2020} moved \"beyond triplets\" to directly learn from hyper-relational facts, capturing richer data semantics. The integration of logical rules, as in RUGE \\cite{guo2017} and RulE \\cite{tang2022}, allowed KGE models to inject prior knowledge and enforce semantic consistency, moving towards more robust and interpretable reasoning. Furthermore, multi-modal KGE, exemplified by SSP \\cite{xiao2016} integrating text descriptions and recent works leveraging pre-trained language models \\cite{shen2022}, has addressed data sparsity and enhanced semantic understanding by fusing diverse information sources.\n\nThe practical deployment of KGE models has driven significant research into efficiency, robustness, and adaptability. Efforts to enhance efficiency include knowledge distillation (DualDE \\cite{zhu2020}), embedding compression \\cite{sachan2020}, parameter-efficient learning (EARL \\cite{chen2023}), and optimized training systems like GE2 \\cite{zheng2024}. Robustness has been improved through techniques like confidence-aware negative sampling \\cite{shan2018}, reinforcement learning-based noise filtering \\cite{zhang2021}, and weighted training for imbalanced data \\cite{zhang2023}. The challenge of dynamic KGs has led to inductive KGE models leveraging neighborhood aggregation \\cite{wang2018} and meta-learning \\cite{chen2021, sun2024}, as well as continual learning approaches like incremental LoRA for efficient updates \\cite{liu2024}. Moreover, the rise of federated learning has spurred research into privacy-preserving KGE, addressing communication efficiency \\cite{zhang2024} and personalization \\cite{zhang2024\\_personalized}, while also acknowledging security vulnerabilities like poisoning attacks \\cite{zhou2024}.\n\nIn summary, the KGE landscape has evolved from simple geometric models to complex deep learning architectures, continuously pushing the boundaries of expressiveness and efficiency. The field's progression is marked by a holistic approach: enhancing core models, enriching them with diverse contextual and logical information, and ensuring their practical utility through robust, scalable, and adaptable designs. These advancements underscore the significant strides made in transforming symbolic knowledge into actionable insights, making KGE a cornerstone for intelligent AI systems across various applications, from link prediction and question answering \\cite{huang2019, zhou2023} to recommendation systems \\cite{sun2018, yang2023}.\n\\subsection{Open Challenges and Theoretical Gaps}\n\\label{sec:8\\_2\\_open\\_challenges\\_\\_and\\_\\_theoretical\\_gaps}\n\nDespite significant advancements in Knowledge Graph Embedding (KGE) research, several critical open challenges and theoretical gaps persist, representing fertile ground for future investigation. These issues often stem from inherent trade-offs, the complexity of real-world knowledge graphs, and the limitations of current theoretical understandings.\n\nOne pervasive challenge lies in balancing model expressiveness with computational complexity. While models like RotatE \\cite{sun2018} and the composition-closed HolmE \\cite{zheng2024} have pushed the boundaries of capturing intricate relational patterns, their increased expressiveness often comes at the cost of higher computational demands for training and inference (Subgroup 1, Overall Perspective). Efforts in efficiency and compression, such as DualDE's distillation \\cite{zhu2020} and LightKG's codebook-based storage \\cite{wang2021}, aim to mitigate this. However, as noted by \\cite{sachan2020}, these often entail a \"minor loss in performance.\" The theoretical gap here is to devise architectures that are \\textit{inherently} expressive yet computationally lean, perhaps through novel mathematical formulations like the Orthogonal Procrustes Analysis in \\cite{peng2021}, which offers a closed-form solution for efficiency, rather than relying on post-hoc compression or distillation.\n\nEnsuring the interpretability of complex deep learning KGE models is another significant hurdle. As KGE increasingly leverages Graph Neural Networks (GNNs) and Transformer architectures (Subgroup 6, Advanced Model Design), their black-box nature becomes a concern, especially in high-stakes applications. While some models like SpherE \\cite{li2024} claim interpretability through their geometric properties, this is often specific to the model's design and does not generalize to the complex reasoning paths learned by deep networks. Application-focused works, such as the explainable drug repurposing by \\cite{islam2023} and contextualized recommendation by \\cite{yang2023}, demonstrate the \\textit{need} for explanations, but typically rely on post-hoc rule extraction or path saliency. A theoretical gap exists in developing intrinsically interpretable deep KGE models that can transparently reveal their reasoning processes without sacrificing predictive power.\n\nThe efficient extraction and integration of high-quality logical rules remain a bottleneck. While methods like RUGE \\cite{guo2017} and RulE \\cite{tang2022} have shown the value of incorporating soft rules to enhance reasoning and consistency (Subgroup 3, Rule-based \\& Constraint-driven KGE), the process of obtaining these rules and balancing their adherence with the flexibility to capture exceptions is challenging. Automatically extracted rules often carry \"uncertainties\" \\cite{guo2017}, and the scalability of integrating complex rule sets can be problematic \\cite{guo2020}. The theoretical challenge lies in developing robust, automated rule induction systems that can generate high-fidelity rules from noisy KGs and seamlessly integrate them into embedding models without introducing significant computational overhead or compromising the data-driven learning of nuanced patterns.\n\nFurthermore, resolving issues related to the 'true' negative distribution in training is fundamental. KGE models rely heavily on negative sampling for contrastive learning, yet the \"true\" negative distribution is inherently unknown (Subgroup 3, Negative Sampling \\& Training Optimization). While approaches like NSCaching \\cite{zhang2018} and confidence-aware sampling \\cite{shan2018} improve efficiency and robustness to noise, they are still heuristic approximations. The \"Efficient Non-Sampling Knowledge Graph Embedding\" \\cite{li2021} attempts to bypass sampling entirely, but requires complex mathematical derivations to manage computational complexity. As highlighted by comprehensive reviews \\cite{qian2021, madushanka2024}, this remains a persistent challenge, particularly when extending to multi-modal KGE \\cite{zhang2023}. A theoretical breakthrough is needed to either accurately model the true negative distribution or develop training paradigms that are robust to its uncertainty without prohibitive computational costs.\n\nThe field also critically needs more robust and unbiased evaluation metrics. As revealed by the \"Evaluation, Benchmarking, and Reproducibility\" subgroup, standard metrics often suffer from biases, such as the over-representation of certain entities \\cite{rossi2020}, and reported results can be sensitive to hyperparameter choices or even data leakage \\cite{lloyd2022}. The lack of standardized frameworks and reproducibility issues \\cite{ali2020, broscheit2020} further complicate fair comparisons. The theoretical gap is in developing evaluation protocols that are robust to dataset characteristics, sensitive to diverse relational patterns (e.g., long-tail, compositional), and truly reflect real-world application performance, moving beyond simple link prediction accuracy.\n\nThe challenges of scalability for extremely large and dynamic knowledge graphs are paramount. While progress has been made in efficiency \\cite{peng2021, zheng2024} and compression \\cite{sachan2020, wang2021}, handling KGs with billions of triples and continuous, real-time updates remains difficult. Dynamic KGE methods like FastKGE \\cite{liu2024} and MetaHG \\cite{sun2024} focus on efficient updates and mitigating catastrophic forgetting, but the sheer volume and velocity of changes in real-world KGs can still overwhelm these systems. Federated KGE introduces additional complexities related to communication efficiency \\cite{zhang2024} and personalized aggregation for diverse client data \\cite{zhang2024\\_personalized}. The theoretical challenge is to design truly elastic and adaptive KGE architectures that can scale horizontally, handle continuous streams of new information, and maintain global consistency and performance in highly distributed and dynamic environments without prohibitive computational or communication costs.\n\nFinally, the development of truly generalizable inductive models remains an open problem. While neighborhood aggregation \\cite{wang2018} and meta-learning approaches \\cite{chen2021, sun2024} have enabled inductive capabilities for new entities (Subgroup 4, Dynamic, Inductive, and Continual KGE), they often rely on existing neighbors or transferable meta-knowledge. Truly novel entities with entirely new semantic or structural patterns, or isolated entities with sparse connections, still pose a significant challenge. A deeper theoretical understanding of \"semantic evidence\" for extrapolation \\cite{li2021} is required to build models that can robustly infer representations for such unseen entities or entirely new relational types, pushing beyond mere interpolation to genuine generalization. These challenges collectively underscore the need for continued fundamental and applied research to unlock the full potential of KGE.\n\\subsection{Emerging Trends and Ethical Considerations}\n\\label{sec:8\\_3\\_emerging\\_trends\\_\\_and\\_\\_ethical\\_considerations}\n\nThe landscape of Knowledge Graph Embedding (KGE) is continuously evolving, driven by both technological advancements and a growing awareness of societal impact. This section delves into key emerging trends that are shaping the future of KGE research, alongside crucial ethical considerations that must guide its development. These discussions are informed by the field's methodological evolution, knowledge progression, and the increasing demand for robust, responsible AI systems.\n\nOne of the most significant emerging trends is the deeper integration of KGE with pre-trained language models (PLMs) for richer semantic understanding. Traditional KGE models, as discussed in the \"Core KGE Model Architectures and Expressiveness\" subgroup, primarily learn representations from the structural patterns of knowledge graphs \\cite{wang2014, sun2018, zheng2024}. While effective, these models often struggle with data sparsity and entities lacking sufficient structural connections, a limitation highlighted in the \"Knowledge Progression\" of KGE research. PLMs, on the other hand, excel at capturing rich contextual semantics from vast amounts of text. Hybrid approaches, such as those that leverage BERT-based models for entity linking and contextual understanding within KGE systems \\cite{zhou2023}, represent a powerful synergy. This integration allows KGEs to infer meaning from textual descriptions, thereby enhancing embedding quality and addressing the cold-start problem for new entities. Surveys like \\cite{dai2020} and \\cite{cao2022} have begun to acknowledge the potential of incorporating textual information, but the current trend moves towards more sophisticated, joint learning frameworks that align and fuse representations from both modalities. A key challenge here lies in effectively bridging the gap between the discrete, symbolic nature of KGs and the continuous, contextualized space of PLMs, while managing the increased computational complexity.\n\nAnother prominent trend involves the development of more adaptive multi-curvature embeddings. As explored in the \"Geometric KGE for Hierarchical and Complex Structures\" subgroup, hyperbolic spaces have demonstrated superior capabilities for modeling hierarchical structures due to their negative curvature \\cite{pan2021, liang2024}. However, real-world knowledge graphs often exhibit a mixture of structural patternshierarchical, cyclic, and Euclidean-like. The emerging direction is to move beyond single-geometry embeddings towards models that can adaptively utilize different curvatures (e.g., hyperbolic, spherical, Euclidean) for different parts of a knowledge graph or for different types of relations. Approaches like \\cite{shang2024}, which propose mixed geometry message functions and scoring functions, exemplify this trend. By integrating information from multiple geometric spaces, these models aim to capture diverse local structures with higher fidelity and fewer dimensions, offering a more expressive and compact representation than any single geometry could provide. The methodological challenge lies in designing robust mechanisms for dynamically selecting or combining appropriate geometries, ensuring stable training, and avoiding an explosion in model complexity. Furthermore, models like \\cite{li2024} which embed entities as spheres, extend rotational embeddings to better capture many-to-many relations and enable set retrieval, showcasing the continuous innovation in geometric KGE.\n\nAdvancements in federated and privacy-preserving KGE also constitute a critical emerging trend, directly addressing the practical and ethical concerns of data distribution and privacy. The \"Federated KGE, Privacy, and Security\" subgroup highlights the growing interest in collaboratively training KGE models across distributed knowledge graphs without centralizing sensitive data. This is crucial for applications where data privacy regulations (e.g., GDPR) are paramount. Recent works focus on improving communication efficiency, such as \\cite{zhang2024} which proposes entity-wise Top-K sparsification to reduce transmitted parameters. Furthermore, addressing data heterogeneity among clients is vital, leading to personalized federated KGE approaches that learn client-specific supplementary knowledge \\cite{zhang2024}. However, this distributed paradigm introduces new security vulnerabilities, as demonstrated by poisoning attacks that can manipulate model outcomes by injecting malicious data indirectly through aggregation \\cite{zhou2024}. The trade-off between privacy guarantees, communication efficiency, personalization, and robustness against adversarial attacks remains a central challenge, requiring sophisticated cryptographic techniques and robust aggregation mechanisms.\n\nBeyond these technological advancements, crucial ethical considerations are increasingly guiding KGE research. Foremost among these is the issue of potential biases in learned representations. KGE models learn from existing knowledge graphs, which are often constructed from diverse sources reflecting historical, societal, or cultural biases. If the training data contains skewed representations (e.g., gender stereotypes, racial biases, under-representation of certain groups), the KGE model will inevitably learn and potentially amplify these biases. This can lead to discriminatory outcomes in downstream applications, such as biased recommendations or unfair decision-making in sensitive domains. While the \"Evaluation, Benchmarking, and Reproducibility\" subgroup has focused on hyperparameter effects \\cite{lloyd2022} and evaluation biases \\cite{rossi2020}, there is a growing imperative to explicitly detect, measure, and mitigate these semantic biases within the embedding space.\n\nThe responsible use of KGE in sensitive applications is another paramount ethical concern. As KGE models are increasingly deployed in high-stakes domains like healthcare (e.g., drug repurposing for COVID-19 \\cite{islam2023}), finance, and legal systems, the consequences of erroneous or biased predictions can be severe. The \"Domain-Specific Application and Explainability\" subgroup underscores the need for rigorous, domain-specific validation and explainability in such contexts. For instance, in drug repurposing, molecular evaluation is integrated to verify predictions \\cite{islam2023}. This highlights a critical shift from solely optimizing for accuracy on standard benchmarks to ensuring real-world safety, fairness, and accountability. The assumptions made during model design and the generalizability of experimental setups must be critically scrutinized, especially when findings from one domain are applied to another.\n\nFinally, the imperative for transparent and explainable AI systems is gaining traction. Complex KGE models, particularly those leveraging deep learning architectures like GNNs or Transformers, often operate as \"black boxes,\" making it difficult to understand \\textit{why} a particular prediction or recommendation was made. The \"KGE for Downstream Applications and Explainability\" subgroup directly addresses this, with models like CKGE \\cite{yang2023} and RKGE \\cite{sun2018} aiming to provide explainable recommendations through path saliency or contextualized neighbor semantics. For sensitive applications, mere performance is insufficient; users and stakeholders need to trust and verify the model's reasoning. This necessitates the development of KGE models that can provide human-understandable explanations, whether through extracting logical rules, highlighting influential paths, or visualizing attention mechanisms. The theoretical gap often lies in balancing the high expressiveness of complex models with the inherent simplicity required for genuine interpretability, presenting a continuous trade-off that future research must navigate. These emerging trends and ethical considerations collectively define the next frontier for KGE research, demanding not only technological ingenuity but also a strong commitment to societal responsibility.\n\n\n\\newpage\n\\section*{References}\n\\addcontentsline{toc}{section}{References}\n\n\\begin{thebibliography}{377}\n\n\\bibitem{sun2018}\nZequn Sun, Wei Hu, Qingheng Zhang, et al. (2018). \\textit{Bootstrapping Entity Alignment with Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{dasgupta2018}\nS. Dasgupta, Swayambhu Nath Ray, and P. Talukdar (2018). \\textit{HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{chen2023}\nMingyang Chen, Wen Zhang, Zhen Yao, et al. (2023). \\textit{Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{yang2023}\nYang Yang, Chubing Zhang, Xin Song, et al. (2023). \\textit{Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation}. ACM Trans. Inf. Syst..\n\n\\bibitem{jia2015}\nYantao Jia, Yuanzhuo Wang, Hailun Lin, et al. (2015). \\textit{Locally Adaptive Translation for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{lloyd2022}\nOliver Lloyd, Yi Liu, and T. Gaunt (2022). \\textit{Assessing the effects of hyperparameters on knowledge graph embedding quality}. Journal of Big Data.\n\n\\bibitem{wu2021}\nJunkang Wu, Wentao Shi, Xuezhi Cao, et al. (2021). \\textit{DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network}. International Conference on Information and Knowledge Management.\n\n\\bibitem{xu2019}\nChengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2019). \\textit{Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition}. arXiv.org.\n\n\\bibitem{shan2018}\nYingchun Shan, Chenyang Bu, Xiaojian Liu, et al. (2018). \\textit{Confidence-Aware Negative Sampling Method for Noisy Knowledge Graph Embedding}. International Conference on Big Knowledge.\n\n\\bibitem{zheng2024}\nZhuoxun Zheng, Baifan Zhou, Hui Yang, et al. (2024). \\textit{Knowledge graph embedding closed under composition}. Data mining and knowledge discovery.\n\n\\bibitem{he2023}\nPeng He, Gang Zhou, Yao Yao, et al. (2023). \\textit{A type-augmented knowledge graph embedding framework for knowledge graph completion}. Scientific Reports.\n\n\\bibitem{xiao2015}\nHan Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \\textit{TransG : A Generative Model for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{guo2017}\nShu Guo, Quan Wang, Lihong Wang, et al. (2017). \\textit{Knowledge Graph Embedding with Iterative Guidance from Soft Rules}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{chen2021}\nMingyang Chen, Wen Zhang, Yushan Zhu, et al. (2021). \\textit{Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\n\\bibitem{li2023}\nGuang-pu Li, Zequn Sun, Wei Hu, et al. (2023). \\textit{Position-Aware Relational Transformer for Knowledge Graph Embedding}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{zhou2023}\nXiaochi Zhou, Shaocong Zhang, Mehal Agarwal, et al. (2023). \\textit{Marie and BERTA Knowledge Graph Embedding Based Question Answering System for Chemistry}. ACS Omega.\n\n\\bibitem{xiang2021}\nYuejia Xiang, Ziheng Zhang, Jiaoyan Chen, et al. (2021). \\textit{OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding}. Findings.\n\n\\bibitem{cao2022}\nJiahang Cao, Jinyuan Fang, Zaiqiao Meng, et al. (2022). \\textit{Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces}. ACM Computing Surveys.\n\n\\bibitem{wang2021}\nPeng Wang, Jing Zhou, Yuzhang Liu, et al. (2021). \\textit{TransET: Knowledge Graph Embedding with Entity Types}. Electronics.\n\n\\bibitem{guo2020}\nShu Guo, Lin Li, Zhen Hui, et al. (2020). \\textit{Knowledge Graph Embedding Preserving Soft Logical Regularity}. International Conference on Information and Knowledge Management.\n\n\\bibitem{zhang2024}\nXiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \\textit{Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification}. Knowledge-Based Systems.\n\n\\bibitem{shen2022}\nJianhao Shen, Chenguang Wang, Linyuan Gong, et al. (2022). \\textit{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion}. International Conference on Computational Linguistics.\n\n\\bibitem{hu2024}\nKairong Hu, Xiaozhi Zhu, Hai Liu, et al. (2024). \\textit{Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation for Knowledge Graph Embedding Learning}. IEEE transactions on consumer electronics.\n\n\\bibitem{liu2024}\nYang Liu, Huang Fang, Yunfeng Cai, et al. (2024). \\textit{MQuinE: a Cure for Z-paradox in Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{zhang2019}\nYongqi Zhang, Quanming Yao, Wenyuan Dai, et al. (2019). \\textit{AutoSF: Searching Scoring Functions for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.\n\n\\bibitem{yang2019}\nShihui Yang, Jidong Tian, Honglun Zhang, et al. (2019). \\textit{TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{xie2023}\nZhiwen Xie, Runjie Zhu, Jin Liu, et al. (2023). \\textit{TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding}. IEEE/ACM Transactions on Audio Speech and Language Processing.\n\n\\bibitem{wang2024}\nJiapu Wang, Boyue Wang, Junbin Gao, et al. (2024). \\textit{MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion}. IEEE Transactions on Cybernetics.\n\n\\bibitem{xiao2019}\nHan Xiao, Yidong Chen, and X. Shi (2019). \\textit{Knowledge Graph Embedding Based on Multi-View Clustering Framework}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{sachan2020}\nMrinmaya Sachan (2020). \\textit{Knowledge Graph Embedding Compression}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{madushanka2024}\nTiroshan Madushanka, and R. Ichise (2024). \\textit{Negative Sampling in Knowledge Graph Representation Learning: A Review}. arXiv.org.\n\n\\bibitem{zhu2022}\nChaoyu Zhu, Zhihao Yang, Xiaoqiong Xia, et al. (2022). \\textit{Multimodal reasoning based on knowledge graph embedding for specific diseases}. Bioinform..\n\n\\bibitem{liang2024}\nQiuyu Liang, Weihua Wang, F. Bao, et al. (2024). \\textit{Fully Hyperbolic Rotation for Knowledge Graph Embedding}. European Conference on Artificial Intelligence.\n\n\\bibitem{li2024}\nLi, Yuyi Ao, and Jingrui He (2024). \\textit{SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\n\\bibitem{ebisu2017}\nTakuma Ebisu, and R. Ichise (2017). \\textit{TorusE: Knowledge Graph Embedding on a Lie Group}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{zhang2021}\nZhao Zhang, Fuzhen Zhuang, Hengshu Zhu, et al. (2021). \\textit{Towards Robust Knowledge Graph Embedding via Multi-Task Reinforcement Learning}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{huang2019}\nXiao Huang, Jingyuan Zhang, Dingcheng Li, et al. (2019). \\textit{Knowledge Graph Embedding Based Question Answering}. Web Search and Data Mining.\n\n\\bibitem{tang2019}\nYun Tang, Jing Huang, Guangtao Wang, et al. (2019). \\textit{Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{sun2018}\nZhu Sun, Jie Yang, Jie Zhang, et al. (2018). \\textit{Recurrent knowledge graph embedding for effective recommendation}. ACM Conference on Recommender Systems.\n\n\\bibitem{ge2023}\nXiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \\textit{Knowledge Graph Embedding: An Overview}. APSIPA Transactions on Signal and Information Processing.\n\n\\bibitem{wang2020}\nRui Wang, Bicheng Li, Shengwei Hu, et al. (2020). \\textit{Knowledge Graph Embedding via Graph Attenuated Attention Networks}. IEEE Access.\n\n\\bibitem{li2022}\nRui Li, Jianan Zhao, Chaozhuo Li, et al. (2022). \\textit{HousE: Knowledge Graph Embedding with Householder Parameterization}. International Conference on Machine Learning.\n\n\\bibitem{zhang2019}\nQingheng Zhang, Zequn Sun, Wei Hu, et al. (2019). \\textit{Multi-view Knowledge Graph Embedding for Entity Alignment}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{tang2022}\nXiaojuan Tang, Song-Chun Zhu, Yitao Liang, et al. (2022). \\textit{RulE: Knowledge Graph Reasoning with Rule Embedding}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{lv2018}\nXin Lv, Lei Hou, Juan-Zi Li, et al. (2018). \\textit{Differentiating Concepts and Instances for Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{chen2025}\nJie Chen, Yinlong Wang, Shu Zhao, et al. (2025). \\textit{Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction}. ACM Trans. Asian Low Resour. Lang. Inf. Process..\n\n\\bibitem{qian2021}\nJing Qian, Gangmin Li, Katie Atkinson, et al. (2021). \\textit{Understanding Negative Sampling in Knowledge Graph Embedding}. International Journal of Artificial Intelligence & Applications.\n\n\\bibitem{dai2020}\nYuanfei Dai, Shiping Wang, N. Xiong, et al. (2020). \\textit{A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks}. Electronics.\n\n\\bibitem{ji2024}\nHao Ji, Li Yan, and Z. Ma (2024). \\textit{FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation}. IEEE transactions on fuzzy systems.\n\n\\bibitem{yan2022}\nQi Yan, Jiaxin Fan, Mohan Li, et al. (2022). \\textit{A Survey on Knowledge Graph Embedding}. International Conference on Data Science in Cyberspace.\n\n\\bibitem{zhang2023}\nYichi Zhang, Mingyang Chen, and Wen Zhang (2023). \\textit{Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.\n\n\\bibitem{li2021}\nRen Li, Yanan Cao, Qiannan Zhu, et al. (2021). \\textit{How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{yang2025}\nQingqing Yang, Min He, Zhongwen Li, et al. (2025). \\textit{A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction}. IEEE transactions on consumer electronics.\n\n\\bibitem{wang2019}\nQuan Wang, Pingping Huang, Haifeng Wang, et al. (2019). \\textit{CoKE: Contextualized Knowledge Graph Embedding}. arXiv.org.\n\n\\bibitem{di2023}\nShimin Di, and Lei Chen (2023). \\textit{Message Function Search for Knowledge Graph Embedding}. The Web Conference.\n\n\\bibitem{jia2017}\nYantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2017). \\textit{Knowledge Graph Embedding}. ACM Transactions on the Web.\n\n\\bibitem{choudhary2021}\nShivani Choudhary, Tarun Luthra, Ashima Mittal, et al. (2021). \\textit{A Survey of Knowledge Graph Embedding and Their Applications}. arXiv.org.\n\n\\bibitem{xiao2015}\nHan Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \\textit{From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{hu2024}\nLei Hu, Wenwen Li, Jun Xu, et al. (2024). \\textit{GeoEntity-type constrained knowledge graph embedding for predicting natural-language spatial relations}. International Journal of Geographical Information Science.\n\n\\bibitem{wang2014}\nZhen Wang, Jianwen Zhang, Jianlin Feng, et al. (2014). \\textit{Knowledge Graph Embedding by Translating on Hyperplanes}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{zhu2020}\nYushan Zhu, Wen Zhang, Mingyang Chen, et al. (2020). \\textit{DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning}. Web Search and Data Mining.\n\n\\bibitem{ali2020}\nMehdi Ali, M. Berrendorf, Charles Tapley Hoyt, et al. (2020). \\textit{Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework}. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n\\bibitem{mohamed2020}\nSameh K. Mohamed, A. Nounu, and V. Novek (2020). \\textit{Biological applications of knowledge graph embedding models}. Briefings Bioinform..\n\n\\bibitem{gao2020}\nChang Gao, Chengjie Sun, Lili Shan, et al. (2020). \\textit{Rotate3D: Representing Relations as Rotations in Three-Dimensional Space for Knowledge Graph Embedding}. International Conference on Information and Knowledge Management.\n\n\\bibitem{peng2021}\nXutan Peng, Guanyi Chen, Chenghua Lin, et al. (2021). \\textit{Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{shi2025}\nFobo Shi, Duantengchuan Li, Xiaoguang Wang, et al. (2025). \\textit{TGformer: A Graph Transformer Framework for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{zhang2024}\nXiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \\textit{Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph}. Applied intelligence (Boston).\n\n\\bibitem{rosso2020}\nPaolo Rosso, Dingqi Yang, and P. Cudr-Mauroux (2020). \\textit{Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction}. The Web Conference.\n\n\\bibitem{zhou2024}\nEnyuan Zhou, Song Guo, Zhixiu Ma, et al. (2024). \\textit{Poisoning Attack on Federated Knowledge Graph Embedding}. The Web Conference.\n\n\\bibitem{xie2020}\nZhiwen Xie, Guangyou Zhou, Jin Liu, et al. (2020). \\textit{ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{song2021}\nTengwei Song, Jie Luo, and Lei Huang (2021). \\textit{Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding}. Neural Information Processing Systems.\n\n\\bibitem{zhang2020}\nZhaoli Zhang, Zhifei Li, Hai Liu, et al. (2020). \\textit{Multi-Scale Dynamic Convolutional Network for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{ge2022}\nXiou Ge, Yun Cheng Wang, Bin Wang, et al. (2022). \\textit{CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations}. arXiv.org.\n\n\\bibitem{ren2020}\nFeiliang Ren, Jucheng Li, Huihui Zhang, et al. (2020). \\textit{Knowledge Graph Embedding with Atrous Convolution and Residual Learning}. International Conference on Computational Linguistics.\n\n\\bibitem{yuan2019}\nJun Yuan, Neng Gao, and Ji Xiang (2019). \\textit{TransGate: Knowledge Graph Embedding with Shared Gate Structure}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{xiao2015}\nHan Xiao, Minlie Huang, Yu Hao, et al. (2015). \\textit{TransA: An Adaptive Approach for Knowledge Graph Embedding}. arXiv.org.\n\n\\bibitem{sun2018}\nZhiqing Sun, Zhihong Deng, Jian-Yun Nie, et al. (2018). \\textit{RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space}. International Conference on Learning Representations.\n\n\\bibitem{ji2015}\nGuoliang Ji, Shizhu He, Liheng Xu, et al. (2015). \\textit{Knowledge Graph Embedding via Dynamic Mapping Matrix}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{lin2020}\nLifan Lin, and Kun She (2020). \\textit{Tensor Decomposition-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.\n\n\\bibitem{islam2023}\nM. Islam, Diego Amaya-Ramirez, B. Maigret, et al. (2023). \\textit{Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding}. Scientific Reports.\n\n\\bibitem{wang2021}\nHaoyu Wang, Yaqing Wang, Defu Lian, et al. (2021). \\textit{A Lightweight Knowledge Graph Embedding Framework for Efficient Inference and Storage}. International Conference on Information and Knowledge Management.\n\n\\bibitem{broscheit2020}\nSamuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek, et al. (2020). \\textit{LibKGE - A knowledge graph embedding library for reproducible research}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{fanourakis2022}\nN. Fanourakis, Vasilis Efthymiou, D. Kotzinos, et al. (2022). \\textit{Knowledge graph embedding methods for entity alignment: experimental review}. Data mining and knowledge discovery.\n\n\\bibitem{wang2018}\nPeifeng Wang, Jialong Han, Chenliang Li, et al. (2018). \\textit{Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{tabacof2019}\nPedro Tabacof, and Luca Costabello (2019). \\textit{Probability Calibration for Knowledge Graph Embedding Models}. International Conference on Learning Representations.\n\n\\bibitem{pei2019}\nShichao Pei, Lu Yu, R. Hoehndorf, et al. (2019). \\textit{Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference}. The Web Conference.\n\n\\bibitem{zhang2018}\nYongqi Zhang, Quanming Yao, Yingxia Shao, et al. (2018). \\textit{NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.\n\n\\bibitem{li2021}\nZelong Li, Jianchao Ji, Zuohui Fu, et al. (2021). \\textit{Efficient Non-Sampling Knowledge Graph Embedding}. The Web Conference.\n\n\\bibitem{li2022}\nGuangtong Li, L. Siddharth, and Jianxi Luo (2022). \\textit{Embedding knowledge graph of patent metadata to measure knowledge proximity}. J. Assoc. Inf. Sci. Technol..\n\n\\bibitem{ding2018}\nBoyang Ding, Quan Wang, Bin Wang, et al. (2018). \\textit{Improving Knowledge Graph Embedding Using Simple Constraints}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{zhang2022}\nXuanyu Zhang, Qing Yang, and Dongliang Xu (2022). \\textit{TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{sun2024}\nHongliang Sun, Jinlan Liu, Can Wang, et al. (2024). \\textit{Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning}. 2024 IEEE International Conference on Web Services (ICWS).\n\n\\bibitem{wang2024}\nJiapu Wang, Zheng Cui, Boyue Wang, et al. (2024). \\textit{IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion}. The Web Conference.\n\n\\bibitem{modak2024}\nS. Modak, Aakarsh Malhotra, Sarthak Malik, et al. (2024). \\textit{CPa-WAC: Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{xiao2016}\nHan Xiao, Minlie Huang, Lian Meng, et al. (2016). \\textit{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{zhang2023}\nZhao Zhang, Zhanpeng Guan, Fuwei Zhang, et al. (2023). \\textit{Weighted Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\n\\bibitem{guo2015}\nShu Guo, Quan Wang, Bin Wang, et al. (2015). \\textit{Semantically Smooth Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{xu2020}\nChengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2020). \\textit{TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation}. International Conference on Computational Linguistics.\n\n\\bibitem{zheng2024}\nChenguang Zheng, Guanxian Jiang, Xiao Yan, et al. (2024). \\textit{GE2: A General and Efficient Knowledge Graph Embedding Learning System}. Proc. ACM Manag. Data.\n\n\\bibitem{zhang2018}\nZhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2018). \\textit{Knowledge Graph Embedding with Hierarchical Relation Structure}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{zhu2024}\nBeibei Zhu, Ruolin Wang, Junyi Wang, et al. (2024). \\textit{A survey: knowledge graph entity alignment research based on graph embedding}. Artificial Intelligence Review.\n\n\\bibitem{liu2023}\nJia Liu, Wei Huang, Tianrui Li, et al. (2023). \\textit{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{choi2020}\nS. Choi, Hyun-Je Song, and Seong-Bae Park (2020). \\textit{An Approach to Knowledge Base Completion by a Committee-Based Knowledge Graph Embedding}. Applied Sciences.\n\n\\bibitem{ge2023}\nXiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \\textit{Knowledge Graph Embedding with 3D Compound Geometric Transformations}. APSIPA Transactions on Signal and Information Processing.\n\n\\bibitem{sadeghian2021}\nA. Sadeghian, Mohammadreza Armandpour, Anthony Colas, et al. (2021). \\textit{ChronoR: Rotation Based Temporal Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{liu2024}\nJiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \\textit{Fast and Continual Knowledge Graph Embedding via Incremental LoRA}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{li2022}\nYizhi Li, Wei Fan, Chaochun Liu, et al. (2022). \\textit{TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{rossi2020}\nAndrea Rossi, D. Firmani, Antonio Matinata, et al. (2020). \\textit{Knowledge Graph Embedding for Link Prediction}. ACM Transactions on Knowledge Discovery from Data.\n\n\\bibitem{li2023}\nJiang Li, Xiangdong Su, and Guanglai Gao (2023). \\textit{TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{peng2020}\nYanhui Peng, and Jing Zhang (2020). \\textit{LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction}. Industrial Conference on Data Mining.\n\n\\bibitem{ji2024}\nHao Ji, Li Yan, and Z. Ma (2024). \\textit{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding}. IEEE transactions on fuzzy systems.\n\n\\bibitem{zhang2024}\nQinggang Zhang, Junnan Dong, Qiaoyu Tan, et al. (2024). \\textit{Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{kochsiek2021}\nAdrian Kochsiek (2021). \\textit{Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques}. Proceedings of the VLDB Endowment.\n\n\\bibitem{yang2021}\nHan Yang, Leilei Zhang, Bingning Wang, et al. (2021). \\textit{Cycle or Minkowski: Which is More Appropriate for Knowledge Graph Embedding?}. International Conference on Information and Knowledge Management.\n\n\\bibitem{shang2024}\nBin Shang, Yinliang Zhao, Jun Liu, et al. (2024). \\textit{Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{asmara2023}\nS. M. Asmara, N. A. Sahabudin, Nor Syahidatul Nadiah Ismail, et al. (2023). \\textit{A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links}. International Conference on Software Engineering and Computer Systems.\n\n\\bibitem{gregucci2023}\nCosimo Gregucci, M. Nayyeri, D. Hern'andez, et al. (2023). \\textit{Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models}. The Web Conference.\n\n\\bibitem{pan2021}\nZhe Pan, and Peng Wang (2021). \\textit{Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{yoon2016}\nHee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, et al. (2016). \\textit{A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{li2024}\nRui Li, Chaozhuo Li, Yanming Shen, et al. (2024). \\textit{Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization}. International Conference on Machine Learning.\n\n\\bibitem{xiong2017zqu}\nChenyan Xiong, Russell Power, and Jamie Callan (2017). \\textit{Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding}. The Web Conference.\n\n\\bibitem{gong2020b2k}\nFan Gong, Meng Wang, Haofen Wang, et al. (2020). \\textit{SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation}. Big Data Research.\n\n\\bibitem{zhou2022ehi}\nBin Zhou, Xingwang Shen, Yuqian Lu, et al. (2022). \\textit{Semantic-aware event link reasoning over industrial knowledge graph embedding time series data}. International Journal of Production Research.\n\n\\bibitem{le2022ji8}\nThanh-Binh Le, N. Le, and H. Le (2022). \\textit{Knowledge graph embedding by relational rotation and complex convolution for link prediction}. Expert systems with applications.\n\n\\bibitem{zhou2022vgb}\nZhehui Zhou, Can Wang, Yan Feng, et al. (2022). \\textit{JointE: Jointly utilizing 1D and 2D convolution for knowledge graph embedding}. Knowledge-Based Systems.\n\n\\bibitem{xu2019t6b}\nDa Xu, Chuanwei Ruan, Evren Krpeoglu, et al. (2019). \\textit{Product Knowledge Graph Embedding for E-commerce}. Web Search and Data Mining.\n\n\\bibitem{mezni20218ml}\nHaithem Mezni, D. Benslimane, and Ladjel Bellatreche (2021). \\textit{Context-Aware Service Recommendation Based on Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{do2021mw0}\nP. Do, and Truong H. V. Phan (2021). \\textit{Developing a BERT based triple classification model using knowledge graph embedding for question answering system}. Applied intelligence (Boston).\n\n\\bibitem{mai2020ei3}\nGengchen Mai, K. Janowicz, Ling Cai, et al. (2020). \\textit{SEKGE: A locationaware Knowledge Graph Embedding model for Geographic Question Answering and Spatial Semantic Lifting}. Trans. GIS.\n\n\\bibitem{zhang2022eab}\nJiarui Zhang, Jian Huang, Jialong Gao, et al. (2022). \\textit{Knowledge graph embedding by logical-default attention graph convolution neural network for link prediction}. Information Sciences.\n\n\\bibitem{sosa2019ih0}\nDaniel N. Sosa, Alexander Derry, Margaret Guo, et al. (2019). \\textit{A Literature-Based Knowledge Graph Embedding Method for Identifying Drug Repurposing Opportunities in Rare Diseases}. bioRxiv.\n\n\\bibitem{guan2019pr4}\nNiannian Guan, Dandan Song, and L. Liao (2019). \\textit{Knowledge graph embedding with concepts}. Knowledge-Based Systems.\n\n\\bibitem{fan2014g7s}\nM. Fan, Qiang Zhou, E. Chang, et al. (2014). \\textit{Transition-based Knowledge Graph Embedding with Relational Mapping Properties}. Pacific Asia Conference on Language, Information and Computation.\n\n\\bibitem{zhang20190zu}\nHengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \\textit{Data Poisoning Attack against Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{chen2022mxn}\nQi Chen, Wei Wang, Kaizhu Huang, et al. (2022). \\textit{Zero-Shot Text Classification via Knowledge Graph Embedding for Social Media Data}. IEEE Internet of Things Journal.\n\n\\bibitem{wang2022hwx}\nXin Wang, Shengfei Lyu, Xiangyu Wang, et al. (2022). \\textit{Temporal knowledge graph embedding via sparse transfer matrix}. Information Sciences.\n\n\\bibitem{chen20226e4}\nMingyang Chen, Wen Zhang, Zonggang Yuan, et al. (2022). \\textit{Federated knowledge graph completion via embedding-contrastive learning}. Knowledge-Based Systems.\n\n\\bibitem{abusalih2020gdu}\nBilal Abu-Salih, Marwan Al-Tawil, Ibrahim Aljarah, et al. (2020). \\textit{Relational Learning Analysis of Social Politics using Knowledge Graph Embedding}. Data mining and knowledge discovery.\n\n\\bibitem{fang2022wp6}\nHaichuan Fang, Youwei Wang, Zhen Tian, et al. (2022). \\textit{Learning knowledge graph embedding with a dual-attention embedding network}. Expert systems with applications.\n\n\\bibitem{elebi2019bzc}\nR. elebi, Hseyin Uyar, Erkan Yasar, et al. (2019). \\textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction in realistic settings}. BMC Bioinformatics.\n\n\\bibitem{sha2019i3a}\nXiao Sha, Zhu Sun, and Jie Zhang (2019). \\textit{Hierarchical attentive knowledge graph embedding for personalized recommendation}. Electronic Commerce Research and Applications.\n\n\\bibitem{li2021ro5}\nZhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \\textit{Recalibration convolutional networks for learning interaction knowledge graph embedding}. Neurocomputing.\n\n\\bibitem{xiao20151fj}\nHan Xiao, Minlie Huang, Yu Hao, et al. (2015). \\textit{TransG : A Generative Mixture Model for Knowledge Graph Embedding}. arXiv.org.\n\n\\bibitem{zhang2021wg7}\nFei Zhang, Bo Sun, Xiaolin Diao, et al. (2021). \\textit{Prediction of adverse drug reactions based on knowledge graph embedding}. BMC Medical Informatics and Decision Making.\n\n\\bibitem{wang20186zs}\nGuanying Wang, Wen Zhang, Ruoxu Wang, et al. (2018). \\textit{Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{li2021x10}\nXinyu Li, P. Zheng, Jinsong Bao, et al. (2021). \\textit{Achieving cognitive mass personalization via the self-X cognitive manufacturing network: An industrial-knowledge-graph- and graph-embedding-enabled pathway}. Engineering.\n\n\\bibitem{wang202110w}\nXin Wang, Xiao Liu, Jin Liu, et al. (2021). \\textit{A novel knowledge graph embedding based API recommendation method for Mashup development}. World wide web (Bussum).\n\n\\bibitem{gutirrezbasulto2018oi0}\nVctor Gutirrez-Basulto, and S. Schockaert (2018). \\textit{From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules}. International Conference on Principles of Knowledge Representation and Reasoning.\n\n\\bibitem{portisch20221rd}\nJan Portisch, Nicolas Heist, and Heiko Paulheim (2022). \\textit{Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction - two sides of the same coin?}. Semantic Web.\n\n\\bibitem{zhang2022muu}\nFuwei Zhang, Zhao Zhang, Xiang Ao, et al. (2022). \\textit{Along the Time: Timeline-traced Embedding for Temporal Knowledge Graph Completion}. International Conference on Information and Knowledge Management.\n\n\\bibitem{feng2016dp7}\nJun Feng, Minlie Huang, Mingdong Wang, et al. (2016). \\textit{Knowledge Graph Embedding by Flexible Translation}. International Conference on Principles of Knowledge Representation and Reasoning.\n\n\\bibitem{liu2021wqa}\nJia Liu, Tianrui Li, Shenggong Ji, et al. (2021). \\textit{Urban Flow Pattern Mining Based on Multi-Source Heterogeneous Data Fusion and Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{sang2019gjl}\nShengtian Sang, Zhihao Yang, Xiaoxia Liu, et al. (2019). \\textit{GrEDeL: A Knowledge Graph Embedding Based Method for Drug Discovery From Biomedical Literatures}. IEEE Access.\n\n\\bibitem{wang2017yjq}\nM. Wang, Mengyue Liu, Jun Liu, et al. (2017). \\textit{Safe Medicine Recommendation via Medical Knowledge Graph Embedding}. arXiv.org.\n\n\\bibitem{jiang20219xl}\nDan Jiang, Ronggui Wang, Juan Yang, et al. (2021). \\textit{Kernel multi-attention neural network for knowledge graph embedding}. Knowledge-Based Systems.\n\n\\bibitem{liu2022fu5}\nYang Liu, Zequn Sun, Guang-pu Li, et al. (2022). \\textit{I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning}. International Conference on Information and Knowledge Management.\n\n\\bibitem{khan202236g}\nNasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \\textit{Similarity attributed knowledge graph embedding enhancement for item recommendation}. Information Sciences.\n\n\\bibitem{mezni2021ezn}\nHaithem Mezni (2021). \\textit{Temporal Knowledge Graph Embedding for Effective Service Recommendation}. IEEE Transactions on Services Computing.\n\n\\bibitem{zhang2021wix}\nQianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \\textit{Structural context-based knowledge graph embedding for link prediction}. Neurocomputing.\n\n\\bibitem{huang2021u42}\nXuqian Huang, Jiuyang Tang, Zhen Tan, et al. (2021). \\textit{Knowledge graph embedding by relational and entity rotation}. Knowledge-Based Systems.\n\n\\bibitem{pavlovic2022qte}\nAleksandar Pavlovic, and Emanuel Sallinger (2022). \\textit{ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion}. International Conference on Learning Representations.\n\n\\bibitem{wang20213kg}\nShensi Wang, Kun Fu, Xian Sun, et al. (2021). \\textit{Hierarchical-aware relation rotational knowledge graph embedding for link prediction}. Neurocomputing.\n\n\\bibitem{zhang2019rlm}\nShuai Zhang, Yi Tay, Lina Yao, et al. (2019). \\textit{Quaternion Knowledge Graph Embedding}. arXiv.org.\n\n\\bibitem{mai20195rp}\nGengchen Mai, Bo Yan, K. Janowicz, et al. (2019). \\textit{Relaxing Unanswerable Geographic Questions Using A Spatially Explicit Knowledge Graph Embedding Model}. Agile Conference.\n\n\\bibitem{han2018tzc}\nZhuobing Han, Xiaohong Li, Hongtao Liu, et al. (2018). \\textit{DeepWeak: Reasoning common software weaknesses via knowledge graph embedding}. IEEE International Conference on Software Analysis, Evolution, and Reengineering.\n\n\\bibitem{wang2022fvx}\nFeiyang Wang, Zhongbao Zhang, Li Sun, et al. (2022). \\textit{DiriE: Knowledge Graph Embedding with Dirichlet Distribution}. The Web Conference.\n\n\\bibitem{ferrari2022r82}\nIlaria Ferrari, Giacomo Frisoni, Paolo Italiani, et al. (2022). \\textit{Comprehensive Analysis of Knowledge Graph Embedding Techniques Benchmarked on Link Prediction}. Electronics.\n\n\\bibitem{fu2022df2}\nGuirong Fu, Zhao Meng, Zhen Han, et al. (2022). \\textit{TempCaps: A Capsule Network-based Embedding Model for Temporal Knowledge Graph Completion}. SPNLP.\n\n\\bibitem{wu2018c4b}\nYanrong Wu, and Zhichun Wang (2018). \\textit{Knowledge Graph Embedding with Numeric Attributes of Entities}. Rep4NLP@ACL.\n\n\\bibitem{zhang202121t}\nQianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \\textit{Knowledge graph embedding by reflection transformation}. Knowledge-Based Systems.\n\n\\bibitem{mohamed2019meq}\nSameh K. Mohamed, V. Novek, P. Vandenbussche, et al. (2019). \\textit{Loss Functions in Knowledge Graph Embedding Models}. DL4KG@ESWC.\n\n\\bibitem{xin2022dam}\nKexuan Xin, Zequn Sun, Wen Hua, et al. (2022). \\textit{Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning and Embedding}. International Conference on Information and Knowledge Management.\n\n\\bibitem{nie20195gc}\nBinling Nie, and Shouqian Sun (2019). \\textit{Knowledge graph embedding via reasoning over entities, relations, and text}. Future generations computer systems.\n\n\\bibitem{liu2018kvd}\nYang Liu, Qingguo Zeng, Huanrui Yang, et al. (2018). \\textit{Stock Price Movement Prediction from Financial News with Deep Learning and Knowledge Graph Embedding}. Pacific Rim Knowledge Acquisition Workshop.\n\n\\bibitem{ni2020ruj}\nChien-Chun Ni, Kin Sum Liu, and Nicolas Torzec (2020). \\textit{Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph}. The Web Conference.\n\n\\bibitem{li20215pu}\nChen Li, Xutan Peng, Yuhang Niu, et al. (2021). \\textit{Learning graph attention-aware knowledge graph embedding}. Neurocomputing.\n\n\\bibitem{yu2019qgs}\nS. Yu, Sujit Rokka Chhetri, A. Canedo, et al. (2019). \\textit{Pykg2vec: A Python Library for Knowledge Graph Embedding}. Journal of machine learning research.\n\n\\bibitem{fatemi2018e6v}\nBahare Fatemi, Siamak Ravanbakhsh, and D. Poole (2018). \\textit{Improved Knowledge Graph Embedding using Background Taxonomic Information}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{chen2021i5t}\nZhuo Chen, Mi-Yen Yeh, and Tei-Wei Kuo (2021). \\textit{PASSLEAF: A Pool-bAsed Semi-Supervised LEArning Framework for Uncertain Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{dong2022c6z}\nSicong Dong, Xupeng Miao, Peng Liu, et al. (2022). \\textit{HET-KG: Communication-Efficient Knowledge Graph Embedding Training via Hotness-Aware Cache}. IEEE International Conference on Data Engineering.\n\n\\bibitem{lu20206x1}\nFengyuan Lu, Peijin Cong, and Xinli Huang (2020). \\textit{Utilizing Textual Information in Knowledge Graph Embedding: A Survey of Methods and Applications}. IEEE Access.\n\n\\bibitem{li2022nr8}\nWeidong Li, Rong Peng, and Zhi Li (2022). \\textit{Improving knowledge graph completion via increasing embedding interactions}. Applied intelligence (Boston).\n\n\\bibitem{luo2015df2}\nYuanfei Luo, Quan Wang, Bin Wang, et al. (2015). \\textit{Context-Dependent Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{zhou20216m0}\nXiaohan Zhou, Yunhui Yi, and Geng Jia (2021). \\textit{Path-RotatE: Knowledge Graph Embedding by Relational Rotation of Path in Complex Space}. International Conference on Innovative Computing and Cloud Computing.\n\n\\bibitem{zhao202095o}\nFeng Zhao, Haoran Sun, Langjunqing Jin, et al. (2020). \\textit{Structure-augmented knowledge graph embedding for sparse data with rule learning}. Computer Communications.\n\n\\bibitem{jia201870f}\nYantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2018). \\textit{Path-specific knowledge graph embedding}. Knowledge-Based Systems.\n\n\\bibitem{mai2018u0h}\nGengchen Mai, K. Janowicz, and Bo Yan (2018). \\textit{Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines}. Semdeep/NLIWoD@ISWC.\n\n\\bibitem{li201949n}\nDingcheng Li, Siamak Zamani, Jingyuan Zhang, et al. (2019). \\textit{Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{tang2020ufr}\nXiaoli Tang, Rui Yuan, Qianyu Li, et al. (2020). \\textit{Timespan-Aware Dynamic Knowledge Graph Embedding by Incorporating Temporal Evolution}. IEEE Access.\n\n\\bibitem{guo2022qtv}\nLingbing Guo, Qiang Zhang, Zequn Sun, et al. (2022). \\textit{Understanding and Improving Knowledge Graph Embedding for Entity Alignment}. International Conference on Machine Learning.\n\n\\bibitem{jiang202235y}\nDan Jiang, Ronggui Wang, Lixia Xue, et al. (2022). \\textit{Multiview feature augmented neural network for knowledge graph embedding}. Knowledge-Based Systems.\n\n\\bibitem{liu201918i}\nYu Liu, Wen Hua, Kexuan Xin, et al. (2019). \\textit{Context-Aware Temporal Knowledge Graph Embedding}. WISE.\n\n\\bibitem{zhang2020s4x}\nQianjin Zhang, Ronggui Wang, Juan Yang, et al. (2020). \\textit{Knowledge graph embedding by translating in time domain space for link prediction}. Knowledge-Based Systems.\n\n\\bibitem{chang20179yf}\nLiang Chang, Manli Zhu, T. Gu, et al. (2017). \\textit{Knowledge Graph Embedding by Dynamic Translation}. IEEE Access.\n\n\\bibitem{lee2022hr9}\nYeon-Chang Lee, and Sang-Wook Kim (2022). \\textit{THOR: Self-Supervised Temporal Knowledge Graph Embedding via Three-Tower Graph Convolutional Networks}. Industrial Conference on Data Mining.\n\n\\bibitem{zhang2022fpm}\nYongqi Zhang, Zhanke Zhou, Quanming Yao, et al. (2022). \\textit{Efficient Hyper-parameter Search for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{liu2019e1u}\nChang Liu, Lun Li, Xiaolu Yao, et al. (2019). \\textit{A Survey of Recommendation Algorithms Based on Knowledge Graph Embedding}. 2019 IEEE International Conference on Computer Science and Educational Informatization (CSEI).\n\n\\bibitem{song2021fnl}\nWei Song, Jingjin Guo, Ruiji Fu, et al. (2021). \\textit{A Knowledge Graph Embedding Approach for Metaphor Processing}. IEEE/ACM Transactions on Audio Speech and Language Processing.\n\n\\bibitem{gradgyenge2017xdy}\nLszl Grad-Gyenge, A. Kiss, and P. Filzmoser (2017). \\textit{Graph Embedding Based Recommendation Techniques on the Knowledge Graph}. User Modeling, Adaptation, and Personalization.\n\n\\bibitem{zhou20218bt}\nXiaofei Zhou, Lingfeng Niu, Qiannan Zhu, et al. (2021). \\textit{Knowledge Graph Embedding by Double Limit Scoring Loss}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{chen20210ah}\nYao Chen, Jiangang Liu, Zhe Zhang, et al. (2021). \\textit{MbiusE: Knowledge Graph Embedding on Mbius Ring}. Knowledge-Based Systems.\n\n\\bibitem{zhang2020i7j}\nYongqi Zhang, Quanming Yao, and Lei Chen (2020). \\textit{Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding}. Neural Information Processing Systems.\n\n\\bibitem{boschin2020ki4}\nArmand Boschin (2020). \\textit{TorchKGE: Knowledge Graph Embedding in Python and PyTorch}. arXiv.org.\n\n\\bibitem{wang20199fe}\nP. Wang, D. Dou, Fangzhao Wu, et al. (2019). \\textit{Logic Rules Powered Knowledge Graph Embedding}. arXiv.org.\n\n\\bibitem{myklebust201941l}\nE. B. Myklebust, Ernesto Jimnez-Ruiz, Jiaoyan Chen, et al. (2019). \\textit{Knowledge Graph Embedding for Ecotoxicological Effect Prediction}. International Workshop on the Semantic Web.\n\n\\bibitem{kartheek2021aj7}\nMiriyala Kartheek, and G. Sajeev (2021). \\textit{Building Semantic Based Recommender System Using Knowledge Graph Embedding}. International Conference on Intelligent Information Processing.\n\n\\bibitem{sha2019plw}\nXiao Sha, Zhu Sun, and Jie Zhang (2019). \\textit{Attentive Knowledge Graph Embedding for Personalized Recommendation}. arXiv.org.\n\n\\bibitem{lu2020x6y}\nHaonan Lu, and Hailin Hu (2020). \\textit{DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding}. arXiv.org.\n\n\\bibitem{zhang2020c15}\nSiheng Zhang, Zhengya Sun, and Wensheng Zhang (2020). \\textit{Improve the translational distance models for knowledge graph embedding}. Journal of Intelligence and Information Systems.\n\n\\bibitem{li2020ek4}\nMingda Li, Zhengya Sun, Siheng Zhang, et al. (2020). \\textit{Enhancing Knowledge Graph Embedding with Relational Constraints}. 2020 IEEE International Conference on Knowledge Graph (ICKG).\n\n\\bibitem{li2020he5}\nJian Li, Zhuoming Xu, Yan Tang, et al. (2020). \\textit{Deep Hybrid Knowledge Graph Embedding for Top-N Recommendation}. Web Information System and Application Conference.\n\n\\bibitem{kim2020zu3}\nKuekyeng Kim, Yuna Hur, Gyeongmin Kim, et al. (2020). \\textit{GREG: A Global Level Relation Extraction with Knowledge Graph Embedding}. Applied Sciences.\n\n\\bibitem{zhu2018l0u}\nJizhao Zhu, Yantao Jia, Jun Xu, et al. (2018). \\textit{Modeling the Correlations of Relations for Knowledge Graph Embedding}. Journal of Computational Science and Technology.\n\n\\bibitem{do20184o2}\nKien Do, T. Tran, and S. Venkatesh (2018). \\textit{Knowledge Graph Embedding with Multiple Relation Projections}. International Conference on Pattern Recognition.\n\n\\bibitem{ma20194ua}\nYunpu Ma, Volker Tresp, Liming Zhao, et al. (2019). \\textit{Variational Quantum Circuit Model for Knowledge Graph Embedding}. Advanced Quantum Technologies.\n\n\\bibitem{zhang2020wou}\nYuhang Zhang, Jun Wang, and Jie Luo (2020). \\textit{Knowledge Graph Embedding Based Collaborative Filtering}. IEEE Access.\n\n\\bibitem{zhang2019hs5}\nWen Zhang, Shumin Deng, Han Wang, et al. (2019). \\textit{XTransE: Explainable Knowledge Graph Embedding for Link Prediction with Lifestyles in e-Commerce}. Joint International Conference of Semantic Technology.\n\n\\bibitem{wang20198d2}\nZhihao Wang, and Xin Li (2019). \\textit{Hybrid-TE: Hybrid Translation-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.\n\n\\bibitem{tran20195x3}\nHung Nghiep Tran, and A. Takasu (2019). \\textit{Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective}. EDBT/ICDT Workshops.\n\n\\bibitem{xiong2018fof}\nShengwu Xiong, Weitao Huang, and P. Duan (2018). \\textit{Knowledge Graph Embedding via Relation Paths and Dynamic Mapping Matrix}. ER Workshops.\n\n\\bibitem{radstok2021yup}\nWessel Radstok, M. Chekol, and M. Schfer (2021). \\textit{Are Knowledge Graph Embedding Models Biased, or Is it the Data That They Are Trained on?}. Wikidata@ISWC.\n\n\\bibitem{zhao2020o6z}\nLing Zhao, Hanhan Deng, L. Qiu, et al. (2020). \\textit{Urban Multi-Source Spatio-Temporal Data Analysis Aware Knowledge Graph Embedding}. Symmetry.\n\n\\bibitem{zhang20182ey}\nMaoyuan Zhang, Qi Wang, Wukui Xu, et al. (2018). \\textit{Discriminative Path-Based Knowledge Graph Embedding for Precise Link Prediction}. European Conference on Information Retrieval.\n\n\\bibitem{jia20207dd}\nNingning Jia, Xiang Cheng, and Sen Su (2020). \\textit{Improving Knowledge Graph Embedding Using Locally and Globally Attentive Relation Paths}. European Conference on Information Retrieval.\n\n\\bibitem{zhu2019ir6}\nQiannan Zhu, Xiaofei Zhou, P. Zhang, et al. (2019). \\textit{A neural translating general hyperplane for knowledge graph embedding}. Journal of Computer Science.\n\n\\bibitem{wang2021dgy}\nShen Wang, Xiaokai Wei, C. D. Santos, et al. (2021). \\textit{Knowledge Graph Representation via Hierarchical Hyperbolic Neural Graph Embedding}. 2021 IEEE International Conference on Big Data (Big Data).\n\n\\bibitem{ning20219et}\nZhiyuan Ning, Ziyue Qiao, Hao Dong, et al. (2021). \\textit{LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding}. Pacific-Asia Conference on Knowledge Discovery and Data Mining.\n\n\\bibitem{sheikh20213qq}\nNasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2021). \\textit{Knowledge Graph Embedding using Graph Convolutional Networks with Relation-Aware Attention}. arXiv.org.\n\n\\bibitem{rim2021s9a}\nWiem Ben Rim, Carolin (Haas) Lawrence, Kiril Gashteovski, et al. (2021). \\textit{Behavioral Testing of Knowledge Graph Embedding Models for Link Prediction}. Conference on Automated Knowledge Base Construction.\n\n\\bibitem{zhang20179i2}\nChunhong Zhang, Miao Zhou, Xiao Han, et al. (2017). \\textit{Knowledge Graph Embedding for Hyper-Relational Data}. Unpublished manuscript.\n\n\\bibitem{elebi20182bd}\nR. elebi, Erkan Yasar, Hseyin Uyar, et al. (2018). \\textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction using Linked Open Data}. Workshop on Semantic Web Applications and Tools for Life Sciences.\n\n\\bibitem{garofalo20185g9}\nMartina Garofalo, Maria Angela Pellegrino, Abdulrahman Altabba, et al. (2018). \\textit{Leveraging Knowledge Graph Embedding Techniques for Industry 4.0 Use Cases}. arXiv.org.\n\n\\bibitem{wang201825m}\nKai Wang, Yu Liu, Xiujuan Xu, et al. (2018). \\textit{Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network}. arXiv.org.\n\n\\bibitem{chung2021u2l}\nChanyoung Chung, and Joyce Jiyoung Whang (2021). \\textit{Knowledge Graph Embedding via Metagraph Learning}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\n\\bibitem{tran2019j42}\nHung Nghiep Tran, and A. Takasu (2019). \\textit{Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space}. International Conference on Theory and Practice of Digital Libraries.\n\n\\bibitem{shi2017m2h}\nJun Shi, Huan Gao, G. Qi, et al. (2017). \\textit{Knowledge Graph Embedding with Triple Context}. International Conference on Information and Knowledge Management.\n\n\\bibitem{zhang2017ixt}\nWen Zhang (2017). \\textit{Knowledge Graph Embedding with Diversity of Structures}. The Web Conference.\n\n\\bibitem{zhu20196p1}\nMing-Yi Zhu, De-sheng Zhen, Ran Tao, et al. (2019). \\textit{Top-N Collaborative Filtering Recommendation Algorithm Based on Knowledge Graph Embedding}. International Conference on Knowledge Management in Organizations.\n\n\\bibitem{kertkeidkachorn2019dkn}\nNatthawut Kertkeidkachorn, Xin Liu, and R. Ichise (2019). \\textit{GTransE: Generalizing Translation-Based Model on Uncertain Knowledge Graph Embedding}. JSAI.\n\n\\bibitem{zhu2019zqy}\nJia Zhu, Zetao Zheng, Min Yang, et al. (2019). \\textit{A semi-supervised model for knowledge graph embedding}. Data mining and knowledge discovery.\n\n\\bibitem{zhang20193g2}\nHengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \\textit{Towards Data Poisoning Attack against Knowledge Graph Embedding}. arXiv.org.\n\n\\bibitem{liu2019fcs}\nWenqiang Liu, Hongyun Cai, Xu Cheng, et al. (2019). \\textit{Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding}. Knowledge-Based Systems.\n\n\\bibitem{kanojia20171in}\nVibhor Kanojia, Hideyuki Maeda, Riku Togashi, et al. (2017). \\textit{Enhancing Knowledge Graph Embedding with Probabilistic Negative Sampling}. The Web Conference.\n\n\\bibitem{gao2018di0}\nHuan Gao, Jun Shi, G. Qi, et al. (2018). \\textit{Triple Context-Based Knowledge Graph Embedding}. IEEE Access.\n\n\\bibitem{mai2018egi}\nGengchen Mai, K. Janowicz, and Bo Yan (2018). \\textit{Support and Centrality: Learning Weights for Knowledge Graph Embedding Models}. International Conference Knowledge Engineering and Knowledge Management.\n\n\\bibitem{xiao2016bb9}\nHan Xiao, Minlie Huang, and Xiaoyan Zhu (2016). \\textit{Knowledge Semantic Representation: A Generative Model for Interpretable Knowledge Graph Embedding}. arXiv.org.\n\n\\bibitem{liu2024q3q}\nPeifeng Liu, Lu Qian, Xingwei Zhao, et al. (2024). \\textit{Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly}. IEEE Transactions on Industrial Informatics.\n\n\\bibitem{zhang2024cjl}\nJin-cheng Zhang, A. Zain, Kai Zhou, et al. (2024). \\textit{A review of recommender systems based on knowledge graph embedding}. Expert systems with applications.\n\n\\bibitem{su2023v6e}\nXiao-Rui Su, Zhuhong You, Deshuang Huang, et al. (2023). \\textit{Biomedical Knowledge Graph Embedding With Capsule Network for Multi-Label Drug-Drug Interaction Prediction}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{zhu2023bfj}\nXiangrong Zhu, Guang-pu Li, and Wei Hu (2023). \\textit{Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning}. The Web Conference.\n\n\\bibitem{liu2024to0}\nJiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \\textit{Towards Continual Knowledge Graph Embedding via Incremental Distillation}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{wang2024vgj}\nWei Wang, Xiaoxuan Shen, Baolin Yi, et al. (2024). \\textit{Knowledge-aware fine-grained attention networks with refined knowledge graph embedding for personalized recommendation}. Expert systems with applications.\n\n\\bibitem{li2024920}\nDuantengchuan Li, Tao Xia, Jing Wang, et al. (2024). \\textit{SDFormer: A shallow-to-deep feature interaction for knowledge graph embedding}. Knowledge-Based Systems.\n\n\\bibitem{lee202380l}\nJaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang (2023). \\textit{InGram: Inductive Knowledge Graph Embedding via Relation Graphs}. International Conference on Machine Learning.\n\n\\bibitem{shokrzadeh2023twj}\nZeinab Shokrzadeh, M. Feizi-Derakhshi, M. Balafar, et al. (2023). \\textit{Knowledge graph-based recommendation system enhanced by neural collaborative filtering and knowledge graph embedding}. Ain Shams Engineering Journal.\n\n\\bibitem{gao2023086}\nWeibo Gao, Hao Wang, Qi Liu, et al. (2023). \\textit{Leveraging Transferable Knowledge Concept Graph Embedding for Cold-Start Cognitive Diagnosis}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\n\\bibitem{li2024sgp}\nYufeng Li, Wenchao Zhao, Bo Dang, et al. (2024). \\textit{Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning}. 2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE).\n\n\\bibitem{xue2023qi7}\nZengcan Xue, Zhao Zhang, Hai Liu, et al. (2023). \\textit{Learning knowledge graph embedding with multi-granularity relational augmentation network}. Expert systems with applications.\n\n\\bibitem{duan2024d3f}\nPengbo Duan, Kuo Yang, Xin Su, et al. (2024). \\textit{HTINet2: herbtarget prediction via knowledge graph embedding and residual-like graph neural network}. Briefings Bioinform..\n\n\\bibitem{chen20246rm}\nZhen Chen, Dalin Zhang, Shanshan Feng, et al. (2024). \\textit{KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{zhu2022o32}\nJia Zhu, Changqin Huang, and P. D. Meo (2022). \\textit{DFMKE: A dual fusion multi-modal knowledge graph embedding framework for entity alignment}. Information Fusion.\n\n\\bibitem{mitropoulou20235t0}\nKaterina Mitropoulou, Panagiotis C. Kokkinos, P. Soumplis, et al. (2023). \\textit{Anomaly Detection in Cloud Computing using Knowledge Graph Embedding and Machine Learning Mechanisms}. Journal of Grid Computing.\n\n\\bibitem{shomer2023imo}\nHarry Shomer, Wei Jin, Wentao Wang, et al. (2023). \\textit{Toward Degree Bias in Embedding-Based Knowledge Graph Completion}. The Web Conference.\n\n\\bibitem{wang202490m}\nMingjie Wang, Zijie Li, Jun Wang, et al. (2024). \\textit{TracKGE: Transformer with Relation-pattern Adaptive Contrastive Learning for Knowledge Graph Embedding}. Knowledge-Based Systems.\n\n\\bibitem{li2024bl5}\nZhifei Li, Wei Huang, Xuchao Gong, et al. (2024). \\textit{Decoupled semantic graph neural network for knowledge graph embedding}. Neurocomputing.\n\n\\bibitem{li2024y2a}\nMingqi Li, Wenming Ma, and Zihao Chu (2024). \\textit{KGIE: Knowledge graph convolutional network for recommender system with interactive embedding}. Knowledge-Based Systems.\n\n\\bibitem{jia2023krv}\nYan Jia, Mengqi Lin, Yechen Wang, et al. (2023). \\textit{Extrapolation over temporal knowledge graph via hyperbolic embedding}. CAAI Transactions on Intelligence Technology.\n\n\\bibitem{huang2023grx}\nWei Huang, Jia Liu, Tianrui Li, et al. (2023). \\textit{FedCKE: Cross-Domain Knowledge Graph Embedding in Federated Learning}. IEEE Transactions on Big Data.\n\n\\bibitem{wang2023s70}\nRuoxin Wang, and C. F. Cheung (2023). \\textit{Knowledge graph embedding learning system for defect diagnosis in additive manufacturing}. Computers in industry (Print).\n\n\\bibitem{hou20237gt}\nXiangning Hou, Ruizhe Ma, Li Yan, et al. (2023). \\textit{T-GAE: A Timespan-aware Graph Attention-based Embedding Model for Temporal Knowledge Graph Completion}. Information Sciences.\n\n\\bibitem{jiang2023opm}\nDan Jiang, Ronggui Wang, Lixia Xue, et al. (2023). \\textit{Multisource hierarchical neural network for knowledge graph embedding}. Expert systems with applications.\n\n\\bibitem{lu2022bwo}\nH. Lu, Hailin Hu, and Xiaodong Lin (2022). \\textit{DensE: An enhanced non-commutative representation for knowledge graph embedding with adaptive semantic hierarchy}. Neurocomputing.\n\n\\bibitem{djeddi2023g71}\nW. Djeddi, Khalil Hermi, S. Yahia, et al. (2023). \\textit{Advancing drugtarget interaction prediction: a comprehensive graph-based approach integrating knowledge graph embedding and ProtBert pretraining}. BMC Bioinformatics.\n\n\\bibitem{zhang20243iw}\nYuchao Zhang, Xiangjie Kong, Zhehui Shen, et al. (2024). \\textit{A survey on temporal knowledge graph embedding: Models and applications}. Knowledge-Based Systems.\n\n\\bibitem{le2023hjy}\nThanh-Binh Le, Huy Tran, and H. Le (2023). \\textit{Knowledge graph embedding with the special orthogonal group in quaternion space for link prediction}. Knowledge-Based Systems.\n\n\\bibitem{yao2023y12}\nZhen Yao, Wen Zhang, Mingyang Chen, et al. (2023). \\textit{Analogical Inference Enhanced Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{li2023y5q}\nZhipeng Li, Shanshan Feng, Jun Shi, et al. (2023). \\textit{Future Event Prediction Based on Temporal Knowledge Graph Embedding}. Computer systems science and engineering.\n\n\\bibitem{yang2022j7z}\nShihan Yang, Weiya Zhang, R. Tang, et al. (2022). \\textit{Approximate inferring with confidence predicting based on uncertain knowledge graph embedding}. Information Sciences.\n\n\\bibitem{banerjee2023fdi}\nDebayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, et al. (2023). \\textit{GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering}. Extended Semantic Web Conference.\n\n\\bibitem{hu20230kr}\nYuke Hu, Wei Liang, Ruofan Wu, et al. (2023). \\textit{Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding}. The Web Conference.\n\n\\bibitem{li2023wgg}\nDaiyi Li, Li Yan, Xiaowen Zhang, et al. (2023). \\textit{EventKGE: Event knowledge graph embedding with event causal transfer}. Knowledge-Based Systems.\n\n\\bibitem{hao2022cl4}\nXinkun Hao, Qingfeng Chen, Haiming Pan, et al. (2022). \\textit{Enhancing drugdrug interaction prediction by three-way decision and knowledge graph embedding}. Granular Computing.\n\n\\bibitem{khan20222j1}\nNasrullah Khan, Z. Ma, Li Yan, et al. (2022). \\textit{Hashing-based semantic relevance attributed knowledge graph embedding enhancement for deep probabilistic recommendation}. Applied intelligence (Boston).\n\n\\bibitem{le2022ybl}\nThanh-Binh Le, Ngoc Huynh, and Bac Le (2022). \\textit{Knowledge graph embedding by projection and rotation on hyperplanes for link prediction}. Applied intelligence (Boston).\n\n\\bibitem{liang202338l}\nShuang Liang (2023). \\textit{Knowledge Graph Embedding Based on Graph Neural Network}. IEEE International Conference on Data Engineering.\n\n\\bibitem{khan2022ipv}\nNasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \\textit{DCA-IoMT: Knowledge-Graph-Embedding-Enhanced Deep Collaborative Alert Recommendation Against COVID-19}. IEEE Transactions on Industrial Informatics.\n\n\\bibitem{he2022e37}\nPeng He, Gang Zhou, Mengli Zhang, et al. (2022). \\textit{Improving temporal knowledge graph embedding using tensor factorization}. Applied intelligence (Boston).\n\n\\bibitem{shen2022d5j}\nLinshan Shen, Rongbo He, and Shaobin Huang (2022). \\textit{Entity alignment with adaptive margin learning knowledge graph embedding}. Data & Knowledge Engineering.\n\n\\bibitem{di20210ib}\nShimin Di, Quanming Yao, Yongqi Zhang, et al. (2021). \\textit{Efficient Relation-aware Scoring Function Search for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.\n\n\\bibitem{niu2020uyy}\nGuanglin Niu, Bo Li, Yongfei Zhang, et al. (2020). \\textit{AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding}. Findings.\n\n\\bibitem{nie2023ejz}\nH. Nie, Xiangguo Zhao, Xin Bi, et al. (2023). \\textit{Correlation embedding learning with dynamic semantic enhanced sampling for knowledge graph completion}. World wide web (Bussum).\n\n\\bibitem{li2022du0}\nJiayi Li, and Yujiu Yang (2022). \\textit{STaR: Knowledge Graph Embedding by Scaling, Translation and Rotation}. Autonomous Infrastructure, Management and Security.\n\n\\bibitem{daruna2022dmk}\nA. Daruna, Devleena Das, and S. Chernova (2022). \\textit{Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions}. IEEE/RJS International Conference on Intelligent RObots and Systems.\n\n\\bibitem{zhou20210ma}\nXing-Chun Zhou, Peng Wang, Qi Luo, et al. (2021). \\textit{Multi-hop Knowledge Graph Reasoning Based on Hyperbolic Knowledge Graph Embedding and Reinforcement Learning}. IJCKG.\n\n\\bibitem{kun202384f}\nKong Wei Kun, Xin Liu, Teeradaj Racharak, et al. (2023). \\textit{WeExt: A Framework of Extending Deterministic Knowledge Graph Embedding Models for Embedding Weighted Knowledge Graphs}. IEEE Access.\n\n\\bibitem{dong2022taz}\nYao Dong, Lei Wang, Ji Xiang, et al. (2022). \\textit{RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space}. International Conference on Computational Linguistics.\n\n\\bibitem{kamigaito20218jz}\nHidetaka Kamigaito, and Katsuhiko Hayashi (2021). \\textit{Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{krause2022th0}\nFranziska Krause (2022). \\textit{Dynamic Knowledge Graph Embeddings via Local Embedding Reconstructions}. Extended Semantic Web Conference.\n\n\\bibitem{zhang20213h6}\nZhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2021). \\textit{Knowledge graph embedding with shared latent semantic units}. Neural Networks.\n\n\\bibitem{li2021tm6}\nGuang-pu Li, Zequn Sun, Lei Qian, et al. (2021). \\textit{Rule-based data augmentation for knowledge graph embedding}. AI Open.\n\n\\bibitem{wang2020au0}\nKai Wang, Yu Liu, Xiujuan Xu, et al. (2020). \\textit{Enhancing knowledge graph embedding by composite neighbors for link prediction}. Computing.\n\n\\bibitem{wei20215a7}\nYuyang Wei, Wei Chen, Zhixu Li, et al. (2021). \\textit{Incremental Update of Knowledge Graph Embedding by Rotating on Hyperplanes}. 2021 IEEE International Conference on Web Services (ICWS).\n\n\\bibitem{zhang2021rjh}\nYongqi Zhang, Quanming Yao, and Lei Chen (2021). \\textit{Simple and automated negative sampling for knowledge graph embedding}. The VLDB journal.\n\n\\bibitem{sheikh202245c}\nNasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2022). \\textit{Scaling knowledge graph embedding models for link prediction}. EuroMLSys@EuroSys.\n\n\\bibitem{ren2021muc}\nChao Ren, Le Zhang, Lintao Fang, et al. (2021). \\textit{Ontological Concept Structure Aware Knowledge Transfer for Inductive Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.\n\n\\bibitem{eyharabide2021wx4}\nVictoria Eyharabide, I. E. I. Bekkouch, and Nicolae Drago Constantin (2021). \\textit{Knowledge Graph Embedding-Based Domain Adaptation for Musical Instrument Recognition}. De Computis.\n\n\\bibitem{hong2020hyg}\nY. Hong, Chenyang Bu, and Tingting Jiang (2020). \\textit{Rule-enhanced Noisy Knowledge Graph Embedding via Low-quality Error Detection}. 2020 IEEE International Conference on Knowledge Graph (ICKG).\n\n\\bibitem{huang2020sqc}\nYan Huang, Haili Sun, Xu Ke, et al. (2020). \\textit{CoRelatE: Learning the correlation in multi-fold relations for knowledge graph embedding}. Knowledge-Based Systems.\n\n\\bibitem{kurokawa2021f4f}\nM. Kurokawa (2021). \\textit{Explainable Knowledge Reasoning Framework Using Multiple Knowledge Graph Embedding}. IJCKG.\n\n\\bibitem{mohamed2021dwg}\nSameh K. Mohamed, Emir Muoz, and V. Novek (2021). \\textit{On Training Knowledge Graph Embedding Models}. Inf..\n\n\\bibitem{gebhart2021gtp}\nThomas Gebhart, J. Hansen, and Paul Schrater (2021). \\textit{Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding}. International Conference on Artificial Intelligence and Statistics.\n\n\\bibitem{deng2024643}\nWeibin Deng, Yiteng Zhang, Hong Yu, et al. (2024). \\textit{Knowledge graph embedding based on dynamic adaptive atrous convolution and attention mechanism for link prediction}. Information Processing & Management.\n\n\\bibitem{liu2024zr9}\nJin Liu, Hao Du, R. Guo, et al. (2024). \\textit{MMGK: Multimodality Multiview Graph Representations and Knowledge Embedding for Mild Cognitive Impairment Diagnosis}. IEEE Transactions on Computational Social Systems.\n\n\\bibitem{zhang2024zmq}\nChengcheng Zhang, Tianyi Zang, and Tianyi Zhao (2024). \\textit{KGE-UNIT: toward the unification of molecular interactions prediction based on knowledge graph and multi-task learning on drug discovery}. Briefings Bioinform..\n\n\\bibitem{he2024vks}\nMingsheng He, Lin Zhu, and Luyi Bai (2024). \\textit{ConvTKG: A query-aware convolutional neural network-based embedding model for temporal knowledge graph completion}. Neurocomputing.\n\n\\bibitem{zhang2024fy0}\nDong Zhang, Zhe Rong, Chengyuan Xue, et al. (2024). \\textit{SimRE: Simple contrastive learning with soft logical rule for knowledge graph embedding}. Information Sciences.\n\n\\bibitem{zhang2024ivc}\nDong Zhang, Wenlong Feng, Zonghang Wu, et al. (2024). \\textit{CDRGN-SDE: Cross-Dimensional Recurrent Graph Network with neural Stochastic Differential Equation for temporal knowledge graph embedding}. Expert systems with applications.\n\n\\bibitem{jing2024nxw}\nYanzhen Jing, Guanghui Zhou, Chao Zhang, et al. (2024). \\textit{XMKR: Explainable manufacturing knowledge recommendation for collaborative design with graph embedding learning}. Advanced Engineering Informatics.\n\n\\bibitem{jiang2024zlc}\nPengcheng Jiang, Lang Cao, Cao Xiao, et al. (2024). \\textit{KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge}. Neural Information Processing Systems.\n\n\\bibitem{han2024u0t}\nZhulin Han, and Jian Wang (2024). \\textit{Knowledge enhanced graph inference network based entity-relation extraction and knowledge graph construction for industrial domain}. Frontiers of Engineering Management.\n\n\\bibitem{quan2024o2a}\nHuafeng Quan, Yiting Li, Dashuai Liu, et al. (2024). \\textit{Protection of Guizhou Miao batik culture based on knowledge graph and deep learning}. Heritage Science.\n\n\\bibitem{liu2024tc2}\nBufan Liu, Chun-Hsien Chen, and Zuoxu Wang (2024). \\textit{A multi-hierarchical aggregation-based graph convolutional network for industrial knowledge graph embedding towards cognitive intelligent manufacturing}. Journal of manufacturing systems.\n\n\\bibitem{hello2024hgf}\nNour Hello, P. Lorenzo, and E. Strinati (2024). \\textit{Semantic Communication Enhanced by Knowledge Graph Representation Learning}. International Workshop on Signal Processing Advances in Wireless Communications.\n\n\\bibitem{li2024z0e}\nJinpeng Li, Hang Yu, Xiangfeng Luo, et al. (2024). \\textit{COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{yan2024joa}\nQun Yan, Juan Zhao, Linfu Xue, et al. (2024). \\textit{Mineral Prospectivity Mapping Based on Spatial Feature Classification with Geological Map Knowledge Graph Embedding: Case Study of Gold Ore Prediction at Wulonggou, Qinghai Province (Western China)}. Natural Resources Research.\n\n\\bibitem{liu2024tn0}\nJhih-Chen Liu, Chiao-Ting Chen, Chi Lee, et al. (2024). \\textit{Evolving Knowledge Graph Representation Learning with Multiple Attention Strategies for Citation Recommendation System}. ACM Transactions on Intelligent Systems and Technology.\n\n\\bibitem{wang20245dw}\nChuanghui Wang, Yunqing Yang, Jinshuai Song, et al. (2024). \\textit{Research Progresses and Applications of Knowledge Graph Embedding Technique in Chemistry}. Journal of Chemical Information and Modeling.\n\n\\bibitem{long2024soi}\nXiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \\textit{KGDM: A Diffusion Model to Capture Multiple Relation Semantics for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{zhou2024ayq}\nQihui Zhou, Peiqi Yin, Xiao Yan, et al. (2024). \\textit{Atom: An Efficient Query Serving System for Embedding-based Knowledge Graph Reasoning with Operator-level Batching}. Proc. ACM Manag. Data.\n\n\\bibitem{huang2024t19}\nChen Huang, Deshan Chen, Tengze Fan, et al. (2024). \\textit{Incorporating environmental knowledge embedding and spatial-temporal graph attention networks for inland vessel traffic flow prediction}. Engineering applications of artificial intelligence.\n\n\\bibitem{lu2024fsd}\nMing Lu, Yancong Li, Jiangxiao Zhang, et al. (2024). \\textit{Deep hyperbolic convolutional model for knowledge graph embedding}. Knowledge-Based Systems.\n\n\\bibitem{liu2024yar}\nQi Liu, Qinghua Zhang, Fan Zhao, et al. (2024). \\textit{Uncertain knowledge graph embedding: an effective method combining multi-relation and multi-path}. Frontiers Comput. Sci..\n\n\\bibitem{khan20242y2}\nNasrullah Khan, Zongmin Ma, Ruizhe Ma, et al. (2024). \\textit{Continual knowledge graph embedding enhancement for joint interaction-based next click recommendation}. Knowledge-Based Systems.\n\n\\bibitem{xue2025ee8}\nZengcan Xue, Zhaoli Zhang, Hai Liu, et al. (2025). \\textit{MHRN: A multi-perspective hierarchical relation network for knowledge graph embedding}. Knowledge-Based Systems.\n\n\\bibitem{long20248vt}\nXiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \\textit{Fact Embedding through Diffusion Model for Knowledge Graph Completion}. The Web Conference.\n\n\\bibitem{huang20240su}\nChen Huang, Fei Yu, Zhiguo Wan, et al. (2024). \\textit{Knowledge graph confidence-aware embedding for recommendation}. Neural Networks.\n\n\\bibitem{wang2024nej}\nYuzhuo Wang, Hongzhi Wang, Xianglong Liu, et al. (2024). \\textit{GFedKG: GNN-based federated embedding model for knowledge graph completion}. Knowledge-Based Systems.\n\n\\bibitem{wang2024c8z}\nXinyan Wang, Kuo Yang, Ting Jia, et al. (2024). \\textit{KDGene: knowledge graph completion for disease gene prediction using interactional tensor decomposition}. Briefings Bioinform..\n\n\\bibitem{liu2024x0k}\nYuhan Liu, Zelin Cao, Xing Gao, et al. (2024). \\textit{Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport}. The Web Conference.\n\n\\bibitem{li2024uio}\nYongfang Li, and Chunhua Zhu (2024). \\textit{TransE-MTP: A New Representation Learning Method for Knowledge Graph Embedding with Multi-Translation Principles and TransE}. Electronics.\n\n\\bibitem{zhang2024z78}\nQianjin Zhang, and Yandan Xu (2024). \\textit{Knowledge graph embedding with inverse function representation for link prediction}. Engineering applications of artificial intelligence.\n\n\\bibitem{wang2024534}\nHao Wang, Dandan Song, Zhijing Wu, et al. (2024). \\textit{A collaborative learning framework for knowledge graph embedding and reasoning}. Knowledge-Based Systems.\n\n\\bibitem{ni202438q}\nShengkun Ni, Xiangtai Kong, Yingying Zhang, et al. (2024). \\textit{Identifying compound-protein interactions with knowledge graph embedding of perturbation transcriptomics}. Cell Genomics.\n\n\\bibitem{nie202499i}\nJixuan Nie, Xia Hou, Wenfeng Song, et al. (2024). \\textit{Knowledge Graph Efficient Construction: Embedding Chain-of-Thought into LLMs}. VLDB Workshops.\n\n\\bibitem{wang2024d52}\nJingchao Wang, Weimin Li, Fangfang Liu, et al. (2024). \\textit{ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion}. Expert systems with applications.\n\n\\bibitem{mao2024v2s}\nYuren Mao, Yu Hao, Xin Cao, et al. (2024). \\textit{Dynamic Graph Embedding via Meta-Learning}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{jafarzadeh202468v}\nParastoo Jafarzadeh, F. Ensan, Mahdiyar Ali Akbar Alavi, et al. (2024). \\textit{A Knowledge Graph Embedding Model for Answering Factoid Entity Questions}. ACM Trans. Inf. Syst..\n\n\\bibitem{wang2024dea}\nYalin Wang, Yubin Peng, and Jingyu Guo (2024). \\textit{Enhancing knowledge graph embedding with structure and semantic features}. Applied intelligence (Boston).\n\n\\bibitem{lu202436n}\nYuhuan Lu, Weijian Yu, Xin Jing, et al. (2024). \\textit{HyperCL: A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{han2024gaq}\nYadan Han, Guangquan Lu, Shichao Zhang, et al. (2024). \\textit{A Temporal Knowledge Graph Embedding Model Based on Variable Translation}. Tsinghua Science and Technology.\n\n\\bibitem{liu2024jz8}\nBingchen Liu, Shifu Hou, Weiyi Zhong, et al. (2024). \\textit{Enhancing Temporal Knowledge Graph Alignment in News Domain With Box Embedding}. IEEE Transactions on Computational Social Systems.\n\n\\bibitem{he2024y6o}\nYunjie He, Daniel Hernndez, M. Nayyeri, et al. (2024). \\textit{Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning}. Unpublished manuscript.\n\n\\bibitem{fang20243a4}\nYan Fang, Xiaodong Liu, Wei Lu, et al. (2024). \\textit{Knowledge graph completion with low-dimensional gated hierarchical hyperbolic embedding}. Knowledge-Based Systems.\n\n\\bibitem{zhang2024h9k}\nMingtao Zhang, Guoli Yang, Yi Liu, et al. (2024). \\textit{Knowledge graph accuracy evaluation: an LLM-enhanced embedding approach}. International Journal of Data Science and Analysis.\n\n\\bibitem{li2024wyh}\nYicong Li, Yu Yang, Jiannong Cao, et al. (2024). \\textit{Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach}. Knowledge Discovery and Data Mining.\n\n\\bibitem{dong2024ijo}\nDibo Dong, Shangwei Wang, Qiaoying Guo, et al. (2024). \\textit{Short-Term Marine Wind Speed Forecasting Based on Dynamic Graph Embedding and Spatiotemporal Information}. Journal of Marine Science and Engineering.\n\n\\bibitem{wang20246c7}\nTao Wang, Bo Shen, Jinglin Zhang, et al. (2024). \\textit{Knowledge Graph Embedding via Triplet Component Interactions}. Neural Processing Letters.\n\n\\bibitem{zhang2024yjo}\nPengfei Zhang, Xiaoxue Zhang, Yang Fang, et al. (2024). \\textit{Knowledge Graph Embedding for Hierarchical Entities Based on Auto-Embedding Size}. Mathematics.\n\n\\bibitem{liang20247wv}\nK. Liang, Yue Liu, Hao Li, et al. (2024). \\textit{Clustering then Propagation: Select Better Anchors for Knowledge Graph Embedding}. Neural Information Processing Systems.\n\n\\bibitem{liu2024t05}\nQi Liu, Yuanyuan Jin, Xuefei Cao, et al. (2024). \\textit{An Entity Ontology-Based Knowledge Graph Embedding Approach to News Credibility Assessment}. IEEE Transactions on Computational Social Systems.\n\n\\bibitem{pham20243mh}\nH. V. Pham, Trung Tuan Nguyen, Luu Minh Tuan, et al. (2024). \\textit{IDGCN: A Proposed Knowledge Graph Embedding With Graph Convolution Network For Context-Aware Recommendation Systems}. Journal of Organizational Computing and Electronic Commerce.\n\n\\bibitem{li2024gar}\nYu Li, Zhu-Hong You, Shu-Min Wang, et al. (2024). \\textit{Attention-Based Learning for Predicting Drug-Drug Interactions in Knowledge Graph Embedding Based on Multisource Fusion Information}. International Journal of Intelligent Systems.\n\n\\bibitem{li2024nje}\nNan Li, Zhihao Yang, Jian Wang, et al. (2024). \\textit{Drugtarget interaction prediction using knowledge graph embedding}. iScience.\n\n\\bibitem{bao20249xp}\nLiming Bao, Yan Wang, Xiaoyu Song, et al. (2024). \\textit{HGCGE: hyperbolic graph convolutional networks-based knowledge graph embedding for link prediction}. Knowledge and Information Systems.\n\n\\bibitem{xu2024fto}\nGuoshun Xu, Guozheng Rao, Li Zhang, et al. (2024). \\textit{Entity-relation aggregation mechanism graph neural network for knowledge graph embedding}. Applied intelligence (Boston).\n\n\\bibitem{liang2024z0q}\nQiuyu Liang, Weihua Wang, Jie Yu, et al. (2024). \\textit{Effective Knowledge Graph Embedding with Quaternion Convolutional Networks}. Natural Language Processing and Chinese Computing.\n\n\\bibitem{liu2024ixy}\nJie Liu, Lizheng Zu, Yunbin Yan, et al. (2024). \\textit{Multi-Filter soft shrinkage network for knowledge graph embedding}. Expert systems with applications.\n\n\\bibitem{dong2025l9k}\nJie Dong, Cuiping Chen, Chi Zhang, et al. (2025). \\textit{Knowledge Graph Embedding With Graph Convolutional Network and Bidirectional Gated Recurrent Unit for Fault Diagnosis of Industrial Processes}. IEEE Sensors Journal.\n\n\\bibitem{zhang2025ebv}\nSensen Zhang, Xun Liang, Simin Niu, et al. (2025). \\textit{Integrating Large Language Models and Mbius Group Transformations for Temporal Knowledge Graph Embedding on the Riemann Sphere}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{liu20242zm}\nXinyue Liu, Jianan Zhang, Chi Ma, et al. (2024). \\textit{Temporal Knowledge Graph Reasoning with Dynamic Hypergraph Embedding}. International Conference on Language Resources and Evaluation.\n\n\\bibitem{yang2024lwa}\nRuiyi Yang, Flora D. Salim, and Hao Xue (2024). \\textit{SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding}. The Web Conference.\n\n\\bibitem{li20246qx}\nBo Li, Haowei Quan, Jiawei Wang, et al. (2024). \\textit{Neural Library Recommendation by Embedding Project-Library Knowledge Graph}. IEEE Transactions on Software Engineering.\n\n\\bibitem{liu2024mji}\nXiaojian Liu, Xinwei Guo, and Wen Gu (2024). \\textit{SecKG2vec: A novel security knowledge graph relational reasoning method based on semantic and structural fusion embedding}. Computers & security.\n\n\\bibitem{chen2024efo}\nBin Chen, Hongyi Li, Di Zhao, et al. (2024). \\textit{Quality assessment of cyber threat intelligence knowledge graph based on adaptive joining of embedding model}. Complex &amp; Intelligent Systems.\n\n\\bibitem{chen2024uld}\nDeng Chen, Weiwei Zhang, and Zuohua Ding (2024). \\textit{Embedding dynamic graph attention mechanism into Clinical Knowledge Graph for enhanced diagnostic accuracy}. Expert systems with applications.\n\n\\bibitem{wang2017zm5}\nQuan Wang, Zhendong Mao, Bin Wang, et al. (2017). \\textit{Knowledge Graph Embedding: A Survey of Approaches and Applications}. IEEE Transactions on Knowledge and Data Engineering.\n\n\\bibitem{li2021qr0}\nZhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \\textit{Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\end{thebibliography}\n\n\\end{document}",
  "generation_date": "2025-10-06T00:46:50.523277",
  "processed_papers_data": [
    {
      "success": true,
      "doc_id": "d2f55bb9389873d0c80995c4bc2cee6f",
      "summary": "Here's a focused summary of the paper \"Bootstrapping Entity Alignment with Knowledge Graph Embedding\" \\cite{sun2018} for a literature review:\n\n---\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing embedding-based entity alignment approaches are significantly challenged by the scarcity of sufficient prior alignment (labeled training data), which leads to low precision in alignment results. Furthermore, alignment-oriented Knowledge Graph (KG) embedding remains largely unexplored.\n    *   **Importance and Challenge:** Knowledge Graphs are crucial for AI applications, but integrating heterogeneous KGs via entity alignment is essential to overcome the limitations of single KGs. While embedding-based methods offer advantages by exploiting inherent semantics independent of KG heterogeneity, their reliance on limited prior alignment hinders their effectiveness and accuracy.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon and extends prior embedding-based entity alignment methods like MTransE \\cite{chen2017}, IPTransE \\cite{zhu2017}, and JAPE \\cite{sun2017}.\n    *   **Limitations of Previous Solutions:**\n        *   **IPTransE \\cite{zhu2017}:** Relies on a local optimal distance measure for finding newly-aligned entities, which is highly sensitive to initial alignment precision. This can lead to error accumulation during iterations, requiring a large amount of known prior alignment to guarantee accuracy.\n        *   **JAPE \\cite{sun2017}:** Its effectiveness is reduced when attributes are heterogeneous or their correlations are vague between KGs, as it leverages attribute embeddings.\n        *   **General Limitation:** Most existing embedding-based approaches suffer from the fundamental challenge of limited prior alignment, preventing them from learning accurate embeddings for robust entity alignment.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{sun2018} proposes a novel **bootstrapping approach** for embedding-based entity alignment. It iteratively labels likely entity alignments to expand the training data, which is then used to learn and refine alignment-oriented KG embeddings. A crucial component is an **alignment editing method** designed to mitigate error accumulation during these iterative steps. The problem is framed as a classification task aiming to maximize alignment likelihood under a one-to-one constraint.\n    *   **Novelty and Differentiation:**\n        *   **Alignment-Oriented KG Embedding:** Introduces a **limit-based objective function** that explicitly enforces positive triples to have absolutely low scores and negative triples to have high scores, reducing embedding drift and better capturing common semantics.\n        *   **-Truncated Uniform Negative Sampling:** Generates more challenging negative triples by limiting the sampling scope to `s`-nearest neighbors in the embedding space, rather than arbitrary entities, forcing the model to learn finer distinctions.\n        *   **Parameter Swapping:** Leverages prior alignment by swapping aligned entities in their triples to calibrate embeddings of different KGs into a unified space.\n        *   **Bootstrapping Process with Global Optimization:** Unlike conventional bootstrapping methods that rely on local confidence thresholds, \\cite{sun2018} labels likely alignment by solving a **max-weighted matching problem on bipartite graphs**. This ensures a global optimal goal for labeling and adheres to the one-to-one alignment constraint, enhancing accuracy.\n        *   **Alignment Editing:** Allows entities to be relabeled or become unlabeled across iterations to resolve conflicts and reduce error propagation, improving the quality of the iteratively labeled data.\n        *   **Holistic Learning:** Combines the KG semantics objective with the alignment likelihood objective into a joint function for comprehensive embedding learning.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A **limit-based objective function** for learning alignment-oriented KG embeddings that explicitly controls the absolute scores of positive and negative triples.\n        *   **-truncated uniform negative sampling** for generating more informative negative triples.\n        *   A **bootstrapping process** that iteratively expands training data by labeling likely alignments.\n        *   A **global optimal labeling strategy** based on max-weighted matching to ensure accurate and one-to-one alignment.\n        *   An **alignment editing method** to reduce error accumulation and resolve conflicts in the iteratively labeled data.\n    *   **System Design/Architectural Innovations:** Integration of a robust KG embedding model with a semi-supervised bootstrapping framework, jointly optimizing for KG semantics and alignment likelihood.\n    *   **Theoretical Insights/Analysis:** The insight that explicitly controlling absolute scores of triples can improve common semantics capture for alignment, and that global optimization for iterative labeling is more robust than local confidence measures.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Evaluated the proposed approach (BootEA) on three cross-lingual datasets (DBP15K: ZH-EN, JA-EN, FR-EN) and two large-scale datasets (DWY100K: DBP-WD, DBP-YG). Compared performance against state-of-the-art embedding-based methods: MTransE \\cite{chen2017}, IPTransE \\cite{zhu2017}, and JAPE \\cite{sun2017}. Ablation studies were performed to analyze the contributions of individual components (bootstrapping, negative sampling, labeling, editing).\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The proposed BootEA \"significantly outperformed the state-of-the-art embedding-based ones for entity alignment.\"\n        *   The bootstrapping process alone led to a \"13%18% improvement on precision.\"\n        *   Experimental analysis confirmed that the alignment-oriented KG embedding, the bootstrapping process, and the alignment editing method all contributed positively to the overall performance improvement.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The approach assumes a one-to-one entity alignment scenario. Its effectiveness relies on the quality of initial embeddings for the -truncated negative sampling. The choice of hyperparameters is crucial.\n    *   **Scope of Applicability:** Primarily designed for embedding-based entity alignment between different KGs, particularly effective in scenarios with limited initial prior alignment. Applicable to both cross-lingual and large-scale KG datasets.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** \\cite{sun2018} significantly advances the state-of-the-art in embedding-based entity alignment by effectively addressing the critical challenge of limited prior alignment. It introduces robust mechanisms for iterative self-training and error mitigation, leading to substantial performance gains.\n    *   **Potential Impact on Future Research:** This work provides a strong foundation for future research in semi-supervised and low-resource entity alignment. The novel techniques for alignment-oriented embedding, adaptive negative sampling, and robust iterative labeling (especially the global optimization and editing methods) could inspire similar advancements in other knowledge graph completion and integration tasks.",
      "intriguing_abstract": "Integrating diverse Knowledge Graphs is vital for advanced AI, yet a critical bottleneck persists: the scarcity of labeled data for robust **entity alignment**. Existing **embedding-based methods**, while powerful, falter under this data scarcity, leading to imprecise alignments and limiting their real-world applicability. We introduce BootEA, a novel **bootstrapping framework** that revolutionizes entity alignment by effectively overcoming the challenge of limited prior alignments.\n\nBootEA iteratively expands training data through a robust self-training process. Crucially, it employs a **global optimal labeling strategy** based on **max-weighted matching** to ensure accurate, one-to-one alignments, coupled with an innovative **alignment editing method** to actively mitigate error accumulation. Furthermore, we propose a unique **alignment-oriented Knowledge Graph embedding** model featuring a limit-based objective function and **-truncated negative sampling**, designed to learn common semantics more effectively. Evaluated on multiple cross-lingual and large-scale datasets, BootEA significantly outperforms state-of-the-art methods, demonstrating substantial precision gains (e.g., 13%-18% improvement from bootstrapping alone). This work establishes a new paradigm for robust, **semi-supervised entity alignment**, paving the way for more comprehensive and accurate Knowledge Graph integration in low-resource settings.",
      "keywords": [
        "Embedding-based entity alignment",
        "Limited prior alignment",
        "Bootstrapping approach",
        "Alignment-oriented KG embedding",
        "Limit-based objective function",
        "-Truncated Uniform Negative Sampling",
        "Global optimal labeling strategy",
        "Max-weighted matching",
        "Alignment editing method",
        "Error accumulation mitigation",
        "Semi-supervised entity alignment",
        "Outperformed state-of-the-art",
        "Cross-lingual entity alignment",
        "Large-scale entity alignment",
        "Heterogeneous KGs"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/d899e434a7f2eecf33a90053df84cf32842fbca9.pdf",
      "citation_key": "sun2018",
      "metadata": {
        "title": "Bootstrapping Entity Alignment with Knowledge Graph Embedding",
        "authors": [
          "Zequn Sun",
          "Wei Hu",
          "Qingheng Zhang",
          "Yuzhong Qu"
        ],
        "published_date": "2018",
        "abstract": "Embedding-based entity alignment represents different knowledge graphs (KGs) as low-dimensional embeddings and finds entity alignment by measuring the similarities between entity embeddings. Existing approaches have achieved promising results, however, they are still challenged by the lack of enough prior alignment as labeled training data. In this paper, we propose a bootstrapping approach to embedding-based entity alignment. It iteratively labels likely entity alignment as training data for learning alignment-oriented KG embeddings. Furthermore, it employs an alignment editing method to reduce error accumulation during iterations. Our experiments on real-world datasets showed that the proposed approach significantly outperformed the state-of-the-art embedding-based ones for entity alignment. The proposed alignment-oriented KG embedding, bootstrapping process and alignment editing method all contributed to the performance improvement.",
        "file_path": "paper_data/knowledge_graph_embedding/d899e434a7f2eecf33a90053df84cf32842fbca9.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Bootstrapping Entity Alignment with Knowledge Graph Embedding\" \\cite{sun2018} for a literature review:\n\n---\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing embedding-based entity alignment approaches are significantly challenged by the scarcity of sufficient prior alignment (labeled training data), which leads to low precision in alignment results. Furthermore, alignment-oriented Knowledge Graph (KG) embedding remains largely unexplored.\n    *   **Importance and Challenge:** Knowledge Graphs are crucial for AI applications, but integrating heterogeneous KGs via entity alignment is essential to overcome the limitations of single KGs. While embedding-based methods offer advantages by exploiting inherent semantics independent of KG heterogeneity, their reliance on limited prior alignment hinders their effectiveness and accuracy.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon and extends prior embedding-based entity alignment methods like MTransE \\cite{chen2017}, IPTransE \\cite{zhu2017}, and JAPE \\cite{sun2017}.\n    *   **Limitations of Previous Solutions:**\n        *   **IPTransE \\cite{zhu2017}:** Relies on a local optimal distance measure for finding newly-aligned entities, which is highly sensitive to initial alignment precision. This can lead to error accumulation during iterations, requiring a large amount of known prior alignment to guarantee accuracy.\n        *   **JAPE \\cite{sun2017}:** Its effectiveness is reduced when attributes are heterogeneous or their correlations are vague between KGs, as it leverages attribute embeddings.\n        *   **General Limitation:** Most existing embedding-based approaches suffer from the fundamental challenge of limited prior alignment, preventing them from learning accurate embeddings for robust entity alignment.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{sun2018} proposes a novel **bootstrapping approach** for embedding-based entity alignment. It iteratively labels likely entity alignments to expand the training data, which is then used to learn and refine alignment-oriented KG embeddings. A crucial component is an **alignment editing method** designed to mitigate error accumulation during these iterative steps. The problem is framed as a classification task aiming to maximize alignment likelihood under a one-to-one constraint.\n    *   **Novelty and Differentiation:**\n        *   **Alignment-Oriented KG Embedding:** Introduces a **limit-based objective function** that explicitly enforces positive triples to have absolutely low scores and negative triples to have high scores, reducing embedding drift and better capturing common semantics.\n        *   **-Truncated Uniform Negative Sampling:** Generates more challenging negative triples by limiting the sampling scope to `s`-nearest neighbors in the embedding space, rather than arbitrary entities, forcing the model to learn finer distinctions.\n        *   **Parameter Swapping:** Leverages prior alignment by swapping aligned entities in their triples to calibrate embeddings of different KGs into a unified space.\n        *   **Bootstrapping Process with Global Optimization:** Unlike conventional bootstrapping methods that rely on local confidence thresholds, \\cite{sun2018} labels likely alignment by solving a **max-weighted matching problem on bipartite graphs**. This ensures a global optimal goal for labeling and adheres to the one-to-one alignment constraint, enhancing accuracy.\n        *   **Alignment Editing:** Allows entities to be relabeled or become unlabeled across iterations to resolve conflicts and reduce error propagation, improving the quality of the iteratively labeled data.\n        *   **Holistic Learning:** Combines the KG semantics objective with the alignment likelihood objective into a joint function for comprehensive embedding learning.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A **limit-based objective function** for learning alignment-oriented KG embeddings that explicitly controls the absolute scores of positive and negative triples.\n        *   **-truncated uniform negative sampling** for generating more informative negative triples.\n        *   A **bootstrapping process** that iteratively expands training data by labeling likely alignments.\n        *   A **global optimal labeling strategy** based on max-weighted matching to ensure accurate and one-to-one alignment.\n        *   An **alignment editing method** to reduce error accumulation and resolve conflicts in the iteratively labeled data.\n    *   **System Design/Architectural Innovations:** Integration of a robust KG embedding model with a semi-supervised bootstrapping framework, jointly optimizing for KG semantics and alignment likelihood.\n    *   **Theoretical Insights/Analysis:** The insight that explicitly controlling absolute scores of triples can improve common semantics capture for alignment, and that global optimization for iterative labeling is more robust than local confidence measures.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Evaluated the proposed approach (BootEA) on three cross-lingual datasets (DBP15K: ZH-EN, JA-EN, FR-EN) and two large-scale datasets (DWY100K: DBP-WD, DBP-YG). Compared performance against state-of-the-art embedding-based methods: MTransE \\cite{chen2017}, IPTransE \\cite{zhu2017}, and JAPE \\cite{sun2017}. Ablation studies were performed to analyze the contributions of individual components (bootstrapping, negative sampling, labeling, editing).\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The proposed BootEA \"significantly outperformed the state-of-the-art embedding-based ones for entity alignment.\"\n        *   The bootstrapping process alone led to a \"13%18% improvement on precision.\"\n        *   Experimental analysis confirmed that the alignment-oriented KG embedding, the bootstrapping process, and the alignment editing method all contributed positively to the overall performance improvement.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The approach assumes a one-to-one entity alignment scenario. Its effectiveness relies on the quality of initial embeddings for the -truncated negative sampling. The choice of hyperparameters is crucial.\n    *   **Scope of Applicability:** Primarily designed for embedding-based entity alignment between different KGs, particularly effective in scenarios with limited initial prior alignment. Applicable to both cross-lingual and large-scale KG datasets.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** \\cite{sun2018} significantly advances the state-of-the-art in embedding-based entity alignment by effectively addressing the critical challenge of limited prior alignment. It introduces robust mechanisms for iterative self-training and error mitigation, leading to substantial performance gains.\n    *   **Potential Impact on Future Research:** This work provides a strong foundation for future research in semi-supervised and low-resource entity alignment. The novel techniques for alignment-oriented embedding, adaptive negative sampling, and robust iterative labeling (especially the global optimization and editing methods) could inspire similar advancements in other knowledge graph completion and integration tasks.",
        "keywords": [
          "Embedding-based entity alignment",
          "Limited prior alignment",
          "Bootstrapping approach",
          "Alignment-oriented KG embedding",
          "Limit-based objective function",
          "-Truncated Uniform Negative Sampling",
          "Global optimal labeling strategy",
          "Max-weighted matching",
          "Alignment editing method",
          "Error accumulation mitigation",
          "Semi-supervised entity alignment",
          "Outperformed state-of-the-art",
          "Cross-lingual entity alignment",
          "Large-scale entity alignment",
          "Heterogeneous KGs"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "d899e434a7f2eecf33a90053df84cf32842fbca9.pdf"
    },
    {
      "success": true,
      "doc_id": "8e38b25b72dbbd59fd60128d97798c90",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Technical Paper Analysis: HyTE: Hyperplane-based Temporally Aware Knowledge Graph Embeddings \\cite{dasgupta2018}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most existing Knowledge Graph (KG) embedding methods ignore the inherent temporal dynamics of relational facts within KGs. For example, a fact like (Cristiano_Ronaldo, playsFor, Manchester_United) is only valid for a specific time period (e.g., 2003-2009).\n    *   **Importance & Challenge**: Ignoring the temporal dimension leads to incomplete or inaccurate KG inference, as the validity of facts is time-dependent. The challenge lies in effectively integrating time into the embedding space without overly complicating the model or losing expressive power.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself against the majority of existing KG embedding methods that do not account for the temporal dimension.\n    *   **Limitations of Previous Solutions**: Previous solutions are limited by their inability to capture the time-varying nature of facts, leading to static representations that cannot perform temporally-guided inference or predict temporal validity.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes HyTE, a temporally aware KG embedding method. Its core innovation is to explicitly incorporate time into the entity-relation embedding space.\n    *   **Novelty**: HyTE achieves this by associating each timestamp with a corresponding hyperplane. This allows the model to represent the temporal validity of facts geometrically within the embedding space.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of HyTE, a novel hyperplane-based approach for integrating temporal information into KG embeddings.\n    *   **Functional Capabilities**:\n        *   Performs KG inference that is guided by temporal information.\n        *   Predicts temporal scopes for relational facts that have missing time annotations, a crucial capability for incomplete KGs.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experimentation was carried out on temporal datasets. These datasets were extracted from real-world KGs.\n    *   **Key Performance Metrics & Results**: The experiments demonstrated the effectiveness of HyTE. It showed superior performance compared to both traditional KG embedding methods (which ignore time) and other existing temporal KG embedding methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract does not explicitly detail specific technical limitations or assumptions beyond the model's design choice of using hyperplanes.\n    *   **Scope of Applicability**: HyTE is specifically designed for Knowledge Graphs where relational facts exhibit temporal dynamics and where temporal information is either available or needs to be inferred.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HyTE significantly advances the technical state-of-the-art in KG embeddings by providing a principled and effective way to explicitly model and leverage temporal information.\n    *   **Potential Impact**: It opens new avenues for more accurate and context-aware KG inference, especially in dynamic environments. The ability to predict temporal scopes for missing annotations is particularly impactful for knowledge graph completion and maintenance, enabling more robust and complete temporal KGs.",
      "intriguing_abstract": "Traditional Knowledge Graph (KG) embeddings often overlook a critical dimension: time. Relational facts are rarely static; their validity is inherently temporal, leading to incomplete and inaccurate inference in dynamic KGs. We introduce **HyTE**, a novel **hyperplane-based temporally aware KG embedding** method that fundamentally redefines how temporal dynamics are integrated into embedding spaces.\n\nHyTE innovatively associates each timestamp with a unique hyperplane, geometrically representing the temporal validity of facts within the entity-relation embedding space. This allows for principled **temporally-guided KG inference** and, crucially, enables the prediction of **temporal scopes** for facts with missing time annotations  a vital capability for robust **knowledge graph completion** and maintenance. Our extensive experiments on real-world temporal datasets demonstrate HyTE's superior performance over both static and existing temporal embedding approaches, marking a significant advancement in the state-of-the-art. HyTE offers a powerful framework for building more accurate, context-aware, and dynamically evolving KGs.",
      "keywords": [
        "Knowledge Graph Embeddings",
        "Temporal Dynamics",
        "Relational Facts",
        "HyTE",
        "Hyperplane-based Approach",
        "Temporally Aware Embeddings",
        "KG Inference",
        "Predicting Temporal Scopes",
        "Missing Time Annotations",
        "Knowledge Graph Completion",
        "State-of-the-Art Advancement",
        "Dynamic Environments"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/83d58bc46b7adb92d8750da52313f060b10f201d.pdf",
      "citation_key": "dasgupta2018",
      "metadata": {
        "title": "HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding",
        "authors": [
          "S. Dasgupta",
          "Swayambhu Nath Ray",
          "P. Talukdar"
        ],
        "published_date": "2018",
        "abstract": "Knowledge Graph (KG) embedding has emerged as an active area of research resulting in the development of several KG embedding methods. Relational facts in KG often show temporal dynamics, e.g., the fact (Cristiano_Ronaldo, playsFor, Manchester_United) is valid only from 2003 to 2009. Most of the existing KG embedding methods ignore this temporal dimension while learning embeddings of the KG elements. In this paper, we propose HyTE, a temporally aware KG embedding method which explicitly incorporates time in the entity-relation space by associating each timestamp with a corresponding hyperplane. HyTE not only performs KG inference using temporal guidance, but also predicts temporal scopes for relational facts with missing time annotations. Through extensive experimentation on temporal datasets extracted from real-world KGs, we demonstrate the effectiveness of our model over both traditional as well as temporal KG embedding methods.",
        "file_path": "paper_data/knowledge_graph_embedding/83d58bc46b7adb92d8750da52313f060b10f201d.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Technical Paper Analysis: HyTE: Hyperplane-based Temporally Aware Knowledge Graph Embeddings \\cite{dasgupta2018}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most existing Knowledge Graph (KG) embedding methods ignore the inherent temporal dynamics of relational facts within KGs. For example, a fact like (Cristiano_Ronaldo, playsFor, Manchester_United) is only valid for a specific time period (e.g., 2003-2009).\n    *   **Importance & Challenge**: Ignoring the temporal dimension leads to incomplete or inaccurate KG inference, as the validity of facts is time-dependent. The challenge lies in effectively integrating time into the embedding space without overly complicating the model or losing expressive power.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself against the majority of existing KG embedding methods that do not account for the temporal dimension.\n    *   **Limitations of Previous Solutions**: Previous solutions are limited by their inability to capture the time-varying nature of facts, leading to static representations that cannot perform temporally-guided inference or predict temporal validity.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes HyTE, a temporally aware KG embedding method. Its core innovation is to explicitly incorporate time into the entity-relation embedding space.\n    *   **Novelty**: HyTE achieves this by associating each timestamp with a corresponding hyperplane. This allows the model to represent the temporal validity of facts geometrically within the embedding space.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of HyTE, a novel hyperplane-based approach for integrating temporal information into KG embeddings.\n    *   **Functional Capabilities**:\n        *   Performs KG inference that is guided by temporal information.\n        *   Predicts temporal scopes for relational facts that have missing time annotations, a crucial capability for incomplete KGs.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experimentation was carried out on temporal datasets. These datasets were extracted from real-world KGs.\n    *   **Key Performance Metrics & Results**: The experiments demonstrated the effectiveness of HyTE. It showed superior performance compared to both traditional KG embedding methods (which ignore time) and other existing temporal KG embedding methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract does not explicitly detail specific technical limitations or assumptions beyond the model's design choice of using hyperplanes.\n    *   **Scope of Applicability**: HyTE is specifically designed for Knowledge Graphs where relational facts exhibit temporal dynamics and where temporal information is either available or needs to be inferred.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HyTE significantly advances the technical state-of-the-art in KG embeddings by providing a principled and effective way to explicitly model and leverage temporal information.\n    *   **Potential Impact**: It opens new avenues for more accurate and context-aware KG inference, especially in dynamic environments. The ability to predict temporal scopes for missing annotations is particularly impactful for knowledge graph completion and maintenance, enabling more robust and complete temporal KGs.",
        "keywords": [
          "Knowledge Graph Embeddings",
          "Temporal Dynamics",
          "Relational Facts",
          "HyTE",
          "Hyperplane-based Approach",
          "Temporally Aware Embeddings",
          "KG Inference",
          "Predicting Temporal Scopes",
          "Missing Time Annotations",
          "Knowledge Graph Completion",
          "State-of-the-Art Advancement",
          "Dynamic Environments"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "83d58bc46b7adb92d8750da52313f060b10f201d.pdf"
    },
    {
      "success": true,
      "doc_id": "68b9e62d077f7a54d1e17a13a8e14a5c",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n---\n\n### Focused Summary for Literature Review: Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding \\cite{chen2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Conventional Knowledge Graph Embedding (KGE) methods suffer from inefficient parameter storage costs, as the number of embedding parameters increases linearly with the growth of knowledge graphs (KGs).\n    *   **Importance and Challenge**:\n        *   KGs are often very large, leading to colossal parameter counts (e.g., 123 million parameters for RotatE on YAGO3-10).\n        *   This linear scaling poses significant challenges for real-world applications, such as deploying KGE models on resource-constrained edge devices or increasing communication costs in federated learning scenarios.\n        *   There is a critical need for KGE methods with a stable, efficient, and lower parameter count that is independent of the number of entities.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Contrasts with conventional KGE methods (e.g., TransE, RotatE, DistMult, ComplEx) and GNN-based KGEs (e.g., R-GCN, CompGCN), which do not prioritize parameter efficiency.\n        *   Relates to general parameter-efficient deep learning techniques (e.g., pruning, quantization, parameter sharing, knowledge distillation) and specific parameter-efficient KGEs based on quantization (TS-CL, LightKG) or knowledge distillation (MulDE, DualDE).\n        *   Identifies NodePiece as the most relevant work, which also uses a compositional method with anchors and relations for entity representation.\n    *   **Limitations of Previous Solutions**:\n        *   Most KGE methods learn a specific embedding for each entity, leading to the parameter explosion problem.\n        *   Existing parameter-efficient KGEs (quantization, distillation) typically require training a standard KGE model *first* and then applying compression, which is a different paradigm than learning parameter-efficient representations from the outset.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Entity-Agnostic Representation Learning (EARL)** \\cite{chen2023}. Instead of learning specific embeddings for *all* entities, EARL only learns embeddings for a small, pre-selected set of \"reserved entities\" ($E_{res}$). For all other entities, it employs universal, entity-agnostic encoders to transform their \"distinguishable information\" into embeddings.\n    *   **Novelty/Difference**:\n        *   **Entity-Agnostic Encoding**: The model's components (encoders) are independent of the number of entities, ensuring a static and efficient parameter count that does not scale linearly with KG size.\n        *   **Three Types of Distinguishable Information**:\n            1.  **ConRel (Connected Relation Information)**: Captures an entity's semantics by its connected relations and their directions. A novel \"relational feature\" is introduced, representing the frequency of an entity being a head or tail for each relation. This feature is then encoded using a 2-layer MLP.\n            2.  **kNResEnt (k-Nearest Reserved Entity Information)**: Addresses the potential ambiguity of ConRel by incorporating information from similar reserved entities. It calculates cosine similarity between an entity's relational feature and those of reserved entities, then uses a weighted sum of the top-k nearest reserved entity embeddings.\n            3.  **MulHop (Multi-hop Neighbor Information)**: Integrates broader structural context by feeding the combined `ConRel` and `kNResEnt` encodings into a Graph Neural Network (GNN). The GNN aggregates multi-hop neighbor information to refine entity representations.\n        *   **Modular Design**: Combines these three information types sequentially: relational features -> ConRel encoding -> kNResEnt encoding -> GNN for MulHop encoding.\n        *   **Training**: Utilizes RotatE \\cite{chen2023} as the score function and a self-adversarial negative sampling loss for optimization.\n\n4.  **Key Technical Contributions** \\cite{chen2023}\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of the concept of \"entity-agnostic representation learning\" as a paradigm shift for KGEs to tackle parameter efficiency.\n        *   Development of EARL, a novel KGE method that encodes entity embeddings from their distinguishable information rather than direct lookup.\n        *   Design of \"relational features\" to effectively capture connected relation information for entities.\n        *   Integration of k-nearest reserved entities and multi-hop neighbor information for robust entity distinguishability.\n    *   **System Design/Architectural Innovations**: A unique architecture that combines a small set of trainable reserved entity embeddings with universal encoders for all other entities, enabling parameter efficiency.\n    *   **Theoretical Insights/Analysis**: Demonstrates that rich entity representations can be learned compositionally from local context and structural information, rather than requiring unique, large-scale embedding vectors for every entity.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive empirical evaluations were performed on various KG benchmarks with diverse characteristics. The primary task was link prediction.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   EARL \\cite{chen2023} consistently uses *fewer parameters* compared to conventional KGE baselines.\n        *   Despite using fewer parameters, EARL achieves *better or competitive performance* on link prediction tasks, demonstrating its parameter efficiency.\n        *   The results empirically validate the effectiveness of the entity-agnostic encoding approach.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The selection of reserved entities is random, and the impact of different selection strategies or the hyper-parameter `k` for nearest reserved entities is not fully explored in the provided text. The paper's primary focus is parameter efficiency, not necessarily outperforming all state-of-the-art KGE models in raw performance.\n    *   **Scope of Applicability**: Primarily applicable to knowledge graph embedding tasks, particularly link prediction. Its main benefit lies in enabling KGE deployment in resource-constrained environments (e.g., edge devices) and distributed learning settings (e.g., federated learning) where parameter count is a critical factor.\n\n7.  **Technical Significance** \\cite{chen2023}\n    *   **Advances State-of-the-Art**: EARL \\cite{chen2023} introduces a novel and highly effective approach to address the long-standing problem of parameter explosion in KGEs, shifting the paradigm from entity-specific embeddings to entity-agnostic encoding. It demonstrates that significant parameter reduction can be achieved without sacrificing performance.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research into compositional and generative entity representation learning in KGs.\n        *   Facilitates the practical deployment of KGE models in real-world, resource-limited scenarios.\n        *   Could inspire further exploration of different types of \"distinguishable information\" and more advanced entity-agnostic encoding architectures.\n        *   Contributes to the development of more scalable, sustainable, and efficient knowledge graph technologies.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) models are indispensable for knowledge-driven AI, yet their utility is severely hampered by a critical bottleneck: parameter explosion. As knowledge graphs scale, embedding costs grow linearly, precluding deployment on resource-constrained edge devices and in federated learning environments. We introduce **Entity-Agnostic Representation Learning (EARL)**, a novel paradigm that fundamentally redefines how entities are represented. Instead of learning a unique embedding for every entity, EARL employs universal, entity-agnostic encoders to derive rich representations from an entity's \"distinguishable information,\" relying only on a small set of reserved entity embeddings.\n\nEARL innovatively combines three components: **ConRel** captures connected relation semantics, **kNResEnt** leverages information from k-nearest reserved entities, and **MulHop** integrates multi-hop structural context via a Graph Neural Network (GNN). This modular design dramatically reduces parameter counts by orders of magnitude compared to conventional KGEs. Crucially, EARL consistently achieves competitive or superior performance on challenging link prediction tasks. This work represents a significant stride towards scalable, efficient, and deployable KGEs, unlocking new possibilities for knowledge graph applications in real-world, resource-limited settings.",
      "keywords": [
        "Entity-Agnostic Representation Learning (EARL)",
        "Knowledge Graph Embedding (KGE)",
        "Parameter Efficiency",
        "Relational Features",
        "Entity-Agnostic Encoders",
        "Multi-hop Neighbor Information",
        "Graph Neural Networks (GNN)",
        "Link Prediction",
        "Resource-Constrained Devices",
        "Federated Learning",
        "Compositional Entity Representation",
        "Distinguishable Information",
        "Reserved Entities"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf",
      "citation_key": "chen2023",
      "metadata": {
        "title": "Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding",
        "authors": [
          "Mingyang Chen",
          "Wen Zhang",
          "Zhen Yao",
          "Yushan Zhu",
          "Yang Gao",
          "Jeff Z. Pan",
          "Hua-zeng Chen"
        ],
        "published_date": "2023",
        "abstract": "We propose an entity-agnostic representation learning method for handling the problem of inefficient parameter storage costs brought by embedding knowledge graphs. Conventional knowledge graph embedding methods map elements in a knowledge graph, including entities and relations, into continuous vector spaces by assigning them one or multiple specific embeddings (i.e., vector representations). Thus the number of embedding parameters increases linearly as the growth of knowledge graphs. In our proposed model, Entity-Agnostic Representation Learning (EARL), we only learn the embeddings for a small set of entities and refer to them as reserved entities. To obtain the embeddings for the full set of entities, we encode their distinguishable information from their connected relations, k-nearest reserved entities, and multi-hop neighbors. We learn universal and entity-agnostic encoders for transforming distinguishable information into entity embeddings. This approach allows our proposed EARL to have a static, efficient, and lower parameter count than conventional knowledge graph embedding methods. Experimental results show that EARL uses fewer parameters and performs better on link prediction tasks than baselines, reflecting its parameter efficiency.",
        "file_path": "paper_data/knowledge_graph_embedding/10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n---\n\n### Focused Summary for Literature Review: Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding \\cite{chen2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Conventional Knowledge Graph Embedding (KGE) methods suffer from inefficient parameter storage costs, as the number of embedding parameters increases linearly with the growth of knowledge graphs (KGs).\n    *   **Importance and Challenge**:\n        *   KGs are often very large, leading to colossal parameter counts (e.g., 123 million parameters for RotatE on YAGO3-10).\n        *   This linear scaling poses significant challenges for real-world applications, such as deploying KGE models on resource-constrained edge devices or increasing communication costs in federated learning scenarios.\n        *   There is a critical need for KGE methods with a stable, efficient, and lower parameter count that is independent of the number of entities.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Contrasts with conventional KGE methods (e.g., TransE, RotatE, DistMult, ComplEx) and GNN-based KGEs (e.g., R-GCN, CompGCN), which do not prioritize parameter efficiency.\n        *   Relates to general parameter-efficient deep learning techniques (e.g., pruning, quantization, parameter sharing, knowledge distillation) and specific parameter-efficient KGEs based on quantization (TS-CL, LightKG) or knowledge distillation (MulDE, DualDE).\n        *   Identifies NodePiece as the most relevant work, which also uses a compositional method with anchors and relations for entity representation.\n    *   **Limitations of Previous Solutions**:\n        *   Most KGE methods learn a specific embedding for each entity, leading to the parameter explosion problem.\n        *   Existing parameter-efficient KGEs (quantization, distillation) typically require training a standard KGE model *first* and then applying compression, which is a different paradigm than learning parameter-efficient representations from the outset.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Entity-Agnostic Representation Learning (EARL)** \\cite{chen2023}. Instead of learning specific embeddings for *all* entities, EARL only learns embeddings for a small, pre-selected set of \"reserved entities\" ($E_{res}$). For all other entities, it employs universal, entity-agnostic encoders to transform their \"distinguishable information\" into embeddings.\n    *   **Novelty/Difference**:\n        *   **Entity-Agnostic Encoding**: The model's components (encoders) are independent of the number of entities, ensuring a static and efficient parameter count that does not scale linearly with KG size.\n        *   **Three Types of Distinguishable Information**:\n            1.  **ConRel (Connected Relation Information)**: Captures an entity's semantics by its connected relations and their directions. A novel \"relational feature\" is introduced, representing the frequency of an entity being a head or tail for each relation. This feature is then encoded using a 2-layer MLP.\n            2.  **kNResEnt (k-Nearest Reserved Entity Information)**: Addresses the potential ambiguity of ConRel by incorporating information from similar reserved entities. It calculates cosine similarity between an entity's relational feature and those of reserved entities, then uses a weighted sum of the top-k nearest reserved entity embeddings.\n            3.  **MulHop (Multi-hop Neighbor Information)**: Integrates broader structural context by feeding the combined `ConRel` and `kNResEnt` encodings into a Graph Neural Network (GNN). The GNN aggregates multi-hop neighbor information to refine entity representations.\n        *   **Modular Design**: Combines these three information types sequentially: relational features -> ConRel encoding -> kNResEnt encoding -> GNN for MulHop encoding.\n        *   **Training**: Utilizes RotatE \\cite{chen2023} as the score function and a self-adversarial negative sampling loss for optimization.\n\n4.  **Key Technical Contributions** \\cite{chen2023}\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of the concept of \"entity-agnostic representation learning\" as a paradigm shift for KGEs to tackle parameter efficiency.\n        *   Development of EARL, a novel KGE method that encodes entity embeddings from their distinguishable information rather than direct lookup.\n        *   Design of \"relational features\" to effectively capture connected relation information for entities.\n        *   Integration of k-nearest reserved entities and multi-hop neighbor information for robust entity distinguishability.\n    *   **System Design/Architectural Innovations**: A unique architecture that combines a small set of trainable reserved entity embeddings with universal encoders for all other entities, enabling parameter efficiency.\n    *   **Theoretical Insights/Analysis**: Demonstrates that rich entity representations can be learned compositionally from local context and structural information, rather than requiring unique, large-scale embedding vectors for every entity.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive empirical evaluations were performed on various KG benchmarks with diverse characteristics. The primary task was link prediction.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   EARL \\cite{chen2023} consistently uses *fewer parameters* compared to conventional KGE baselines.\n        *   Despite using fewer parameters, EARL achieves *better or competitive performance* on link prediction tasks, demonstrating its parameter efficiency.\n        *   The results empirically validate the effectiveness of the entity-agnostic encoding approach.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The selection of reserved entities is random, and the impact of different selection strategies or the hyper-parameter `k` for nearest reserved entities is not fully explored in the provided text. The paper's primary focus is parameter efficiency, not necessarily outperforming all state-of-the-art KGE models in raw performance.\n    *   **Scope of Applicability**: Primarily applicable to knowledge graph embedding tasks, particularly link prediction. Its main benefit lies in enabling KGE deployment in resource-constrained environments (e.g., edge devices) and distributed learning settings (e.g., federated learning) where parameter count is a critical factor.\n\n7.  **Technical Significance** \\cite{chen2023}\n    *   **Advances State-of-the-Art**: EARL \\cite{chen2023} introduces a novel and highly effective approach to address the long-standing problem of parameter explosion in KGEs, shifting the paradigm from entity-specific embeddings to entity-agnostic encoding. It demonstrates that significant parameter reduction can be achieved without sacrificing performance.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research into compositional and generative entity representation learning in KGs.\n        *   Facilitates the practical deployment of KGE models in real-world, resource-limited scenarios.\n        *   Could inspire further exploration of different types of \"distinguishable information\" and more advanced entity-agnostic encoding architectures.\n        *   Contributes to the development of more scalable, sustainable, and efficient knowledge graph technologies.",
        "keywords": [
          "Entity-Agnostic Representation Learning (EARL)",
          "Knowledge Graph Embedding (KGE)",
          "Parameter Efficiency",
          "Relational Features",
          "Entity-Agnostic Encoders",
          "Multi-hop Neighbor Information",
          "Graph Neural Networks (GNN)",
          "Link Prediction",
          "Resource-Constrained Devices",
          "Federated Learning",
          "Compositional Entity Representation",
          "Distinguishable Information",
          "Reserved Entities"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf"
    },
    {
      "success": true,
      "doc_id": "4ebf9fbbbeff731eb437fc4fa1659ba7",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Providing explainable recommendations for employee training courses while effectively considering the diverse learning motivations of talents \\cite{yang2023}.\n    *   **Importance and challenge**: Personalized training is vital for talent management, but existing recommender systems often lack transparency (explainability) and fail to incorporate the underlying motivations of learners. This deficiency can hinder user trust, adoption, and the overall effectiveness of training programs \\cite{yang2023}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: The work builds upon existing efforts in personalized employee training course recommender systems \\cite{yang2023}.\n    *   **Limitations of previous solutions**: Previous solutions generally struggle with two key aspects: providing clear explanations for recommendations and adequately accounting for the different learning motivations of employees \\cite{yang2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes CKGE (Contextualized Knowledge Graph Embedding), an approach for developing an explainable training course recommender system \\cite{yang2023}.\n    *   **Novelty/Differentiation**:\n        *   **Motivation-aware information integration**: CKGE uniquely integrates both contextualized neighbor semantics and high-order connections within a Knowledge Graph (KG) as \"motivation-aware information\" to learn effective representations of talents and courses \\cite{yang2023}.\n        *   **Meta-graph construction**: For each talent-course pair, a specific meta-graph is constructed, incorporating entity neighbors and meta-paths to capture motivation-aware context \\cite{yang2023}.\n        *   **Novel KG-based Transformer**: A specialized Transformer architecture is developed to process the meta-graph. It serializes entities and paths from the meta-graph into a sequential input \\cite{yang2023}.\n        *   **Relational attention and structural encoding**: The KG-based Transformer incorporates specially designed relational attention and structural encoding mechanisms to effectively model the global dependencies inherent in KG structured data \\cite{yang2023}.\n        *   **Local path mask prediction**: This mechanism is introduced to reveal the importance (saliency) of different meta-paths, directly contributing to the explainability of recommendations \\cite{yang2023}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms/methods**:\n        *   **CKGE Framework**: A comprehensive contextualized knowledge graph embedding approach for explainable and motivation-aware training course recommendation \\cite{yang2023}.\n        *   **Meta-graph construction for motivation awareness**: A method to dynamically build context-rich meta-graphs for talent-course pairs, integrating neighbor semantics and high-order connections \\cite{yang2023}.\n        *   **KG-based Transformer**: A novel Transformer architecture tailored for processing serialized KG structures, featuring specialized relational attention and structural encoding to capture global dependencies \\cite{yang2023}.\n        *   **Local path mask prediction**: A unique mechanism that quantifies and highlights the saliency of meta-paths, providing explicit explanations for recommendations \\cite{yang2023}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments were performed to validate both the effectiveness (prediction accuracy) and interpretability (explainability) of CKGE \\cite{yang2023}.\n    *   **Key performance metrics and comparison results**:\n        *   Experiments were conducted on real-world and public datasets \\cite{yang2023}.\n        *   CKGE demonstrated superior performance in making precise predictions compared to state-of-the-art baselines \\cite{yang2023}.\n        *   The system effectively discriminated the saliencies of meta-paths, confirming its interpretability and ability to characterize user preferences \\cite{yang2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions**: The approach implicitly relies on the availability and quality of structured knowledge graph data to construct meaningful meta-graphs and define relevant meta-paths. The computational complexity associated with meta-graph construction and the KG-based Transformer might be a consideration for extremely large-scale KGs.\n    *   **Scope of applicability**: The primary focus is on personalized employee training course recommendation, particularly in scenarios where explainability and understanding user motivations are critical \\cite{yang2023}. The underlying KG embedding and Transformer principles could potentially be generalized to other explainable recommendation domains.\n\n7.  **Technical Significance**\n    *   **Advancement of state-of-the-art**: CKGE significantly advances the state-of-the-art in explainable recommender systems by explicitly integrating motivation-aware information through contextualized KG embeddings and a novel, adapted Transformer architecture \\cite{yang2023}.\n    *   **Potential impact on future research**:\n        *   Enhances the trustworthiness and adoption of recommender systems in Learning & Development by providing transparent, motivation-driven explanations \\cite{yang2023}.\n        *   Provides a robust framework for modeling complex, high-order relationships within KGs for more nuanced and personalized recommendations \\cite{yang2023}.\n        *   Opens new avenues for research into integrating diverse contextual information and advanced neural architectures for explainable AI in various recommendation tasks.",
      "intriguing_abstract": "Unlocking human potential in talent management hinges on personalized, transparent training. Yet, existing employee training recommender systems often lack explainability and fail to capture diverse learning motivations, hindering user trust and program effectiveness. We introduce CKGE (Contextualized Knowledge Graph Embedding), a novel framework designed to deliver explainable and motivation-aware training course recommendations.\n\nCKGE innovatively integrates contextualized neighbor semantics and high-order Knowledge Graph (KG) connections as \"motivation-aware information\" to learn robust talent and course representations. For each recommendation, a dynamic meta-graph is constructed, processed by a specialized KG-based Transformer featuring unique relational attention and structural encoding to model global dependencies. Crucially, a local path mask prediction mechanism quantifies meta-path saliency, providing explicit, interpretable explanations. Extensive experiments on real-world datasets demonstrate CKGE's superior prediction accuracy and confirmed interpretability, significantly advancing the state-of-the-art in explainable recommender systems and fostering greater trust and adoption in Learning & Development.",
      "keywords": [
        "Explainable recommendations",
        "Employee training course recommendation",
        "Learning motivations",
        "Contextualized Knowledge Graph Embedding (CKGE)",
        "Knowledge Graph (KG)",
        "Motivation-aware information integration",
        "Meta-graph construction",
        "KG-based Transformer",
        "Relational attention and structural encoding",
        "Local path mask prediction",
        "Superior prediction accuracy",
        "Interpretability"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/b1d807fc6b184d757ebdea67acd81132d8298ff6.pdf",
      "citation_key": "yang2023",
      "metadata": {
        "title": "Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation",
        "authors": [
          "Yang Yang",
          "Chubing Zhang",
          "Xin Song",
          "Zheng Dong",
          "Hengshu Zhu",
          "Wenjie Li"
        ],
        "published_date": "2023",
        "abstract": "Learning and development, or L&D, plays an important role in talent management, which aims to improve the knowledge and capabilities of employees through a variety of performance-oriented training activities. Recently, with the rapid development of enterprise management information systems, many research efforts and industrial practices have been devoted to building personalized employee training course recommender systems. Nevertheless, a widespread challenge is how to provide explainable recommendations with the consideration of different learning motivations from talents. To this end, we propose CKGE, a contextualized knowledge graph (KG) embedding approach for developing an explainable training course recommender system. A novel perspective of CKGE is to integrate both the contextualized neighbor semantics and high-order connections as motivation-aware information for learning effective representations of talents and courses. Specifically, in CKGE, for each entity pair (i.e., the talent-course pair), we first construct a meta-graph, including the neighbors of each entity and the meta-paths between entities as motivation-aware information. Then, we develop a novel KG-based Transformer, which can serialize entities and paths in the meta-graph as a sequential input, with the specially designed relational attention and structural encoding mechanisms to better model the global dependence of KG structured data. Meanwhile, the local path mask prediction can effectively reveal the importance of different paths. As a result, CKGE not only can make precise predictions but also can discriminate the saliencies of meta-paths in characterizing corresponding preferences. Extensive experiments on real-world and public datasets clearly validate the effectiveness and interpretability of CKGE compared with state-of-the-art baselines.",
        "file_path": "paper_data/knowledge_graph_embedding/b1d807fc6b184d757ebdea67acd81132d8298ff6.pdf",
        "venue": "ACM Trans. Inf. Syst.",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Providing explainable recommendations for employee training courses while effectively considering the diverse learning motivations of talents \\cite{yang2023}.\n    *   **Importance and challenge**: Personalized training is vital for talent management, but existing recommender systems often lack transparency (explainability) and fail to incorporate the underlying motivations of learners. This deficiency can hinder user trust, adoption, and the overall effectiveness of training programs \\cite{yang2023}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: The work builds upon existing efforts in personalized employee training course recommender systems \\cite{yang2023}.\n    *   **Limitations of previous solutions**: Previous solutions generally struggle with two key aspects: providing clear explanations for recommendations and adequately accounting for the different learning motivations of employees \\cite{yang2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes CKGE (Contextualized Knowledge Graph Embedding), an approach for developing an explainable training course recommender system \\cite{yang2023}.\n    *   **Novelty/Differentiation**:\n        *   **Motivation-aware information integration**: CKGE uniquely integrates both contextualized neighbor semantics and high-order connections within a Knowledge Graph (KG) as \"motivation-aware information\" to learn effective representations of talents and courses \\cite{yang2023}.\n        *   **Meta-graph construction**: For each talent-course pair, a specific meta-graph is constructed, incorporating entity neighbors and meta-paths to capture motivation-aware context \\cite{yang2023}.\n        *   **Novel KG-based Transformer**: A specialized Transformer architecture is developed to process the meta-graph. It serializes entities and paths from the meta-graph into a sequential input \\cite{yang2023}.\n        *   **Relational attention and structural encoding**: The KG-based Transformer incorporates specially designed relational attention and structural encoding mechanisms to effectively model the global dependencies inherent in KG structured data \\cite{yang2023}.\n        *   **Local path mask prediction**: This mechanism is introduced to reveal the importance (saliency) of different meta-paths, directly contributing to the explainability of recommendations \\cite{yang2023}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms/methods**:\n        *   **CKGE Framework**: A comprehensive contextualized knowledge graph embedding approach for explainable and motivation-aware training course recommendation \\cite{yang2023}.\n        *   **Meta-graph construction for motivation awareness**: A method to dynamically build context-rich meta-graphs for talent-course pairs, integrating neighbor semantics and high-order connections \\cite{yang2023}.\n        *   **KG-based Transformer**: A novel Transformer architecture tailored for processing serialized KG structures, featuring specialized relational attention and structural encoding to capture global dependencies \\cite{yang2023}.\n        *   **Local path mask prediction**: A unique mechanism that quantifies and highlights the saliency of meta-paths, providing explicit explanations for recommendations \\cite{yang2023}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments were performed to validate both the effectiveness (prediction accuracy) and interpretability (explainability) of CKGE \\cite{yang2023}.\n    *   **Key performance metrics and comparison results**:\n        *   Experiments were conducted on real-world and public datasets \\cite{yang2023}.\n        *   CKGE demonstrated superior performance in making precise predictions compared to state-of-the-art baselines \\cite{yang2023}.\n        *   The system effectively discriminated the saliencies of meta-paths, confirming its interpretability and ability to characterize user preferences \\cite{yang2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions**: The approach implicitly relies on the availability and quality of structured knowledge graph data to construct meaningful meta-graphs and define relevant meta-paths. The computational complexity associated with meta-graph construction and the KG-based Transformer might be a consideration for extremely large-scale KGs.\n    *   **Scope of applicability**: The primary focus is on personalized employee training course recommendation, particularly in scenarios where explainability and understanding user motivations are critical \\cite{yang2023}. The underlying KG embedding and Transformer principles could potentially be generalized to other explainable recommendation domains.\n\n7.  **Technical Significance**\n    *   **Advancement of state-of-the-art**: CKGE significantly advances the state-of-the-art in explainable recommender systems by explicitly integrating motivation-aware information through contextualized KG embeddings and a novel, adapted Transformer architecture \\cite{yang2023}.\n    *   **Potential impact on future research**:\n        *   Enhances the trustworthiness and adoption of recommender systems in Learning & Development by providing transparent, motivation-driven explanations \\cite{yang2023}.\n        *   Provides a robust framework for modeling complex, high-order relationships within KGs for more nuanced and personalized recommendations \\cite{yang2023}.\n        *   Opens new avenues for research into integrating diverse contextual information and advanced neural architectures for explainable AI in various recommendation tasks.",
        "keywords": [
          "Explainable recommendations",
          "Employee training course recommendation",
          "Learning motivations",
          "Contextualized Knowledge Graph Embedding (CKGE)",
          "Knowledge Graph (KG)",
          "Motivation-aware information integration",
          "Meta-graph construction",
          "KG-based Transformer",
          "Relational attention and structural encoding",
          "Local path mask prediction",
          "Superior prediction accuracy",
          "Interpretability"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "b1d807fc6b184d757ebdea67acd81132d8298ff6.pdf"
    },
    {
      "success": true,
      "doc_id": "0c920d83aaa8adf4f0513eaa1571d334",
      "summary": "Here's a focused summary of the paper \"Locally Adaptive Translation for Knowledge Graph Embedding\" by Jia et al. \\cite{jia2015} for a literature review:\n\n---\n\n### Technical Paper Analysis: Locally Adaptive Translation for Knowledge Graph Embedding \\cite{jia2015}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) methods (e.g., TransE, TransH) rely on global, margin-based loss functions where the optimal margin is determined experimentally from a limited, pre-defined set of candidates. This approach fails to account for the \"locality\" of different knowledge graphs or even different parts within a single graph, which possess distinct entities and relations.\n    *   **Importance & Challenge**: Knowledge graphs are vast and heterogeneous. A one-size-fits-all global margin for the loss function leads to suboptimal embedding representations, limiting the performance of downstream applications like link prediction and triple classification. The challenge lies in adaptively determining an optimal margin that reflects the unique structural properties (locality) of different knowledge graphs or their subgraphs, without resorting to exhaustive grid search over an infinite parameter space.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon translation-based KGE methods like TransE \\cite{jia2015} (entities as points, relations as translations) and TransH \\cite{jia2015} (relations as translations on hyperplanes). It also acknowledges other embedding paradigms such as energy-based methods (e.g., SE, SLM, SME, LFM, NTN) and matrix factorization methods (e.g., RESCAL).\n    *   **Limitations of Previous Solutions**:\n        *   **Global Margin Determination**: Previous methods determine the optimal loss function (specifically, its margin) through experiments, selecting from a small, closed set of candidates (e.g., {1, 2, 10} for TransE on Freebase). This process is arbitrary and lacks theoretical justification for the chosen candidate set.\n        *   **Ignoring Locality**: Different knowledge graphs, or even different subsets of a single graph, have distinct entities and relations, exhibiting different \"localities.\" Existing methods apply the same set of candidate margins across these diverse graphs, ignoring their individual characteristics and leading to suboptimal performance.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes TransA (Locally Adaptive Translation), a method that adaptively determines the optimal margin for the loss function based on the specific structure and locality of the knowledge graph. Instead of a global, fixed margin, TransA calculates a dynamic `Mopt` (optimal margin) for each knowledge graph. This `Mopt` is a linear combination of an entity-specific margin (`Ment`) and a relation-specific margin (`Mrel`), weighted by a parameter ``.\n    *   **Novelty/Difference**:\n        *   **Adaptive Margin Calculation**: Unlike prior methods that rely on pre-defined global margins, TransA computes margins adaptively, eliminating the need for a closed set of candidates.\n        *   **Locality-Sensitive Margins**: It introduces `Ment` and `Mrel` to capture the local characteristics of entities and relations, respectively, thereby making the embedding process sensitive to the graph's structure.\n        *   **Theoretical Justification**: The paper provides a theoretical analysis (Theorem 1) demonstrating the relationship between margin size and generalization error, showing that large margins can lead to overfitting and motivating the need for an appropriately chosen margin.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Insight**: Experimentally proves that knowledge graphs with different localities correspond to different optimal loss functions with varying margins. It further derives a theoretical relation (Theorem 1) between the margin and the performance error, indicating that a large margin can lead to overfitting.\n    *   **Novel Algorithms/Methods**:\n        *   **Locally Adaptive Translation (TransA)**: A new KGE method that adaptively determines the optimal margin for its loss function.\n        *   **Entity-Specific Margin (`Ment`)**: Defined as the average minimum distance between negative entities and positive entities relative to a specific head/tail entity, inspired by metric learning. It aims to push negative entities away while keeping positive ones close.\n        *   **Relation-Specific Margin (`Mrel`)**: Defined based on the proximity of relation embedding vector lengths concerning a given entity, pushing dissimilar relations away from a target relation.\n        *   **Combined Optimal Margin (`Mopt`)**: A weighted linear combination of `Ment` and `Mrel` (`Mopt =  * Ment + (1 - ) * Mrel`).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Validation of margin sensitivity: TransE was run on partitioned subsets of FB15K to show that different subsets require different optimal margins (Table 1).\n        *   Performance evaluation of TransA: Compared TransA against state-of-the-art methods on standard KGE tasks.\n    *   **Key Performance Metrics**:\n        *   Mean Rank (of correct entities in link prediction).\n        *   Hits@10 (percentage of correct entities ranked within the top 10).\n    *   **Comparison Results**: Experiments on two benchmark datasets (FB15K and another unnamed dataset, likely WordNet based on abstract/related work) demonstrate the \"superiority\" and \"effectiveness and efficiency\" of TransA compared to state-of-the-art methods (e.g., TransE, TransH). For instance, Table 1 shows that optimal margins for subsets of FB15K (e.g., Subset1, Subset2) are 3 and 2 respectively, while the whole FB15K dataset performs best with a margin of 1, validating the need for adaptive margins.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The paper does not explicitly state limitations of TransA itself, but rather addresses the limitations of prior methods.\n        *   The definitions of `Ment` and `Mrel` rely on specific distance metrics and assumptions about how \"positive\" and \"negative\" entities/relations should be separated.\n        *   The calculation of `Ment` can be computationally intensive, which is mitigated by adopting an \"active set method\" for speed-up, implying a potential efficiency concern without this optimization.\n    *   **Scope of Applicability**: TransA is primarily designed for translation-based KGE models. Its core idea of adaptive margins could potentially be extended to other KGE paradigms, but the specific definitions of `Ment` and `Mrel` are tailored to the translation model's geometric interpretation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: TransA significantly advances KGE by introducing the concept of locally adaptive margins, moving beyond the limitations of globally fixed or arbitrarily chosen margins. This makes KGE models more robust and better suited for the inherent heterogeneity of knowledge graphs.\n    *   **Potential Impact on Future Research**:\n        *   **Improved KGE Performance**: By providing a principled way to determine optimal margins, TransA can lead to more accurate and robust knowledge graph embeddings, enhancing performance in applications like link prediction, triple classification, and knowledge graph completion.\n        *   **Foundation for Adaptive Learning**: The idea of adaptively learning hyper-parameters (like margins) based on data locality could inspire similar adaptive mechanisms in other machine learning tasks, especially those dealing with heterogeneous or structured data.\n        *   **Deeper Understanding of KGE Loss Functions**: The theoretical analysis of margin's effect on performance provides valuable insights into the design of loss functions for KGE.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) models often grapple with a fundamental limitation: their reliance on globally fixed or arbitrarily chosen margins within loss functions. This conventional approach critically overlooks the inherent heterogeneity and distinct \"locality\" of diverse knowledge graphs and their subgraphs, leading to suboptimal embedding quality and hindering performance in vital downstream tasks like link prediction and triple classification.\n\nWe introduce **TransA (Locally Adaptive Translation)**, a novel KGE method that revolutionizes margin determination. Instead of a one-size-fits-all parameter, TransA adaptively computes an optimal margin (`Mopt`) for each knowledge graph by dynamically combining entity-specific (`Ment`) and relation-specific (`Mrel`) margins. This innovation not only eliminates the need for exhaustive grid searches but also provides theoretical justification, demonstrating the critical relationship between margin size and generalization error, thereby mitigating overfitting. Extensive experiments on benchmark datasets like FB15K and WordNet showcase TransA's superior effectiveness and efficiency in tasks like link prediction and triple classification, significantly outperforming state-of-the-art translation-based models such as TransE and TransH. Our work establishes a new paradigm for learning robust, locality-sensitive knowledge graph embeddings, paving the way for more adaptive knowledge representation learning.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Locally Adaptive Translation (TransA)",
        "adaptive margin",
        "loss function margin",
        "knowledge graph locality",
        "entity-specific margin (Ment)",
        "relation-specific margin (Mrel)",
        "translation-based KGE",
        "link prediction",
        "overfitting",
        "hyperparameter adaptation",
        "heterogeneous knowledge graphs",
        "theoretical justification"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/abea782b5d0bdb4cd90ec42f672711613e71e43e.pdf",
      "citation_key": "jia2015",
      "metadata": {
        "title": "Locally Adaptive Translation for Knowledge Graph Embedding",
        "authors": [
          "Yantao Jia",
          "Yuanzhuo Wang",
          "Hailun Lin",
          "Xiaolong Jin",
          "Xueqi Cheng"
        ],
        "published_date": "2015",
        "abstract": "\n \n Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, e.g., TransE and TransH, learn embedding representation by defining a global margin-based loss function over the data. However, the optimal loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidate loss functions, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this paper, we propose a locally adaptive translation method for knowledge graph embedding, called TransA, to find the optimal loss function by adaptively determining its margin over different knowledge graphs. Experiments on two benchmark data sets demonstrate the superiority of the proposed method, as compared to the-state-of-the-art ones.\n \n",
        "file_path": "paper_data/knowledge_graph_embedding/abea782b5d0bdb4cd90ec42f672711613e71e43e.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Locally Adaptive Translation for Knowledge Graph Embedding\" by Jia et al. \\cite{jia2015} for a literature review:\n\n---\n\n### Technical Paper Analysis: Locally Adaptive Translation for Knowledge Graph Embedding \\cite{jia2015}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) methods (e.g., TransE, TransH) rely on global, margin-based loss functions where the optimal margin is determined experimentally from a limited, pre-defined set of candidates. This approach fails to account for the \"locality\" of different knowledge graphs or even different parts within a single graph, which possess distinct entities and relations.\n    *   **Importance & Challenge**: Knowledge graphs are vast and heterogeneous. A one-size-fits-all global margin for the loss function leads to suboptimal embedding representations, limiting the performance of downstream applications like link prediction and triple classification. The challenge lies in adaptively determining an optimal margin that reflects the unique structural properties (locality) of different knowledge graphs or their subgraphs, without resorting to exhaustive grid search over an infinite parameter space.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon translation-based KGE methods like TransE \\cite{jia2015} (entities as points, relations as translations) and TransH \\cite{jia2015} (relations as translations on hyperplanes). It also acknowledges other embedding paradigms such as energy-based methods (e.g., SE, SLM, SME, LFM, NTN) and matrix factorization methods (e.g., RESCAL).\n    *   **Limitations of Previous Solutions**:\n        *   **Global Margin Determination**: Previous methods determine the optimal loss function (specifically, its margin) through experiments, selecting from a small, closed set of candidates (e.g., {1, 2, 10} for TransE on Freebase). This process is arbitrary and lacks theoretical justification for the chosen candidate set.\n        *   **Ignoring Locality**: Different knowledge graphs, or even different subsets of a single graph, have distinct entities and relations, exhibiting different \"localities.\" Existing methods apply the same set of candidate margins across these diverse graphs, ignoring their individual characteristics and leading to suboptimal performance.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes TransA (Locally Adaptive Translation), a method that adaptively determines the optimal margin for the loss function based on the specific structure and locality of the knowledge graph. Instead of a global, fixed margin, TransA calculates a dynamic `Mopt` (optimal margin) for each knowledge graph. This `Mopt` is a linear combination of an entity-specific margin (`Ment`) and a relation-specific margin (`Mrel`), weighted by a parameter ``.\n    *   **Novelty/Difference**:\n        *   **Adaptive Margin Calculation**: Unlike prior methods that rely on pre-defined global margins, TransA computes margins adaptively, eliminating the need for a closed set of candidates.\n        *   **Locality-Sensitive Margins**: It introduces `Ment` and `Mrel` to capture the local characteristics of entities and relations, respectively, thereby making the embedding process sensitive to the graph's structure.\n        *   **Theoretical Justification**: The paper provides a theoretical analysis (Theorem 1) demonstrating the relationship between margin size and generalization error, showing that large margins can lead to overfitting and motivating the need for an appropriately chosen margin.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Insight**: Experimentally proves that knowledge graphs with different localities correspond to different optimal loss functions with varying margins. It further derives a theoretical relation (Theorem 1) between the margin and the performance error, indicating that a large margin can lead to overfitting.\n    *   **Novel Algorithms/Methods**:\n        *   **Locally Adaptive Translation (TransA)**: A new KGE method that adaptively determines the optimal margin for its loss function.\n        *   **Entity-Specific Margin (`Ment`)**: Defined as the average minimum distance between negative entities and positive entities relative to a specific head/tail entity, inspired by metric learning. It aims to push negative entities away while keeping positive ones close.\n        *   **Relation-Specific Margin (`Mrel`)**: Defined based on the proximity of relation embedding vector lengths concerning a given entity, pushing dissimilar relations away from a target relation.\n        *   **Combined Optimal Margin (`Mopt`)**: A weighted linear combination of `Ment` and `Mrel` (`Mopt =  * Ment + (1 - ) * Mrel`).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Validation of margin sensitivity: TransE was run on partitioned subsets of FB15K to show that different subsets require different optimal margins (Table 1).\n        *   Performance evaluation of TransA: Compared TransA against state-of-the-art methods on standard KGE tasks.\n    *   **Key Performance Metrics**:\n        *   Mean Rank (of correct entities in link prediction).\n        *   Hits@10 (percentage of correct entities ranked within the top 10).\n    *   **Comparison Results**: Experiments on two benchmark datasets (FB15K and another unnamed dataset, likely WordNet based on abstract/related work) demonstrate the \"superiority\" and \"effectiveness and efficiency\" of TransA compared to state-of-the-art methods (e.g., TransE, TransH). For instance, Table 1 shows that optimal margins for subsets of FB15K (e.g., Subset1, Subset2) are 3 and 2 respectively, while the whole FB15K dataset performs best with a margin of 1, validating the need for adaptive margins.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The paper does not explicitly state limitations of TransA itself, but rather addresses the limitations of prior methods.\n        *   The definitions of `Ment` and `Mrel` rely on specific distance metrics and assumptions about how \"positive\" and \"negative\" entities/relations should be separated.\n        *   The calculation of `Ment` can be computationally intensive, which is mitigated by adopting an \"active set method\" for speed-up, implying a potential efficiency concern without this optimization.\n    *   **Scope of Applicability**: TransA is primarily designed for translation-based KGE models. Its core idea of adaptive margins could potentially be extended to other KGE paradigms, but the specific definitions of `Ment` and `Mrel` are tailored to the translation model's geometric interpretation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: TransA significantly advances KGE by introducing the concept of locally adaptive margins, moving beyond the limitations of globally fixed or arbitrarily chosen margins. This makes KGE models more robust and better suited for the inherent heterogeneity of knowledge graphs.\n    *   **Potential Impact on Future Research**:\n        *   **Improved KGE Performance**: By providing a principled way to determine optimal margins, TransA can lead to more accurate and robust knowledge graph embeddings, enhancing performance in applications like link prediction, triple classification, and knowledge graph completion.\n        *   **Foundation for Adaptive Learning**: The idea of adaptively learning hyper-parameters (like margins) based on data locality could inspire similar adaptive mechanisms in other machine learning tasks, especially those dealing with heterogeneous or structured data.\n        *   **Deeper Understanding of KGE Loss Functions**: The theoretical analysis of margin's effect on performance provides valuable insights into the design of loss functions for KGE.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Locally Adaptive Translation (TransA)",
          "adaptive margin",
          "loss function margin",
          "knowledge graph locality",
          "entity-specific margin (Ment)",
          "relation-specific margin (Mrel)",
          "translation-based KGE",
          "link prediction",
          "overfitting",
          "hyperparameter adaptation",
          "heterogeneous knowledge graphs",
          "theoretical justification"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "abea782b5d0bdb4cd90ec42f672711613e71e43e.pdf"
    },
    {
      "success": true,
      "doc_id": "bf39c50c6a1aa53b403ddb09ccdc8a92",
      "summary": "Here is a focused summary of the empirical study by \\cite{lloyd2022} for a literature review:\n\n1.  **Research Questions & Hypotheses**\n    This study empirically investigates the relative importance of hyperparameters in knowledge graph embedding (KGE) quality. It aims to determine how tuning different hyperparameters affects the variance of embedding quality and if this importance varies across different knowledge graphs. The implicit hypothesis is that not all hyperparameters contribute equally to embedding quality, and their impact is dataset-dependent.\n\n2.  **Study Design & Methodology**\n    The study employed an experimental design involving tens of thousands of KGE trials across various datasets and methods. Data collection involved running 13,450 embedding trials with hyperparameter values chosen by Sobol sequence. A Sobol sensitivity analysis was then performed on linear regression models, which regressed embedding quality (measured by MRR) on hyperparameter configurations.\n\n3.  **Data & Participants**\n    The study utilized three well-known benchmark knowledge graphs: FB15k-237, UMLS, and WN18RR, which vary in node count, edge count, density, and other structural characteristics. A total of 13,450 embedding trials were successfully completed across 167 valid jobs, using various KGE methods available in the LibKGE library.\n\n4.  **Key Empirical Findings**\n    *   There is substantial variability in hyperparameter sensitivities between knowledge graphs, suggesting that optimal tuning strategies are dataset-specific.\n    *   Differing dataset characteristics, such as graph density and node degree distribution, are identified as probable causes for these inconsistencies in hyperparameter importance.\n    *   The UMLS dataset consistently yielded higher Mean Reciprocal Rank (MRR) scores compared to FB15k-237 and WN18RR, indicating it presents an \"easier\" link prediction problem.\n    *   The authors identified several inverse relations in the UMLS knowledge graph (e.g., 'degree_of', 'precedes', 'derivative_of') that could cause data leakage, leading to the derivation of UMLS-43, a leakage-robust variant.\n\n5.  **Statistical Analysis**\n    For each knowledge graph, the top 5% of trials by embedding quality had their MRR scores regressed on corresponding hyperparameter configurations using linear regression. A Sobol sensitivity analysis was then applied to these models to calculate first-order, second-order, and total-order Sobol indices, indicating the proportion of output variance attributable to individual inputs or their interactions. Pairwise Pearson's correlation was used to quantify agreement between sensitivities across datasets.\n\n6.  **Validity & Limitations**\n    A limitation is that the MRR metric used for embedding quality does not allow for direct comparisons of quality *between* different knowledge graphs, only within them. Additionally, some KGE methods (e.g., RotatE, TransH) failed to complete trials on larger datasets, potentially due to computational demands or implementation inefficiencies.\n\n7.  **Empirical Contribution**\n    This study provides novel empirical evidence on the varying importance of KGE hyperparameters across diverse knowledge graphs, which can inform more efficient and targeted hyperparameter tuning strategies. It also contributes UMLS-43, a new leakage-robust variant of the UMLS knowledge graph, enhancing its utility for future research.",
      "intriguing_abstract": "Navigating the complex landscape of Knowledge Graph Embedding (KGE) hyperparameter tuning is a persistent challenge, often relying on resource-intensive trial-and-error. This study presents a groundbreaking empirical investigation into the true impact of KGE hyperparameters, revealing that their relative importance is *not universal* but profoundly *dataset-dependent*.\n\nThrough tens of thousands of KGE trials and rigorous Sobol sensitivity analysis on Mean Reciprocal Rank (MRR) scores across benchmark datasets (FB15k-237, UMLS, WN18RR), we precisely quantify how hyperparameter influence varies. Our findings attribute these critical inconsistencies to distinct graph characteristics like density and node degree distribution, fundamentally challenging conventional tuning wisdom. Moreover, we identify and mitigate data leakage in the UMLS dataset, introducing UMLS-43, a novel, leakage-robust variant that significantly enhances its utility for future link prediction research. This work shifts the paradigm from 'one-size-fits-all' tuning to a more informed, dataset-specific approach, offering invaluable guidance for researchers to achieve superior KGE quality with unprecedented efficiency and rigor.",
      "keywords": [
        "knowledge graph embedding (KGE)",
        "hyperparameter importance",
        "embedding quality",
        "Sobol sensitivity analysis",
        "dataset-specific tuning strategies",
        "Mean Reciprocal Rank (MRR)",
        "knowledge graph characteristics",
        "data leakage",
        "UMLS-43 leakage-robust variant",
        "hyperparameter sensitivities variability",
        "empirical evidence",
        "link prediction"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/658702b2fa647ae7eaf1255058105da9eefe6f52.pdf",
      "citation_key": "lloyd2022",
      "metadata": {
        "title": "Assessing the effects of hyperparameters on knowledge graph embedding quality",
        "authors": [
          "Oliver Lloyd",
          "Yi Liu",
          "T. Gaunt"
        ],
        "published_date": "2022",
        "abstract": "Embedding knowledge graphs into low-dimensional spaces is a popular method for applying approaches, such as link prediction or node classification, to these databases. This embedding process is very costly in terms of both computational time and space. Part of the reason for this is the optimisation of hyperparameters, which involves repeatedly sampling, by random, guided, or brute-force selection, from a large hyperparameter space and testing the resulting embeddings for their quality. However, not all hyperparameters in this search space will be equally important. In fact, with prior knowledge of the relative importance of the hyperparameters, some could be eliminated from the search altogether without significantly impacting the overall quality of the outputted embeddings. To this end, we ran a Sobol sensitivity analysis to evaluate the effects of tuning different hyperparameters on the variance of embedding quality. This was achieved by performing thousands of embedding trials, each time measuring the quality of embeddings produced by different hyperparameter configurations. We regressed the embedding quality on those hyperparameter configurations, using this model to generate Sobol sensitivity indices for each of the hyperparameters. By evaluating the correlation between Sobol indices, we find substantial variability in the hyperparameter sensitivities between knowledge graphs with differing dataset characteristics as the probable cause of these inconsistencies. As an additional contribution of this work we identify several relations in the UMLS knowledge graph that may cause data leakage via inverse relations, and derive and present UMLS-43, a leakage-robust variant of that graph.",
        "file_path": "paper_data/knowledge_graph_embedding/658702b2fa647ae7eaf1255058105da9eefe6f52.pdf",
        "venue": "Journal of Big Data",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the empirical study by \\cite{lloyd2022} for a literature review:\n\n1.  **Research Questions & Hypotheses**\n    This study empirically investigates the relative importance of hyperparameters in knowledge graph embedding (KGE) quality. It aims to determine how tuning different hyperparameters affects the variance of embedding quality and if this importance varies across different knowledge graphs. The implicit hypothesis is that not all hyperparameters contribute equally to embedding quality, and their impact is dataset-dependent.\n\n2.  **Study Design & Methodology**\n    The study employed an experimental design involving tens of thousands of KGE trials across various datasets and methods. Data collection involved running 13,450 embedding trials with hyperparameter values chosen by Sobol sequence. A Sobol sensitivity analysis was then performed on linear regression models, which regressed embedding quality (measured by MRR) on hyperparameter configurations.\n\n3.  **Data & Participants**\n    The study utilized three well-known benchmark knowledge graphs: FB15k-237, UMLS, and WN18RR, which vary in node count, edge count, density, and other structural characteristics. A total of 13,450 embedding trials were successfully completed across 167 valid jobs, using various KGE methods available in the LibKGE library.\n\n4.  **Key Empirical Findings**\n    *   There is substantial variability in hyperparameter sensitivities between knowledge graphs, suggesting that optimal tuning strategies are dataset-specific.\n    *   Differing dataset characteristics, such as graph density and node degree distribution, are identified as probable causes for these inconsistencies in hyperparameter importance.\n    *   The UMLS dataset consistently yielded higher Mean Reciprocal Rank (MRR) scores compared to FB15k-237 and WN18RR, indicating it presents an \"easier\" link prediction problem.\n    *   The authors identified several inverse relations in the UMLS knowledge graph (e.g., 'degree_of', 'precedes', 'derivative_of') that could cause data leakage, leading to the derivation of UMLS-43, a leakage-robust variant.\n\n5.  **Statistical Analysis**\n    For each knowledge graph, the top 5% of trials by embedding quality had their MRR scores regressed on corresponding hyperparameter configurations using linear regression. A Sobol sensitivity analysis was then applied to these models to calculate first-order, second-order, and total-order Sobol indices, indicating the proportion of output variance attributable to individual inputs or their interactions. Pairwise Pearson's correlation was used to quantify agreement between sensitivities across datasets.\n\n6.  **Validity & Limitations**\n    A limitation is that the MRR metric used for embedding quality does not allow for direct comparisons of quality *between* different knowledge graphs, only within them. Additionally, some KGE methods (e.g., RotatE, TransH) failed to complete trials on larger datasets, potentially due to computational demands or implementation inefficiencies.\n\n7.  **Empirical Contribution**\n    This study provides novel empirical evidence on the varying importance of KGE hyperparameters across diverse knowledge graphs, which can inform more efficient and targeted hyperparameter tuning strategies. It also contributes UMLS-43, a new leakage-robust variant of the UMLS knowledge graph, enhancing its utility for future research.",
        "keywords": [
          "knowledge graph embedding (KGE)",
          "hyperparameter importance",
          "embedding quality",
          "Sobol sensitivity analysis",
          "dataset-specific tuning strategies",
          "Mean Reciprocal Rank (MRR)",
          "knowledge graph characteristics",
          "data leakage",
          "UMLS-43 leakage-robust variant",
          "hyperparameter sensitivities variability",
          "empirical evidence",
          "link prediction"
        ],
        "is_new_direction": "0",
        "paper_type": "empirical"
      },
      "file_name": "658702b2fa647ae7eaf1255058105da9eefe6f52.pdf"
    },
    {
      "success": true,
      "doc_id": "6fd2a6cf7824dc703daeeb02ee34ae89",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network \\cite{wu2021}\n\n1.  **Research Problem & Motivation**\n    *   Existing Knowledge Graph Completion (KGC) models primarily rely on single and static entity/relation representations, which are insufficient for accurately capturing complex relations (e.g., one-to-many, many-to-one, many-to-many).\n    *   This limitation leads to: (i) an inability to effectively model critical relationships in specific scenarios where entities exhibit distinct meanings in different contexts; (ii) a failure to account for the entanglement of latent factors, as an entity often possesses multiple aspects, and various relations focus on distinct facets; and (iii) reduced interpretability and robustness, as models may overreact to irrelevant neighboring information.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** This work builds upon advancements in knowledge graph embedding (translational, bilinear, CNN-based) and Graph Neural Network (GNN) based KGC models. It also draws inspiration from disentangled representation learning applied in other domains (e.g., text, images, homogeneous graphs).\n    *   **Limitations of previous solutions:**\n        *   Even GNN-based KGC models learn *static representations*, which inherently limit their flexibility and expressiveness when dealing with complex relation types.\n        *   While some methods (e.g., relation-specific projections, Transformers) attempt to capture dynamic representations, there has been little formal discussion or dedicated work on *disentangled representation learning* specifically for investigating latent factors within knowledge graphs.\n        *   Previous disentangled graph learning efforts (e.g., DisenGCN) often focused on homogeneous networks and sometimes lacked explicit mechanisms for *macro-separability* (ensuring independence between learned components), which is crucial for complex KGs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** DisenKGAT (Disentangled Knowledge Graph Attention Network) is an end-to-end deep model designed to learn disentangled entity representations by incorporating both *micro-disentanglement* and *macro-disentanglement* \\cite{wu2021}.\n    *   **Micro-disentanglement:** Achieved through a novel *relation-aware aggregation* mechanism. For each entity and its `k`-th component, the model dynamically identifies and aggregates information from relevant neighbors based on the specific relation. This involves component-level interaction (`phi` operator, which can be subtraction, multiplication, cross interaction, or circular-correlation) and a relation-aware attention mechanism to weigh neighbor contributions.\n    *   **Macro-disentanglement:** Ensured by introducing *mutual information (MI) regularization*. This regularization term is applied to enhance the independence between different learned components of an entity, preventing them from becoming entangled.\n    *   **Adaptive Scoring:** The final prediction for KGC adaptively combines the results from each disentangled component based on the given relation (scenario), allowing for context-specific predictions.\n    *   **Novelty:** DisenKGAT represents the first attempt to explicitly leverage disentangled representation learning in the context of knowledge graph completion, providing adaptive, robust, and interpretable entity embeddings \\cite{wu2021}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Proposed DisenKGAT, a novel Disentangled Knowledge Graph Attention Network, for learning disentangled embeddings in KGs, designed to be generalizable to various score functions \\cite{wu2021}.\n        *   Introduced a *relation-aware aggregation mechanism* for micro-disentanglement, which semantically aggregates neighborhood information while maintaining consistency with the adaptive scoring part of the model.\n        *   Developed a *mutual information-based regularization* technique for macro-disentanglement, effectively reducing intra-component correlation and promoting independence among components.\n    *   **System Design/Architectural Innovations:** The overall architecture integrates disentangled transformation, relation-aware aggregation, independence constraints, and adaptive scoring into a cohesive framework.\n    *   **Theoretical Insights/Analysis:** Formalizes the application of micro- and macro-disentanglement concepts to knowledge graphs, addressing the multi-faceted nature of entities and the context-dependency of relations in KGC.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were performed on public benchmark datasets to evaluate the model's effectiveness \\cite{wu2021}.\n    *   **Key performance metrics and comparison results:** DisenKGAT demonstrated superiority over existing state-of-the-art methods in terms of both *accuracy* and *explainability*. The experiments also validated the model's strong *robustness*.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions:** The model assumes that entities can be effectively decomposed into a predefined number of `K` independent components. The effectiveness of the mutual information regularization relies on its ability to accurately enforce this independence. The choice of `K` is a hyperparameter that needs tuning.\n    *   **Scope of applicability:** The primary focus is on Knowledge Graph Completion (KGC), specifically predicting missing entities in `(h, r, ?)` triplets. The proposed encoder model is designed with flexibility to be potentially generalized to work with various existing score functions.\n\n7.  **Technical Significance**\n    *   **Advancement of state-of-the-art:** DisenKGAT significantly advances the state-of-the-art in KGC by moving beyond static entity representations to adaptive, disentangled ones. This approach better captures the complex, multi-faceted nature of entities and the context-dependency of relations, leading to more accurate and nuanced predictions \\cite{wu2021}.\n    *   **Potential impact on future research:** This work opens new avenues for research into disentangled representation learning within complex, heterogeneous graph structures like KGs. It provides a strong foundation for developing more interpretable, robust, and context-aware KGC models, and could inspire similar dynamic embedding approaches for other graph-based tasks.",
      "intriguing_abstract": "The intricate tapestry of Knowledge Graphs (KGs) often hides a critical challenge: existing Knowledge Graph Completion (KGC) models struggle to capture the multi-faceted nature of entities and the context-dependency of complex relations due to their reliance on static, entangled representations. This limitation severely hinders interpretability and predictive accuracy.\n\nWe introduce DisenKGAT, a novel Disentangled Knowledge Graph Attention Network, that revolutionizes KGC by learning adaptive, disentangled entity representations. DisenKGAT achieves *micro-disentanglement* through a sophisticated *relation-aware aggregation* mechanism, dynamically identifying and combining relevant neighborhood information for each latent component. Crucially, *mutual information regularization* ensures *macro-disentanglement*, promoting independence among these learned facets. This framework enables *adaptive scoring*, tailoring predictions to specific relational contexts. Extensive experiments demonstrate DisenKGAT's superior accuracy, enhanced interpretability, and remarkable robustness over state-of-the-art methods. By moving beyond static *entity embeddings*, DisenKGAT not only significantly advances *Knowledge Graph Completion* but also paves the way for more nuanced and context-aware *disentangled representation learning* in complex graph structures.",
      "keywords": [
        "DisenKGAT",
        "Knowledge Graph Completion (KGC)",
        "Disentangled representation learning",
        "Knowledge Graph Embedding",
        "Graph Attention Network",
        "Micro-disentanglement",
        "Macro-disentanglement",
        "Relation-aware aggregation",
        "Mutual information regularization",
        "Adaptive scoring",
        "Complex relations",
        "Interpretability",
        "Robustness",
        "State-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/29eb99518d16ccf8ac306d92f4a6377ae109d9be.pdf",
      "citation_key": "wu2021",
      "metadata": {
        "title": "DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network",
        "authors": [
          "Junkang Wu",
          "Wentao Shi",
          "Xuezhi Cao",
          "Jiawei Chen",
          "Wenqiang Lei",
          "Fuzheng Zhang",
          "Wei Wu",
          "Xiangnan He"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph completion (KGC) has become a focus of attention across deep learning community owing to its excellent contribution to numerous downstream tasks. Although recently have witnessed a surge of work on KGC, they are still insufficient to accurately capture complex relations, since they adopt the single and static representations. In this work, we propose a novel Disentangled Knowledge Graph Attention Network (DisenKGAT) for KGC, which leverages both micro-disentanglement and macro-disentanglement to exploit representations behind Knowledge graphs (KGs). To achieve micro-disentanglement, we put forward a novel relation-aware aggregation to learn diverse component representation. For macro-disentanglement, we leverage mutual information as a regularization to enhance independence. With the assistance of disentanglement, our model is able to generate adaptive representations in terms of the given scenario. Besides, our work has strong robustness and flexibility to adapt to various score functions. Extensive experiments on public benchmark datasets have been conducted to validate the superiority of DisenKGAT over existing methods in terms of both accuracy and explainability.",
        "file_path": "paper_data/knowledge_graph_embedding/29eb99518d16ccf8ac306d92f4a6377ae109d9be.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network \\cite{wu2021}\n\n1.  **Research Problem & Motivation**\n    *   Existing Knowledge Graph Completion (KGC) models primarily rely on single and static entity/relation representations, which are insufficient for accurately capturing complex relations (e.g., one-to-many, many-to-one, many-to-many).\n    *   This limitation leads to: (i) an inability to effectively model critical relationships in specific scenarios where entities exhibit distinct meanings in different contexts; (ii) a failure to account for the entanglement of latent factors, as an entity often possesses multiple aspects, and various relations focus on distinct facets; and (iii) reduced interpretability and robustness, as models may overreact to irrelevant neighboring information.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** This work builds upon advancements in knowledge graph embedding (translational, bilinear, CNN-based) and Graph Neural Network (GNN) based KGC models. It also draws inspiration from disentangled representation learning applied in other domains (e.g., text, images, homogeneous graphs).\n    *   **Limitations of previous solutions:**\n        *   Even GNN-based KGC models learn *static representations*, which inherently limit their flexibility and expressiveness when dealing with complex relation types.\n        *   While some methods (e.g., relation-specific projections, Transformers) attempt to capture dynamic representations, there has been little formal discussion or dedicated work on *disentangled representation learning* specifically for investigating latent factors within knowledge graphs.\n        *   Previous disentangled graph learning efforts (e.g., DisenGCN) often focused on homogeneous networks and sometimes lacked explicit mechanisms for *macro-separability* (ensuring independence between learned components), which is crucial for complex KGs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** DisenKGAT (Disentangled Knowledge Graph Attention Network) is an end-to-end deep model designed to learn disentangled entity representations by incorporating both *micro-disentanglement* and *macro-disentanglement* \\cite{wu2021}.\n    *   **Micro-disentanglement:** Achieved through a novel *relation-aware aggregation* mechanism. For each entity and its `k`-th component, the model dynamically identifies and aggregates information from relevant neighbors based on the specific relation. This involves component-level interaction (`phi` operator, which can be subtraction, multiplication, cross interaction, or circular-correlation) and a relation-aware attention mechanism to weigh neighbor contributions.\n    *   **Macro-disentanglement:** Ensured by introducing *mutual information (MI) regularization*. This regularization term is applied to enhance the independence between different learned components of an entity, preventing them from becoming entangled.\n    *   **Adaptive Scoring:** The final prediction for KGC adaptively combines the results from each disentangled component based on the given relation (scenario), allowing for context-specific predictions.\n    *   **Novelty:** DisenKGAT represents the first attempt to explicitly leverage disentangled representation learning in the context of knowledge graph completion, providing adaptive, robust, and interpretable entity embeddings \\cite{wu2021}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Proposed DisenKGAT, a novel Disentangled Knowledge Graph Attention Network, for learning disentangled embeddings in KGs, designed to be generalizable to various score functions \\cite{wu2021}.\n        *   Introduced a *relation-aware aggregation mechanism* for micro-disentanglement, which semantically aggregates neighborhood information while maintaining consistency with the adaptive scoring part of the model.\n        *   Developed a *mutual information-based regularization* technique for macro-disentanglement, effectively reducing intra-component correlation and promoting independence among components.\n    *   **System Design/Architectural Innovations:** The overall architecture integrates disentangled transformation, relation-aware aggregation, independence constraints, and adaptive scoring into a cohesive framework.\n    *   **Theoretical Insights/Analysis:** Formalizes the application of micro- and macro-disentanglement concepts to knowledge graphs, addressing the multi-faceted nature of entities and the context-dependency of relations in KGC.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were performed on public benchmark datasets to evaluate the model's effectiveness \\cite{wu2021}.\n    *   **Key performance metrics and comparison results:** DisenKGAT demonstrated superiority over existing state-of-the-art methods in terms of both *accuracy* and *explainability*. The experiments also validated the model's strong *robustness*.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions:** The model assumes that entities can be effectively decomposed into a predefined number of `K` independent components. The effectiveness of the mutual information regularization relies on its ability to accurately enforce this independence. The choice of `K` is a hyperparameter that needs tuning.\n    *   **Scope of applicability:** The primary focus is on Knowledge Graph Completion (KGC), specifically predicting missing entities in `(h, r, ?)` triplets. The proposed encoder model is designed with flexibility to be potentially generalized to work with various existing score functions.\n\n7.  **Technical Significance**\n    *   **Advancement of state-of-the-art:** DisenKGAT significantly advances the state-of-the-art in KGC by moving beyond static entity representations to adaptive, disentangled ones. This approach better captures the complex, multi-faceted nature of entities and the context-dependency of relations, leading to more accurate and nuanced predictions \\cite{wu2021}.\n    *   **Potential impact on future research:** This work opens new avenues for research into disentangled representation learning within complex, heterogeneous graph structures like KGs. It provides a strong foundation for developing more interpretable, robust, and context-aware KGC models, and could inspire similar dynamic embedding approaches for other graph-based tasks.",
        "keywords": [
          "DisenKGAT",
          "Knowledge Graph Completion (KGC)",
          "Disentangled representation learning",
          "Knowledge Graph Embedding",
          "Graph Attention Network",
          "Micro-disentanglement",
          "Macro-disentanglement",
          "Relation-aware aggregation",
          "Mutual information regularization",
          "Adaptive scoring",
          "Complex relations",
          "Interpretability",
          "Robustness",
          "State-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "29eb99518d16ccf8ac306d92f4a6377ae109d9be.pdf"
    },
    {
      "success": true,
      "doc_id": "ab13b57b031d0ddb0fa99c5b32330dc2",
      "summary": "Here's a focused summary of the paper \\cite{xu2019} for a literature review:\n\n*   **CITATION**: \\cite{xu2019}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most Knowledge Graph (KG) embedding models learn from time-unaware facts, failing to capture the dynamic nature of knowledge and the temporal validity of triples. This leads to limitations in reasoning over Temporal KGs (TKGs).\n    *   **Importance & Challenge**: Temporal information is crucial for accurate KG reasoning (e.g., distinguishing `(Obama, presidentOf, USA)` in 2010 vs. 2020). Existing temporal KGE models often represent time as a simple vector, which cannot capture properties like time interval length or the inherent *uncertainty* in how entity/relation representations evolve over time (e.g., sudden, unpredictable changes).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Static KGEs (e.g., TransE, DistMult, ComplEx, RotatE, QuatE)**: These models ignore time, treating `(s,p,o,t1)` and `(s,p,o,t2)` identically, which is problematic for temporal facts.\n        *   **Previous Temporal KGEs (e.g., TTransE, HyTE, TA-TransE, TA-DistMult, DE-SimplE, Know-Evolve)**: These models attempt to incorporate time, often by embedding time as a vector or using RNNs.\n    *   **Limitations of Previous Solutions**:\n        *   Most existing TKGEs fail to capture complex temporal properties like the length of time intervals or the order of time points.\n        *   A critical limitation is their ignorance of *temporal uncertainty* during the evolution of entity/relation representations, assuming deterministic changes.\n        *   Some models (e.g., TA-TransE, TA-DistMult, DE-SimplE) cannot effectively model facts involving time intervals `[ts, te]`.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Additive Time Series Decomposition**: \\cite{xu2019} proposes ATiSE (Additive Time Series Embedding), which models the evolution of each entity and relation representation as a multi-dimensional additive time series: `Yt = Tt + St + Rt` (Trend + Seasonal + Random components).\n        *   **Gaussian Distribution Embeddings**: Each entity and relation is represented as a *multi-dimensional Gaussian distribution* at each time step `t`.\n            *   The *mean vector* (`es,t`, `rp,t`, `eo,t`) captures the expected position, derived from an initial representation, a linear trend component, and a sine-based seasonal component.\n            *   The *covariance matrix* (`s`, `r`, `o`) explicitly represents the *temporal uncertainty* during evolution, modeled as a constant diagonal matrix for computational efficiency.\n        *   **Score Function**: A *symmetric KL-divergence* is used to measure the similarity between the relation's Gaussian distribution (`Pr,t`) and the entity-transformed Gaussian distribution (`Pe,t = N(es,t - eo,t, s + o)`) to score a fact `(s,p,o,t)`.\n    *   **Novelty or Difference**:\n        *   **First to use Additive Time Series Decomposition**: This is a novel application of time series analysis to model the dynamic evolution of KG embeddings, establishing a new connection between relational processes and time series.\n        *   **Explicit Modeling of Temporal Uncertainty**: By representing entities/relations as Gaussian distributions, ATiSE explicitly accounts for the inherent randomness and uncertainty in their temporal evolution, a significant departure from deterministic approaches.\n        *   **Efficient Gaussian Embedding**: The use of constant diagonal covariance matrices and a symmetric KL-divergence allows for efficient computation while still capturing uncertainty.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of ATiSE, a temporal KG embedding model based on additive time series decomposition for entity and relation representations.\n        *   Modeling of entity and relation embeddings as multi-dimensional Gaussian distributions to capture temporal uncertainty, with means evolving via trend and seasonal components.\n        *   Development of a symmetric KL-divergence-based score function for facts in this Gaussian embedding space.\n    *   **Theoretical Insights/Analysis**: Established a novel connection between relational processes in KGs and time series analysis, opening new avenues for research in temporal reasoning.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{xu2019} compared ATiSE against several state-of-the-art static KGE models (TransE, DistMult, ComplEx, RotatE, QuatE) and existing temporal KGE models (TTransE, HyTE, TA-TransE, TA-DistMult, DE-SimplE) on the task of link prediction. An ablation study was also performed to analyze component effects.\n    *   **Datasets**: Experiments were conducted on four temporal KG datasets: ICEWS14, ICEWS05-15, YAGO11k, and Wikidata12k.\n    *   **Key Performance Metrics & Comparison Results**: ATiSE \"significantly outperforms\" all compared state-of-the-art KGE models and existing temporal KGE models on link prediction across all four temporal KG datasets \\cite{xu2019}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The model assumes diagonal and temporally stationary (constant) covariance matrices for computational efficiency, which might simplify the true complexity of uncertainty. The trend and seasonal components are modeled with simple linear and sine functions, respectively.\n    *   **Scope of Applicability**: Primarily focused on temporal KGs with explicit time stamps or intervals, and validated for link prediction tasks.\n\n7.  **Technical Significance**\n    *   **Advance State-of-the-Art**: \\cite{xu2019} significantly advances the state-of-the-art in temporal KG embedding by introducing a principled way to model the dynamic evolution and inherent uncertainty of knowledge over time.\n    *   **Potential Impact**: This work opens a new research direction by bridging relational processes with time series analysis. It provides a more robust and expressive framework for understanding and predicting evolving knowledge, with potential applications in dynamic knowledge base completion, question answering, and event forecasting.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are inherently dynamic, yet most embedding models struggle to capture the temporal evolution and validity of facts, often ignoring the critical aspect of *temporal uncertainty*. Existing temporal KGEs typically represent time deterministically, failing to account for the unpredictable, complex changes in entity and relation representations over time. We introduce ATiSE (Additive Time Series Embedding), a novel framework that fundamentally redefines how temporal KGs are modeled. ATiSE represents each entity and relation as a multi-dimensional *Gaussian distribution* at every time step, explicitly capturing *temporal uncertainty* through its covariance matrix. Crucially, we leverage *additive time series decomposition* to model the evolution of these Gaussian means, incorporating distinct trend and seasonal components. A symmetric *KL-divergence* scores factual triples. Our approach is the first to bridge relational processes with time series analysis, offering a principled way to understand evolving knowledge. Extensive experiments on four temporal KG datasets demonstrate that ATiSE significantly outperforms state-of-the-art static and temporal KGE models in *link prediction*, opening new avenues for dynamic knowledge base completion and event forecasting.",
      "keywords": [
        "Temporal Knowledge Graph embedding",
        "Temporal uncertainty modeling",
        "Additive Time Series Decomposition",
        "Gaussian distribution embeddings",
        "ATiSE (Additive Time Series Embedding)",
        "Relational processes and time series analysis",
        "Symmetric KL-divergence",
        "Link prediction",
        "Dynamic knowledge base completion",
        "State-of-the-art performance",
        "Multi-dimensional additive time series",
        "Covariance matrix"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/58e1b93b18370433633152cb8825917edc2f16a6.pdf",
      "citation_key": "xu2019",
      "metadata": {
        "title": "Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition",
        "authors": [
          "Chengjin Xu",
          "M. Nayyeri",
          "Fouad Alkhoury",
          "Jens Lehmann",
          "H. S. Yazdi"
        ],
        "published_date": "2019",
        "abstract": "Knowledge Graph (KG) embedding has attracted more attention in recent years. Most KG embedding models learn from time-unaware triples. However, the inclusion of temporal information beside triples would further improve the performance of a KGE model. In this regard, we propose ATiSE, a temporal KG embedding model which incorporates time information into entity/relation representations by using Additive Time Series decomposition. Moreover, considering the temporal uncertainty during the evolution of entity/relation representations over time, we map the representations of temporal KGs into the space of multi-dimensional Gaussian distributions. The mean of each entity/relation embedding at a time step shows the current expected position, whereas its covariance (which is temporally stationary) represents its temporal uncertainty. Experimental results show that ATiSE chieves the state-of-the-art on link prediction over four temporal KGs.",
        "file_path": "paper_data/knowledge_graph_embedding/58e1b93b18370433633152cb8825917edc2f16a6.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{xu2019} for a literature review:\n\n*   **CITATION**: \\cite{xu2019}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most Knowledge Graph (KG) embedding models learn from time-unaware facts, failing to capture the dynamic nature of knowledge and the temporal validity of triples. This leads to limitations in reasoning over Temporal KGs (TKGs).\n    *   **Importance & Challenge**: Temporal information is crucial for accurate KG reasoning (e.g., distinguishing `(Obama, presidentOf, USA)` in 2010 vs. 2020). Existing temporal KGE models often represent time as a simple vector, which cannot capture properties like time interval length or the inherent *uncertainty* in how entity/relation representations evolve over time (e.g., sudden, unpredictable changes).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Static KGEs (e.g., TransE, DistMult, ComplEx, RotatE, QuatE)**: These models ignore time, treating `(s,p,o,t1)` and `(s,p,o,t2)` identically, which is problematic for temporal facts.\n        *   **Previous Temporal KGEs (e.g., TTransE, HyTE, TA-TransE, TA-DistMult, DE-SimplE, Know-Evolve)**: These models attempt to incorporate time, often by embedding time as a vector or using RNNs.\n    *   **Limitations of Previous Solutions**:\n        *   Most existing TKGEs fail to capture complex temporal properties like the length of time intervals or the order of time points.\n        *   A critical limitation is their ignorance of *temporal uncertainty* during the evolution of entity/relation representations, assuming deterministic changes.\n        *   Some models (e.g., TA-TransE, TA-DistMult, DE-SimplE) cannot effectively model facts involving time intervals `[ts, te]`.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Additive Time Series Decomposition**: \\cite{xu2019} proposes ATiSE (Additive Time Series Embedding), which models the evolution of each entity and relation representation as a multi-dimensional additive time series: `Yt = Tt + St + Rt` (Trend + Seasonal + Random components).\n        *   **Gaussian Distribution Embeddings**: Each entity and relation is represented as a *multi-dimensional Gaussian distribution* at each time step `t`.\n            *   The *mean vector* (`es,t`, `rp,t`, `eo,t`) captures the expected position, derived from an initial representation, a linear trend component, and a sine-based seasonal component.\n            *   The *covariance matrix* (`s`, `r`, `o`) explicitly represents the *temporal uncertainty* during evolution, modeled as a constant diagonal matrix for computational efficiency.\n        *   **Score Function**: A *symmetric KL-divergence* is used to measure the similarity between the relation's Gaussian distribution (`Pr,t`) and the entity-transformed Gaussian distribution (`Pe,t = N(es,t - eo,t, s + o)`) to score a fact `(s,p,o,t)`.\n    *   **Novelty or Difference**:\n        *   **First to use Additive Time Series Decomposition**: This is a novel application of time series analysis to model the dynamic evolution of KG embeddings, establishing a new connection between relational processes and time series.\n        *   **Explicit Modeling of Temporal Uncertainty**: By representing entities/relations as Gaussian distributions, ATiSE explicitly accounts for the inherent randomness and uncertainty in their temporal evolution, a significant departure from deterministic approaches.\n        *   **Efficient Gaussian Embedding**: The use of constant diagonal covariance matrices and a symmetric KL-divergence allows for efficient computation while still capturing uncertainty.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of ATiSE, a temporal KG embedding model based on additive time series decomposition for entity and relation representations.\n        *   Modeling of entity and relation embeddings as multi-dimensional Gaussian distributions to capture temporal uncertainty, with means evolving via trend and seasonal components.\n        *   Development of a symmetric KL-divergence-based score function for facts in this Gaussian embedding space.\n    *   **Theoretical Insights/Analysis**: Established a novel connection between relational processes in KGs and time series analysis, opening new avenues for research in temporal reasoning.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{xu2019} compared ATiSE against several state-of-the-art static KGE models (TransE, DistMult, ComplEx, RotatE, QuatE) and existing temporal KGE models (TTransE, HyTE, TA-TransE, TA-DistMult, DE-SimplE) on the task of link prediction. An ablation study was also performed to analyze component effects.\n    *   **Datasets**: Experiments were conducted on four temporal KG datasets: ICEWS14, ICEWS05-15, YAGO11k, and Wikidata12k.\n    *   **Key Performance Metrics & Comparison Results**: ATiSE \"significantly outperforms\" all compared state-of-the-art KGE models and existing temporal KGE models on link prediction across all four temporal KG datasets \\cite{xu2019}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The model assumes diagonal and temporally stationary (constant) covariance matrices for computational efficiency, which might simplify the true complexity of uncertainty. The trend and seasonal components are modeled with simple linear and sine functions, respectively.\n    *   **Scope of Applicability**: Primarily focused on temporal KGs with explicit time stamps or intervals, and validated for link prediction tasks.\n\n7.  **Technical Significance**\n    *   **Advance State-of-the-Art**: \\cite{xu2019} significantly advances the state-of-the-art in temporal KG embedding by introducing a principled way to model the dynamic evolution and inherent uncertainty of knowledge over time.\n    *   **Potential Impact**: This work opens a new research direction by bridging relational processes with time series analysis. It provides a more robust and expressive framework for understanding and predicting evolving knowledge, with potential applications in dynamic knowledge base completion, question answering, and event forecasting.",
        "keywords": [
          "Temporal Knowledge Graph embedding",
          "Temporal uncertainty modeling",
          "Additive Time Series Decomposition",
          "Gaussian distribution embeddings",
          "ATiSE (Additive Time Series Embedding)",
          "Relational processes and time series analysis",
          "Symmetric KL-divergence",
          "Link prediction",
          "Dynamic knowledge base completion",
          "State-of-the-art performance",
          "Multi-dimensional additive time series",
          "Covariance matrix"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "58e1b93b18370433633152cb8825917edc2f16a6.pdf"
    },
    {
      "success": true,
      "doc_id": "d83bcbf2165c8696cf1d42d19c0aa3a0",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Most conventional Knowledge Graph Embedding (KGE) models assume that all triple facts within a Knowledge Graph (KG) share uniform confidence and are free from noise. This assumption is often violated in real-world KGs.\n    *   **Importance & Challenge:** KGs frequently contain noise and conflicts due to automatic construction processes and inherent data quality issues. Relying on the uniform confidence assumption leads to inaccurate embeddings and hinders the performance of downstream tasks like link prediction and relation extraction.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the Confidence-aware Knowledge Representation Learning (CKRL) framework, which was proposed to incorporate triple confidence into translation-based KGE models \\cite{shan2018}.\n    *   **Limitations of Previous Solutions:** While CKRL was effective at detecting noise, it suffered from several issues:\n        *   It used uniform negative sampling methods.\n        *   It employed a harsh triple quality function.\n        *   These limitations could lead to zero loss problems and false detection issues during training \\cite{shan2018}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** To address the limitations of CKRL, the paper introduces the novel concept of *negative triple confidence* \\cite{shan2018}.\n    *   **Novelty:** The core innovation is a *confidence-aware negative sampling method* that leverages this negative triple confidence. This method is designed to support the robust training of CKRL models specifically in noisy KG environments, mitigating the zero loss and false detection problems associated with previous uniform sampling strategies \\cite{shan2018}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of the concept of *negative triple confidence*. Development of a *confidence-aware negative sampling method* tailored for training KGE models in noisy KGs \\cite{shan2018}.\n    *   **System Design/Architectural Innovations:** The proposed method enhances and supports the training process of existing confidence-aware KGE frameworks (like CKRL) without requiring a complete architectural overhaul, focusing on improving the sampling strategy.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The model was evaluated on the standard *knowledge graph completion* task \\cite{shan2018}.\n    *   **Key Performance Metrics & Comparison Results:** Experimental results demonstrated that the integration of *negative triple confidence* significantly facilitated performance improvement in the knowledge graph completion task. This empirically confirmed the model's superior capability in noisy knowledge representation learning (NKRL) \\cite{shan2018}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper primarily focuses on improving the training of translation-based KGE models within the CKRL framework. While it addresses CKRL's limitations, it doesn't explicitly detail new limitations introduced by its own method.\n    *   **Scope of Applicability:** The method is particularly applicable to KGE scenarios where triple confidence information is available or can be estimated, and where noise and conflicts are prevalent in the KG.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** This work advances the technical state-of-the-art by providing a more robust and effective training mechanism for confidence-aware KGE models, particularly in the presence of noisy data. It highlights the critical role of sophisticated negative sampling in such contexts \\cite{shan2018}.\n    *   **Potential Impact:** The introduction of negative triple confidence and the associated sampling method can lead to more accurate and reliable knowledge graph embeddings, which are crucial for various AI applications that rely on high-quality KG representations. It paves the way for future research into more nuanced handling of uncertainty and noise in KGE.",
      "intriguing_abstract": "Real-world Knowledge Graphs (KGs) are inherently noisy, yet most Knowledge Graph Embedding (KGE) models erroneously assume uniform triple confidence, leading to suboptimal representations and hindering downstream AI tasks. While Confidence-aware Knowledge Representation Learning (CKRL) frameworks attempted to address this, they struggled with uniform negative sampling, often resulting in zero loss problems and false detections during training.\n\nThis paper introduces a paradigm shift by proposing the novel concept of *negative triple confidence*. Leveraging this, we develop a *confidence-aware negative sampling method* specifically designed to facilitate robust training of KGE models in noisy environments. Our approach effectively mitigates the critical limitations of prior methods, ensuring more reliable learning. Evaluated on the challenging knowledge graph completion task, our method demonstrates significant performance improvements, empirically confirming its superior capability in noisy knowledge representation learning. This work advances the state-of-the-art, paving the way for more accurate and resilient KG embeddings vital for high-stakes AI applications.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Noisy Knowledge Graphs",
        "Triple confidence",
        "Confidence-aware Knowledge Representation Learning (CKRL)",
        "Negative triple confidence",
        "Confidence-aware negative sampling method",
        "Robust training",
        "Knowledge graph completion",
        "Zero loss and false detection mitigation",
        "Noisy knowledge representation learning (NKRL)",
        "Translation-based KGE models"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/d4220644ef94fa4c2e5138a619cfcd86508d2ea1.pdf",
      "citation_key": "shan2018",
      "metadata": {
        "title": "Confidence-Aware Negative Sampling Method for Noisy Knowledge Graph Embedding",
        "authors": [
          "Yingchun Shan",
          "Chenyang Bu",
          "Xiaojian Liu",
          "Shengwei Ji",
          "Lei Li"
        ],
        "published_date": "2018",
        "abstract": "Knowledge graph embedding (KGE) can benefit a variety of downstream tasks, such as link prediction and relation extraction, and has therefore quickly gained much attention. However, most conventional embedding models assume that all triple facts share the same confidence without any noise, which is inappropriate. In fact, many noises and conflicts can be brought into a knowledge graph (KG) because of both the automatic construction process and data quality problems. Fortunately, the novel confidence-aware knowledge representation learning (CKRL) framework was proposed, to incorporate triple confidence into translation-based models for KGE. Though effective at detecting noises, with uniform negative sampling methods, and a harsh triple quality function, CKRL could easily cause zero loss problems and false detection issues. To address these problems, we introduce the concept of negative triple confidence and propose a confidence-aware negative sampling method to support the training of CKRL in noisy KGs. We evaluate our model on the knowledge graph completion task. Experimental results demonstrate that the idea of introducing negative triple confidence can greatly facilitate performance improvement in this task, which confirms the capability of our model in noisy knowledge representation learning (NKRL).",
        "file_path": "paper_data/knowledge_graph_embedding/d4220644ef94fa4c2e5138a619cfcd86508d2ea1.pdf",
        "venue": "International Conference on Big Knowledge",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Most conventional Knowledge Graph Embedding (KGE) models assume that all triple facts within a Knowledge Graph (KG) share uniform confidence and are free from noise. This assumption is often violated in real-world KGs.\n    *   **Importance & Challenge:** KGs frequently contain noise and conflicts due to automatic construction processes and inherent data quality issues. Relying on the uniform confidence assumption leads to inaccurate embeddings and hinders the performance of downstream tasks like link prediction and relation extraction.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the Confidence-aware Knowledge Representation Learning (CKRL) framework, which was proposed to incorporate triple confidence into translation-based KGE models \\cite{shan2018}.\n    *   **Limitations of Previous Solutions:** While CKRL was effective at detecting noise, it suffered from several issues:\n        *   It used uniform negative sampling methods.\n        *   It employed a harsh triple quality function.\n        *   These limitations could lead to zero loss problems and false detection issues during training \\cite{shan2018}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** To address the limitations of CKRL, the paper introduces the novel concept of *negative triple confidence* \\cite{shan2018}.\n    *   **Novelty:** The core innovation is a *confidence-aware negative sampling method* that leverages this negative triple confidence. This method is designed to support the robust training of CKRL models specifically in noisy KG environments, mitigating the zero loss and false detection problems associated with previous uniform sampling strategies \\cite{shan2018}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of the concept of *negative triple confidence*. Development of a *confidence-aware negative sampling method* tailored for training KGE models in noisy KGs \\cite{shan2018}.\n    *   **System Design/Architectural Innovations:** The proposed method enhances and supports the training process of existing confidence-aware KGE frameworks (like CKRL) without requiring a complete architectural overhaul, focusing on improving the sampling strategy.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The model was evaluated on the standard *knowledge graph completion* task \\cite{shan2018}.\n    *   **Key Performance Metrics & Comparison Results:** Experimental results demonstrated that the integration of *negative triple confidence* significantly facilitated performance improvement in the knowledge graph completion task. This empirically confirmed the model's superior capability in noisy knowledge representation learning (NKRL) \\cite{shan2018}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper primarily focuses on improving the training of translation-based KGE models within the CKRL framework. While it addresses CKRL's limitations, it doesn't explicitly detail new limitations introduced by its own method.\n    *   **Scope of Applicability:** The method is particularly applicable to KGE scenarios where triple confidence information is available or can be estimated, and where noise and conflicts are prevalent in the KG.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** This work advances the technical state-of-the-art by providing a more robust and effective training mechanism for confidence-aware KGE models, particularly in the presence of noisy data. It highlights the critical role of sophisticated negative sampling in such contexts \\cite{shan2018}.\n    *   **Potential Impact:** The introduction of negative triple confidence and the associated sampling method can lead to more accurate and reliable knowledge graph embeddings, which are crucial for various AI applications that rely on high-quality KG representations. It paves the way for future research into more nuanced handling of uncertainty and noise in KGE.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Noisy Knowledge Graphs",
          "Triple confidence",
          "Confidence-aware Knowledge Representation Learning (CKRL)",
          "Negative triple confidence",
          "Confidence-aware negative sampling method",
          "Robust training",
          "Knowledge graph completion",
          "Zero loss and false detection mitigation",
          "Noisy knowledge representation learning (NKRL)",
          "Translation-based KGE models"
        ],
        "is_new_direction": "0",
        "paper_type": "technical"
      },
      "file_name": "d4220644ef94fa4c2e5138a619cfcd86508d2ea1.pdf"
    },
    {
      "success": true,
      "doc_id": "b7561ef721b058f956869ba124a66cc0",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Prior Knowledge Graph Embedding (KGE) approaches struggle to effectively model composition patterns, particularly those that are under-represented (long-tail) in the training data \\cite{zheng2024}. Existing models often implicitly assume relations are non-compositional if their patterns are not well-represented, leading to performance degradation \\cite{zheng2024}.\n    *   **Importance and challenge:** Composition patterns are crucial as they involve nearly all relations in KGs, enabling logical deductions and reasoning, which are fundamental to acquiring new knowledge \\cite{zheng2024}. The challenge lies in developing KGE models that can robustly learn and extrapolate these patterns, especially when learning instances are limited, and adapt to evolving relation patterns in dynamic KGs \\cite{zheng2024}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** The work relates to traditional KGE models (geometric and bilinear) that explicitly capture relation patterns, such as TransE, RotatE, and hyperbolic space models like MurP \\cite{zheng2024}. It also acknowledges neural network-based KGEs (e.g., R-GCN, ConvE) and few-shot KGEs for long-tail entities/relations \\cite{zheng2024}.\n    *   **Limitations of previous solutions:** Previous KGEs often model relations as compositional only if well-represented in training data, which is a restrictive assumption that contradicts the nature of real-world relations \\cite{zheng2024}. This leads to poor performance on under-represented composition patterns and limits their ability to adapt to new compositional relationships \\cite{zheng2024}. The paper explicitly states it differs from typical few-shot learning KGEs by focusing on *composition patterns* with minimal representation, rather than long-tail entities or relations themselves \\cite{zheng2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method or algorithm:** The paper proposes **HolmE**, a general form of Riemannian KGE. The core idea is to design a relation embedding space that is **closed under composition**, meaning the composition of any two given relation embeddings remains within the same embedding space \\cite{zheng2024}. HolmE leverages Riemannian geometry, specifically hyperbolic space, and extends Mbius addition to product spaces for computational efficiency \\cite{zheng2024}.\n    *   **What makes this approach novel or different:** HolmE is pioneering in discussing KGE with the property of being closed under composition \\cite{zheng2024}. This property ensures that every relation embedding can compose or be composed by other relation embeddings, aligning with the theoretical nature of real-world relations and enabling robust modeling of composition patterns regardless of their representation in training data \\cite{zheng2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques:**\n        *   Introduction and formal definition of the property of KGE models being \"closed under composition\" \\cite{zheng2024}.\n        *   Proposal of **HolmE**, a novel Riemannian KGE model designed to satisfy this closure property \\cite{zheng2024}.\n        *   Extension of Mbius addition to product spaces of hyperbolic spaces for efficient computation \\cite{zheng2024}.\n    *   **Theoretical insights or analysis:**\n        *   Detailed theoretical proof and empirical evaluation demonstrating the property of being closed under composition for HolmE \\cite{zheng2024}.\n        *   Theoretical proof that prominent KGE models like TransE \\cite{bordes2013} and RotatE \\cite{sun2019} are special cases of HolmE, providing a unifying framework \\cite{zheng2024}.\n        *   In-depth analysis and quantification of composition patterns within KG data using \"triple count\" and \"representing ratio\" metrics \\cite{zheng2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were conducted on benchmark datasets (and extra datasets in the journal extension) to evaluate HolmE's performance \\cite{zheng2024}. These experiments specifically focused on demonstrating HolmE's advantages in modeling composition patterns, particularly in long-tail scenarios with restricted learning instances, and its ability to extrapolate to unseen relations \\cite{zheng2024}. Three research hypotheses regarding HolmE's superior properties in capturing composition patterns were verified \\cite{zheng2024}.\n    *   **Key performance metrics and comparison results:** The results highlight HolmE's notable advantages in modeling composition patterns, especially for long-tail patterns \\cite{zheng2024}. It demonstrated effectiveness in extrapolating to unseen relations through composition and achieved state-of-the-art performance on benchmark datasets \\cite{zheng2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The paper primarily focuses on the mathematical properties inherent in traditional KGEs that explicitly capture relation patterns \\cite{zheng2024}. It explicitly omits discussion on neural network-based models \\cite{zheng2024}. The approach specifically targets composition patterns with minimal representation in training data, rather than general few-shot learning for entities or relations themselves \\cite{zheng2024}.\n    *   **Scope of applicability:** HolmE is applicable to scenarios where robust modeling of diverse relation patterns, especially composition, is critical, and where long-tail composition patterns are prevalent. Its theoretical elegance and ability to extrapolate make it suitable for evolving KGs and knowledge discovery tasks.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art:** HolmE advances the state-of-the-art by introducing and formalizing the crucial property of \"closure under composition\" for KGE models, which was previously underexplored \\cite{zheng2024}. This property fundamentally enhances KGEs' capability to model complex, under-represented composition patterns and extrapolate to unseen relations, overcoming a significant limitation of prior work \\cite{zheng2024}. Its theoretical unification of existing models like TransE and RotatE also provides a deeper understanding of KGE architectures \\cite{zheng2024}.\n    *   **Potential impact on future research:** This work opens new avenues for KGE research by emphasizing the importance of algebraic properties in embedding spaces. It could inspire the development of other KGE models with strong theoretical guarantees for various relation patterns, leading to more robust, interpretable, and generalizable knowledge graph reasoning systems. The focus on long-tail composition patterns has implications for improving knowledge graph completion in real-world, incomplete KGs \\cite{zheng2024}.",
      "intriguing_abstract": "The vast, interconnected knowledge within Knowledge Graphs (KGs) hinges on understanding complex relational compositions, yet current Knowledge Graph Embedding (KGE) models falter when these patterns are under-represented or 'long-tail'. We introduce a fundamental, yet overlooked, property for KGEs: **closure under composition**. This ensures that any relation can theoretically compose with others, mirroring real-world relational dynamics.\n\nTo realize this, we propose **HolmE**, a novel Riemannian KGE model. HolmE leverages the rich geometry of hyperbolic space, extending Mbius addition to product spaces for efficient and robust composition modeling. HolmE is the first KGE explicitly designed with this closure property, enabling it to robustly learn and extrapolate even highly under-represented composition patterns, a critical challenge for existing models. Intriguingly, we theoretically prove that prominent KGEs like TransE and RotatE emerge as special cases of HolmE, offering a unifying framework. Extensive experiments demonstrate HolmE's superior performance, particularly in modeling long-tail composition patterns and extrapolating to unseen relations. This work not only advances the state-of-the-art in KGEs but also opens new avenues for developing theoretically grounded, highly generalizable knowledge graph reasoning systems, crucial for dynamic and evolving KGs.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "composition patterns",
        "long-tail patterns",
        "HolmE",
        "closed under composition",
        "Riemannian KGE",
        "hyperbolic space",
        "Mbius addition",
        "relation embedding space",
        "unifying framework",
        "extrapolation to unseen relations",
        "robust modeling",
        "state-of-the-art performance",
        "knowledge graph reasoning",
        "knowledge graph completion"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/15710515bae025372f298570267d234d4a3141cb.pdf",
      "citation_key": "zheng2024",
      "metadata": {
        "title": "Knowledge graph embedding closed under composition",
        "authors": [
          "Zhuoxun Zheng",
          "Baifan Zhou",
          "Hui Yang",
          "Zhipeng Tan",
          "Zequn Sun",
          "Chunnong Li",
          "A. Waaler",
          "Evgeny Kharlamov",
          "A. Soylu"
        ],
        "published_date": "2024",
        "abstract": "Knowledge Graph Embedding (KGE) has attracted increasing attention. Relation patterns, such as symmetry and inversion, have received considerable focus. Among them, composition patterns are particularly important, as they involve nearly all relations in KGs. However, prior KGE approaches often consider relations to be compositional only if they are well-represented in the training data. Consequently, it can lead to performance degradation, especially for under-represented composition patterns. To this end, we propose HolmE, a general form of KGE with its relation embedding space closed under composition, namely that the composition of any two given relation embeddings remains within the embedding space. This property ensures that every relation embedding can compose, or be composed by other relation embeddings. It enhances HolmEs capability to model under-represented (also called long-tail) composition patterns with limited learning instances. To our best knowledge, our work is pioneering in discussing KGE with this property of being closed under composition. We provide detailed theoretical proof and extensive experiments to demonstrate the notable advantages of HolmE in modelling composition patterns, particularly for long-tail patterns. Our results also highlight HolmEs effectiveness in extrapolating to unseen relations through composition and its state-of-the-art performance on benchmark datasets.",
        "file_path": "paper_data/knowledge_graph_embedding/15710515bae025372f298570267d234d4a3141cb.pdf",
        "venue": "Data mining and knowledge discovery",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Prior Knowledge Graph Embedding (KGE) approaches struggle to effectively model composition patterns, particularly those that are under-represented (long-tail) in the training data \\cite{zheng2024}. Existing models often implicitly assume relations are non-compositional if their patterns are not well-represented, leading to performance degradation \\cite{zheng2024}.\n    *   **Importance and challenge:** Composition patterns are crucial as they involve nearly all relations in KGs, enabling logical deductions and reasoning, which are fundamental to acquiring new knowledge \\cite{zheng2024}. The challenge lies in developing KGE models that can robustly learn and extrapolate these patterns, especially when learning instances are limited, and adapt to evolving relation patterns in dynamic KGs \\cite{zheng2024}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** The work relates to traditional KGE models (geometric and bilinear) that explicitly capture relation patterns, such as TransE, RotatE, and hyperbolic space models like MurP \\cite{zheng2024}. It also acknowledges neural network-based KGEs (e.g., R-GCN, ConvE) and few-shot KGEs for long-tail entities/relations \\cite{zheng2024}.\n    *   **Limitations of previous solutions:** Previous KGEs often model relations as compositional only if well-represented in training data, which is a restrictive assumption that contradicts the nature of real-world relations \\cite{zheng2024}. This leads to poor performance on under-represented composition patterns and limits their ability to adapt to new compositional relationships \\cite{zheng2024}. The paper explicitly states it differs from typical few-shot learning KGEs by focusing on *composition patterns* with minimal representation, rather than long-tail entities or relations themselves \\cite{zheng2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method or algorithm:** The paper proposes **HolmE**, a general form of Riemannian KGE. The core idea is to design a relation embedding space that is **closed under composition**, meaning the composition of any two given relation embeddings remains within the same embedding space \\cite{zheng2024}. HolmE leverages Riemannian geometry, specifically hyperbolic space, and extends Mbius addition to product spaces for computational efficiency \\cite{zheng2024}.\n    *   **What makes this approach novel or different:** HolmE is pioneering in discussing KGE with the property of being closed under composition \\cite{zheng2024}. This property ensures that every relation embedding can compose or be composed by other relation embeddings, aligning with the theoretical nature of real-world relations and enabling robust modeling of composition patterns regardless of their representation in training data \\cite{zheng2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques:**\n        *   Introduction and formal definition of the property of KGE models being \"closed under composition\" \\cite{zheng2024}.\n        *   Proposal of **HolmE**, a novel Riemannian KGE model designed to satisfy this closure property \\cite{zheng2024}.\n        *   Extension of Mbius addition to product spaces of hyperbolic spaces for efficient computation \\cite{zheng2024}.\n    *   **Theoretical insights or analysis:**\n        *   Detailed theoretical proof and empirical evaluation demonstrating the property of being closed under composition for HolmE \\cite{zheng2024}.\n        *   Theoretical proof that prominent KGE models like TransE \\cite{bordes2013} and RotatE \\cite{sun2019} are special cases of HolmE, providing a unifying framework \\cite{zheng2024}.\n        *   In-depth analysis and quantification of composition patterns within KG data using \"triple count\" and \"representing ratio\" metrics \\cite{zheng2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were conducted on benchmark datasets (and extra datasets in the journal extension) to evaluate HolmE's performance \\cite{zheng2024}. These experiments specifically focused on demonstrating HolmE's advantages in modeling composition patterns, particularly in long-tail scenarios with restricted learning instances, and its ability to extrapolate to unseen relations \\cite{zheng2024}. Three research hypotheses regarding HolmE's superior properties in capturing composition patterns were verified \\cite{zheng2024}.\n    *   **Key performance metrics and comparison results:** The results highlight HolmE's notable advantages in modeling composition patterns, especially for long-tail patterns \\cite{zheng2024}. It demonstrated effectiveness in extrapolating to unseen relations through composition and achieved state-of-the-art performance on benchmark datasets \\cite{zheng2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The paper primarily focuses on the mathematical properties inherent in traditional KGEs that explicitly capture relation patterns \\cite{zheng2024}. It explicitly omits discussion on neural network-based models \\cite{zheng2024}. The approach specifically targets composition patterns with minimal representation in training data, rather than general few-shot learning for entities or relations themselves \\cite{zheng2024}.\n    *   **Scope of applicability:** HolmE is applicable to scenarios where robust modeling of diverse relation patterns, especially composition, is critical, and where long-tail composition patterns are prevalent. Its theoretical elegance and ability to extrapolate make it suitable for evolving KGs and knowledge discovery tasks.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art:** HolmE advances the state-of-the-art by introducing and formalizing the crucial property of \"closure under composition\" for KGE models, which was previously underexplored \\cite{zheng2024}. This property fundamentally enhances KGEs' capability to model complex, under-represented composition patterns and extrapolate to unseen relations, overcoming a significant limitation of prior work \\cite{zheng2024}. Its theoretical unification of existing models like TransE and RotatE also provides a deeper understanding of KGE architectures \\cite{zheng2024}.\n    *   **Potential impact on future research:** This work opens new avenues for KGE research by emphasizing the importance of algebraic properties in embedding spaces. It could inspire the development of other KGE models with strong theoretical guarantees for various relation patterns, leading to more robust, interpretable, and generalizable knowledge graph reasoning systems. The focus on long-tail composition patterns has implications for improving knowledge graph completion in real-world, incomplete KGs \\cite{zheng2024}.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "composition patterns",
          "long-tail patterns",
          "HolmE",
          "closed under composition",
          "Riemannian KGE",
          "hyperbolic space",
          "Mbius addition",
          "relation embedding space",
          "unifying framework",
          "extrapolation to unseen relations",
          "robust modeling",
          "state-of-the-art performance",
          "knowledge graph reasoning",
          "knowledge graph completion"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "15710515bae025372f298570267d234d4a3141cb.pdf"
    },
    {
      "success": true,
      "doc_id": "28a89f347ad5f145e2a8ee27c1bc7f00",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem**: Knowledge Graphs (KGs) suffer from incompleteness, limiting their utility in AI applications like question answering and recommendation systems \\cite{he2023}. Knowledge Graph Embedding (KGE) is a promising approach for KG completion, but traditional methods often neglect valuable entity type information.\n    *   **Importance and challenge**: Entity types provide crucial semantic context (e.g., \"painter\" for \"Da Vinci\" when linked by \"paint\") that can significantly improve KGE performance \\cite{he2023}. However, existing type-sensitive KGE models face challenges: they are often inflexible, tightly coupled to specific KGE architectures, require explicit type information (which is frequently incomplete or unavailable in real-world KGs like Freebase or WordNet), and fail to account for the diversity of entity types (an entity can have multiple types, and different relations highlight distinct type features) \\cite{he2023}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: The work builds upon traditional KGE models (e.g., TransE, DistMult, ComplEx, SimplE) that learn embeddings from structured triples \\cite{he2023}. It also relates to prior type-sensitive KGE models (e.g., TKRL, TransC, JOIE, TransT, TaRP, CAKE, TypeDM, TypeComplex) that attempt to incorporate type information \\cite{he2023}.\n    *   **Limitations of previous solutions**:\n        *   Traditional KGEs ignore entity type information, limiting their expressive power \\cite{he2023}.\n        *   Previous type-sensitive KGEs often require explicit type supervision, which is a significant limitation for many real-world KGs \\cite{he2023}.\n        *   Many existing type-sensitive models tightly encode type information into their objective functions, making them less flexible for integration with diverse KGE models \\cite{he2023}.\n        *   They often neglect the diversity of entity types, where an entity's relevant type can vary based on the specific relation it participates in \\cite{he2023}.\n        *   Existing negative sampling strategies (uniform, Bernoulli, or simple type-constrained) can introduce false negatives, hinder entity clustering, or also require explicit type information \\cite{he2023}.\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes a universal **Type-augmented Knowledge graph Embedding framework (TaKE)**, designed to enhance any traditional KGE model by incorporating implicit type features \\cite{he2023}.\n    *   **Novelty**:\n        *   **Model-agnostic framework**: TaKE is designed to be combined with *any* traditional KGE model, making it highly flexible and universally applicable \\cite{he2023}.\n        *   **Automatic implicit type feature capture**: It learns type features automatically without requiring explicit type information supervision, addressing a major limitation of prior work \\cite{he2023}.\n        *   **Diversity of entity types**: TaKE models the diversity of entity types by employing a **relation-specific hyperplane mechanism**. This projects an entity's type representation onto different hyperplanes corresponding to its distinct connected relations, highlighting specific type features for each context \\cite{he2023}.\n        *   **Two-view representation**: The framework conceptually divides a type-aware KG into an \"entity-view\" (relation-entity triples) and a \"type-view\" (relation-type triples), mapping them into distinct vector spaces to capture specific and general features, respectively \\cite{he2023}.\n        *   **Type compatibility function**: A novel function is designed to model the type constraint between entities and their connected relations, facilitating the learning of implicit type features \\cite{he2023}.\n        *   **New type-constrained negative sampling strategy**: This strategy constructs more effective negative samples by dynamically sampling from both homogeneous and non-homogeneous candidate sets, leveraging type-constrained prior knowledge without explicit type information \\cite{he2023}.\n\n*   **Key Technical Contributions**\n    *   A novel, model-agnostic **TaKE framework** that can augment any traditional KGE model to be type-sensitive, without requiring explicit type information \\cite{he2023}.\n    *   An innovative **relation-specific hyperplane mechanism** to capture and distinguish the diversity of entity types based on their associated relations \\cite{he2023}.\n    *   A **type compatibility function** for automatically learning implicit type features and modeling type constraints \\cite{he2023}.\n    *   A new **type-constrained negative sampling strategy** that generates high-quality negative samples efficiently, even without explicit type supervision \\cite{he2023}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments were performed on the knowledge graph completion (link prediction) task \\cite{he2023}.\n    *   **Datasets**: Four widely used benchmarks derived from three real-world KGs: Freebase, WordNet, and YAGO \\cite{he2023}.\n    *   **Key performance metrics and comparison results**:\n        *   TaKE-augmented KGE models consistently outperformed their corresponding base models across all experiments \\cite{he2023}.\n        *   Specifically, combining TaKE with SimplE (TaKE-SimplE) achieved state-of-the-art performance on the KG completion task compared to all baselines \\cite{he2023}.\n        *   Visualization of vectorial representations demonstrated that type embeddings learned by TaKE cluster more effectively than entity embeddings, confirming its ability to capture meaningful type features \\cite{he2023}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations/assumptions**: The paper primarily focuses on addressing the limitations of *previous* KGE methods. While TaKE is presented as a universal framework, its performance is demonstrated within the scope of KG completion. The implicit assumption is that type information, even when not explicitly labeled, can be effectively inferred and leveraged from the existing graph structure.\n    *   **Scope of applicability**: TaKE is broadly applicable to various KGs, including those lacking explicit type information. It can be integrated with any traditional KGE model, enhancing its capabilities for downstream tasks like link prediction \\cite{he2023}.\n\n*   **Technical Significance**\n    *   **Advances state-of-the-art**: TaKE significantly advances the technical state-of-the-art in KG completion by providing a flexible and effective method to incorporate type information, achieving top performance when combined with strong base models like SimplE \\cite{he2023}.\n    *   **Potential impact on future research**: The model-agnostic nature of TaKE and its ability to learn implicit type features without explicit supervision offer a powerful paradigm for future KGE research. It opens avenues for enhancing a wide range of existing and novel KGE models, making them more robust and semantically aware, especially in scenarios where explicit type data is scarce \\cite{he2023}. The novel negative sampling strategy also provides a valuable contribution to improving training efficiency and quality in KGE.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are foundational for advanced AI, yet their inherent incompleteness severely limits applications like question answering and recommendation systems. While Knowledge Graph Embedding (KGE) offers a powerful solution for KG completion, traditional methods often overlook crucial entity type information, and existing type-sensitive models are typically inflexible, demand explicit type supervision, and fail to capture the nuanced diversity of entity types.\n\nWe introduce **TaKE (Type-augmented Knowledge graph Embedding)**, a novel, model-agnostic framework designed to universally enhance *any* traditional KGE model. TaKE innovatively captures implicit type features automatically, circumventing the need for explicit type labels. Its core novelty lies in a **relation-specific hyperplane mechanism** that dynamically models an entity's diverse type features based on its contextual relations, alongside a new **type-constrained negative sampling strategy** for superior training. Extensive experiments on Freebase, WordNet, and YAGO demonstrate that TaKE-augmented models consistently outperform baselines, with TaKE-SimplE achieving state-of-the-art performance in **link prediction**. TaKE significantly advances KG completion by providing a flexible, semantically rich paradigm for KGE, paving the way for more robust and intelligent AI applications even in data-scarce environments.",
      "keywords": [
        "Knowledge Graphs",
        "Knowledge Graph Embedding",
        "entity type information",
        "KG incompleteness",
        "Type-augmented Knowledge graph Embedding (TaKE) framework",
        "model-agnostic framework",
        "automatic implicit type feature capture",
        "relation-specific hyperplane mechanism",
        "type compatibility function",
        "type-constrained negative sampling",
        "KG completion",
        "state-of-the-art performance",
        "AI applications"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/354fb91810c6d3756600c99ad84d2e6ef4136021.pdf",
      "citation_key": "he2023",
      "metadata": {
        "title": "A type-augmented knowledge graph embedding framework for knowledge graph completion",
        "authors": [
          "Peng He",
          "Gang Zhou",
          "Yao Yao",
          "Zhe Wang",
          "Hao Yang"
        ],
        "published_date": "2023",
        "abstract": "Knowledge graphs (KGs) are of great importance to many artificial intelligence applications, but they usually suffer from the incomplete problem. Knowledge graph embedding (KGE), which aims to represent entities and relations in low-dimensional continuous vector spaces, has been proved to be a promising approach for KG completion. Traditional KGE methods only concentrate on structured triples, while paying less attention to the type information of entities. In fact, incorporating entity types into embedding learning could further improve the performance of KG completion. To this end, we propose a universal Type-augmented Knowledge graph Embedding framework (TaKE) which could utilize type features to enhance any traditional KGE models. TaKE automatically captures type features under no explicit type information supervision. And by learning different type representations of each entity, TaKE could distinguish the diversity of types specific to distinct relations. We also design a new type-constrained negative sampling strategy to construct more effective negative samples for the training process. Extensive experiments on four datasets from three real-world KGs (Freebase, WordNet and YAGO) demonstrate the merits of our proposed framework. In particular, combining TaKE with the recent tensor factorization KGE model SimplE can achieve state-of-the-art performance on the KG completion task.",
        "file_path": "paper_data/knowledge_graph_embedding/354fb91810c6d3756600c99ad84d2e6ef4136021.pdf",
        "venue": "Scientific Reports",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem**: Knowledge Graphs (KGs) suffer from incompleteness, limiting their utility in AI applications like question answering and recommendation systems \\cite{he2023}. Knowledge Graph Embedding (KGE) is a promising approach for KG completion, but traditional methods often neglect valuable entity type information.\n    *   **Importance and challenge**: Entity types provide crucial semantic context (e.g., \"painter\" for \"Da Vinci\" when linked by \"paint\") that can significantly improve KGE performance \\cite{he2023}. However, existing type-sensitive KGE models face challenges: they are often inflexible, tightly coupled to specific KGE architectures, require explicit type information (which is frequently incomplete or unavailable in real-world KGs like Freebase or WordNet), and fail to account for the diversity of entity types (an entity can have multiple types, and different relations highlight distinct type features) \\cite{he2023}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: The work builds upon traditional KGE models (e.g., TransE, DistMult, ComplEx, SimplE) that learn embeddings from structured triples \\cite{he2023}. It also relates to prior type-sensitive KGE models (e.g., TKRL, TransC, JOIE, TransT, TaRP, CAKE, TypeDM, TypeComplex) that attempt to incorporate type information \\cite{he2023}.\n    *   **Limitations of previous solutions**:\n        *   Traditional KGEs ignore entity type information, limiting their expressive power \\cite{he2023}.\n        *   Previous type-sensitive KGEs often require explicit type supervision, which is a significant limitation for many real-world KGs \\cite{he2023}.\n        *   Many existing type-sensitive models tightly encode type information into their objective functions, making them less flexible for integration with diverse KGE models \\cite{he2023}.\n        *   They often neglect the diversity of entity types, where an entity's relevant type can vary based on the specific relation it participates in \\cite{he2023}.\n        *   Existing negative sampling strategies (uniform, Bernoulli, or simple type-constrained) can introduce false negatives, hinder entity clustering, or also require explicit type information \\cite{he2023}.\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes a universal **Type-augmented Knowledge graph Embedding framework (TaKE)**, designed to enhance any traditional KGE model by incorporating implicit type features \\cite{he2023}.\n    *   **Novelty**:\n        *   **Model-agnostic framework**: TaKE is designed to be combined with *any* traditional KGE model, making it highly flexible and universally applicable \\cite{he2023}.\n        *   **Automatic implicit type feature capture**: It learns type features automatically without requiring explicit type information supervision, addressing a major limitation of prior work \\cite{he2023}.\n        *   **Diversity of entity types**: TaKE models the diversity of entity types by employing a **relation-specific hyperplane mechanism**. This projects an entity's type representation onto different hyperplanes corresponding to its distinct connected relations, highlighting specific type features for each context \\cite{he2023}.\n        *   **Two-view representation**: The framework conceptually divides a type-aware KG into an \"entity-view\" (relation-entity triples) and a \"type-view\" (relation-type triples), mapping them into distinct vector spaces to capture specific and general features, respectively \\cite{he2023}.\n        *   **Type compatibility function**: A novel function is designed to model the type constraint between entities and their connected relations, facilitating the learning of implicit type features \\cite{he2023}.\n        *   **New type-constrained negative sampling strategy**: This strategy constructs more effective negative samples by dynamically sampling from both homogeneous and non-homogeneous candidate sets, leveraging type-constrained prior knowledge without explicit type information \\cite{he2023}.\n\n*   **Key Technical Contributions**\n    *   A novel, model-agnostic **TaKE framework** that can augment any traditional KGE model to be type-sensitive, without requiring explicit type information \\cite{he2023}.\n    *   An innovative **relation-specific hyperplane mechanism** to capture and distinguish the diversity of entity types based on their associated relations \\cite{he2023}.\n    *   A **type compatibility function** for automatically learning implicit type features and modeling type constraints \\cite{he2023}.\n    *   A new **type-constrained negative sampling strategy** that generates high-quality negative samples efficiently, even without explicit type supervision \\cite{he2023}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments were performed on the knowledge graph completion (link prediction) task \\cite{he2023}.\n    *   **Datasets**: Four widely used benchmarks derived from three real-world KGs: Freebase, WordNet, and YAGO \\cite{he2023}.\n    *   **Key performance metrics and comparison results**:\n        *   TaKE-augmented KGE models consistently outperformed their corresponding base models across all experiments \\cite{he2023}.\n        *   Specifically, combining TaKE with SimplE (TaKE-SimplE) achieved state-of-the-art performance on the KG completion task compared to all baselines \\cite{he2023}.\n        *   Visualization of vectorial representations demonstrated that type embeddings learned by TaKE cluster more effectively than entity embeddings, confirming its ability to capture meaningful type features \\cite{he2023}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations/assumptions**: The paper primarily focuses on addressing the limitations of *previous* KGE methods. While TaKE is presented as a universal framework, its performance is demonstrated within the scope of KG completion. The implicit assumption is that type information, even when not explicitly labeled, can be effectively inferred and leveraged from the existing graph structure.\n    *   **Scope of applicability**: TaKE is broadly applicable to various KGs, including those lacking explicit type information. It can be integrated with any traditional KGE model, enhancing its capabilities for downstream tasks like link prediction \\cite{he2023}.\n\n*   **Technical Significance**\n    *   **Advances state-of-the-art**: TaKE significantly advances the technical state-of-the-art in KG completion by providing a flexible and effective method to incorporate type information, achieving top performance when combined with strong base models like SimplE \\cite{he2023}.\n    *   **Potential impact on future research**: The model-agnostic nature of TaKE and its ability to learn implicit type features without explicit supervision offer a powerful paradigm for future KGE research. It opens avenues for enhancing a wide range of existing and novel KGE models, making them more robust and semantically aware, especially in scenarios where explicit type data is scarce \\cite{he2023}. The novel negative sampling strategy also provides a valuable contribution to improving training efficiency and quality in KGE.",
        "keywords": [
          "Knowledge Graphs",
          "Knowledge Graph Embedding",
          "entity type information",
          "KG incompleteness",
          "Type-augmented Knowledge graph Embedding (TaKE) framework",
          "model-agnostic framework",
          "automatic implicit type feature capture",
          "relation-specific hyperplane mechanism",
          "type compatibility function",
          "type-constrained negative sampling",
          "KG completion",
          "state-of-the-art performance",
          "AI applications"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "354fb91810c6d3756600c99ad84d2e6ef4136021.pdf"
    },
    {
      "success": true,
      "doc_id": "5f8b1e2fd471410ef1a5355c7942a668",
      "summary": "Here's a focused summary of the paper \"TransG: A Generative Model for Knowledge Graph Embedding\" by \\cite{xiao2015} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the \"issue of multiple relation semantics\" in knowledge graph embedding, where a single relation can have multiple distinct meanings depending on the associated entity pairs \\cite{xiao2015}.\n    *   **Importance and Challenge:** Existing translation-based models (e.g., TransE, TransH, TransR) assign only one fixed translation vector to each relation, which is insufficient to capture these diverse meanings. Visualizations show that even for a single relation, entity pairs form multiple clusters, each representing a different latent semantic (e.g., \"HasPart\" can mean composition or location). This ambiguity arises from both artificial simplification in knowledge base curation and the inherent nature of language and knowledge representation.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the foundation of translation-based embedding models (e.g., TransE, TransH, TransR, CTransR, KG2E) which aim to embed entities and relations into continuous vector spaces following the principle `h + r  t`.\n    *   **Limitations of Previous Solutions:** Prior models had not formally discussed or effectively addressed the multiple relation semantics problem \\cite{xiao2015}. While CTransR attempted to cluster entity pairs for a relation, it was an \"ad-hoc clustering-based method\" requiring pre-processing, whereas \\cite{xiao2015} proposes a more \"elegant\" and theoretical solution without such requirements.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{xiao2015} proposes TransG, a novel generative model that employs a Bayesian non-parametric infinite mixture model (specifically, the Chinese Restaurant Process - CRP) to discover latent semantic components for each relation.\n    *   **Generative Process:**\n        *   Entity embedding mean vectors are drawn from a standard normal distribution.\n        *   For each triple `(h,r,t)`, a semantic component `r,m` is drawn from the CRP.\n        *   Head and tail entity vectors are drawn from normal distributions centered at their mean embeddings with learned variances.\n        *   A relation embedding vector `ur,m` for that specific semantic component is drawn from a normal distribution centered at `(ut - uh)`.\n    *   **Score Function:** The model defines a score function `P(f(h,r,t))  _{m=1}^{Mr} _{r,m} e^(-||uh + ur,m - ut||^2 / (2(h^2 + t^2)))`, where `_{r,m}` is the mixing factor and `Mr` is the adaptively learned number of semantic components for relation `r`.\n    *   **Novelty:** TransG is the first generative model for knowledge graph embedding and the first to formally address multiple relation semantics. It automatically discovers semantic clusters and adaptively learns the number of components for each relation, allowing it to select the most appropriate translation vector for a given triple, thereby reducing noise from unrelated semantics.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduces TransG, a generative model for knowledge graph embedding, which is a significant departure from previous discriminative translation-based models \\cite{xiao2015}.\n        *   Presents a principled method using a Bayesian non-parametric infinite mixture model (CRP) to automatically discover and model multiple latent semantic components for relations.\n    *   **Theoretical Insights:** Provides a formal discussion and solution for the previously unaddressed problem of multiple relation semantics. It offers a generalized geometric interpretation where `h + u_{r,m*(h,r,t)}  t`, with `m*(h,r,t)` being the primary semantic component dynamically chosen for each triple.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Evaluated on two standard tasks: Link Prediction (predicting missing entities) and Triple Classification (classifying triples as correct or incorrect). Semantic component analysis was also performed.\n    *   **Datasets:** Experiments were conducted on four benchmark datasets: WN18, FB15K, WN11, and FB13 (subsets of Wordnet and Freebase).\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Link Prediction:** TransG achieved \"substantial improvements against the state-of-the-art baselines\" \\cite{xiao2015}.\n            *   On FB15K (Filter setting), TransG achieved a HITS@10 of 79.8%, significantly outperforming previous state-of-the-art models like KG2E (71.5%), CTransR (70.2%), TransR (68.7%), TransH (64.4%), and TransE (47.1%).\n            *   On WN18 (Filter setting), TransG achieved a HITS@10 of 93.3%, matching KG2E and outperforming CTransR (92.3%), TransR (92.0%), TransE (89.2%), and TransH (82.3%).\n            *   Performance across different relation categories (1-1, 1-N, N-1, N-N) on FB15K showed TransG consistently achieving the highest HITS@10 for both head and tail prediction, demonstrating its robustness across relation types.\n        *   **Efficiency:** TransG demonstrated competitive efficiency, taking 4.8s per iteration on FB15K, which is significantly faster than TransR (136.8s) and PTransE (1200.0s), and converges at a similar speed to TransE.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The model's effectiveness relies on the assumption that multiple relation semantics can be adequately captured by a mixture of translation vectors and that the generative process with normal distributions is appropriate for entity and relation embeddings. The CRP's concentration parameter `` needs to be tuned.\n    *   **Scope of Applicability:** TransG is primarily applicable to knowledge graph embedding tasks where relations exhibit polysemy or multiple meanings, enhancing the accuracy of link prediction and triple classification.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art:** TransG significantly advances the technical state-of-the-art by formally identifying and providing a principled solution to the critical problem of multiple relation semantics in knowledge graph embedding, a challenge previously overlooked \\cite{xiao2015}. Its generative approach offers a novel perspective compared to existing discriminative models.\n    *   **Potential Impact:** This work opens new avenues for developing more nuanced, context-aware, and robust knowledge graph embedding models. It has the potential to improve the performance of downstream AI tasks such as knowledge inference, question answering, and relation extraction, which rely on accurate and rich knowledge representations.",
      "intriguing_abstract": "Knowledge graph embedding (KGE) models are foundational for AI, yet they fundamentally struggle with the pervasive issue of *multiple relation semantics*, where a single relation can embody diverse meanings depending on the entities involved. Traditional translation-based models, constrained by assigning only one fixed vector per relation, fail to capture this crucial nuance, leading to ambiguous and less accurate representations.\n\nWe introduce **TransG**, the first *generative model* for knowledge graph embedding, which offers a principled solution to this long-standing problem. TransG employs a novel *Bayesian non-parametric infinite mixture model*, specifically the *Chinese Restaurant Process (CRP)*, to automatically discover and dynamically select the most appropriate latent semantic component for each relation instance. This innovative approach allows TransG to adaptively learn the number of distinct meanings for each relation, significantly reducing semantic noise.\n\nOur experiments demonstrate TransG's superior performance, achieving substantial improvements in *link prediction* and *triple classification* on benchmark datasets like FB15K and WN18. TransG consistently outperforms state-of-the-art baselines, including TransE, TransH, TransR, CTransR, and KG2E, while maintaining competitive efficiency. This generative framework offers a more robust, context-aware KGE, paving the way for more accurate knowledge inference and advanced AI applications.",
      "keywords": [
        "knowledge graph embedding",
        "multiple relation semantics",
        "TransG",
        "generative model",
        "Bayesian non-parametric infinite mixture model",
        "Chinese Restaurant Process (CRP)",
        "latent semantic components",
        "adaptive semantic component discovery",
        "link prediction",
        "triple classification",
        "state-of-the-art improvements",
        "entity and relation embeddings",
        "polysemy in relations"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/67cab3bafc8fa9e1ae3ff89791ad43c81441d271.pdf",
      "citation_key": "xiao2015",
      "metadata": {
        "title": "TransG : A Generative Model for Knowledge Graph Embedding",
        "authors": [
          "Han Xiao",
          "Minlie Huang",
          "Xiaoyan Zhu"
        ],
        "published_date": "2015",
        "abstract": "Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, TransG. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, which is able to deal with multiple relation semantics. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.",
        "file_path": "paper_data/knowledge_graph_embedding/67cab3bafc8fa9e1ae3ff89791ad43c81441d271.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"TransG: A Generative Model for Knowledge Graph Embedding\" by \\cite{xiao2015} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the \"issue of multiple relation semantics\" in knowledge graph embedding, where a single relation can have multiple distinct meanings depending on the associated entity pairs \\cite{xiao2015}.\n    *   **Importance and Challenge:** Existing translation-based models (e.g., TransE, TransH, TransR) assign only one fixed translation vector to each relation, which is insufficient to capture these diverse meanings. Visualizations show that even for a single relation, entity pairs form multiple clusters, each representing a different latent semantic (e.g., \"HasPart\" can mean composition or location). This ambiguity arises from both artificial simplification in knowledge base curation and the inherent nature of language and knowledge representation.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the foundation of translation-based embedding models (e.g., TransE, TransH, TransR, CTransR, KG2E) which aim to embed entities and relations into continuous vector spaces following the principle `h + r  t`.\n    *   **Limitations of Previous Solutions:** Prior models had not formally discussed or effectively addressed the multiple relation semantics problem \\cite{xiao2015}. While CTransR attempted to cluster entity pairs for a relation, it was an \"ad-hoc clustering-based method\" requiring pre-processing, whereas \\cite{xiao2015} proposes a more \"elegant\" and theoretical solution without such requirements.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{xiao2015} proposes TransG, a novel generative model that employs a Bayesian non-parametric infinite mixture model (specifically, the Chinese Restaurant Process - CRP) to discover latent semantic components for each relation.\n    *   **Generative Process:**\n        *   Entity embedding mean vectors are drawn from a standard normal distribution.\n        *   For each triple `(h,r,t)`, a semantic component `r,m` is drawn from the CRP.\n        *   Head and tail entity vectors are drawn from normal distributions centered at their mean embeddings with learned variances.\n        *   A relation embedding vector `ur,m` for that specific semantic component is drawn from a normal distribution centered at `(ut - uh)`.\n    *   **Score Function:** The model defines a score function `P(f(h,r,t))  _{m=1}^{Mr} _{r,m} e^(-||uh + ur,m - ut||^2 / (2(h^2 + t^2)))`, where `_{r,m}` is the mixing factor and `Mr` is the adaptively learned number of semantic components for relation `r`.\n    *   **Novelty:** TransG is the first generative model for knowledge graph embedding and the first to formally address multiple relation semantics. It automatically discovers semantic clusters and adaptively learns the number of components for each relation, allowing it to select the most appropriate translation vector for a given triple, thereby reducing noise from unrelated semantics.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduces TransG, a generative model for knowledge graph embedding, which is a significant departure from previous discriminative translation-based models \\cite{xiao2015}.\n        *   Presents a principled method using a Bayesian non-parametric infinite mixture model (CRP) to automatically discover and model multiple latent semantic components for relations.\n    *   **Theoretical Insights:** Provides a formal discussion and solution for the previously unaddressed problem of multiple relation semantics. It offers a generalized geometric interpretation where `h + u_{r,m*(h,r,t)}  t`, with `m*(h,r,t)` being the primary semantic component dynamically chosen for each triple.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Evaluated on two standard tasks: Link Prediction (predicting missing entities) and Triple Classification (classifying triples as correct or incorrect). Semantic component analysis was also performed.\n    *   **Datasets:** Experiments were conducted on four benchmark datasets: WN18, FB15K, WN11, and FB13 (subsets of Wordnet and Freebase).\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Link Prediction:** TransG achieved \"substantial improvements against the state-of-the-art baselines\" \\cite{xiao2015}.\n            *   On FB15K (Filter setting), TransG achieved a HITS@10 of 79.8%, significantly outperforming previous state-of-the-art models like KG2E (71.5%), CTransR (70.2%), TransR (68.7%), TransH (64.4%), and TransE (47.1%).\n            *   On WN18 (Filter setting), TransG achieved a HITS@10 of 93.3%, matching KG2E and outperforming CTransR (92.3%), TransR (92.0%), TransE (89.2%), and TransH (82.3%).\n            *   Performance across different relation categories (1-1, 1-N, N-1, N-N) on FB15K showed TransG consistently achieving the highest HITS@10 for both head and tail prediction, demonstrating its robustness across relation types.\n        *   **Efficiency:** TransG demonstrated competitive efficiency, taking 4.8s per iteration on FB15K, which is significantly faster than TransR (136.8s) and PTransE (1200.0s), and converges at a similar speed to TransE.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The model's effectiveness relies on the assumption that multiple relation semantics can be adequately captured by a mixture of translation vectors and that the generative process with normal distributions is appropriate for entity and relation embeddings. The CRP's concentration parameter `` needs to be tuned.\n    *   **Scope of Applicability:** TransG is primarily applicable to knowledge graph embedding tasks where relations exhibit polysemy or multiple meanings, enhancing the accuracy of link prediction and triple classification.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art:** TransG significantly advances the technical state-of-the-art by formally identifying and providing a principled solution to the critical problem of multiple relation semantics in knowledge graph embedding, a challenge previously overlooked \\cite{xiao2015}. Its generative approach offers a novel perspective compared to existing discriminative models.\n    *   **Potential Impact:** This work opens new avenues for developing more nuanced, context-aware, and robust knowledge graph embedding models. It has the potential to improve the performance of downstream AI tasks such as knowledge inference, question answering, and relation extraction, which rely on accurate and rich knowledge representations.",
        "keywords": [
          "knowledge graph embedding",
          "multiple relation semantics",
          "TransG",
          "generative model",
          "Bayesian non-parametric infinite mixture model",
          "Chinese Restaurant Process (CRP)",
          "latent semantic components",
          "adaptive semantic component discovery",
          "link prediction",
          "triple classification",
          "state-of-the-art improvements",
          "entity and relation embeddings",
          "polysemy in relations"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271.pdf"
    },
    {
      "success": true,
      "doc_id": "48f0b7c24809cb7645cf7e75805bae24",
      "summary": "Here's a focused summary of the paper \"Knowledge Graph Embedding with Iterative Guidance from Soft Rules\" by `\\cite{guo2017}` for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the problem of effectively combining knowledge graph (KG) embedding models with logic rules to enhance KG completion and representation learning.\n    *   Previous approaches suffered from two main limitations:\n        1.  They typically made a **one-time injection of logic rules**, ignoring the interactive nature between embedding learning and logical inference. This prevented rules from iteratively refining embeddings and predictions.\n        2.  They focused **only on hard rules**, which are absolute and require extensive manual effort for creation or validation. This overlooked the vast amount of background information available as \"soft rules\" with varying confidence levels, which can be automatically extracted.\n    *   This problem is important because KGs are often incomplete, and logic rules offer a powerful mechanism for knowledge acquisition and inference, but their full potential in conjunction with embeddings has not been realized.\n\n*   **Related Work & Positioning**\n    *   Existing KG embedding techniques primarily rely solely on observed triples.\n    *   Prior work on combining embeddings with rules either used pipelined frameworks (where rules don't influence embedding learning) or joint learning paradigms.\n    *   Joint models `\\cite{guo2017}` refers to (e.g., Rocktshel et al. 2015, Demeester et al. 2016) injected rules as additional training instances or regularization terms in a **one-time manner**, failing to model the iterative interaction between embedding updates and logical inference.\n    *   These joint models were also limited to **hard rules**, neglecting the utility of automatically extracted soft rules.\n    *   Relation path methods also incorporated knowledge in a one-time fashion.\n    *   `\\cite{guo2017}` positions its work as the first to model the **interactive nature** between embedding learning and logical inference and to effectively utilize **automatically extracted soft rules** with varying confidence levels.\n\n*   **Technical Approach & Innovation**\n    *   `\\cite{guo2017}` proposes **RUGE (RUle-Guided Embedding)**, a novel paradigm for KG embedding with iterative guidance from soft rules.\n    *   **Core Method**: RUGE enables an embedding model to learn simultaneously from:\n        1.  Labeled triples (observed in the KG).\n        2.  Unlabeled triples (whose labels are predicted iteratively).\n        3.  Soft rules with various confidence levels (automatically extracted from the KG).\n    *   **Iterative Procedure**: The learning process alternates between two stages in each iteration:\n        1.  **Soft Label Prediction Stage**: Uses currently learned embeddings and propositionalized soft rules to predict soft labels (truth values between 0 and 1) for unlabeled triples. This is formulated as a rule-constrained optimization problem, projecting truth values into a subspace constrained by rules, where rule confidence levels are encoded.\n        2.  **Embedding Rectification Stage**: Integrates both labeled triples (with hard labels) and unlabeled triples (with newly predicted soft labels) to update the current embeddings.\n    *   **Rule Modeling**: `\\cite{guo2017}` restricts rules to Horn clauses and uses t-norm based fuzzy logics to model the truth value of propositionalized rules (groundings) as a composition of constituent triple truth values. The ComplEx model is used for scoring triples.\n    *   **Unlabeled Triples**: Only conclusion triples of valid groundings (where premise is observed, but conclusion is not) are considered as unlabeled triples, maximizing utility for knowledge acquisition.\n\n*   **Key Technical Contributions**\n    *   **Novel Paradigm**: Devised the first principled framework that models the **iterative interactions** between embedding learning and logical inference for KG embedding.\n    *   **Soft Rule Utilization**: Demonstrated the usefulness of **automatically extracted soft rules** (with varying confidence levels) in KG embedding, eliminating the need for laborious manual rule creation.\n    *   **Generic and Flexible Approach**: RUGE is designed to be generic, capable of integrating various types of rules and enhancing a good variety of KG embedding models.\n\n*   **Experimental Validation**\n    *   **Experiments**: Evaluated RUGE on the task of **link prediction**.\n    *   **Datasets**: Used large-scale public KGs: **Freebase** and **YAGO**.\n    *   **Key Performance Metrics**: Standard link prediction metrics (e.g., Mean Rank, Hits@N).\n    *   **Comparison Results**:\n        *   RUGE achieved **significant and consistent improvements** over state-of-the-art baseline embedding models (without rules).\n        *   The **iterative injection strategy** substantially outperformed one-time injection schemes, maximizing the utility of logic rules.\n        *   Automatically extracted **soft rules**, even those with moderate confidence levels, were found to be **highly beneficial** to KG embedding, despite their uncertainties.\n\n*   **Limitations & Scope**\n    *   The paper restricts the logic rules to **Horn clauses** (conclusion contains a single atom, premise is a conjunction of several atoms).\n    *   Unlabeled triples are specifically those encoded in the conclusion of a soft rule, not all unobserved triples.\n    *   The approach relies on the availability of modern rule mining systems (like AMIE/AMIE+) for automatic soft rule extraction.\n    *   While generic, the specific implementation uses ComplEx as the base embedding model.\n\n*   **Technical Significance**\n    *   `\\cite{guo2017}` significantly advances the technical state-of-the-art in rule-guided KG embedding by introducing an **iterative and interactive learning paradigm**.\n    *   It broadens the scope of rule-guided learning by demonstrating the efficacy of **automatically extracted soft rules**, moving beyond the limitations of manually curated hard rules.\n    *   The proposed framework provides a flexible foundation for future research, enabling the integration of diverse rule types and embedding models, potentially leading to more robust and complete knowledge graph representations.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are vital for AI, yet their inherent incompleteness hinders their full potential. While logic rules offer powerful inference, their integration with Knowledge Graph Embedding (KGE) models has been limited by one-time rule injection and an exclusive focus on hard, manually curated rules. We introduce RUGE (RUle-Guided Embedding), a novel paradigm for Knowledge Graph Embedding that pioneers iterative guidance from automatically extracted soft rules. RUGE uniquely models the interactive nature between embedding learning and logical inference, alternating between predicting soft labels for unlabeled triples using rule-constrained optimization and rectifying embeddings with these new insights. This allows rules, even uncertain ones, to continuously refine KG representations. Evaluated on Freebase and YAGO, RUGE achieves significant improvements in link prediction over state-of-the-art baselines. Our work demonstrates the profound benefits of iterative rule-embedding interaction and the utility of soft, automatically extracted Horn clauses, paving the way for more robust and complete KGs.",
      "keywords": [
        "Knowledge Graph Embedding",
        "Logic Rules",
        "Soft Rules",
        "Iterative Guidance",
        "RUGE (Rule-Guided Embedding)",
        "Knowledge Graph Completion",
        "Link Prediction",
        "Iterative Learning Paradigm",
        "Automatic Soft Rule Extraction",
        "Horn Clauses",
        "Embedding Rectification",
        "Soft Label Prediction",
        "Interactive Learning"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/405a7a7464cfe175333d6f04703ac272e00a85b4.pdf",
      "citation_key": "guo2017",
      "metadata": {
        "title": "Knowledge Graph Embedding with Iterative Guidance from Soft Rules",
        "authors": [
          "Shu Guo",
          "Quan Wang",
          "Lihong Wang",
          "Bin Wang",
          "Li Guo"
        ],
        "published_date": "2017",
        "abstract": "\n \n Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Combining such an embedding model with logic rules has recently attracted increasing attention. Most previous attempts made a one-time injection of logic rules, ignoring the interactive nature between embedding learning and logical inference. And they focused only on hard rules, which always hold with no exception and usually require extensive manual effort to create or validate. In this paper, we propose Rule-Guided Embedding (RUGE), a novel paradigm of KG embedding with iterative guidance from soft rules. RUGE enables an embedding model to learn simultaneously from 1) labeled triples that have been directly observed in a given KG, 2) unlabeled triples whose labels are going to be predicted iteratively, and 3) soft rules with various confidence levels extracted automatically from the KG. In the learning process, RUGE iteratively queries rules to obtain soft labels for unlabeled triples, and integrates such newly labeled triples to update the embedding model. Through this iterative procedure, knowledge embodied in logic rules may be better transferred into the learned embeddings. We evaluate RUGE in link prediction on Freebase and YAGO. Experimental results show that: 1) with rule knowledge injected iteratively, RUGE achieves significant and consistent improvements over state-of-the-art baselines; and 2) despite their uncertainties, automatically extracted soft rules are highly beneficial to KG embedding, even those with moderate confidence levels. The code and data used for this paper can be obtained from https://github.com/iieir-km/RUGE.\n \n",
        "file_path": "paper_data/knowledge_graph_embedding/405a7a7464cfe175333d6f04703ac272e00a85b4.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Knowledge Graph Embedding with Iterative Guidance from Soft Rules\" by `\\cite{guo2017}` for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the problem of effectively combining knowledge graph (KG) embedding models with logic rules to enhance KG completion and representation learning.\n    *   Previous approaches suffered from two main limitations:\n        1.  They typically made a **one-time injection of logic rules**, ignoring the interactive nature between embedding learning and logical inference. This prevented rules from iteratively refining embeddings and predictions.\n        2.  They focused **only on hard rules**, which are absolute and require extensive manual effort for creation or validation. This overlooked the vast amount of background information available as \"soft rules\" with varying confidence levels, which can be automatically extracted.\n    *   This problem is important because KGs are often incomplete, and logic rules offer a powerful mechanism for knowledge acquisition and inference, but their full potential in conjunction with embeddings has not been realized.\n\n*   **Related Work & Positioning**\n    *   Existing KG embedding techniques primarily rely solely on observed triples.\n    *   Prior work on combining embeddings with rules either used pipelined frameworks (where rules don't influence embedding learning) or joint learning paradigms.\n    *   Joint models `\\cite{guo2017}` refers to (e.g., Rocktshel et al. 2015, Demeester et al. 2016) injected rules as additional training instances or regularization terms in a **one-time manner**, failing to model the iterative interaction between embedding updates and logical inference.\n    *   These joint models were also limited to **hard rules**, neglecting the utility of automatically extracted soft rules.\n    *   Relation path methods also incorporated knowledge in a one-time fashion.\n    *   `\\cite{guo2017}` positions its work as the first to model the **interactive nature** between embedding learning and logical inference and to effectively utilize **automatically extracted soft rules** with varying confidence levels.\n\n*   **Technical Approach & Innovation**\n    *   `\\cite{guo2017}` proposes **RUGE (RUle-Guided Embedding)**, a novel paradigm for KG embedding with iterative guidance from soft rules.\n    *   **Core Method**: RUGE enables an embedding model to learn simultaneously from:\n        1.  Labeled triples (observed in the KG).\n        2.  Unlabeled triples (whose labels are predicted iteratively).\n        3.  Soft rules with various confidence levels (automatically extracted from the KG).\n    *   **Iterative Procedure**: The learning process alternates between two stages in each iteration:\n        1.  **Soft Label Prediction Stage**: Uses currently learned embeddings and propositionalized soft rules to predict soft labels (truth values between 0 and 1) for unlabeled triples. This is formulated as a rule-constrained optimization problem, projecting truth values into a subspace constrained by rules, where rule confidence levels are encoded.\n        2.  **Embedding Rectification Stage**: Integrates both labeled triples (with hard labels) and unlabeled triples (with newly predicted soft labels) to update the current embeddings.\n    *   **Rule Modeling**: `\\cite{guo2017}` restricts rules to Horn clauses and uses t-norm based fuzzy logics to model the truth value of propositionalized rules (groundings) as a composition of constituent triple truth values. The ComplEx model is used for scoring triples.\n    *   **Unlabeled Triples**: Only conclusion triples of valid groundings (where premise is observed, but conclusion is not) are considered as unlabeled triples, maximizing utility for knowledge acquisition.\n\n*   **Key Technical Contributions**\n    *   **Novel Paradigm**: Devised the first principled framework that models the **iterative interactions** between embedding learning and logical inference for KG embedding.\n    *   **Soft Rule Utilization**: Demonstrated the usefulness of **automatically extracted soft rules** (with varying confidence levels) in KG embedding, eliminating the need for laborious manual rule creation.\n    *   **Generic and Flexible Approach**: RUGE is designed to be generic, capable of integrating various types of rules and enhancing a good variety of KG embedding models.\n\n*   **Experimental Validation**\n    *   **Experiments**: Evaluated RUGE on the task of **link prediction**.\n    *   **Datasets**: Used large-scale public KGs: **Freebase** and **YAGO**.\n    *   **Key Performance Metrics**: Standard link prediction metrics (e.g., Mean Rank, Hits@N).\n    *   **Comparison Results**:\n        *   RUGE achieved **significant and consistent improvements** over state-of-the-art baseline embedding models (without rules).\n        *   The **iterative injection strategy** substantially outperformed one-time injection schemes, maximizing the utility of logic rules.\n        *   Automatically extracted **soft rules**, even those with moderate confidence levels, were found to be **highly beneficial** to KG embedding, despite their uncertainties.\n\n*   **Limitations & Scope**\n    *   The paper restricts the logic rules to **Horn clauses** (conclusion contains a single atom, premise is a conjunction of several atoms).\n    *   Unlabeled triples are specifically those encoded in the conclusion of a soft rule, not all unobserved triples.\n    *   The approach relies on the availability of modern rule mining systems (like AMIE/AMIE+) for automatic soft rule extraction.\n    *   While generic, the specific implementation uses ComplEx as the base embedding model.\n\n*   **Technical Significance**\n    *   `\\cite{guo2017}` significantly advances the technical state-of-the-art in rule-guided KG embedding by introducing an **iterative and interactive learning paradigm**.\n    *   It broadens the scope of rule-guided learning by demonstrating the efficacy of **automatically extracted soft rules**, moving beyond the limitations of manually curated hard rules.\n    *   The proposed framework provides a flexible foundation for future research, enabling the integration of diverse rule types and embedding models, potentially leading to more robust and complete knowledge graph representations.",
        "keywords": [
          "Knowledge Graph Embedding",
          "Logic Rules",
          "Soft Rules",
          "Iterative Guidance",
          "RUGE (Rule-Guided Embedding)",
          "Knowledge Graph Completion",
          "Link Prediction",
          "Iterative Learning Paradigm",
          "Automatic Soft Rule Extraction",
          "Horn Clauses",
          "Embedding Rectification",
          "Soft Label Prediction",
          "Interactive Learning"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "405a7a7464cfe175333d6f04703ac272e00a85b4.pdf"
    },
    {
      "success": true,
      "doc_id": "b7623e9f01f7b9a897f0ffb821d5cca9",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding \\cite{chen2021}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Knowledge Graph Embedding (KGE) methods are primarily designed for transductive settings, meaning they can only produce embeddings for entities seen during training. They fail in inductive settings where models need to generalize to entirely new Knowledge Graphs (KGs) containing entities unseen during training. While some inductive methods exist, they typically focus only on inductive relation prediction and do not produce general entity embeddings, thus limiting their applicability to a wide range of in-KG and out-of-KG tasks.\n    *   **Importance & Challenge:** KGs are constantly evolving, with new entities appearing daily. The inability of conventional KGEs to handle unseen entities restricts their utility in dynamic environments. Developing a general inductive KGE solution that can produce high-quality embeddings for novel entities is crucial for enabling various downstream applications (e.g., link prediction, question answering) on new or evolving KGs without extensive retraining. The challenge lies in learning transferable knowledge that is independent of specific entities.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Conventional KGEs (e.g., TransE, ComplEx, RotatE, R-GCN, CompGCN):** `\\cite{chen2021}` acknowledges their effectiveness in transductive settings but highlights their fundamental limitation in inductive scenarios due to learning fixed entity embeddings.\n        *   **Inductive KG methods (e.g., GraIL, CoMPILE, TACT, INDIGO):** These works address inductive settings by learning relation prediction from subgraph structures.\n        *   **Meta-learning in KGs (e.g., GMatching, MetaR, GEN, L2P-GNN, MI-GNN):** Previous meta-learning applications in KGs often focus on few-shot scenarios or out-of-knowledge-base (OOKB) entities connected to a known KG.\n    *   **Limitations of Previous Solutions:**\n        *   Conventional KGEs are inherently transductive and cannot generalize to unseen entities.\n        *   Existing inductive KG methods (like GraIL) are limited to inductive *relation prediction* and do not produce *entity embeddings*, making them unsuitable for general in-KG and out-of-KG tasks that require entity representations.\n        *   Other inductive methods relying on textual descriptions are not general enough for scenarios where such information is unavailable.\n        *   Meta-learning approaches for KGs have not fully addressed the problem of generating embeddings for *entirely new entities in new KGs* in a general inductive setting.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (MorsE):** `\\cite{chen2021}` proposes MorsE, a model that learns \"meta-knowledge\"  transferable structural patterns  instead of specific entity embeddings. This meta-knowledge is then used to produce embeddings for unseen entities.\n        *   **Meta-knowledge Modeling:** MorsE instantiates meta-knowledge as two entity-independent modules:\n            *   **Entity Initializer:** Initializes entity embeddings using learnable relation-domain and relation-range embeddings. This captures type-level information based on the relations an entity is connected to.\n            *   **GNN Modulator:** Enhances these initialized embeddings by aggregating information from the entity's multi-hop neighborhood structure using a Graph Neural Network (GNN). This captures instance-level information.\n        *   **Meta-knowledge Learning:** MorsE employs a meta-learning framework. During meta-training on a source KG, tasks are sampled, each comprising a support set and a query set. Entities within these tasks are treated as \"unseen\" to simulate the inductive setting. The model learns to produce effective entity embeddings (via the initializer and modulator) based on the support triples, which are then evaluated on the query triples. This \"learning to produce embeddings\" capability allows MorsE to generalize to target KGs with entirely new entities.\n    *   **Novelty & Differentiation:**\n        *   Unlike prior inductive methods, MorsE provides a *general solution* for inductive KGE by producing *actual entity embeddings* for unseen entities, enabling a broader range of tasks.\n        *   It explicitly models and learns \"meta-knowledge\" through entity-independent modules (Initializer and GNN Modulator) that capture transferable structural patterns.\n        *   The meta-learning strategy is specifically designed to simulate the inductive setting by treating entities within training tasks as unseen, fostering generalization to entirely new KGs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of MorsE, a novel framework for inductive KGE that leverages meta-knowledge transfer.\n    *   **System Design/Architectural Innovations:**\n        *   The design of entity-independent modules: an **Entity Initializer** (using relation-domain and relation-range embeddings) and a **GNN Modulator** for refining embeddings based on neighborhood structure.\n        *   Integration of these modules within a meta-learning paradigm to enable \"learning to produce embeddings\" for unseen entities.\n    *   **Theoretical Insights/Analysis:** Emphasizes the concept of \"meta-knowledge\" as universal, entity-independent, and transferable structural patterns, drawing an analogy to human cognition. This provides a conceptual foundation for designing inductive KGE models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** `\\cite{chen2021}` conducted extensive experiments to evaluate MorsE's performance in inductive settings for both in-KG and out-of-KG tasks.\n        *   **In-KG Task:** Link Prediction (predicting missing head or tail entities in triples).\n        *   **Out-of-KG Task:** Question Answering (likely involving entity retrieval or relation prediction based on questions).\n    *   **Key Performance Metrics & Comparison Results:** The paper states that MorsE \"significantly outperforms corresponding baselines\" for both in-KG and out-of-KG tasks in inductive settings. This indicates superior performance in generating reasonable and effective embeddings for unseen entities compared to existing methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper does not explicitly list technical limitations in the provided abstract/introduction. However, it implicitly assumes that structural information (neighboring relations and multi-hop structures) is sufficient for cognizing new entities, without relying on additional features like textual descriptions. While framed as a strength for generality, this could be a limitation in extremely sparse KGs where structural patterns are minimal.\n    *   **Scope of Applicability:** MorsE is designed for inductive settings where a model trained on source KGs needs to generalize to *target KGs with entities entirely unseen* during training. It aims to provide general entity embeddings for these unseen entities, making it applicable to a wide array of in-KG (e.g., link prediction) and out-of-KG (e.g., question answering) tasks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** `\\cite{chen2021}` significantly advances the state-of-the-art by providing the first general inductive KGE framework that produces *entity embeddings* for unseen entities. This addresses a critical gap left by previous inductive methods that focused solely on relation prediction.\n    *   **Potential Impact on Future Research:**\n        *   Opens new avenues for research in dynamic and evolving KGs, enabling KGE models to adapt to new entities and domains without complete retraining.\n        *   Encourages further exploration of meta-learning techniques for transferring structural knowledge in graph-structured data.\n        *   Could inspire the development of more sophisticated meta-knowledge modeling techniques that integrate other forms of information (e.g., textual, temporal) in an entity-independent manner.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are dynamic, constantly evolving with new entities, yet conventional Knowledge Graph Embedding (KGE) models are inherently transductive, failing to generalize to unseen entities. Existing inductive methods fall short by only predicting relations, leaving a critical gap: the inability to generate general *entity embeddings* for novel entities in new KGs.\n\nWe introduce MorsE, a groundbreaking framework that pioneers a **general inductive KGE** solution. Instead of learning fixed entity representations, MorsE learns \"meta-knowledge\"universal, transferable structural patterns. This is achieved through novel entity-independent modules: an **Entity Initializer** leveraging relation-domain and relation-range embeddings for type-level information, and a **GNN Modulator** refining embeddings with multi-hop neighborhood structures. Our meta-learning paradigm trains MorsE to \"learn to produce embeddings\" for entirely unseen entities, simulating real-world inductive scenarios.\n\nMorsE significantly outperforms state-of-the-art baselines across both in-KG (e.g., link prediction) and out-of-KG (e.g., question answering) tasks. This work represents a pivotal advancement, enabling KGE models to adapt seamlessly to evolving KGs and unlocking a myriad of applications without extensive retraining, pushing the boundaries of transferable knowledge in graph-structured data.",
      "keywords": [
        "Inductive Knowledge Graph Embedding (KGE)",
        "Meta-knowledge transfer",
        "MorsE",
        "Entity embeddings",
        "Meta-learning framework",
        "Unseen entities generalization",
        "Entity Initializer",
        "GNN Modulator",
        "Graph Neural Networks (GNN)",
        "Dynamic Knowledge Graphs",
        "Link Prediction",
        "Question Answering",
        "Relation-domain/range embeddings"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/8b717c4dfb309638307fcc7d2c798b1c20927a3e.pdf",
      "citation_key": "chen2021",
      "metadata": {
        "title": "Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding",
        "authors": [
          "Mingyang Chen",
          "Wen Zhang",
          "Yushan Zhu",
          "Hongting Zhou",
          "Zonggang Yuan",
          "Changliang Xu",
          "Hua-zeng Chen"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graphs (KGs) consisting of a large number of triples have become widespread recently, and many knowledge graph embedding (KGE) methods are proposed to embed entities and relations of a KG into continuous vector spaces. Such embedding methods simplify the operations of conducting various in-KG tasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering). They can be viewed as general solutions for representing KGs. However, existing KGE methods are not applicable to inductive settings, where a model trained on source KGs will be tested on target KGs with entities unseen during model training. Existing works focusing on KGs in inductive settings can only solve the inductive relation prediction task. They can not handle other out-of-KG tasks as general as KGE methods since they don't produce embeddings for entities. In this paper, to achieve inductive knowledge graph embedding, we propose a model MorsE, which does not learn embeddings for entities but learns transferable meta-knowledge that can be used to produce entity embeddings. Such meta-knowledge is modeled by entity-independent modules and learned by meta-learning. Experimental results show that our model significantly outperforms corresponding baselines for in-KG and out-of-KG tasks in inductive settings.",
        "file_path": "paper_data/knowledge_graph_embedding/8b717c4dfb309638307fcc7d2c798b1c20927a3e.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding \\cite{chen2021}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Knowledge Graph Embedding (KGE) methods are primarily designed for transductive settings, meaning they can only produce embeddings for entities seen during training. They fail in inductive settings where models need to generalize to entirely new Knowledge Graphs (KGs) containing entities unseen during training. While some inductive methods exist, they typically focus only on inductive relation prediction and do not produce general entity embeddings, thus limiting their applicability to a wide range of in-KG and out-of-KG tasks.\n    *   **Importance & Challenge:** KGs are constantly evolving, with new entities appearing daily. The inability of conventional KGEs to handle unseen entities restricts their utility in dynamic environments. Developing a general inductive KGE solution that can produce high-quality embeddings for novel entities is crucial for enabling various downstream applications (e.g., link prediction, question answering) on new or evolving KGs without extensive retraining. The challenge lies in learning transferable knowledge that is independent of specific entities.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Conventional KGEs (e.g., TransE, ComplEx, RotatE, R-GCN, CompGCN):** `\\cite{chen2021}` acknowledges their effectiveness in transductive settings but highlights their fundamental limitation in inductive scenarios due to learning fixed entity embeddings.\n        *   **Inductive KG methods (e.g., GraIL, CoMPILE, TACT, INDIGO):** These works address inductive settings by learning relation prediction from subgraph structures.\n        *   **Meta-learning in KGs (e.g., GMatching, MetaR, GEN, L2P-GNN, MI-GNN):** Previous meta-learning applications in KGs often focus on few-shot scenarios or out-of-knowledge-base (OOKB) entities connected to a known KG.\n    *   **Limitations of Previous Solutions:**\n        *   Conventional KGEs are inherently transductive and cannot generalize to unseen entities.\n        *   Existing inductive KG methods (like GraIL) are limited to inductive *relation prediction* and do not produce *entity embeddings*, making them unsuitable for general in-KG and out-of-KG tasks that require entity representations.\n        *   Other inductive methods relying on textual descriptions are not general enough for scenarios where such information is unavailable.\n        *   Meta-learning approaches for KGs have not fully addressed the problem of generating embeddings for *entirely new entities in new KGs* in a general inductive setting.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (MorsE):** `\\cite{chen2021}` proposes MorsE, a model that learns \"meta-knowledge\"  transferable structural patterns  instead of specific entity embeddings. This meta-knowledge is then used to produce embeddings for unseen entities.\n        *   **Meta-knowledge Modeling:** MorsE instantiates meta-knowledge as two entity-independent modules:\n            *   **Entity Initializer:** Initializes entity embeddings using learnable relation-domain and relation-range embeddings. This captures type-level information based on the relations an entity is connected to.\n            *   **GNN Modulator:** Enhances these initialized embeddings by aggregating information from the entity's multi-hop neighborhood structure using a Graph Neural Network (GNN). This captures instance-level information.\n        *   **Meta-knowledge Learning:** MorsE employs a meta-learning framework. During meta-training on a source KG, tasks are sampled, each comprising a support set and a query set. Entities within these tasks are treated as \"unseen\" to simulate the inductive setting. The model learns to produce effective entity embeddings (via the initializer and modulator) based on the support triples, which are then evaluated on the query triples. This \"learning to produce embeddings\" capability allows MorsE to generalize to target KGs with entirely new entities.\n    *   **Novelty & Differentiation:**\n        *   Unlike prior inductive methods, MorsE provides a *general solution* for inductive KGE by producing *actual entity embeddings* for unseen entities, enabling a broader range of tasks.\n        *   It explicitly models and learns \"meta-knowledge\" through entity-independent modules (Initializer and GNN Modulator) that capture transferable structural patterns.\n        *   The meta-learning strategy is specifically designed to simulate the inductive setting by treating entities within training tasks as unseen, fostering generalization to entirely new KGs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of MorsE, a novel framework for inductive KGE that leverages meta-knowledge transfer.\n    *   **System Design/Architectural Innovations:**\n        *   The design of entity-independent modules: an **Entity Initializer** (using relation-domain and relation-range embeddings) and a **GNN Modulator** for refining embeddings based on neighborhood structure.\n        *   Integration of these modules within a meta-learning paradigm to enable \"learning to produce embeddings\" for unseen entities.\n    *   **Theoretical Insights/Analysis:** Emphasizes the concept of \"meta-knowledge\" as universal, entity-independent, and transferable structural patterns, drawing an analogy to human cognition. This provides a conceptual foundation for designing inductive KGE models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** `\\cite{chen2021}` conducted extensive experiments to evaluate MorsE's performance in inductive settings for both in-KG and out-of-KG tasks.\n        *   **In-KG Task:** Link Prediction (predicting missing head or tail entities in triples).\n        *   **Out-of-KG Task:** Question Answering (likely involving entity retrieval or relation prediction based on questions).\n    *   **Key Performance Metrics & Comparison Results:** The paper states that MorsE \"significantly outperforms corresponding baselines\" for both in-KG and out-of-KG tasks in inductive settings. This indicates superior performance in generating reasonable and effective embeddings for unseen entities compared to existing methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper does not explicitly list technical limitations in the provided abstract/introduction. However, it implicitly assumes that structural information (neighboring relations and multi-hop structures) is sufficient for cognizing new entities, without relying on additional features like textual descriptions. While framed as a strength for generality, this could be a limitation in extremely sparse KGs where structural patterns are minimal.\n    *   **Scope of Applicability:** MorsE is designed for inductive settings where a model trained on source KGs needs to generalize to *target KGs with entities entirely unseen* during training. It aims to provide general entity embeddings for these unseen entities, making it applicable to a wide array of in-KG (e.g., link prediction) and out-of-KG (e.g., question answering) tasks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** `\\cite{chen2021}` significantly advances the state-of-the-art by providing the first general inductive KGE framework that produces *entity embeddings* for unseen entities. This addresses a critical gap left by previous inductive methods that focused solely on relation prediction.\n    *   **Potential Impact on Future Research:**\n        *   Opens new avenues for research in dynamic and evolving KGs, enabling KGE models to adapt to new entities and domains without complete retraining.\n        *   Encourages further exploration of meta-learning techniques for transferring structural knowledge in graph-structured data.\n        *   Could inspire the development of more sophisticated meta-knowledge modeling techniques that integrate other forms of information (e.g., textual, temporal) in an entity-independent manner.",
        "keywords": [
          "Inductive Knowledge Graph Embedding (KGE)",
          "Meta-knowledge transfer",
          "MorsE",
          "Entity embeddings",
          "Meta-learning framework",
          "Unseen entities generalization",
          "Entity Initializer",
          "GNN Modulator",
          "Graph Neural Networks (GNN)",
          "Dynamic Knowledge Graphs",
          "Link Prediction",
          "Question Answering",
          "Relation-domain/range embeddings"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "8b717c4dfb309638307fcc7d2c798b1c20927a3e.pdf"
    },
    {
      "success": true,
      "doc_id": "64772c7824918a5e1c18a591de9506ed",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis: Knowformer for Knowledge Graph Embedding \\cite{li2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Standard Transformer architectures, particularly their self-attention (SA) mechanism, struggle with Knowledge Graph (KG) embedding. SA's invariance to input token order prevents it from distinguishing a valid (subject-relation-object) triple from its semantically incorrect, shuffled variants (e.g., object-relation-subject).\n    *   **Importance & Challenge**: This order-invariance leads to training inconsistency and a failure to capture the correct relational semantics, thus limiting the exploitation of Transformers' proven capacity for KG embedding despite their success in other language and vision tasks.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself against existing Transformer applications that have not fully exploited their capacity for KG embedding due to the inherent order-invariance of self-attention. It builds upon the general success of Transformers while addressing a fundamental architectural mismatch for structured relational data like KGs.\n    *   **Limitations of Previous Solutions**: Previous Transformer-based approaches for KGs implicitly suffer from the inability to distinguish entity roles (subject vs. object) within a relation, leading to an incorrect capture of relational semantics. This paper directly tackles this core limitation.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Knowformer**, a novel Transformer architecture for KG embedding. Its core innovation lies in incorporating \"relational compositions\" into entity representations.\n    *   **Novelty**:\n        *   **Relational Compositions**: These are operators on a relation and one of its entities (e.g., relation and object for a subject entity) designed to explicitly inject semantics and capture the role (subject or object) of an entity based on its position within a relation triple. Ideas from translational and semantic-matching embedding techniques are borrowed for their design.\n        *   **Residual Block Integration**: A carefully designed residual block is used to integrate these relational compositions into the self-attention mechanism, ensuring efficient, layer-by-layer propagation of the composed relational semantics.\n\n4.  **Key Technical Contributions**\n    *   **Novel Architecture**: Introduction of **Knowformer**, a Transformer variant specifically designed to overcome the order-invariance problem for KG embedding.\n    *   **Semantic Injection Mechanism**: Development of \"relational compositions\" to explicitly encode entity roles and relational semantics into entity representations.\n    *   **Efficient Integration**: Design of a residual block for seamless and effective integration of relational compositions into the Transformer's self-attention layers.\n    *   **Theoretical Insight**: Formal proof demonstrating that the self-attention mechanism, when augmented with relational compositions, can correctly distinguish entity roles in different positions and accurately capture relational semantics.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on six benchmark datasets.\n    *   **Key Performance Metrics & Results**: Knowformer was evaluated on two crucial KG tasks:\n        *   **Link Prediction**: Predicting missing links (relations) between entities.\n        *   **Entity Alignment**: Identifying equivalent entities across different KGs.\n    *   **Comparison Results**: The experiments demonstrate that Knowformer achieves state-of-the-art (SOTA) performance on both link prediction and entity alignment tasks, outperforming existing methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the order-invariance of self-attention for KG triples. While it successfully mitigates this, the text does not explicitly state new limitations introduced by Knowformer itself. The design assumes that relational compositions, derived from translational and semantic-matching ideas, are sufficient to capture the necessary semantics.\n    *   **Scope of Applicability**: Knowformer is specifically designed for knowledge graph embedding tasks, particularly link prediction and entity alignment, where understanding the directional and positional roles of entities within triples is critical.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{li2023} significantly advances the technical state-of-the-art in KG embedding by successfully adapting the powerful Transformer architecture to handle the unique structural and semantic challenges of knowledge graphs. It resolves a fundamental limitation of applying vanilla Transformers to relational data.\n    *   **Potential Impact on Future Research**: This work opens new avenues for applying Transformer-based models to other structured data types where positional or directional semantics are crucial. It provides a blueprint for how to inject domain-specific structural information into general-purpose attention mechanisms, potentially inspiring further research into more robust and semantically aware graph neural networks and Transformer variants for complex data.",
      "intriguing_abstract": "Transformers have revolutionized AI, yet their inherent order-invariance poses a critical challenge for Knowledge Graph (KG) embedding, preventing them from accurately capturing the directional semantics of relational triples. This fundamental architectural mismatch leads to inconsistent training and limits their exploitation for structured data. We introduce **Knowformer**, a novel Transformer architecture specifically engineered to overcome this limitation.\n\nKnowformer ingeniously integrates \"relational compositions\" directly into entity representations. These compositions, acting as semantic operators, explicitly inject crucial role-based information (subject vs. object) into the self-attention mechanism, ensuring consistent and semantically correct learning. A carefully designed residual block seamlessly propagates these rich relational semantics across layers, supported by a formal proof demonstrating its ability to distinguish entity roles. Extensive experiments on six benchmark datasets confirm Knowformer's superior performance, achieving state-of-the-art (SOTA) results in both link prediction and entity alignment. This work not only resolves a long-standing architectural mismatch but also unlocks the full potential of Transformers for structured relational data, paving the way for more robust and semantically aware graph neural networks.",
      "keywords": [
        "Knowformer",
        "Knowledge Graph embedding",
        "Transformer architectures",
        "self-attention mechanism",
        "order-invariance",
        "relational compositions",
        "semantic injection",
        "residual block integration",
        "link prediction",
        "entity alignment",
        "state-of-the-art performance",
        "distinguishing entity roles",
        "structured relational data"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/29052ddd048acb1afa2c42613068b63bb7428a34.pdf",
      "citation_key": "li2023",
      "metadata": {
        "title": "Position-Aware Relational Transformer for Knowledge Graph Embedding",
        "authors": [
          "Guang-pu Li",
          "Zequn Sun",
          "Wei Hu",
          "Gong Cheng",
          "Yuzhong Qu"
        ],
        "published_date": "2023",
        "abstract": "Although Transformer has achieved success in language and vision tasks, its capacity for knowledge graph (KG) embedding has not been fully exploited. Using the self-attention (SA) mechanism in Transformer to model the subject-relation-object triples in KGs suffers from training inconsistency as SA is invariant to the order of input tokens. As a result, it is unable to distinguish a (real) relation triple from its shuffled (fake) variants (e.g., object-relation-subject) and, thus, fails to capture the correct semantics. To cope with this issue, we propose a novel Transformer architecture, namely, Knowformer, for KG embedding. It incorporates relational compositions in entity representations to explicitly inject semantics and capture the role of an entity based on its position (subject or object) in a relation triple. The relational composition for a subject (or object) entity of a relation triple refers to an operator on the relation and the object (or subject). We borrow ideas from the typical translational and semantic-matching embedding techniques to design relational compositions. We carefully design a residual block to integrate relational compositions into SA and efficiently propagate the composed relational semantics layer by layer. We formally prove that the SA with relational compositions is able to distinguish the entity roles in different positions and correctly capture relational semantics. Extensive experiments and analyses on six benchmark datasets show that Knowformer achieves state-of-the-art performance on both link prediction and entity alignment.",
        "file_path": "paper_data/knowledge_graph_embedding/29052ddd048acb1afa2c42613068b63bb7428a34.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis: Knowformer for Knowledge Graph Embedding \\cite{li2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Standard Transformer architectures, particularly their self-attention (SA) mechanism, struggle with Knowledge Graph (KG) embedding. SA's invariance to input token order prevents it from distinguishing a valid (subject-relation-object) triple from its semantically incorrect, shuffled variants (e.g., object-relation-subject).\n    *   **Importance & Challenge**: This order-invariance leads to training inconsistency and a failure to capture the correct relational semantics, thus limiting the exploitation of Transformers' proven capacity for KG embedding despite their success in other language and vision tasks.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself against existing Transformer applications that have not fully exploited their capacity for KG embedding due to the inherent order-invariance of self-attention. It builds upon the general success of Transformers while addressing a fundamental architectural mismatch for structured relational data like KGs.\n    *   **Limitations of Previous Solutions**: Previous Transformer-based approaches for KGs implicitly suffer from the inability to distinguish entity roles (subject vs. object) within a relation, leading to an incorrect capture of relational semantics. This paper directly tackles this core limitation.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Knowformer**, a novel Transformer architecture for KG embedding. Its core innovation lies in incorporating \"relational compositions\" into entity representations.\n    *   **Novelty**:\n        *   **Relational Compositions**: These are operators on a relation and one of its entities (e.g., relation and object for a subject entity) designed to explicitly inject semantics and capture the role (subject or object) of an entity based on its position within a relation triple. Ideas from translational and semantic-matching embedding techniques are borrowed for their design.\n        *   **Residual Block Integration**: A carefully designed residual block is used to integrate these relational compositions into the self-attention mechanism, ensuring efficient, layer-by-layer propagation of the composed relational semantics.\n\n4.  **Key Technical Contributions**\n    *   **Novel Architecture**: Introduction of **Knowformer**, a Transformer variant specifically designed to overcome the order-invariance problem for KG embedding.\n    *   **Semantic Injection Mechanism**: Development of \"relational compositions\" to explicitly encode entity roles and relational semantics into entity representations.\n    *   **Efficient Integration**: Design of a residual block for seamless and effective integration of relational compositions into the Transformer's self-attention layers.\n    *   **Theoretical Insight**: Formal proof demonstrating that the self-attention mechanism, when augmented with relational compositions, can correctly distinguish entity roles in different positions and accurately capture relational semantics.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on six benchmark datasets.\n    *   **Key Performance Metrics & Results**: Knowformer was evaluated on two crucial KG tasks:\n        *   **Link Prediction**: Predicting missing links (relations) between entities.\n        *   **Entity Alignment**: Identifying equivalent entities across different KGs.\n    *   **Comparison Results**: The experiments demonstrate that Knowformer achieves state-of-the-art (SOTA) performance on both link prediction and entity alignment tasks, outperforming existing methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the order-invariance of self-attention for KG triples. While it successfully mitigates this, the text does not explicitly state new limitations introduced by Knowformer itself. The design assumes that relational compositions, derived from translational and semantic-matching ideas, are sufficient to capture the necessary semantics.\n    *   **Scope of Applicability**: Knowformer is specifically designed for knowledge graph embedding tasks, particularly link prediction and entity alignment, where understanding the directional and positional roles of entities within triples is critical.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{li2023} significantly advances the technical state-of-the-art in KG embedding by successfully adapting the powerful Transformer architecture to handle the unique structural and semantic challenges of knowledge graphs. It resolves a fundamental limitation of applying vanilla Transformers to relational data.\n    *   **Potential Impact on Future Research**: This work opens new avenues for applying Transformer-based models to other structured data types where positional or directional semantics are crucial. It provides a blueprint for how to inject domain-specific structural information into general-purpose attention mechanisms, potentially inspiring further research into more robust and semantically aware graph neural networks and Transformer variants for complex data.",
        "keywords": [
          "Knowformer",
          "Knowledge Graph embedding",
          "Transformer architectures",
          "self-attention mechanism",
          "order-invariance",
          "relational compositions",
          "semantic injection",
          "residual block integration",
          "link prediction",
          "entity alignment",
          "state-of-the-art performance",
          "distinguishing entity roles",
          "structured relational data"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "29052ddd048acb1afa2c42613068b63bb7428a34.pdf"
    },
    {
      "success": true,
      "doc_id": "7f29b80519d22edecc82927375e702c6",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Developing an effective Knowledge Graph Question Answering (KGQA) system specifically tailored for the complex domain of chemistry \\cite{zhou2023}.\n    *   **Importance and challenge**: The problem is crucial for providing fact-oriented information retrieval in chemistry-related research and industrial applications. Challenges include handling deep ontologies, numerical filtering questions, intricate chemical reaction mechanisms, and ensuring robust entity linking within a specialized vocabulary.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: The paper positions its system as novel by operating on multiple embedding spaces and querying them in parallel, \"unlike other existing designs\" \\cite{zhou2023}.\n    *   **Limitations of previous solutions**: Implied limitations of prior KGQA systems include a lack of integrated multi-embedding space operation, less robust handling of deep chemical ontologies, numerical questions, and specific chemical reaction mechanisms, as well as potentially less accurate entity linking.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: The system is built upon hybrid knowledge graph embeddings, designed to operate on multiple embedding spaces using various embedding methods, which are queried in parallel \\cite{zhou2023}.\n    *   **Novelty**:\n        *   **Hybrid Multi-Embedding Space Architecture**: Queries diverse embedding spaces in parallel to leverage different embedding strengths.\n        *   **Score Alignment Model**: Adjusts and reranks answers from multiple embedding spaces for consolidated results.\n        *   **Implicit Multihop Relation Algorithm**: Derives complex, implicit multihop relations to navigate deep ontologies and improve multihop question answering.\n        *   **BERT-based Bidirectional Entity-Linking Model**: Enhances the robustness and accuracy of entity linking within the chemical domain.\n        *   **Joint Numerical Embedding Model**: Efficiently handles numerical filtering questions.\n        *   **Semantic Agents**: Enables autonomous dynamic calculations.\n        *   **Semantic Parsing for Chemical Reactions**: Specifically handles numerous chemical reaction mechanisms, supported by a Linked Data Fragment server.\n\n4.  **Key Technical Contributions**\n    *   A novel KGQA system architecture for chemistry, integrating hybrid knowledge graph embeddings with parallel querying across multiple embedding spaces \\cite{zhou2023}.\n    *   Introduction of a score alignment model for effective answer reranking from diverse embedding sources.\n    *   An algorithm for deriving implicit multihop relations, addressing complexities of deep ontologies.\n    *   A BERT-based bidirectional entity-linking model tailored for chemical entities.\n    *   A joint numerical embedding model for efficient numerical question answering.\n    *   Integration of semantic agents for dynamic, autonomous calculations.\n    *   A semantic parsing approach for handling complex chemical reaction mechanisms.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: The paper evaluates the accuracy of *each module* within the KGQA system \\cite{zhou2023}.\n    *   **Key performance metrics and comparison results**: Evaluation was performed using a dedicated chemistry question dataset. While specific quantitative results (e.g., F1 scores, precision, recall) are not detailed in the provided text, the focus was on assessing the \"accuracy\" of individual modules.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The provided text does not explicitly state limitations. However, the highly specialized nature of the system for chemistry suggests potential challenges in direct generalization to vastly different domains without significant adaptation. The evaluation focuses on module-level accuracy, which may not fully capture end-to-end system performance or user experience.\n    *   **Scope of applicability**: The system is primarily designed for fact-oriented information retrieval within chemistry-related research and industrial applications \\cite{zhou2023}.\n\n7.  **Technical Significance**\n    *   **Advancement of the technical state-of-the-art**: This work significantly advances KGQA by providing a highly specialized, modular, and robust system for the challenging domain of chemistry \\cite{zhou2023}. It integrates and innovates upon multiple cutting-edge techniques (hybrid embeddings, BERT, semantic parsing, semantic agents) to address domain-specific complexities.\n    *   **Potential impact on future research**: The system's modular design and specialized components for handling deep ontologies, numerical data, and reaction mechanisms could serve as a strong foundation and inspiration for future domain-specific KGQA systems, particularly in scientific and technical fields. It demonstrates a comprehensive approach to building intelligent systems for complex knowledge domains.",
      "intriguing_abstract": "Navigating the vast and intricate landscape of chemical knowledge graphs presents a formidable challenge for traditional question answering systems. We introduce a novel Knowledge Graph Question Answering (KGQA) system specifically engineered for the complex domain of chemistry, addressing critical needs in research and industrial applications. Our architecture pioneers a hybrid multi-embedding space approach, querying diverse embedding methods in parallel to leverage their distinct strengths.\n\nThis system integrates a sophisticated score alignment model for consolidated results, an implicit multihop relation algorithm to navigate deep ontologies, and a BERT-based bidirectional entity-linking model robustly tailored for chemical entities. Further innovations include a joint numerical embedding model for efficient numerical filtering, semantic agents for autonomous calculations, and advanced semantic parsing for intricate chemical reaction mechanisms. Validated on a dedicated chemistry dataset, this modular and robust system significantly advances the state-of-the-art in domain-specific KGQA, promising to revolutionize fact-oriented information retrieval and intelligent knowledge access in scientific and industrial applications.",
      "keywords": [
        "Knowledge Graph Question Answering (KGQA)",
        "Chemistry domain",
        "Hybrid knowledge graph embeddings",
        "Multiple embedding spaces",
        "Parallel querying",
        "Score alignment model",
        "Implicit multihop relations",
        "BERT-based entity linking",
        "Joint numerical embedding model",
        "Semantic parsing for chemical reactions",
        "Semantic agents",
        "Deep ontologies",
        "Fact-oriented information retrieval"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff.pdf",
      "citation_key": "zhou2023",
      "metadata": {
        "title": "Marie and BERTA Knowledge Graph Embedding Based Question Answering System for Chemistry",
        "authors": [
          "Xiaochi Zhou",
          "Shaocong Zhang",
          "Mehal Agarwal",
          "J. Akroyd",
          "S. Mosbach",
          "Markus Kraft"
        ],
        "published_date": "2023",
        "abstract": "This paper presents a novel knowledge graph question answering (KGQA) system for chemistry, which is implemented on hybrid knowledge graph embeddings, aiming to provide fact-oriented information retrieval for chemistry-related research and industrial applications. Unlike other existing designs, the system operates on multiple embedding spaces, which use various embedding methods and queries the embedding spaces in parallel. With the answers returned from multiple embedding spaces, the system leverages a score alignment model to adjust the answer scores and rerank the answers. Further, the system implements an algorithm to derive implicit multihop relations to handle the complexities of deep ontologies and improve multihop question answering. The system also implements a BERT-based bidirectional entity-linking model to enhance the robustness and accuracy of the entity-linking module. The system uses a joint numerical embedding model to efficiently handle numerical filtering questions. Further, it can invoke semantic agents to perform dynamic calculations autonomously. Finally, the KGQA system handles numerous chemical reaction mechanisms using semantic parsing supported by a Linked Data Fragment server. This paper evaluates the accuracy of each module within the KGQA system with a chemistry question data set.",
        "file_path": "paper_data/knowledge_graph_embedding/23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff.pdf",
        "venue": "ACS Omega",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Developing an effective Knowledge Graph Question Answering (KGQA) system specifically tailored for the complex domain of chemistry \\cite{zhou2023}.\n    *   **Importance and challenge**: The problem is crucial for providing fact-oriented information retrieval in chemistry-related research and industrial applications. Challenges include handling deep ontologies, numerical filtering questions, intricate chemical reaction mechanisms, and ensuring robust entity linking within a specialized vocabulary.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: The paper positions its system as novel by operating on multiple embedding spaces and querying them in parallel, \"unlike other existing designs\" \\cite{zhou2023}.\n    *   **Limitations of previous solutions**: Implied limitations of prior KGQA systems include a lack of integrated multi-embedding space operation, less robust handling of deep chemical ontologies, numerical questions, and specific chemical reaction mechanisms, as well as potentially less accurate entity linking.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: The system is built upon hybrid knowledge graph embeddings, designed to operate on multiple embedding spaces using various embedding methods, which are queried in parallel \\cite{zhou2023}.\n    *   **Novelty**:\n        *   **Hybrid Multi-Embedding Space Architecture**: Queries diverse embedding spaces in parallel to leverage different embedding strengths.\n        *   **Score Alignment Model**: Adjusts and reranks answers from multiple embedding spaces for consolidated results.\n        *   **Implicit Multihop Relation Algorithm**: Derives complex, implicit multihop relations to navigate deep ontologies and improve multihop question answering.\n        *   **BERT-based Bidirectional Entity-Linking Model**: Enhances the robustness and accuracy of entity linking within the chemical domain.\n        *   **Joint Numerical Embedding Model**: Efficiently handles numerical filtering questions.\n        *   **Semantic Agents**: Enables autonomous dynamic calculations.\n        *   **Semantic Parsing for Chemical Reactions**: Specifically handles numerous chemical reaction mechanisms, supported by a Linked Data Fragment server.\n\n4.  **Key Technical Contributions**\n    *   A novel KGQA system architecture for chemistry, integrating hybrid knowledge graph embeddings with parallel querying across multiple embedding spaces \\cite{zhou2023}.\n    *   Introduction of a score alignment model for effective answer reranking from diverse embedding sources.\n    *   An algorithm for deriving implicit multihop relations, addressing complexities of deep ontologies.\n    *   A BERT-based bidirectional entity-linking model tailored for chemical entities.\n    *   A joint numerical embedding model for efficient numerical question answering.\n    *   Integration of semantic agents for dynamic, autonomous calculations.\n    *   A semantic parsing approach for handling complex chemical reaction mechanisms.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: The paper evaluates the accuracy of *each module* within the KGQA system \\cite{zhou2023}.\n    *   **Key performance metrics and comparison results**: Evaluation was performed using a dedicated chemistry question dataset. While specific quantitative results (e.g., F1 scores, precision, recall) are not detailed in the provided text, the focus was on assessing the \"accuracy\" of individual modules.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The provided text does not explicitly state limitations. However, the highly specialized nature of the system for chemistry suggests potential challenges in direct generalization to vastly different domains without significant adaptation. The evaluation focuses on module-level accuracy, which may not fully capture end-to-end system performance or user experience.\n    *   **Scope of applicability**: The system is primarily designed for fact-oriented information retrieval within chemistry-related research and industrial applications \\cite{zhou2023}.\n\n7.  **Technical Significance**\n    *   **Advancement of the technical state-of-the-art**: This work significantly advances KGQA by providing a highly specialized, modular, and robust system for the challenging domain of chemistry \\cite{zhou2023}. It integrates and innovates upon multiple cutting-edge techniques (hybrid embeddings, BERT, semantic parsing, semantic agents) to address domain-specific complexities.\n    *   **Potential impact on future research**: The system's modular design and specialized components for handling deep ontologies, numerical data, and reaction mechanisms could serve as a strong foundation and inspiration for future domain-specific KGQA systems, particularly in scientific and technical fields. It demonstrates a comprehensive approach to building intelligent systems for complex knowledge domains.",
        "keywords": [
          "Knowledge Graph Question Answering (KGQA)",
          "Chemistry domain",
          "Hybrid knowledge graph embeddings",
          "Multiple embedding spaces",
          "Parallel querying",
          "Score alignment model",
          "Implicit multihop relations",
          "BERT-based entity linking",
          "Joint numerical embedding model",
          "Semantic parsing for chemical reactions",
          "Semantic agents",
          "Deep ontologies",
          "Fact-oriented information retrieval"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff.pdf"
    },
    {
      "success": true,
      "doc_id": "020281a109dc2ba733a4c097c4c12b8c",
      "summary": "Here's a focused summary of the paper \"OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding\" by Xiang et al. \\cite{xiang2021} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing embedding-based entity alignment (EA) methods for knowledge graphs (KGs) often produce incorrect mappings due to ignoring the ontological schema (classes, class hierarchies, and logical constraints like class disjointness). These errors, termed \"class conflicts,\" are a significant source of false positives in EA.\n    *   **Importance and Challenge**: KGs are crucial but often incomplete and heterogeneous, necessitating EA for comprehensive usability. Current methods, while utilizing graph structure, entity names, and attributes, overlook the critical meta-information within ontologies. For instance, \\cite{xiang2021} found that 42.2% and 55.7% of wrongly predicted mappings in the EN-FR-15K-V1 benchmark by BootEA and RSN4EA, respectively, were class-conflicted. The challenges in leveraging ontologies include: (1) the difficulty of jointly embedding KGs and their associated ontologies into a unified space, and (2) the fact that class conflicts (e.g., disjointness) are often not explicitly defined in ontologies and can vary contextually.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: Prior embedding-based EA methods primarily focus on embedding KGs by utilizing their graph structure, entity names, and attributes. Examples include MTransE, JAPE, SEA, BootEA, GCNAlign, AliNet, and RSN4EA.\n    *   **Limitations of Previous Solutions**: These methods universally *ignore* the ontological schema, which contains vital meta-information such as classes, their hierarchical relationships, and logical constraints (e.g., class disjointness). This oversight leads to the aforementioned \"class conflict\" errors, where entities belonging to semantically incompatible classes are incorrectly aligned.\n    *   **Positioning**: OntoEA \\cite{xiang2021} is presented as the first method to effectively utilize both ontology information and embedding techniques for KG alignment, directly addressing the limitations of prior work by integrating ontological semantics into the alignment process.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: OntoEA proposes an ontology-guided entity alignment method that jointly embeds KGs and their associated ontologies. It leverages class hierarchy and class disjointness to enrich semantic embeddings and prevent false mappings.\n    *   **Novelty/Difference**:\n        *   **Joint Embedding Framework**: Integrates five modules: entity embedding, ontology embedding, confliction loss, membership loss, and alignment loss, enabling simultaneous learning of KG and ontology representations.\n        *   **Class Conflict Matrix (CCM)**: A novel mechanism to represent inter-class conflicts. It captures explicitly defined disjointness, implicitly indicated conflicts via class hierarchy distance, and conflicts deduced from common entity members or seed mappings.\n        *   **Non-linear Ontology Embedding**: Employs a non-linear transformation (tanh) for embedding class hierarchical structures, which is more suitable for transitive relations like `subClassOf` than traditional translation-based models (e.g., TransE).\n        *   **Membership Embedding**: Uses a non-linear transformation to map entity embeddings to the ontology embedding space, explicitly linking entities to their classes.\n        *   **Iterative Co-Training Strategy**: Optimizes the combined loss function in an iterative manner, enhancing model convergence and reducing complexity.\n        *   **Weighted Similarity for Prediction**: Combines cosine similarities of both entity embeddings and their corresponding class embeddings for robust alignment prediction.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The **Class Conflict Matrix (CCM)** and its associated **Confliction Loss** (minimizing negative log-likelihood based on CCM and cosine similarity of class embeddings) for explicitly modeling and learning inter-class conflicts, including implicit ones.\n        *   A **non-linear transformation-based ontology embedding module** designed to handle the transitive nature of `subClassOf` relations effectively.\n        *   A **membership embedding module** that uses non-linear transformations to bridge the entity and class embedding spaces, enriching KG embeddings with ontological semantics.\n        *   An **iterative co-training strategy** for optimizing the multi-component loss function.\n    *   **System Design/Architectural Innovations**: The modular framework that seamlessly integrates diverse knowledge sources (KG triples, ontology hierarchy, class disjointness, entity-class memberships, and seed alignments) into a unified embedding space.\n    *   **Theoretical Insights/Analysis**: The recognition that class conflicts are a significant and quantifiable source of error in EA, and that these conflicts can be systematically modeled and learned, even when not explicitly defined, through a combination of explicit constraints and structural heuristics (e.g., class hierarchy distance).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on seven benchmarks: six popular public EA benchmarks (EN-FR-15K-V1/V2, EN-DE-15K-V1/V2, D-W-15K-V1/V2) and a new, challenging industrial benchmark (MED-BBK-9K). The authors also extended these benchmarks by extracting and appending ontologies and membership relationships. For benchmarks with non-shared ontologies, ontology alignment was performed using either manual annotation or the PARIS system.\n    *   **Key Performance Metrics and Comparison Results**: OntoEA was compared against state-of-the-art EA models, including translation-based (MTransE, JAPE, SEA, BootEA), graph neural network-based (GCNAlign, AliNet), and recurrent neural network-based (RSN4EA) approaches, as well as models utilizing entity surface information.\n        *   OntoEA consistently **outperformed all state-of-the-art baselines** across all seven benchmarks.\n        *   Notably, it achieved **over 35% higher Hits@1, Hits@5, and MRR** compared to the best baseline on the challenging MED-BBK-9K industrial benchmark.\n        *   Ablation studies confirmed the effectiveness of each individual module (ontology embedding, confliction loss, membership loss) in contributing to the overall performance.\n        *   The experiments validated the effectiveness of ontology guidance in both scenarios: when KGs share a common ontology and when they have separate ontologies (after pre-alignment).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Requires a small set of known entity mappings (seed mappings) for the alignment loss.\n        *   For KGs with separate ontologies, a pre-alignment step for the ontologies themselves is necessary, which can involve manual annotation or existing ontology alignment systems.\n        *   The entity embedding module currently uses TransE for simplicity and efficiency, though the framework is designed to be compatible with more advanced KG embedding methods.\n        *   For entities associated with multiple classes, the approach averages their class embeddings, which might be a simplification in complex scenarios.\n    *   **Scope of Applicability**: OntoEA is applicable to knowledge graphs that are accompanied by an ontological schema and membership relationships between entities and classes. It is versatile enough to handle scenarios where KGs share a single ontology or have distinct ontologies (provided they can be pre-aligned).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: OntoEA significantly advances the technical state-of-the-art in entity alignment by introducing the first comprehensive framework that effectively integrates ontological knowledge into embedding-based EA. It directly addresses a critical, previously overlooked source of error (class conflicts), leading to substantial performance improvements.\n    *   **Potential Impact on Future Research**:\n        *   Establishes a new paradigm for KG alignment, highlighting the crucial role of ontological context and opening new research directions for leveraging richer semantic information.\n        *   Provides a robust and extensible framework that can be adapted with more advanced KG embedding techniques or sophisticated ontology alignment methods.\n        *   Its success, particularly on an industrial benchmark, underscores its practical utility for real-world applications, especially in domain-specific areas (e.g., medical AI) where semantic consistency and accuracy are paramount.\n        *   The release of extended benchmarks with ontological information serves as a valuable resource for future research and comparative studies in ontology-guided EA.",
      "intriguing_abstract": "Integrating heterogeneous knowledge graphs (KGs) is crucial, yet existing embedding-based entity alignment (EA) methods often falter, producing erroneous mappings by critically overlooking their rich ontological schema. These \"class conflicts\"semantically incompatible entity alignmentsare a pervasive source of false positives, severely undermining KG usability.\n\nWe introduce OntoEA, a novel ontology-guided entity alignment framework that pioneers the joint embedding of KGs and their associated ontologies. OntoEA directly addresses class conflicts through a sophisticated modular design. Our core innovation, the **Class Conflict Matrix (CCM)**, systematically models inter-class conflictsexplicitly defined and implicitly derived from class hierarchiesto proactively prevent erroneous alignments. Coupled with novel non-linear ontology and membership embedding modules, OntoEA enriches semantic representations by explicitly linking entities to their classes and leveraging hierarchical structures.\n\nExtensive experiments on seven benchmarks, including a challenging industrial dataset, demonstrate OntoEA's superior performance, consistently outperforming state-of-the-art baselines. Notably, it achieved over 35% higher Hits@1 on the industrial benchmark. OntoEA establishes a new paradigm for robust, semantically consistent KG alignment, marking a vital step towards more accurate and reliable knowledge integration.",
      "keywords": [
        "Ontology-guided Entity Alignment",
        "Knowledge Graph Embedding",
        "Ontological Schema",
        "Class Conflicts",
        "Joint Embedding Framework",
        "Class Conflict Matrix (CCM)",
        "Confliction Loss",
        "Non-linear Ontology Embedding",
        "Membership Embedding",
        "Iterative Co-Training Strategy",
        "State-of-the-art performance",
        "Industrial benchmark validation"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/af051c87cecca64c2de4ad9110608f7579766653.pdf",
      "citation_key": "xiang2021",
      "metadata": {
        "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding",
        "authors": [
          "Yuejia Xiang",
          "Ziheng Zhang",
          "Jiaoyan Chen",
          "Xi Chen",
          "Zhenxi Lin",
          "Yefeng Zheng"
        ],
        "published_date": "2021",
        "abstract": "Semantic embedding has been widely investigated for aligning knowledge graph (KG) entities. Current methods have explored and utilized the graph structure, the entity names and attributes, but ignore the ontology (or ontological schema) which contains critical meta information such as classes and their membership relationships with entities. In this paper, we propose an ontology-guided entity alignment method named OntoEA, where both KGs and their ontologies are jointly embedded, and the class hierarchy and the class disjointness are utilized to avoid false mappings. Extensive experiments on seven public and industrial benchmarks have demonstrated the state-of-the-art performance of OntoEA and the effectiveness of the ontologies.",
        "file_path": "paper_data/knowledge_graph_embedding/af051c87cecca64c2de4ad9110608f7579766653.pdf",
        "venue": "Findings",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding\" by Xiang et al. \\cite{xiang2021} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing embedding-based entity alignment (EA) methods for knowledge graphs (KGs) often produce incorrect mappings due to ignoring the ontological schema (classes, class hierarchies, and logical constraints like class disjointness). These errors, termed \"class conflicts,\" are a significant source of false positives in EA.\n    *   **Importance and Challenge**: KGs are crucial but often incomplete and heterogeneous, necessitating EA for comprehensive usability. Current methods, while utilizing graph structure, entity names, and attributes, overlook the critical meta-information within ontologies. For instance, \\cite{xiang2021} found that 42.2% and 55.7% of wrongly predicted mappings in the EN-FR-15K-V1 benchmark by BootEA and RSN4EA, respectively, were class-conflicted. The challenges in leveraging ontologies include: (1) the difficulty of jointly embedding KGs and their associated ontologies into a unified space, and (2) the fact that class conflicts (e.g., disjointness) are often not explicitly defined in ontologies and can vary contextually.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: Prior embedding-based EA methods primarily focus on embedding KGs by utilizing their graph structure, entity names, and attributes. Examples include MTransE, JAPE, SEA, BootEA, GCNAlign, AliNet, and RSN4EA.\n    *   **Limitations of Previous Solutions**: These methods universally *ignore* the ontological schema, which contains vital meta-information such as classes, their hierarchical relationships, and logical constraints (e.g., class disjointness). This oversight leads to the aforementioned \"class conflict\" errors, where entities belonging to semantically incompatible classes are incorrectly aligned.\n    *   **Positioning**: OntoEA \\cite{xiang2021} is presented as the first method to effectively utilize both ontology information and embedding techniques for KG alignment, directly addressing the limitations of prior work by integrating ontological semantics into the alignment process.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: OntoEA proposes an ontology-guided entity alignment method that jointly embeds KGs and their associated ontologies. It leverages class hierarchy and class disjointness to enrich semantic embeddings and prevent false mappings.\n    *   **Novelty/Difference**:\n        *   **Joint Embedding Framework**: Integrates five modules: entity embedding, ontology embedding, confliction loss, membership loss, and alignment loss, enabling simultaneous learning of KG and ontology representations.\n        *   **Class Conflict Matrix (CCM)**: A novel mechanism to represent inter-class conflicts. It captures explicitly defined disjointness, implicitly indicated conflicts via class hierarchy distance, and conflicts deduced from common entity members or seed mappings.\n        *   **Non-linear Ontology Embedding**: Employs a non-linear transformation (tanh) for embedding class hierarchical structures, which is more suitable for transitive relations like `subClassOf` than traditional translation-based models (e.g., TransE).\n        *   **Membership Embedding**: Uses a non-linear transformation to map entity embeddings to the ontology embedding space, explicitly linking entities to their classes.\n        *   **Iterative Co-Training Strategy**: Optimizes the combined loss function in an iterative manner, enhancing model convergence and reducing complexity.\n        *   **Weighted Similarity for Prediction**: Combines cosine similarities of both entity embeddings and their corresponding class embeddings for robust alignment prediction.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The **Class Conflict Matrix (CCM)** and its associated **Confliction Loss** (minimizing negative log-likelihood based on CCM and cosine similarity of class embeddings) for explicitly modeling and learning inter-class conflicts, including implicit ones.\n        *   A **non-linear transformation-based ontology embedding module** designed to handle the transitive nature of `subClassOf` relations effectively.\n        *   A **membership embedding module** that uses non-linear transformations to bridge the entity and class embedding spaces, enriching KG embeddings with ontological semantics.\n        *   An **iterative co-training strategy** for optimizing the multi-component loss function.\n    *   **System Design/Architectural Innovations**: The modular framework that seamlessly integrates diverse knowledge sources (KG triples, ontology hierarchy, class disjointness, entity-class memberships, and seed alignments) into a unified embedding space.\n    *   **Theoretical Insights/Analysis**: The recognition that class conflicts are a significant and quantifiable source of error in EA, and that these conflicts can be systematically modeled and learned, even when not explicitly defined, through a combination of explicit constraints and structural heuristics (e.g., class hierarchy distance).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on seven benchmarks: six popular public EA benchmarks (EN-FR-15K-V1/V2, EN-DE-15K-V1/V2, D-W-15K-V1/V2) and a new, challenging industrial benchmark (MED-BBK-9K). The authors also extended these benchmarks by extracting and appending ontologies and membership relationships. For benchmarks with non-shared ontologies, ontology alignment was performed using either manual annotation or the PARIS system.\n    *   **Key Performance Metrics and Comparison Results**: OntoEA was compared against state-of-the-art EA models, including translation-based (MTransE, JAPE, SEA, BootEA), graph neural network-based (GCNAlign, AliNet), and recurrent neural network-based (RSN4EA) approaches, as well as models utilizing entity surface information.\n        *   OntoEA consistently **outperformed all state-of-the-art baselines** across all seven benchmarks.\n        *   Notably, it achieved **over 35% higher Hits@1, Hits@5, and MRR** compared to the best baseline on the challenging MED-BBK-9K industrial benchmark.\n        *   Ablation studies confirmed the effectiveness of each individual module (ontology embedding, confliction loss, membership loss) in contributing to the overall performance.\n        *   The experiments validated the effectiveness of ontology guidance in both scenarios: when KGs share a common ontology and when they have separate ontologies (after pre-alignment).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Requires a small set of known entity mappings (seed mappings) for the alignment loss.\n        *   For KGs with separate ontologies, a pre-alignment step for the ontologies themselves is necessary, which can involve manual annotation or existing ontology alignment systems.\n        *   The entity embedding module currently uses TransE for simplicity and efficiency, though the framework is designed to be compatible with more advanced KG embedding methods.\n        *   For entities associated with multiple classes, the approach averages their class embeddings, which might be a simplification in complex scenarios.\n    *   **Scope of Applicability**: OntoEA is applicable to knowledge graphs that are accompanied by an ontological schema and membership relationships between entities and classes. It is versatile enough to handle scenarios where KGs share a single ontology or have distinct ontologies (provided they can be pre-aligned).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: OntoEA significantly advances the technical state-of-the-art in entity alignment by introducing the first comprehensive framework that effectively integrates ontological knowledge into embedding-based EA. It directly addresses a critical, previously overlooked source of error (class conflicts), leading to substantial performance improvements.\n    *   **Potential Impact on Future Research**:\n        *   Establishes a new paradigm for KG alignment, highlighting the crucial role of ontological context and opening new research directions for leveraging richer semantic information.\n        *   Provides a robust and extensible framework that can be adapted with more advanced KG embedding techniques or sophisticated ontology alignment methods.\n        *   Its success, particularly on an industrial benchmark, underscores its practical utility for real-world applications, especially in domain-specific areas (e.g., medical AI) where semantic consistency and accuracy are paramount.\n        *   The release of extended benchmarks with ontological information serves as a valuable resource for future research and comparative studies in ontology-guided EA.",
        "keywords": [
          "Ontology-guided Entity Alignment",
          "Knowledge Graph Embedding",
          "Ontological Schema",
          "Class Conflicts",
          "Joint Embedding Framework",
          "Class Conflict Matrix (CCM)",
          "Confliction Loss",
          "Non-linear Ontology Embedding",
          "Membership Embedding",
          "Iterative Co-Training Strategy",
          "State-of-the-art performance",
          "Industrial benchmark validation"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "af051c87cecca64c2de4ad9110608f7579766653.pdf"
    },
    {
      "success": true,
      "doc_id": "eac47ae6354d7af2619843f360b1b4ee",
      "summary": "Here is a focused summary of the survey paper by \\cite{cao2022} for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey covers Knowledge Graph Embedding (KGE) techniques, specifically analyzing them from the novel perspective of their underlying representation spaces.\n    *   Its main objectives are to systematically review existing KGE methods based on mathematical representation spaces, summarize their properties, and provide guidance for designing new KGE models.\n\n2.  **Literature Coverage**\n    *   The survey provides a systematic review of \"existing KGE techniques,\" focusing on how different mathematical spaces are utilized.\n    *   It distinguishes itself from prior KGE surveys by adopting a unique classification methodology centered on mathematical space properties, rather than encoding models or applications.\n\n3.  **Classification Framework**\n    *   The survey organizes KGE models into a fine-grained classification based on three mathematical perspectives of their representation spaces:\n        *   Algebraic Structure\n        *   Geometric Structure\n        *   Analytical Structure\n\n4.  **Key Findings & Insights**\n    *   Different mathematical spaces possess unique strengths, enabling them to capture distinct relational and structural patterns within Knowledge Graphs.\n    *   The choice of representation space significantly influences the types of KG properties (e.g., chain, ring, hierarchy) that can be effectively modeled.\n    *   The survey explores how spatial advantages contribute to different embedding needs and analyzes experimental results from downstream tasks to highlight the benefits of specific mathematical spaces in various scenarios.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey identifies a significant gap in existing literature, noting the absence of a systematic review of KGE methods from the perspective of mathematical spaces.\n    *   It proposes promising research directions by leveraging unique properties of different mathematical spaces, encouraging researchers to consider these properties more deeply when designing KGE models and related applications.\n\n6.  **Survey Contribution**\n    *   This is the first comprehensive survey to establish a mathematical spatial architecture for KGE models, offering a novel and systematic understanding of their underlying principles.\n    *   It provides valuable guidance for researchers and practitioners in selecting appropriate representation spaces and designing more expressive KGE methods.",
      "intriguing_abstract": "Unlocking the true potential of Knowledge Graph Embedding (KGE) hinges on a profound understanding of their underlying representation spaces. While KGE techniques have proliferated, a systematic analysis from this fundamental mathematical perspective has remained elusive. This pioneering survey addresses this critical gap, offering the first comprehensive review that classifies existing KGE models based on the **algebraic, geometric, and analytical structures** of their embedding spaces.\n\nWe reveal how the inherent properties of these diverse mathematical spaces uniquely enable the capture of distinct relational and structural patterns within Knowledge Graphs, profoundly influencing their ability to model complex KG properties like hierarchies and chains. By systematically analyzing spatial advantages and their impact on downstream tasks, this work provides unprecedented clarity into why certain spaces excel in specific scenarios. This survey not only establishes a novel mathematical spatial architecture for KGE but also offers invaluable guidance for researchers to select optimal representation spaces and design more expressive KGE models, charting a clear roadmap for future innovation in the field.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "mathematical representation spaces",
        "systematic review",
        "classification framework",
        "algebraic structure",
        "geometric structure",
        "analytical structure",
        "relational and structural patterns",
        "KG properties",
        "novel classification methodology",
        "mathematical spatial architecture",
        "research gaps",
        "future directions",
        "designing KGE models"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/85064a4b1b96863af4fccff9ad34ce484945ad7b.pdf",
      "citation_key": "cao2022",
      "metadata": {
        "title": "Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces",
        "authors": [
          "Jiahang Cao",
          "Jinyuan Fang",
          "Zaiqiao Meng",
          "Shangsong Liang"
        ],
        "published_date": "2022",
        "abstract": "Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this article, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) algebraic perspective, (2) geometric perspective and (3) analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of mathematical space in different scenarios and the reasons behind them. We further state some promising research directions from a representation space perspective, with which we hope to inspire researchers to design their KGE models as well as their related applications with more consideration of their mathematical space properties.",
        "file_path": "paper_data/knowledge_graph_embedding/85064a4b1b96863af4fccff9ad34ce484945ad7b.pdf",
        "venue": "ACM Computing Surveys",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper by \\cite{cao2022} for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey covers Knowledge Graph Embedding (KGE) techniques, specifically analyzing them from the novel perspective of their underlying representation spaces.\n    *   Its main objectives are to systematically review existing KGE methods based on mathematical representation spaces, summarize their properties, and provide guidance for designing new KGE models.\n\n2.  **Literature Coverage**\n    *   The survey provides a systematic review of \"existing KGE techniques,\" focusing on how different mathematical spaces are utilized.\n    *   It distinguishes itself from prior KGE surveys by adopting a unique classification methodology centered on mathematical space properties, rather than encoding models or applications.\n\n3.  **Classification Framework**\n    *   The survey organizes KGE models into a fine-grained classification based on three mathematical perspectives of their representation spaces:\n        *   Algebraic Structure\n        *   Geometric Structure\n        *   Analytical Structure\n\n4.  **Key Findings & Insights**\n    *   Different mathematical spaces possess unique strengths, enabling them to capture distinct relational and structural patterns within Knowledge Graphs.\n    *   The choice of representation space significantly influences the types of KG properties (e.g., chain, ring, hierarchy) that can be effectively modeled.\n    *   The survey explores how spatial advantages contribute to different embedding needs and analyzes experimental results from downstream tasks to highlight the benefits of specific mathematical spaces in various scenarios.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey identifies a significant gap in existing literature, noting the absence of a systematic review of KGE methods from the perspective of mathematical spaces.\n    *   It proposes promising research directions by leveraging unique properties of different mathematical spaces, encouraging researchers to consider these properties more deeply when designing KGE models and related applications.\n\n6.  **Survey Contribution**\n    *   This is the first comprehensive survey to establish a mathematical spatial architecture for KGE models, offering a novel and systematic understanding of their underlying principles.\n    *   It provides valuable guidance for researchers and practitioners in selecting appropriate representation spaces and designing more expressive KGE methods.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "mathematical representation spaces",
          "systematic review",
          "classification framework",
          "algebraic structure",
          "geometric structure",
          "analytical structure",
          "relational and structural patterns",
          "KG properties",
          "novel classification methodology",
          "mathematical spatial architecture",
          "research gaps",
          "future directions",
          "designing KGE models"
        ],
        "is_new_direction": "1",
        "paper_type": "survey"
      },
      "file_name": "85064a4b1b96863af4fccff9ad34ce484945ad7b.pdf"
    },
    {
      "success": true,
      "doc_id": "d38ac0ac4188020e333bb823d1225408",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the challenge of knowledge graph embedding, which aims to represent entities and relations in low-dimensional vector spaces \\cite{wang2021}.\n    *   The problem is important because most existing methods are limited to focusing only on triple facts and struggle to fully represent complex relations, especially those based on simple translation or distance measurements \\cite{wang2021}.\n    *   The motivation is to leverage well-constructed prior knowledge, specifically entity types, to learn more semantic and robust representations \\cite{wang2021}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches primarily focus on triple facts, which limits their scope \\cite{wang2021}.\n    *   Previous solutions, particularly those based on translation or distance measurement, are insufficient for fully representing complex relations \\cite{wang2021}.\n    *   This work positions itself by addressing these limitations through the incorporation of entity type information.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is a novel knowledge graph embedding model named TransET \\cite{wang2021}.\n    *   TransET's innovation lies in its utilization of entity types to learn more semantic features \\cite{wang2021}.\n    *   Specifically, it employs **circle convolution** based on the embeddings of both entities and their types to map head and tail entities to **type-specific representations** \\cite{wang2021}.\n    *   A translation-based score function is then used to learn the representation of triples from these type-specific embeddings \\cite{wang2021}.\n\n*   **Key Technical Contributions**\n    *   **Novel Model**: Introduction of TransET, a new knowledge graph embedding model \\cite{wang2021}.\n    *   **Methodological Innovation**: Integration of entity types as prior knowledge to enrich entity and relation embeddings \\cite{wang2021}.\n    *   **Algorithmic Novelty**: Application of circle convolution to generate type-specific representations for entities, enhancing the capture of semantic features \\cite{wang2021}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted on real-world datasets \\cite{wang2021}.\n    *   The model was evaluated on two benchmark tasks: link prediction and triple classification \\cite{wang2021}.\n    *   **Key Performance Result**: TransET demonstrates superior performance, outperforming state-of-the-art models in most cases \\cite{wang2021}.\n\n*   **Limitations & Scope**\n    *   The provided abstract does not explicitly state technical limitations or assumptions of TransET.\n    *   The scope of applicability is knowledge graph embedding, particularly for tasks like link prediction and triple classification \\cite{wang2021}.\n\n*   **Technical Significance**\n    *   This work advances the technical state-of-the-art by providing a more semantically rich embedding approach that moves beyond simple triple facts \\cite{wang2021}.\n    *   By effectively incorporating entity types and using circle convolution, it offers a promising direction for better representing complex relations in knowledge graphs \\cite{wang2021}.\n    *   It has the potential to impact future research by encouraging the exploration of other forms of prior knowledge and advanced convolutional mechanisms for knowledge graph embedding.",
      "intriguing_abstract": "Unlocking the full potential of knowledge graphs hinges on robust, semantically rich entity and relation representations. Current knowledge graph embedding (KGE) models, often constrained by focusing solely on triple facts, struggle to capture the intricate nuances of complex relations, limiting their expressive power. We introduce **TransET**, a novel KGE model that fundamentally redefines how semantic features are learned by strategically leveraging well-constructed **entity types** as crucial prior knowledge.\n\nTransET's innovation lies in its unique architectural design: it employs **circle convolution** on both entity and type embeddings to generate highly discriminative **type-specific representations** for head and tail entities. This allows for a profound integration of contextual type information, moving beyond simple translation or distance measurements. Evaluated on challenging link prediction and triple classification benchmarks, TransET consistently achieves superior performance, significantly outperforming state-of-the-art methods. Our work offers a powerful paradigm for learning more robust and semantically enriched knowledge graph embeddings, paving the way for a deeper understanding and utilization of complex relational data.",
      "keywords": [
        "knowledge graph embedding",
        "TransET",
        "entity types",
        "circle convolution",
        "type-specific representations",
        "complex relations",
        "semantic features",
        "link prediction",
        "triple classification",
        "prior knowledge integration",
        "superior performance",
        "state-of-the-art models"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/06315f8b2633a54b087c6094cdb281f01dd06482.pdf",
      "citation_key": "wang2021",
      "metadata": {
        "title": "TransET: Knowledge Graph Embedding with Entity Types",
        "authors": [
          "Peng Wang",
          "Jing Zhou",
          "Yuzhang Liu",
          "Xing-Chun Zhou"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding aims to embed entities and relations into low-dimensional vector spaces. Most existing methods only focus on triple facts in knowledge graphs. In addition, models based on translation or distance measurement cannot fully represent complex relations. As well-constructed prior knowledge, entity types can be employed to learn the representations of entities and relations. In this paper, we propose a novel knowledge graph embedding model named TransET, which takes advantage of entity types to learn more semantic features. More specifically, circle convolution based on the embeddings of entity and entity types is utilized to map head entity and tail entity to type-specific representations, then translation-based score function is used to learn the presentation triples. We evaluated our model on real-world datasets with two benchmark tasks of link prediction and triple classification. Experimental results demonstrate that it outperforms state-of-the-art models in most cases.",
        "file_path": "paper_data/knowledge_graph_embedding/06315f8b2633a54b087c6094cdb281f01dd06482.pdf",
        "venue": "Electronics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the challenge of knowledge graph embedding, which aims to represent entities and relations in low-dimensional vector spaces \\cite{wang2021}.\n    *   The problem is important because most existing methods are limited to focusing only on triple facts and struggle to fully represent complex relations, especially those based on simple translation or distance measurements \\cite{wang2021}.\n    *   The motivation is to leverage well-constructed prior knowledge, specifically entity types, to learn more semantic and robust representations \\cite{wang2021}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches primarily focus on triple facts, which limits their scope \\cite{wang2021}.\n    *   Previous solutions, particularly those based on translation or distance measurement, are insufficient for fully representing complex relations \\cite{wang2021}.\n    *   This work positions itself by addressing these limitations through the incorporation of entity type information.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is a novel knowledge graph embedding model named TransET \\cite{wang2021}.\n    *   TransET's innovation lies in its utilization of entity types to learn more semantic features \\cite{wang2021}.\n    *   Specifically, it employs **circle convolution** based on the embeddings of both entities and their types to map head and tail entities to **type-specific representations** \\cite{wang2021}.\n    *   A translation-based score function is then used to learn the representation of triples from these type-specific embeddings \\cite{wang2021}.\n\n*   **Key Technical Contributions**\n    *   **Novel Model**: Introduction of TransET, a new knowledge graph embedding model \\cite{wang2021}.\n    *   **Methodological Innovation**: Integration of entity types as prior knowledge to enrich entity and relation embeddings \\cite{wang2021}.\n    *   **Algorithmic Novelty**: Application of circle convolution to generate type-specific representations for entities, enhancing the capture of semantic features \\cite{wang2021}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted on real-world datasets \\cite{wang2021}.\n    *   The model was evaluated on two benchmark tasks: link prediction and triple classification \\cite{wang2021}.\n    *   **Key Performance Result**: TransET demonstrates superior performance, outperforming state-of-the-art models in most cases \\cite{wang2021}.\n\n*   **Limitations & Scope**\n    *   The provided abstract does not explicitly state technical limitations or assumptions of TransET.\n    *   The scope of applicability is knowledge graph embedding, particularly for tasks like link prediction and triple classification \\cite{wang2021}.\n\n*   **Technical Significance**\n    *   This work advances the technical state-of-the-art by providing a more semantically rich embedding approach that moves beyond simple triple facts \\cite{wang2021}.\n    *   By effectively incorporating entity types and using circle convolution, it offers a promising direction for better representing complex relations in knowledge graphs \\cite{wang2021}.\n    *   It has the potential to impact future research by encouraging the exploration of other forms of prior knowledge and advanced convolutional mechanisms for knowledge graph embedding.",
        "keywords": [
          "knowledge graph embedding",
          "TransET",
          "entity types",
          "circle convolution",
          "type-specific representations",
          "complex relations",
          "semantic features",
          "link prediction",
          "triple classification",
          "prior knowledge integration",
          "superior performance",
          "state-of-the-art models"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "06315f8b2633a54b087c6094cdb281f01dd06482.pdf"
    },
    {
      "success": true,
      "doc_id": "12d4838a4aec7b9600f39f196bacee89",
      "summary": "Here's a focused summary of the technical paper \\cite{guo2020} for literature review:\n\n### Focused Summary for Literature Review: \\cite{guo2020}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of effectively and efficiently integrating soft logical rules into knowledge graph (KG) embedding models.\n    *   **Importance & Challenge**: Soft rules are highly beneficial for KG embedding despite their inherent uncertainty, but existing methods have not adequately explored their integration. The main challenge lies in devising a principled framework to incorporate this uncertain logical information into continuous vector space embeddings.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself by highlighting a gap in current KG embedding research, noting that soft rules, despite their benefits, \"have not been studied enough in recent methods.\"\n    *   **Limitations of Previous Solutions**: The implicit limitation of previous solutions is their inability to efficiently and effectively integrate soft logical information in a principled manner, leading to a missed opportunity for leveraging valuable uncertain knowledge.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{guo2020} proposes a scalable and effective method called Soft Logical Rule Embedding (SLRE) for preserving soft logical regularities.\n        *   It represents relations as bilinear forms.\n        *   Entity representations are mapped into a non-negative and bounded space.\n        *   A novel rule-based regularization is derived that directly enforces relation representations to satisfy constraints introduced by soft rules.\n    *   **Novelty**: The approach is novel because it directly regularizes relation representations, making the complexity of rule learning independent of the entity set size. This significantly improves scalability compared to methods that might regularize entities or require more complex rule integration.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of a principled framework for integrating soft logical rules into KG embeddings.\n    *   **System Design/Architectural Innovations**:\n        *   Representing relations as bilinear forms.\n        *   Mapping entity representations into a non-negative and bounded space.\n        *   A novel rule-based regularization mechanism that directly operates on relation representations.\n    *   **Scalability Improvement**: The regularization's complexity is independent of the entity set size, leading to improved scalability.\n    *   **Enhanced Knowledge Reasoning**: By imposing prior logical information upon the structure of the embedding space, the method is beneficial for knowledge reasoning tasks.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The effectiveness of the proposed approach was evaluated through link prediction tasks.\n    *   **Key Performance Metrics & Comparison Results**: The method demonstrated superior performance (\"effectiveness of our approach over many competitive baselines\") on standard KG datasets.\n    *   **Datasets**: Experiments were conducted on Freebase and DBpedia.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract does not explicitly detail specific technical limitations or assumptions beyond the inherent uncertainty of soft rules themselves, which the method aims to leverage.\n    *   **Scope of Applicability**: The method is applicable to knowledge graph embedding tasks where soft logical rules can be extracted or defined, particularly for improving link prediction and knowledge reasoning by incorporating uncertain logical regularities.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{guo2020} advances the technical state-of-the-art by providing a highly scalable and effective principled framework for integrating soft logical information into KG embedding models, a previously underexplored area.\n    *   **Potential Impact on Future Research**: This work opens avenues for future research in leveraging uncertain logical knowledge more effectively in various KG-related tasks, potentially leading to more robust and interpretable embedding models, especially in scenarios with incomplete or noisy knowledge bases.",
      "intriguing_abstract": "The vast potential of Knowledge Graph (KG) embeddings is often constrained by their inability to effectively leverage the nuanced, uncertain information embedded in soft logical rules. Existing methods struggle with principled and scalable integration of this invaluable, yet imprecise, knowledge. We introduce Soft Logical Rule Embedding (SLRE), a novel and highly scalable framework that directly addresses this critical gap.\n\nSLRE innovatively represents relations as bilinear forms and maps entity representations into a non-negative, bounded space. Its core contribution lies in a novel rule-based regularization mechanism that directly enforces constraints on *relation representations*, rather than entities. This groundbreaking design ensures that the complexity of rule learning remains independent of the entity set size, dramatically enhancing scalability for large KGs. By imposing these prior logical regularities, SLRE significantly improves knowledge reasoning and link prediction. Extensive experiments on Freebase and DBpedia demonstrate SLRE's superior performance, establishing a new state-of-the-art for integrating uncertain logical knowledge into KG embeddings.",
      "keywords": [
        "Knowledge graph embedding",
        "soft logical rules",
        "Soft Logical Rule Embedding (SLRE)",
        "principled framework",
        "integrating soft logical rules",
        "direct regularization of relation representations",
        "bilinear forms",
        "scalability improvement",
        "link prediction",
        "knowledge reasoning",
        "non-negative and bounded entity representations",
        "rule-based regularization"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0.pdf",
      "citation_key": "guo2020",
      "metadata": {
        "title": "Knowledge Graph Embedding Preserving Soft Logical Regularity",
        "authors": [
          "Shu Guo",
          "Lin Li",
          "Zhen Hui",
          "Lingshuai Meng",
          "Bingnan Ma",
          "Wei Liu",
          "Lihong Wang",
          "Haibin Zhai",
          "Hong Zhang"
        ],
        "published_date": "2020",
        "abstract": "Embedding knowledge graphs (KGs) into continuous vector spaces is currently an active research area. Soft rules, despite their uncertainty, are highly beneficial to KG embedding. However, they have not been studied enough in recent methods. A major challenge here is how to devise a principled framework, which efficiently and effectively integrates such soft logical information into embedding models. This paper proposes a highly scalable and effective method for preserving soft logical regularities by imposing soft rule constraints on relation representations. Specifically, we first represent relations as bilinear forms and map entity representations into a non-negative and bounded space. Then we derive a rule-based regularization that merely enforces relation representations to satisfy constraints introduced by soft rules. The proposed method has the following advantages: 1) it regularizes relations directly with the complexity of rule learning independent of entity set size, improving scalability; 2) it imposes prior logical information upon the structure of the embedding space, and would be beneficial to knowledge reasoning. Evaluation in link prediction on Freebase and DBpedia shows the effectiveness of our approach over many competitive baselines. Code and datasets are available at https://github.com/StudyGroup-lab/SLRE.",
        "file_path": "paper_data/knowledge_graph_embedding/a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper \\cite{guo2020} for literature review:\n\n### Focused Summary for Literature Review: \\cite{guo2020}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of effectively and efficiently integrating soft logical rules into knowledge graph (KG) embedding models.\n    *   **Importance & Challenge**: Soft rules are highly beneficial for KG embedding despite their inherent uncertainty, but existing methods have not adequately explored their integration. The main challenge lies in devising a principled framework to incorporate this uncertain logical information into continuous vector space embeddings.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself by highlighting a gap in current KG embedding research, noting that soft rules, despite their benefits, \"have not been studied enough in recent methods.\"\n    *   **Limitations of Previous Solutions**: The implicit limitation of previous solutions is their inability to efficiently and effectively integrate soft logical information in a principled manner, leading to a missed opportunity for leveraging valuable uncertain knowledge.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{guo2020} proposes a scalable and effective method called Soft Logical Rule Embedding (SLRE) for preserving soft logical regularities.\n        *   It represents relations as bilinear forms.\n        *   Entity representations are mapped into a non-negative and bounded space.\n        *   A novel rule-based regularization is derived that directly enforces relation representations to satisfy constraints introduced by soft rules.\n    *   **Novelty**: The approach is novel because it directly regularizes relation representations, making the complexity of rule learning independent of the entity set size. This significantly improves scalability compared to methods that might regularize entities or require more complex rule integration.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of a principled framework for integrating soft logical rules into KG embeddings.\n    *   **System Design/Architectural Innovations**:\n        *   Representing relations as bilinear forms.\n        *   Mapping entity representations into a non-negative and bounded space.\n        *   A novel rule-based regularization mechanism that directly operates on relation representations.\n    *   **Scalability Improvement**: The regularization's complexity is independent of the entity set size, leading to improved scalability.\n    *   **Enhanced Knowledge Reasoning**: By imposing prior logical information upon the structure of the embedding space, the method is beneficial for knowledge reasoning tasks.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The effectiveness of the proposed approach was evaluated through link prediction tasks.\n    *   **Key Performance Metrics & Comparison Results**: The method demonstrated superior performance (\"effectiveness of our approach over many competitive baselines\") on standard KG datasets.\n    *   **Datasets**: Experiments were conducted on Freebase and DBpedia.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract does not explicitly detail specific technical limitations or assumptions beyond the inherent uncertainty of soft rules themselves, which the method aims to leverage.\n    *   **Scope of Applicability**: The method is applicable to knowledge graph embedding tasks where soft logical rules can be extracted or defined, particularly for improving link prediction and knowledge reasoning by incorporating uncertain logical regularities.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{guo2020} advances the technical state-of-the-art by providing a highly scalable and effective principled framework for integrating soft logical information into KG embedding models, a previously underexplored area.\n    *   **Potential Impact on Future Research**: This work opens avenues for future research in leveraging uncertain logical knowledge more effectively in various KG-related tasks, potentially leading to more robust and interpretable embedding models, especially in scenarios with incomplete or noisy knowledge bases.",
        "keywords": [
          "Knowledge graph embedding",
          "soft logical rules",
          "Soft Logical Rule Embedding (SLRE)",
          "principled framework",
          "integrating soft logical rules",
          "direct regularization of relation representations",
          "bilinear forms",
          "scalability improvement",
          "link prediction",
          "knowledge reasoning",
          "non-negative and bounded entity representations",
          "rule-based regularization"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0.pdf"
    },
    {
      "success": true,
      "doc_id": "48ca5191f79b4bdd276e80926d3842fa",
      "summary": "Here's a focused summary of the paper by \\cite{zhang2024} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Federated Knowledge Graph Embedding (FKGE) learning faces significant challenges in communication efficiency due to the considerable size of parameters (entity embeddings) and the extensive number of communication rounds required for training.\n    *   **Importance and Challenge**: High communication overhead impedes the training process, especially with numerous clients, large KGs, and high embedding dimensions, conflicting with bandwidth-constrained wireless edge networks and costly data plans. Existing FKGE methods only address reducing communication *rounds* (e.g., via more local iterations) but fail to reduce the *size of parameters transmitted within each round*, leading to a sustained high communication load.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Existing FKGE methods (e.g., FedE \\cite{zhang2024}, FedEC \\cite{zhang2024}, FedLu \\cite{zhang2024}, FedR \\cite{zhang2024}) primarily focus on improving the quality of learned embeddings or reducing communication *rounds*. They are based on client-server or peer-to-peer architectures.\n    *   **Limitations of Previous Solutions**:\n        *   They do not address the problem of reducing the *size of transmitted parameters* per communication round.\n        *   Initial attempts by the authors to integrate model compression techniques like Knowledge Distillation (KD) and Low-Rank Approximation (LRA) into FKGE proved ineffective. These methods universally reduce embedding precision across *all* entities, significantly slowing convergence and increasing total communication costs, even at low compression ratios. This highlights the critical importance of maintaining embedding precision for effective FKGE.\n        *   Traditional sparsification methods in Federated Learning operate parameter-wise, which can corrupt the semantic integrity of entity embeddings in FKGE due to the inherent coherence of multiple parameters forming an embedding.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **FedS**, a bidirectional communication-efficient framework based on an **Entity-Wise Top-K Sparsification strategy** and an **Intermittent Synchronization Mechanism**.\n        *   **Upstream Entity-Wise Top-K Sparsification**: Clients dynamically identify and upload only the Top-K entity embeddings that exhibit the *greatest changes* (quantified by Cosine Similarity between current and history embeddings) to the server. This preserves the original precision of the selected entities.\n        *   **Downstream Personalized Entity-Wise Top-K Sparsification**: The server first performs personalized embedding aggregation for each client. Then, it identifies and transmits the Top-K aggregated embeddings back to each client, selecting based on *entity upload frequency* (rather than changes, due to FKG heterogeneity).\n        *   **Intermittent Synchronization Mechanism**: To mitigate negative effects of embedding inconsistency among shared entities caused by the heterogeneity of Federated Knowledge Graphs, FedS periodically (at fixed intervals) transmits *all* parameters between clients and the server.\n    *   **Novelty**:\n        *   First attempt to mitigate FKGE communication overhead by reducing the *size of transmitted parameters per communication round*.\n        *   Introduces a novel **Entity-Wise Top-K Sparsification strategy** that operates on entire entity embeddings, preserving their semantic integrity, unlike previous parameter-wise sparsification methods.\n        *   The bidirectional sparsification (upstream and downstream) combined with personalized aggregation and the intermittent synchronization mechanism specifically addresses the unique challenges of FKGE heterogeneity.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insight/Finding**: Demonstrated through extensive experiments that universal reduction in embedding precision (e.g., via KD or LRA) across all entities significantly impedes convergence speed in FKGE, underscoring the importance of maintaining embedding precision for critical entities.\n    *   **Novel Algorithms/Methods**:\n        *   **FedS framework**: A novel communication-efficient FKGE method.\n        *   **Entity-Wise Top-K Sparsification**: A new sparsification strategy tailored for FKGE, applied bidirectionally (upstream and downstream).\n        *   **Intermittent Synchronization Mechanism**: A mechanism to handle embedding inconsistency due to FKG heterogeneity.\n    *   **System Design/Architectural Innovations**: Integration of these components into a federated learning architecture for FKGE, compatible with existing FKGE methods as a constituent.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted across three datasets (FB15k-237-R10, FB15k-237-R5, FB15k-237-R3) and evaluated with three different knowledge graph embedding methods (TransE, RotatE, and presumably a third, though only two are explicitly mentioned in Table I).\n    *   **Key Performance Metrics & Comparison Results**: The primary metric is communication efficiency (total transmitted parameter size, scaled by FedE's baseline) and performance degradation (convergence accuracy). Results show that FedS significantly enhances communication efficiency with negligible (even no) performance degradation, outperforming baseline FedE and the failed KD/SVD/SVD+ attempts in terms of total transmitted parameter size to reach comparable convergence accuracy.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly acknowledges the challenge of embedding inconsistency due to FKG heterogeneity, which necessitates the Intermittent Synchronization Mechanism. The effectiveness of Top-K selection relies on the assumption that changes in entity embeddings correlate with their importance for communication.\n    *   **Scope of Applicability**: FedS is designed for Federated Knowledge Graph Embedding learning, particularly in scenarios where communication bandwidth is a bottleneck. It is presented as a constituent that can be integrated into existing FKGE methods.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art in communication-efficient federated learning for knowledge graphs by being the first to effectively reduce the *size of transmitted parameters per communication round* without compromising model performance. It provides a crucial insight into the pitfalls of universal precision reduction in FKGE.\n    *   **Potential Impact**: FedS has the potential to enable more practical and scalable deployment of FKGE in resource-constrained environments (e.g., wireless edge networks), making collaborative KG learning more feasible for a wider range of applications. It opens new avenues for research into entity-aware communication strategies in federated learning.",
      "intriguing_abstract": "The promise of Federated Knowledge Graph Embedding (FKGE) is severely bottlenecked by exorbitant communication costs, particularly in bandwidth-constrained wireless edge networks. Existing methods primarily reduce communication *rounds*, overlooking the critical issue of large *parameter sizes* transmitted per round. Crucially, we reveal that attempts at universal compression, like Knowledge Distillation, critically degrade entity embedding precision, crippling convergence.\n\nWe introduce **FedS**, a novel bidirectional communication-efficient framework that radically redefines FKGE training. FedS employs an innovative **Entity-Wise Top-K Sparsification strategy**, where clients dynamically upload only the most significantly changed entity embeddings, preserving their original precision. This is complemented by a personalized downstream sparsification and an **Intermittent Synchronization Mechanism** to robustly handle knowledge graph heterogeneity. Our extensive experiments demonstrate that FedS achieves unprecedented communication efficiency, drastically reducing transmitted parameter size by up to 90% with negligible performance degradation across diverse datasets and KGE models. This breakthrough enables scalable and practical FKGE deployment, advancing the state-of-the-art in federated learning for knowledge graphs and opening new avenues for entity-aware communication strategies.",
      "keywords": [
        "Federated Knowledge Graph Embedding (FKGE)",
        "communication efficiency",
        "FedS framework",
        "Entity-Wise Top-K Sparsification",
        "Intermittent Synchronization Mechanism",
        "reducing transmitted parameter size",
        "entity embeddings",
        "embedding precision",
        "personalized aggregation",
        "resource-constrained environments",
        "bidirectional communication",
        "semantic integrity of embeddings",
        "universal precision reduction"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/3ac716ac5d47d4420010678fda766ebb5b882ba9.pdf",
      "citation_key": "zhang2024",
      "metadata": {
        "title": "Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification",
        "authors": [
          "Xiaoxiong Zhang",
          "Zhiwei Zeng",
          "Xin Zhou",
          "D. Niyato",
          "Zhiqi Shen"
        ],
        "published_date": "2024",
        "abstract": "Federated Knowledge Graphs Embedding learning (FKGE) encounters challenges in communication efficiency stemming from the considerable size of parameters and extensive communication rounds. However, existing FKGE methods only focus on reducing communication rounds by conducting multiple rounds of local training in each communication round, and ignore reducing the size of parameters transmitted within each communication round. To tackle the problem, we first find that universal reduction in embedding precision across all entities during compression can significantly impede convergence speed, underscoring the importance of maintaining embedding precision. We then propose bidirectional communication-efficient FedS based on Entity-Wise Top-K Sparsification strategy. During upload, clients dynamically identify and upload only the Top-K entity embeddings with the greater changes to the server. During download, the server first performs personalized embedding aggregation for each client. It then identifies and transmits the Top-K aggregated embeddings to each client. Besides, an Intermittent Synchronization Mechanism is used by FedS to mitigate negative effect of embedding inconsistency among shared entities of clients caused by heterogeneity of Federated Knowledge Graph. Extensive experiments across three datasets showcase that FedS significantly enhances communication efficiency with negligible (even no) performance degradation.",
        "file_path": "paper_data/knowledge_graph_embedding/3ac716ac5d47d4420010678fda766ebb5b882ba9.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper by \\cite{zhang2024} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Federated Knowledge Graph Embedding (FKGE) learning faces significant challenges in communication efficiency due to the considerable size of parameters (entity embeddings) and the extensive number of communication rounds required for training.\n    *   **Importance and Challenge**: High communication overhead impedes the training process, especially with numerous clients, large KGs, and high embedding dimensions, conflicting with bandwidth-constrained wireless edge networks and costly data plans. Existing FKGE methods only address reducing communication *rounds* (e.g., via more local iterations) but fail to reduce the *size of parameters transmitted within each round*, leading to a sustained high communication load.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Existing FKGE methods (e.g., FedE \\cite{zhang2024}, FedEC \\cite{zhang2024}, FedLu \\cite{zhang2024}, FedR \\cite{zhang2024}) primarily focus on improving the quality of learned embeddings or reducing communication *rounds*. They are based on client-server or peer-to-peer architectures.\n    *   **Limitations of Previous Solutions**:\n        *   They do not address the problem of reducing the *size of transmitted parameters* per communication round.\n        *   Initial attempts by the authors to integrate model compression techniques like Knowledge Distillation (KD) and Low-Rank Approximation (LRA) into FKGE proved ineffective. These methods universally reduce embedding precision across *all* entities, significantly slowing convergence and increasing total communication costs, even at low compression ratios. This highlights the critical importance of maintaining embedding precision for effective FKGE.\n        *   Traditional sparsification methods in Federated Learning operate parameter-wise, which can corrupt the semantic integrity of entity embeddings in FKGE due to the inherent coherence of multiple parameters forming an embedding.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **FedS**, a bidirectional communication-efficient framework based on an **Entity-Wise Top-K Sparsification strategy** and an **Intermittent Synchronization Mechanism**.\n        *   **Upstream Entity-Wise Top-K Sparsification**: Clients dynamically identify and upload only the Top-K entity embeddings that exhibit the *greatest changes* (quantified by Cosine Similarity between current and history embeddings) to the server. This preserves the original precision of the selected entities.\n        *   **Downstream Personalized Entity-Wise Top-K Sparsification**: The server first performs personalized embedding aggregation for each client. Then, it identifies and transmits the Top-K aggregated embeddings back to each client, selecting based on *entity upload frequency* (rather than changes, due to FKG heterogeneity).\n        *   **Intermittent Synchronization Mechanism**: To mitigate negative effects of embedding inconsistency among shared entities caused by the heterogeneity of Federated Knowledge Graphs, FedS periodically (at fixed intervals) transmits *all* parameters between clients and the server.\n    *   **Novelty**:\n        *   First attempt to mitigate FKGE communication overhead by reducing the *size of transmitted parameters per communication round*.\n        *   Introduces a novel **Entity-Wise Top-K Sparsification strategy** that operates on entire entity embeddings, preserving their semantic integrity, unlike previous parameter-wise sparsification methods.\n        *   The bidirectional sparsification (upstream and downstream) combined with personalized aggregation and the intermittent synchronization mechanism specifically addresses the unique challenges of FKGE heterogeneity.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insight/Finding**: Demonstrated through extensive experiments that universal reduction in embedding precision (e.g., via KD or LRA) across all entities significantly impedes convergence speed in FKGE, underscoring the importance of maintaining embedding precision for critical entities.\n    *   **Novel Algorithms/Methods**:\n        *   **FedS framework**: A novel communication-efficient FKGE method.\n        *   **Entity-Wise Top-K Sparsification**: A new sparsification strategy tailored for FKGE, applied bidirectionally (upstream and downstream).\n        *   **Intermittent Synchronization Mechanism**: A mechanism to handle embedding inconsistency due to FKG heterogeneity.\n    *   **System Design/Architectural Innovations**: Integration of these components into a federated learning architecture for FKGE, compatible with existing FKGE methods as a constituent.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted across three datasets (FB15k-237-R10, FB15k-237-R5, FB15k-237-R3) and evaluated with three different knowledge graph embedding methods (TransE, RotatE, and presumably a third, though only two are explicitly mentioned in Table I).\n    *   **Key Performance Metrics & Comparison Results**: The primary metric is communication efficiency (total transmitted parameter size, scaled by FedE's baseline) and performance degradation (convergence accuracy). Results show that FedS significantly enhances communication efficiency with negligible (even no) performance degradation, outperforming baseline FedE and the failed KD/SVD/SVD+ attempts in terms of total transmitted parameter size to reach comparable convergence accuracy.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly acknowledges the challenge of embedding inconsistency due to FKG heterogeneity, which necessitates the Intermittent Synchronization Mechanism. The effectiveness of Top-K selection relies on the assumption that changes in entity embeddings correlate with their importance for communication.\n    *   **Scope of Applicability**: FedS is designed for Federated Knowledge Graph Embedding learning, particularly in scenarios where communication bandwidth is a bottleneck. It is presented as a constituent that can be integrated into existing FKGE methods.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art in communication-efficient federated learning for knowledge graphs by being the first to effectively reduce the *size of transmitted parameters per communication round* without compromising model performance. It provides a crucial insight into the pitfalls of universal precision reduction in FKGE.\n    *   **Potential Impact**: FedS has the potential to enable more practical and scalable deployment of FKGE in resource-constrained environments (e.g., wireless edge networks), making collaborative KG learning more feasible for a wider range of applications. It opens new avenues for research into entity-aware communication strategies in federated learning.",
        "keywords": [
          "Federated Knowledge Graph Embedding (FKGE)",
          "communication efficiency",
          "FedS framework",
          "Entity-Wise Top-K Sparsification",
          "Intermittent Synchronization Mechanism",
          "reducing transmitted parameter size",
          "entity embeddings",
          "embedding precision",
          "personalized aggregation",
          "resource-constrained environments",
          "bidirectional communication",
          "semantic integrity of embeddings",
          "universal precision reduction"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "3ac716ac5d47d4420010678fda766ebb5b882ba9.pdf"
    },
    {
      "success": true,
      "doc_id": "8f20f02aeacbdd532455ca92bdaa8660",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the long-standing issue of incompleteness in Knowledge Graphs (KGs) by focusing on Knowledge Graph Completion (KGC), which involves predicting missing entities or relations in factual triplets \\cite{shen2022}.\n    *   **Importance and Challenge**: KGs are vital resources for various applications (e.g., question answering, web search), but their incompleteness hinders wider adoption. The challenge lies in effectively leveraging both structural patterns (e.g., composition of relations) and semantic relatedness (e.g., meanings of entities and relations) for KGC, as existing methods typically rely on one or the other \\cite{shen2022}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: Previous KGC approaches fall into two main categories: structure-based methods (using graph embedding) and semantic-based methods (encoding text descriptions via language models) \\cite{shen2022}.\n    *   **Limitations of previous solutions**: Existing methods struggle to jointly process and integrate both structural and semantic information effectively, leading to suboptimal performance, especially in data-scarce scenarios \\cite{shen2022}.\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes LASS (Joint Language Semantic and Structure Embedding), a method that jointly embeds the semantics from natural language descriptions of knowledge triplets with their structural information \\cite{shen2022}. LASS fine-tunes pre-trained language models (LMs) using a probabilistic structured loss.\n    *   **Novelty**: LASS's innovation lies in its unified approach: the forward pass of the LM captures semantics from textual descriptions, while a structured loss function, optimized via LM backpropagation, reconstructs KG structures. This allows for simultaneous learning of both types of information within a single framework \\cite{shen2022}.\n\n*   **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**: LASS integrates structural and semantic information for KGC by fine-tuning pre-trained LMs with a structured loss. Semantic embedding is achieved by mean pooling over LM outputs for concatenated textual descriptions of head, relation, and tail entities. Structure embedding is performed by optimizing a probabilistic structured loss, inspired by TransE, which models relationships as translations between entity embeddings \\cite{shen2022}.\n    *   **System design or architectural innovations**: The method constructs input sequences as `[B]Th[S]Tr[S]Tt[S]` for LMs, where `Th, Tr, Tt` are token sequences for head, relation, and tail descriptions. A probabilistic model `Pr(h|r,t)` is defined based on a score function `f(h,r,t) = -1/2 ||h+r-t||^2_2`, and negative sampling is used for efficient optimization of the negative log-likelihood loss \\cite{shen2022}.\n    *   **Theoretical insights or analysis**: LASS demonstrates how deep language representations can be effectively connected with KG structures, providing a mechanism to transfer rich semantic knowledge from LMs to structural patterns in KGs \\cite{shen2022}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: LASS was evaluated on two KGC tasks: link prediction and triplet classification, and its performance was also assessed in low-resource settings \\cite{shen2022}.\n    *   **Key performance metrics and comparison results**:\n        *   **Datasets**: FB15K-237, WN18RR, UMLS (link prediction); WN11, FB13 (triplet classification, low-resource) \\cite{shen2022}.\n        *   **LMs used**: BERT (BASE/LARGE) and RoBERTa (BASE/LARGE) \\cite{shen2022}.\n        *   **Triplet Classification**: LASS consistently achieved state-of-the-art (SOTA) accuracy on WN11 and FB13, outperforming KG-BERT and various structure-based methods. LASS-BERT variants generally showed slightly better results than LASS-RoBERTa \\cite{shen2022}.\n        *   **Low-Resource Settings**: LASS-BERT LARGE significantly outperformed KG-BERT and other baselines when trained with limited data (e.g., 5-30% of training data), demonstrating superior data efficiency and improved knowledge transfer \\cite{shen2022}.\n        *   **Link Prediction**: LASS achieved SOTA Hits@10 and Mean Rank (MR) on WN18RR, and competitive performance on FB15k-237 and UMLS, surpassing many shallow and deep structure embedding methods, as well as other language semantic embedding approaches like KG-BERT and StAR \\cite{shen2022}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The paper does not explicitly detail technical limitations of LASS itself. However, its reliance on natural language descriptions for entities and relations implies that its performance might be affected by the quality and availability of such textual data. The TransE-inspired score function, while effective, might inherit some of TransE's known limitations in handling complex relation patterns (e.g., 1-N, N-1, N-N relations) \\cite{shen2022}.\n    *   **Scope of applicability**: LASS is primarily applicable to knowledge graph completion tasks (link prediction, triplet classification) where natural language descriptions for entities and relations are available \\cite{shen2022}.\n\n*   **Technical Significance**\n    *   **Advance the technical state-of-the-art**: LASS significantly advances the state-of-the-art in KGC by providing a robust and effective method for jointly leveraging both structural and semantic information, leading to SOTA performance across various benchmarks \\cite{shen2022}.\n    *   **Potential impact on future research**: The work highlights the critical importance of integrating both semantics and structures for understanding KGs. Its strong performance in low-resource settings suggests a path for building more data-efficient KGC models. It also sheds light on the connections between KGs and deep language representation, opening avenues for future research at this intersection \\cite{shen2022}.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are indispensable for AI applications, yet their inherent incompleteness severely limits their utility. Current Knowledge Graph Completion (KGC) methods struggle to effectively integrate both the rich semantics from natural language descriptions and the underlying structural patterns of KGs, leading to suboptimal performance, particularly in data-scarce environments.\n\nWe introduce LASS (Joint Language Semantic and Structure Embedding), a novel framework that revolutionizes KGC by seamlessly unifying these disparate information sources. LASS fine-tunes pre-trained Language Models (LMs) using a probabilistic structured loss. This innovative approach allows the LM's forward pass to capture deep semantic embeddings while its backpropagation simultaneously reconstructs KG structures, enabling joint learning of language semantics and graph topology within a single, coherent model.\n\nOur extensive experiments demonstrate LASS achieving state-of-the-art (SOTA) performance across diverse KGC tasks, including link prediction and triplet classification, on benchmarks like FB15K-237 and WN18RR. Crucially, LASS exhibits superior data efficiency, significantly outperforming baselines in low-resource settings. This work not only advances the technical state-of-the-art but also provides a powerful paradigm for bridging deep language representations with structured knowledge, paving the way for more robust and data-efficient knowledge systems.",
      "keywords": [
        "Knowledge Graph Completion (KGC)",
        "Knowledge Graphs",
        "LASS (Joint Language Semantic and Structure Embedding)",
        "Joint structural and semantic embedding",
        "Pre-trained language models",
        "Probabilistic structured loss",
        "Link prediction",
        "Triplet classification",
        "Low-resource settings",
        "State-of-the-art performance",
        "Data efficiency",
        "Deep language representations",
        "Knowledge transfer"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/933cb8bf1cd50d6d5833a627683327b15db28836.pdf",
      "citation_key": "shen2022",
      "metadata": {
        "title": "Joint Language Semantic and Structure Embedding for Knowledge Graph Completion",
        "authors": [
          "Jianhao Shen",
          "Chenguang Wang",
          "Linyuan Gong",
          "Dawn Song"
        ],
        "published_date": "2022",
        "abstract": "The task of completing knowledge triplets has broad downstream applications. Both structural and semantic information plays an important role in knowledge graph completion. Unlike previous approaches that rely on either the structures or semantics of the knowledge graphs, we propose to jointly embed the semantics in the natural language description of the knowledge triplets with their structure information. Our method embeds knowledge graphs for the completion task via fine-tuning pre-trained language models with respect to a probabilistic structured loss, where the forward pass of the language models captures semantics and the loss reconstructs structures. Our extensive experiments on a variety of knowledge graph benchmarks have demonstrated the state-of-the-art performance of our method. We also show that our method can significantly improve the performance in a low-resource regime, thanks to the better use of semantics. The code and datasets are available at https://github.com/pkusjh/LASS.",
        "file_path": "paper_data/knowledge_graph_embedding/933cb8bf1cd50d6d5833a627683327b15db28836.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the long-standing issue of incompleteness in Knowledge Graphs (KGs) by focusing on Knowledge Graph Completion (KGC), which involves predicting missing entities or relations in factual triplets \\cite{shen2022}.\n    *   **Importance and Challenge**: KGs are vital resources for various applications (e.g., question answering, web search), but their incompleteness hinders wider adoption. The challenge lies in effectively leveraging both structural patterns (e.g., composition of relations) and semantic relatedness (e.g., meanings of entities and relations) for KGC, as existing methods typically rely on one or the other \\cite{shen2022}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: Previous KGC approaches fall into two main categories: structure-based methods (using graph embedding) and semantic-based methods (encoding text descriptions via language models) \\cite{shen2022}.\n    *   **Limitations of previous solutions**: Existing methods struggle to jointly process and integrate both structural and semantic information effectively, leading to suboptimal performance, especially in data-scarce scenarios \\cite{shen2022}.\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes LASS (Joint Language Semantic and Structure Embedding), a method that jointly embeds the semantics from natural language descriptions of knowledge triplets with their structural information \\cite{shen2022}. LASS fine-tunes pre-trained language models (LMs) using a probabilistic structured loss.\n    *   **Novelty**: LASS's innovation lies in its unified approach: the forward pass of the LM captures semantics from textual descriptions, while a structured loss function, optimized via LM backpropagation, reconstructs KG structures. This allows for simultaneous learning of both types of information within a single framework \\cite{shen2022}.\n\n*   **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**: LASS integrates structural and semantic information for KGC by fine-tuning pre-trained LMs with a structured loss. Semantic embedding is achieved by mean pooling over LM outputs for concatenated textual descriptions of head, relation, and tail entities. Structure embedding is performed by optimizing a probabilistic structured loss, inspired by TransE, which models relationships as translations between entity embeddings \\cite{shen2022}.\n    *   **System design or architectural innovations**: The method constructs input sequences as `[B]Th[S]Tr[S]Tt[S]` for LMs, where `Th, Tr, Tt` are token sequences for head, relation, and tail descriptions. A probabilistic model `Pr(h|r,t)` is defined based on a score function `f(h,r,t) = -1/2 ||h+r-t||^2_2`, and negative sampling is used for efficient optimization of the negative log-likelihood loss \\cite{shen2022}.\n    *   **Theoretical insights or analysis**: LASS demonstrates how deep language representations can be effectively connected with KG structures, providing a mechanism to transfer rich semantic knowledge from LMs to structural patterns in KGs \\cite{shen2022}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: LASS was evaluated on two KGC tasks: link prediction and triplet classification, and its performance was also assessed in low-resource settings \\cite{shen2022}.\n    *   **Key performance metrics and comparison results**:\n        *   **Datasets**: FB15K-237, WN18RR, UMLS (link prediction); WN11, FB13 (triplet classification, low-resource) \\cite{shen2022}.\n        *   **LMs used**: BERT (BASE/LARGE) and RoBERTa (BASE/LARGE) \\cite{shen2022}.\n        *   **Triplet Classification**: LASS consistently achieved state-of-the-art (SOTA) accuracy on WN11 and FB13, outperforming KG-BERT and various structure-based methods. LASS-BERT variants generally showed slightly better results than LASS-RoBERTa \\cite{shen2022}.\n        *   **Low-Resource Settings**: LASS-BERT LARGE significantly outperformed KG-BERT and other baselines when trained with limited data (e.g., 5-30% of training data), demonstrating superior data efficiency and improved knowledge transfer \\cite{shen2022}.\n        *   **Link Prediction**: LASS achieved SOTA Hits@10 and Mean Rank (MR) on WN18RR, and competitive performance on FB15k-237 and UMLS, surpassing many shallow and deep structure embedding methods, as well as other language semantic embedding approaches like KG-BERT and StAR \\cite{shen2022}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The paper does not explicitly detail technical limitations of LASS itself. However, its reliance on natural language descriptions for entities and relations implies that its performance might be affected by the quality and availability of such textual data. The TransE-inspired score function, while effective, might inherit some of TransE's known limitations in handling complex relation patterns (e.g., 1-N, N-1, N-N relations) \\cite{shen2022}.\n    *   **Scope of applicability**: LASS is primarily applicable to knowledge graph completion tasks (link prediction, triplet classification) where natural language descriptions for entities and relations are available \\cite{shen2022}.\n\n*   **Technical Significance**\n    *   **Advance the technical state-of-the-art**: LASS significantly advances the state-of-the-art in KGC by providing a robust and effective method for jointly leveraging both structural and semantic information, leading to SOTA performance across various benchmarks \\cite{shen2022}.\n    *   **Potential impact on future research**: The work highlights the critical importance of integrating both semantics and structures for understanding KGs. Its strong performance in low-resource settings suggests a path for building more data-efficient KGC models. It also sheds light on the connections between KGs and deep language representation, opening avenues for future research at this intersection \\cite{shen2022}.",
        "keywords": [
          "Knowledge Graph Completion (KGC)",
          "Knowledge Graphs",
          "LASS (Joint Language Semantic and Structure Embedding)",
          "Joint structural and semantic embedding",
          "Pre-trained language models",
          "Probabilistic structured loss",
          "Link prediction",
          "Triplet classification",
          "Low-resource settings",
          "State-of-the-art performance",
          "Data efficiency",
          "Deep language representations",
          "Knowledge transfer"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "933cb8bf1cd50d6d5833a627683327b15db28836.pdf"
    },
    {
      "success": true,
      "doc_id": "15789e6d97f16c5dfc46a9603e09533f",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the underexplored area of leveraging entity-specific common feature aggregation to enhance knowledge graph representation learning \\cite{hu2024}.\n    *   While deep learning excels at automatic feature extraction and common feature aggregation has improved performance in other NLP tasks (e.g., text classification), its application to knowledge graph embedding (KGE) for entity-specific features has not been fully investigated, despite the development of diverse KGE strategies \\cite{hu2024}.\n\n*   **Related Work & Positioning**\n    *   Existing work includes common features-based aggregation for tasks like text classification and sentiment analysis, and various strategies for knowledge graph embedding models \\cite{hu2024}.\n    *   The limitation of previous KGE solutions is their failure to fully explore the potential of *entity-specific common feature aggregation* for improving representation learning \\cite{hu2024}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the proposed **Convolutional Neural Network-based Entity-specific Common Feature Aggregation (CNN-ECFA)** strategy \\cite{hu2024}.\n    *   This approach is novel because it specifically applies a CNN-based mechanism for *entity-specific* common feature aggregation within the context of knowledge graph embedding, and it introduces a new *universal framework* built upon CNN-ECFA for KGE learning \\cite{hu2024}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the CNN-ECFA strategy, which innovatively uses Convolutional Neural Networks for entity-specific common feature aggregation in KGE \\cite{hu2024}.\n    *   **System Design/Architectural Innovation**: Development of a new universal framework for knowledge graph embedding learning, which integrates the CNN-ECFA strategy \\cite{hu2024}.\n\n*   **Experimental Validation**\n    *   **Experiments**: Conducted on a link prediction task to evaluate the effectiveness of CNN-ECFA and its universal framework \\cite{hu2024}.\n    *   **Datasets**: Publicly available standard datasets including WN18RR, YAGO3-10, and NELL-995 were used \\cite{hu2024}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   The CNN-ECFA strategy demonstrated superior performance over state-of-the-art feature projection strategies, achieving average improvements of 0.6% in MRR (Mean Reciprocal Rank) and 0.7% in Hits@1 across all datasets \\cite{hu2024}.\n        *   The proposed universal framework significantly outperformed a generalized relation learning framework on WN18RR and NELL-995, with average improvements of 1.7% in MRR and 1.9% in Hits@1 \\cite{hu2024}.\n    *   **Reproducibility**: Source code is publicly available at `https://github.com/peterhu95/ConvE-CNN-ECFA` \\cite{hu2024}.\n\n*   **Limitations & Scope**\n    *   The provided abstract does not explicitly detail technical limitations or assumptions of the CNN-ECFA strategy or its framework.\n    *   The scope of applicability is focused on knowledge graph embedding learning, particularly for improving performance on tasks like link prediction \\cite{hu2024}.\n\n*   **Technical Significance**\n    *   This work advances the technical state-of-the-art by demonstrating that entity-specific common feature aggregation, when implemented via a CNN-based strategy (CNN-ECFA), is more effective for knowledge graph embedding learning than existing feature projection and generalized relation learning frameworks \\cite{hu2024}.\n    *   It has the potential to impact future research by introducing a novel and effective approach to leverage fine-grained entity features, potentially inspiring new directions in KGE model design and feature engineering \\cite{hu2024}.",
      "intriguing_abstract": "The intricate structure of Knowledge Graphs (KGs) holds vast potential, yet current Knowledge Graph Embedding (KGE) models often overlook the crucial role of *entity-specific common feature aggregation* in refining representation learning. This paper addresses this critical gap by introducing **Convolutional Neural Network-based Entity-specific Common Feature Aggregation (CNN-ECFA)**, a novel strategy that harnesses the power of CNNs to precisely capture and aggregate fine-grained, entity-specific features. We further propose a new universal framework for KGE learning, seamlessly integrating CNN-ECFA to enhance model robustness and accuracy. Our rigorous experimental validation on benchmark datasets (WN18RR, YAGO3-10, NELL-995) for link prediction reveals CNN-ECFA's remarkable efficacy, achieving average improvements of 0.6% MRR and 0.7% Hits@1 over state-of-the-art feature projection methods. Moreover, our universal framework significantly outperforms generalized relation learning frameworks, demonstrating average gains of 1.7% MRR and 1.9% Hits@1. This pioneering work not only establishes a new paradigm for leveraging fine-grained entity information in KGE but also paves the way for developing more sophisticated and impactful knowledge graph applications. Source code is publicly available for reproducibility.",
      "keywords": [
        "Convolutional Neural Network-based Entity-specific Common Feature Aggregation (CNN-ECFA)",
        "Knowledge Graph Embedding (KGE)",
        "Entity-specific common feature aggregation",
        "Universal framework for KGE",
        "Convolutional Neural Network (CNN)",
        "Knowledge graph representation learning",
        "Link prediction task",
        "Superior performance",
        "Mean Reciprocal Rank (MRR)",
        "Hits@1",
        "Feature projection strategies",
        "Generalized relation learning framework"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/bb3e135757bfb82c4de202c807c9e381caecb623.pdf",
      "citation_key": "hu2024",
      "metadata": {
        "title": "Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation for Knowledge Graph Embedding Learning",
        "authors": [
          "Kairong Hu",
          "Xiaozhi Zhu",
          "Hai Liu",
          "Yingying Qu",
          "Fu Lee Wang",
          "Tianyong Hao"
        ],
        "published_date": "2024",
        "abstract": "Deep learning models present impressive capability for automatic feature extraction, where common features-based aggregation have demonstrated valuable potential in improving the model performance on text classification, sentiment analysis, etc. However, leveraging entity-specific common feature aggregation for enhancing knowledge graph representation learning has not been fully explored yet, though diverse strategies in knowledge graph embedding models have been developed in recent years. This paper proposes an innovative Convolutional Neural Network-based Entity-specific Common Feature Aggregation strategy named CNN-ECFA. Besides, a new universal framework based on the CNN-ECFA strategy is introduced for knowledge graph embedding learning. Experiments are conducted on publicly-available standard datasets for a link prediction task including WN18RR, YAGO3-10 and NELL-995. Results show that the CNN-ECFA strategy outperforms the state-of-the-art feature projection strategies with average improvements of 0.6% and 0.7% of MRR and Hits@1 on all the datasets, demonstrating our CNN-ECFA strategy is more effective for knowledge graph embedding learning. In addition, our universal framework significantly outperforms a generalized relation learning framework on WN18RR and NELL-995 with average improvements of 1.7% and 1.9% on MRR and Hits@1. The source code is publicly available at https://github.com/peterhu95/ConvE-CNN-ECFA.",
        "file_path": "paper_data/knowledge_graph_embedding/bb3e135757bfb82c4de202c807c9e381caecb623.pdf",
        "venue": "IEEE transactions on consumer electronics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the underexplored area of leveraging entity-specific common feature aggregation to enhance knowledge graph representation learning \\cite{hu2024}.\n    *   While deep learning excels at automatic feature extraction and common feature aggregation has improved performance in other NLP tasks (e.g., text classification), its application to knowledge graph embedding (KGE) for entity-specific features has not been fully investigated, despite the development of diverse KGE strategies \\cite{hu2024}.\n\n*   **Related Work & Positioning**\n    *   Existing work includes common features-based aggregation for tasks like text classification and sentiment analysis, and various strategies for knowledge graph embedding models \\cite{hu2024}.\n    *   The limitation of previous KGE solutions is their failure to fully explore the potential of *entity-specific common feature aggregation* for improving representation learning \\cite{hu2024}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the proposed **Convolutional Neural Network-based Entity-specific Common Feature Aggregation (CNN-ECFA)** strategy \\cite{hu2024}.\n    *   This approach is novel because it specifically applies a CNN-based mechanism for *entity-specific* common feature aggregation within the context of knowledge graph embedding, and it introduces a new *universal framework* built upon CNN-ECFA for KGE learning \\cite{hu2024}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the CNN-ECFA strategy, which innovatively uses Convolutional Neural Networks for entity-specific common feature aggregation in KGE \\cite{hu2024}.\n    *   **System Design/Architectural Innovation**: Development of a new universal framework for knowledge graph embedding learning, which integrates the CNN-ECFA strategy \\cite{hu2024}.\n\n*   **Experimental Validation**\n    *   **Experiments**: Conducted on a link prediction task to evaluate the effectiveness of CNN-ECFA and its universal framework \\cite{hu2024}.\n    *   **Datasets**: Publicly available standard datasets including WN18RR, YAGO3-10, and NELL-995 were used \\cite{hu2024}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   The CNN-ECFA strategy demonstrated superior performance over state-of-the-art feature projection strategies, achieving average improvements of 0.6% in MRR (Mean Reciprocal Rank) and 0.7% in Hits@1 across all datasets \\cite{hu2024}.\n        *   The proposed universal framework significantly outperformed a generalized relation learning framework on WN18RR and NELL-995, with average improvements of 1.7% in MRR and 1.9% in Hits@1 \\cite{hu2024}.\n    *   **Reproducibility**: Source code is publicly available at `https://github.com/peterhu95/ConvE-CNN-ECFA` \\cite{hu2024}.\n\n*   **Limitations & Scope**\n    *   The provided abstract does not explicitly detail technical limitations or assumptions of the CNN-ECFA strategy or its framework.\n    *   The scope of applicability is focused on knowledge graph embedding learning, particularly for improving performance on tasks like link prediction \\cite{hu2024}.\n\n*   **Technical Significance**\n    *   This work advances the technical state-of-the-art by demonstrating that entity-specific common feature aggregation, when implemented via a CNN-based strategy (CNN-ECFA), is more effective for knowledge graph embedding learning than existing feature projection and generalized relation learning frameworks \\cite{hu2024}.\n    *   It has the potential to impact future research by introducing a novel and effective approach to leverage fine-grained entity features, potentially inspiring new directions in KGE model design and feature engineering \\cite{hu2024}.",
        "keywords": [
          "Convolutional Neural Network-based Entity-specific Common Feature Aggregation (CNN-ECFA)",
          "Knowledge Graph Embedding (KGE)",
          "Entity-specific common feature aggregation",
          "Universal framework for KGE",
          "Convolutional Neural Network (CNN)",
          "Knowledge graph representation learning",
          "Link prediction task",
          "Superior performance",
          "Mean Reciprocal Rank (MRR)",
          "Hits@1",
          "Feature projection strategies",
          "Generalized relation learning framework"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "bb3e135757bfb82c4de202c807c9e381caecb623.pdf"
    },
    {
      "success": true,
      "doc_id": "c97200c17760cbbe02d4bded0186395c",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper identifies a fundamental deficiency in the expressiveness of many popular Knowledge Graph Embedding (KGE) models, termed \"Z-paradox.\" This paradox causes KGE models to incorrectly infer relationships based on a specific graph pattern, leading to false positives.\n    *   **Importance and Challenge**: The Z-paradox is a serious issue, affecting a significant portion of test facts in standard KG benchmark datasets (e.g., ~35% in FB15k-237). It can lead to substantial accuracy drops (over 20% for models like TransE and RotatE on affected samples), degrading the practical performance and reliability of KGE models in applications like link prediction and information retrieval.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon and critically analyzes existing KGE models, particularly translation-based methods (e.g., TransE, RotatE, OTE, MQuadE) and bilinear semantic matching methods (e.g., DisMult, ComplEX, TuckER).\n    *   **Limitations of Previous Solutions**: The paper theoretically proves that a wide range of existing KGE models, including all translation-based models and many bilinear models under certain conditions, suffer from the Z-paradox. This inherent limitation restricts their expressiveness and ability to accurately model complex relational data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes MQuinE (Matrix Quin tuple Embedding), a new KGE model designed to circumvent the Z-paradox while maintaining strong expressiveness for various relation patterns.\n    *   **Score Function**: MQuinE measures the plausibility of a fact triplet `(h, r, t)` using the score function `s(h, r, t) = ||H R_h - R_t T + H R_c T||_F^2`. Here, `H` and `T` are symmetric matrix embeddings for head and tail entities, respectively, and `R_h, R_t, R_c` is a matrix triplet representing the relation `r`.\n    *   **Novelty**: The key innovation lies in the introduction of the `H R_c T` cross-term in the score function. This term is theoretically shown to be central to MQuinE's ability to avoid the Z-paradox, a property lacking in previous distance-based models. Additionally, the paper introduces **Z-sampling**, a novel negative sampling technique that explicitly collects and utilizes Z-patterns during training to mitigate their negative impact.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Formal definition and characterization of the \"Z-paradox\" as a fundamental expressiveness bottleneck in KGE models \\cite{liu2024}.\n        *   Introduction of MQuinE, a novel matrix-based KGE model that inherently avoids the Z-paradox \\cite{liu2024}.\n        *   Development of Z-sampling, a specialized negative sampling strategy to explicitly address Z-patterns during model training \\cite{liu2024}.\n    *   **Theoretical Insights/Analysis**:\n        *   Theoretical proof that a broad class of existing KGE models (including all translation-based models) suffer from the Z-paradox \\cite{liu2024}.\n        *   Theoretical justification that MQuinE does not suffer from Z-paradox (Theorem 3.4) while preserving the ability to model complex relation patterns such as symmetric/asymmetric, inverse, 1-N/N-1/N-N, and Abelian/non-Abelian compositions (Theorems 3.2, 3.3) \\cite{liu2024}.\n    *   **System Design/Architectural Innovations**: MQuinE utilizes symmetric matrix embeddings for entities and a quin-tuple of matrices for relations, offering a richer representation space compared to vector-based or simpler matrix-based models.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed on standard knowledge graph benchmark datasets: FB15k-237, WN18, WN18RR, and YAGO3-10. The evaluation focused on the link prediction task.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Impact of Z-paradox**: Demonstrated that Z-patterns negatively affect a significant portion of test facts (e.g., 35% in FB15k-237), causing over 20% accuracy drop for models like TransE and RotatE on these specific samples.\n        *   **MQuinE Performance**:\n            *   Successfully mitigates the negative impact of Z-paradox.\n            *   Achieved a 10% improvement in Hit@10 on test facts negatively impacted by Z-patterns on the FB15k-237 dataset \\cite{liu2024}.\n            *   Attained overall improvements of 7% in Hit@1 and 4% in Hit@10 on all test facts on FB15k-237 \\cite{liu2024}.\n            *   Outperformed existing KGE methods by a visible margin on most benchmark datasets \\cite{liu2024}.\n            *   The effectiveness of the proposed Z-sampling technique was empirically validated.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on overcoming the Z-paradox and does not explicitly detail new limitations introduced by MQuinE. The model assumes entity embeddings are symmetric matrices. The increased complexity of matrix embeddings compared to vector embeddings might imply higher computational costs, though this is not highlighted as a limitation in the provided text.\n    *   **Scope of Applicability**: MQuinE is designed for general KGE tasks, particularly link prediction and information retrieval, where capturing complex relational patterns and avoiding spurious inferences are crucial.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: MQuinE significantly advances the technical state-of-the-art by identifying and providing a robust solution to the previously unaddressed \"Z-paradox\" expressiveness bottleneck in KGE models \\cite{liu2024}. It is presented as the first KGE model that is free from Z-paradox while preserving the ability to capture all major relation patterns.\n    *   **Potential Impact on Future Research**: This work introduces a new criterion for evaluating KGE model expressiveness. Future KGE models will need to consider and ideally circumvent the Z-paradox. The Z-sampling technique could be adopted or adapted by other KGE models or graph neural network frameworks (e.g., NBFNet) to improve their robustness and accuracy.",
      "intriguing_abstract": "Despite their widespread success, Knowledge Graph Embedding (KGE) models harbor a fundamental expressiveness bottleneck we term the \"Z-paradox.\" This critical flaw causes KGE models to incorrectly infer relationships based on specific graph patterns, leading to significant accuracy drops (over 20%) on a substantial portion (~35%) of test facts in benchmark datasets like FB15k-237, severely degrading practical performance in applications such as link prediction. We theoretically prove that a broad class of existing KGE models, including all translation-based methods, are inherently susceptible to this crippling limitation.\n\nTo overcome this, we introduce MQuinE (Matrix Quin-tuple Embedding), a novel matrix-based KGE model. MQuinE's unique score function, featuring a crucial cross-term, is theoretically shown to inherently circumvent the Z-paradox while preserving the ability to model complex relational patterns. Complementing MQuinE, we propose Z-sampling, a novel negative sampling technique that explicitly leverages Z-patterns during training for enhanced robustness. Extensive experiments on standard benchmarks demonstrate MQuinE's superior performance, achieving up to a 10% Hit@10 improvement on Z-paradox-affected samples and overall state-of-the-art results. This work not only provides a robust solution to a critical, previously unaddressed KGE expressiveness bottleneck but also establishes a new criterion for evaluating future models, paving the way for more reliable and accurate knowledge graph applications.",
      "keywords": [
        "Z-paradox",
        "Knowledge Graph Embedding (KGE) models",
        "MQuinE",
        "Z-sampling",
        "expressiveness bottleneck",
        "link prediction",
        "matrix embeddings",
        "score function cross-term",
        "theoretical proof",
        "complex relation patterns",
        "false positives",
        "state-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/398978c84ca8dab093d0b7fa73c6d380f5fa914c.pdf",
      "citation_key": "liu2024",
      "metadata": {
        "title": "MQuinE: a Cure for Z-paradox in Knowledge Graph Embedding",
        "authors": [
          "Yang Liu",
          "Huang Fang",
          "Yunfeng Cai",
          "Mingming Sun"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called Z-paradox. Motivated by the existence of Z-paradox, we propose a new KGE model called MQuinE that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction tasks.",
        "file_path": "paper_data/knowledge_graph_embedding/398978c84ca8dab093d0b7fa73c6d380f5fa914c.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper identifies a fundamental deficiency in the expressiveness of many popular Knowledge Graph Embedding (KGE) models, termed \"Z-paradox.\" This paradox causes KGE models to incorrectly infer relationships based on a specific graph pattern, leading to false positives.\n    *   **Importance and Challenge**: The Z-paradox is a serious issue, affecting a significant portion of test facts in standard KG benchmark datasets (e.g., ~35% in FB15k-237). It can lead to substantial accuracy drops (over 20% for models like TransE and RotatE on affected samples), degrading the practical performance and reliability of KGE models in applications like link prediction and information retrieval.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon and critically analyzes existing KGE models, particularly translation-based methods (e.g., TransE, RotatE, OTE, MQuadE) and bilinear semantic matching methods (e.g., DisMult, ComplEX, TuckER).\n    *   **Limitations of Previous Solutions**: The paper theoretically proves that a wide range of existing KGE models, including all translation-based models and many bilinear models under certain conditions, suffer from the Z-paradox. This inherent limitation restricts their expressiveness and ability to accurately model complex relational data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes MQuinE (Matrix Quin tuple Embedding), a new KGE model designed to circumvent the Z-paradox while maintaining strong expressiveness for various relation patterns.\n    *   **Score Function**: MQuinE measures the plausibility of a fact triplet `(h, r, t)` using the score function `s(h, r, t) = ||H R_h - R_t T + H R_c T||_F^2`. Here, `H` and `T` are symmetric matrix embeddings for head and tail entities, respectively, and `R_h, R_t, R_c` is a matrix triplet representing the relation `r`.\n    *   **Novelty**: The key innovation lies in the introduction of the `H R_c T` cross-term in the score function. This term is theoretically shown to be central to MQuinE's ability to avoid the Z-paradox, a property lacking in previous distance-based models. Additionally, the paper introduces **Z-sampling**, a novel negative sampling technique that explicitly collects and utilizes Z-patterns during training to mitigate their negative impact.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Formal definition and characterization of the \"Z-paradox\" as a fundamental expressiveness bottleneck in KGE models \\cite{liu2024}.\n        *   Introduction of MQuinE, a novel matrix-based KGE model that inherently avoids the Z-paradox \\cite{liu2024}.\n        *   Development of Z-sampling, a specialized negative sampling strategy to explicitly address Z-patterns during model training \\cite{liu2024}.\n    *   **Theoretical Insights/Analysis**:\n        *   Theoretical proof that a broad class of existing KGE models (including all translation-based models) suffer from the Z-paradox \\cite{liu2024}.\n        *   Theoretical justification that MQuinE does not suffer from Z-paradox (Theorem 3.4) while preserving the ability to model complex relation patterns such as symmetric/asymmetric, inverse, 1-N/N-1/N-N, and Abelian/non-Abelian compositions (Theorems 3.2, 3.3) \\cite{liu2024}.\n    *   **System Design/Architectural Innovations**: MQuinE utilizes symmetric matrix embeddings for entities and a quin-tuple of matrices for relations, offering a richer representation space compared to vector-based or simpler matrix-based models.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed on standard knowledge graph benchmark datasets: FB15k-237, WN18, WN18RR, and YAGO3-10. The evaluation focused on the link prediction task.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Impact of Z-paradox**: Demonstrated that Z-patterns negatively affect a significant portion of test facts (e.g., 35% in FB15k-237), causing over 20% accuracy drop for models like TransE and RotatE on these specific samples.\n        *   **MQuinE Performance**:\n            *   Successfully mitigates the negative impact of Z-paradox.\n            *   Achieved a 10% improvement in Hit@10 on test facts negatively impacted by Z-patterns on the FB15k-237 dataset \\cite{liu2024}.\n            *   Attained overall improvements of 7% in Hit@1 and 4% in Hit@10 on all test facts on FB15k-237 \\cite{liu2024}.\n            *   Outperformed existing KGE methods by a visible margin on most benchmark datasets \\cite{liu2024}.\n            *   The effectiveness of the proposed Z-sampling technique was empirically validated.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on overcoming the Z-paradox and does not explicitly detail new limitations introduced by MQuinE. The model assumes entity embeddings are symmetric matrices. The increased complexity of matrix embeddings compared to vector embeddings might imply higher computational costs, though this is not highlighted as a limitation in the provided text.\n    *   **Scope of Applicability**: MQuinE is designed for general KGE tasks, particularly link prediction and information retrieval, where capturing complex relational patterns and avoiding spurious inferences are crucial.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: MQuinE significantly advances the technical state-of-the-art by identifying and providing a robust solution to the previously unaddressed \"Z-paradox\" expressiveness bottleneck in KGE models \\cite{liu2024}. It is presented as the first KGE model that is free from Z-paradox while preserving the ability to capture all major relation patterns.\n    *   **Potential Impact on Future Research**: This work introduces a new criterion for evaluating KGE model expressiveness. Future KGE models will need to consider and ideally circumvent the Z-paradox. The Z-sampling technique could be adopted or adapted by other KGE models or graph neural network frameworks (e.g., NBFNet) to improve their robustness and accuracy.",
        "keywords": [
          "Z-paradox",
          "Knowledge Graph Embedding (KGE) models",
          "MQuinE",
          "Z-sampling",
          "expressiveness bottleneck",
          "link prediction",
          "matrix embeddings",
          "score function cross-term",
          "theoretical proof",
          "complex relation patterns",
          "false positives",
          "state-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "398978c84ca8dab093d0b7fa73c6d380f5fa914c.pdf"
    },
    {
      "success": true,
      "doc_id": "ec4cb25e3cbf03180d13fc6eef5dab28",
      "summary": "Here's a focused summary of the paper \"AutoSF: Searching Scoring Functions for Knowledge Graph Embedding\" by \\cite{zhang2019} for a literature review:\n\n---\n\n### Analysis of \"AutoSF: Searching Scoring Functions for Knowledge Graph Embedding\" \\cite{zhang2019}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of designing effective scoring functions (SFs) for Knowledge Graph Embedding (KGE) models. SFs are crucial for measuring the plausibility of triplets (h, r, t) in a Knowledge Graph (KG).\n    *   **Importance and Challenge**:\n        *   Existing human-designed SFs, while numerous, cannot consistently outperform others across diverse KGs due to complex and varied relation patterns (symmetric, asymmetric, inverse, etc.).\n        *   Manually designing new, state-of-the-art SFs is a difficult and time-consuming task requiring significant human expertise.\n        *   The optimal SF is often KG-dependent, necessitating a tailored approach for each dataset.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   The paper categorizes existing SFs into Translational Distance Models (TDMs), Bilinear Models (BLMs), and Neural Network Models (NNMs).\n        *   It acknowledges BLMs (e.g., DistMult, ComplEx, Analogy, SimplE) as the most powerful and state-of-the-art due to their expressiveness and empirical performance.\n    *   **Limitations of Previous Solutions**:\n        *   TDMs have limited expressive ability.\n        *   NNMs, despite their power, often perform worse than BLMs in KGE due to high complexity, training difficulties, and a lack of domain-specific constraints.\n        *   Crucially, even state-of-the-art BLMs are fixed designs, meaning an SF well-suited for one KG may not perform well on another, leading to a \"no absolute winner\" scenario. This highlights the need for adaptive SF design.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{zhang2019} proposes **AutoSF**, an automated machine learning (AutoML) framework to automatically search for optimal SFs for distinct KGs. This is framed as a bi-level optimization problem.\n    *   **Novelty of Approach**:\n        *   **Unified Representation**: The authors identify a unified representation for popular BLM-based SFs by splitting entity/relation embeddings into four parts (h=[h1;h2;h3;h4]) and expressing SFs as `f(h,r,t) = h^T g(r) t`, where `g(r)` is a 4x4 block matrix. This representation forms a tractable search space `G` that covers existing SFs and allows for novel combinations.\n        *   **Domain-Specific Analysis**: AutoSF incorporates domain-specific insights into KG relation properties (symmetric, anti-symmetric, inverse) to guide the search.\n        *   **Progressive Greedy Search Algorithm**: A greedy algorithm is developed to efficiently explore the search space.\n        *   **Filter and Predictor**: To further accelerate the search, a filter is introduced to avoid training redundant SFs with identical expressive abilities. A predictor, utilizing specifically designed **Symmetry-Related Features (SRF)**, is used to evaluate and prune unpromising SF candidates *before* full model training, significantly reducing computational cost.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A novel AutoML framework (AutoSF) for automatically designing KGE scoring functions.\n        *   A progressive greedy search algorithm tailored for the SF search space.\n        *   A filter mechanism to prune redundant SFs based on expressive equivalence.\n        *   A predictor model leveraging Symmetry-Related Features (SRF) to efficiently evaluate SF candidates.\n    *   **System Design/Architectural Innovations**:\n        *   The unified representation of BLM-based SFs as a 4x4 block matrix `g(r)`, which defines a structured and extensible search space.\n    *   **Theoretical Insights/Analysis**:\n        *   The observation that different BLMs essentially regularize the relational matrix `R` in distinct ways, and AutoSF aims to search for data-dependent regularization schemes.\n        *   Domain-specific analysis linking common relation types (symmetric, anti-symmetric, inverse) to specific requirements on the SF and its `g(r)` matrix, which informs the search constraints.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on two primary KGE tasks:\n        *   Link Prediction (predicting missing entities in triplets).\n        *   Triplet Classification (classifying whether a triplet is valid or invalid).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Evaluated on five popular benchmark datasets (FB15k, WN18, FB15k-237, WN18RR, YAGO3-10).\n        *   AutoSF consistently **outperforms state-of-the-art human-designed SFs** (e.g., ComplEx, SimplE, Analogy) across these benchmarks.\n        *   The searched SFs are demonstrated to be **KG-dependent** (different KGs yield different optimal SFs) and **new to the literature**.\n        *   Case studies on the searched SFs provide insights into KG properties, suggesting AutoSF can also be a tool for KG analysis.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The search space is primarily focused on **Bilinear Models (BLMs)**, as they are identified as the current state-of-the-art. TDMs and NNMs are not directly explored within the AutoSF search space.\n        *   The unified representation uses a 4-part split (k=4) for embeddings to ensure a **tractable search space**, implying a trade-off between generality and computational feasibility.\n    *   **Scope of Applicability**:\n        *   Primarily applicable to KGE tasks where the plausibility of triplets is measured by scoring functions.\n        *   The methodology is designed to adapt SFs to specific KG properties, making it suitable for diverse KGs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: AutoSF significantly advances the technical state-of-the-art in KGE by automating the design of scoring functions, moving beyond fixed, human-designed models. It demonstrates that data-dependent, automatically searched SFs can consistently outperform the best human-engineered ones.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for applying AutoML techniques to other components of KGE and broader graph machine learning problems.\n        *   The ability to discover novel, KG-dependent SFs can inspire new theoretical understandings of relation patterns and embedding techniques.\n        *   The framework provides a tool for analyzing KG properties through the structure of the discovered SFs, potentially leading to better-informed human designs in the future.",
      "intriguing_abstract": "The quest for optimal Knowledge Graph Embedding (KGE) models is often hampered by the arduous and expert-dependent task of designing effective scoring functions (SFs). Existing human-designed SFs struggle to generalize across diverse KGs due to varied relation patterns, leading to a 'no absolute winner' scenario. We introduce **AutoSF**, an innovative AutoML framework that automates the discovery of superior, KG-dependent SFs, fundamentally transforming KGE model development.\n\nAutoSF unifies popular Bilinear Model (BLM)-based SFs into a novel 4x4 block matrix representation, defining a structured and extensible search space. Guided by domain-specific insights into relation properties, a progressive greedy search algorithm efficiently explores this space. Crucially, a predictor leveraging Symmetry-Related Features (SRF) prunes unpromising candidates *before* full model training, drastically accelerating the search. Extensive experiments on five benchmark datasets demonstrate that AutoSF consistently outperforms state-of-the-art human-designed SFs in **link prediction** and **triplet classification**. Our framework not only discovers novel, dataset-specific SFs but also offers profound insights into underlying Knowledge Graph properties, paving the way for adaptive, high-performance KGE.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Scoring Functions (SFs)",
        "Automated Machine Learning (AutoML)",
        "AutoSF framework",
        "Bilinear Models (BLMs)",
        "Unified representation",
        "Progressive greedy search",
        "Symmetry-Related Features (SRF)",
        "Link Prediction",
        "Triplet Classification",
        "KG-dependent scoring functions",
        "Bi-level optimization",
        "Automated SF design"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/b594b21557395c6a8fa8356249373f8e318c2df2.pdf",
      "citation_key": "zhang2019",
      "metadata": {
        "title": "AutoSF: Searching Scoring Functions for Knowledge Graph Embedding",
        "authors": [
          "Yongqi Zhang",
          "Quanming Yao",
          "Wenyuan Dai",
          "Lei Chen"
        ],
        "published_date": "2019",
        "abstract": "Scoring functions (SFs), which measure the plausibility of triplets in knowledge graph (KG), have become the crux of KG embedding. Lots of SFs, which target at capturing different kinds of relations in KGs, have been designed by humans in recent years. However, as relations can exhibit complex patterns that are hard to infer before training, none of them can consistently perform better than others on existing benchmark data sets. In this paper, inspired by the recent success of automated machine learning (AutoML), we propose to automatically design SFs (AutoSF) for distinct KGs by the AutoML techniques. However, it is non-trivial to explore domain- specific information here to make AutoSF efficient and effective. We firstly identify a unified representation over popularly used SFs, which helps to set up a search space for AutoSF. Then, we propose a greedy algorithm to search in such a space efficiently. The algorithm is further sped up by a filter and a predictor, which can avoid repeatedly training SFs with same expressive ability and help removing bad candidates during the search before model training. Finally, we perform extensive experiments on benchmark data sets. Results on link prediction and triplets classification show that the searched SFs by AutoSF, are KG dependent, new to the literature, and outperform the state-of- the-art SFs designed by humans. 1",
        "file_path": "paper_data/knowledge_graph_embedding/b594b21557395c6a8fa8356249373f8e318c2df2.pdf",
        "venue": "IEEE International Conference on Data Engineering",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"AutoSF: Searching Scoring Functions for Knowledge Graph Embedding\" by \\cite{zhang2019} for a literature review:\n\n---\n\n### Analysis of \"AutoSF: Searching Scoring Functions for Knowledge Graph Embedding\" \\cite{zhang2019}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of designing effective scoring functions (SFs) for Knowledge Graph Embedding (KGE) models. SFs are crucial for measuring the plausibility of triplets (h, r, t) in a Knowledge Graph (KG).\n    *   **Importance and Challenge**:\n        *   Existing human-designed SFs, while numerous, cannot consistently outperform others across diverse KGs due to complex and varied relation patterns (symmetric, asymmetric, inverse, etc.).\n        *   Manually designing new, state-of-the-art SFs is a difficult and time-consuming task requiring significant human expertise.\n        *   The optimal SF is often KG-dependent, necessitating a tailored approach for each dataset.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   The paper categorizes existing SFs into Translational Distance Models (TDMs), Bilinear Models (BLMs), and Neural Network Models (NNMs).\n        *   It acknowledges BLMs (e.g., DistMult, ComplEx, Analogy, SimplE) as the most powerful and state-of-the-art due to their expressiveness and empirical performance.\n    *   **Limitations of Previous Solutions**:\n        *   TDMs have limited expressive ability.\n        *   NNMs, despite their power, often perform worse than BLMs in KGE due to high complexity, training difficulties, and a lack of domain-specific constraints.\n        *   Crucially, even state-of-the-art BLMs are fixed designs, meaning an SF well-suited for one KG may not perform well on another, leading to a \"no absolute winner\" scenario. This highlights the need for adaptive SF design.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{zhang2019} proposes **AutoSF**, an automated machine learning (AutoML) framework to automatically search for optimal SFs for distinct KGs. This is framed as a bi-level optimization problem.\n    *   **Novelty of Approach**:\n        *   **Unified Representation**: The authors identify a unified representation for popular BLM-based SFs by splitting entity/relation embeddings into four parts (h=[h1;h2;h3;h4]) and expressing SFs as `f(h,r,t) = h^T g(r) t`, where `g(r)` is a 4x4 block matrix. This representation forms a tractable search space `G` that covers existing SFs and allows for novel combinations.\n        *   **Domain-Specific Analysis**: AutoSF incorporates domain-specific insights into KG relation properties (symmetric, anti-symmetric, inverse) to guide the search.\n        *   **Progressive Greedy Search Algorithm**: A greedy algorithm is developed to efficiently explore the search space.\n        *   **Filter and Predictor**: To further accelerate the search, a filter is introduced to avoid training redundant SFs with identical expressive abilities. A predictor, utilizing specifically designed **Symmetry-Related Features (SRF)**, is used to evaluate and prune unpromising SF candidates *before* full model training, significantly reducing computational cost.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A novel AutoML framework (AutoSF) for automatically designing KGE scoring functions.\n        *   A progressive greedy search algorithm tailored for the SF search space.\n        *   A filter mechanism to prune redundant SFs based on expressive equivalence.\n        *   A predictor model leveraging Symmetry-Related Features (SRF) to efficiently evaluate SF candidates.\n    *   **System Design/Architectural Innovations**:\n        *   The unified representation of BLM-based SFs as a 4x4 block matrix `g(r)`, which defines a structured and extensible search space.\n    *   **Theoretical Insights/Analysis**:\n        *   The observation that different BLMs essentially regularize the relational matrix `R` in distinct ways, and AutoSF aims to search for data-dependent regularization schemes.\n        *   Domain-specific analysis linking common relation types (symmetric, anti-symmetric, inverse) to specific requirements on the SF and its `g(r)` matrix, which informs the search constraints.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on two primary KGE tasks:\n        *   Link Prediction (predicting missing entities in triplets).\n        *   Triplet Classification (classifying whether a triplet is valid or invalid).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Evaluated on five popular benchmark datasets (FB15k, WN18, FB15k-237, WN18RR, YAGO3-10).\n        *   AutoSF consistently **outperforms state-of-the-art human-designed SFs** (e.g., ComplEx, SimplE, Analogy) across these benchmarks.\n        *   The searched SFs are demonstrated to be **KG-dependent** (different KGs yield different optimal SFs) and **new to the literature**.\n        *   Case studies on the searched SFs provide insights into KG properties, suggesting AutoSF can also be a tool for KG analysis.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The search space is primarily focused on **Bilinear Models (BLMs)**, as they are identified as the current state-of-the-art. TDMs and NNMs are not directly explored within the AutoSF search space.\n        *   The unified representation uses a 4-part split (k=4) for embeddings to ensure a **tractable search space**, implying a trade-off between generality and computational feasibility.\n    *   **Scope of Applicability**:\n        *   Primarily applicable to KGE tasks where the plausibility of triplets is measured by scoring functions.\n        *   The methodology is designed to adapt SFs to specific KG properties, making it suitable for diverse KGs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: AutoSF significantly advances the technical state-of-the-art in KGE by automating the design of scoring functions, moving beyond fixed, human-designed models. It demonstrates that data-dependent, automatically searched SFs can consistently outperform the best human-engineered ones.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for applying AutoML techniques to other components of KGE and broader graph machine learning problems.\n        *   The ability to discover novel, KG-dependent SFs can inspire new theoretical understandings of relation patterns and embedding techniques.\n        *   The framework provides a tool for analyzing KG properties through the structure of the discovered SFs, potentially leading to better-informed human designs in the future.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Scoring Functions (SFs)",
          "Automated Machine Learning (AutoML)",
          "AutoSF framework",
          "Bilinear Models (BLMs)",
          "Unified representation",
          "Progressive greedy search",
          "Symmetry-Related Features (SRF)",
          "Link Prediction",
          "Triplet Classification",
          "KG-dependent scoring functions",
          "Bi-level optimization",
          "Automated SF design"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "b594b21557395c6a8fa8356249373f8e318c2df2.pdf"
    },
    {
      "success": true,
      "doc_id": "542d8331a71d311f43af1ee99ae7c8c1",
      "summary": "Here's a focused summary of the paper \"TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics\" by \\cite{yang2019} for a literature review:\n\n---\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of knowledge graph completion, particularly the accurate prediction of links involving *complex relations* (1-N, N-1, N-N) within knowledge graph embedding (KGE) models.\n    *   **Importance & Challenge:** Knowledge graphs are inherently incomplete, making link prediction crucial for various applications like information retrieval. Existing translation-based KGE models (e.g., TransE, TransH, TransR) struggle with complex relations due to several limitations:\n        *   They primarily transmit semantics *from relations to entities*, neglecting semantic flow *between head and tail entities* and *from entities to relations*.\n        *   Many previous models introduce a large number of additional parameters (e.g., projection matrices in TransR), leading to poor scalability for large-scale knowledge graphs.\n        *   They often rely on linear transformations, which may be insufficient to capture complex semantic interactions.\n        *   Mathematically, when one entity and the relation are fixed, the other entity's vectors tend to cluster around a single center, limiting expressiveness.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work is positioned within the family of translation-based KGE models, building upon the foundational TransE \\cite{yang2019}. It discusses advancements made by TransH, TransR/CTransR, TransD, TranSparse, and GTrans, which attempted to overcome TransE's limitations.\n    *   **Limitations of Previous Solutions (as identified by \\cite{yang2019}):**\n        *   **Incomplete Semantic Transmission:** Prior models largely ignore the semantic transmission *between head and tail entities* and *from entities to relations*, focusing only on relation-to-entity semantics.\n        *   **Scalability Issues:** Many improved models introduce a significant number of additional parameters (e.g., TransR adds `k_e * k_r` parameters per relation), hindering their scalability for large knowledge graphs.\n        *   **Linearity Constraint:** The reliance on linear transformations for semantic translation may limit their ability to model intricate semantic relationships.\n        *   **Entity Vector Clustering:** Previous models can cause entity vectors to cluster around a single point when the other entity and relation are fixed, reducing their discriminative power.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{yang2019} proposes **TransMS (Translates and Transmits Multidirectional Semantics)**, a novel KGE method.\n        *   It projects entities `h, t` into `k`-dimensional vectors and relations `r` into `k`-dimensional vectors, introducing only *one additional scalar parameter* `\\lambda` per triplet.\n        *   **Multidirectional Semantic Transmission:**\n            *   **Entity-to-Entity (via relation):** It transforms the head entity embedding `h` by transmitting semantics from the tail entity `t` and relation `r` using a nonlinear function: `h' = tanh(t \\odot r) \\odot h`. Similarly, for the tail entity: `t' = tanh(h \\odot r) \\odot t`. (`\\odot` denotes Hadamard product).\n            *   **Entities-to-Relation:** It transforms the relation embedding `r` by adding a bias vector derived from the entities: `r' = r + \\lambda (h \\odot t)`.\n        *   The score function for a triplet `(h, r, t)` is defined as `f_r(h, t) = ||h' + r' - t'||_{L1/L2}`.\n    *   **Novelty & Differentiation:**\n        *   **Comprehensive Semantic Flow:** TransMS is novel in explicitly modeling semantic transmission in *multidirectional ways*: from head/tail entities to tail/head entities, and from entities to relations, in addition to the traditional relation-to-entity flow.\n        *   **Nonlinear Transformations:** It employs the `tanh` nonlinear function for entity transformations, moving beyond the limitations of purely linear approaches in previous translation models.\n        *   **Exceptional Parameter Efficiency:** It achieves this enhanced semantic modeling with remarkable parameter efficiency, adding only a single scalar parameter `\\lambda` per triplet, making it highly scalable compared to other advanced translation models.\n        *   **Addresses Clustering Issue:** By incorporating the other entity's semantics into the transformation, it prevents entity vectors from clustering around a single center.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of the TransMS model with its unique formulation for multidirectional semantic transmission, including nonlinear entity transformations (`tanh(e_1 \\odot r) \\odot e_2`) and entity-dependent relation bias (`r + \\lambda (h \\odot t)`).\n    *   **System Design/Architectural Innovations:** A highly parameter-efficient design that significantly improves scalability for large knowledge graphs by minimizing additional parameters compared to TransE.\n    *   **Theoretical Insights/Analysis:** The motivation draws an analogy to subject-predicate-object grammar, highlighting the intuitive need for semantic flow between all components of a triplet, which is then mathematically formalized. It also identifies and addresses the mathematical issue of entity vector clustering.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** The model's performance was validated through the standard link prediction task.\n    *   **Key Performance Metrics:** MeanRank and Hit@10 (both Raw and Filtered settings).\n    *   **Datasets:** Experiments were conducted on widely used benchmark datasets: FB15K, FB15K-237, WN18, and WN18RR.\n    *   **Comparison Results:** TransMS achieved substantial improvements over state-of-the-art baselines.\n        *   Notably, it improved Hit@10 for head entity prediction for N-1 relations by approximately 27.1% on FB15K.\n        *   It also improved Hit@10 for tail entity prediction for 1-N relations by approximately 24.8% on FB15K.\n        *   The model consistently outperformed TransE, TransH, TransR, CTransR, TransD, TranSparse, and GTrans (DW/SW variants) across various metrics, particularly demonstrating superior performance in Hit@10 scores. For instance, on FB15K (Filtered), TransMS achieved a Hit@10 of 80.1, surpassing TranSparse (79.9) and TransD (77.3).\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The choice of `tanh` as the nonlinear activation function and the specific form of the entity-to-relation bias (`\\lambda (h \\odot t)`) are empirical design choices. The model assumes equal dimensions for entity and relation embeddings (`k_e = k_r = k`).\n    *   **Scope of Applicability:** TransMS is primarily designed for knowledge graph completion, with a particular focus on improving link prediction performance for complex relations (1-N, N-1, N-N). Its applicability extends to general knowledge graph embedding tasks where capturing intricate semantic interactions is crucial.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** TransMS significantly advances the technical state-of-the-art in KGE by effectively addressing the long-standing challenge of complex relations. It demonstrates that a more comprehensive modeling of semantic flow, combined with nonlinear transformations and parameter efficiency, can lead to superior performance.\n    *   **Potential Impact on Future Research:**\n        *   It opens new avenues for research into multidirectional semantic modeling within KGE and other graph-based learning tasks.\n        *   The success of its parameter-efficient design could inspire the development of more scalable and effective KGE models for increasingly large knowledge graphs.\n        *   Future work could explore alternative nonlinear functions, more sophisticated entity-to-relation interaction mechanisms, or adaptive `\\lambda` parameters.",
      "intriguing_abstract": "Knowledge graph completion remains a critical challenge, particularly for accurately predicting links involving complex relations (1-N, N-1, N-N). Existing translation-based knowledge graph embedding (KGE) models often struggle with these, hampered by incomplete semantic transmission, reliance on linear transformations, and parameter inefficiency. We introduce **TransMS (Translates and Transmits Multidirectional Semantics)**, a novel KGE model that revolutionizes how semantic interactions are captured.\n\nTransMS uniquely models multidirectional semantic flow: from entities to other entities via relations using nonlinear transformations (e.g., `tanh`), and from entities to relations, all while introducing only a single scalar parameter per triplet. This innovative design not only overcomes the limitations of prior models, preventing entity vector clustering and enhancing expressiveness, but also achieves remarkable parameter efficiency. Extensive experiments on FB15K, FB15K-237, WN18, and WN18RR demonstrate that TransMS significantly outperforms state-of-the-art baselines in link prediction, achieving substantial improvements, especially for complex relations (e.g., 27.1% Hit@10 for N-1 relations on FB15K). TransMS sets a new benchmark for scalable and accurate knowledge graph embedding, paving the way for more intelligent and complete knowledge systems.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Knowledge Graph Completion",
        "Complex Relations (1-N",
        "N-1",
        "N-N)",
        "Link Prediction",
        "TransMS",
        "Multidirectional Semantic Transmission",
        "Nonlinear Transformations",
        "Parameter Efficiency",
        "Entity Vector Clustering",
        "Translation-based KGE Models",
        "Improved Link Prediction Performance",
        "Scalability"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/3e3a84bbceba79843ca1105939b2eb438c149e9e.pdf",
      "citation_key": "yang2019",
      "metadata": {
        "title": "TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics",
        "authors": [
          "Shihui Yang",
          "Jidong Tian",
          "Honglun Zhang",
          "Junchi Yan",
          "Hao He",
          "Yaohui Jin"
        ],
        "published_date": "2019",
        "abstract": "Knowledge graph embedding, which projects the symbolic relations and entities onto low-dimension continuous spaces, is essential to knowledge graph completion. Recently, translation-based embedding models (e.g. TransE) have aroused increasing attention for their simplicity and effectiveness. These models attempt to translate semantics from head entities to tail entities with the relations and infer richer facts outside the knowledge graph. In this paper, we propose a novel knowledge graph embedding method named TransMS, which translates and transmits multidirectional semantics: i) the semantics of head/tail entities and relations to tail/head entities with nonlinear functionsand ii) the semantics from entities to relations with linear bias vectors. Our model has merely one additional parameterthan TransE for each triplet, which results in its better scalability in large-scale knowledge graph. Experiments show that TransMS achieves substantial improvements against state-of-the-art baselines, especially the Hit@10s of head entity prediction for N-1 relations and tail entity prediction for 1-N relations improved by about 27.1% and 24.8% on FB15K database respectively.",
        "file_path": "paper_data/knowledge_graph_embedding/3e3a84bbceba79843ca1105939b2eb438c149e9e.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics\" by \\cite{yang2019} for a literature review:\n\n---\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of knowledge graph completion, particularly the accurate prediction of links involving *complex relations* (1-N, N-1, N-N) within knowledge graph embedding (KGE) models.\n    *   **Importance & Challenge:** Knowledge graphs are inherently incomplete, making link prediction crucial for various applications like information retrieval. Existing translation-based KGE models (e.g., TransE, TransH, TransR) struggle with complex relations due to several limitations:\n        *   They primarily transmit semantics *from relations to entities*, neglecting semantic flow *between head and tail entities* and *from entities to relations*.\n        *   Many previous models introduce a large number of additional parameters (e.g., projection matrices in TransR), leading to poor scalability for large-scale knowledge graphs.\n        *   They often rely on linear transformations, which may be insufficient to capture complex semantic interactions.\n        *   Mathematically, when one entity and the relation are fixed, the other entity's vectors tend to cluster around a single center, limiting expressiveness.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work is positioned within the family of translation-based KGE models, building upon the foundational TransE \\cite{yang2019}. It discusses advancements made by TransH, TransR/CTransR, TransD, TranSparse, and GTrans, which attempted to overcome TransE's limitations.\n    *   **Limitations of Previous Solutions (as identified by \\cite{yang2019}):**\n        *   **Incomplete Semantic Transmission:** Prior models largely ignore the semantic transmission *between head and tail entities* and *from entities to relations*, focusing only on relation-to-entity semantics.\n        *   **Scalability Issues:** Many improved models introduce a significant number of additional parameters (e.g., TransR adds `k_e * k_r` parameters per relation), hindering their scalability for large knowledge graphs.\n        *   **Linearity Constraint:** The reliance on linear transformations for semantic translation may limit their ability to model intricate semantic relationships.\n        *   **Entity Vector Clustering:** Previous models can cause entity vectors to cluster around a single point when the other entity and relation are fixed, reducing their discriminative power.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{yang2019} proposes **TransMS (Translates and Transmits Multidirectional Semantics)**, a novel KGE method.\n        *   It projects entities `h, t` into `k`-dimensional vectors and relations `r` into `k`-dimensional vectors, introducing only *one additional scalar parameter* `\\lambda` per triplet.\n        *   **Multidirectional Semantic Transmission:**\n            *   **Entity-to-Entity (via relation):** It transforms the head entity embedding `h` by transmitting semantics from the tail entity `t` and relation `r` using a nonlinear function: `h' = tanh(t \\odot r) \\odot h`. Similarly, for the tail entity: `t' = tanh(h \\odot r) \\odot t`. (`\\odot` denotes Hadamard product).\n            *   **Entities-to-Relation:** It transforms the relation embedding `r` by adding a bias vector derived from the entities: `r' = r + \\lambda (h \\odot t)`.\n        *   The score function for a triplet `(h, r, t)` is defined as `f_r(h, t) = ||h' + r' - t'||_{L1/L2}`.\n    *   **Novelty & Differentiation:**\n        *   **Comprehensive Semantic Flow:** TransMS is novel in explicitly modeling semantic transmission in *multidirectional ways*: from head/tail entities to tail/head entities, and from entities to relations, in addition to the traditional relation-to-entity flow.\n        *   **Nonlinear Transformations:** It employs the `tanh` nonlinear function for entity transformations, moving beyond the limitations of purely linear approaches in previous translation models.\n        *   **Exceptional Parameter Efficiency:** It achieves this enhanced semantic modeling with remarkable parameter efficiency, adding only a single scalar parameter `\\lambda` per triplet, making it highly scalable compared to other advanced translation models.\n        *   **Addresses Clustering Issue:** By incorporating the other entity's semantics into the transformation, it prevents entity vectors from clustering around a single center.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of the TransMS model with its unique formulation for multidirectional semantic transmission, including nonlinear entity transformations (`tanh(e_1 \\odot r) \\odot e_2`) and entity-dependent relation bias (`r + \\lambda (h \\odot t)`).\n    *   **System Design/Architectural Innovations:** A highly parameter-efficient design that significantly improves scalability for large knowledge graphs by minimizing additional parameters compared to TransE.\n    *   **Theoretical Insights/Analysis:** The motivation draws an analogy to subject-predicate-object grammar, highlighting the intuitive need for semantic flow between all components of a triplet, which is then mathematically formalized. It also identifies and addresses the mathematical issue of entity vector clustering.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** The model's performance was validated through the standard link prediction task.\n    *   **Key Performance Metrics:** MeanRank and Hit@10 (both Raw and Filtered settings).\n    *   **Datasets:** Experiments were conducted on widely used benchmark datasets: FB15K, FB15K-237, WN18, and WN18RR.\n    *   **Comparison Results:** TransMS achieved substantial improvements over state-of-the-art baselines.\n        *   Notably, it improved Hit@10 for head entity prediction for N-1 relations by approximately 27.1% on FB15K.\n        *   It also improved Hit@10 for tail entity prediction for 1-N relations by approximately 24.8% on FB15K.\n        *   The model consistently outperformed TransE, TransH, TransR, CTransR, TransD, TranSparse, and GTrans (DW/SW variants) across various metrics, particularly demonstrating superior performance in Hit@10 scores. For instance, on FB15K (Filtered), TransMS achieved a Hit@10 of 80.1, surpassing TranSparse (79.9) and TransD (77.3).\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The choice of `tanh` as the nonlinear activation function and the specific form of the entity-to-relation bias (`\\lambda (h \\odot t)`) are empirical design choices. The model assumes equal dimensions for entity and relation embeddings (`k_e = k_r = k`).\n    *   **Scope of Applicability:** TransMS is primarily designed for knowledge graph completion, with a particular focus on improving link prediction performance for complex relations (1-N, N-1, N-N). Its applicability extends to general knowledge graph embedding tasks where capturing intricate semantic interactions is crucial.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** TransMS significantly advances the technical state-of-the-art in KGE by effectively addressing the long-standing challenge of complex relations. It demonstrates that a more comprehensive modeling of semantic flow, combined with nonlinear transformations and parameter efficiency, can lead to superior performance.\n    *   **Potential Impact on Future Research:**\n        *   It opens new avenues for research into multidirectional semantic modeling within KGE and other graph-based learning tasks.\n        *   The success of its parameter-efficient design could inspire the development of more scalable and effective KGE models for increasingly large knowledge graphs.\n        *   Future work could explore alternative nonlinear functions, more sophisticated entity-to-relation interaction mechanisms, or adaptive `\\lambda` parameters.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Knowledge Graph Completion",
          "Complex Relations (1-N",
          "N-1",
          "N-N)",
          "Link Prediction",
          "TransMS",
          "Multidirectional Semantic Transmission",
          "Nonlinear Transformations",
          "Parameter Efficiency",
          "Entity Vector Clustering",
          "Translation-based KGE Models",
          "Improved Link Prediction Performance",
          "Scalability"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "3e3a84bbceba79843ca1105939b2eb438c149e9e.pdf"
    },
    {
      "success": true,
      "doc_id": "02b56f8bce482b957b4f97877a773253",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Learning effective embeddings for entities and relations in Temporal Knowledge Graphs (TKGs), known as Temporal Knowledge Graph Embedding (TKGE).\n    *   **Importance & Challenge:** Existing Graph Neural Network (GNN)-based models, despite their promising results, struggle to directly capture the complex interactions of *multi-facts* occurring at *different timestamps*. This limitation hinders their ability to fully leverage the temporal and relational dynamics within TKGs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon and aims to surpass previous GNN-based models for TKGE.\n    *   **Limitations of Previous Solutions:** Prior GNN models are unable to directly model and capture the interactions among multiple facts that happen at varying timestamps, leading to a less comprehensive understanding of TKG dynamics \\cite{xie2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a **Time-Aware Relational Graph Attention Model (TARGAT)**. TARGAT treats multi-facts across different timestamps as a unified graph to explicitly capture their interactions.\n    *   **Novelty:**\n        *   **Dynamic Time-Aware Relational Generator:** A novel component that dynamically generates a series of *time-aware relational message transformation matrices*. This unifies the modeling of relations and timestamp information.\n        *   **Time-Aware Feature Projection and Aggregation:** These generated matrices are used to project neighborhood features into distinct time-aware spaces, followed by aggregation to explicitly capture multi-fact interactions.\n        *   **Temporal Transformer Classifier:** Utilized for learning query quadruple representations and predicting missing entities, integrating temporal context into the final prediction.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduction of a **relational generator** that dynamically creates time-aware relational message transformation matrices, jointly modeling relations and timestamps \\cite{xie2023}.\n        *   A mechanism to project and aggregate neighborhood features in different time-aware spaces, specifically designed to capture multi-fact interactions \\cite{xie2023}.\n    *   **System Design/Architectural Innovations:** The TARGAT architecture unifies multi-facts at different timestamps into a single graph processing framework, enhancing the capture of temporal dynamics.\n    *   **Theoretical Insights/Analysis:** The approach implicitly suggests that dynamic, time-aware transformations are crucial for effectively modeling complex temporal and relational dependencies in TKGs.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The TARGAT model was evaluated against existing methods.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   TARGAT significantly outperforms GNN-based models.\n        *   It achieves new state-of-the-art results on four popular benchmark datasets, demonstrating its superior performance in TKGE tasks \\cite{xie2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The provided abstract does not explicitly detail technical limitations or assumptions beyond addressing the specific challenge of multi-fact interactions at different timestamps.\n    *   **Scope of Applicability:** The model is specifically designed for Temporal Knowledge Graph Embedding (TKGE) tasks, particularly focusing on improving the capture of multi-fact interactions over time.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** TARGAT advances the technical state-of-the-art in TKGE by introducing a novel mechanism to explicitly model time-aware relational interactions and multi-facts, overcoming a key limitation of prior GNN-based approaches \\cite{xie2023}.\n    *   **Potential Impact on Future Research:** This work opens avenues for future research into dynamic, time-aware message passing mechanisms in graph neural networks, particularly for complex temporal graph structures where interactions across different time points are critical. It highlights the importance of unifying relational and temporal information in a dynamic transformation process.",
      "intriguing_abstract": "Unlocking the full potential of Temporal Knowledge Graphs (TKGs) hinges on effectively modeling their intricate, evolving dynamics. While Graph Neural Networks (GNNs) show promise for Temporal Knowledge Graph Embedding (TKGE), they notoriously falter in capturing the complex interplay of *multi-facts* occurring at *varying timestamps*. We introduce **TARGAT (Time-Aware Relational Graph Attention Model)**, a novel framework that fundamentally redefines how temporal and relational information are processed.\n\nTARGAT's core innovation lies in its **Dynamic Time-Aware Relational Generator**, which dynamically creates unique *time-aware relational message transformation matrices*. This groundbreaking mechanism unifies the modeling of relations and timestamps, enabling the explicit projection and aggregation of neighborhood features into distinct time-aware spaces. This directly captures the elusive interactions among multi-facts across different temporal instances. Coupled with a Temporal Transformer Classifier, TARGAT achieves unprecedented performance, significantly outperforming existing GNN-based models and establishing new state-of-the-art results on four benchmark datasets. TARGAT represents a crucial advancement in TKGE, paving the way for more sophisticated temporal reasoning and dynamic graph analysis.",
      "keywords": [
        "Temporal Knowledge Graphs",
        "Temporal Knowledge Graph Embedding",
        "Graph Neural Networks",
        "Multi-facts",
        "Timestamps",
        "Time-Aware Relational Graph Attention Model (TARGAT)",
        "Dynamic Time-Aware Relational Generator",
        "Time-aware relational message transformation matrices",
        "Time-aware feature projection and aggregation",
        "Temporal Transformer Classifier",
        "Unifying relations and timestamps",
        "Explicit capture of multi-fact interactions",
        "State-of-the-art results"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/b3f0cdc217a3d192d2671e44913542903c94105b.pdf",
      "citation_key": "xie2023",
      "metadata": {
        "title": "TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding",
        "authors": [
          "Zhiwen Xie",
          "Runjie Zhu",
          "Jin Liu",
          "Guangyou Zhou",
          "J. Huang"
        ],
        "published_date": "2023",
        "abstract": "Temporal knowledge graph embedding (TKGE) aims to learn the embedding of entities and relations in a temporal knowledge graph (TKG). Although the previous graph neural networks (GNN) based models have achieved promising results, they cannot directly capture the interactions of multi-facts at different timestamps. To address the above limitation, we propose a time-aware relational graph attention model (TARGAT), which takes the multi-facts at different timestamps as a unified graph. First, we develop a relational generator to dynamically generate a series of time-aware relational message transformation matrices, which jointly models the relations and the timestamp information into a unified way. Then, we apply the generated message transformation matrices to project the neighborhood features into different time-aware spaces and aggregate these neighborhood features to explicitly capture the interactions of multi-facts. Finally, a temporal transformer classifier is applied to learn the representation of the query quadruples and predict the missing entities. The experimental results show that our TARGAT model beats the GNN-based models by a large margin and achieves new state-of-the-art results on four popular benchmark datasets.",
        "file_path": "paper_data/knowledge_graph_embedding/b3f0cdc217a3d192d2671e44913542903c94105b.pdf",
        "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Learning effective embeddings for entities and relations in Temporal Knowledge Graphs (TKGs), known as Temporal Knowledge Graph Embedding (TKGE).\n    *   **Importance & Challenge:** Existing Graph Neural Network (GNN)-based models, despite their promising results, struggle to directly capture the complex interactions of *multi-facts* occurring at *different timestamps*. This limitation hinders their ability to fully leverage the temporal and relational dynamics within TKGs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon and aims to surpass previous GNN-based models for TKGE.\n    *   **Limitations of Previous Solutions:** Prior GNN models are unable to directly model and capture the interactions among multiple facts that happen at varying timestamps, leading to a less comprehensive understanding of TKG dynamics \\cite{xie2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a **Time-Aware Relational Graph Attention Model (TARGAT)**. TARGAT treats multi-facts across different timestamps as a unified graph to explicitly capture their interactions.\n    *   **Novelty:**\n        *   **Dynamic Time-Aware Relational Generator:** A novel component that dynamically generates a series of *time-aware relational message transformation matrices*. This unifies the modeling of relations and timestamp information.\n        *   **Time-Aware Feature Projection and Aggregation:** These generated matrices are used to project neighborhood features into distinct time-aware spaces, followed by aggregation to explicitly capture multi-fact interactions.\n        *   **Temporal Transformer Classifier:** Utilized for learning query quadruple representations and predicting missing entities, integrating temporal context into the final prediction.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduction of a **relational generator** that dynamically creates time-aware relational message transformation matrices, jointly modeling relations and timestamps \\cite{xie2023}.\n        *   A mechanism to project and aggregate neighborhood features in different time-aware spaces, specifically designed to capture multi-fact interactions \\cite{xie2023}.\n    *   **System Design/Architectural Innovations:** The TARGAT architecture unifies multi-facts at different timestamps into a single graph processing framework, enhancing the capture of temporal dynamics.\n    *   **Theoretical Insights/Analysis:** The approach implicitly suggests that dynamic, time-aware transformations are crucial for effectively modeling complex temporal and relational dependencies in TKGs.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The TARGAT model was evaluated against existing methods.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   TARGAT significantly outperforms GNN-based models.\n        *   It achieves new state-of-the-art results on four popular benchmark datasets, demonstrating its superior performance in TKGE tasks \\cite{xie2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The provided abstract does not explicitly detail technical limitations or assumptions beyond addressing the specific challenge of multi-fact interactions at different timestamps.\n    *   **Scope of Applicability:** The model is specifically designed for Temporal Knowledge Graph Embedding (TKGE) tasks, particularly focusing on improving the capture of multi-fact interactions over time.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** TARGAT advances the technical state-of-the-art in TKGE by introducing a novel mechanism to explicitly model time-aware relational interactions and multi-facts, overcoming a key limitation of prior GNN-based approaches \\cite{xie2023}.\n    *   **Potential Impact on Future Research:** This work opens avenues for future research into dynamic, time-aware message passing mechanisms in graph neural networks, particularly for complex temporal graph structures where interactions across different time points are critical. It highlights the importance of unifying relational and temporal information in a dynamic transformation process.",
        "keywords": [
          "Temporal Knowledge Graphs",
          "Temporal Knowledge Graph Embedding",
          "Graph Neural Networks",
          "Multi-facts",
          "Timestamps",
          "Time-Aware Relational Graph Attention Model (TARGAT)",
          "Dynamic Time-Aware Relational Generator",
          "Time-aware relational message transformation matrices",
          "Time-aware feature projection and aggregation",
          "Temporal Transformer Classifier",
          "Unifying relations and timestamps",
          "Explicit capture of multi-fact interactions",
          "State-of-the-art results"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "b3f0cdc217a3d192d2671e44913542903c94105b.pdf"
    },
    {
      "success": true,
      "doc_id": "6ee9ee88d480016bf69f25d8f44061e3",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   Temporal Knowledge Graphs (TKGs) contain complex and mixed geometric structures (e.g., hierarchical, ring, chain).\n    *   Embedding TKGs into traditional Euclidean space, as commonly done by TKG Completion (TKGC) models, struggles to effectively represent these high-dimensional, nonlinear data and diverse geometric structures. This limitation hinders accurate TKG completion.\n\n*   **Related Work & Positioning**\n    *   Existing TKG Completion (TKGC) models primarily rely on Euclidean space embeddings \\cite{wang2024}.\n    *   The limitation of previous solutions is their inability to adequately capture the complex, mixed geometric structures inherent in TKGs when restricted to a single, flat Euclidean geometry \\cite{wang2024}.\n\n*   **Technical Approach & Innovation**\n    *   The paper proposes a novel TKGC model called Multicurvature Adaptive Embedding (MADE) \\cite{wang2024}.\n    *   MADE models TKGs in *multicurvature spaces*, including flat Euclidean (zero curvature), hyperbolic (negative curvature), and hyperspherical (positive curvature) spaces, to accommodate various geometric structures \\cite{wang2024}.\n    *   It employs a *data-driven weighting mechanism* to assign different weights to each curvature space, dynamically strengthening the most suitable spaces for modeling and weakening less appropriate ones \\cite{wang2024}.\n    *   A *quadruplet distributor (QD)* is introduced to facilitate information interaction within each geometric space \\cite{wang2024}.\n    *   An *innovative temporal regularization* is developed to enhance the smoothness of timestamp embeddings by reinforcing the correlation between neighboring timestamps \\cite{wang2024}.\n\n*   **Key Technical Contributions**\n    *   **Novel Model:** Multicurvature Adaptive Embedding (MADE) for TKGC \\cite{wang2024}.\n    *   **Multicurvature Embedding:** Utilizing a combination of Euclidean, hyperbolic, and hyperspherical spaces to model diverse TKG geometries \\cite{wang2024}.\n    *   **Adaptive Weighting:** A data-driven mechanism to dynamically weight the contribution of different curvature spaces \\cite{wang2024}.\n    *   **Quadruplet Distributor (QD):** A novel component to assist information interaction across geometric spaces \\cite{wang2024}.\n    *   **Temporal Regularization:** An innovative method to ensure smoothness and correlation among neighboring timestamp embeddings \\cite{wang2024}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted to evaluate the performance of MADE \\cite{wang2024}.\n    *   Key results indicate that MADE significantly outperforms existing state-of-the-art TKGC models \\cite{wang2024}.\n\n*   **Limitations & Scope**\n    *   The primary scope of this work is focused on improving Temporal Knowledge Graph Completion (TKGC) by addressing the geometric representation challenges \\cite{wang2024}.\n    *   Specific technical limitations or assumptions beyond the inherent complexity of multicurvature modeling are not detailed in the provided abstract.\n\n*   **Technical Significance**\n    *   MADE advances the technical state-of-the-art in TKGC by providing a more robust and flexible framework for embedding complex TKG structures \\cite{wang2024}.\n    *   Its multicurvature adaptive approach and novel components (QD, temporal regularization) offer a promising direction for future research in knowledge graph embeddings, particularly for data with diverse and evolving geometric properties \\cite{wang2024}.",
      "intriguing_abstract": "Temporal Knowledge Graphs (TKGs) are rich with intricate, mixed geometric structures, yet their embedding into traditional Euclidean spaces severely limits the accuracy of Temporal Knowledge Graph Completion (TKGC). We introduce Multicurvature Adaptive Embedding (MADE), a novel TKGC model that shatters this limitation by dynamically modeling TKGs across a spectrum of geometric spaces: flat Euclidean, negatively curved hyperbolic, and positively curved hyperspherical.\n\nMADE's innovation lies in its data-driven weighting mechanism, which adaptively strengthens the most suitable curvature spaces for different TKG components. Furthermore, we propose a Quadruplet Distributor (QD) to facilitate robust information interaction within each geometric space and an innovative temporal regularization technique to ensure smooth, correlated timestamp embeddings. Extensive experiments demonstrate that MADE significantly outperforms existing state-of-the-art TKGC models, offering a more robust and flexible framework for representing complex TKG dynamics. This work opens new avenues for knowledge graph embeddings, particularly for data exhibiting diverse and evolving geometric properties.",
      "keywords": [
        "Temporal Knowledge Graphs (TKGs)",
        "TKG Completion (TKGC)",
        "Multicurvature Adaptive Embedding (MADE)",
        "multicurvature spaces",
        "Euclidean",
        "hyperbolic",
        "hyperspherical spaces",
        "complex geometric structures",
        "data-driven weighting mechanism",
        "quadruplet distributor (QD)",
        "innovative temporal regularization",
        "timestamp embeddings",
        "state-of-the-art performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/52eb7f27cdfbf359096b8b5ef56b2c2826beb660.pdf",
      "citation_key": "wang2024",
      "metadata": {
        "title": "MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion",
        "authors": [
          "Jiapu Wang",
          "Boyue Wang",
          "Junbin Gao",
          "Shirui Pan",
          "Tengfei Liu",
          "Baocai Yin",
          "Wen Gao"
        ],
        "published_date": "2024",
        "abstract": "Temporal knowledge graphs (TKGs) are receiving increased attention due to their time-dependent properties and the evolving nature of knowledge over time. TKGs typically contain complex geometric structures, such as hierarchical, ring, and chain structures, which can often be mixed together. However, embedding TKGs into Euclidean space, as is typically done with TKG completion (TKGC) models, presents a challenge when dealing with high-dimensional nonlinear data and complex geometric structures. To address this issue, we propose a novel TKGC model called multicurvature adaptive embedding (MADE). MADE models TKGs in multicurvature spaces, including flat Euclidean space (zero curvature), hyperbolic space (negative curvature), and hyperspherical space (positive curvature), to handle multiple geometric structures. We assign different weights to different curvature spaces in a data-driven manner to strengthen the ideal curvature spaces for modeling and weaken the inappropriate ones. Additionally, we introduce the quadruplet distributor (QD) to assist the information interaction in each geometric space. Ultimately, we develop an innovative temporal regularization to enhance the smoothness of timestamp embeddings by strengthening the correlation of neighboring timestamps. Experimental results show that MADE outperforms the existing state-of-the-art TKGC models.",
        "file_path": "paper_data/knowledge_graph_embedding/52eb7f27cdfbf359096b8b5ef56b2c2826beb660.pdf",
        "venue": "IEEE Transactions on Cybernetics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   Temporal Knowledge Graphs (TKGs) contain complex and mixed geometric structures (e.g., hierarchical, ring, chain).\n    *   Embedding TKGs into traditional Euclidean space, as commonly done by TKG Completion (TKGC) models, struggles to effectively represent these high-dimensional, nonlinear data and diverse geometric structures. This limitation hinders accurate TKG completion.\n\n*   **Related Work & Positioning**\n    *   Existing TKG Completion (TKGC) models primarily rely on Euclidean space embeddings \\cite{wang2024}.\n    *   The limitation of previous solutions is their inability to adequately capture the complex, mixed geometric structures inherent in TKGs when restricted to a single, flat Euclidean geometry \\cite{wang2024}.\n\n*   **Technical Approach & Innovation**\n    *   The paper proposes a novel TKGC model called Multicurvature Adaptive Embedding (MADE) \\cite{wang2024}.\n    *   MADE models TKGs in *multicurvature spaces*, including flat Euclidean (zero curvature), hyperbolic (negative curvature), and hyperspherical (positive curvature) spaces, to accommodate various geometric structures \\cite{wang2024}.\n    *   It employs a *data-driven weighting mechanism* to assign different weights to each curvature space, dynamically strengthening the most suitable spaces for modeling and weakening less appropriate ones \\cite{wang2024}.\n    *   A *quadruplet distributor (QD)* is introduced to facilitate information interaction within each geometric space \\cite{wang2024}.\n    *   An *innovative temporal regularization* is developed to enhance the smoothness of timestamp embeddings by reinforcing the correlation between neighboring timestamps \\cite{wang2024}.\n\n*   **Key Technical Contributions**\n    *   **Novel Model:** Multicurvature Adaptive Embedding (MADE) for TKGC \\cite{wang2024}.\n    *   **Multicurvature Embedding:** Utilizing a combination of Euclidean, hyperbolic, and hyperspherical spaces to model diverse TKG geometries \\cite{wang2024}.\n    *   **Adaptive Weighting:** A data-driven mechanism to dynamically weight the contribution of different curvature spaces \\cite{wang2024}.\n    *   **Quadruplet Distributor (QD):** A novel component to assist information interaction across geometric spaces \\cite{wang2024}.\n    *   **Temporal Regularization:** An innovative method to ensure smoothness and correlation among neighboring timestamp embeddings \\cite{wang2024}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted to evaluate the performance of MADE \\cite{wang2024}.\n    *   Key results indicate that MADE significantly outperforms existing state-of-the-art TKGC models \\cite{wang2024}.\n\n*   **Limitations & Scope**\n    *   The primary scope of this work is focused on improving Temporal Knowledge Graph Completion (TKGC) by addressing the geometric representation challenges \\cite{wang2024}.\n    *   Specific technical limitations or assumptions beyond the inherent complexity of multicurvature modeling are not detailed in the provided abstract.\n\n*   **Technical Significance**\n    *   MADE advances the technical state-of-the-art in TKGC by providing a more robust and flexible framework for embedding complex TKG structures \\cite{wang2024}.\n    *   Its multicurvature adaptive approach and novel components (QD, temporal regularization) offer a promising direction for future research in knowledge graph embeddings, particularly for data with diverse and evolving geometric properties \\cite{wang2024}.",
        "keywords": [
          "Temporal Knowledge Graphs (TKGs)",
          "TKG Completion (TKGC)",
          "Multicurvature Adaptive Embedding (MADE)",
          "multicurvature spaces",
          "Euclidean",
          "hyperbolic",
          "hyperspherical spaces",
          "complex geometric structures",
          "data-driven weighting mechanism",
          "quadruplet distributor (QD)",
          "innovative temporal regularization",
          "timestamp embeddings",
          "state-of-the-art performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660.pdf"
    },
    {
      "success": true,
      "doc_id": "b8c3ec6fe233913bb4a5f3943ac0d34f",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical problem of knowledge representation in knowledge engineering and artificial intelligence.\n    *   Specifically, it focuses on knowledge embedding, which represents knowledge graph entities and relations as low-dimensional, continuous vectors to make knowledge graphs compatible with numerical machine learning models.\n\n*   **Related Work & Positioning**\n    *   Existing major knowledge embedding methods primarily rely on geometric translation to design score functions.\n    *   The key limitation identified is that these geometric translation-based methods are \"weak-semantic\" for natural language processing tasks.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method proposed is a novel model based on a **multi-view clustering framework** \\cite{xiao2019}.\n    *   This approach is innovative because it is designed to generate **semantic representations** of knowledge elements (entities and relations), directly addressing the semantic weakness of previous geometric translation models.\n    *   Beyond just embeddings, the paper also presents an empowered solution for **entity retrieval that incorporates entity descriptions**, leveraging the semantic model \\cite{xiao2019}.\n\n*   **Key Technical Contributions**\n    *   Introduction of a novel knowledge embedding model built upon a multi-view clustering framework for generating semantic representations \\cite{xiao2019}.\n    *   Development of an empowered entity retrieval solution that utilizes entity descriptions, enabled by the proposed semantic model \\cite{xiao2019}.\n\n*   **Experimental Validation**\n    *   Extensive experiments were conducted to evaluate the model's performance.\n    *   The model was tested on multiple tasks: knowledge graph completion, triple classification, entity classification, and entity retrieval.\n    *   Key results indicate that the proposed model achieves **substantial improvements against baselines** across all evaluated tasks \\cite{xiao2019}.\n\n*   **Limitations & Scope**\n    *   The provided abstract does not explicitly detail specific technical limitations or assumptions of the proposed model, nor does it define a narrow scope of applicability beyond general knowledge graph tasks.\n\n*   **Technical Significance**\n    *   The work advances the technical state-of-the-art by overcoming the \"weak-semantic\" nature of traditional geometric translation-based knowledge embedding methods, offering a more semantically rich representation.\n    *   Its potential impact includes improving performance in various knowledge graph-related tasks, particularly those requiring deeper semantic understanding for natural language processing applications, and enhancing entity retrieval capabilities.",
      "intriguing_abstract": "Traditional knowledge embedding methods, often relying on geometric translation, fall short in capturing the rich semantic nuances crucial for advanced AI and natural language processing, leading to \"weak-semantic\" representations. This paper introduces a transformative approach to knowledge representation, overcoming these limitations. We propose a novel knowledge embedding model founded on an innovative **multi-view clustering framework** specifically engineered to generate truly **semantic representations** of knowledge graph entities and relations. This paradigm directly addresses the inherent semantic weakness of previous models. Beyond embeddings, our work also presents an empowered **entity retrieval** solution that effectively incorporates **entity descriptions**, leveraging the deep semantic understanding provided by our model. Extensive experiments across knowledge graph completion, triple classification, entity classification, and entity retrieval demonstrate **substantial improvements** over existing baselines. This research significantly advances the state-of-the-art in knowledge embedding, paving the way for more intelligent, semantically aware AI systems and robust NLP applications.",
      "keywords": [
        "Knowledge embedding",
        "knowledge graphs",
        "multi-view clustering framework",
        "semantic representations",
        "entity retrieval",
        "geometric translation-based methods",
        "weak-semantic knowledge embedding",
        "natural language processing",
        "knowledge graph completion",
        "entity descriptions",
        "substantial performance improvements",
        "artificial intelligence"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/ecb80d1e5507e163be4a6757b00c8809a2de4863.pdf",
      "citation_key": "xiao2019",
      "metadata": {
        "title": "Knowledge Graph Embedding Based on Multi-View Clustering Framework",
        "authors": [
          "Han Xiao",
          "Yidong Chen",
          "X. Shi"
        ],
        "published_date": "2019",
        "abstract": "Knowledge representation is one of the critical problems in knowledge engineering and artificial intelligence, while knowledge embedding as a knowledge representation methodology indicates entities and relations in knowledge graph as low-dimensional, continuous vectors. In this way, knowledge graph is compatible with numerical machine learning models. Major knowledge embedding methods employ geometric translation to design score function, which is weak-semantic for natural language processing. To overcome this disadvantage, in this paper, we propose our model based on multi-view clustering framework, which could generate semantic representations of knowledge elements (i.e., entities/relations). With our semantic model, we also present an empowered solution to entity retrieval with entity description. Extensive experiments show that our model achieves substantial improvements against baselines on the task of knowledge graph completion, triple classification, entity classification, and entity retrieval.",
        "file_path": "paper_data/knowledge_graph_embedding/ecb80d1e5507e163be4a6757b00c8809a2de4863.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical problem of knowledge representation in knowledge engineering and artificial intelligence.\n    *   Specifically, it focuses on knowledge embedding, which represents knowledge graph entities and relations as low-dimensional, continuous vectors to make knowledge graphs compatible with numerical machine learning models.\n\n*   **Related Work & Positioning**\n    *   Existing major knowledge embedding methods primarily rely on geometric translation to design score functions.\n    *   The key limitation identified is that these geometric translation-based methods are \"weak-semantic\" for natural language processing tasks.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method proposed is a novel model based on a **multi-view clustering framework** \\cite{xiao2019}.\n    *   This approach is innovative because it is designed to generate **semantic representations** of knowledge elements (entities and relations), directly addressing the semantic weakness of previous geometric translation models.\n    *   Beyond just embeddings, the paper also presents an empowered solution for **entity retrieval that incorporates entity descriptions**, leveraging the semantic model \\cite{xiao2019}.\n\n*   **Key Technical Contributions**\n    *   Introduction of a novel knowledge embedding model built upon a multi-view clustering framework for generating semantic representations \\cite{xiao2019}.\n    *   Development of an empowered entity retrieval solution that utilizes entity descriptions, enabled by the proposed semantic model \\cite{xiao2019}.\n\n*   **Experimental Validation**\n    *   Extensive experiments were conducted to evaluate the model's performance.\n    *   The model was tested on multiple tasks: knowledge graph completion, triple classification, entity classification, and entity retrieval.\n    *   Key results indicate that the proposed model achieves **substantial improvements against baselines** across all evaluated tasks \\cite{xiao2019}.\n\n*   **Limitations & Scope**\n    *   The provided abstract does not explicitly detail specific technical limitations or assumptions of the proposed model, nor does it define a narrow scope of applicability beyond general knowledge graph tasks.\n\n*   **Technical Significance**\n    *   The work advances the technical state-of-the-art by overcoming the \"weak-semantic\" nature of traditional geometric translation-based knowledge embedding methods, offering a more semantically rich representation.\n    *   Its potential impact includes improving performance in various knowledge graph-related tasks, particularly those requiring deeper semantic understanding for natural language processing applications, and enhancing entity retrieval capabilities.",
        "keywords": [
          "Knowledge embedding",
          "knowledge graphs",
          "multi-view clustering framework",
          "semantic representations",
          "entity retrieval",
          "geometric translation-based methods",
          "weak-semantic knowledge embedding",
          "natural language processing",
          "knowledge graph completion",
          "entity descriptions",
          "substantial performance improvements",
          "artificial intelligence"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "ecb80d1e5507e163be4a6757b00c8809a2de4863.pdf"
    },
    {
      "success": true,
      "doc_id": "863e250040dda536bd8cffc18e9289e4",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Knowledge Graph (KG) representation learning techniques, which generate continuous embeddings for entities and relations, consume a large amount of storage and memory, particularly for large KGs.\n    *   **Motivation**: This high resource consumption is a critical barrier, preventing the deployment of these powerful AI techniques in many real-world applications due to practical limitations.\n\n*   **Related Work & Positioning**\n    *   **Positioning**: The work addresses a fundamental practical limitation (storage and memory footprint) inherent in existing continuous KG embedding techniques.\n    *   **Limitations of previous solutions**: Current KG embedding methods, by design, produce large continuous vectors for each entity, leading to substantial storage and memory overheads that hinder their scalability and real-world applicability.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes an approach to compress the KG embedding layer.\n    *   **Mechanism**: It represents each entity in the KG as a vector of discrete codes.\n    *   **Composition**: The full, continuous embeddings are then composed from these discrete codes.\n    *   **Integration**: The approach is designed for end-to-end training and can be integrated with simple modifications into any existing KG embedding technique \\cite{sachan2020}.\n    *   **Novelty**: The core innovation lies in moving away from directly storing large continuous vectors for each entity, instead using a compact, discrete code representation from which embeddings are composed.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: A novel compression method for KG embeddings that leverages discrete codes for entity representation and composition for embedding generation.\n    *   **System Design/Architectural Innovation**: A flexible and modular design that allows for end-to-end training and easy adaptation to existing KG embedding architectures.\n    *   **Practicality**: Demonstrates a significant reduction in storage/memory footprint while maintaining functional performance, addressing a major deployment challenge.\n\n*   **Experimental Validation**\n    *   **Experiments**: The approach was evaluated on various standard KG embedding evaluations.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Compression**: Achieves substantial compression, ranging from 50x to 1000x, for embeddings \\cite{sachan2020}.\n        *   **Performance**: Demonstrates only a minor loss in performance compared to uncompressed embeddings.\n        *   **Functionality**: The compressed embeddings successfully retain the ability to perform various reasoning tasks, including KG inference.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper notes a \"minor loss in performance,\" indicating a trade-off between compression efficiency and absolute accuracy, though the specific impact is described as minimal.\n    *   **Scope of Applicability**: The approach is broadly applicable to existing KG embedding techniques due to its modular and adaptable design, focusing primarily on compressing entity embeddings.\n\n*   **Technical Significance**\n    *   **Advancement**: Significantly advances the technical state-of-the-art by providing a practical solution to the critical problem of high storage and memory consumption in KG representation learning.\n    *   **Potential Impact**: Enables the deployment of powerful KG embedding techniques in resource-constrained real-world settings where their use was previously prohibitive. It also opens new avenues for research into efficient and compact representations for large-scale knowledge graphs and other embedding-based AI models.",
      "intriguing_abstract": "The immense storage and memory demands of continuous Knowledge Graph (KG) representation learning techniques severely limit their real-world deployment, despite their powerful capabilities. We introduce a novel, end-to-end trainable compression method that fundamentally redefines KG embedding efficiency. Our approach moves beyond storing large continuous vectors, instead representing each entity as a compact vector of discrete codes. Full, continuous embeddings are then dynamically composed from these codes, offering a paradigm shift in resource management. This modular design seamlessly integrates with any existing KG embedding architecture. Experimental validation reveals unprecedented compression, achieving 50x to 1000x reduction in storage and memory footprint, with only a minor, acceptable performance trade-off on KG inference tasks. This breakthrough enables the practical deployment of advanced KG embeddings in resource-constrained environments, unlocking their full potential for scalable AI applications and opening new avenues for research into efficient knowledge systems.",
      "keywords": [
        "Knowledge Graph (KG) embeddings",
        "Representation learning",
        "Storage and memory consumption",
        "KG embedding compression",
        "Discrete codes",
        "Entity representation",
        "End-to-end training",
        "Significant resource reduction",
        "Minor performance loss",
        "Real-world deployment",
        "Scalability",
        "KG inference",
        "Compact representations"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/33d469c6d9fc09b59522d91b7696b15dc60a9a93.pdf",
      "citation_key": "sachan2020",
      "metadata": {
        "title": "Knowledge Graph Embedding Compression",
        "authors": [
          "Mrinmaya Sachan"
        ],
        "published_date": "2020",
        "abstract": "Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes. The approach can be trained end-to-end with simple modifications to any existing KG embedding technique. We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.",
        "file_path": "paper_data/knowledge_graph_embedding/33d469c6d9fc09b59522d91b7696b15dc60a9a93.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Knowledge Graph (KG) representation learning techniques, which generate continuous embeddings for entities and relations, consume a large amount of storage and memory, particularly for large KGs.\n    *   **Motivation**: This high resource consumption is a critical barrier, preventing the deployment of these powerful AI techniques in many real-world applications due to practical limitations.\n\n*   **Related Work & Positioning**\n    *   **Positioning**: The work addresses a fundamental practical limitation (storage and memory footprint) inherent in existing continuous KG embedding techniques.\n    *   **Limitations of previous solutions**: Current KG embedding methods, by design, produce large continuous vectors for each entity, leading to substantial storage and memory overheads that hinder their scalability and real-world applicability.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes an approach to compress the KG embedding layer.\n    *   **Mechanism**: It represents each entity in the KG as a vector of discrete codes.\n    *   **Composition**: The full, continuous embeddings are then composed from these discrete codes.\n    *   **Integration**: The approach is designed for end-to-end training and can be integrated with simple modifications into any existing KG embedding technique \\cite{sachan2020}.\n    *   **Novelty**: The core innovation lies in moving away from directly storing large continuous vectors for each entity, instead using a compact, discrete code representation from which embeddings are composed.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: A novel compression method for KG embeddings that leverages discrete codes for entity representation and composition for embedding generation.\n    *   **System Design/Architectural Innovation**: A flexible and modular design that allows for end-to-end training and easy adaptation to existing KG embedding architectures.\n    *   **Practicality**: Demonstrates a significant reduction in storage/memory footprint while maintaining functional performance, addressing a major deployment challenge.\n\n*   **Experimental Validation**\n    *   **Experiments**: The approach was evaluated on various standard KG embedding evaluations.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Compression**: Achieves substantial compression, ranging from 50x to 1000x, for embeddings \\cite{sachan2020}.\n        *   **Performance**: Demonstrates only a minor loss in performance compared to uncompressed embeddings.\n        *   **Functionality**: The compressed embeddings successfully retain the ability to perform various reasoning tasks, including KG inference.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper notes a \"minor loss in performance,\" indicating a trade-off between compression efficiency and absolute accuracy, though the specific impact is described as minimal.\n    *   **Scope of Applicability**: The approach is broadly applicable to existing KG embedding techniques due to its modular and adaptable design, focusing primarily on compressing entity embeddings.\n\n*   **Technical Significance**\n    *   **Advancement**: Significantly advances the technical state-of-the-art by providing a practical solution to the critical problem of high storage and memory consumption in KG representation learning.\n    *   **Potential Impact**: Enables the deployment of powerful KG embedding techniques in resource-constrained real-world settings where their use was previously prohibitive. It also opens new avenues for research into efficient and compact representations for large-scale knowledge graphs and other embedding-based AI models.",
        "keywords": [
          "Knowledge Graph (KG) embeddings",
          "Representation learning",
          "Storage and memory consumption",
          "KG embedding compression",
          "Discrete codes",
          "Entity representation",
          "End-to-end training",
          "Significant resource reduction",
          "Minor performance loss",
          "Real-world deployment",
          "Scalability",
          "KG inference",
          "Compact representations"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "33d469c6d9fc09b59522d91b7696b15dc60a9a93.pdf"
    },
    {
      "success": true,
      "doc_id": "7998c2ab716cb24422024a7eec0893aa",
      "summary": "Here is a focused summary of the survey paper by Madushanka and Ichise \\cite{madushanka2024} for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey covers the domain of Negative Sampling (NS) methods within Knowledge Graph Representation Learning (KGRL), also known as Knowledge Graph Embedding (KGE).\n    *   Its main objectives are to systematically review various NS methods, categorize them into distinct groups, outline their advantages and disadvantages, and identify open research questions for future investigations.\n\n2.  **Literature Coverage**\n    *   The survey meticulously selected 64 research papers from an initial pool of 106, following PRISMA guidelines.\n    *   Literature was identified through extensive searches across databases including ACM Digital Library, IEEE Xplore, ScienceDirect, Web of Science, SpringerLink, and ArXiv.\n\n3.  **Classification Framework**\n    *   The survey organizes the literature by categorizing existing Negative Sampling methods into six distinct groups.\n    *   These categories are based on the architectures of the negative sampling strategies.\n\n4.  **Key Findings & Insights**\n    *   The survey outlines the respective advantages and disadvantages of the categorized negative sampling methods.\n    *   It provides a comprehensive overview of the architecture underlying each negative sampling category.\n    *   The paper aims to offer valuable insights for designing effective NS methods in KGRL by generalizing and aligning fundamental NS concepts.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey identifies several unresolved research challenges in the field of negative sampling.\n    *   It suggests potential directions for future investigations, serving as a motivating force for further advancements in KGRL.\n\n6.  **Survey Contribution**\n    *   This survey provides a comprehensive review of both historical and contemporary negative sampling methodologies in KGRL, along with an in-depth, novel classification.\n    *   It offers unique value by systematically discussing the strengths and limitations of various approaches and outlining future research prospects, making it a comprehensive and authoritative resource.",
      "intriguing_abstract": "The efficacy of Knowledge Graph Representation Learning (KGRL), also known as Knowledge Graph Embedding (KGE), hinges critically on effective Negative Sampling (NS) strategies. Despite its pivotal role, a comprehensive, architecturally-driven understanding of NS methods has remained elusive. This paper presents a rigorous systematic review, meticulously analyzing 64 research papers identified through PRISMA guidelines across major scientific databases.\n\nWe introduce a novel classification framework, categorizing existing NS methods into six distinct architectural groups. For each category, we provide an in-depth overview of its underlying architecture, alongside a critical discussion of its advantages and disadvantages. This systematic approach offers invaluable insights for researchers aiming to design more effective NS methods. Furthermore, we pinpoint key unresolved research challenges and propose compelling future directions, serving as a foundational resource to propel advancements in KGRL. This survey is an indispensable guide for navigating the complex landscape of negative sampling, empowering the next generation of KGRL innovations.",
      "keywords": [
        "Negative Sampling (NS)",
        "Knowledge Graph Representation Learning (KGRL)",
        "Knowledge Graph Embedding (KGE)",
        "Systematic review",
        "Novel NS methods classification",
        "Negative sampling architectures",
        "Advantages and disadvantages of NS methods",
        "Open research questions",
        "Future research directions",
        "Research gaps",
        "PRISMA guidelines",
        "Designing effective NS methods"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/4801db5c5cb24a9069f2d264252fa26986ceefa9.pdf",
      "citation_key": "madushanka2024",
      "metadata": {
        "title": "Negative Sampling in Knowledge Graph Representation Learning: A Review",
        "authors": [
          "Tiroshan Madushanka",
          "R. Ichise"
        ],
        "published_date": "2024",
        "abstract": "Knowledge Graph Representation Learning (KGRL), or Knowledge Graph Embedding (KGE), is essential for AI applications such as knowledge construction and information retrieval. These models encode entities and relations into lower-dimensional vectors, supporting tasks like link prediction and recommendation systems. Training KGE models relies on both positive and negative samples for effective learning, but generating high-quality negative samples from existing knowledge graphs is challenging. The quality of these samples significantly impacts the model's accuracy. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS methods into six distinct categories. Moreover, this survey identifies open research questions that serve as potential directions for future investigations. By offering a generalization and alignment of fundamental NS concepts, this survey provides valuable insights for designing effective NS methods in the context of KGRL and serves as a motivating force for further advancements in the field.",
        "file_path": "paper_data/knowledge_graph_embedding/4801db5c5cb24a9069f2d264252fa26986ceefa9.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper by Madushanka and Ichise \\cite{madushanka2024} for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey covers the domain of Negative Sampling (NS) methods within Knowledge Graph Representation Learning (KGRL), also known as Knowledge Graph Embedding (KGE).\n    *   Its main objectives are to systematically review various NS methods, categorize them into distinct groups, outline their advantages and disadvantages, and identify open research questions for future investigations.\n\n2.  **Literature Coverage**\n    *   The survey meticulously selected 64 research papers from an initial pool of 106, following PRISMA guidelines.\n    *   Literature was identified through extensive searches across databases including ACM Digital Library, IEEE Xplore, ScienceDirect, Web of Science, SpringerLink, and ArXiv.\n\n3.  **Classification Framework**\n    *   The survey organizes the literature by categorizing existing Negative Sampling methods into six distinct groups.\n    *   These categories are based on the architectures of the negative sampling strategies.\n\n4.  **Key Findings & Insights**\n    *   The survey outlines the respective advantages and disadvantages of the categorized negative sampling methods.\n    *   It provides a comprehensive overview of the architecture underlying each negative sampling category.\n    *   The paper aims to offer valuable insights for designing effective NS methods in KGRL by generalizing and aligning fundamental NS concepts.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey identifies several unresolved research challenges in the field of negative sampling.\n    *   It suggests potential directions for future investigations, serving as a motivating force for further advancements in KGRL.\n\n6.  **Survey Contribution**\n    *   This survey provides a comprehensive review of both historical and contemporary negative sampling methodologies in KGRL, along with an in-depth, novel classification.\n    *   It offers unique value by systematically discussing the strengths and limitations of various approaches and outlining future research prospects, making it a comprehensive and authoritative resource.",
        "keywords": [
          "Negative Sampling (NS)",
          "Knowledge Graph Representation Learning (KGRL)",
          "Knowledge Graph Embedding (KGE)",
          "Systematic review",
          "Novel NS methods classification",
          "Negative sampling architectures",
          "Advantages and disadvantages of NS methods",
          "Open research questions",
          "Future research directions",
          "Research gaps",
          "PRISMA guidelines",
          "Designing effective NS methods"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "4801db5c5cb24a9069f2d264252fa26986ceefa9.pdf"
    },
    {
      "success": true,
      "doc_id": "f6675eb03f21b90264764ef4fbd64970",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Problem:** While Knowledge Graphs (KGs) and KG embedding are crucial for deriving new biomedical knowledge, few existing works based on biomedical KGs specifically focus on *specific diseases* \\cite{zhu2022}.\n    *   **Motivation:** There is a need to discover new, reliable knowledge and provide universal pre-trained knowledge tailored for specific disease fields, which is challenging due to the broad nature of general biomedical KGs \\cite{zhu2022}.\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches:** The paper acknowledges the importance of KG embedding technology for knowledge derivation and notes that some approaches incorporate additional information for reasoning (multimodal reasoning) \\cite{zhu2022}.\n    *   **Limitations of Previous Solutions:** The primary limitation identified is the lack of focus on *specific diseases* within existing biomedical KG-based research \\cite{zhu2022}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper develops a comprehensive process for the construction and multimodal reasoning of Specific Disease Knowledge Graphs (SDKGs) \\cite{zhu2022}.\n    *   **SDKG Construction:** SDKG-11, a set of 11 disease-specific KGs (including cancers and non-cancer diseases), is constructed through original triplet extraction, standard entity set construction, entity linking, and relation linking \\cite{zhu2022}.\n    *   **Multimodal Reasoning:** This is implemented using a novel *reverse-hyperplane projection* method for SDKGs, which integrates information from *structure, category, and description embeddings* \\cite{zhu2022}.\n    *   **Novelty:** The innovation lies in the dedicated focus on constructing and reasoning over *disease-specific KGs* (SDKGs) and the proposed multimodal reasoning approach that effectively combines diverse data modalities (structure, category, description) via reverse-hyperplane projection to enhance knowledge discovery \\cite{zhu2022}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A structured process for constructing Specific Disease Knowledge Graphs (SDKGs) from existing biomedical data \\cite{zhu2022}.\n        *   A multimodal reasoning algorithm based on *reverse-hyperplane projection* that effectively integrates structural, categorical, and descriptive embeddings for improved knowledge inference within SDKGs \\cite{zhu2022}.\n    *   **System Design/Architectural Innovations:**\n        *   The creation of SDKG-11, a novel and comprehensive dataset comprising 11 specific disease knowledge graphs, designed to serve as universal pre-trained knowledge for their respective fields \\cite{zhu2022}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluation of the multimodal reasoning model's performance on all constructed SDKGs \\cite{zhu2022}.\n        *   Verification of the reliability of newly discovered knowledge \\cite{zhu2022}.\n        *   Demonstration of the universality of the embedding models \\cite{zhu2022}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Multimodal reasoning *improves pre-existing models* across all SDKGs, evaluated using the *entity prediction task* \\cite{zhu2022}.\n        *   The reliability of discovering new knowledge (e.g., druggene, genedisease, diseasedrug pairs) was confirmed through *manual proofreading* \\cite{zhu2022}.\n        *   The universality of the embedding models was shown by successfully using their results as *initialization parameters for biomolecular interaction classification* \\cite{zhu2022}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The abstract does not explicitly state limitations of their *own* technical approach, but the manual proofreading step for new knowledge discovery might imply scalability considerations for extremely large-scale predictions.\n    *   **Scope of Applicability:** The work is specifically focused on the biomedical field, particularly on constructing and reasoning over knowledge graphs for *specific diseases* \\cite{zhu2022}.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing a novel framework for constructing and performing multimodal reasoning on *disease-specific knowledge graphs*, addressing a critical gap in existing biomedical KG research \\cite{zhu2022}. The integration of diverse embedding types via reverse-hyperplane projection offers a powerful new approach to knowledge discovery.\n    *   **Potential Impact:** The developed SDKG-11 dataset and the multimodal reasoning process offer a robust method for discovering new, reliable knowledge in specific disease domains. This has the potential to accelerate research in drug discovery, disease understanding, and personalized medicine by providing targeted, pre-trained knowledge and a universal embedding framework \\cite{zhu2022}.",
      "intriguing_abstract": "The vast potential of biomedical Knowledge Graphs (KGs) for knowledge discovery is often diluted by their broad scope, leaving a critical gap in generating *disease-specific* insights. We address this by introducing a novel framework for constructing and reasoning over **Specific Disease Knowledge Graphs (SDKGs)**. Our work presents **SDKG-11**, a comprehensive dataset of 11 meticulously curated disease-specific KGs, designed to serve as universal pre-trained knowledge. A cornerstone of our approach is a groundbreaking **multimodal reasoning** algorithm, which leverages a novel **reverse-hyperplane projection** method to seamlessly integrate structural, categorical, and descriptive embeddings. This innovative integration significantly enhances knowledge inference, outperforming existing models in **entity prediction tasks** across all SDKGs. We rigorously validate the reliability of newly discovered knowledge, such as novel drug-gene and gene-disease associations, through manual proofreading. Furthermore, the universality of our **KG embedding** models is demonstrated by their effectiveness as initialization parameters for **biomolecular interaction classification**. This research provides a powerful, targeted approach to accelerate **drug discovery**, deepen **disease understanding**, and advance **personalized medicine** by transforming how we extract and utilize knowledge in specific disease domains.",
      "keywords": [
        "Specific Disease Knowledge Graphs (SDKGs)",
        "Multimodal reasoning",
        "Reverse-hyperplane projection",
        "Biomedical knowledge discovery",
        "KG embedding",
        "SDKG construction",
        "SDKG-11 dataset",
        "Structure",
        "category",
        "description embeddings",
        "Entity prediction task",
        "Reliable knowledge discovery",
        "Universal pre-trained knowledge",
        "Drug discovery",
        "Personalized medicine"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/a166957ec488cd20e61360d630568b3b81af3397.pdf",
      "citation_key": "zhu2022",
      "metadata": {
        "title": "Multimodal reasoning based on knowledge graph embedding for specific diseases",
        "authors": [
          "Chaoyu Zhu",
          "Zhihao Yang",
          "Xiaoqiong Xia",
          "Nan Li",
          "Fan Zhong",
          "Lei Liu"
        ],
        "published_date": "2022",
        "abstract": "Abstract Motivation Knowledge Graph (KG) is becoming increasingly important in the biomedical field. Deriving new and reliable knowledge from existing knowledge by KG embedding technology is a cutting-edge method. Some add a variety of additional information to aid reasoning, namely multimodal reasoning. However, few works based on the existing biomedical KGs are focused on specific diseases. Results This work develops a construction and multimodal reasoning process of Specific Disease Knowledge Graphs (SDKGs). We construct SDKG-11, a SDKG set including five cancers, six non-cancer diseases, a combined Cancer5 and a combined Diseases11, aiming to discover new reliable knowledge and provide universal pre-trained knowledge for that specific disease field. SDKG-11 is obtained through original triplet extraction, standard entity set construction, entity linking and relation linking. We implement multimodal reasoning by reverse-hyperplane projection for SDKGs based on structure, category and description embeddings. Multimodal reasoning improves pre-existing models on all SDKGs using entity prediction task as the evaluation protocol. We verify the models reliability in discovering new knowledge by manually proofreading predicted druggene, genedisease and diseasedrug pairs. Using embedding results as initialization parameters for the biomolecular interaction classification, we demonstrate the universality of embedding models. Availability and implementation The constructed SDKG-11 and the implementation by TensorFlow are available from https://github.com/ZhuChaoY/SDKG-11. Supplementary information Supplementary data are available at Bioinformatics online.",
        "file_path": "paper_data/knowledge_graph_embedding/a166957ec488cd20e61360d630568b3b81af3397.pdf",
        "venue": "Bioinform.",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Problem:** While Knowledge Graphs (KGs) and KG embedding are crucial for deriving new biomedical knowledge, few existing works based on biomedical KGs specifically focus on *specific diseases* \\cite{zhu2022}.\n    *   **Motivation:** There is a need to discover new, reliable knowledge and provide universal pre-trained knowledge tailored for specific disease fields, which is challenging due to the broad nature of general biomedical KGs \\cite{zhu2022}.\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches:** The paper acknowledges the importance of KG embedding technology for knowledge derivation and notes that some approaches incorporate additional information for reasoning (multimodal reasoning) \\cite{zhu2022}.\n    *   **Limitations of Previous Solutions:** The primary limitation identified is the lack of focus on *specific diseases* within existing biomedical KG-based research \\cite{zhu2022}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper develops a comprehensive process for the construction and multimodal reasoning of Specific Disease Knowledge Graphs (SDKGs) \\cite{zhu2022}.\n    *   **SDKG Construction:** SDKG-11, a set of 11 disease-specific KGs (including cancers and non-cancer diseases), is constructed through original triplet extraction, standard entity set construction, entity linking, and relation linking \\cite{zhu2022}.\n    *   **Multimodal Reasoning:** This is implemented using a novel *reverse-hyperplane projection* method for SDKGs, which integrates information from *structure, category, and description embeddings* \\cite{zhu2022}.\n    *   **Novelty:** The innovation lies in the dedicated focus on constructing and reasoning over *disease-specific KGs* (SDKGs) and the proposed multimodal reasoning approach that effectively combines diverse data modalities (structure, category, description) via reverse-hyperplane projection to enhance knowledge discovery \\cite{zhu2022}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A structured process for constructing Specific Disease Knowledge Graphs (SDKGs) from existing biomedical data \\cite{zhu2022}.\n        *   A multimodal reasoning algorithm based on *reverse-hyperplane projection* that effectively integrates structural, categorical, and descriptive embeddings for improved knowledge inference within SDKGs \\cite{zhu2022}.\n    *   **System Design/Architectural Innovations:**\n        *   The creation of SDKG-11, a novel and comprehensive dataset comprising 11 specific disease knowledge graphs, designed to serve as universal pre-trained knowledge for their respective fields \\cite{zhu2022}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluation of the multimodal reasoning model's performance on all constructed SDKGs \\cite{zhu2022}.\n        *   Verification of the reliability of newly discovered knowledge \\cite{zhu2022}.\n        *   Demonstration of the universality of the embedding models \\cite{zhu2022}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Multimodal reasoning *improves pre-existing models* across all SDKGs, evaluated using the *entity prediction task* \\cite{zhu2022}.\n        *   The reliability of discovering new knowledge (e.g., druggene, genedisease, diseasedrug pairs) was confirmed through *manual proofreading* \\cite{zhu2022}.\n        *   The universality of the embedding models was shown by successfully using their results as *initialization parameters for biomolecular interaction classification* \\cite{zhu2022}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The abstract does not explicitly state limitations of their *own* technical approach, but the manual proofreading step for new knowledge discovery might imply scalability considerations for extremely large-scale predictions.\n    *   **Scope of Applicability:** The work is specifically focused on the biomedical field, particularly on constructing and reasoning over knowledge graphs for *specific diseases* \\cite{zhu2022}.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing a novel framework for constructing and performing multimodal reasoning on *disease-specific knowledge graphs*, addressing a critical gap in existing biomedical KG research \\cite{zhu2022}. The integration of diverse embedding types via reverse-hyperplane projection offers a powerful new approach to knowledge discovery.\n    *   **Potential Impact:** The developed SDKG-11 dataset and the multimodal reasoning process offer a robust method for discovering new, reliable knowledge in specific disease domains. This has the potential to accelerate research in drug discovery, disease understanding, and personalized medicine by providing targeted, pre-trained knowledge and a universal embedding framework \\cite{zhu2022}.",
        "keywords": [
          "Specific Disease Knowledge Graphs (SDKGs)",
          "Multimodal reasoning",
          "Reverse-hyperplane projection",
          "Biomedical knowledge discovery",
          "KG embedding",
          "SDKG construction",
          "SDKG-11 dataset",
          "Structure",
          "category",
          "description embeddings",
          "Entity prediction task",
          "Reliable knowledge discovery",
          "Universal pre-trained knowledge",
          "Drug discovery",
          "Personalized medicine"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "a166957ec488cd20e61360d630568b3b81af3397.pdf"
    },
    {
      "success": true,
      "doc_id": "db2cfe55fdf196fcd191e98edb2a1521",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Fully Hyperbolic Rotation for Knowledge Graph Embedding \\cite{liang2024}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing hyperbolic knowledge graph embedding (KGE) models, while effective for hierarchical structures, rely on complex and potentially unstable logarithmic and exponential mappings to transform data features between hyperbolic and tangent spaces. This limits their ability to fully exploit the inherent properties of hyperbolic space.\n    *   **Importance and Challenge:** Knowledge Graphs (KGs) inherently exhibit hierarchical structures that Euclidean space struggles to model accurately. Hyperbolic space is naturally suited for this, but current hybrid hyperbolic models introduce computational overhead and stability issues due to frequent spatial transformations. The complexity and infinite range of hyperbolic mapping functions weaken model stability.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Euclidean-based models (e.g., TransE, RotE):** Struggle with hierarchical structures due to their flat, uniform space.\n        *   **Complex-based models (e.g., RotatE, QuatE):** Offer improved representation but often require high-dimensional spaces and increased memory costs.\n        *   **Hyperbolic-based models (e.g., MuRP, RotH, FFTRotH, CoPE):** Acknowledge the hierarchical nature of KGs but are criticized for their reliance on logarithmic and exponential mappings, which project data features into hyperbolic space only for transformations, not for direct operation within it.\n    *   **Limitations of Previous Solutions:** Previous hyperbolic models are \"hybrid\" in nature, constantly mapping between hyperbolic and tangent spaces during training. This process is computationally intensive, can be numerically unstable due to complex functions with infinite ranges, and prevents full utilization of hyperbolic geometry.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{liang2024} proposes the Fully Hyperbolic Rotation model (FHRE), which defines the KGE model directly and entirely within hyperbolic space, specifically using the Lorentz model.\n        *   It treats each relation as a Lorentz rotation that transforms a head entity embedding to its corresponding tail entity embedding.\n        *   Entity and relation embeddings are initialized in tangent space and mapped *once* to hyperbolic space using exponential mapping during initialization, eliminating subsequent spatial transformations during training.\n        *   The plausibility of triplets is measured using a Lorentzian distance-based scoring function.\n    *   **Novelty/Difference:** The key innovation is the complete avoidance of iterative logarithmic and exponential mappings during the training phase. Unlike prior hyperbolic models that operate in a hybrid fashion, FHRE performs all rotational transformations directly within the Lorentz hyperbolic space, leading to a \"fully hyperbolic\" approach. This direct operation leverages the Lorentz rotation theorem for linear transformations within the Lorentz model.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   First model to propose modeling knowledge graphs using Lorentz rotations directly in a fully hyperbolic space.\n        *   A novel hyperbolic rotation model (FHRE) that eliminates the reliance on spatial mappings between hyperbolic and tangent spaces during training.\n        *   Definition of relations as Lorentz rotations (`vh' = vh  r = vh Rot(r)`) and use of Lorentzian distance as the scoring function.\n    *   **System Design/Architectural Innovations:** A simplified training architecture that avoids the computational overhead and potential instability associated with repeated spatial mappings in hybrid hyperbolic models.\n    *   **Theoretical Insights/Analysis:** Leverages the Lorentz model's properties and the Lorentz rotation theorem for direct operations within hyperbolic space, offering a more stable and direct way to model hierarchical data.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Link prediction (knowledge graph completion) experiments were performed on various benchmark datasets.\n    *   **Key Performance Metrics:** Mean Reciprocal Rank (MRR) and Hits@k (k=1, 3, 10) in the filtered setting.\n    *   **Comparison Results:**\n        *   **Standard Benchmarks (FB15k-237, WN18RR):** FHRE \\cite{liang2024} achieved competitive results against a wide range of Euclidean, Complex, and existing Hyperbolic baselines, often ranking among the top performers, especially on WN18RR, and with fewer parameters.\n        *   **Challenging Benchmarks (CoDEx-s, CoDEx-m):** FHRE \\cite{liang2024} demonstrated state-of-the-art performance on these more diverse and challenging datasets, which include hard negative triples, validating its effectiveness and generalization ability.\n        *   **Nations Dataset:** Also evaluated on this smaller dataset with a high relation-to-entity ratio.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The model focuses on shallow network-based KGE models. It relies on the Lorentz model for its simplicity and numerical stability, implying that other hyperbolic geometries (e.g., Poincar ball) might have different characteristics not explored in this specific approach. The paper primarily focuses on head/tail completion for KGC.\n    *   **Scope of Applicability:** Primarily applicable to knowledge graph completion tasks, particularly where hierarchical structures are prominent. The benefits of a \"fully hyperbolic\" approach are most pronounced in scenarios where the overhead and instability of spatial mappings in hybrid models are a concern.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** FHRE \\cite{liang2024} significantly advances hyperbolic KGE by introducing a truly \"fully hyperbolic\" paradigm, moving beyond hybrid models that rely on spatial mappings. This addresses a fundamental limitation of previous hyperbolic approaches.\n    *   **Potential Impact on Future Research:** This work opens new avenues for designing more stable, efficient, and direct hyperbolic models for KGE and potentially other domains dealing with hierarchical data. By demonstrating the feasibility and benefits of direct operations in hyperbolic space, it could inspire further research into fully hyperbolic architectures and algorithms, reducing reliance on complex mapping functions and potentially leading to more robust and scalable solutions.",
      "intriguing_abstract": "Knowledge Graphs (KGs) inherently possess hierarchical structures, yet their accurate representation remains a significant challenge for traditional Euclidean embedding models. While hyperbolic geometry offers a natural fit, existing hyperbolic Knowledge Graph Embedding (KGE) models are often \"hybrid,\" relying on computationally intensive and numerically unstable logarithmic and exponential mappings between hyperbolic and tangent spaces during training. This constant spatial transformation limits their ability to fully leverage hyperbolic properties.\n\nWe introduce **Fully Hyperbolic Rotation (FHRE)**, a novel KGE model that redefines the paradigm by operating *entirely* within the Lorentz hyperbolic space. FHRE uniquely models relations as direct Lorentz rotations, transforming head entities to tail entities without any iterative spatial mappings after initial embedding. This innovative \"fully hyperbolic\" approach eliminates the computational overhead and instability associated with hybrid models, offering a more stable and efficient framework. Our experiments demonstrate that FHRE achieves state-of-the-art performance on challenging link prediction benchmarks, including CoDEx datasets, often with fewer parameters. FHRE not only advances the state-of-the-art in hyperbolic KGE but also paves the way for designing robust and scalable direct hyperbolic architectures for hierarchical data modeling.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Hyperbolic space",
        "Lorentz model",
        "Fully Hyperbolic Rotation (FHRE)",
        "Lorentz rotation",
        "Hierarchical structures",
        "Direct hyperbolic operations",
        "Spatial transformations",
        "Link prediction",
        "Model stability",
        "Computational overhead reduction",
        "State-of-the-art performance",
        "Knowledge graph completion"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/bcffbb40e7922d2a34e752f8faaa4fe99649e21a.pdf",
      "citation_key": "liang2024",
      "metadata": {
        "title": "Fully Hyperbolic Rotation for Knowledge Graph Embedding",
        "authors": [
          "Qiuyu Liang",
          "Weihua Wang",
          "F. Bao",
          "Guanglai Gao"
        ],
        "published_date": "2024",
        "abstract": "Hyperbolic rotation is commonly used to effectively model knowledge graphs and their inherent hierarchies. However, existing hyperbolic rotation models rely on logarithmic and exponential mappings for feature transformation. These models only project data features into hyperbolic space for rotation, limiting their ability to fully exploit the hyperbolic space. To address this problem, we propose a novel fully hyperbolic model designed for knowledge graph embedding. Instead of feature mappings, we define the model directly in hyperbolic space with the Lorentz model. Our model considers each relation in knowledge graphs as a Lorentz rotation from the head entity to the tail entity. We adopt the Lorentzian version distance as the scoring function for measuring the plausibility of triplets. Extensive results on standard knowledge graph completion benchmarks demonstrated that our model achieves competitive results with fewer parameters. In addition, our model get the state-of-the-art performance on datasets of CoDEx-s and CoDEx-m, which are more diverse and challenging than before. Our code is available at https://github.com/llqy123/FHRE.",
        "file_path": "paper_data/knowledge_graph_embedding/bcffbb40e7922d2a34e752f8faaa4fe99649e21a.pdf",
        "venue": "European Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Fully Hyperbolic Rotation for Knowledge Graph Embedding \\cite{liang2024}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing hyperbolic knowledge graph embedding (KGE) models, while effective for hierarchical structures, rely on complex and potentially unstable logarithmic and exponential mappings to transform data features between hyperbolic and tangent spaces. This limits their ability to fully exploit the inherent properties of hyperbolic space.\n    *   **Importance and Challenge:** Knowledge Graphs (KGs) inherently exhibit hierarchical structures that Euclidean space struggles to model accurately. Hyperbolic space is naturally suited for this, but current hybrid hyperbolic models introduce computational overhead and stability issues due to frequent spatial transformations. The complexity and infinite range of hyperbolic mapping functions weaken model stability.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Euclidean-based models (e.g., TransE, RotE):** Struggle with hierarchical structures due to their flat, uniform space.\n        *   **Complex-based models (e.g., RotatE, QuatE):** Offer improved representation but often require high-dimensional spaces and increased memory costs.\n        *   **Hyperbolic-based models (e.g., MuRP, RotH, FFTRotH, CoPE):** Acknowledge the hierarchical nature of KGs but are criticized for their reliance on logarithmic and exponential mappings, which project data features into hyperbolic space only for transformations, not for direct operation within it.\n    *   **Limitations of Previous Solutions:** Previous hyperbolic models are \"hybrid\" in nature, constantly mapping between hyperbolic and tangent spaces during training. This process is computationally intensive, can be numerically unstable due to complex functions with infinite ranges, and prevents full utilization of hyperbolic geometry.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{liang2024} proposes the Fully Hyperbolic Rotation model (FHRE), which defines the KGE model directly and entirely within hyperbolic space, specifically using the Lorentz model.\n        *   It treats each relation as a Lorentz rotation that transforms a head entity embedding to its corresponding tail entity embedding.\n        *   Entity and relation embeddings are initialized in tangent space and mapped *once* to hyperbolic space using exponential mapping during initialization, eliminating subsequent spatial transformations during training.\n        *   The plausibility of triplets is measured using a Lorentzian distance-based scoring function.\n    *   **Novelty/Difference:** The key innovation is the complete avoidance of iterative logarithmic and exponential mappings during the training phase. Unlike prior hyperbolic models that operate in a hybrid fashion, FHRE performs all rotational transformations directly within the Lorentz hyperbolic space, leading to a \"fully hyperbolic\" approach. This direct operation leverages the Lorentz rotation theorem for linear transformations within the Lorentz model.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   First model to propose modeling knowledge graphs using Lorentz rotations directly in a fully hyperbolic space.\n        *   A novel hyperbolic rotation model (FHRE) that eliminates the reliance on spatial mappings between hyperbolic and tangent spaces during training.\n        *   Definition of relations as Lorentz rotations (`vh' = vh  r = vh Rot(r)`) and use of Lorentzian distance as the scoring function.\n    *   **System Design/Architectural Innovations:** A simplified training architecture that avoids the computational overhead and potential instability associated with repeated spatial mappings in hybrid hyperbolic models.\n    *   **Theoretical Insights/Analysis:** Leverages the Lorentz model's properties and the Lorentz rotation theorem for direct operations within hyperbolic space, offering a more stable and direct way to model hierarchical data.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Link prediction (knowledge graph completion) experiments were performed on various benchmark datasets.\n    *   **Key Performance Metrics:** Mean Reciprocal Rank (MRR) and Hits@k (k=1, 3, 10) in the filtered setting.\n    *   **Comparison Results:**\n        *   **Standard Benchmarks (FB15k-237, WN18RR):** FHRE \\cite{liang2024} achieved competitive results against a wide range of Euclidean, Complex, and existing Hyperbolic baselines, often ranking among the top performers, especially on WN18RR, and with fewer parameters.\n        *   **Challenging Benchmarks (CoDEx-s, CoDEx-m):** FHRE \\cite{liang2024} demonstrated state-of-the-art performance on these more diverse and challenging datasets, which include hard negative triples, validating its effectiveness and generalization ability.\n        *   **Nations Dataset:** Also evaluated on this smaller dataset with a high relation-to-entity ratio.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The model focuses on shallow network-based KGE models. It relies on the Lorentz model for its simplicity and numerical stability, implying that other hyperbolic geometries (e.g., Poincar ball) might have different characteristics not explored in this specific approach. The paper primarily focuses on head/tail completion for KGC.\n    *   **Scope of Applicability:** Primarily applicable to knowledge graph completion tasks, particularly where hierarchical structures are prominent. The benefits of a \"fully hyperbolic\" approach are most pronounced in scenarios where the overhead and instability of spatial mappings in hybrid models are a concern.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** FHRE \\cite{liang2024} significantly advances hyperbolic KGE by introducing a truly \"fully hyperbolic\" paradigm, moving beyond hybrid models that rely on spatial mappings. This addresses a fundamental limitation of previous hyperbolic approaches.\n    *   **Potential Impact on Future Research:** This work opens new avenues for designing more stable, efficient, and direct hyperbolic models for KGE and potentially other domains dealing with hierarchical data. By demonstrating the feasibility and benefits of direct operations in hyperbolic space, it could inspire further research into fully hyperbolic architectures and algorithms, reducing reliance on complex mapping functions and potentially leading to more robust and scalable solutions.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Hyperbolic space",
          "Lorentz model",
          "Fully Hyperbolic Rotation (FHRE)",
          "Lorentz rotation",
          "Hierarchical structures",
          "Direct hyperbolic operations",
          "Spatial transformations",
          "Link prediction",
          "Model stability",
          "Computational overhead reduction",
          "State-of-the-art performance",
          "Knowledge graph completion"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "bcffbb40e7922d2a34e752f8faaa4fe99649e21a.pdf"
    },
    {
      "success": true,
      "doc_id": "ecb178fd14ebbb7b88e9b66db6cf8fe4",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval \\cite{li2024}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Knowledge Graph Embedding (KGE) methods primarily follow an entity ranking protocol, where they predict the plausibility of a triple and return a ranked list of entities. This design is inherently inexpressive for modeling one-to-many, many-to-one, and especially many-to-many relations.\n    *   **Importance & Challenge:** In many real-world applications (e.g., bioinformatics, \"which genes cause an illness\"), users require an *exact set* of answers rather than a ranked list. Determining an appropriate cutoff threshold for a ranked list to obtain a precise set is challenging and often arbitrary. This paper introduces and formulates the novel problem of \"Knowledge Graph Set Retrieval\" to address this gap.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon rotational embedding methods like RotatE, RotatE3D, and HousE, which model relations as rotations and are known for handling symmetric and anti-symmetric relations.\n    *   **Limitations of Previous Solutions:**\n        *   **Translation-based methods (e.g., TransE, TransH):** Inexpressive for symmetric relations.\n        *   **Rotation-based methods (e.g., RotatE, QuatE):** While good for symmetry, they embed entities as vectors (points), making them inexpressive for one-to-many, many-to-one, and many-to-many relations.\n        *   **Complex models (e.g., BoxE, HousE):** Achieve high expressiveness but often at the cost of interpretability due to their intricate modeling.\n        *   **All existing KGE methods:** Primarily focus on entity ranking, which is unsuitable for \"set retrieval\" tasks requiring precise, unranked sets of answers.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes SpherE, a novel KGE model that embeds each entity as a *sphere* (defined by a center vector and a radius) in a Euclidean space, while relations are modeled as *rotations*.\n    *   **Novelty/Difference:**\n        *   **Sphere-based Entity Embedding:** Unlike traditional KGE models that embed entities as points (vectors), SpherE uses spheres. This allows for a natural representation of \"universality\" (how often an entity appears) through the sphere's radius.\n        *   **Set Retrieval Paradigm:** SpherE directly addresses the set retrieval problem by defining a triple `(h, r, t)` as true if the sphere of the transformed head entity `f_r(c_h)` *overlaps* (is non-disjoint) with the sphere of the tail entity `c_t`. This inherently supports multiple correct answers without requiring a ranking threshold.\n        *   **Expressiveness for Many-to-Many Relations:** The sphere-based modeling naturally captures one-to-many, many-to-one, and many-to-many relations, as a single transformed head sphere can overlap with multiple tail spheres.\n        *   **Interpretability:** SpherE inherits the interpretability of rotational models, and the entity radius provides an intuitive measure of an entity's \"universality\" or prevalence in the KG.\n\n4.  **Key Technical Contributions**\n    *   **Novel Problem Formulation:** First to introduce and formally define the \"Knowledge Graph Set Retrieval\" problem.\n    *   **Novel Embedding Model (SpherE):** Proposes embedding entities as spheres and relations as rotations, enabling direct set retrieval.\n    *   **Theoretical Expressiveness:** Provides theoretical proofs (Theorems 3.1, 3.2, 3.3) demonstrating SpherE's ability to model various inference patterns, including symmetry, anti-symmetry, inversion, composition, non-commutative composition (for k>=3D), multiplicity (for k>=3D), and all relation mapping properties (one-to-one, one-to-many, many-to-one, many-to-many).\n    *   **Interpretable Radius:** The entity radius is shown to correlate with the entity's occurrence frequency in the KG, providing a clear interpretation of its learned parameter.\n    *   **Specialized Loss Function:** Designs a sigmoid-based loss function that encourages sphere intersection for positive triples and disjointness for negative triples, incorporating hyperparameters `alpha` and `beta` to fine-tune intersection behavior.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Evaluated SpherE on two widely-used KGE benchmark datasets: FB15K237 and WN18RR.\n    *   **Baselines:** Compared against state-of-the-art rotational KGE methods (RotatE, RotatE3D, HousE-kD) adapted for set retrieval by truncating their ranked lists at various `top-l` cutoffs (l=1, 3, 5, 10, 20, 100).\n    *   **Key Performance Metrics:**\n        *   **F1 Score:** Measures the accuracy of the retrieved set against the ground truth set (Head F1, Tail F1).\n        *   **Retrieve Rate (RR):** Probability that the actual correct entity for a test triple is included in the retrieved set, indicating link prediction ability (Head RR, Tail RR).\n        *   **n-to-n F1:** F1 score specifically for many-to-many relations.\n    *   **Comparison Results:**\n        *   SpherE significantly *outperforms all baseline methods* in terms of Head F1, Tail F1, and especially n-to-n F1 on both datasets, demonstrating its superior ability for set retrieval and modeling many-to-many relations.\n        *   SpherE maintains a *comparable \"Retrieve Rate\"* (RR) to strong baselines (e.g., top-20 for FB15K237, top-3 for WN18RR), indicating it still has good predictive ability for inferring missing links.\n        *   The experiments empirically validate the interpretability of the radius, showing a positive correlation between an entity's radius and its occurrence count in the KG.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** The paper notes an observed performance drop in set retrieval tasks as the dimension of rotation (`k` in SpherE-kD) increases, suggesting an area for future investigation.\n    *   **Scope of Applicability:** SpherE is primarily designed for knowledge graph set retrieval tasks where precise, unranked sets of answers are required. While it also shows good link prediction capabilities, its core innovation lies in the set retrieval paradigm.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{li2024} makes a pioneering contribution by formally introducing and addressing the Knowledge Graph Set Retrieval problem, shifting the paradigm from entity ranking to direct set prediction.\n    *   **Potential Impact:**\n        *   **Enhanced KG Utility:** Enables more precise and user-friendly information retrieval from KGs, particularly in domains like bioinformatics, legal research, or any application requiring exact sets of related entities.\n        *   **New Research Direction:** Opens up a new avenue for KGE research focused on set-based predictions rather than solely ranking, potentially leading to more expressive and application-specific embedding models.\n        *   **Interpretable Models:** The sphere-based entity representation with an interpretable radius offers a more intuitive understanding of entity properties within the embedding space.",
      "intriguing_abstract": "Current Knowledge Graph Embedding (KGE) methods are fundamentally limited, offering only ranked lists of entities and struggling to express complex many-to-many relations. This paradigm fails critical real-world applications demanding precise, unranked sets of answers. We introduce the novel problem of **Knowledge Graph Set Retrieval** and propose SpherE, a groundbreaking KGE model that redefines entity representation. SpherE embeds entities as *spheres* in Euclidean space, with relations modeled as *rotations*. This innovative geometric approach allows SpherE to directly predict exact sets of answers by evaluating sphere overlap, eliminating arbitrary ranking thresholds. Our model achieves unparalleled expressiveness for one-to-many, many-to-one, and particularly many-to-many relations, backed by theoretical proofs. Furthermore, the learned entity radius provides an intuitive measure of \"universality,\" enhancing interpretability. Extensive experiments on benchmark datasets demonstrate SpherE's significant superiority in F1 score for set retrieval over state-of-the-art rotational baselines, especially for many-to-many relations. SpherE not only advances KGE expressiveness but also opens a vital new research direction, enabling more accurate and user-friendly knowledge discovery.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Knowledge Graph Set Retrieval",
        "SpherE model",
        "sphere-based entity embedding",
        "relations as rotations",
        "many-to-many relations",
        "interpretability",
        "theoretical expressiveness",
        "F1 Score",
        "direct set prediction",
        "entity radius",
        "occurrence frequency correlation",
        "bioinformatics"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/7029ecb5d5fc04f54e1e25e739db2e993fb147c8.pdf",
      "citation_key": "li2024",
      "metadata": {
        "title": "SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval",
        "authors": [
          "Li",
          "Yuyi Ao",
          "Jingrui He"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graphs (KGs), which store an extensive number of relational facts (head, relation, tail), serve various applications. While many downstream tasks highly rely on the expressive modeling and predictive embedding of KGs, most of the current KG representation learning methods, where each entity is embedded as a vector in the Euclidean space and each relation is embedded as a transformation, follow an entity ranking protocol. On one hand, such an embedding design cannot capture many-to-many relations. On the other hand, in many retrieval cases, the users wish to get an exact set of answers without any ranking, especially when the results are expected to be precise, e.g., which genes cause an illness. Such scenarios are commonly referred to as \"set retrieval\". This work presents a pioneering study on the KG set retrieval problem. We show that the set retrieval highly depends on expressive modeling of many-to-many relations, and propose a new KG embedding model SpherE to address this problem. SpherE is based on rotational embedding methods, but each entity is embedded as a sphere instead of a vector. While inheriting the high interpretability of rotational-based models, our SpherE can more expressively model one-to-many, many-to-one, and many-to-many relations. Through extensive experiments, we show that our SpherE can well address the set retrieval problem while still having a good predictive ability to infer missing facts. The code is available at https://github.com/Violet24K/SpherE.",
        "file_path": "paper_data/knowledge_graph_embedding/7029ecb5d5fc04f54e1e25e739db2e993fb147c8.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval \\cite{li2024}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Knowledge Graph Embedding (KGE) methods primarily follow an entity ranking protocol, where they predict the plausibility of a triple and return a ranked list of entities. This design is inherently inexpressive for modeling one-to-many, many-to-one, and especially many-to-many relations.\n    *   **Importance & Challenge:** In many real-world applications (e.g., bioinformatics, \"which genes cause an illness\"), users require an *exact set* of answers rather than a ranked list. Determining an appropriate cutoff threshold for a ranked list to obtain a precise set is challenging and often arbitrary. This paper introduces and formulates the novel problem of \"Knowledge Graph Set Retrieval\" to address this gap.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon rotational embedding methods like RotatE, RotatE3D, and HousE, which model relations as rotations and are known for handling symmetric and anti-symmetric relations.\n    *   **Limitations of Previous Solutions:**\n        *   **Translation-based methods (e.g., TransE, TransH):** Inexpressive for symmetric relations.\n        *   **Rotation-based methods (e.g., RotatE, QuatE):** While good for symmetry, they embed entities as vectors (points), making them inexpressive for one-to-many, many-to-one, and many-to-many relations.\n        *   **Complex models (e.g., BoxE, HousE):** Achieve high expressiveness but often at the cost of interpretability due to their intricate modeling.\n        *   **All existing KGE methods:** Primarily focus on entity ranking, which is unsuitable for \"set retrieval\" tasks requiring precise, unranked sets of answers.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes SpherE, a novel KGE model that embeds each entity as a *sphere* (defined by a center vector and a radius) in a Euclidean space, while relations are modeled as *rotations*.\n    *   **Novelty/Difference:**\n        *   **Sphere-based Entity Embedding:** Unlike traditional KGE models that embed entities as points (vectors), SpherE uses spheres. This allows for a natural representation of \"universality\" (how often an entity appears) through the sphere's radius.\n        *   **Set Retrieval Paradigm:** SpherE directly addresses the set retrieval problem by defining a triple `(h, r, t)` as true if the sphere of the transformed head entity `f_r(c_h)` *overlaps* (is non-disjoint) with the sphere of the tail entity `c_t`. This inherently supports multiple correct answers without requiring a ranking threshold.\n        *   **Expressiveness for Many-to-Many Relations:** The sphere-based modeling naturally captures one-to-many, many-to-one, and many-to-many relations, as a single transformed head sphere can overlap with multiple tail spheres.\n        *   **Interpretability:** SpherE inherits the interpretability of rotational models, and the entity radius provides an intuitive measure of an entity's \"universality\" or prevalence in the KG.\n\n4.  **Key Technical Contributions**\n    *   **Novel Problem Formulation:** First to introduce and formally define the \"Knowledge Graph Set Retrieval\" problem.\n    *   **Novel Embedding Model (SpherE):** Proposes embedding entities as spheres and relations as rotations, enabling direct set retrieval.\n    *   **Theoretical Expressiveness:** Provides theoretical proofs (Theorems 3.1, 3.2, 3.3) demonstrating SpherE's ability to model various inference patterns, including symmetry, anti-symmetry, inversion, composition, non-commutative composition (for k>=3D), multiplicity (for k>=3D), and all relation mapping properties (one-to-one, one-to-many, many-to-one, many-to-many).\n    *   **Interpretable Radius:** The entity radius is shown to correlate with the entity's occurrence frequency in the KG, providing a clear interpretation of its learned parameter.\n    *   **Specialized Loss Function:** Designs a sigmoid-based loss function that encourages sphere intersection for positive triples and disjointness for negative triples, incorporating hyperparameters `alpha` and `beta` to fine-tune intersection behavior.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Evaluated SpherE on two widely-used KGE benchmark datasets: FB15K237 and WN18RR.\n    *   **Baselines:** Compared against state-of-the-art rotational KGE methods (RotatE, RotatE3D, HousE-kD) adapted for set retrieval by truncating their ranked lists at various `top-l` cutoffs (l=1, 3, 5, 10, 20, 100).\n    *   **Key Performance Metrics:**\n        *   **F1 Score:** Measures the accuracy of the retrieved set against the ground truth set (Head F1, Tail F1).\n        *   **Retrieve Rate (RR):** Probability that the actual correct entity for a test triple is included in the retrieved set, indicating link prediction ability (Head RR, Tail RR).\n        *   **n-to-n F1:** F1 score specifically for many-to-many relations.\n    *   **Comparison Results:**\n        *   SpherE significantly *outperforms all baseline methods* in terms of Head F1, Tail F1, and especially n-to-n F1 on both datasets, demonstrating its superior ability for set retrieval and modeling many-to-many relations.\n        *   SpherE maintains a *comparable \"Retrieve Rate\"* (RR) to strong baselines (e.g., top-20 for FB15K237, top-3 for WN18RR), indicating it still has good predictive ability for inferring missing links.\n        *   The experiments empirically validate the interpretability of the radius, showing a positive correlation between an entity's radius and its occurrence count in the KG.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** The paper notes an observed performance drop in set retrieval tasks as the dimension of rotation (`k` in SpherE-kD) increases, suggesting an area for future investigation.\n    *   **Scope of Applicability:** SpherE is primarily designed for knowledge graph set retrieval tasks where precise, unranked sets of answers are required. While it also shows good link prediction capabilities, its core innovation lies in the set retrieval paradigm.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{li2024} makes a pioneering contribution by formally introducing and addressing the Knowledge Graph Set Retrieval problem, shifting the paradigm from entity ranking to direct set prediction.\n    *   **Potential Impact:**\n        *   **Enhanced KG Utility:** Enables more precise and user-friendly information retrieval from KGs, particularly in domains like bioinformatics, legal research, or any application requiring exact sets of related entities.\n        *   **New Research Direction:** Opens up a new avenue for KGE research focused on set-based predictions rather than solely ranking, potentially leading to more expressive and application-specific embedding models.\n        *   **Interpretable Models:** The sphere-based entity representation with an interpretable radius offers a more intuitive understanding of entity properties within the embedding space.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Knowledge Graph Set Retrieval",
          "SpherE model",
          "sphere-based entity embedding",
          "relations as rotations",
          "many-to-many relations",
          "interpretability",
          "theoretical expressiveness",
          "F1 Score",
          "direct set prediction",
          "entity radius",
          "occurrence frequency correlation",
          "bioinformatics"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "7029ecb5d5fc04f54e1e25e739db2e993fb147c8.pdf"
    },
    {
      "success": true,
      "doc_id": "cb0698f332d6f12d7abbe6162ae21173",
      "summary": "Here's a focused summary of the paper \"TorusE: Knowledge Graph Embedding on a Lie Group\" by Ebisu and Ichise \\cite{ebisu2017} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Knowledge graphs often have missing facts, requiring knowledge graph embedding (KGE) models for completion. TransE, a prominent translation-based KGE model, suffers from a fundamental flaw related to its regularization strategy.\n    *   **Importance & Challenge**: TransE's principle (`h+r=t`) is effective but its regularization (forcing entity embeddings onto a unit sphere) conflicts with this principle. This conflict warps embeddings, adversely affects link prediction accuracy, and prevents the model from fully realizing its potential, even though regularization is necessary to prevent embeddings from diverging.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: KGE models are broadly categorized into translation-based (e.g., TransE, TransH, TransR), bilinear (e.g., DistMult, ComplEx), and neural network-based (e.g., NTN).\n    *   **Limitations of Previous Solutions**:\n        *   **TransE**: Its regularization on a real vector space (unit sphere) conflicts with its core translation principle, leading to warped embeddings and reduced accuracy, particularly for HITS@1. It also struggles with 1-N, N-1, and N-N relations.\n        *   **Bilinear Models (e.g., DistMult, ComplEx)**: While achieving high accuracy on some metrics, they can have more redundancy, are prone to overfitting, and may require low-dimensional embedding spaces, which can be problematic for very large knowledge graphs.\n        *   **Neural Network Models**: Highly expressive but most susceptible to overfitting.\n    *   **Positioning**: TorusE directly addresses the regularization flaw of TransE by changing the embedding space, aiming to achieve better accuracy and scalability while retaining TransE's simplicity and efficiency. It is the first model to embed objects on a non-vector space (a Lie group) and formally discusses TransE's regularization problem.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: TorusE proposes embedding entities and relations not on a real vector space (Rn), but on a **torus (Tn)**, which is a compact Abelian Lie group.\n    *   **Novelty**:\n        *   **Elimination of Regularization**: By choosing a *compact* embedding space like a torus, embeddings are inherently bounded and cannot diverge indefinitely. This eliminates the need for explicit regularization (like sphere normalization), resolving the conflict between TransE's principle and its regularization.\n        *   **Lie Group as Embedding Space**: This is the first model to utilize a Lie group (specifically, a torus) as an embedding space for knowledge graph entities and relations, opening a new mathematical direction for KGE.\n        *   **Preservation of TransE Principle**: The model maintains the core translation principle `[h] + [r] = [t]` but defines it within the group operation of the torus.\n        *   **Novel Scoring Functions**: Introduces three scoring functions (`fL1`, `fL2`, `feL2`) derived from different distance metrics on the torus, which are normalized and bounded.\n\n*   **Key Technical Contributions**\n    *   **Novel Embedding Space**: Introduction of compact Lie groups (specifically, the n-dimensional torus) as a suitable embedding space for KGE, moving beyond traditional real or complex vector spaces.\n    *   **Formal Analysis of TransE's Flaw**: Provides a formal discussion and visualization of the inherent conflict between TransE's translation principle and its sphere-based regularization.\n    *   **Regularization-Free Learning**: Demonstrates that by leveraging the compactness of the torus, KGE models can be trained without explicit regularization, leading to more accurate and less warped embeddings.\n    *   **Scalability and Efficiency**: The removal of regularization steps and the inherent properties of the torus lead to a simpler model with lower computational complexity (O(n) time and space), making it more scalable and faster than the original TransE.\n    *   **Connection to Bilinear Models**: Shows an interesting mathematical similarity between its `feL2` scoring function and ComplEx when mapped to complex space.\n\n*   **Experimental Validation**\n    *   **Experiments**: Conducted link prediction tasks to evaluate the model's performance.\n    *   **Datasets**: Evaluated on standard benchmark datasets (though specific names are not provided in the abstract/introduction, Section 5 mentions \"benchmark datasets\").\n    *   **Metrics**: Likely uses standard link prediction metrics such as HITS@N (e.g., HITS@1, HITS@10), Mean Rank, Mean Reciprocal Rank (MRR), as implied by comparisons with other models.\n    *   **Key Results**:\n        *   **Performance**: TorusE outperforms state-of-the-art approaches including TransE, DistMult, and ComplEx on the standard link prediction task.\n        *   **Scalability**: Demonstrated to be scalable to large-size knowledge graphs.\n        *   **Efficiency**: Empirically shown to be faster than the original TransE due to reduced calculation times without regularization.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper primarily focuses on solving TransE's regularization problem. While it improves accuracy, it inherits the fundamental translation-based approach, which might still face challenges with complex relation types (e.g., 1-N, N-1, N-N relations) that more complex models like TransH or TransR aim to address. The paper does not explicitly state limitations of TorusE itself, but rather highlights how it overcomes limitations of previous models.\n    *   **Scope of Applicability**: Primarily focused on knowledge graph completion through link prediction. Applicable to various knowledge graphs, including large-scale ones, due to its improved scalability and efficiency.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: Significantly advances the technical state-of-the-art in KGE by identifying and effectively resolving a critical, previously unaddressed flaw in the widely used TransE model.\n    *   **New Research Direction**: Introduces a novel mathematical framework for KGE by demonstrating the utility of Lie groups as embedding spaces, potentially inspiring future research into other geometric or algebraic structures for representation learning.\n    *   **Practical Impact**: Offers a more accurate, scalable, and efficient model for knowledge graph completion, which is crucial for many AI tasks relying on complete and accurate knowledge bases.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) models are vital for completing incomplete knowledge bases, yet even prominent approaches like TransE suffer from a fundamental, often overlooked flaw: a critical conflict between its core translation principle and its regularization strategy. This conflict warps embeddings, hindering accuracy and scalability. We formally identify and resolve this long-standing issue by introducing **TorusE**, a novel KGE model that pioneers embedding entities and relations on a **compact Abelian Lie group**, specifically an *n*-dimensional **torus**.\n\nBy leveraging the inherent compactness of the torus, TorusE entirely eliminates the need for explicit regularization, thereby resolving the detrimental conflict and enabling truly unwarped, accurate representations. This groundbreaking shift to a non-vector embedding space not only preserves TransE's elegant translation principle but also significantly advances the state-of-the-art. Our experiments demonstrate that TorusE outperforms leading KGE models, including TransE, DistMult, and ComplEx, in **link prediction** tasks, while offering superior scalability and efficiency. TorusE opens a new mathematical frontier for KGE, showcasing the profound potential of Lie groups for robust and regularization-free representation learning.",
      "keywords": [
        "TorusE",
        "Knowledge Graph Embedding (KGE)",
        "Lie Group Embedding",
        "Torus (Tn)",
        "Regularization-Free Learning",
        "TransE Regularization Flaw",
        "Knowledge Graph Completion",
        "Link Prediction",
        "Compact Embedding Space",
        "Scalability and Efficiency",
        "Novel Scoring Functions",
        "Formal Analysis",
        "Outperforms State-of-the-Art",
        "New Research Direction"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/990334cf76845e2da64d3baa10b0a671e433d4b6.pdf",
      "citation_key": "ebisu2017",
      "metadata": {
        "title": "TorusE: Knowledge Graph Embedding on a Lie Group",
        "authors": [
          "Takuma Ebisu",
          "R. Ichise"
        ],
        "published_date": "2017",
        "abstract": "\n \n Knowledge graphs are useful for many artificial intelligence (AI) tasks. However, knowledge graphs often have missing facts. To populate the graphs, knowledge graph embedding models have been developed. Knowledge graph embedding models map entities and relations in a knowledge graph to a vector space and predict unknown triples by scoring candidate triples. TransE is the first translation-based method and it is well known because of its simplicity and efficiency for knowledge graph completion. It employs the principle that the differences between entity embeddings represent their relations. The principle seems very simple, but it can effectively capture the rules of a knowledge graph. However, TransE has a problem with its regularization. TransE forces entity embeddings to be on a sphere in the embedding vector space. This regularization warps the embeddings and makes it difficult for them to fulfill the abovementioned principle. The regularization also affects adversely the accuracies of the link predictions. On the other hand, regularization is important because entity embeddings diverge by negative sampling without it. This paper proposes a novel embedding model, TorusE, to solve the regularization problem. The principle of TransE can be defined on any Lie group. A torus, which is one of the compact Lie groups, can be chosen for the embedding space to avoid regularization. To the best of our knowledge, TorusE is the first model that embeds objects on other than a real or complex vector space, and this paper is the first to formally discuss the problem of regularization of TransE. Our approach outperforms other state-of-the-art approaches such as TransE, DistMult and ComplEx on a standard link prediction task. We show that TorusE is scalable to large-size knowledge graphs and is faster than the original TransE.\n \n",
        "file_path": "paper_data/knowledge_graph_embedding/990334cf76845e2da64d3baa10b0a671e433d4b6.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"TorusE: Knowledge Graph Embedding on a Lie Group\" by Ebisu and Ichise \\cite{ebisu2017} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Knowledge graphs often have missing facts, requiring knowledge graph embedding (KGE) models for completion. TransE, a prominent translation-based KGE model, suffers from a fundamental flaw related to its regularization strategy.\n    *   **Importance & Challenge**: TransE's principle (`h+r=t`) is effective but its regularization (forcing entity embeddings onto a unit sphere) conflicts with this principle. This conflict warps embeddings, adversely affects link prediction accuracy, and prevents the model from fully realizing its potential, even though regularization is necessary to prevent embeddings from diverging.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: KGE models are broadly categorized into translation-based (e.g., TransE, TransH, TransR), bilinear (e.g., DistMult, ComplEx), and neural network-based (e.g., NTN).\n    *   **Limitations of Previous Solutions**:\n        *   **TransE**: Its regularization on a real vector space (unit sphere) conflicts with its core translation principle, leading to warped embeddings and reduced accuracy, particularly for HITS@1. It also struggles with 1-N, N-1, and N-N relations.\n        *   **Bilinear Models (e.g., DistMult, ComplEx)**: While achieving high accuracy on some metrics, they can have more redundancy, are prone to overfitting, and may require low-dimensional embedding spaces, which can be problematic for very large knowledge graphs.\n        *   **Neural Network Models**: Highly expressive but most susceptible to overfitting.\n    *   **Positioning**: TorusE directly addresses the regularization flaw of TransE by changing the embedding space, aiming to achieve better accuracy and scalability while retaining TransE's simplicity and efficiency. It is the first model to embed objects on a non-vector space (a Lie group) and formally discusses TransE's regularization problem.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: TorusE proposes embedding entities and relations not on a real vector space (Rn), but on a **torus (Tn)**, which is a compact Abelian Lie group.\n    *   **Novelty**:\n        *   **Elimination of Regularization**: By choosing a *compact* embedding space like a torus, embeddings are inherently bounded and cannot diverge indefinitely. This eliminates the need for explicit regularization (like sphere normalization), resolving the conflict between TransE's principle and its regularization.\n        *   **Lie Group as Embedding Space**: This is the first model to utilize a Lie group (specifically, a torus) as an embedding space for knowledge graph entities and relations, opening a new mathematical direction for KGE.\n        *   **Preservation of TransE Principle**: The model maintains the core translation principle `[h] + [r] = [t]` but defines it within the group operation of the torus.\n        *   **Novel Scoring Functions**: Introduces three scoring functions (`fL1`, `fL2`, `feL2`) derived from different distance metrics on the torus, which are normalized and bounded.\n\n*   **Key Technical Contributions**\n    *   **Novel Embedding Space**: Introduction of compact Lie groups (specifically, the n-dimensional torus) as a suitable embedding space for KGE, moving beyond traditional real or complex vector spaces.\n    *   **Formal Analysis of TransE's Flaw**: Provides a formal discussion and visualization of the inherent conflict between TransE's translation principle and its sphere-based regularization.\n    *   **Regularization-Free Learning**: Demonstrates that by leveraging the compactness of the torus, KGE models can be trained without explicit regularization, leading to more accurate and less warped embeddings.\n    *   **Scalability and Efficiency**: The removal of regularization steps and the inherent properties of the torus lead to a simpler model with lower computational complexity (O(n) time and space), making it more scalable and faster than the original TransE.\n    *   **Connection to Bilinear Models**: Shows an interesting mathematical similarity between its `feL2` scoring function and ComplEx when mapped to complex space.\n\n*   **Experimental Validation**\n    *   **Experiments**: Conducted link prediction tasks to evaluate the model's performance.\n    *   **Datasets**: Evaluated on standard benchmark datasets (though specific names are not provided in the abstract/introduction, Section 5 mentions \"benchmark datasets\").\n    *   **Metrics**: Likely uses standard link prediction metrics such as HITS@N (e.g., HITS@1, HITS@10), Mean Rank, Mean Reciprocal Rank (MRR), as implied by comparisons with other models.\n    *   **Key Results**:\n        *   **Performance**: TorusE outperforms state-of-the-art approaches including TransE, DistMult, and ComplEx on the standard link prediction task.\n        *   **Scalability**: Demonstrated to be scalable to large-size knowledge graphs.\n        *   **Efficiency**: Empirically shown to be faster than the original TransE due to reduced calculation times without regularization.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper primarily focuses on solving TransE's regularization problem. While it improves accuracy, it inherits the fundamental translation-based approach, which might still face challenges with complex relation types (e.g., 1-N, N-1, N-N relations) that more complex models like TransH or TransR aim to address. The paper does not explicitly state limitations of TorusE itself, but rather highlights how it overcomes limitations of previous models.\n    *   **Scope of Applicability**: Primarily focused on knowledge graph completion through link prediction. Applicable to various knowledge graphs, including large-scale ones, due to its improved scalability and efficiency.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: Significantly advances the technical state-of-the-art in KGE by identifying and effectively resolving a critical, previously unaddressed flaw in the widely used TransE model.\n    *   **New Research Direction**: Introduces a novel mathematical framework for KGE by demonstrating the utility of Lie groups as embedding spaces, potentially inspiring future research into other geometric or algebraic structures for representation learning.\n    *   **Practical Impact**: Offers a more accurate, scalable, and efficient model for knowledge graph completion, which is crucial for many AI tasks relying on complete and accurate knowledge bases.",
        "keywords": [
          "TorusE",
          "Knowledge Graph Embedding (KGE)",
          "Lie Group Embedding",
          "Torus (Tn)",
          "Regularization-Free Learning",
          "TransE Regularization Flaw",
          "Knowledge Graph Completion",
          "Link Prediction",
          "Compact Embedding Space",
          "Scalability and Efficiency",
          "Novel Scoring Functions",
          "Formal Analysis",
          "Outperforms State-of-the-Art",
          "New Research Direction"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "990334cf76845e2da64d3baa10b0a671e433d4b6.pdf"
    },
    {
      "success": true,
      "doc_id": "5193df8f17e5704783fe6ce1961c0b6c",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review: Towards Robust Knowledge Graph Embedding via Multi-task Reinforcement Learning \\cite{zhang2021}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graphs (KGs) are often incomplete and contain significant noise and conflicts, primarily due to automatic knowledge construction and update mechanisms. Most Knowledge Graph Embedding (KGE) methods assume all triple facts are correct, leading to low-quality and unreliable representations.\n    *   **Importance and Challenge**: KGs are fundamental to many AI applications (e.g., information retrieval, question answering). Noisy data severely degrades KGE model performance, resulting in unsatisfactory outcomes in downstream tasks. Detecting and filtering noise without extensive human supervision is a critical and challenging problem.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **KGE Models**: \\cite{zhang2021} acknowledges three main categories: translation/rotation-based (e.g., TransE, RotatE), tensor factorization-based (e.g., DistMult), and neural network-based (e.g., ConvE). The proposed framework is designed to extend models from all these categories.\n        *   **KG Noise Detection**: Previous works include human supervision (labor-intensive), and confidence scoring methods like CKRL \\cite{zhang2021} and NoiGAN \\cite{zhang2021}.\n        *   **Multi-task Learning (MTL) in KGs**: Prior studies have used MTL for learning embeddings of similar entities/relations or joint learning in recommender systems.\n        *   **Reinforcement Learning (RL) in KGs**: RL has been applied to path-based KG reasoning and relation extraction tasks.\n    *   **Limitations of Previous Solutions**:\n        *   Most KGE models ignore the noisy data problem.\n        *   Confidence scoring methods (CKRL, NoiGAN) assign soft or hard confidence scores; \\cite{zhang2021} argues for a *hard decision* (true/false) for optimal leveraging of positive triples and complete removal of negative ones.\n        *   Existing noise detection methods often rely on costly human supervision or do not fully integrate with KGE learning in a robust, adaptive manner.\n        *   Previous RL applications in KGs did not address the specific problem of filtering noisy triples for KGE training.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{zhang2021} proposes a general multi-task reinforcement learning (MTRL) framework for robust KGE.\n        *   **Reinforcement Learning for Triple Selection**: Policy-based RL agents are designed to select high-quality knowledge triples while filtering out noisy ones. The agent makes a *hard decision* (select or discard) for each triple.\n        *   **Multi-task Learning for Similar Relations**: To leverage correlations, the triple selection processes for semantically similar relations are trained collectively using multi-task learning. Relation clusters are obtained (e.g., via k-means on TransE embeddings).\n        *   **Joint Training**: The RL agents and the KGE model are trained in an interleaved, joint manner. The KGE model provides a *delayed reward* to the RL agents based on the quality of the selected triples, guiding the learning process.\n    *   **Novelty/Differentiation**:\n        *   First work to apply RL to filter noise specifically for the KGE task.\n        *   Combines RL with MTL to enhance robustness and leverage relational similarities for noise filtering.\n        *   The framework is general and extensible, demonstrated by extending popular KGE models like TransE, DistMult, ConvE, and RotatE without requiring external information (text, logical rules).\n        *   The policy parameter `w_r` for each relation `r` is decomposed into a common part `u_c` (for relations in the same cluster) and a specific part `v_r`, enabling knowledge sharing while retaining individual characteristics.\n        *   A novel reward function is designed, incorporating the KGE model's score and a heuristic term to encourage the selection of a sufficient number of positive triples.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A multi-task reinforcement learning framework for robust KGE, integrating RL agents for hard triple selection and MTL for collective training of similar relations.\n        *   A policy-based agent design with a state representation encoding relation, current triple, and already selected triples.\n        *   Decomposition of policy parameters (`w_r = u_c + v_r`) to facilitate knowledge sharing among semantically similar relations within clusters.\n        *   A novel reward function that balances KGE model performance with the quantity of selected triples, preventing agents from selecting only a few high-score triples.\n    *   **System Design/Architectural Innovations**: A general, extensible framework that can be seamlessly integrated with various existing KGE models (e.g., TransE, DistMult, ConvE, RotatE) to enhance their robustness against noise.\n    *   **Theoretical Insights/Analysis**: The paper focuses on algorithmic design and empirical validation, with the decomposition of policy parameters and the reward function design being key algorithmic innovations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on noisy datasets to evaluate the effectiveness of the proposed framework.\n    *   **Key Performance Metrics and Comparison Results**: The extended models (X-MTRL, where X is TransE, DistMult, ConvE, or RotatE) were compared against their base models and other baseline competitors. Experimental results demonstrate that the proposed framework substantially enhances existing KGE models, providing more robust representations in noisy scenarios. A variant, X-STRL (Single-Task Reinforcement Learning), was also evaluated to highlight the benefits of multi-task learning.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The method relies on an initial clustering of relations (e.g., using k-means on TransE embeddings) to define \"semantically similar relations.\" The heuristic term in the reward function might require careful tuning. The \"hard decision\" approach, while argued for its benefits, might be less flexible than soft confidence scores in certain nuanced scenarios.\n    *   **Scope of Applicability**: The framework is applicable to KGE tasks where the training data is expected to contain noise. It is designed to work with internal KG information only, without requiring external textual or logical rule data.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{zhang2021} significantly advances the technical state-of-the-art in robust KGE by explicitly addressing the pervasive problem of noisy data. It introduces a novel paradigm that integrates reinforcement learning for adaptive data cleansing with multi-task learning for leveraging relational similarities, a combination previously unexplored for this specific problem.\n    *   **Potential Impact on Future Research**: This work paves the way for more reliable and robust KGE models, which are crucial for the performance of downstream AI applications. It opens new avenues for research into adaptive data filtering mechanisms using RL and MTL in other knowledge-intensive domains, potentially inspiring further innovations in self-correcting knowledge systems.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are foundational to AI, yet their pervasive incompleteness and inherent noise severely compromise the reliability of Knowledge Graph Embeddings (KGEs), undermining downstream applications. Current KGE methods largely assume perfect data, leading to brittle representations. We introduce a novel, generalizable **Multi-task Reinforcement Learning (MTRL)** framework designed to robustly train KGE models by adaptively filtering noisy triples. Our approach employs **policy-based Reinforcement Learning (RL) agents** that make *hard decisions* to select high-quality triples, a critical departure from soft confidence scoring. To enhance robustness and efficiency, these agents are trained collectively via **Multi-task Learning (MTL)**, leveraging semantic similarities within relation clusters through a decomposed policy parameterization. The RL agents and KGE model are jointly trained, with the KGE model providing a crucial **delayed reward** to guide optimal triple selection. Extensive experiments demonstrate that our framework significantly boosts the robustness of various KGE models (e.g., TransE, DistMult, ConvE, RotatE) against noise, yielding superior and more reliable representations. This work marks a significant advancement in building truly robust KGE systems, paving the way for more dependable knowledge-driven AI.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Noisy Knowledge Graphs",
        "Multi-task Reinforcement Learning (MTRL)",
        "Robust KGE",
        "Triple selection",
        "Policy-based RL agents",
        "Joint training",
        "Relation clustering",
        "Policy parameter decomposition",
        "Novel reward function",
        "Extensible framework",
        "Adaptive data filtering",
        "State-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/0367603c0197ab48eeba29aa6af391584a5077c0.pdf",
      "citation_key": "zhang2021",
      "metadata": {
        "title": "Towards Robust Knowledge Graph Embedding via Multi-Task Reinforcement Learning",
        "authors": [
          "Zhao Zhang",
          "Fuzhen Zhuang",
          "Hengshu Zhu",
          "Chao Li",
          "Hui Xiong",
          "Qing He",
          "Yongjun Xu"
        ],
        "published_date": "2021",
        "abstract": "Nowadays, Knowledge graphs (KGs) have been playing a pivotal role in AI-related applications. Despite the large sizes, existing KGs are far from complete and comprehensive. In order to continuously enrich KGs, automatic knowledge construction and update mechanisms are usually utilized, which inevitably bring in plenty of noise. However, most existing knowledge graph embedding (KGE) methods assume that all the triple facts in KGs are correct, and project both entities and relations into a low-dimensional space without considering noise and knowledge conflicts. This will lead to low-quality and unreliable representations of KGs. To this end, in this paper, we propose a general multi-task reinforcement learning framework, which can greatly alleviate the noisy data problem. In our framework, we exploit reinforcement learning for choosing high-quality knowledge triples while filtering out the noisy ones. Also, in order to take full advantage of the correlations among semantically similar relations, the triple selection processes of similar relations are trained in a collective way with multi-task learning. Moreover, we extend popular KGE models TransE, DistMult, ConvE and RotatE with the proposed framework. Finally, the experimental validation shows that our approach is able to enhance existing KGE models and can provide more robust representations of KGs in noisy scenarios.",
        "file_path": "paper_data/knowledge_graph_embedding/0367603c0197ab48eeba29aa6af391584a5077c0.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review: Towards Robust Knowledge Graph Embedding via Multi-task Reinforcement Learning \\cite{zhang2021}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graphs (KGs) are often incomplete and contain significant noise and conflicts, primarily due to automatic knowledge construction and update mechanisms. Most Knowledge Graph Embedding (KGE) methods assume all triple facts are correct, leading to low-quality and unreliable representations.\n    *   **Importance and Challenge**: KGs are fundamental to many AI applications (e.g., information retrieval, question answering). Noisy data severely degrades KGE model performance, resulting in unsatisfactory outcomes in downstream tasks. Detecting and filtering noise without extensive human supervision is a critical and challenging problem.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **KGE Models**: \\cite{zhang2021} acknowledges three main categories: translation/rotation-based (e.g., TransE, RotatE), tensor factorization-based (e.g., DistMult), and neural network-based (e.g., ConvE). The proposed framework is designed to extend models from all these categories.\n        *   **KG Noise Detection**: Previous works include human supervision (labor-intensive), and confidence scoring methods like CKRL \\cite{zhang2021} and NoiGAN \\cite{zhang2021}.\n        *   **Multi-task Learning (MTL) in KGs**: Prior studies have used MTL for learning embeddings of similar entities/relations or joint learning in recommender systems.\n        *   **Reinforcement Learning (RL) in KGs**: RL has been applied to path-based KG reasoning and relation extraction tasks.\n    *   **Limitations of Previous Solutions**:\n        *   Most KGE models ignore the noisy data problem.\n        *   Confidence scoring methods (CKRL, NoiGAN) assign soft or hard confidence scores; \\cite{zhang2021} argues for a *hard decision* (true/false) for optimal leveraging of positive triples and complete removal of negative ones.\n        *   Existing noise detection methods often rely on costly human supervision or do not fully integrate with KGE learning in a robust, adaptive manner.\n        *   Previous RL applications in KGs did not address the specific problem of filtering noisy triples for KGE training.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{zhang2021} proposes a general multi-task reinforcement learning (MTRL) framework for robust KGE.\n        *   **Reinforcement Learning for Triple Selection**: Policy-based RL agents are designed to select high-quality knowledge triples while filtering out noisy ones. The agent makes a *hard decision* (select or discard) for each triple.\n        *   **Multi-task Learning for Similar Relations**: To leverage correlations, the triple selection processes for semantically similar relations are trained collectively using multi-task learning. Relation clusters are obtained (e.g., via k-means on TransE embeddings).\n        *   **Joint Training**: The RL agents and the KGE model are trained in an interleaved, joint manner. The KGE model provides a *delayed reward* to the RL agents based on the quality of the selected triples, guiding the learning process.\n    *   **Novelty/Differentiation**:\n        *   First work to apply RL to filter noise specifically for the KGE task.\n        *   Combines RL with MTL to enhance robustness and leverage relational similarities for noise filtering.\n        *   The framework is general and extensible, demonstrated by extending popular KGE models like TransE, DistMult, ConvE, and RotatE without requiring external information (text, logical rules).\n        *   The policy parameter `w_r` for each relation `r` is decomposed into a common part `u_c` (for relations in the same cluster) and a specific part `v_r`, enabling knowledge sharing while retaining individual characteristics.\n        *   A novel reward function is designed, incorporating the KGE model's score and a heuristic term to encourage the selection of a sufficient number of positive triples.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A multi-task reinforcement learning framework for robust KGE, integrating RL agents for hard triple selection and MTL for collective training of similar relations.\n        *   A policy-based agent design with a state representation encoding relation, current triple, and already selected triples.\n        *   Decomposition of policy parameters (`w_r = u_c + v_r`) to facilitate knowledge sharing among semantically similar relations within clusters.\n        *   A novel reward function that balances KGE model performance with the quantity of selected triples, preventing agents from selecting only a few high-score triples.\n    *   **System Design/Architectural Innovations**: A general, extensible framework that can be seamlessly integrated with various existing KGE models (e.g., TransE, DistMult, ConvE, RotatE) to enhance their robustness against noise.\n    *   **Theoretical Insights/Analysis**: The paper focuses on algorithmic design and empirical validation, with the decomposition of policy parameters and the reward function design being key algorithmic innovations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on noisy datasets to evaluate the effectiveness of the proposed framework.\n    *   **Key Performance Metrics and Comparison Results**: The extended models (X-MTRL, where X is TransE, DistMult, ConvE, or RotatE) were compared against their base models and other baseline competitors. Experimental results demonstrate that the proposed framework substantially enhances existing KGE models, providing more robust representations in noisy scenarios. A variant, X-STRL (Single-Task Reinforcement Learning), was also evaluated to highlight the benefits of multi-task learning.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The method relies on an initial clustering of relations (e.g., using k-means on TransE embeddings) to define \"semantically similar relations.\" The heuristic term in the reward function might require careful tuning. The \"hard decision\" approach, while argued for its benefits, might be less flexible than soft confidence scores in certain nuanced scenarios.\n    *   **Scope of Applicability**: The framework is applicable to KGE tasks where the training data is expected to contain noise. It is designed to work with internal KG information only, without requiring external textual or logical rule data.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{zhang2021} significantly advances the technical state-of-the-art in robust KGE by explicitly addressing the pervasive problem of noisy data. It introduces a novel paradigm that integrates reinforcement learning for adaptive data cleansing with multi-task learning for leveraging relational similarities, a combination previously unexplored for this specific problem.\n    *   **Potential Impact on Future Research**: This work paves the way for more reliable and robust KGE models, which are crucial for the performance of downstream AI applications. It opens new avenues for research into adaptive data filtering mechanisms using RL and MTL in other knowledge-intensive domains, potentially inspiring further innovations in self-correcting knowledge systems.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Noisy Knowledge Graphs",
          "Multi-task Reinforcement Learning (MTRL)",
          "Robust KGE",
          "Triple selection",
          "Policy-based RL agents",
          "Joint training",
          "Relation clustering",
          "Policy parameter decomposition",
          "Novel reward function",
          "Extensible framework",
          "Adaptive data filtering",
          "State-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "0367603c0197ab48eeba29aa6af391584a5077c0.pdf"
    },
    {
      "success": true,
      "doc_id": "6daaa98adfaacdaaa6749af0275d3ec4",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Question Answering over Knowledge Graph (QA-KG), which aims to answer natural language questions using facts stored in a Knowledge Graph (KG) \\cite{huang2019}.\n    *   **Importance**: Enables end-users to access valuable KG knowledge efficiently without needing to understand its underlying data structures \\cite{huang2019}.\n    *   **Challenges**: Capturing the semantic meaning of natural language is difficult for machines. Predicates can be expressed in various ways in questions, and entity name ambiguity (including partial names) leads to a large number of possible answers \\cite{huang2019}.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Many Knowledge Graph Embedding (KGE) methods exist, representing entities and predicates as low-dimensional vectors to preserve KG relation information. These have benefited applications like KG completion and recommender systems \\cite{huang2019}.\n    *   **Positioning**: This work explores leveraging these existing KG embedding methods to address the QA-KG problem, specifically highlighting that despite KGEs, QA-KG remains challenging due to natural language complexities \\cite{huang2019}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes the Knowledge Embedding based Question Answering (KEQA) framework \\cite{huang2019}.\n    *   **Focus**: KEQA specifically targets \"simple questions,\" defined as those answerable by identifying a single head entity and a single predicate \\cite{huang2019}.\n    *   **Novelty**: Instead of directly inferring the head entity and predicate, KEQA innovatively aims to *jointly recover* the question's head entity, predicate, and *tail entity* representations within the KG embedding spaces \\cite{huang2019}.\n    *   **Mechanism**: An answer is derived by returning the closest fact in the KG based on a \"carefully-designed joint distance metric\" applied to these three jointly learned vectors \\cite{huang2019}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: Introduction of the KEQA framework for QA-KG \\cite{huang2019}.\n    *   **Novel Method**: A unique approach to jointly recover head entity, predicate, and tail entity representations from natural language questions within KG embedding spaces \\cite{huang2019}.\n    *   **Novel Technique**: Development of a \"carefully-designed joint distance metric\" to effectively match the recovered representations to KG facts \\cite{huang2019}.\n\n*   **Experimental Validation**\n    *   **Experiments**: Conducted on a \"widely-adopted benchmark\" for QA-KG \\cite{huang2019}.\n    *   **Key Results**: The proposed KEQA framework \"outperforms the state-of-the-art QA-KG methods\" on this benchmark \\cite{huang2019}.\n\n*   **Limitations & Scope**\n    *   **Scope**: The current KEQA framework is specifically designed for and focuses on answering \"simple questions,\" which are defined by a single head entity and a single predicate \\cite{huang2019}. This implies potential limitations for more complex question types.\n\n*   **Technical Significance**\n    *   **Advancement**: KEQA significantly advances the technical state-of-the-art in QA-KG by demonstrating superior performance over existing methods \\cite{huang2019}.\n    *   **Impact**: It provides an effective and novel approach to bridge the gap between natural language questions and KG embeddings, particularly for simple questions, paving the way for future research in leveraging joint representation recovery for complex QA tasks.",
      "intriguing_abstract": "Unlocking the vast potential of Knowledge Graphs (KGs) for end-users demands robust Question Answering (QA) systems, yet bridging the semantic gap between natural language and structured KG data remains a formidable challenge. While Knowledge Graph Embedding (KGE) methods have advanced various KG applications, their direct use in QA-KG struggles with natural language ambiguity and diverse predicate expressions.\n\nThis paper introduces the **Knowledge Embedding based Question Answering (KEQA)** framework, a novel approach for simple questions. KEQA innovatively moves beyond inferring just head entities and predicates; it proposes a unique mechanism to *jointly recover* the question's head entity, predicate, and crucially, the *tail entity* representations directly within KG embedding spaces. An answer is then precisely identified using a carefully-designed joint distance metric to match these recovered representations to KG facts.\n\nEvaluated on a widely-adopted benchmark, KEQA significantly outperforms state-of-the-art QA-KG methods, demonstrating superior capability. This work advances the technical frontier in QA-KG, offering a powerful paradigm for bridging natural language understanding with KG embeddings and paving the way for more intuitive access to structured knowledge.",
      "keywords": [
        "Question Answering over Knowledge Graph (QA-KG)",
        "Knowledge Graph Embedding (KGE)",
        "KEQA framework",
        "Joint representation recovery",
        "Head entity",
        "predicate",
        "tail entity",
        "KG embedding spaces",
        "Joint distance metric",
        "Simple questions",
        "Natural language understanding",
        "State-of-the-art performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/7572aefcd241ec76341addcb2e2e417587cb2e4c.pdf",
      "citation_key": "huang2019",
      "metadata": {
        "title": "Knowledge Graph Embedding Based Question Answering",
        "authors": [
          "Xiao Huang",
          "Jingyuan Zhang",
          "Dingcheng Li",
          "Ping Li"
        ],
        "published_date": "2019",
        "abstract": "Question answering over knowledge graph (QA-KG) aims to use facts in the knowledge graph (KG) to answer natural language questions. It helps end users more efficiently and more easily access the substantial and valuable knowledge in the KG, without knowing its data structures. QA-KG is a nontrivial problem since capturing the semantic meaning of natural language is difficult for a machine. Meanwhile, many knowledge graph embedding methods have been proposed. The key idea is to represent each predicate/entity as a low-dimensional vector, such that the relation information in the KG could be preserved. The learned vectors could benefit various applications such as KG completion and recommender systems. In this paper, we explore to use them to handle the QA-KG problem. However, this remains a challenging task since a predicate could be expressed in different ways in natural language questions. Also, the ambiguity of entity names and partial names makes the number of possible answers large. To bridge the gap, we propose an effective Knowledge Embedding based Question Answering (KEQA) framework. We focus on answering the most common types of questions, i.e., simple questions, in which each question could be answered by the machine straightforwardly if its single head entity and single predicate are correctly identified. To answer a simple question, instead of inferring its head entity and predicate directly, KEQA targets at jointly recovering the question's head entity, predicate, and tail entity representations in the KG embedding spaces. Based on a carefully-designed joint distance metric, the three learned vectors' closest fact in the KG is returned as the answer. Experiments on a widely-adopted benchmark demonstrate that the proposed KEQA outperforms the state-of-the-art QA-KG methods.",
        "file_path": "paper_data/knowledge_graph_embedding/7572aefcd241ec76341addcb2e2e417587cb2e4c.pdf",
        "venue": "Web Search and Data Mining",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Question Answering over Knowledge Graph (QA-KG), which aims to answer natural language questions using facts stored in a Knowledge Graph (KG) \\cite{huang2019}.\n    *   **Importance**: Enables end-users to access valuable KG knowledge efficiently without needing to understand its underlying data structures \\cite{huang2019}.\n    *   **Challenges**: Capturing the semantic meaning of natural language is difficult for machines. Predicates can be expressed in various ways in questions, and entity name ambiguity (including partial names) leads to a large number of possible answers \\cite{huang2019}.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Many Knowledge Graph Embedding (KGE) methods exist, representing entities and predicates as low-dimensional vectors to preserve KG relation information. These have benefited applications like KG completion and recommender systems \\cite{huang2019}.\n    *   **Positioning**: This work explores leveraging these existing KG embedding methods to address the QA-KG problem, specifically highlighting that despite KGEs, QA-KG remains challenging due to natural language complexities \\cite{huang2019}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes the Knowledge Embedding based Question Answering (KEQA) framework \\cite{huang2019}.\n    *   **Focus**: KEQA specifically targets \"simple questions,\" defined as those answerable by identifying a single head entity and a single predicate \\cite{huang2019}.\n    *   **Novelty**: Instead of directly inferring the head entity and predicate, KEQA innovatively aims to *jointly recover* the question's head entity, predicate, and *tail entity* representations within the KG embedding spaces \\cite{huang2019}.\n    *   **Mechanism**: An answer is derived by returning the closest fact in the KG based on a \"carefully-designed joint distance metric\" applied to these three jointly learned vectors \\cite{huang2019}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: Introduction of the KEQA framework for QA-KG \\cite{huang2019}.\n    *   **Novel Method**: A unique approach to jointly recover head entity, predicate, and tail entity representations from natural language questions within KG embedding spaces \\cite{huang2019}.\n    *   **Novel Technique**: Development of a \"carefully-designed joint distance metric\" to effectively match the recovered representations to KG facts \\cite{huang2019}.\n\n*   **Experimental Validation**\n    *   **Experiments**: Conducted on a \"widely-adopted benchmark\" for QA-KG \\cite{huang2019}.\n    *   **Key Results**: The proposed KEQA framework \"outperforms the state-of-the-art QA-KG methods\" on this benchmark \\cite{huang2019}.\n\n*   **Limitations & Scope**\n    *   **Scope**: The current KEQA framework is specifically designed for and focuses on answering \"simple questions,\" which are defined by a single head entity and a single predicate \\cite{huang2019}. This implies potential limitations for more complex question types.\n\n*   **Technical Significance**\n    *   **Advancement**: KEQA significantly advances the technical state-of-the-art in QA-KG by demonstrating superior performance over existing methods \\cite{huang2019}.\n    *   **Impact**: It provides an effective and novel approach to bridge the gap between natural language questions and KG embeddings, particularly for simple questions, paving the way for future research in leveraging joint representation recovery for complex QA tasks.",
        "keywords": [
          "Question Answering over Knowledge Graph (QA-KG)",
          "Knowledge Graph Embedding (KGE)",
          "KEQA framework",
          "Joint representation recovery",
          "Head entity",
          "predicate",
          "tail entity",
          "KG embedding spaces",
          "Joint distance metric",
          "Simple questions",
          "Natural language understanding",
          "State-of-the-art performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "7572aefcd241ec76341addcb2e2e417587cb2e4c.pdf"
    },
    {
      "success": true,
      "doc_id": "31204394a81d16638750333da347774a",
      "summary": "Here's a focused summary of the technical paper \\cite{tang2019} for a literature review:\n\n---\n\n### Analysis of \"Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding\" \\cite{tang2019}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of accurately predicting complex relations (N-to-1, 1-to-N, and N-to-N) in knowledge graph link prediction. Existing distance-based knowledge graph embedding (KGE) models, including the state-of-the-art RotatE \\cite{tang2019}, struggle with these relation types.\n    *   **Importance & Challenge:** Knowledge graphs are crucial for many AI applications (e.g., recommendation, question answering), but they are often incomplete and require periodic updates. Link prediction is vital for knowledge graph completion. Complex relations are challenging because a single entity-relation pair can map to multiple different entities, leading to ambiguity and reduced prediction accuracy \\cite{tang2019}. RotatE's limitation to 2D complex domain restricts its modeling capacity, and it does not explicitly consider graph context, which is beneficial for these complex relation types \\cite{tang2019}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** \\cite{tang2019} builds upon distance-based KGE models, particularly RotatE \\cite{tang2019}, which models relations as 2D rotations and naturally handles symmetric/anti-symmetric, inverse, and compositional relation patterns.\n    *   **Limitations of Previous Solutions:**\n        *   **RotatE \\cite{tang2019}:** Limited to 2D complex domain, restricting its overall modeling capacity. It also does not incorporate graph context, which is crucial for resolving ambiguities in complex relations.\n        *   **General KGE methods:** Many traditional KGE methods (e.g., TransE, DistMult, ComplEx) focus on modeling individual triples and often ignore the broader knowledge graph structure and context from neighboring nodes and edges \\cite{tang2019}.\n        *   **GNN-based context modeling:** While some approaches use Graph Neural Networks (GNNs) in an encoder-decoder framework to capture graph structure \\cite{tang2019}, this paper takes a different approach by integrating graph context directly into the distance scoring function.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The proposed approach, **Orthogonal Transform Embedding (OTE) with Graph Context (GC-OTE)**, combines two main innovations:\n        1.  **Orthogonal Transform Embedding (OTE):** Extends RotatE's 2D complex rotations to high-dimensional orthogonal transforms for relations. Entity embeddings are divided into *K* sub-embeddings, and each relation is represented by *K* orthogonal matrices, each operating on a sub-embedding. The Gram-Schmidt process is used to ensure the orthogonality of these relation matrices during training, with gradients handled by PyTorch's autograd \\cite{tang2019}.\n        2.  **Directed Graph Context Modeling:** Integrates explicit graph context directly into the distance scoring function. For each entity, two directed context representations are computed:\n            *   **Head-Relation Pair Context:** For a tail entity *t*, it's the average of representations of (head, relation) pairs where *t* is the tail.\n            *   **Relation-Tail Pair Context:** For a head entity *h*, it's the average of representations of (relation, tail) pairs where *h* is the head.\n            These context representations are then used as part of the distance scoring function \\cite{tang2019}.\n    *   **Novelty/Difference:**\n        *   **High-dimensional relation modeling:** Unlike RotatE's 2D rotations, OTE uses high-dimensional orthogonal transforms, significantly increasing modeling capacity while retaining the ability to model symmetric/anti-symmetric, inverse, and compositional relation patterns \\cite{tang2019}.\n        *   **Direct context integration:** Instead of using GNNs as a separate encoder, \\cite{tang2019} directly incorporates directed graph context into the distance scoring function, making it an integral part of the plausibility measurement.\n        *   **Ensemble-like scoring:** The final scoring function combines four distance scores (head-to-tail projection, tail-to-head projection, head-relation context, relation-tail context) across *K* sub-embeddings, effectively acting as an ensemble of *K* local GC-OTE models \\cite{tang2019}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Orthogonal Transform Embedding (OTE):** A new method that extends RotatE's relation modeling from 2D complex space to high-dimensional orthogonal transforms, enhancing modeling capacity while preserving key relation properties (symmetry/antisymmetry, inversion, composition) \\cite{tang2019}.\n        *   **Directed Graph Context Modeling:** A novel approach to explicitly model and integrate graph context (neighboring entities and relations) directly into the distance scoring function for KGE, specifically designed to address complex relation types \\cite{tang2019}.\n    *   **System Design/Architectural Innovations:** The integration of orthogonal transforms with a sub-embedding group structure and the direct incorporation of directed graph context into a unified scoring function (GC-OTE) represents a novel architectural design for distance-based KGE \\cite{tang2019}.\n    *   **Theoretical Insights/Analysis:** The paper proves that OTE retains the ability to model symmetry/antisymmetry, inversion, and compositional relation patterns, similar to RotatE \\cite{tang2019}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Link prediction experiments were performed on two standard benchmark datasets \\cite{tang2019}.\n    *   **Key Performance Metrics:** Mean Reciprocal Rank (MRR) and Hits@k (k=1, 3, 10) were used to evaluate performance \\cite{tang2019}.\n    *   **Comparison Results:**\n        *   GC-OTE consistently outperformed RotatE \\cite{tang2019}, the previous state-of-the-art distance-based model, on both FB15k-237 and WN18RR datasets.\n        *   Achieved state-of-the-art results on FB15k-237, particularly noted for its effectiveness on datasets with many high in-degree nodes (implying better handling of complex relations) \\cite{tang2019}.\n        *   Achieved new state-of-the-art performance on the WN18RR dataset \\cite{tang2019}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The context modeling relies on averaging neighboring entity-relation representations, which might be a relatively simple aggregation compared to more advanced GNN aggregation schemes \\cite{tang2019}.\n        *   The Gram-Schmidt process is applied during each forward pass to ensure orthogonality, which, while stable with autograd, might introduce some computational overhead compared to models without such constraints \\cite{tang2019}.\n    *   **Scope of Applicability:** The method is primarily applicable to knowledge graph link prediction tasks, especially those involving complex N-to-1, 1-to-N, and N-to-N relations. It is designed for distance-based embedding models and could potentially be adapted to other translational embedding algorithms \\cite{tang2019}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{tang2019} significantly advances the technical state-of-the-art in distance-based knowledge graph embedding by overcoming the modeling capacity limitations of RotatE and effectively integrating graph context. It provides new state-of-the-art results on prominent benchmarks \\cite{tang2019}.\n    *   **Potential Impact on Future Research:**\n        *   **Improved handling of complex relations:** The explicit modeling of directed graph context and high-dimensional orthogonal transforms offers a robust framework for addressing challenging N-to-N type relations, which can inspire future research in this area.\n        *   **Hybrid KGE models:** The direct integration of context into the scoring function, rather than a separate encoder, presents an alternative paradigm for leveraging graph structure, potentially leading to more tightly coupled and efficient hybrid KGE models.\n        *   **Orthogonal transforms in KGE:** The successful application of orthogonal transforms via Gram-Schmidt and autograd could encourage further exploration of orthogonal constraints for relation modeling in other KGE architectures \\cite{tang2019}.",
      "intriguing_abstract": "Knowledge graphs are indispensable for AI, yet their inherent incompleteness, particularly concerning complex N-to-N relations, remains a critical bottleneck for applications like question answering and recommendation. Existing knowledge graph embedding (KGE) models, including the state-of-the-art RotatE, often struggle with these ambiguous relation types due to limited modeling capacity and a failure to explicitly leverage crucial graph context.\n\nWe introduce **Orthogonal Transform Embedding with Graph Context (GC-OTE)**, a novel framework that significantly advances link prediction for complex relations. GC-OTE innovates on two fronts: First, it extends RotatE's 2D complex rotations to high-dimensional *orthogonal transforms*, dramatically enhancing relation modeling capacity while preserving essential relational patterns. Second, it directly integrates *directed graph context* into the distance scoring function, effectively resolving the ambiguities inherent in complex N-to-N relations. This unique combination allows GC-OTE to capture intricate structural information previously overlooked. Our extensive experiments demonstrate that GC-OTE achieves new state-of-the-art performance on benchmark datasets like FB15k-237 and WN18RR, significantly outperforming RotatE. This work offers a robust solution for knowledge graph completion, paving the way for more accurate and comprehensive AI applications.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Complex Relations",
        "Link Prediction",
        "Orthogonal Transform Embedding (OTE)",
        "Graph Context Modeling",
        "GC-OTE",
        "High-dimensional Orthogonal Transforms",
        "Directed Graph Context",
        "RotatE",
        "Distance-based KGE Models",
        "Gram-Schmidt Process",
        "State-of-the-art performance",
        "Modeling Capacity"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/c2c6edc5750a438bddd1217481832d38df6336de.pdf",
      "citation_key": "tang2019",
      "metadata": {
        "title": "Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding",
        "authors": [
          "Yun Tang",
          "Jing Huang",
          "Guangtao Wang",
          "Xiaodong He",
          "Bowen Zhou"
        ],
        "published_date": "2019",
        "abstract": "Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.",
        "file_path": "paper_data/knowledge_graph_embedding/c2c6edc5750a438bddd1217481832d38df6336de.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper \\cite{tang2019} for a literature review:\n\n---\n\n### Analysis of \"Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding\" \\cite{tang2019}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of accurately predicting complex relations (N-to-1, 1-to-N, and N-to-N) in knowledge graph link prediction. Existing distance-based knowledge graph embedding (KGE) models, including the state-of-the-art RotatE \\cite{tang2019}, struggle with these relation types.\n    *   **Importance & Challenge:** Knowledge graphs are crucial for many AI applications (e.g., recommendation, question answering), but they are often incomplete and require periodic updates. Link prediction is vital for knowledge graph completion. Complex relations are challenging because a single entity-relation pair can map to multiple different entities, leading to ambiguity and reduced prediction accuracy \\cite{tang2019}. RotatE's limitation to 2D complex domain restricts its modeling capacity, and it does not explicitly consider graph context, which is beneficial for these complex relation types \\cite{tang2019}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** \\cite{tang2019} builds upon distance-based KGE models, particularly RotatE \\cite{tang2019}, which models relations as 2D rotations and naturally handles symmetric/anti-symmetric, inverse, and compositional relation patterns.\n    *   **Limitations of Previous Solutions:**\n        *   **RotatE \\cite{tang2019}:** Limited to 2D complex domain, restricting its overall modeling capacity. It also does not incorporate graph context, which is crucial for resolving ambiguities in complex relations.\n        *   **General KGE methods:** Many traditional KGE methods (e.g., TransE, DistMult, ComplEx) focus on modeling individual triples and often ignore the broader knowledge graph structure and context from neighboring nodes and edges \\cite{tang2019}.\n        *   **GNN-based context modeling:** While some approaches use Graph Neural Networks (GNNs) in an encoder-decoder framework to capture graph structure \\cite{tang2019}, this paper takes a different approach by integrating graph context directly into the distance scoring function.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The proposed approach, **Orthogonal Transform Embedding (OTE) with Graph Context (GC-OTE)**, combines two main innovations:\n        1.  **Orthogonal Transform Embedding (OTE):** Extends RotatE's 2D complex rotations to high-dimensional orthogonal transforms for relations. Entity embeddings are divided into *K* sub-embeddings, and each relation is represented by *K* orthogonal matrices, each operating on a sub-embedding. The Gram-Schmidt process is used to ensure the orthogonality of these relation matrices during training, with gradients handled by PyTorch's autograd \\cite{tang2019}.\n        2.  **Directed Graph Context Modeling:** Integrates explicit graph context directly into the distance scoring function. For each entity, two directed context representations are computed:\n            *   **Head-Relation Pair Context:** For a tail entity *t*, it's the average of representations of (head, relation) pairs where *t* is the tail.\n            *   **Relation-Tail Pair Context:** For a head entity *h*, it's the average of representations of (relation, tail) pairs where *h* is the head.\n            These context representations are then used as part of the distance scoring function \\cite{tang2019}.\n    *   **Novelty/Difference:**\n        *   **High-dimensional relation modeling:** Unlike RotatE's 2D rotations, OTE uses high-dimensional orthogonal transforms, significantly increasing modeling capacity while retaining the ability to model symmetric/anti-symmetric, inverse, and compositional relation patterns \\cite{tang2019}.\n        *   **Direct context integration:** Instead of using GNNs as a separate encoder, \\cite{tang2019} directly incorporates directed graph context into the distance scoring function, making it an integral part of the plausibility measurement.\n        *   **Ensemble-like scoring:** The final scoring function combines four distance scores (head-to-tail projection, tail-to-head projection, head-relation context, relation-tail context) across *K* sub-embeddings, effectively acting as an ensemble of *K* local GC-OTE models \\cite{tang2019}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Orthogonal Transform Embedding (OTE):** A new method that extends RotatE's relation modeling from 2D complex space to high-dimensional orthogonal transforms, enhancing modeling capacity while preserving key relation properties (symmetry/antisymmetry, inversion, composition) \\cite{tang2019}.\n        *   **Directed Graph Context Modeling:** A novel approach to explicitly model and integrate graph context (neighboring entities and relations) directly into the distance scoring function for KGE, specifically designed to address complex relation types \\cite{tang2019}.\n    *   **System Design/Architectural Innovations:** The integration of orthogonal transforms with a sub-embedding group structure and the direct incorporation of directed graph context into a unified scoring function (GC-OTE) represents a novel architectural design for distance-based KGE \\cite{tang2019}.\n    *   **Theoretical Insights/Analysis:** The paper proves that OTE retains the ability to model symmetry/antisymmetry, inversion, and compositional relation patterns, similar to RotatE \\cite{tang2019}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Link prediction experiments were performed on two standard benchmark datasets \\cite{tang2019}.\n    *   **Key Performance Metrics:** Mean Reciprocal Rank (MRR) and Hits@k (k=1, 3, 10) were used to evaluate performance \\cite{tang2019}.\n    *   **Comparison Results:**\n        *   GC-OTE consistently outperformed RotatE \\cite{tang2019}, the previous state-of-the-art distance-based model, on both FB15k-237 and WN18RR datasets.\n        *   Achieved state-of-the-art results on FB15k-237, particularly noted for its effectiveness on datasets with many high in-degree nodes (implying better handling of complex relations) \\cite{tang2019}.\n        *   Achieved new state-of-the-art performance on the WN18RR dataset \\cite{tang2019}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The context modeling relies on averaging neighboring entity-relation representations, which might be a relatively simple aggregation compared to more advanced GNN aggregation schemes \\cite{tang2019}.\n        *   The Gram-Schmidt process is applied during each forward pass to ensure orthogonality, which, while stable with autograd, might introduce some computational overhead compared to models without such constraints \\cite{tang2019}.\n    *   **Scope of Applicability:** The method is primarily applicable to knowledge graph link prediction tasks, especially those involving complex N-to-1, 1-to-N, and N-to-N relations. It is designed for distance-based embedding models and could potentially be adapted to other translational embedding algorithms \\cite{tang2019}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{tang2019} significantly advances the technical state-of-the-art in distance-based knowledge graph embedding by overcoming the modeling capacity limitations of RotatE and effectively integrating graph context. It provides new state-of-the-art results on prominent benchmarks \\cite{tang2019}.\n    *   **Potential Impact on Future Research:**\n        *   **Improved handling of complex relations:** The explicit modeling of directed graph context and high-dimensional orthogonal transforms offers a robust framework for addressing challenging N-to-N type relations, which can inspire future research in this area.\n        *   **Hybrid KGE models:** The direct integration of context into the scoring function, rather than a separate encoder, presents an alternative paradigm for leveraging graph structure, potentially leading to more tightly coupled and efficient hybrid KGE models.\n        *   **Orthogonal transforms in KGE:** The successful application of orthogonal transforms via Gram-Schmidt and autograd could encourage further exploration of orthogonal constraints for relation modeling in other KGE architectures \\cite{tang2019}.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Complex Relations",
          "Link Prediction",
          "Orthogonal Transform Embedding (OTE)",
          "Graph Context Modeling",
          "GC-OTE",
          "High-dimensional Orthogonal Transforms",
          "Directed Graph Context",
          "RotatE",
          "Distance-based KGE Models",
          "Gram-Schmidt Process",
          "State-of-the-art performance",
          "Modeling Capacity"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "c2c6edc5750a438bddd1217481832d38df6336de.pdf"
    },
    {
      "success": true,
      "doc_id": "5e4cb5a8ba24935856f2955f3fcd8cc3",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review: RKGE \\cite{sun2018}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Improving recommendation systems by effectively leveraging Knowledge Graphs (KGs).\n    *   **Importance & Challenge:** Existing KG-based recommendation methods primarily depend on hand-engineered features (e.g., meta-paths) derived from KGs. This process is labor-intensive, requires significant domain knowledge, and can limit the discovery of complex, implicit relationships.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the established effectiveness of KGs in enhancing recommendation systems.\n    *   **Limitations of Previous Solutions:** Prior methods are constrained by their reliance on manual feature engineering from KGs, which demands specialized domain expertise and can be a bottleneck for scalability and adaptability.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces RKGE (Recurrent Knowledge Graph Embedding), a KG embedding approach designed to automatically learn semantic representations for both entities and the paths connecting them. These learned representations are then used to characterize user preferences towards items.\n    *   **Novelty:**\n        *   Employs a novel recurrent network architecture.\n        *   This architecture contains a *batch of recurrent networks* specifically designed to model the semantics of multiple paths linking the *same entity pair*.\n        *   These learned path semantics are seamlessly fused into the recommendation process.\n        *   Further incorporates a pooling operator to discriminate and leverage the saliency (importance) of different paths in characterizing user preferences.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** RKGE, a KG embedding approach that automates the learning of entity and path semantics for recommendation.\n    *   **System Design/Architectural Innovations:** A novel recurrent network architecture featuring a batch of recurrent networks to capture diverse path semantics between entity pairs, and its seamless integration into a recommendation framework.\n    *   **Novel Techniques:** Introduction of a pooling operator to assess and utilize the saliency of different paths in preference modeling.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive validation was performed on real-world datasets.\n    *   **Key Performance Metrics & Comparison Results:** RKGE demonstrated superior performance against state-of-the-art methods.\n    *   **Additional Finding:** The paper also highlights that RKGE provides meaningful explanations for its recommendation results, adding a valuable interpretability aspect.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The provided abstract does not explicitly state specific technical limitations or assumptions of RKGE.\n    *   **Scope of Applicability:** Primarily focused on improving recommendation systems through advanced KG embedding techniques, particularly by automating the feature learning process from KG paths.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** RKGE significantly advances the technical state-of-the-art by moving beyond manual feature engineering in KG-based recommendation. It offers an automated, data-driven approach to learn complex semantic relationships from KG paths.\n    *   **Potential Impact on Future Research:** This work paves the way for more robust, scalable, and less domain-knowledge-dependent KG-enhanced recommendation systems. The ability to provide explanations also opens avenues for research into more transparent and trustworthy AI in recommendation.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graphs (KGs) in recommendation systems has been hampered by the laborious and domain-specific process of manual feature engineering. We introduce Recurrent Knowledge Graph Embedding (RKGE), a novel approach that automates the discovery of complex, implicit relationships by learning rich semantic representations for both entities and the diverse paths connecting them. RKGE employs a unique recurrent network architecture, featuring a *batch of recurrent networks* specifically designed to capture the varied semantics of multiple paths between the same entity pair. A novel pooling operator further discriminates and leverages the saliency of these paths, seamlessly fusing them into the recommendation process. Extensive experiments on real-world datasets demonstrate RKGE's superior performance over state-of-the-art methods. Crucially, RKGE also provides meaningful explanations for its recommendations, moving beyond black-box models. This work not only advances the state-of-the-art in KG-enhanced recommendation by eliminating the bottleneck of manual feature engineering but also paves the way for more robust, scalable, and interpretable AI systems.",
      "keywords": [
        "RKGE (Recurrent Knowledge Graph Embedding)",
        "Recommendation systems",
        "Knowledge Graphs (KGs)",
        "KG embedding",
        "Recurrent network architecture",
        "Automated feature learning",
        "Learning path semantics",
        "Batch of recurrent networks",
        "Pooling operator",
        "Path saliency",
        "User preferences characterization",
        "Interpretability",
        "Superior performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/a6a735f8e218f772e5b9dac411fa4abea87fdb9c.pdf",
      "citation_key": "sun2018",
      "metadata": {
        "title": "Recurrent knowledge graph embedding for effective recommendation",
        "authors": [
          "Zhu Sun",
          "Jie Yang",
          "Jie Zhang",
          "A. Bozzon",
          "Long-Kai Huang",
          "Chi Xu"
        ],
        "published_date": "2018",
        "abstract": "Knowledge graphs (KGs) have proven to be effective to improve recommendation. Existing methods mainly rely on hand-engineered features from KGs (e.g., meta paths), which requires domain knowledge. This paper presents RKGE, a KG embedding approach that automatically learns semantic representations of both entities and paths between entities for characterizing user preferences towards items. Specifically, RKGE employs a novel recurrent network architecture that contains a batch of recurrent networks to model the semantics of paths linking a same entity pair, which are seamlessly fused into recommendation. It further employs a pooling operator to discriminate the saliency of different paths in characterizing user preferences towards items. Extensive validation on real-world datasets shows the superiority of RKGE against state-of-the-art methods. Furthermore, we show that RKGE provides meaningful explanations for recommendation results.",
        "file_path": "paper_data/knowledge_graph_embedding/a6a735f8e218f772e5b9dac411fa4abea87fdb9c.pdf",
        "venue": "ACM Conference on Recommender Systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review: RKGE \\cite{sun2018}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Improving recommendation systems by effectively leveraging Knowledge Graphs (KGs).\n    *   **Importance & Challenge:** Existing KG-based recommendation methods primarily depend on hand-engineered features (e.g., meta-paths) derived from KGs. This process is labor-intensive, requires significant domain knowledge, and can limit the discovery of complex, implicit relationships.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the established effectiveness of KGs in enhancing recommendation systems.\n    *   **Limitations of Previous Solutions:** Prior methods are constrained by their reliance on manual feature engineering from KGs, which demands specialized domain expertise and can be a bottleneck for scalability and adaptability.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces RKGE (Recurrent Knowledge Graph Embedding), a KG embedding approach designed to automatically learn semantic representations for both entities and the paths connecting them. These learned representations are then used to characterize user preferences towards items.\n    *   **Novelty:**\n        *   Employs a novel recurrent network architecture.\n        *   This architecture contains a *batch of recurrent networks* specifically designed to model the semantics of multiple paths linking the *same entity pair*.\n        *   These learned path semantics are seamlessly fused into the recommendation process.\n        *   Further incorporates a pooling operator to discriminate and leverage the saliency (importance) of different paths in characterizing user preferences.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** RKGE, a KG embedding approach that automates the learning of entity and path semantics for recommendation.\n    *   **System Design/Architectural Innovations:** A novel recurrent network architecture featuring a batch of recurrent networks to capture diverse path semantics between entity pairs, and its seamless integration into a recommendation framework.\n    *   **Novel Techniques:** Introduction of a pooling operator to assess and utilize the saliency of different paths in preference modeling.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive validation was performed on real-world datasets.\n    *   **Key Performance Metrics & Comparison Results:** RKGE demonstrated superior performance against state-of-the-art methods.\n    *   **Additional Finding:** The paper also highlights that RKGE provides meaningful explanations for its recommendation results, adding a valuable interpretability aspect.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The provided abstract does not explicitly state specific technical limitations or assumptions of RKGE.\n    *   **Scope of Applicability:** Primarily focused on improving recommendation systems through advanced KG embedding techniques, particularly by automating the feature learning process from KG paths.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** RKGE significantly advances the technical state-of-the-art by moving beyond manual feature engineering in KG-based recommendation. It offers an automated, data-driven approach to learn complex semantic relationships from KG paths.\n    *   **Potential Impact on Future Research:** This work paves the way for more robust, scalable, and less domain-knowledge-dependent KG-enhanced recommendation systems. The ability to provide explanations also opens avenues for research into more transparent and trustworthy AI in recommendation.",
        "keywords": [
          "RKGE (Recurrent Knowledge Graph Embedding)",
          "Recommendation systems",
          "Knowledge Graphs (KGs)",
          "KG embedding",
          "Recurrent network architecture",
          "Automated feature learning",
          "Learning path semantics",
          "Batch of recurrent networks",
          "Pooling operator",
          "Path saliency",
          "User preferences characterization",
          "Interpretability",
          "Superior performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "a6a735f8e218f772e5b9dac411fa4abea87fdb9c.pdf"
    },
    {
      "success": true,
      "doc_id": "87c629791e3a43cee100e3e6e4cc3001",
      "summary": "Here is a focused summary of the survey paper by \\cite{ge2023} for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey provides a comprehensive overview of Knowledge Graph Embedding (KGE) models, primarily focusing on Knowledge Graph (KG) completion tasks like link prediction.\n    *   Its main objectives are to analyze two core KGE design branchesdistance-based and semantic matching-based methodsto uncover intrinsic connections and underlying trends, and to discuss the integration of KGE with pre-trained language models (PLMs).\n\n2.  **Literature Coverage**\n    *   The survey reviews KGE models published from 2013 to 2022, as evidenced by its timeline figure, and references 12 other survey papers from the same period.\n    *   Literature inclusion focuses on models categorized as distance-based or semantic matching-based, along with emerging approaches leveraging neural networks and PLMs. The paper also compiles relevant resources such as open-source KGs, benchmarking datasets, and performance leaderboards.\n\n3.  **Classification Framework**\n    *   The survey primarily categorizes KGE models into two major classes based on their scoring functions and interaction modeling: distance-based models and semantic matching-based models.\n    *   It further discusses CompoundE and CompoundE3D as unifying frameworks for distance-based models that utilize affine operations, and also addresses neural network-based models and PLM-integrated approaches as emerging directions.\n\n4.  **Key Findings & Insights**\n    *   A significant trend identified is the combination of various geometric transformations (translation, rotation, scaling, reflection, projection) to enhance KGE model performance and capture complex relation patterns.\n    *   The survey highlights the intrinsic connections between diverse distance-based models, proposing that CompoundE and CompoundE3D can unify many of these affine operation-based techniques.\n    *   It contrasts distance-based models (modeling relations as transformations to minimize entity vector distance) with semantic matching models (measuring semantic scores via bilinear functions).\n    *   An emerging consensus points towards the integration of KGE methods with pre-trained language models (PLMs) and textual descriptions as a promising direction for KG completion.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey identifies a gap in existing literature regarding the intrinsic connections between different distance-based embedding models that utilize geometric transformations, which this paper aims to address.\n    *   Recommended future research directions include exploring the underlying trend of combining geometric transformations to invent novel models, and further integrating KGE embedding methods with PLMs for enhanced KG completion.\n\n6.  **Survey Contribution**\n    *   This survey provides unique value by offering a perspective on the intrinsic connections and unifying principles among distance-based KGE models that employ geometric transformations.\n    *   It is comprehensive in its overview of KGE models, benchmarking resources, and discussion of emerging trends, including the integration of KGE with PLMs.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graphs (KGs) hinges on robust Knowledge Graph Embedding (KGE) models, particularly for critical tasks like link prediction. This comprehensive survey delves into the evolving landscape of KGEs from 2013-2022, offering a novel perspective that unifies the diverse family of **distance-based models**. We reveal the intrinsic connections underlying various **geometric transformations**translation, rotation, scaling, reflection, and projectiondemonstrating how frameworks like **CompoundE** and **CompoundE3D** can serve as unifying principles for many affine operation-based techniques. Beyond this foundational clarity, the survey meticulously contrasts distance-based and **semantic matching** models, and critically examines the burgeoning integration of KGEs with **pre-trained language models (PLMs)** and textual descriptions as a pivotal future direction for **KG completion**. By synthesizing a decade of research, identifying key trends, and proposing a unifying lens for geometric KGEs, this work provides an indispensable guide for researchers, illuminating paths for inventing novel models and advancing the frontier of KG completion.",
      "keywords": [
        "Knowledge Graph Embedding (KGE) models",
        "Knowledge Graph completion",
        "link prediction",
        "distance-based KGE models",
        "semantic matching-based KGE models",
        "pre-trained language models (PLMs) integration",
        "geometric transformations",
        "CompoundE and CompoundE3D unifying frameworks",
        "intrinsic connections between KGE models",
        "scoring functions",
        "affine operations",
        "emerging trends"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/f2b924e69735fb7fd6fd95c6a032954480862029.pdf",
      "citation_key": "ge2023",
      "metadata": {
        "title": "Knowledge Graph Embedding: An Overview",
        "authors": [
          "Xiou Ge",
          "Yun Cheng Wang",
          "Bin Wang",
          "C.-C. Jay Kuo"
        ],
        "published_date": "2023",
        "abstract": "Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including distance-based and semantic-based methods. We will also discuss an emerging approach for KG completion which leverages pre-trained language models (PLMs) and textual descriptions of entities and relations and offer insights into the integration of KGE embedding methods with PLMs for KG completion.",
        "file_path": "paper_data/knowledge_graph_embedding/f2b924e69735fb7fd6fd95c6a032954480862029.pdf",
        "venue": "APSIPA Transactions on Signal and Information Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper by \\cite{ge2023} for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey provides a comprehensive overview of Knowledge Graph Embedding (KGE) models, primarily focusing on Knowledge Graph (KG) completion tasks like link prediction.\n    *   Its main objectives are to analyze two core KGE design branchesdistance-based and semantic matching-based methodsto uncover intrinsic connections and underlying trends, and to discuss the integration of KGE with pre-trained language models (PLMs).\n\n2.  **Literature Coverage**\n    *   The survey reviews KGE models published from 2013 to 2022, as evidenced by its timeline figure, and references 12 other survey papers from the same period.\n    *   Literature inclusion focuses on models categorized as distance-based or semantic matching-based, along with emerging approaches leveraging neural networks and PLMs. The paper also compiles relevant resources such as open-source KGs, benchmarking datasets, and performance leaderboards.\n\n3.  **Classification Framework**\n    *   The survey primarily categorizes KGE models into two major classes based on their scoring functions and interaction modeling: distance-based models and semantic matching-based models.\n    *   It further discusses CompoundE and CompoundE3D as unifying frameworks for distance-based models that utilize affine operations, and also addresses neural network-based models and PLM-integrated approaches as emerging directions.\n\n4.  **Key Findings & Insights**\n    *   A significant trend identified is the combination of various geometric transformations (translation, rotation, scaling, reflection, projection) to enhance KGE model performance and capture complex relation patterns.\n    *   The survey highlights the intrinsic connections between diverse distance-based models, proposing that CompoundE and CompoundE3D can unify many of these affine operation-based techniques.\n    *   It contrasts distance-based models (modeling relations as transformations to minimize entity vector distance) with semantic matching models (measuring semantic scores via bilinear functions).\n    *   An emerging consensus points towards the integration of KGE methods with pre-trained language models (PLMs) and textual descriptions as a promising direction for KG completion.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey identifies a gap in existing literature regarding the intrinsic connections between different distance-based embedding models that utilize geometric transformations, which this paper aims to address.\n    *   Recommended future research directions include exploring the underlying trend of combining geometric transformations to invent novel models, and further integrating KGE embedding methods with PLMs for enhanced KG completion.\n\n6.  **Survey Contribution**\n    *   This survey provides unique value by offering a perspective on the intrinsic connections and unifying principles among distance-based KGE models that employ geometric transformations.\n    *   It is comprehensive in its overview of KGE models, benchmarking resources, and discussion of emerging trends, including the integration of KGE with PLMs.",
        "keywords": [
          "Knowledge Graph Embedding (KGE) models",
          "Knowledge Graph completion",
          "link prediction",
          "distance-based KGE models",
          "semantic matching-based KGE models",
          "pre-trained language models (PLMs) integration",
          "geometric transformations",
          "CompoundE and CompoundE3D unifying frameworks",
          "intrinsic connections between KGE models",
          "scoring functions",
          "affine operations",
          "emerging trends"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "f2b924e69735fb7fd6fd95c6a032954480862029.pdf"
    },
    {
      "success": true,
      "doc_id": "3bdab0e80082d0aba4a7afbc1e63ea83",
      "summary": "Here is a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the problem of knowledge graph completion, aiming to improve the representation learning of entities and relations within knowledge graphs.\n    *   **Importance & Challenge**: Knowledge graphs are crucial for AI applications. Existing state-of-the-art graph convolutional neural network (GCN)-based models for knowledge graph completion face challenges because they assign uniform weights to relation paths and neglect the rich information available in neighbor nodes, leading to incomplete mining of triple features.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon and aims to improve state-of-the-art knowledge graph completion models, which are primarily based on graph convolutional neural networks (GCNs). These GCN-based models are recognized for their ability to automatically extract features and generate expressive feature embeddings.\n    *   **Limitations of Previous Solutions**: Previous GCN-based methods are limited by assigning the same weights across different relation paths and failing to effectively leverage the rich information present in neighbor nodes. This results in an incomplete capture of triple features.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Graph Attenuated Attention networks (GAATs)**, a novel representation learning method for knowledge graphs \\cite{wang2020}.\n    *   **Novelty**: GAATs integrate an **attenuated attention mechanism**. This mechanism is designed to assign different weights to different relation paths and to actively acquire information from neighboring nodes. This allows for a more nuanced and comprehensive learning of entity and relation embeddings.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of **Graph Attenuated Attention networks (GAATs)**.\n    *   **Novel Techniques**: Development of an **attenuated attention mechanism** that dynamically assigns varying weights to relation paths.\n    *   **Enhanced Information Acquisition**: The mechanism specifically enables the acquisition of information from neighbor nodes, which was previously overlooked.\n    *   **Improved Learning Scope**: As a result, entities and relations can be learned effectively from *any* neighbors, leading to more robust and expressive representations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Empirical research was conducted to evaluate the effectiveness of the proposed attenuated attention-based models.\n    *   **Key Performance Metrics & Comparison Results**: The GAATs model demonstrated **significant improvement** compared to state-of-the-art methods. This superior performance was validated on two widely used benchmark datasets: **WN18RR** and **FB15k-237**.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract does not explicitly detail specific technical limitations or assumptions of the GAATs model itself. Its primary focus is on addressing the limitations of prior GCN-based approaches.\n    *   **Scope of Applicability**: The method is specifically designed for knowledge graph completion tasks, aiming to enhance the quality of entity and relation embeddings by better leveraging graph structure and neighbor information.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The introduction of GAATs and its attenuated attention mechanism advances the technical state-of-the-art in knowledge graph completion by overcoming the limitations of uniform weighting and neglected neighbor information in previous GCN-based models.\n    *   **Potential Impact**: This work highlights the effectiveness of attention-based mechanisms for dynamically weighting information in knowledge graphs. It suggests a promising direction for future research in representation learning, particularly for tasks requiring a fine-grained understanding of relational paths and local graph structures.",
      "intriguing_abstract": "Unlocking the full potential of knowledge graphs for AI applications hinges on robust representation learning. Current state-of-the-art graph convolutional neural networks (GCNs) for **knowledge graph completion**, however, are hampered by their inability to dynamically weight relation paths and effectively leverage rich neighbor node information, resulting in incomplete triple feature mining. We introduce **Graph Attenuated Attention networks (GAATs)**, a novel architecture that fundamentally redefines how entities and relations are learned. Our core innovation is an **attenuated attention mechanism** that intelligently assigns varying weights to relation paths and actively acquires nuanced information from *any* neighboring nodes. This breakthrough enables the learning of significantly more robust and expressive **entity and relation embeddings**. Empirical validation on benchmark datasets **WN18RR** and **FB15k-237** demonstrates GAATs' superior performance, substantially outperforming existing methods. GAATs offer a powerful new paradigm, pushing the boundaries of **knowledge graph completion** and paving the way for more sophisticated, context-aware AI systems.",
      "keywords": [
        "knowledge graph completion",
        "representation learning",
        "Graph Attenuated Attention networks (GAATs)",
        "attenuated attention mechanism",
        "entity and relation embeddings",
        "graph convolutional neural networks (GCNs)",
        "dynamic relation path weighting",
        "neighbor node information acquisition",
        "state-of-the-art performance",
        "WN18RR",
        "FB15k-237 benchmarks"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a.pdf",
      "citation_key": "wang2020",
      "metadata": {
        "title": "Knowledge Graph Embedding via Graph Attenuated Attention Networks",
        "authors": [
          "Rui Wang",
          "Bicheng Li",
          "Shengwei Hu",
          "W. Du",
          "Min Zhang"
        ],
        "published_date": "2020",
        "abstract": "Knowledge graphs contain a wealth of real-world knowledge that can provide strong support for artificial intelligence applications. Much progress has been made in knowledge graph completion, state-of-the-art models are based on graph convolutional neural networks. These models automatically extract features, in combination with the features of the graph model, to generate feature embeddings with a strong expressive ability. However, these methods assign the same weights on the relation path in the knowledge graph and ignore the rich information presented in neighbor nodes, which result in incomplete mining of triple features. To this end, we propose Graph Attenuated Attention networks(GAATs), a novel representation method, which integrates an attenuated attention mechanism to assign different weight in different relation path and acquire the information from the neighborhoods. As a result, entities and relations can be learned in any neighbors. Our empirical research provides insight into the effectiveness of the attenuated attention-based models, and we show significant improvement compared to the state-of-the-art methods on two benchmark datasets WN18RR and FB15k-237.",
        "file_path": "paper_data/knowledge_graph_embedding/e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a.pdf",
        "venue": "IEEE Access",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the problem of knowledge graph completion, aiming to improve the representation learning of entities and relations within knowledge graphs.\n    *   **Importance & Challenge**: Knowledge graphs are crucial for AI applications. Existing state-of-the-art graph convolutional neural network (GCN)-based models for knowledge graph completion face challenges because they assign uniform weights to relation paths and neglect the rich information available in neighbor nodes, leading to incomplete mining of triple features.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon and aims to improve state-of-the-art knowledge graph completion models, which are primarily based on graph convolutional neural networks (GCNs). These GCN-based models are recognized for their ability to automatically extract features and generate expressive feature embeddings.\n    *   **Limitations of Previous Solutions**: Previous GCN-based methods are limited by assigning the same weights across different relation paths and failing to effectively leverage the rich information present in neighbor nodes. This results in an incomplete capture of triple features.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Graph Attenuated Attention networks (GAATs)**, a novel representation learning method for knowledge graphs \\cite{wang2020}.\n    *   **Novelty**: GAATs integrate an **attenuated attention mechanism**. This mechanism is designed to assign different weights to different relation paths and to actively acquire information from neighboring nodes. This allows for a more nuanced and comprehensive learning of entity and relation embeddings.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of **Graph Attenuated Attention networks (GAATs)**.\n    *   **Novel Techniques**: Development of an **attenuated attention mechanism** that dynamically assigns varying weights to relation paths.\n    *   **Enhanced Information Acquisition**: The mechanism specifically enables the acquisition of information from neighbor nodes, which was previously overlooked.\n    *   **Improved Learning Scope**: As a result, entities and relations can be learned effectively from *any* neighbors, leading to more robust and expressive representations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Empirical research was conducted to evaluate the effectiveness of the proposed attenuated attention-based models.\n    *   **Key Performance Metrics & Comparison Results**: The GAATs model demonstrated **significant improvement** compared to state-of-the-art methods. This superior performance was validated on two widely used benchmark datasets: **WN18RR** and **FB15k-237**.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract does not explicitly detail specific technical limitations or assumptions of the GAATs model itself. Its primary focus is on addressing the limitations of prior GCN-based approaches.\n    *   **Scope of Applicability**: The method is specifically designed for knowledge graph completion tasks, aiming to enhance the quality of entity and relation embeddings by better leveraging graph structure and neighbor information.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The introduction of GAATs and its attenuated attention mechanism advances the technical state-of-the-art in knowledge graph completion by overcoming the limitations of uniform weighting and neglected neighbor information in previous GCN-based models.\n    *   **Potential Impact**: This work highlights the effectiveness of attention-based mechanisms for dynamically weighting information in knowledge graphs. It suggests a promising direction for future research in representation learning, particularly for tasks requiring a fine-grained understanding of relational paths and local graph structures.",
        "keywords": [
          "knowledge graph completion",
          "representation learning",
          "Graph Attenuated Attention networks (GAATs)",
          "attenuated attention mechanism",
          "entity and relation embeddings",
          "graph convolutional neural networks (GCNs)",
          "dynamic relation path weighting",
          "neighbor node information acquisition",
          "state-of-the-art performance",
          "WN18RR",
          "FB15k-237 benchmarks"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a.pdf"
    },
    {
      "success": true,
      "doc_id": "218699823ec280b478dc8672bab4e676",
      "summary": "Here's a focused summary of the paper \"HousE: Knowledge Graph Embedding with Householder Parameterization\" \\cite{li2022} for a literature review:\n\n---\n\n### HousE: Knowledge Graph Embedding with Householder Parameterization \\cite{li2022}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) models struggle to simultaneously and effectively model the diverse intrinsic relation patterns (e.g., symmetry, antisymmetry, inversion, composition) and complex relation mapping properties (RMPs, e.g., 1-to-N, N-to-1, N-to-N). Many models have insufficient modeling capacity, often restricted to low-dimensional spaces.\n    *   **Importance & Challenge**: KGE is crucial for predicting missing links in incomplete real-world KGs. Accurately capturing these varied relation characteristics is fundamental for learning robust and informative entity and relation representations, but it's challenging because different properties often require conflicting mathematical operations (e.g., distance-preserving rotations for patterns vs. distance-adjusting projections for RMPs).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon and generalizes rotation-based KGE models like RotatE, Rotate3D, and QuatE, which represent relations as rotations in 2D, 3D, and 4D spaces, respectively. It also relates to projection-based models like TransH, TransR, and TransD.\n    *   **Limitations of Previous Solutions**:\n        *   **TransE and its variants (TransX)**: Fail to model symmetry and RMPs effectively.\n        *   **DistMult, ComplEx**: Can model some patterns but not all (e.g., DistMult struggles with antisymmetry, ComplEx with composition).\n        *   **Rotation-based models (RotatE, Rotate3D, QuatE, DualE)**: While effective at modeling relation patterns (symmetry, antisymmetry, inversion, composition), they are inherently distance-preserving, making them incapable of handling sophisticated RMPs (1-to-N, N-to-1, N-to-N) where relative distances need to change.\n        *   **Dimensionality Constraint**: Many advanced rotation-based approaches are specifically designed for fixed, low-dimensional spaces (2D, 3D, 4D), which may be inadequate for capturing the complex structures of large KGs.\n        *   **Projection-based models**: Existing projection methods are often irreversible, leading to failures in modeling inversion and composition patterns.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: HousE introduces a novel parameterization based on two types of Householder transformations:\n        1.  **Householder Rotations**: Relations are modeled as high-dimensional rotations (k-dimensional, where k can be >4) using compositions of Householder reflections. This forms the basis of `HousE-r`.\n        2.  **Householder Projections**: To address RMPs, `HousE` modifies vanilla Householder reflections into \"Householder projections.\" These are invertible transformations that can flexibly adjust the relative distances between points.\n    *   **Novelty/Differentiation**:\n        *   **Unified Framework**: HousE is the first to combine high-dimensional rotations and invertible projections within a single framework to simultaneously model all crucial relation patterns and RMPs.\n        *   **High-Dimensional Rotations**: It leverages a theoretical proof that any k-dimensional rotation can be represented as a composition of `2 * floor(k/2)` Householder reflections, allowing for rotations in arbitrary high-dimensional spaces, unlike previous fixed-low-dimensional approaches.\n        *   **Invertible Projections**: The proposed Householder projections are invertible, which is crucial for maintaining the ability to model inversion and composition patterns, a limitation of prior projection-based methods.\n        *   **Generalization**: HousE is a generalization of existing rotation-based models (RotatE, Rotate3D, QuatE) by extending rotations to k-dimensional spaces.\n        *   **Efficient Computation**: Matrix-vector multiplications for Householder transformations are optimized into vector operations, reducing time complexity from O(2nk^2) to O(2nk) for rotations and similar for projections.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of **Householder parameterization** for KGE, utilizing Householder reflections and projections.\n        *   **HousE-r**: A model for high-dimensional (k-dimensional) relational rotations based on compositions of Householder reflections, theoretically capable of modeling symmetry, antisymmetry, inversion, and composition.\n        *   **Householder Projections**: A novel type of invertible projection, derived from modified Householder matrices, designed to flexibly adjust distances and handle RMPs without sacrificing pattern modeling.\n        *   **HousE**: A unified framework combining Householder rotations and Householder projections to model all relation patterns and RMPs simultaneously.\n    *   **Theoretical Insights/Analysis**:\n        *   Proof that any k-dimensional rotation can be represented as `2 * floor(k/2)` Householder reflections (Theorem 3.1).\n        *   Theoretical claims demonstrating HousE's capability to model symmetry, antisymmetry, inversion, composition, and RMPs (Claims 3.2-3.5).\n        *   Analysis of the invertibility of Householder projections.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Link prediction tasks.\n    *   **Key Performance Metrics**: Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits@1 (H@1), Hits@3 (H@3), Hits@10 (H@10).\n    *   **Comparison Results**: HousE consistently achieves new state-of-the-art performance across five benchmark datasets, including WN18 and FB15k, outperforming strong baselines like TransE, DistMult, ComplEx, ConvE, RotatE, Rotate3D, and QuatE. For example, on WN18, HousE achieves MRR of 0.952 and H@10 of 0.962, surpassing RotatE (0.949 MRR, 0.959 H@10) and QuatE (0.949 MRR, 0.959 H@10). On FB15k, HousE achieves MRR of 0.801 and H@10 of 0.889, outperforming RotatE (0.797 MRR, 0.884 H@10).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   While HousE-r (pure Householder rotations) initially suffered from the limitation of not being able to model RMPs due to its distance-preserving nature, the full HousE framework explicitly addresses and overcomes this by integrating Householder projections.\n        *   The paper does not explicitly state new limitations of the *final* HousE model, but rather positions it as a comprehensive solution to previous limitations.\n    *   **Scope of Applicability**: Primarily focused on knowledge graph embedding for link prediction tasks. The framework's generalizability to other KG-related tasks (e.g., KG completion, entity classification) is implied but not explicitly demonstrated beyond link prediction.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HousE significantly advances the technical state-of-the-art in KGE by providing a powerful and general framework capable of simultaneously modeling all crucial relation patterns and RMPs, a challenge that previous models could only partially address. Its ability to perform high-dimensional rotations offers superior modeling capacity.\n    *   **Potential Impact on Future Research**:\n        *   Provides a new paradigm for KGE by leveraging Householder parameterization, potentially inspiring further research into geometric transformations for representation learning.\n        *   The generalization of rotation-based models to arbitrary high dimensions opens avenues for exploring optimal embedding dimensions for different KGs.\n        *   The concept of invertible projections for RMPs could be adapted to other domains requiring flexible distance adjustments while preserving structural properties.\n        *   The theoretical proofs and efficient computation methods contribute to a deeper understanding and practical application of advanced linear algebra in machine learning.",
      "intriguing_abstract": "Effectively modeling **Knowledge Graph Embedding (KGE)** requires simultaneously capturing diverse **relation patterns** (e.g., symmetry, composition) and complex **relation mapping properties (RMPs)** like 1-to-N. Current models often fall short, limited by fixed low-dimensional spaces or an inability to reconcile these conflicting demands.\n\nWe introduce **HousE**, a novel KGE framework leveraging **Householder parameterization** to overcome these limitations. HousE pioneers **high-dimensional Householder rotations**, generalizing previous rotation-based models to arbitrary *k*-dimensional spaces, profoundly enhancing modeling capacity for intricate relation patterns.\n\nCrucially, HousE introduces **invertible Householder projections** that flexibly adjust relative distances to model sophisticated RMPs, a limitation of prior models. This unified framework is the first to simultaneously and effectively capture *all* critical relation patterns and RMPs, overcoming inherent KGE trade-offs.\n\nTheoretical analysis and extensive experiments confirm HousE's comprehensive modeling and practical superiority. HousE achieves new **state-of-the-art** performance on benchmark **link prediction** tasks, significantly outperforming strong baselines. This work offers a powerful new paradigm for KGE, paving the way for more robust and expressive knowledge representation.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Householder Parameterization",
        "Relation Patterns",
        "Relation Mapping Properties (RMPs)",
        "High-dimensional Rotations",
        "Invertible Householder Projections",
        "Unified KGE Framework",
        "Link Prediction",
        "State-of-the-Art Performance",
        "Geometric Transformations",
        "Modeling Capacity",
        "Householder Reflections"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/63836e669416668744c3676a831060e8de3f58a1.pdf",
      "citation_key": "li2022",
      "metadata": {
        "title": "HousE: Knowledge Graph Embedding with Householder Parameterization",
        "authors": [
          "Rui Li",
          "Jianan Zhao",
          "Chaozhuo Li",
          "Di He",
          "Yiqi Wang",
          "Yuming Liu",
          "Hao Sun",
          "Senzhang Wang",
          "Weiwei Deng",
          "Yanming Shen",
          "Xing Xie",
          "Qi Zhang"
        ],
        "published_date": "2022",
        "abstract": "The effectiveness of knowledge graph embedding (KGE) largely depends on the ability to model intrinsic relation patterns and mapping properties. However, existing approaches can only capture some of them with insufficient modeling capacity. In this work, we propose a more powerful KGE framework named HousE, which involves a novel parameterization based on two kinds of Householder transformations: (1) Householder rotations to achieve superior capacity of modeling relation patterns; (2) Householder projections to handle sophisticated relation mapping properties. Theoretically, HousE is capable of modeling crucial relation patterns and mapping properties simultaneously. Besides, HousE is a generalization of existing rotation-based models while extending the rotations to high-dimensional spaces. Empirically, HousE achieves new state-of-the-art performance on five benchmark datasets. Our code is available at https://github.com/anrep/HousE.",
        "file_path": "paper_data/knowledge_graph_embedding/63836e669416668744c3676a831060e8de3f58a1.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"HousE: Knowledge Graph Embedding with Householder Parameterization\" \\cite{li2022} for a literature review:\n\n---\n\n### HousE: Knowledge Graph Embedding with Householder Parameterization \\cite{li2022}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) models struggle to simultaneously and effectively model the diverse intrinsic relation patterns (e.g., symmetry, antisymmetry, inversion, composition) and complex relation mapping properties (RMPs, e.g., 1-to-N, N-to-1, N-to-N). Many models have insufficient modeling capacity, often restricted to low-dimensional spaces.\n    *   **Importance & Challenge**: KGE is crucial for predicting missing links in incomplete real-world KGs. Accurately capturing these varied relation characteristics is fundamental for learning robust and informative entity and relation representations, but it's challenging because different properties often require conflicting mathematical operations (e.g., distance-preserving rotations for patterns vs. distance-adjusting projections for RMPs).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon and generalizes rotation-based KGE models like RotatE, Rotate3D, and QuatE, which represent relations as rotations in 2D, 3D, and 4D spaces, respectively. It also relates to projection-based models like TransH, TransR, and TransD.\n    *   **Limitations of Previous Solutions**:\n        *   **TransE and its variants (TransX)**: Fail to model symmetry and RMPs effectively.\n        *   **DistMult, ComplEx**: Can model some patterns but not all (e.g., DistMult struggles with antisymmetry, ComplEx with composition).\n        *   **Rotation-based models (RotatE, Rotate3D, QuatE, DualE)**: While effective at modeling relation patterns (symmetry, antisymmetry, inversion, composition), they are inherently distance-preserving, making them incapable of handling sophisticated RMPs (1-to-N, N-to-1, N-to-N) where relative distances need to change.\n        *   **Dimensionality Constraint**: Many advanced rotation-based approaches are specifically designed for fixed, low-dimensional spaces (2D, 3D, 4D), which may be inadequate for capturing the complex structures of large KGs.\n        *   **Projection-based models**: Existing projection methods are often irreversible, leading to failures in modeling inversion and composition patterns.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: HousE introduces a novel parameterization based on two types of Householder transformations:\n        1.  **Householder Rotations**: Relations are modeled as high-dimensional rotations (k-dimensional, where k can be >4) using compositions of Householder reflections. This forms the basis of `HousE-r`.\n        2.  **Householder Projections**: To address RMPs, `HousE` modifies vanilla Householder reflections into \"Householder projections.\" These are invertible transformations that can flexibly adjust the relative distances between points.\n    *   **Novelty/Differentiation**:\n        *   **Unified Framework**: HousE is the first to combine high-dimensional rotations and invertible projections within a single framework to simultaneously model all crucial relation patterns and RMPs.\n        *   **High-Dimensional Rotations**: It leverages a theoretical proof that any k-dimensional rotation can be represented as a composition of `2 * floor(k/2)` Householder reflections, allowing for rotations in arbitrary high-dimensional spaces, unlike previous fixed-low-dimensional approaches.\n        *   **Invertible Projections**: The proposed Householder projections are invertible, which is crucial for maintaining the ability to model inversion and composition patterns, a limitation of prior projection-based methods.\n        *   **Generalization**: HousE is a generalization of existing rotation-based models (RotatE, Rotate3D, QuatE) by extending rotations to k-dimensional spaces.\n        *   **Efficient Computation**: Matrix-vector multiplications for Householder transformations are optimized into vector operations, reducing time complexity from O(2nk^2) to O(2nk) for rotations and similar for projections.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of **Householder parameterization** for KGE, utilizing Householder reflections and projections.\n        *   **HousE-r**: A model for high-dimensional (k-dimensional) relational rotations based on compositions of Householder reflections, theoretically capable of modeling symmetry, antisymmetry, inversion, and composition.\n        *   **Householder Projections**: A novel type of invertible projection, derived from modified Householder matrices, designed to flexibly adjust distances and handle RMPs without sacrificing pattern modeling.\n        *   **HousE**: A unified framework combining Householder rotations and Householder projections to model all relation patterns and RMPs simultaneously.\n    *   **Theoretical Insights/Analysis**:\n        *   Proof that any k-dimensional rotation can be represented as `2 * floor(k/2)` Householder reflections (Theorem 3.1).\n        *   Theoretical claims demonstrating HousE's capability to model symmetry, antisymmetry, inversion, composition, and RMPs (Claims 3.2-3.5).\n        *   Analysis of the invertibility of Householder projections.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Link prediction tasks.\n    *   **Key Performance Metrics**: Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits@1 (H@1), Hits@3 (H@3), Hits@10 (H@10).\n    *   **Comparison Results**: HousE consistently achieves new state-of-the-art performance across five benchmark datasets, including WN18 and FB15k, outperforming strong baselines like TransE, DistMult, ComplEx, ConvE, RotatE, Rotate3D, and QuatE. For example, on WN18, HousE achieves MRR of 0.952 and H@10 of 0.962, surpassing RotatE (0.949 MRR, 0.959 H@10) and QuatE (0.949 MRR, 0.959 H@10). On FB15k, HousE achieves MRR of 0.801 and H@10 of 0.889, outperforming RotatE (0.797 MRR, 0.884 H@10).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   While HousE-r (pure Householder rotations) initially suffered from the limitation of not being able to model RMPs due to its distance-preserving nature, the full HousE framework explicitly addresses and overcomes this by integrating Householder projections.\n        *   The paper does not explicitly state new limitations of the *final* HousE model, but rather positions it as a comprehensive solution to previous limitations.\n    *   **Scope of Applicability**: Primarily focused on knowledge graph embedding for link prediction tasks. The framework's generalizability to other KG-related tasks (e.g., KG completion, entity classification) is implied but not explicitly demonstrated beyond link prediction.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HousE significantly advances the technical state-of-the-art in KGE by providing a powerful and general framework capable of simultaneously modeling all crucial relation patterns and RMPs, a challenge that previous models could only partially address. Its ability to perform high-dimensional rotations offers superior modeling capacity.\n    *   **Potential Impact on Future Research**:\n        *   Provides a new paradigm for KGE by leveraging Householder parameterization, potentially inspiring further research into geometric transformations for representation learning.\n        *   The generalization of rotation-based models to arbitrary high dimensions opens avenues for exploring optimal embedding dimensions for different KGs.\n        *   The concept of invertible projections for RMPs could be adapted to other domains requiring flexible distance adjustments while preserving structural properties.\n        *   The theoretical proofs and efficient computation methods contribute to a deeper understanding and practical application of advanced linear algebra in machine learning.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Householder Parameterization",
          "Relation Patterns",
          "Relation Mapping Properties (RMPs)",
          "High-dimensional Rotations",
          "Invertible Householder Projections",
          "Unified KGE Framework",
          "Link Prediction",
          "State-of-the-Art Performance",
          "Geometric Transformations",
          "Modeling Capacity",
          "Householder Reflections"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "63836e669416668744c3676a831060e8de3f58a1.pdf"
    },
    {
      "success": true,
      "doc_id": "d55f674d00aab2affacb0d680d48b134",
      "summary": "Here's a focused summary of the paper \"Multi-view Knowledge Graph Embedding for Entity Alignment\" by Zhang et al. \\cite{zhang2019} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the problem of embedding-based entity alignment between knowledge graphs (KGs) \\cite{zhang2019}.\n    *   **Importance & Challenge:**\n        *   Entity alignment is fundamental for KG construction, fusion, and supports downstream applications like semantic search and question answering \\cite{zhang2019}.\n        *   **Challenge 1 (Feature Exploitation):** Previous embedding-based methods primarily focus on relational structure, with some incorporating only one additional feature type (e.g., attributes). A vast array of entity features remains unexplored or not equally treated, which impairs the accuracy and robustness of alignment \\cite{zhang2019}.\n        *   **Challenge 2 (Seed Alignment Dependency):** Existing methods heavily rely on abundant, costly seed entity alignment for training. Furthermore, they often assume the easy availability of seed relation and attribute alignment, which is not always practical \\cite{zhang2019}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   Builds upon KG embedding techniques (e.g., TransE, DistMult, ConvE) but extends them from single-KG link prediction to cross-KG entity alignment \\cite{zhang2019}.\n        *   Relates to existing embedding-based entity alignment methods like MTransE, IPTransE, BootEA, and GCN-Align, which primarily use relational features \\cite{zhang2019}.\n        *   Compares to methods that incorporate additional features (e.g., JAPE, KDCoE, AttrE) but notes their limited scope \\cite{zhang2019}.\n        *   Leverages principles from multi-view representation learning, adapting them for KG embedding \\cite{zhang2019}.\n    *   **Limitations of Previous Solutions:**\n        *   Existing embedding-based entity alignment methods exploit only one or two types of entity features, failing to capture the full spectrum of entity characteristics and being \"incapable of incorporating new features\" \\cite{zhang2019}.\n        *   They are overly reliant on abundant and costly seed entity alignment, and often make unrealistic assumptions about the availability of seed relation and attribute alignment \\cite{zhang2019}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **MultiKE**, a novel framework that unifies multiple views of entities to learn comprehensive embeddings for entity alignment \\cite{zhang2019}.\n    *   **Novelty/Difference:**\n        *   **Multi-view Embedding:** MultiKE explicitly divides various KG features into complementary \"views\" (name, relation, attribute) and learns view-specific embeddings, which are then jointly optimized \\cite{zhang2019}. This is a significant departure from prior work that uses one or two features or treats additional features merely as refinements \\cite{zhang2019}.\n        *   **Cross-KG Inference:** It designs novel cross-KG inference methods at both the entity level and, innovatively, at the relation and attribute levels, to preserve and enhance alignment between KGs \\cite{zhang2019}.\n        *   **Soft Alignment for Relations/Attributes:** Unlike previous methods assuming pre-existing seed relation/attribute alignment, MultiKE introduces a \"soft alignment\" method that automatically finds and updates relation and attribute alignment during training, based on a weighted sum of name and semantic similarities \\cite{zhang2019}.\n        *   **Combination Strategies:** It explores and evaluates three distinct strategies for combining the multiple view-specific entity embeddings: Weighted View Averaging, Shared Space Learning, and In-training Combination \\cite{zhang2019}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   Formal definition of three representative entity views: name, relation, and attribute, each with a tailored embedding model (literal embedding with autoencoder for names, TransE for relations, CNN for attributes) \\cite{zhang2019}.\n        *   Cross-KG entity identity inference mechanism that maximizes auxiliary probabilities based on seed entity alignment for both relation and attribute views \\cite{zhang2019}.\n        *   A novel \"soft alignment\" method for relations and attributes, which dynamically identifies and incorporates alignment information during training, reducing reliance on pre-existing labels \\cite{zhang2019}.\n    *   **System Design or Architectural Innovations:**\n        *   A unified framework (MultiKE) that systematically integrates heterogeneous feature types (names, relations, attributes) into a coherent embedding learning process \\cite{zhang2019}.\n        *   Introduction of three distinct view combination strategies (Weighted View Averaging, Shared Space Learning, In-training Combination) to effectively merge view-specific embeddings into a comprehensive representation \\cite{zhang2019}.\n    *   **Theoretical Insights or Analysis:**\n        *   The insight that leveraging multiple, complementary views of entities significantly improves the accuracy and robustness of embedding-based entity alignment \\cite{zhang2019}.\n        *   The demonstration that automatic, \"soft\" inference of relation and attribute alignment during training can effectively enhance entity alignment and mitigate the problem of scarce seed alignment \\cite{zhang2019}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** MultiKE was evaluated on two real-world datasets for entity alignment \\cite{zhang2019}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The proposed framework \"significantly outperforms the state-of-the-art embedding-based entity alignment methods\" \\cite{zhang2019}.\n        *   MultiKE also achieved \"promising results on unsupervised entity alignment and is comparable to conventional entity alignment methods\" \\cite{zhang2019}.\n        *   Ablation studies confirmed that \"The selected views, cross-KG inference and combination strategies all contribute to the performance improvement\" \\cite{zhang2019}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions:**\n        *   The literal embedding component truncates long literals (max 5 tokens) and pads short ones \\cite{zhang2019}.\n        *   Negative sampling was not employed for the attribute view embedding, as it did not yield noticeable improvement \\cite{zhang2019}.\n        *   The \"soft alignment\" method relies on a similarity threshold `\\theta` for identifying relation/attribute alignment \\cite{zhang2019}.\n    *   **Scope of Applicability:** Primarily focused on entity alignment between KGs where entity names, relational structures, and attribute-value pairs are available. It is particularly beneficial in scenarios with limited seed entity alignment \\cite{zhang2019}.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** MultiKE significantly advances the state-of-the-art in embedding-based entity alignment by providing a robust framework that effectively integrates and jointly optimizes heterogeneous entity features from multiple views, outperforming prior methods \\cite{zhang2019}.\n    *   **Potential Impact on Future Research:**\n        *   Offers a flexible paradigm for incorporating diverse entity features, paving the way for more comprehensive and accurate KG embedding models.\n        *   The \"soft alignment\" mechanism for relations and attributes provides a valuable technique for reducing reliance on costly manual annotations, which could be extended to other alignment tasks.\n        *   Its success in unsupervised entity alignment suggests avenues for developing more autonomous KG integration systems.\n        *   The multi-view approach could inspire similar strategies for other KG-related tasks beyond entity alignment \\cite{zhang2019}.",
      "intriguing_abstract": "Bridging disparate Knowledge Graphs (KGs) through accurate **entity alignment** is fundamental for robust data integration and downstream AI applications. Existing **Knowledge Graph Embedding** (KGE) methods, however, are severely limited by their narrow exploitation of entity features and heavy reliance on costly seed alignments. We introduce **MultiKE**, a novel framework that revolutionizes KGE-based entity alignment by comprehensively leveraging **multiple views** of entities: name, **relational structure**, and **attributes**.\n\nMultiKE uniquely learns view-specific embeddings, employing tailored models like **literal embedding** for names and TransE for relations, which are then synergistically combined. A key innovation is our **soft alignment** mechanism, which dynamically infers and updates **relation and attribute alignment** during training, drastically reducing the need for pre-existing labels. Coupled with novel **cross-KG inference** methods, MultiKE achieves significantly superior performance over state-of-the-art methods, even demonstrating promising results in **unsupervised entity alignment**. This flexible, multi-view paradigm offers a powerful solution for robust KG integration, paving the way for more autonomous and comprehensive knowledge systems.",
      "keywords": [
        "Knowledge Graph Embedding",
        "Entity Alignment",
        "MultiKE Framework",
        "Multi-view Embedding",
        "Cross-KG Inference",
        "Soft Alignment (Relations/Attributes)",
        "Heterogeneous Entity Features",
        "View Combination Strategies",
        "Seed Alignment Dependency Mitigation",
        "Unsupervised Entity Alignment",
        "State-of-the-art Performance",
        "Multi-view Representation Learning",
        "Tailored Embedding Models"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/11e402c699bcb54d57da1a5fdbc57076d7255baf.pdf",
      "citation_key": "zhang2019",
      "metadata": {
        "title": "Multi-view Knowledge Graph Embedding for Entity Alignment",
        "authors": [
          "Qingheng Zhang",
          "Zequn Sun",
          "Wei Hu",
          "Muhao Chen",
          "Lingbing Guo",
          "Yuzhong Qu"
        ],
        "published_date": "2019",
        "abstract": "We study the problem of embedding-based entity alignment between knowledge graphs (KGs). Previous works mainly focus on the relational structure of entities. Some further incorporate another type of features, such as attributes, for refinement. However, a vast of entity features are still unexplored or not equally treated together, which impairs the accuracy and robustness of embedding-based entity alignment. In this paper, we propose a novel framework that unifies multiple views of entities to learn embeddings for entity alignment. Specifically, we embed entities based on the views of entity names, relations and attributes, with several combination strategies. Furthermore, we design some cross-KG inference methods to enhance the alignment between two KGs. Our experiments on real-world datasets show that the proposed framework significantly outperforms the state-of-the-art embedding-based entity alignment methods. The selected views, cross-KG inference and combination strategies all contribute to the performance improvement.",
        "file_path": "paper_data/knowledge_graph_embedding/11e402c699bcb54d57da1a5fdbc57076d7255baf.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Multi-view Knowledge Graph Embedding for Entity Alignment\" by Zhang et al. \\cite{zhang2019} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the problem of embedding-based entity alignment between knowledge graphs (KGs) \\cite{zhang2019}.\n    *   **Importance & Challenge:**\n        *   Entity alignment is fundamental for KG construction, fusion, and supports downstream applications like semantic search and question answering \\cite{zhang2019}.\n        *   **Challenge 1 (Feature Exploitation):** Previous embedding-based methods primarily focus on relational structure, with some incorporating only one additional feature type (e.g., attributes). A vast array of entity features remains unexplored or not equally treated, which impairs the accuracy and robustness of alignment \\cite{zhang2019}.\n        *   **Challenge 2 (Seed Alignment Dependency):** Existing methods heavily rely on abundant, costly seed entity alignment for training. Furthermore, they often assume the easy availability of seed relation and attribute alignment, which is not always practical \\cite{zhang2019}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   Builds upon KG embedding techniques (e.g., TransE, DistMult, ConvE) but extends them from single-KG link prediction to cross-KG entity alignment \\cite{zhang2019}.\n        *   Relates to existing embedding-based entity alignment methods like MTransE, IPTransE, BootEA, and GCN-Align, which primarily use relational features \\cite{zhang2019}.\n        *   Compares to methods that incorporate additional features (e.g., JAPE, KDCoE, AttrE) but notes their limited scope \\cite{zhang2019}.\n        *   Leverages principles from multi-view representation learning, adapting them for KG embedding \\cite{zhang2019}.\n    *   **Limitations of Previous Solutions:**\n        *   Existing embedding-based entity alignment methods exploit only one or two types of entity features, failing to capture the full spectrum of entity characteristics and being \"incapable of incorporating new features\" \\cite{zhang2019}.\n        *   They are overly reliant on abundant and costly seed entity alignment, and often make unrealistic assumptions about the availability of seed relation and attribute alignment \\cite{zhang2019}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **MultiKE**, a novel framework that unifies multiple views of entities to learn comprehensive embeddings for entity alignment \\cite{zhang2019}.\n    *   **Novelty/Difference:**\n        *   **Multi-view Embedding:** MultiKE explicitly divides various KG features into complementary \"views\" (name, relation, attribute) and learns view-specific embeddings, which are then jointly optimized \\cite{zhang2019}. This is a significant departure from prior work that uses one or two features or treats additional features merely as refinements \\cite{zhang2019}.\n        *   **Cross-KG Inference:** It designs novel cross-KG inference methods at both the entity level and, innovatively, at the relation and attribute levels, to preserve and enhance alignment between KGs \\cite{zhang2019}.\n        *   **Soft Alignment for Relations/Attributes:** Unlike previous methods assuming pre-existing seed relation/attribute alignment, MultiKE introduces a \"soft alignment\" method that automatically finds and updates relation and attribute alignment during training, based on a weighted sum of name and semantic similarities \\cite{zhang2019}.\n        *   **Combination Strategies:** It explores and evaluates three distinct strategies for combining the multiple view-specific entity embeddings: Weighted View Averaging, Shared Space Learning, and In-training Combination \\cite{zhang2019}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   Formal definition of three representative entity views: name, relation, and attribute, each with a tailored embedding model (literal embedding with autoencoder for names, TransE for relations, CNN for attributes) \\cite{zhang2019}.\n        *   Cross-KG entity identity inference mechanism that maximizes auxiliary probabilities based on seed entity alignment for both relation and attribute views \\cite{zhang2019}.\n        *   A novel \"soft alignment\" method for relations and attributes, which dynamically identifies and incorporates alignment information during training, reducing reliance on pre-existing labels \\cite{zhang2019}.\n    *   **System Design or Architectural Innovations:**\n        *   A unified framework (MultiKE) that systematically integrates heterogeneous feature types (names, relations, attributes) into a coherent embedding learning process \\cite{zhang2019}.\n        *   Introduction of three distinct view combination strategies (Weighted View Averaging, Shared Space Learning, In-training Combination) to effectively merge view-specific embeddings into a comprehensive representation \\cite{zhang2019}.\n    *   **Theoretical Insights or Analysis:**\n        *   The insight that leveraging multiple, complementary views of entities significantly improves the accuracy and robustness of embedding-based entity alignment \\cite{zhang2019}.\n        *   The demonstration that automatic, \"soft\" inference of relation and attribute alignment during training can effectively enhance entity alignment and mitigate the problem of scarce seed alignment \\cite{zhang2019}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** MultiKE was evaluated on two real-world datasets for entity alignment \\cite{zhang2019}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The proposed framework \"significantly outperforms the state-of-the-art embedding-based entity alignment methods\" \\cite{zhang2019}.\n        *   MultiKE also achieved \"promising results on unsupervised entity alignment and is comparable to conventional entity alignment methods\" \\cite{zhang2019}.\n        *   Ablation studies confirmed that \"The selected views, cross-KG inference and combination strategies all contribute to the performance improvement\" \\cite{zhang2019}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions:**\n        *   The literal embedding component truncates long literals (max 5 tokens) and pads short ones \\cite{zhang2019}.\n        *   Negative sampling was not employed for the attribute view embedding, as it did not yield noticeable improvement \\cite{zhang2019}.\n        *   The \"soft alignment\" method relies on a similarity threshold `\\theta` for identifying relation/attribute alignment \\cite{zhang2019}.\n    *   **Scope of Applicability:** Primarily focused on entity alignment between KGs where entity names, relational structures, and attribute-value pairs are available. It is particularly beneficial in scenarios with limited seed entity alignment \\cite{zhang2019}.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art:** MultiKE significantly advances the state-of-the-art in embedding-based entity alignment by providing a robust framework that effectively integrates and jointly optimizes heterogeneous entity features from multiple views, outperforming prior methods \\cite{zhang2019}.\n    *   **Potential Impact on Future Research:**\n        *   Offers a flexible paradigm for incorporating diverse entity features, paving the way for more comprehensive and accurate KG embedding models.\n        *   The \"soft alignment\" mechanism for relations and attributes provides a valuable technique for reducing reliance on costly manual annotations, which could be extended to other alignment tasks.\n        *   Its success in unsupervised entity alignment suggests avenues for developing more autonomous KG integration systems.\n        *   The multi-view approach could inspire similar strategies for other KG-related tasks beyond entity alignment \\cite{zhang2019}.",
        "keywords": [
          "Knowledge Graph Embedding",
          "Entity Alignment",
          "MultiKE Framework",
          "Multi-view Embedding",
          "Cross-KG Inference",
          "Soft Alignment (Relations/Attributes)",
          "Heterogeneous Entity Features",
          "View Combination Strategies",
          "Seed Alignment Dependency Mitigation",
          "Unsupervised Entity Alignment",
          "State-of-the-art Performance",
          "Multi-view Representation Learning",
          "Tailored Embedding Models"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "11e402c699bcb54d57da1a5fdbc57076d7255baf.pdf"
    },
    {
      "success": true,
      "doc_id": "41cc4d96600573bd587623489f2d66a1",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Knowledge graphs (KGs) are inherently incomplete, making the task of knowledge graph reasoning (predicting missing facts) challenging.\n    *   **Importance & Challenge**: Existing approaches, Knowledge Graph Embedding (KGE) methods and rule-based KG reasoning, each have significant limitations. KGE is efficient and robust but lacks explicit first-order logic and interpretability. Rule-based methods offer interpretability and generalization but suffer from \"brittleness\" due to their absolute inference nature, even when rules have exceptions \\cite{tang2022}. The challenge is to integrate these two complementary paradigms in a principled manner to leverage their strengths while mitigating their weaknesses.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous efforts to combine logical rules with KGE typically involve using rules to infer new facts for KGE training data or converting rules into regularization terms for specific KGE models \\cite{tang2022}.\n    *   **Limitations of Previous Solutions**: These methods primarily enhance KGE training without explicitly using logical rules for reasoning, potentially losing important information from explicit rules and leading to suboptimal performance \\cite{tang2022}.\n    *   **RulE's Positioning**: RulE distinguishes itself by learning explicit *rule embeddings* and jointly representing entities, relations, and logical rules in a unified embedding space. This allows for soft rule inference and mutual regularization between KGE and rule-based components, addressing the limitations of prior integration strategies \\cite{tang2022}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: RulE (Rule Embedding) is a principled framework that learns rule embeddings by jointly representing entities, relations, and logical rules in a unified continuous space. It consists of three key components: Joint entity/relation/rule embedding, Soft rule reasoning, and Inference \\cite{tang2022}.\n    *   **Novelty**:\n        *   **Joint Embedding**: RulE extends traditional KGE (using RotatE as a base) by additionally modeling the relationship between *relations and logical rules*. For a rule `R: r1  ...  rl  rl+1`, it defines a distance function `dr(r1, ..., rl+1, R) = ||  g(ri) + g(R) - g(rl+1) ||`, where `g(r)` represents the angle vector of relation `r` in a complex space. This allows for joint optimization of entity, relation, and rule embeddings \\cite{tang2022}.\n        *   **Soft Rule Reasoning**: RulE calculates a *confidence score* `wi` for each logical rule `Ri` based on its learned embeddings, reflecting its consistency with observed triplets. To predict a triplet, it constructs a \"soft multi-hot encoding\" `v` where `vi` is the product of `wi` and the number of grounding paths activating `Ri`. An MLP then processes `v` to output a grounding rule score `sg(h,r,t) = MLP(v)`, enabling soft, context-dependent inference and alleviating the brittleness of logic \\cite{tang2022}.\n        *   **Unified Inference**: The final prediction score `s(h,r,t)` is a weighted sum of the KGE score `st` and the grounding rule score `sg`, balancing embedding-based and rule-based reasoning \\cite{tang2022}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: A novel framework for neural-symbolic KG reasoning that jointly learns embeddings for entities, relations, and *first-order logical rules* in a unified continuous space \\cite{tang2022}.\n    *   **Novel Algorithms/Methods**: A method to model logical rules as multi-step rotations/summations in a complex embedding space, allowing for the embedding of rules themselves \\cite{tang2022}.\n    *   **Novel Algorithms/Methods**: Introduction of rule confidence scores, derived from rule embeddings, to quantify the plausibility and consistency of logical rules with observed facts \\cite{tang2022}.\n    *   **Novel Algorithms/Methods**: A soft rule reasoning mechanism that uses an MLP to aggregate rule confidences and grounding path counts, effectively addressing the brittleness of traditional logical inference and modeling interdependencies among rules \\cite{tang2022}.\n    *   **System Design/Architectural Innovations**: A unified architecture where KGE and rule-based reasoning components mutually regularize and enhance each other during joint training \\cite{tang2022}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on benchmark link prediction tasks, along with ablation studies to verify the effectiveness of each RulE component \\cite{tang2022}.\n    *   **Datasets**: Evaluated on six benchmark KGs: FB15k-237, WN18RR, YAGO3-10, UMLS, Kinship, and Family \\cite{tang2022}.\n    *   **Key Performance Metrics**: Mean Reciprocal Rank (MRR) and Hits@k (H@1, H@3, H@10) \\cite{tang2022}.\n    *   **Comparison Results**: RulE consistently outperforms the majority of existing embedding-based (e.g., TransE, RotatE, TuckER) and rule-based (e.g., Neural-LP, DRUM, pLogicNet) methods across multiple benchmarks \\cite{tang2022}. Ablation studies confirmed the individual contributions and effectiveness of RulE's components, demonstrating that the joint embedding itself boosts KGE performance \\cite{tang2022}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The framework relies on pre-extracted logical rules. While a position-aware variant for rule modeling is mentioned, the default approach simplifies relation order. The efficiency of grounding path computation via BFS is stated but not detailed in the provided text \\cite{tang2022}.\n    *   **Scope of Applicability**: RulE is primarily designed for link prediction in KGs using first-order logical rules. Its KGE component is flexible and can integrate with various KGE models beyond RotatE \\cite{tang2022}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: RulE significantly advances the technical state-of-the-art in KG reasoning by providing a novel, principled, and empirically effective neural-symbolic framework that surpasses many existing embedding-based and rule-based methods \\cite{tang2022}.\n    *   **Potential Impact on Future Research**: It offers a robust paradigm for integrating symbolic knowledge (logical rules) with neural representations (embeddings), paving the way for more interpretable, generalizable, and accurate KG reasoning systems. The concept of learning explicit rule embeddings and soft rule inference can inspire future research in neural-symbolic AI and knowledge representation \\cite{tang2022}.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are powerful knowledge representations, yet their inherent incompleteness severely hinders effective knowledge graph reasoning. Current approaches, from efficient but uninterpretable Knowledge Graph Embeddings (KGE) to brittle rule-based systems, fall short. We introduce RulE, a novel neural-symbolic framework that masterfully integrates these paradigms, offering a principled solution to leverage their strengths.\n\nRulE uniquely learns explicit *rule embeddings* alongside entities and relations within a unified continuous space, modeling first-order logical rules as multi-step rotations. This innovation enables the calculation of dynamic *rule confidence scores* and facilitates a sophisticated *soft rule reasoning* mechanism. By aggregating these confidences with grounding paths via an MLP, RulE overcomes the brittleness of traditional logical inference, providing context-dependent and robust predictions. Our unified architecture ensures mutual regularization between KGE and rule-based components, enhancing overall performance. Extensive experiments on six benchmark datasets demonstrate RulE consistently outperforms state-of-the-art KGE and rule-based methods in link prediction, delivering a more interpretable, generalizable, and accurate approach to KG reasoning and paving the way for robust neural-symbolic AI.",
      "keywords": [
        "Knowledge Graph Reasoning",
        "Knowledge Graph Embeddings (KGE)",
        "Rule-based Reasoning",
        "Neural-Symbolic AI",
        "Rule Embeddings",
        "Joint Embedding",
        "Soft Rule Reasoning",
        "First-Order Logical Rules",
        "Rule Confidence Scores",
        "Unified Embedding Space",
        "Link Prediction",
        "Mutual Regularization",
        "Interpretability",
        "Brittleness Alleviation",
        "State-of-the-Art Performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/191815e4109ee392b9120b61642c0e859fb662a1.pdf",
      "citation_key": "tang2022",
      "metadata": {
        "title": "RulE: Knowledge Graph Reasoning with Rule Embedding",
        "authors": [
          "Xiaojuan Tang",
          "Song-Chun Zhu",
          "Yitao Liang",
          "Muhan Zhang"
        ],
        "published_date": "2022",
        "abstract": "Knowledge graph (KG) reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \\textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \\textbf{entities}, \\textbf{relations} and \\textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct extensive experiments to verify each component of RulE. Results on multiple benchmarks reveal that our model outperforms the majority of existing embedding-based and rule-based approaches.",
        "file_path": "paper_data/knowledge_graph_embedding/191815e4109ee392b9120b61642c0e859fb662a1.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Knowledge graphs (KGs) are inherently incomplete, making the task of knowledge graph reasoning (predicting missing facts) challenging.\n    *   **Importance & Challenge**: Existing approaches, Knowledge Graph Embedding (KGE) methods and rule-based KG reasoning, each have significant limitations. KGE is efficient and robust but lacks explicit first-order logic and interpretability. Rule-based methods offer interpretability and generalization but suffer from \"brittleness\" due to their absolute inference nature, even when rules have exceptions \\cite{tang2022}. The challenge is to integrate these two complementary paradigms in a principled manner to leverage their strengths while mitigating their weaknesses.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous efforts to combine logical rules with KGE typically involve using rules to infer new facts for KGE training data or converting rules into regularization terms for specific KGE models \\cite{tang2022}.\n    *   **Limitations of Previous Solutions**: These methods primarily enhance KGE training without explicitly using logical rules for reasoning, potentially losing important information from explicit rules and leading to suboptimal performance \\cite{tang2022}.\n    *   **RulE's Positioning**: RulE distinguishes itself by learning explicit *rule embeddings* and jointly representing entities, relations, and logical rules in a unified embedding space. This allows for soft rule inference and mutual regularization between KGE and rule-based components, addressing the limitations of prior integration strategies \\cite{tang2022}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: RulE (Rule Embedding) is a principled framework that learns rule embeddings by jointly representing entities, relations, and logical rules in a unified continuous space. It consists of three key components: Joint entity/relation/rule embedding, Soft rule reasoning, and Inference \\cite{tang2022}.\n    *   **Novelty**:\n        *   **Joint Embedding**: RulE extends traditional KGE (using RotatE as a base) by additionally modeling the relationship between *relations and logical rules*. For a rule `R: r1  ...  rl  rl+1`, it defines a distance function `dr(r1, ..., rl+1, R) = ||  g(ri) + g(R) - g(rl+1) ||`, where `g(r)` represents the angle vector of relation `r` in a complex space. This allows for joint optimization of entity, relation, and rule embeddings \\cite{tang2022}.\n        *   **Soft Rule Reasoning**: RulE calculates a *confidence score* `wi` for each logical rule `Ri` based on its learned embeddings, reflecting its consistency with observed triplets. To predict a triplet, it constructs a \"soft multi-hot encoding\" `v` where `vi` is the product of `wi` and the number of grounding paths activating `Ri`. An MLP then processes `v` to output a grounding rule score `sg(h,r,t) = MLP(v)`, enabling soft, context-dependent inference and alleviating the brittleness of logic \\cite{tang2022}.\n        *   **Unified Inference**: The final prediction score `s(h,r,t)` is a weighted sum of the KGE score `st` and the grounding rule score `sg`, balancing embedding-based and rule-based reasoning \\cite{tang2022}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: A novel framework for neural-symbolic KG reasoning that jointly learns embeddings for entities, relations, and *first-order logical rules* in a unified continuous space \\cite{tang2022}.\n    *   **Novel Algorithms/Methods**: A method to model logical rules as multi-step rotations/summations in a complex embedding space, allowing for the embedding of rules themselves \\cite{tang2022}.\n    *   **Novel Algorithms/Methods**: Introduction of rule confidence scores, derived from rule embeddings, to quantify the plausibility and consistency of logical rules with observed facts \\cite{tang2022}.\n    *   **Novel Algorithms/Methods**: A soft rule reasoning mechanism that uses an MLP to aggregate rule confidences and grounding path counts, effectively addressing the brittleness of traditional logical inference and modeling interdependencies among rules \\cite{tang2022}.\n    *   **System Design/Architectural Innovations**: A unified architecture where KGE and rule-based reasoning components mutually regularize and enhance each other during joint training \\cite{tang2022}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted on benchmark link prediction tasks, along with ablation studies to verify the effectiveness of each RulE component \\cite{tang2022}.\n    *   **Datasets**: Evaluated on six benchmark KGs: FB15k-237, WN18RR, YAGO3-10, UMLS, Kinship, and Family \\cite{tang2022}.\n    *   **Key Performance Metrics**: Mean Reciprocal Rank (MRR) and Hits@k (H@1, H@3, H@10) \\cite{tang2022}.\n    *   **Comparison Results**: RulE consistently outperforms the majority of existing embedding-based (e.g., TransE, RotatE, TuckER) and rule-based (e.g., Neural-LP, DRUM, pLogicNet) methods across multiple benchmarks \\cite{tang2022}. Ablation studies confirmed the individual contributions and effectiveness of RulE's components, demonstrating that the joint embedding itself boosts KGE performance \\cite{tang2022}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The framework relies on pre-extracted logical rules. While a position-aware variant for rule modeling is mentioned, the default approach simplifies relation order. The efficiency of grounding path computation via BFS is stated but not detailed in the provided text \\cite{tang2022}.\n    *   **Scope of Applicability**: RulE is primarily designed for link prediction in KGs using first-order logical rules. Its KGE component is flexible and can integrate with various KGE models beyond RotatE \\cite{tang2022}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: RulE significantly advances the technical state-of-the-art in KG reasoning by providing a novel, principled, and empirically effective neural-symbolic framework that surpasses many existing embedding-based and rule-based methods \\cite{tang2022}.\n    *   **Potential Impact on Future Research**: It offers a robust paradigm for integrating symbolic knowledge (logical rules) with neural representations (embeddings), paving the way for more interpretable, generalizable, and accurate KG reasoning systems. The concept of learning explicit rule embeddings and soft rule inference can inspire future research in neural-symbolic AI and knowledge representation \\cite{tang2022}.",
        "keywords": [
          "Knowledge Graph Reasoning",
          "Knowledge Graph Embeddings (KGE)",
          "Rule-based Reasoning",
          "Neural-Symbolic AI",
          "Rule Embeddings",
          "Joint Embedding",
          "Soft Rule Reasoning",
          "First-Order Logical Rules",
          "Rule Confidence Scores",
          "Unified Embedding Space",
          "Link Prediction",
          "Mutual Regularization",
          "Interpretability",
          "Brittleness Alleviation",
          "State-of-the-Art Performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "191815e4109ee392b9120b61642c0e859fb662a1.pdf"
    },
    {
      "success": true,
      "doc_id": "05865c50d3ddbcddaef8fca17fed9ad6",
      "summary": "Here's a focused summary of the paper \"Differentiating Concepts and Instances for Knowledge Graph Embedding\" by Lv et al. \\cite{lv2018} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most conventional knowledge graph embedding (KGE) methods encode all entities (both concepts and instances) and relations as vectors in a low-dimensional semantic space, treating them equally. This ignores the fundamental difference between concepts (representing groups of instances with common properties) and instances (specific entities).\n    *   **Importance and Challenge**:\n        *   Concepts are essential for knowledge representation, providing categorization and hierarchical organization (e.g., in YAGO, Freebase, WordNet).\n        *   Ignoring this distinction leads to **insufficient concept representation**, as a simple vector cannot fully capture the nature of a concept as a category encompassing many instances.\n        *   It also results in a **lack of transitivity for `isA` relations** (`instanceOf` and `subClassOf`). These relations inherently exhibit transitivity (e.g., if \"Alice\" `instanceOf` \"AcademicStaffMember\" and \"AcademicStaffMember\" `subClassOf` \"StaffMember\", then \"Alice\" `instanceOf` \"StaffMember\"). Previous indiscriminate vector representations fail to preserve this crucial property.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon and contrasts with various KGE models:\n        *   **Translation-based models**: TransE, TransH, TransR/CTransR, TransD, and their extensions (e.g., TranSparse, PTransE, ManifoldE, TransF, TransG, KG2E).\n        *   **Bilinear models**: RESCAL, DistMult, HolE, ComplEx.\n        *   **External information learning models**: TEKE, DKRL, and models using logical rules.\n    *   **Limitations of Previous Solutions**: `\\cite{lv2018}` highlights that \"All these methods ignore to distinguish between concepts and instances, and regard both as entities to make a simplication.\" This simplification leads directly to the problems of insufficient concept representation and failure to capture `isA` transitivity.\n    *   **Positioning**: `\\cite{lv2018}` claims to be \"the rst to propose and formalize the problem of knowledge graph embedding which differentiates between concepts and instances.\"\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{lv2018}` proposes **TransC (Translating Concepts)**, a novel knowledge graph embedding model that differentiates concepts and instances.\n        *   Each **concept** is encoded as a **sphere** (defined by a center vector `p` and a radius `m`) in a low-dimensional semantic space.\n        *   Each **instance** is encoded as a **vector** in the same semantic space.\n        *   **Instance relations** (non-`isA` relations) are encoded as **vectors**, similar to translation-based models like TransE.\n        *   **Relative positions** in the embedding space are used to model `isA` relations.\n    *   **Novelty/Difference**:\n        *   **Differentiated Geometric Representation**: The core innovation is the distinct geometric representation for concepts (spheres) and instances (vectors), allowing for a more nuanced semantic encoding than uniform vector representations.\n        *   **Modeling `instanceOf`**: An `instanceOf` relation `(instance, instanceOf, concept)` is modeled by requiring the instance vector to be *inside* the concept sphere. A loss function `fe(i,c) = ||i - p||^2 - m` is defined, penalizing instances outside their concept spheres.\n        *   **Modeling `subClassOf`**: A `subClassOf` relation `(sub-concept, subClassOf, super-concept)` is modeled by requiring the sub-concept sphere to be *inside* the super-concept sphere. `\\cite{lv2018}` enumerates four possible relative positions between two spheres and defines specific loss functions for each, guiding the optimization towards the desired \"sub-sphere inside super-sphere\" configuration.\n        *   **Inherent Transitivity**: This geometric modeling naturally preserves `instanceOf`-`subClassOf` transitivity (if an instance is inside a sub-concept sphere, and that sub-concept sphere is inside a super-concept sphere, the instance is also inside the super-concept sphere) and `subClassOf`-`subClassOf` transitivity.\n        *   **Joint Learning Framework**: The specialized loss functions for `isA` relations are integrated with a standard translation-based loss (like TransE's `||h + r - t||^2`) for other relational triples into a unified margin-based ranking loss function for joint optimization.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The TransC model itself, which introduces a novel geometric approach to represent concepts (spheres) and instances (vectors) in KGE.\n        *   Specific loss functions designed to capture the semantic meaning and transitivity of `instanceOf` and `subClassOf` relations through spatial containment.\n    *   **Theoretical Insights/Analysis**: Demonstrates how the chosen geometric representations (instance vector inside concept sphere, sub-concept sphere inside super-concept sphere) inherently and elegantly capture the transitivity properties of `isA` relations, which is a significant improvement over previous models.\n    *   **System Design/Architectural Innovations**: Proposes a comprehensive framework that seamlessly integrates these differentiated representations and specialized loss functions with existing translation-based models for general relations, allowing for joint learning across all types of triples in a knowledge graph.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: `\\cite{lv2018}` evaluates TransC on two standard KGE tasks:\n        *   **Link Prediction**: Predicting missing head or tail entities in a triple.\n        *   **Triple Classification**: Classifying whether a given triple is true or false.\n    *   **Datasets**:\n        *   **YAGO39K**: A custom subset of YAGO, specifically constructed by `\\cite{lv2018}` because common datasets like FB15K (instance-heavy) and WN18 (concept-heavy) are not suitable for evaluating models that differentiate concepts and instances.\n        *   **M-YAGO39K**: An augmented version of YAGO39K, created by adding triples inferred through `isA` transitivity, specifically designed to test the model's ability to handle transitivity.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Link Prediction**: Evaluated using Mean Reciprocal Rank (MRR) and Hits@N (N=1, 3, 10), with both \"Raw\" and \"Filter\" settings.\n        *   **Triple Classification**: Evaluated using Accuracy, Precision, Recall, and F1-Score.\n        *   **Results**: TransC consistently outperforms several state-of-the-art baseline methods (TransE, TransH, TransR, TransD, HolE, DistMult, ComplEx) across most metrics on YAGO39K. Notably, TransC shows significant improvements in Hits@N for link prediction. The performance on M-YAGO39K implicitly validates its ability to capture `isA` transitivity.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The model's effectiveness relies on the assumption that concepts can be adequately represented as spheres and instances as points within those spheres. While effective for `isA` relations, this geometric choice might have limitations for more complex conceptual relationships or non-hierarchical structures. The paper does not explicitly discuss limitations of the TransC model itself, but rather focuses on addressing the limitations of prior work.\n    *   **Scope of Applicability**: TransC is particularly well-suited for knowledge graphs that contain a rich hierarchy of concepts and instances, and where `isA` relations are prominent and semantically important (e.g., YAGO, Freebase, WordNet). Its benefits might be less pronounced in KGs that are primarily flat or instance-centric, or where `isA` relations are not a major component.\n\n7.  **Technical Significance**\n    *   **Advances the Technical State-of-the-Art**: `\\cite{lv2018}` makes a significant advancement by introducing the first KGE model that explicitly differentiates between concepts and instances. It provides a novel and effective geometric approach to represent and reason about hierarchical `isA` relations, successfully addressing the long-standing challenge of capturing their transitivity within embedding spaces.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research into more sophisticated and semantically rich geometric or topological representations for different types of entities in KGE.\n        *   Encourages further exploration of how to explicitly model and leverage hierarchical and taxonomic structures within knowledge graphs, moving beyond uniform entity representations.\n        *   Could inspire new methods for direct reasoning and inference over conceptual hierarchies within learned embedding spaces.\n        *   The introduced YAGO39K and M-YAGO39K datasets provide valuable benchmarks for future work focusing on concept-instance differentiation and transitivity.",
      "intriguing_abstract": "Despite their ubiquity, most Knowledge Graph Embedding (KGE) models overlook a fundamental distinction: treating all entitieswhether abstract **concepts** or specific **instances**uniformly as vectors in a low-dimensional **semantic space**. This critical oversight leads to insufficient concept representation and a pervasive failure to capture the inherent **transitivity** of **`isA` relations** (`instanceOf`, `subClassOf`).\n\nWe introduce **TransC (Translating Concepts)**, a pioneering KGE model that explicitly differentiates concepts and instances through a novel **geometric representation**. TransC encodes **concepts as spheres** and **instances as vectors**, allowing for a richer semantic encoding. We elegantly model `instanceOf` by requiring instance vectors to reside within concept spheres, and `subClassOf` by nesting sub-concept spheres within super-concept spheres. This unique spatial containment inherently preserves `isA` transitivity, a challenge unmet by prior work. Integrated into a joint learning framework, TransC significantly outperforms state-of-the-art baselines on **link prediction** and **triple classification** tasks, particularly on our newly constructed YAGO39K and M-YAGO39K datasets. TransC offers a powerful paradigm shift, opening new avenues for semantically richer knowledge graph reasoning and hierarchical representation.",
      "keywords": [
        "Knowledge graph embedding (KGE)",
        "Concepts and instances differentiation",
        "`isA` relations transitivity",
        "TransC model",
        "Sphere-based concept representation",
        "Geometric entity representation",
        "Joint learning framework",
        "Link prediction",
        "Triple classification",
        "YAGO39K dataset",
        "Spatial containment modeling",
        "Hierarchical knowledge representation"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/d3c287ff061f295ddf8dc3cb02a6f39e301cae3b.pdf",
      "citation_key": "lv2018",
      "metadata": {
        "title": "Differentiating Concepts and Instances for Knowledge Graph Embedding",
        "authors": [
          "Xin Lv",
          "Lei Hou",
          "Juan-Zi Li",
          "Zhiyuan Liu"
        ],
        "published_date": "2018",
        "abstract": "Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. Most conventional knowledge embedding methods encode both entities (concepts and instances) and relations as vectors in a low dimensional semantic space equally, ignoring the difference between concepts and instances. In this paper, we propose a novel knowledge graph embedding model named TransC by differentiating concepts and instances. Specifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. We use the relative positions to model the relations between concepts and instances (i.e.,instanceOf), and the relations between concepts and sub-concepts (i.e., subClassOf). We evaluate our model on both link prediction and triple classification tasks on the dataset based on YAGO. Experimental results show that TransC outperforms state-of-the-art methods, and captures the semantic transitivity for instanceOf and subClassOf relation. Our codes and datasets can be obtained from https://github.com/davidlvxin/TransC.",
        "file_path": "paper_data/knowledge_graph_embedding/d3c287ff061f295ddf8dc3cb02a6f39e301cae3b.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Differentiating Concepts and Instances for Knowledge Graph Embedding\" by Lv et al. \\cite{lv2018} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most conventional knowledge graph embedding (KGE) methods encode all entities (both concepts and instances) and relations as vectors in a low-dimensional semantic space, treating them equally. This ignores the fundamental difference between concepts (representing groups of instances with common properties) and instances (specific entities).\n    *   **Importance and Challenge**:\n        *   Concepts are essential for knowledge representation, providing categorization and hierarchical organization (e.g., in YAGO, Freebase, WordNet).\n        *   Ignoring this distinction leads to **insufficient concept representation**, as a simple vector cannot fully capture the nature of a concept as a category encompassing many instances.\n        *   It also results in a **lack of transitivity for `isA` relations** (`instanceOf` and `subClassOf`). These relations inherently exhibit transitivity (e.g., if \"Alice\" `instanceOf` \"AcademicStaffMember\" and \"AcademicStaffMember\" `subClassOf` \"StaffMember\", then \"Alice\" `instanceOf` \"StaffMember\"). Previous indiscriminate vector representations fail to preserve this crucial property.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon and contrasts with various KGE models:\n        *   **Translation-based models**: TransE, TransH, TransR/CTransR, TransD, and their extensions (e.g., TranSparse, PTransE, ManifoldE, TransF, TransG, KG2E).\n        *   **Bilinear models**: RESCAL, DistMult, HolE, ComplEx.\n        *   **External information learning models**: TEKE, DKRL, and models using logical rules.\n    *   **Limitations of Previous Solutions**: `\\cite{lv2018}` highlights that \"All these methods ignore to distinguish between concepts and instances, and regard both as entities to make a simplication.\" This simplification leads directly to the problems of insufficient concept representation and failure to capture `isA` transitivity.\n    *   **Positioning**: `\\cite{lv2018}` claims to be \"the rst to propose and formalize the problem of knowledge graph embedding which differentiates between concepts and instances.\"\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{lv2018}` proposes **TransC (Translating Concepts)**, a novel knowledge graph embedding model that differentiates concepts and instances.\n        *   Each **concept** is encoded as a **sphere** (defined by a center vector `p` and a radius `m`) in a low-dimensional semantic space.\n        *   Each **instance** is encoded as a **vector** in the same semantic space.\n        *   **Instance relations** (non-`isA` relations) are encoded as **vectors**, similar to translation-based models like TransE.\n        *   **Relative positions** in the embedding space are used to model `isA` relations.\n    *   **Novelty/Difference**:\n        *   **Differentiated Geometric Representation**: The core innovation is the distinct geometric representation for concepts (spheres) and instances (vectors), allowing for a more nuanced semantic encoding than uniform vector representations.\n        *   **Modeling `instanceOf`**: An `instanceOf` relation `(instance, instanceOf, concept)` is modeled by requiring the instance vector to be *inside* the concept sphere. A loss function `fe(i,c) = ||i - p||^2 - m` is defined, penalizing instances outside their concept spheres.\n        *   **Modeling `subClassOf`**: A `subClassOf` relation `(sub-concept, subClassOf, super-concept)` is modeled by requiring the sub-concept sphere to be *inside* the super-concept sphere. `\\cite{lv2018}` enumerates four possible relative positions between two spheres and defines specific loss functions for each, guiding the optimization towards the desired \"sub-sphere inside super-sphere\" configuration.\n        *   **Inherent Transitivity**: This geometric modeling naturally preserves `instanceOf`-`subClassOf` transitivity (if an instance is inside a sub-concept sphere, and that sub-concept sphere is inside a super-concept sphere, the instance is also inside the super-concept sphere) and `subClassOf`-`subClassOf` transitivity.\n        *   **Joint Learning Framework**: The specialized loss functions for `isA` relations are integrated with a standard translation-based loss (like TransE's `||h + r - t||^2`) for other relational triples into a unified margin-based ranking loss function for joint optimization.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The TransC model itself, which introduces a novel geometric approach to represent concepts (spheres) and instances (vectors) in KGE.\n        *   Specific loss functions designed to capture the semantic meaning and transitivity of `instanceOf` and `subClassOf` relations through spatial containment.\n    *   **Theoretical Insights/Analysis**: Demonstrates how the chosen geometric representations (instance vector inside concept sphere, sub-concept sphere inside super-concept sphere) inherently and elegantly capture the transitivity properties of `isA` relations, which is a significant improvement over previous models.\n    *   **System Design/Architectural Innovations**: Proposes a comprehensive framework that seamlessly integrates these differentiated representations and specialized loss functions with existing translation-based models for general relations, allowing for joint learning across all types of triples in a knowledge graph.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: `\\cite{lv2018}` evaluates TransC on two standard KGE tasks:\n        *   **Link Prediction**: Predicting missing head or tail entities in a triple.\n        *   **Triple Classification**: Classifying whether a given triple is true or false.\n    *   **Datasets**:\n        *   **YAGO39K**: A custom subset of YAGO, specifically constructed by `\\cite{lv2018}` because common datasets like FB15K (instance-heavy) and WN18 (concept-heavy) are not suitable for evaluating models that differentiate concepts and instances.\n        *   **M-YAGO39K**: An augmented version of YAGO39K, created by adding triples inferred through `isA` transitivity, specifically designed to test the model's ability to handle transitivity.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Link Prediction**: Evaluated using Mean Reciprocal Rank (MRR) and Hits@N (N=1, 3, 10), with both \"Raw\" and \"Filter\" settings.\n        *   **Triple Classification**: Evaluated using Accuracy, Precision, Recall, and F1-Score.\n        *   **Results**: TransC consistently outperforms several state-of-the-art baseline methods (TransE, TransH, TransR, TransD, HolE, DistMult, ComplEx) across most metrics on YAGO39K. Notably, TransC shows significant improvements in Hits@N for link prediction. The performance on M-YAGO39K implicitly validates its ability to capture `isA` transitivity.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The model's effectiveness relies on the assumption that concepts can be adequately represented as spheres and instances as points within those spheres. While effective for `isA` relations, this geometric choice might have limitations for more complex conceptual relationships or non-hierarchical structures. The paper does not explicitly discuss limitations of the TransC model itself, but rather focuses on addressing the limitations of prior work.\n    *   **Scope of Applicability**: TransC is particularly well-suited for knowledge graphs that contain a rich hierarchy of concepts and instances, and where `isA` relations are prominent and semantically important (e.g., YAGO, Freebase, WordNet). Its benefits might be less pronounced in KGs that are primarily flat or instance-centric, or where `isA` relations are not a major component.\n\n7.  **Technical Significance**\n    *   **Advances the Technical State-of-the-Art**: `\\cite{lv2018}` makes a significant advancement by introducing the first KGE model that explicitly differentiates between concepts and instances. It provides a novel and effective geometric approach to represent and reason about hierarchical `isA` relations, successfully addressing the long-standing challenge of capturing their transitivity within embedding spaces.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research into more sophisticated and semantically rich geometric or topological representations for different types of entities in KGE.\n        *   Encourages further exploration of how to explicitly model and leverage hierarchical and taxonomic structures within knowledge graphs, moving beyond uniform entity representations.\n        *   Could inspire new methods for direct reasoning and inference over conceptual hierarchies within learned embedding spaces.\n        *   The introduced YAGO39K and M-YAGO39K datasets provide valuable benchmarks for future work focusing on concept-instance differentiation and transitivity.",
        "keywords": [
          "Knowledge graph embedding (KGE)",
          "Concepts and instances differentiation",
          "`isA` relations transitivity",
          "TransC model",
          "Sphere-based concept representation",
          "Geometric entity representation",
          "Joint learning framework",
          "Link prediction",
          "Triple classification",
          "YAGO39K dataset",
          "Spatial containment modeling",
          "Hierarchical knowledge representation"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b.pdf"
    },
    {
      "success": true,
      "doc_id": "7de841f977c259bf1462854dae44b887",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Current Knowledge Graph Embedding (KGE) models struggle with the \"polysemy issue,\" where entities exhibit different semantic characteristics depending on the relations they participate in. This leads to inaccurate link predictions due to the incompleteness of Knowledge Graphs.\n    *   **Importance & Challenge:** The problem is crucial because KGE is fundamental for predicting missing links in KGs. The challenge lies in the weak interactions between entities and their relational contexts in existing models, which limits their expressiveness in modeling complex structures and capturing diverse semantic dimensions.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the foundation of KGE models, which map entities and relations into low-dimensional continuous vector spaces.\n    *   **Limitations of Previous Solutions:** Existing KGE models often suffer from weak interactions between entities and their relation contexts, leading to low expressiveness and an inability to effectively handle the polysemy of entities.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **Contextualized Quaternion Embedding (ConQuatE)** \\cite{chen2025}. This model enhances entity representation learning by capturing diverse relational contexts.\n    *   **Novelty/Difference:** ConQuatE's novelty lies in its use of **quaternion rotation** to efficiently incorporate contextual cues from various connected relations. It enriches original entity representations through efficient vector transformations in quaternion space, crucially without requiring any extra information beyond the original knowledge graph triples. This allows it to model multiple semantic dimensions for entities.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** Introduction of ConQuatE, a novel KGE model specifically designed to address the polysemy issue.\n    *   **Technical Innovation:** Leveraging quaternion rotation for contextualization, enabling the capture of diverse relational contexts and enriching entity representations across multiple semantic dimensions.\n    *   **Efficiency:** Achieves contextualization through efficient vector transformations in quaternion space, without needing additional external information.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The model was evaluated on the task of **Link Prediction**.\n    *   **Key Performance Metrics & Results:** ConQuatE \\cite{chen2025} demonstrated superior performance, outperforming state-of-the-art models for Link Prediction on four widely recognized datasets: FB15k-237, WN18RR, FB15k, and WN18.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper implicitly assumes that quaternion space is an effective medium for capturing and transforming diverse relational contexts. No explicit limitations are stated regarding the model's complexity or specific types of KGs it might struggle with.\n    *   **Scope of Applicability:** Primarily focused on Knowledge Graph Embedding for Link Prediction, specifically targeting the challenge of entity polysemy.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** ConQuatE \\cite{chen2025} significantly advances the technical state-of-the-art in KGE by providing an effective and efficient solution to the long-standing polysemy problem.\n    *   **Potential Impact:** This work could inspire future research into leveraging higher-dimensional algebras (like quaternions) and novel contextualization mechanisms for more expressive and accurate knowledge graph representations, particularly for handling complex semantic variations of entities.",
      "intriguing_abstract": "Despite significant advancements, Knowledge Graph Embedding (KGE) models continue to grapple with the pervasive 'polysemy issue,' where entities exhibit varied semantic characteristics across different relational contexts, leading to inaccurate link predictions and hindering the completion of Knowledge Graphs. Existing approaches often suffer from weak interactions between entities and their relational environments, limiting their expressiveness.\n\nWe introduce **Contextualized Quaternion Embedding (ConQuatE)**, a novel KGE model specifically engineered to overcome this fundamental challenge. ConQuatE innovatively leverages **quaternion rotation** to efficiently incorporate diverse contextual cues from connected relations. This allows it to enrich original entity representations by modeling multiple semantic dimensions through efficient vector transformations in quaternion space, crucially without requiring any additional external information.\n\nOur extensive experiments on four benchmark datasets (FB15k-237, WN18RR, FB15k, WN18) demonstrate ConQuatE's superior performance in **link prediction**, significantly outperforming state-of-the-art models. ConQuatE offers an effective and efficient solution to the long-standing polysemy problem, advancing the state-of-the-art in KGE and paving the way for more expressive and accurate knowledge graph representations by exploring higher-dimensional algebras for robust **contextualization**.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "polysemy issue",
        "link prediction",
        "Knowledge Graphs (KGs)",
        "Contextualized Quaternion Embedding (ConQuatE)",
        "quaternion rotation",
        "contextualization",
        "entity representation learning",
        "diverse relational contexts",
        "multiple semantic dimensions",
        "efficient vector transformations",
        "superior performance",
        "state-of-the-art advancement",
        "higher-dimensional algebras"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/c64433657869ecdaaa7988a029eabfe774d3ac47.pdf",
      "citation_key": "chen2025",
      "metadata": {
        "title": "Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction",
        "authors": [
          "Jie Chen",
          "Yinlong Wang",
          "Shu Zhao",
          "Peng Zhou",
          "Yanping Zhang"
        ],
        "published_date": "2025",
        "abstract": "To meet the challenge of incompleteness within Knowledge Graphs, Knowledge Graph Embedding (KGE) has emerged as the fundamental methodology for predicting the missing link (Link Prediction), by mapping entities and relations as low-dimensional vectors in continuous space. However, current KGE models often struggle with the polysemy issue, where entities exhibit different semantic characteristics depending on the relations in which they participate. Such limitation stems from weak interactions between entities and their relation contexts, leading to low expressiveness in modeling complex structures and resulting in inaccurate predictions. To address this, we propose Contextualized Quaternion Embedding (ConQuatE), a model that enhances the representation learning of entities across multiple semantic dimensions by leveraging quaternion rotation to capture diverse relational contexts. In specific, ConQuatE incorporates contextual cues from various connected relations to enrich the original entity representations. Notably, this is achieved through efficient vector transformations in quaternion space, without any extra information required other than original triples. Experimental results demonstrate that our model outperforms state-of-the-art models for Link Prediction on four widely recognized datasets: FB15k-237, WN18RR, FB15k, and WN18.",
        "file_path": "paper_data/knowledge_graph_embedding/c64433657869ecdaaa7988a029eabfe774d3ac47.pdf",
        "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Current Knowledge Graph Embedding (KGE) models struggle with the \"polysemy issue,\" where entities exhibit different semantic characteristics depending on the relations they participate in. This leads to inaccurate link predictions due to the incompleteness of Knowledge Graphs.\n    *   **Importance & Challenge:** The problem is crucial because KGE is fundamental for predicting missing links in KGs. The challenge lies in the weak interactions between entities and their relational contexts in existing models, which limits their expressiveness in modeling complex structures and capturing diverse semantic dimensions.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon the foundation of KGE models, which map entities and relations into low-dimensional continuous vector spaces.\n    *   **Limitations of Previous Solutions:** Existing KGE models often suffer from weak interactions between entities and their relation contexts, leading to low expressiveness and an inability to effectively handle the polysemy of entities.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **Contextualized Quaternion Embedding (ConQuatE)** \\cite{chen2025}. This model enhances entity representation learning by capturing diverse relational contexts.\n    *   **Novelty/Difference:** ConQuatE's novelty lies in its use of **quaternion rotation** to efficiently incorporate contextual cues from various connected relations. It enriches original entity representations through efficient vector transformations in quaternion space, crucially without requiring any extra information beyond the original knowledge graph triples. This allows it to model multiple semantic dimensions for entities.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** Introduction of ConQuatE, a novel KGE model specifically designed to address the polysemy issue.\n    *   **Technical Innovation:** Leveraging quaternion rotation for contextualization, enabling the capture of diverse relational contexts and enriching entity representations across multiple semantic dimensions.\n    *   **Efficiency:** Achieves contextualization through efficient vector transformations in quaternion space, without needing additional external information.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The model was evaluated on the task of **Link Prediction**.\n    *   **Key Performance Metrics & Results:** ConQuatE \\cite{chen2025} demonstrated superior performance, outperforming state-of-the-art models for Link Prediction on four widely recognized datasets: FB15k-237, WN18RR, FB15k, and WN18.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper implicitly assumes that quaternion space is an effective medium for capturing and transforming diverse relational contexts. No explicit limitations are stated regarding the model's complexity or specific types of KGs it might struggle with.\n    *   **Scope of Applicability:** Primarily focused on Knowledge Graph Embedding for Link Prediction, specifically targeting the challenge of entity polysemy.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** ConQuatE \\cite{chen2025} significantly advances the technical state-of-the-art in KGE by providing an effective and efficient solution to the long-standing polysemy problem.\n    *   **Potential Impact:** This work could inspire future research into leveraging higher-dimensional algebras (like quaternions) and novel contextualization mechanisms for more expressive and accurate knowledge graph representations, particularly for handling complex semantic variations of entities.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "polysemy issue",
          "link prediction",
          "Knowledge Graphs (KGs)",
          "Contextualized Quaternion Embedding (ConQuatE)",
          "quaternion rotation",
          "contextualization",
          "entity representation learning",
          "diverse relational contexts",
          "multiple semantic dimensions",
          "efficient vector transformations",
          "superior performance",
          "state-of-the-art advancement",
          "higher-dimensional algebras"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "c64433657869ecdaaa7988a029eabfe774d3ac47.pdf"
    },
    {
      "success": true,
      "doc_id": "b2550adb94734ba939fd9a243650bac1",
      "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey paper focuses on negative sampling approaches within the domain of Knowledge Graph Embedding (KGE). Its main objectives are to summarize current negative sampling methodologies, discuss their characteristics, and offer guidelines for future research in this area.\n\n2.  **Literature Coverage**\n    The survey reviews \"current negative sampling approaches\" and discusses \"the most prevalent existing approaches.\" While it does not specify a particular time period or explicit selection criteria, it aims to cover the significant methods currently in use.\n\n3.  **Classification Framework**\n    *   Static distribution-based approaches\n    *   Dynamic distribution-based approaches\n    *   Custom cluster-based approaches\n\n4.  **Key Findings & Insights**\n    *   Negative sampling is crucial for training conventional KGE methods, especially translational distance-based models, due to KGs primarily storing positive samples \\cite{qian2021}.\n    *   The quality of generated negative samples directly impacts the performance of learned knowledge representations in various downstream tasks, such as recommendation and link prediction \\cite{qian2021}.\n    *   The survey discusses the characteristics of prevalent negative sampling approaches based on its proposed categorization \\cite{qian2021}.\n\n5.  **Research Gaps & Future Directions**\n    The survey implicitly identifies a need for further innovation in negative sampling by aiming to provide guidelines for \"new thoughts\" about these methods in KGE \\cite{qian2021}. This suggests future research should explore novel approaches beyond the current categories.\n\n6.  **Survey Contribution**\n    This survey provides a structured categorization of current negative sampling approaches in KGE, offering a clear overview of the field. It aims to stimulate new research by providing guidelines for developing more effective negative sampling strategies \\cite{qian2021}.",
      "intriguing_abstract": "The efficacy of Knowledge Graph Embeddings (KGEs) hinges critically on effective training, a process profoundly challenged by the inherent positive-only nature of knowledge graphs. This survey paper delves into the indispensable role of **negative sampling**, a cornerstone technique for training conventional KGE methods, particularly **translational distance-based models**. We present the first comprehensive review and a novel, structured categorization of current negative sampling approaches, classifying them into **static distribution-based**, **dynamic distribution-based**, and **custom cluster-based** methodologies.\n\nOur analysis reveals how the quality of generated negative samples directly dictates the performance of learned knowledge representations in vital downstream tasks such as **link prediction** and **recommendation**. Beyond a systematic overview of prevalent techniques and their characteristics, this paper meticulously identifies key research gaps and offers actionable guidelines to stimulate innovative 'new thoughts' in negative sampling. This work provides a crucial roadmap for researchers to develop more effective and sophisticated negative sampling strategies, ultimately advancing the robustness and applicability of KGEs across diverse applications.",
      "keywords": [
        "Negative sampling",
        "Knowledge Graph Embedding (KGE)",
        "Survey paper",
        "Negative sampling classification framework",
        "Static distribution-based approaches",
        "Dynamic distribution-based approaches",
        "Custom cluster-based approaches",
        "Translational distance-based models",
        "Knowledge representations",
        "Link prediction",
        "Recommendation systems",
        "Research guidelines",
        "Effective negative sampling strategies",
        "Performance impact"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/8fef3f8bb8bcd254898b5d24f3d78beab09e99d4.pdf",
      "citation_key": "qian2021",
      "metadata": {
        "title": "Understanding Negative Sampling in Knowledge Graph Embedding",
        "authors": [
          "Jing Qian",
          "Gangmin Li",
          "Katie Atkinson",
          "Yong Yue"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding (KGE) is to project entities and relations of a knowledge graph (KG) into a low-dimensional vector space, which has made steady progress in recent years. Conventional KGE methods, especially translational distance-based models, are trained through discriminating positive samples from negative ones. Most KGs store only positive samples for space efficiency. Negative sampling thus plays a crucial role in encoding triples of a KG. The quality of generated negative samples has a direct impact on the performance of learnt knowledge representation in a myriad of downstream tasks, such as recommendation, link prediction and node classification. We summarize current negative sampling approaches in KGE into three categories, static distribution-based, dynamic distribution-based and custom cluster-based respectively. Based on this categorization we discuss the most prevalent existing approaches and their characteristics. It is a hope that this review can provide some guidelines for new thoughts about negative sampling in KGE.",
        "file_path": "paper_data/knowledge_graph_embedding/8fef3f8bb8bcd254898b5d24f3d78beab09e99d4.pdf",
        "venue": "International Journal of Artificial Intelligence & Applications",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey paper focuses on negative sampling approaches within the domain of Knowledge Graph Embedding (KGE). Its main objectives are to summarize current negative sampling methodologies, discuss their characteristics, and offer guidelines for future research in this area.\n\n2.  **Literature Coverage**\n    The survey reviews \"current negative sampling approaches\" and discusses \"the most prevalent existing approaches.\" While it does not specify a particular time period or explicit selection criteria, it aims to cover the significant methods currently in use.\n\n3.  **Classification Framework**\n    *   Static distribution-based approaches\n    *   Dynamic distribution-based approaches\n    *   Custom cluster-based approaches\n\n4.  **Key Findings & Insights**\n    *   Negative sampling is crucial for training conventional KGE methods, especially translational distance-based models, due to KGs primarily storing positive samples \\cite{qian2021}.\n    *   The quality of generated negative samples directly impacts the performance of learned knowledge representations in various downstream tasks, such as recommendation and link prediction \\cite{qian2021}.\n    *   The survey discusses the characteristics of prevalent negative sampling approaches based on its proposed categorization \\cite{qian2021}.\n\n5.  **Research Gaps & Future Directions**\n    The survey implicitly identifies a need for further innovation in negative sampling by aiming to provide guidelines for \"new thoughts\" about these methods in KGE \\cite{qian2021}. This suggests future research should explore novel approaches beyond the current categories.\n\n6.  **Survey Contribution**\n    This survey provides a structured categorization of current negative sampling approaches in KGE, offering a clear overview of the field. It aims to stimulate new research by providing guidelines for developing more effective negative sampling strategies \\cite{qian2021}.",
        "keywords": [
          "Negative sampling",
          "Knowledge Graph Embedding (KGE)",
          "Survey paper",
          "Negative sampling classification framework",
          "Static distribution-based approaches",
          "Dynamic distribution-based approaches",
          "Custom cluster-based approaches",
          "Translational distance-based models",
          "Knowledge representations",
          "Link prediction",
          "Recommendation systems",
          "Research guidelines",
          "Effective negative sampling strategies",
          "Performance impact"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4.pdf"
    },
    {
      "success": true,
      "doc_id": "a66ee17f827bcfb87a08df25e32face0",
      "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey \\cite{dai2020} focuses on knowledge graph (KG) embedding, a technique proposed to alleviate data sparsity in large-scale KGs by embedding entities and relations into a low-dimensional feature space. Its main objectives are to systematically introduce state-of-the-art KG embedding approaches, discuss their applications, and explore future development prospects.\n\n2.  **Literature Coverage**\n    The paper reviews existing state-of-the-art approaches in KG embedding, implying a focus on recent and impactful research in the field. While specific time periods or detailed selection criteria are not explicitly stated, the coverage aims to be systematic and comprehensive regarding current prominent methods.\n\n3.  **Classification Framework**\n    *   Models leveraging only observed triplets in the KG.\n    *   Advanced models utilizing additional semantic information to enhance performance.\n    *   Additional semantic information is further categorized into textual descriptions and relation paths.\n\n4.  **Key Findings & Insights**\n    *   The survey compares the advantages and disadvantages of various KG embedding approaches, highlighting their strengths and weaknesses.\n    *   It presents experimental comparisons of the performance of listed methods, providing empirical insights into their effectiveness.\n    *   KG embedding methods are shown to benefit broader domain tasks, including question answering and recommender systems.\n    *   The paper implicitly identifies trends by categorizing the evolution from triplet-based models to those incorporating additional semantic information.\n\n5.  **Research Gaps & Future Directions**\n    The survey identifies several hurdles that need to be overcome in KG embedding research. It provides specific future research directions aimed at advancing techniques and application trends in the field.\n\n6.  **Survey Contribution**\n    This survey \\cite{dai2020} offers a systematic and comprehensive introduction to state-of-the-art KG embedding approaches and their applications. It provides significant value by organizing the diverse literature, comparing methods, and outlining critical future research avenues.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graphs (KGs) hinges on overcoming data sparsity, a challenge effectively addressed by **Knowledge Graph embedding**. This comprehensive survey systematically reviews the **state-of-the-art** in KG embedding, a pivotal technique for mapping entities and relations into a **low-dimensional feature space**. We present a novel classification framework, distinguishing between models leveraging only observed triplets and advanced approaches that harness additional **semantic information**, including **textual descriptions** and **relation paths**. Our analysis critically compares the advantages and disadvantages of diverse methods, providing empirical insights into their performance and demonstrating how KG embedding significantly benefits downstream tasks such as **question answering** and **recommender systems**. Furthermore, this paper identifies crucial research gaps and outlines specific future directions, guiding researchers toward advancing techniques and expanding application trends. This survey offers an invaluable resource for navigating the complex landscape of KG embedding, fostering innovation in this rapidly evolving field.",
      "keywords": [
        "knowledge graph (KG) embedding",
        "data sparsity alleviation",
        "low-dimensional feature space",
        "entities and relations",
        "state-of-the-art approaches",
        "triplet-based models",
        "additional semantic information",
        "textual descriptions",
        "relation paths",
        "question answering",
        "recommender systems",
        "experimental comparisons",
        "research gaps",
        "future research directions",
        "systematic survey"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/68f34ed64fdf07bb1325097c93576658e061231e.pdf",
      "citation_key": "dai2020",
      "metadata": {
        "title": "A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks",
        "authors": [
          "Yuanfei Dai",
          "Shiping Wang",
          "N. Xiong",
          "Wenzhong Guo"
        ],
        "published_date": "2020",
        "abstract": "A knowledge graph (KG), also known as a knowledge base, is a particular kind of network structure in which the node indicates entity and the edge represent relation. However, with the explosion of network volume, the problem of data sparsity that causes large-scale KG systems to calculate and manage difficultly has become more significant. For alleviating the issue, knowledge graph embedding is proposed to embed entities and relations in a KG to a low-, dense and continuous feature space, and endow the yield model with abilities of knowledge inference and fusion. In recent years, many researchers have poured much attention in this approach, and we will systematically introduce the existing state-of-the-art approaches and a variety of applications that benefit from these methods in this paper. In addition, we discuss future prospects for the development of techniques and application trends. Specifically, we first introduce the embedding models that only leverage the information of observed triplets in the KG. We illustrate the overall framework and specific idea and compare the advantages and disadvantages of such approaches. Next, we introduce the advanced models that utilize additional semantic information to improve the performance of the original methods. We divide the additional information into two categories, including textual descriptions and relation paths. The extension approaches in each category are described, following the same classification criteria as those defined for the triplet fact-based models. We then describe two experiments for comparing the performance of listed methods and mention some broader domain tasks such as question answering, recommender systems, and so forth. Finally, we collect several hurdles that need to be overcome and provide a few future research directions for knowledge graph embedding.",
        "file_path": "paper_data/knowledge_graph_embedding/68f34ed64fdf07bb1325097c93576658e061231e.pdf",
        "venue": "Electronics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey \\cite{dai2020} focuses on knowledge graph (KG) embedding, a technique proposed to alleviate data sparsity in large-scale KGs by embedding entities and relations into a low-dimensional feature space. Its main objectives are to systematically introduce state-of-the-art KG embedding approaches, discuss their applications, and explore future development prospects.\n\n2.  **Literature Coverage**\n    The paper reviews existing state-of-the-art approaches in KG embedding, implying a focus on recent and impactful research in the field. While specific time periods or detailed selection criteria are not explicitly stated, the coverage aims to be systematic and comprehensive regarding current prominent methods.\n\n3.  **Classification Framework**\n    *   Models leveraging only observed triplets in the KG.\n    *   Advanced models utilizing additional semantic information to enhance performance.\n    *   Additional semantic information is further categorized into textual descriptions and relation paths.\n\n4.  **Key Findings & Insights**\n    *   The survey compares the advantages and disadvantages of various KG embedding approaches, highlighting their strengths and weaknesses.\n    *   It presents experimental comparisons of the performance of listed methods, providing empirical insights into their effectiveness.\n    *   KG embedding methods are shown to benefit broader domain tasks, including question answering and recommender systems.\n    *   The paper implicitly identifies trends by categorizing the evolution from triplet-based models to those incorporating additional semantic information.\n\n5.  **Research Gaps & Future Directions**\n    The survey identifies several hurdles that need to be overcome in KG embedding research. It provides specific future research directions aimed at advancing techniques and application trends in the field.\n\n6.  **Survey Contribution**\n    This survey \\cite{dai2020} offers a systematic and comprehensive introduction to state-of-the-art KG embedding approaches and their applications. It provides significant value by organizing the diverse literature, comparing methods, and outlining critical future research avenues.",
        "keywords": [
          "knowledge graph (KG) embedding",
          "data sparsity alleviation",
          "low-dimensional feature space",
          "entities and relations",
          "state-of-the-art approaches",
          "triplet-based models",
          "additional semantic information",
          "textual descriptions",
          "relation paths",
          "question answering",
          "recommender systems",
          "experimental comparisons",
          "research gaps",
          "future research directions",
          "systematic survey"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "68f34ed64fdf07bb1325097c93576658e061231e.pdf"
    },
    {
      "success": true,
      "doc_id": "0450e44c4a82d4fc6e68cddd606c3d52",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Knowledge Graph Embedding (KGE) models are insufficient for representing and embedding uncertain and dynamic knowledge, as traditional Knowledge Graphs (KGs) primarily model crisp and static resources \\cite{ji2024}.\n    *   **Importance & Challenge:** Real-world information is often inherently uncertain and dynamic, posing a significant challenge to capture its full meaning and relationships within the static, crisp structures of conventional KGs and their vector space embeddings \\cite{ji2024}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work extends the paradigm of KGE models by specifically addressing the integration of uncertainty and dynamism, which are typically overlooked in standard KGE frameworks \\cite{ji2024}.\n    *   **Limitations of Previous Solutions:** Previous KGE models are limited because they are designed for \"crisp and static resources\" and are \"insufficient to deal with uncertain dynamic knowledge in vector spaces\" \\cite{ji2024}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes the Strongly Adaptive Fuzzy Spatiotemporal RDF Embedding model (FSTRE) \\cite{ji2024}.\n        *   It first introduces a fine-grained fuzzy spatiotemporal RDF model to serve as the underlying representation framework \\cite{ji2024}.\n        *   Within a complex vector space, spatial information is embedded using projection, and temporal information is embedded using rotation \\cite{ji2024}.\n        *   Fine-grained fuzziness is integrated into each element of the spatiotemporal five-tuples by leveraging the modal length of anisotropic vectors \\cite{ji2024}.\n        *   Geometric operations are employed as transformation operators to capture rich interactions between crisp/static knowledge and fuzzy spatiotemporal knowledge \\cite{ji2024}.\n    *   **Novelty:** FSTRE's novelty lies in its comprehensive integration of fuzziness, spatial, and temporal dimensions directly into KGE within a complex vector space, specifically designed to overcome the limitations of existing models for uncertain and dynamic knowledge \\cite{ji2024}. The distinct use of projection for spatial and rotation for temporal embedding, combined with anisotropic vectors for fine-grained fuzziness, represents a key innovation \\cite{ji2024}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A novel fine-grained fuzzy spatiotemporal RDF model for foundational knowledge representation \\cite{ji2024}.\n        *   The FSTRE model, which uniquely embeds spatial information via projection and temporal information via rotation within a complex vector space \\cite{ji2024}.\n        *   A method for integrating fine-grained fuzziness into spatiotemporal five-tuples using the modal length of anisotropic vectors \\cite{ji2024}.\n        *   Utilization of geometric operations as transformation operators to effectively model the interactions between crisp/static and fuzzy spatiotemporal knowledge \\cite{ji2024}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** An experimental evaluation of the FSTRE model was performed \\cite{ji2024}.\n    *   **Dataset:** The evaluation was based on a \"built fuzzy spatiotemporal KG,\" indicating the creation of a specialized dataset tailored to the problem \\cite{ji2024}.\n    *   **Key Performance Metrics & Comparison Results:** The FSTRE model \"is superior to state-of-the-art methods\" and demonstrates its capability to \"handle complex fuzzy spatiotemporal knowledge\" \\cite{ji2024}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The abstract does not explicitly detail technical limitations. However, the reliance on a \"built fuzzy spatiotemporal KG\" for evaluation suggests that real-world application might require specific data preparation or adaptation for existing KGs to fit the fuzzy spatiotemporal RDF model \\cite{ji2024}.\n    *   **Scope of Applicability:** The model is primarily applicable to scenarios and knowledge graphs that necessitate the simultaneous modeling of uncertainty, dynamism, spatial, and temporal aspects \\cite{ji2024}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art in KGE by providing a robust framework for representing and embedding uncertain and dynamic knowledge, a capability largely absent in previous models \\cite{ji2024}.\n    *   **Potential Impact on Future Research:** FSTRE opens new research avenues for KGE applications in domains where information is inherently fuzzy, spatiotemporal, and dynamic (e.g., environmental monitoring, real-time event analysis, social network evolution), offering a foundational model for integrating these complex dimensions into vector space representations \\cite{ji2024}.",
      "intriguing_abstract": "Real-world knowledge is rarely crisp and static; it is inherently uncertain, dynamic, and often tied to specific spatiotemporal contexts. Traditional Knowledge Graph Embedding (KGE) models, designed for static resources, fundamentally struggle to capture this rich complexity, limiting their applicability. We introduce FSTRE, the Strongly Adaptive Fuzzy Spatiotemporal RDF Embedding model, a novel paradigm that revolutionizes KGE by comprehensively integrating fuzziness, spatial, and temporal dimensions.\n\nFSTRE first establishes a fine-grained fuzzy spatiotemporal RDF model. Within a complex vector space, it uniquely embeds spatial information via projection and temporal information through rotation. Crucially, fine-grained fuzziness is integrated into each element using the modal length of anisotropic vectors, while geometric operations model intricate interactions between crisp and fuzzy spatiotemporal knowledge. Our experimental evaluation on a specialized dataset demonstrates FSTRE's superior performance over state-of-the-art methods in handling complex fuzzy spatiotemporal knowledge. This work significantly advances KGE, opening new avenues for applications in dynamic, uncertain domains like environmental monitoring and real-time event analysis.",
      "keywords": [
        "Strongly Adaptive Fuzzy Spatiotemporal RDF Embedding (FSTRE)",
        "Knowledge Graph Embedding (KGE)",
        "uncertain and dynamic knowledge",
        "fuzzy spatiotemporal RDF model",
        "complex vector space",
        "spatial embedding via projection",
        "temporal embedding via rotation",
        "fine-grained fuzziness",
        "anisotropic vectors",
        "geometric operations",
        "state-of-the-art advancement",
        "environmental monitoring"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/efea0197c956e981e98c4d2532fa720c58954492.pdf",
      "citation_key": "ji2024",
      "metadata": {
        "title": "FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation",
        "authors": [
          "Hao Ji",
          "Li Yan",
          "Z. Ma"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graphs (KGs) use resource description framework (RDF) triples to model various crisp and static resources in the world. Meanwhile, knowledge embedded into vector space can imply more meanings. Much real-world information, however, is often uncertain and dynamic. Existing KG embedding (KGE) models are insufficient to deal with uncertain dynamic knowledge in vector spaces. To overcome this drawback, this article concentrates on an embedding module for the distributed representation of uncertain dynamic knowledge and proposes a strongly adaptive fuzzy spatiotemporal RDF embedding model (FSTRE). Specifically, we first propose a fine-grained fuzzy spatiotemporal RDF model, which provides the underlying representation framework for FSTRE. Then, within the complex vector space, spatial and temporal information is embedded by projection and rotation, respectively. Fine-grained fuzziness penetrates each element of the spatiotemporal five-tuples by a modal length of the anisotropic vectors. By using geometric operations as its transformation operator, FSTRE can capture the rich interaction between crisp and static knowledge and fuzzy spatiotemporal knowledge. We performed an experimental evaluation of FSTRE based on the built fuzzy spatiotemporal KG. It was shown that our FSTRE model is superior to state-of-the-art methods and can handle complex fuzzy spatiotemporal knowledge.",
        "file_path": "paper_data/knowledge_graph_embedding/efea0197c956e981e98c4d2532fa720c58954492.pdf",
        "venue": "IEEE transactions on fuzzy systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Knowledge Graph Embedding (KGE) models are insufficient for representing and embedding uncertain and dynamic knowledge, as traditional Knowledge Graphs (KGs) primarily model crisp and static resources \\cite{ji2024}.\n    *   **Importance & Challenge:** Real-world information is often inherently uncertain and dynamic, posing a significant challenge to capture its full meaning and relationships within the static, crisp structures of conventional KGs and their vector space embeddings \\cite{ji2024}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work extends the paradigm of KGE models by specifically addressing the integration of uncertainty and dynamism, which are typically overlooked in standard KGE frameworks \\cite{ji2024}.\n    *   **Limitations of Previous Solutions:** Previous KGE models are limited because they are designed for \"crisp and static resources\" and are \"insufficient to deal with uncertain dynamic knowledge in vector spaces\" \\cite{ji2024}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes the Strongly Adaptive Fuzzy Spatiotemporal RDF Embedding model (FSTRE) \\cite{ji2024}.\n        *   It first introduces a fine-grained fuzzy spatiotemporal RDF model to serve as the underlying representation framework \\cite{ji2024}.\n        *   Within a complex vector space, spatial information is embedded using projection, and temporal information is embedded using rotation \\cite{ji2024}.\n        *   Fine-grained fuzziness is integrated into each element of the spatiotemporal five-tuples by leveraging the modal length of anisotropic vectors \\cite{ji2024}.\n        *   Geometric operations are employed as transformation operators to capture rich interactions between crisp/static knowledge and fuzzy spatiotemporal knowledge \\cite{ji2024}.\n    *   **Novelty:** FSTRE's novelty lies in its comprehensive integration of fuzziness, spatial, and temporal dimensions directly into KGE within a complex vector space, specifically designed to overcome the limitations of existing models for uncertain and dynamic knowledge \\cite{ji2024}. The distinct use of projection for spatial and rotation for temporal embedding, combined with anisotropic vectors for fine-grained fuzziness, represents a key innovation \\cite{ji2024}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A novel fine-grained fuzzy spatiotemporal RDF model for foundational knowledge representation \\cite{ji2024}.\n        *   The FSTRE model, which uniquely embeds spatial information via projection and temporal information via rotation within a complex vector space \\cite{ji2024}.\n        *   A method for integrating fine-grained fuzziness into spatiotemporal five-tuples using the modal length of anisotropic vectors \\cite{ji2024}.\n        *   Utilization of geometric operations as transformation operators to effectively model the interactions between crisp/static and fuzzy spatiotemporal knowledge \\cite{ji2024}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** An experimental evaluation of the FSTRE model was performed \\cite{ji2024}.\n    *   **Dataset:** The evaluation was based on a \"built fuzzy spatiotemporal KG,\" indicating the creation of a specialized dataset tailored to the problem \\cite{ji2024}.\n    *   **Key Performance Metrics & Comparison Results:** The FSTRE model \"is superior to state-of-the-art methods\" and demonstrates its capability to \"handle complex fuzzy spatiotemporal knowledge\" \\cite{ji2024}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The abstract does not explicitly detail technical limitations. However, the reliance on a \"built fuzzy spatiotemporal KG\" for evaluation suggests that real-world application might require specific data preparation or adaptation for existing KGs to fit the fuzzy spatiotemporal RDF model \\cite{ji2024}.\n    *   **Scope of Applicability:** The model is primarily applicable to scenarios and knowledge graphs that necessitate the simultaneous modeling of uncertainty, dynamism, spatial, and temporal aspects \\cite{ji2024}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art in KGE by providing a robust framework for representing and embedding uncertain and dynamic knowledge, a capability largely absent in previous models \\cite{ji2024}.\n    *   **Potential Impact on Future Research:** FSTRE opens new research avenues for KGE applications in domains where information is inherently fuzzy, spatiotemporal, and dynamic (e.g., environmental monitoring, real-time event analysis, social network evolution), offering a foundational model for integrating these complex dimensions into vector space representations \\cite{ji2024}.",
        "keywords": [
          "Strongly Adaptive Fuzzy Spatiotemporal RDF Embedding (FSTRE)",
          "Knowledge Graph Embedding (KGE)",
          "uncertain and dynamic knowledge",
          "fuzzy spatiotemporal RDF model",
          "complex vector space",
          "spatial embedding via projection",
          "temporal embedding via rotation",
          "fine-grained fuzziness",
          "anisotropic vectors",
          "geometric operations",
          "state-of-the-art advancement",
          "environmental monitoring"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "efea0197c956e981e98c4d2532fa720c58954492.pdf"
    },
    {
      "success": true,
      "doc_id": "c2ba37c229afefdf287a9737861535a2",
      "summary": "Here's a focused summary of the survey paper \\cite{yan2022} for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey \\cite{yan2022} focuses on Knowledge Graph Embedding (KGE), a technique to represent entities and relations in low-dimensional vector spaces to address computational inefficiencies and limitations of symbolic logic in KGs. Its main objectives are to classify KGE models, compare their characteristics, summarize their applications, and outline current challenges and future research directions.\n\n2.  **Literature Coverage**\n    The provided abstract for \\cite{yan2022} does not explicitly detail the time period, specific number of papers reviewed, or the methodology used for literature selection. It broadly covers various KGE methods and their applications, aiming to provide a comprehensive overview of the field.\n\n3.  **Classification Framework**\n    The survey \\cite{yan2022} organizes Knowledge Graph Embedding (KGE) models into three primary categories:\n    *   Translational distance models\n    *   Semantic matching models\n    *   Neural network based models\n\n4.  **Key Findings & Insights**\n    *   KGE significantly enhances computational efficiency and manageability of large KGs by transforming symbolic logic into dense, continuous vector operations.\n    *   KGE models serve as effective pre-trained models, particularly beneficial for downstream deep learning applications.\n    *   The survey provides a comparative analysis of the advantages and disadvantages across translational distance, semantic matching, and neural network-based KGE models.\n    *   It summarizes the diverse main applications where KGE has demonstrated utility, highlighting its practical impact.\n\n5.  **Research Gaps & Future Directions**\n    The survey \\cite{yan2022} identifies current challenges faced by KGE methods, which hinder their full potential in various applications. It also provides insights and views on promising future research directions to overcome these limitations and advance the field.\n\n6.  **Survey Contribution**\n    This survey \\cite{yan2022} offers a structured overview of KGE, providing a valuable classification framework and a comparative analysis of different models. It serves as a comprehensive resource by summarizing applications, challenges, and future trends in the field.",
      "intriguing_abstract": "The burgeoning complexity of Knowledge Graphs (KGs) demands sophisticated solutions to overcome the limitations of symbolic logic and enhance computational efficiency. This survey delves into Knowledge Graph Embedding (KGE), a transformative paradigm that projects entities and relations into low-dimensional vector spaces. We present a novel, structured classification of KGE models, categorizing them into translational distance, semantic matching, and neural network-based approaches.\n\nOur comprehensive analysis reveals KGE's profound impact, significantly boosting computational efficiency and manageability while serving as powerful pre-trained models for diverse downstream deep learning applications. A meticulous comparative study elucidates the unique advantages and disadvantages of each model type, showcasing their broad utility across various domains. Beyond current successes, we pinpoint critical challenges hindering KGE's full potential and illuminate promising future research directions. This paper offers an indispensable, panoramic resource for researchers and practitioners navigating the dynamic and evolving landscape of KGE.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "low-dimensional vector spaces",
        "computational efficiency",
        "translational distance models",
        "semantic matching models",
        "neural network based models",
        "KGE applications",
        "pre-trained models",
        "deep learning applications",
        "classification framework",
        "comparative analysis",
        "research challenges",
        "future research directions",
        "symbolic logic limitations"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/f470e11faa6200026cf39e248510070c078e509a.pdf",
      "citation_key": "yan2022",
      "metadata": {
        "title": "A Survey on Knowledge Graph Embedding",
        "authors": [
          "Qi Yan",
          "Jiaxin Fan",
          "Mohan Li",
          "Guanqun Qu",
          "Yang Xiao"
        ],
        "published_date": "2022",
        "abstract": "Knowledge graph (KG) is used to represent the relationships between different concepts in the real world. It is a special network in which nodes represent entities and edges represent relationships. KGs can intuitively model the connections between facts, but in many applications, there are certain limitations in directly using symbolic logic to represent knowledge in KGs and perform calculations, making it difficult to achieve expected results in downstream tasks. Meanwhile, with the explosive growth of Internet capacity, the traditional KG structure faces the problems of computational inefficiency and management difficulties. To alleviate these problems, Knowledge graph embedding (KGE) is proposed to improve the computational efficiency by embedding entities and relations in the KG into a low-dimensional, dense and continuous vector space, and thus the solution of some problems in the knowledge graph is transformed into vector operations. Moreover, KGE also can be used as a pre-trained model which is more beneficial to downstream applications, such as applications based on deep learning. In this paper, we classify KGE into three categories, namely translational distance models, semantic matching models and neural network based models. The advantages and disadvantages of different embedding methods are compared, while the main applications of KGE are summarized. Some current challenges of KGE are summarized, as well as some views on the future research directions of KGE.",
        "file_path": "paper_data/knowledge_graph_embedding/f470e11faa6200026cf39e248510070c078e509a.pdf",
        "venue": "International Conference on Data Science in Cyberspace",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the survey paper \\cite{yan2022} for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey \\cite{yan2022} focuses on Knowledge Graph Embedding (KGE), a technique to represent entities and relations in low-dimensional vector spaces to address computational inefficiencies and limitations of symbolic logic in KGs. Its main objectives are to classify KGE models, compare their characteristics, summarize their applications, and outline current challenges and future research directions.\n\n2.  **Literature Coverage**\n    The provided abstract for \\cite{yan2022} does not explicitly detail the time period, specific number of papers reviewed, or the methodology used for literature selection. It broadly covers various KGE methods and their applications, aiming to provide a comprehensive overview of the field.\n\n3.  **Classification Framework**\n    The survey \\cite{yan2022} organizes Knowledge Graph Embedding (KGE) models into three primary categories:\n    *   Translational distance models\n    *   Semantic matching models\n    *   Neural network based models\n\n4.  **Key Findings & Insights**\n    *   KGE significantly enhances computational efficiency and manageability of large KGs by transforming symbolic logic into dense, continuous vector operations.\n    *   KGE models serve as effective pre-trained models, particularly beneficial for downstream deep learning applications.\n    *   The survey provides a comparative analysis of the advantages and disadvantages across translational distance, semantic matching, and neural network-based KGE models.\n    *   It summarizes the diverse main applications where KGE has demonstrated utility, highlighting its practical impact.\n\n5.  **Research Gaps & Future Directions**\n    The survey \\cite{yan2022} identifies current challenges faced by KGE methods, which hinder their full potential in various applications. It also provides insights and views on promising future research directions to overcome these limitations and advance the field.\n\n6.  **Survey Contribution**\n    This survey \\cite{yan2022} offers a structured overview of KGE, providing a valuable classification framework and a comparative analysis of different models. It serves as a comprehensive resource by summarizing applications, challenges, and future trends in the field.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "low-dimensional vector spaces",
          "computational efficiency",
          "translational distance models",
          "semantic matching models",
          "neural network based models",
          "KGE applications",
          "pre-trained models",
          "deep learning applications",
          "classification framework",
          "comparative analysis",
          "research challenges",
          "future research directions",
          "symbolic logic limitations"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "f470e11faa6200026cf39e248510070c078e509a.pdf"
    },
    {
      "success": true,
      "doc_id": "578d4cfe4a830124b9101f10433dbe4f",
      "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your citation requirements:\n\n---\n\n*   **CITATION**: \\cite{zhang2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Negative Sampling (NS) methods, widely used in Knowledge Graph Embedding (KGE), are unsuitable and inefficient for Multi-modal Knowledge Graph Embedding (MMKGE) models. They fail to properly handle the multiple heterogeneous embeddings (e.g., structural and visual) associated with entities in MMKGs.\n    *   **Importance & Challenge**:\n        *   Knowledge Graph Completion (KGC) is a critical task due to the inherent incompleteness of real-world KGs, and KGE models trained with effective NS are key to addressing it.\n        *   MMKGE models leverage rich modal information, but current NS methods perform \"entity-level\" replacement, treating all embeddings of an entity as a single unit. This implicitly assumes modality alignment, hindering the model's ability to explicitly learn and align distinct modal embeddings (e.g., structural and visual).\n        *   Many existing NS methods are computationally expensive due to complex designs (e.g., GANs, large caches, clustering), making them inefficient for MMKGE training.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon general KGE methods (e.g., TransE \\cite{7}, DistMult \\cite{9}) and existing MMKGE frameworks (e.g., IKRL \\cite{11}, TransAE \\cite{13}) by specifically innovating the negative sampling component. It is compared against various state-of-the-art NS methods like No-Samp \\cite{17}, NSCaching \\cite{15}, SANS \\cite{16}, CAKE \\cite{18}, and EANS \\cite{19}.\n    *   **Limitations of Previous Solutions**:\n        *   **Unimodal Design**: Prior NS methods are primarily designed for unimodal KGE, where entities typically have only one structural embedding, making them ill-suited for the multi-modal nature of MMKGE.\n        *   **Lack of Modality Alignment**: By performing entity-level replacement, previous NS methods overlook the crucial task of aligning different modal embeddings within an entity, leading to less comprehensive semantic information being learned.\n        *   **Inefficiency**: Many existing NS approaches introduce complex auxiliary modules (e.g., GANs \\cite{14}, large-scale caches \\cite{15}, entity clustering \\cite{19}), making them computationally expensive and not lightweight.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Modality-Aware Negative Sampling (MANS) \\cite{zhang2023}, a lightweight and effective NS strategy specifically for MMKGE. MANS is fundamentally based on Visual Negative Sampling (MANS-V) \\cite{zhang2023} and is extended into three combined strategies: Two-Stage (MANS-T) \\cite{zhang2023}, Hybrid (MANS-H) \\cite{zhang2023}, and Adaptive (MANS-A) \\cite{zhang2023}.\n    *   **Novelty/Difference**:\n        *   **Modal-Level Sampling (MANS-V) \\cite{zhang2023}**: Unlike traditional entity-level NS, MANS-V samples *only* negative visual embeddings for contrast, while preserving the original structural embeddings. This fine-grained approach directly addresses the challenge of modality alignment.\n        *   **Combined Strategies \\cite{zhang2023}**: MANS integrates MANS-V with normal NS through structured approaches:\n            *   **MANS-T**: Divides training into two stages: an initial phase for modality alignment using MANS-V, followed by a phase for plausibility discrimination using normal NS.\n            *   **MANS-H**: Blends MANS-V and normal NS within each training epoch using a fixed, tunable proportion.\n            *   **MANS-A**: Adaptively determines the proportion of MANS-V based on the relative scores of unimodal and multi-modal components of the score function, thereby reducing the need for manual hyper-parameter tuning.\n        *   **Lightweight Design**: MANS avoids complex auxiliary modules, aiming for computational efficiency while improving the quality of negative samples for MMKGE.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Modality-Aware Negative Sampling (MANS) \\cite{zhang2023}**: The first negative sampling strategy specifically designed for multi-modal knowledge graph embedding.\n        *   **Visual Negative Sampling (MANS-V) \\cite{zhang2023}**: A novel modal-level sampling technique that samples only negative visual embeddings to explicitly achieve modality alignment between structural and visual features.\n        *   **Combined Sampling Strategies \\cite{zhang2023}**: Introduction of MANS-T, MANS-H, and MANS-A, which systematically integrate modal-level and entity-level negative sampling for comprehensive training.\n        *   **Adaptive Sampling Mechanism \\cite{zhang2023}**: MANS-A introduces an adaptive proportion for MANS-V based on the comparison of unimodal and multi-modal scores, eliminating the need for manual tuning of this hyper-parameter.\n    *   **Theoretical Insights/Analysis**: MANS-V provides a mechanism to guide the model to identify visual features corresponding to each entity, thereby strengthening the alignment between different modal embeddings.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated on two core Knowledge Graph Completion (KGC) tasks: link prediction and triple classification.\n        *   Compared MANS \\cite{zhang2023} variants against normal NS and several state-of-the-art NS methods (No-Samp \\cite{17}, NSCaching \\cite{15}, SANS \\cite{16}, CAKE \\cite{18}, EANS \\cite{19}).\n        *   Further analysis explored the impact of sampling proportions, the effectiveness and trend of adaptive sampling, efficiency, and the quality of learned embeddings.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Datasets**: Two well-known MMKG datasets: FB15K and DB15K (augmented with entity images).\n        *   **Link Prediction Metrics**: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hit@K (K=1, 3, 10), using a filtered setting.\n        *   **Triple Classification Metrics**: Accuracy (Acc), Precision (P), Recall (R), and F1-score (F1).\n        *   **Results**: Empirical results demonstrate that MANS \\cite{zhang2023} consistently outperforms existing NS baseline methods across various tasks and datasets, confirming its effectiveness in MMKGE. The paper also provides further explorations to substantiate MANS's efficiency and its ability to learn better, more semantically rich embeddings.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Primarily focuses on the visual modality for modal-level negative sampling; explicit extension to other modalities (e.g., text) within the sampling mechanism is not detailed.\n        *   The underlying MMKGE model used for evaluation (IKRL \\cite{11}) employs a TransE-based score function, which might influence the generalizability of the adaptive sampling logic to other scoring functions.\n        *   Relies on pre-trained models (VGG-16 \\cite{27}) for visual feature extraction.\n    *   **Scope of Applicability**:\n        *   Specifically designed for Multi-modal Knowledge Graph Embedding (MMKGE) models that utilize distinct embeddings for different modalities (e.g., structural and visual).\n        *   Applicable to KGC tasks such as link prediction and triple classification.\n        *   The \"lightweight\" design suggests applicability in scenarios where computational efficiency during training is a significant concern.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   MANS \\cite{zhang2023} is the first dedicated negative sampling strategy for MMKGE, addressing a critical gap in the field.\n        *   It introduces a novel modal-level sampling paradigm that explicitly tackles the challenge of aligning heterogeneous modal embeddings, a crucial aspect overlooked by previous entity-level NS methods.\n        *   Offers a lightweight and efficient alternative to complex, computationally expensive NS methods, making MMKGE training more practical.\n    *   **Potential Impact on Future Research**:\n        *   Provides a foundational NS strategy that can potentially improve the performance and robustness of future MMKGE models.\n        *   The concept of modal-aware sampling could be extended to other multi-modal learning tasks beyond KGE.\n        *   The adaptive sampling mechanism could inspire further research into self-tuning or context-aware training strategies in complex embedding scenarios.\n        *   Encourages deeper investigation into the interplay between negative sampling strategies and modality alignment in multi-modal representation learning.",
      "intriguing_abstract": "Multi-modal Knowledge Graph Embedding (MMKGE) promises richer knowledge representation, yet its full potential is bottlenecked by conventional Negative Sampling (NS) methods. Designed for unimodal KGE, these approaches are inefficient and fail to explicitly align the heterogeneous structural and visual embeddings critical for MMKGE, treating entities as monolithic units. We introduce **Modality-Aware Negative Sampling (MANS)**, a pioneering strategy specifically engineered to address this gap.\n\nMANS revolutionizes negative sampling with **Visual Negative Sampling (MANS-V)**, a novel modal-level technique that samples only negative visual embeddings, preserving structural context to explicitly foster modality alignment. We further present MANS-T (Two-Stage), MANS-H (Hybrid), and critically, **MANS-A (Adaptive)**, which intelligently adjusts sampling proportions based on unimodal and multi-modal score functions, eliminating manual tuning. MANS is lightweight, avoiding complex auxiliary modules, and significantly enhances computational efficiency.\n\nExtensive experiments on link prediction and triple classification demonstrate MANS's superior performance over state-of-the-art NS methods across MMKG datasets. MANS not only boosts **Knowledge Graph Completion (KGC)** but also learns more semantically rich embeddings, paving the way for robust and efficient multi-modal representation learning and inspiring future adaptive training paradigms.",
      "keywords": [
        "Multi-modal Knowledge Graph Embedding (MMKGE)",
        "Negative Sampling (NS)",
        "Modality-Aware Negative Sampling (MANS)",
        "Modal-level sampling",
        "Visual Negative Sampling (MANS-V)",
        "Modality alignment",
        "Adaptive sampling mechanism",
        "Lightweight design",
        "Knowledge Graph Completion (KGC)",
        "Link prediction",
        "Triple classification",
        "Heterogeneous embeddings",
        "Computational efficiency",
        "Semantic embedding learning"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/5dc88d795cbcd01e6e99ba673e91e9024f0c3318.pdf",
      "citation_key": "zhang2023",
      "metadata": {
        "title": "Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding",
        "authors": [
          "Yichi Zhang",
          "Mingyang Chen",
          "Wen Zhang"
        ],
        "published_date": "2023",
        "abstract": "Negative sampling (NS) is widely used in knowledge graph embedding (KGE), which aims to generate negative triples to make a positive-negative contrast during training. However, existing NS methods are unsuitable when multi-modal information is considered in KGE models. They are also inefficient due to their complex design. In this paper, we propose Modality-Aware Negative Sampling (MANS) for multi-modal knowledge graph embedding (MMKGE) to address the mentioned problems. MANS could align structural and visual embeddings for entities in KGs and learn meaningful embeddings to perform better in multi-modal KGE while keeping lightweight and efficient. Empirical results on two benchmarks demonstrate that MANS outperforms existing NS methods. Meanwhile, we make further explorations about MANS to confirm its effectiveness.",
        "file_path": "paper_data/knowledge_graph_embedding/5dc88d795cbcd01e6e99ba673e91e9024f0c3318.pdf",
        "venue": "IEEE International Joint Conference on Neural Network",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your citation requirements:\n\n---\n\n*   **CITATION**: \\cite{zhang2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Negative Sampling (NS) methods, widely used in Knowledge Graph Embedding (KGE), are unsuitable and inefficient for Multi-modal Knowledge Graph Embedding (MMKGE) models. They fail to properly handle the multiple heterogeneous embeddings (e.g., structural and visual) associated with entities in MMKGs.\n    *   **Importance & Challenge**:\n        *   Knowledge Graph Completion (KGC) is a critical task due to the inherent incompleteness of real-world KGs, and KGE models trained with effective NS are key to addressing it.\n        *   MMKGE models leverage rich modal information, but current NS methods perform \"entity-level\" replacement, treating all embeddings of an entity as a single unit. This implicitly assumes modality alignment, hindering the model's ability to explicitly learn and align distinct modal embeddings (e.g., structural and visual).\n        *   Many existing NS methods are computationally expensive due to complex designs (e.g., GANs, large caches, clustering), making them inefficient for MMKGE training.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon general KGE methods (e.g., TransE \\cite{7}, DistMult \\cite{9}) and existing MMKGE frameworks (e.g., IKRL \\cite{11}, TransAE \\cite{13}) by specifically innovating the negative sampling component. It is compared against various state-of-the-art NS methods like No-Samp \\cite{17}, NSCaching \\cite{15}, SANS \\cite{16}, CAKE \\cite{18}, and EANS \\cite{19}.\n    *   **Limitations of Previous Solutions**:\n        *   **Unimodal Design**: Prior NS methods are primarily designed for unimodal KGE, where entities typically have only one structural embedding, making them ill-suited for the multi-modal nature of MMKGE.\n        *   **Lack of Modality Alignment**: By performing entity-level replacement, previous NS methods overlook the crucial task of aligning different modal embeddings within an entity, leading to less comprehensive semantic information being learned.\n        *   **Inefficiency**: Many existing NS approaches introduce complex auxiliary modules (e.g., GANs \\cite{14}, large-scale caches \\cite{15}, entity clustering \\cite{19}), making them computationally expensive and not lightweight.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Modality-Aware Negative Sampling (MANS) \\cite{zhang2023}, a lightweight and effective NS strategy specifically for MMKGE. MANS is fundamentally based on Visual Negative Sampling (MANS-V) \\cite{zhang2023} and is extended into three combined strategies: Two-Stage (MANS-T) \\cite{zhang2023}, Hybrid (MANS-H) \\cite{zhang2023}, and Adaptive (MANS-A) \\cite{zhang2023}.\n    *   **Novelty/Difference**:\n        *   **Modal-Level Sampling (MANS-V) \\cite{zhang2023}**: Unlike traditional entity-level NS, MANS-V samples *only* negative visual embeddings for contrast, while preserving the original structural embeddings. This fine-grained approach directly addresses the challenge of modality alignment.\n        *   **Combined Strategies \\cite{zhang2023}**: MANS integrates MANS-V with normal NS through structured approaches:\n            *   **MANS-T**: Divides training into two stages: an initial phase for modality alignment using MANS-V, followed by a phase for plausibility discrimination using normal NS.\n            *   **MANS-H**: Blends MANS-V and normal NS within each training epoch using a fixed, tunable proportion.\n            *   **MANS-A**: Adaptively determines the proportion of MANS-V based on the relative scores of unimodal and multi-modal components of the score function, thereby reducing the need for manual hyper-parameter tuning.\n        *   **Lightweight Design**: MANS avoids complex auxiliary modules, aiming for computational efficiency while improving the quality of negative samples for MMKGE.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Modality-Aware Negative Sampling (MANS) \\cite{zhang2023}**: The first negative sampling strategy specifically designed for multi-modal knowledge graph embedding.\n        *   **Visual Negative Sampling (MANS-V) \\cite{zhang2023}**: A novel modal-level sampling technique that samples only negative visual embeddings to explicitly achieve modality alignment between structural and visual features.\n        *   **Combined Sampling Strategies \\cite{zhang2023}**: Introduction of MANS-T, MANS-H, and MANS-A, which systematically integrate modal-level and entity-level negative sampling for comprehensive training.\n        *   **Adaptive Sampling Mechanism \\cite{zhang2023}**: MANS-A introduces an adaptive proportion for MANS-V based on the comparison of unimodal and multi-modal scores, eliminating the need for manual tuning of this hyper-parameter.\n    *   **Theoretical Insights/Analysis**: MANS-V provides a mechanism to guide the model to identify visual features corresponding to each entity, thereby strengthening the alignment between different modal embeddings.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated on two core Knowledge Graph Completion (KGC) tasks: link prediction and triple classification.\n        *   Compared MANS \\cite{zhang2023} variants against normal NS and several state-of-the-art NS methods (No-Samp \\cite{17}, NSCaching \\cite{15}, SANS \\cite{16}, CAKE \\cite{18}, EANS \\cite{19}).\n        *   Further analysis explored the impact of sampling proportions, the effectiveness and trend of adaptive sampling, efficiency, and the quality of learned embeddings.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Datasets**: Two well-known MMKG datasets: FB15K and DB15K (augmented with entity images).\n        *   **Link Prediction Metrics**: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hit@K (K=1, 3, 10), using a filtered setting.\n        *   **Triple Classification Metrics**: Accuracy (Acc), Precision (P), Recall (R), and F1-score (F1).\n        *   **Results**: Empirical results demonstrate that MANS \\cite{zhang2023} consistently outperforms existing NS baseline methods across various tasks and datasets, confirming its effectiveness in MMKGE. The paper also provides further explorations to substantiate MANS's efficiency and its ability to learn better, more semantically rich embeddings.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Primarily focuses on the visual modality for modal-level negative sampling; explicit extension to other modalities (e.g., text) within the sampling mechanism is not detailed.\n        *   The underlying MMKGE model used for evaluation (IKRL \\cite{11}) employs a TransE-based score function, which might influence the generalizability of the adaptive sampling logic to other scoring functions.\n        *   Relies on pre-trained models (VGG-16 \\cite{27}) for visual feature extraction.\n    *   **Scope of Applicability**:\n        *   Specifically designed for Multi-modal Knowledge Graph Embedding (MMKGE) models that utilize distinct embeddings for different modalities (e.g., structural and visual).\n        *   Applicable to KGC tasks such as link prediction and triple classification.\n        *   The \"lightweight\" design suggests applicability in scenarios where computational efficiency during training is a significant concern.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   MANS \\cite{zhang2023} is the first dedicated negative sampling strategy for MMKGE, addressing a critical gap in the field.\n        *   It introduces a novel modal-level sampling paradigm that explicitly tackles the challenge of aligning heterogeneous modal embeddings, a crucial aspect overlooked by previous entity-level NS methods.\n        *   Offers a lightweight and efficient alternative to complex, computationally expensive NS methods, making MMKGE training more practical.\n    *   **Potential Impact on Future Research**:\n        *   Provides a foundational NS strategy that can potentially improve the performance and robustness of future MMKGE models.\n        *   The concept of modal-aware sampling could be extended to other multi-modal learning tasks beyond KGE.\n        *   The adaptive sampling mechanism could inspire further research into self-tuning or context-aware training strategies in complex embedding scenarios.\n        *   Encourages deeper investigation into the interplay between negative sampling strategies and modality alignment in multi-modal representation learning.",
        "keywords": [
          "Multi-modal Knowledge Graph Embedding (MMKGE)",
          "Negative Sampling (NS)",
          "Modality-Aware Negative Sampling (MANS)",
          "Modal-level sampling",
          "Visual Negative Sampling (MANS-V)",
          "Modality alignment",
          "Adaptive sampling mechanism",
          "Lightweight design",
          "Knowledge Graph Completion (KGC)",
          "Link prediction",
          "Triple classification",
          "Heterogeneous embeddings",
          "Computational efficiency",
          "Semantic embedding learning"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "5dc88d795cbcd01e6e99ba673e91e9024f0c3318.pdf"
    },
    {
      "success": true,
      "doc_id": "734818fe1daf55997d21007e38943311",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{li2021}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: While Knowledge Graph Embedding (KGE) models demonstrate impressive extrapolation ability (predicting unseen triples), existing works primarily focus on designing triple modeling functions and offer limited explanation for *why* KGE models extrapolate and *what factors* contribute to this ability.\n    *   **Importance and Challenge**: Understanding the underlying mechanisms of KGE extrapolation is crucial for designing models with better generalization capabilities. This problem is challenging because KGE involves a matching task between a query `(h, r, ?)` and a tail entity `t`, with three mutually influencing targets, which differs significantly from typical machine learning extrapolation studies focused on classification or regression. Knowledge Graphs also possess rich data patterns and interdependencies that need to be considered.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Current KGE models (Translational Distance, Semantic Matching, GNN-based) achieve success in Knowledge Graph Completion, including extrapolation, but lack explicit explanations for their generalization power.\n    *   **Limitations of Previous Solutions**: Studies on generalization/extrapolation in Machine Learning Theory (e.g., for MLPs or GNNs) primarily focus on classification or regression tasks with single objects/distributions. Their conclusions cannot directly apply to KGE due to its unique triple-matching nature and the complex interdependencies within Knowledge Graphs. This paper positions itself by taking a \"data relevant and model independent view\" to study KGE extrapolation, focusing on intrinsic data patterns.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (Problem 1: How KGE extrapolates)**: The paper proposes three levels of \"Semantic Evidence\" (SEs) observable from the training set that provide crucial semantic information for extrapolation:\n        *   **Relation-level SE (`Srel`)**: Quantified by the co-occurrence frequency of a relation `r` and an entity `t` in training triples.\n        *   **Entity-level SE (`Sent`)**: Quantified by the number of direct or indirect (up to length 2) path connections between `h` and `t` in the training set.\n        *   **Triple-level SE (`Stri`)**: Quantified by the similarity between `t` and other ground truth entities `t'` for the same `(h, r, ?)` query in the training set, where entity similarity is based on shared neighbor entity-relation pairs.\n    *   **Core Technical Method (Problem 2: How to design better KGE models)**: Based on the identified SEs, the paper proposes **Semantic Evidence aware Graph Neural Network (SE-GNN)**. SE-GNN explicitly models each level of SE using distinct neighbor patterns and aggregates them through a multi-layer GNN mechanism. Each SE type (relation, entity, triple) has a dedicated aggregation function with attention mechanisms to capture its specific contribution.\n    *   **Novelty/Difference**: This is the first work to systematically explore KGE extrapolation from a data-relevant and model-independent perspective. It introduces the novel concept of \"Semantic Evidence\" to explain extrapolation and proposes SE-GNN, a novel GNN-based KGE model that *explicitly* and *sufficiently* integrates these three levels of SE, unlike previous implicit approaches.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Insights/Analysis**: Identifies and formalizes three levels of Semantic Evidence (relation, entity, triple) as fundamental factors explaining the extrapolation ability of KGE models. Empirically demonstrates a strong, consistent correlation between the strength of these SEs and KGE model performance on unseen data, irrespective of the specific KGE method.\n    *   **Novel Algorithms, Methods, or Techniques**: Introduces quantitative metrics for each of the three Semantic Evidence types. Proposes **SE-GNN**, a novel GNN-based KGE model that explicitly captures and integrates these three levels of Semantic Evidence through distinct neighbor aggregation patterns and multi-layer GNN architecture, including specific attention mechanisms for each SE type.\n    *   **System Design or Architectural Innovations**: The architectural design of SE-GNN, which dedicates specific aggregation functions and attention mechanisms to model relation-level, entity-level, and triple-level SEs, and then merges them to produce more extrapolative knowledge representations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **SE Concept Verification**: Evaluated six diverse KGE models (TransE, RotatE, DistMult, ComplEx, ConvE, CompGCN) on FB15k-237 and WN18RR datasets. Test data was partitioned into low, medium, and high ranges based on the quantified strength of each SE, and model performance was analyzed within these ranges.\n        *   **SE-GNN Performance**: Compared SE-GNN against state-of-the-art KGE models on the Knowledge Graph Completion task using FB15k-237 and WN18RR datasets.\n    *   **Key Performance Metrics**: Mean Rank (MR) for KGE prediction (lower values indicate better performance).\n    *   **Comparison Results**:\n        *   **SE Verification**: All tested KGE models consistently exhibited significantly better prediction results (lower Mean Rank) as the Semantic Evidence strength increased across all three SE levels. This strong correlation was observed on both datasets, validating the proposed SE concept.\n        *   **SE-GNN**: Achieved state-of-the-art performance on the Knowledge Graph Completion task and demonstrated superior extrapolation ability compared to existing methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: For simplicity, the entity-level SE (`Sent`) was limited to path lengths of up to two. The definition of \"unseen data\" refers to new triple combinations, assuming that all entities and relations in the test set have appeared in the training set to allow for embedding learning.\n    *   **Scope of Applicability**: The research primarily focuses on the Knowledge Graph Completion task and is applicable to KGE models aiming to improve their ability to extrapolate to unseen triples involving existing entities and relations.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This work provides the first systematic, data-centric explanation for the impressive extrapolation ability of KGE models, shifting the focus beyond just architectural innovations. It introduces a novel conceptual framework (Semantic Evidence) for understanding KGE generalization and proposes SE-GNN, a new KGE model that explicitly leverages these insights to achieve state-of-the-art performance in KGC and superior extrapolation.\n    *   **Potential Impact on Future Research**: The findings open new avenues for designing more robust and generalizable KGE models by explicitly considering and modeling different types of semantic evidence. It encourages further research into data-driven explanations for KGE performance, which could lead to more effective KGE solutions for real-world applications where unseen facts are prevalent.",
      "intriguing_abstract": "The impressive extrapolation ability of Knowledge Graph Embedding (KGE) models, crucial for predicting unseen facts, remains largely unexplained, hindering the design of truly robust systems. Existing machine learning generalization theories fall short for KGE's unique triple-matching task. This paper demystifies KGE extrapolation by introducing the novel concept of **Semantic Evidence (SE)**: intrinsic, observable data patterns that fundamentally drive this capability. We identify and formalize three distinct levels of SErelation-level, entity-level, and triple-leveldemonstrating a strong, consistent correlation between their strength and KGE model performance across diverse architectures.\n\nBuilding on these insights, we propose **SE-GNN**, a novel Graph Neural Network specifically designed to explicitly capture and integrate these three types of Semantic Evidence through dedicated aggregation functions and attention mechanisms. SE-GNN achieves state-of-the-art performance in Knowledge Graph Completion and exhibits superior extrapolation ability compared to existing methods. Our work provides the first systematic, data-centric explanation for KGE generalization, offering a new paradigm for designing highly generalizable KGE models crucial for real-world applications.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "KGE Extrapolation",
        "Semantic Evidence (SE)",
        "Relation-level Semantic Evidence",
        "Entity-level Semantic Evidence",
        "Triple-level Semantic Evidence",
        "Semantic Evidence aware Graph Neural Network (SE-GNN)",
        "Graph Neural Networks (GNNs)",
        "Knowledge Graph Completion (KGC)",
        "Data-centric explanation",
        "Extrapolation mechanisms",
        "Unseen triples",
        "Generalization capabilities",
        "Correlation between SE strength and KGE performance",
        "State-of-the-art performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf",
      "citation_key": "li2021",
      "metadata": {
        "title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View",
        "authors": [
          "Ren Li",
          "Yanan Cao",
          "Qiannan Zhu",
          "Guanqun Bi",
          "Fang Fang",
          "Yi Liu",
          "Qian Li"
        ],
        "published_date": "2021",
        "abstract": "Knowledge Graph Embedding (KGE) aims to learn representations for entities and relations. Most KGE models have gained great success, especially on extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a trained model can still correctly predict t from (h, r, ?), or h from (?, r, t), such extrapolation ability is impressive. However, most existing KGE works focus on the design of delicate triple modeling function, which mainly tells us how to measure the plausibility of observed triples, but offers limited explanation of why the methods can extrapolate to unseen data, and what are the important factors to help KGE extrapolate. Therefore in this work, we attempt to study the KGE extrapolation of two problems: 1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with better extrapolation ability? \nFor the problem 1, we first discuss the impact factors for extrapolation and from relation, entity and triple level respectively, propose three Semantic Evidences (SEs), which can be observed from train set and provide important semantic information for extrapolation. Then we verify the effectiveness of SEs through extensive experiments on several typical KGE methods.\nFor the problem 2, to make better use of the three levels of SE, we propose a novel GNN-based KGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In SE-GNN, each level of SE is modeled explicitly by the corresponding neighbor pattern, and merged sufficiently by the multi-layer aggregation, which contributes to obtaining more extrapolative knowledge representation. \nFinally, through extensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task and performs a better extrapolation ability. Our code is available at https://github.com/renli1024/SE-GNN.",
        "file_path": "paper_data/knowledge_graph_embedding/0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{li2021}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: While Knowledge Graph Embedding (KGE) models demonstrate impressive extrapolation ability (predicting unseen triples), existing works primarily focus on designing triple modeling functions and offer limited explanation for *why* KGE models extrapolate and *what factors* contribute to this ability.\n    *   **Importance and Challenge**: Understanding the underlying mechanisms of KGE extrapolation is crucial for designing models with better generalization capabilities. This problem is challenging because KGE involves a matching task between a query `(h, r, ?)` and a tail entity `t`, with three mutually influencing targets, which differs significantly from typical machine learning extrapolation studies focused on classification or regression. Knowledge Graphs also possess rich data patterns and interdependencies that need to be considered.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Current KGE models (Translational Distance, Semantic Matching, GNN-based) achieve success in Knowledge Graph Completion, including extrapolation, but lack explicit explanations for their generalization power.\n    *   **Limitations of Previous Solutions**: Studies on generalization/extrapolation in Machine Learning Theory (e.g., for MLPs or GNNs) primarily focus on classification or regression tasks with single objects/distributions. Their conclusions cannot directly apply to KGE due to its unique triple-matching nature and the complex interdependencies within Knowledge Graphs. This paper positions itself by taking a \"data relevant and model independent view\" to study KGE extrapolation, focusing on intrinsic data patterns.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (Problem 1: How KGE extrapolates)**: The paper proposes three levels of \"Semantic Evidence\" (SEs) observable from the training set that provide crucial semantic information for extrapolation:\n        *   **Relation-level SE (`Srel`)**: Quantified by the co-occurrence frequency of a relation `r` and an entity `t` in training triples.\n        *   **Entity-level SE (`Sent`)**: Quantified by the number of direct or indirect (up to length 2) path connections between `h` and `t` in the training set.\n        *   **Triple-level SE (`Stri`)**: Quantified by the similarity between `t` and other ground truth entities `t'` for the same `(h, r, ?)` query in the training set, where entity similarity is based on shared neighbor entity-relation pairs.\n    *   **Core Technical Method (Problem 2: How to design better KGE models)**: Based on the identified SEs, the paper proposes **Semantic Evidence aware Graph Neural Network (SE-GNN)**. SE-GNN explicitly models each level of SE using distinct neighbor patterns and aggregates them through a multi-layer GNN mechanism. Each SE type (relation, entity, triple) has a dedicated aggregation function with attention mechanisms to capture its specific contribution.\n    *   **Novelty/Difference**: This is the first work to systematically explore KGE extrapolation from a data-relevant and model-independent perspective. It introduces the novel concept of \"Semantic Evidence\" to explain extrapolation and proposes SE-GNN, a novel GNN-based KGE model that *explicitly* and *sufficiently* integrates these three levels of SE, unlike previous implicit approaches.\n\n4.  **Key Technical Contributions**\n    *   **Theoretical Insights/Analysis**: Identifies and formalizes three levels of Semantic Evidence (relation, entity, triple) as fundamental factors explaining the extrapolation ability of KGE models. Empirically demonstrates a strong, consistent correlation between the strength of these SEs and KGE model performance on unseen data, irrespective of the specific KGE method.\n    *   **Novel Algorithms, Methods, or Techniques**: Introduces quantitative metrics for each of the three Semantic Evidence types. Proposes **SE-GNN**, a novel GNN-based KGE model that explicitly captures and integrates these three levels of Semantic Evidence through distinct neighbor aggregation patterns and multi-layer GNN architecture, including specific attention mechanisms for each SE type.\n    *   **System Design or Architectural Innovations**: The architectural design of SE-GNN, which dedicates specific aggregation functions and attention mechanisms to model relation-level, entity-level, and triple-level SEs, and then merges them to produce more extrapolative knowledge representations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **SE Concept Verification**: Evaluated six diverse KGE models (TransE, RotatE, DistMult, ComplEx, ConvE, CompGCN) on FB15k-237 and WN18RR datasets. Test data was partitioned into low, medium, and high ranges based on the quantified strength of each SE, and model performance was analyzed within these ranges.\n        *   **SE-GNN Performance**: Compared SE-GNN against state-of-the-art KGE models on the Knowledge Graph Completion task using FB15k-237 and WN18RR datasets.\n    *   **Key Performance Metrics**: Mean Rank (MR) for KGE prediction (lower values indicate better performance).\n    *   **Comparison Results**:\n        *   **SE Verification**: All tested KGE models consistently exhibited significantly better prediction results (lower Mean Rank) as the Semantic Evidence strength increased across all three SE levels. This strong correlation was observed on both datasets, validating the proposed SE concept.\n        *   **SE-GNN**: Achieved state-of-the-art performance on the Knowledge Graph Completion task and demonstrated superior extrapolation ability compared to existing methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: For simplicity, the entity-level SE (`Sent`) was limited to path lengths of up to two. The definition of \"unseen data\" refers to new triple combinations, assuming that all entities and relations in the test set have appeared in the training set to allow for embedding learning.\n    *   **Scope of Applicability**: The research primarily focuses on the Knowledge Graph Completion task and is applicable to KGE models aiming to improve their ability to extrapolate to unseen triples involving existing entities and relations.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This work provides the first systematic, data-centric explanation for the impressive extrapolation ability of KGE models, shifting the focus beyond just architectural innovations. It introduces a novel conceptual framework (Semantic Evidence) for understanding KGE generalization and proposes SE-GNN, a new KGE model that explicitly leverages these insights to achieve state-of-the-art performance in KGC and superior extrapolation.\n    *   **Potential Impact on Future Research**: The findings open new avenues for designing more robust and generalizable KGE models by explicitly considering and modeling different types of semantic evidence. It encourages further research into data-driven explanations for KGE performance, which could lead to more effective KGE solutions for real-world applications where unseen facts are prevalent.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "KGE Extrapolation",
          "Semantic Evidence (SE)",
          "Relation-level Semantic Evidence",
          "Entity-level Semantic Evidence",
          "Triple-level Semantic Evidence",
          "Semantic Evidence aware Graph Neural Network (SE-GNN)",
          "Graph Neural Networks (GNNs)",
          "Knowledge Graph Completion (KGC)",
          "Data-centric explanation",
          "Extrapolation mechanisms",
          "Unseen triples",
          "Generalization capabilities",
          "Correlation between SE strength and KGE performance",
          "State-of-the-art performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf"
    },
    {
      "success": true,
      "doc_id": "40cc27b465425a1e82a9e15120009c33",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional Knowledge Graph Embedding (KGE) models for medical knowledge graphs (KGs) primarily exploit simple structural features, neglecting complex structural information during feature learning. This limits their effectiveness for link prediction in medical decision-making and disease prediction.\n    *   **Importance & Challenge**: Accurate link prediction on medical KGs is crucial for enhancing medical decision-making and disease prediction. The challenge lies in effectively capturing complex, deeper structural features within these KGs, especially for deployment on resource-limited consumer electronics, while leveraging the potential of AI-generated content (AIGC) in healthcare electronics (HE).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon existing KGE models that use AI technology for link prediction.\n    *   **Limitations of Previous Solutions**: Traditional KGE completion models are limited by their focus on exploiting only simple structural features, failing to capture more informative, deeper structural patterns within triplets.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**: The paper proposes SEConv, a Knowledge Graph Embedding model enhanced with AIGC principles for medical knowledge graph completion.\n        *   It integrates a less resource-consuming self-attention mechanism to generate more expressive embedding representations.\n        *   It employs a multilayer convolutional neural network (CNN) to learn deeper and more informative structural features from triplets.\n    *   **Novelty/Difference**: SEConv's novelty lies in its dual approach: combining a resource-efficient self-attention mechanism for expressive embeddings with a multilayer CNN for extracting complex, deeper structural features. This addresses both the expressiveness of embeddings and the depth of feature learning, specifically tailored for medical KGs and resource-constrained environments, drawing inspiration from AIGC's potential.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of a less resource-consuming self-attention mechanism for generating highly expressive embedding representations, suitable for resource-limited consumer electronics.\n        *   Adoption of a multilayer convolutional neural network to extract deeper and more informative structural features from knowledge graph triplets.\n    *   **System Design/Architectural Innovations**: The SEConv model integrates these two components (self-attention and multilayer CNN) into a cohesive architecture for medical knowledge graph completion.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were implemented on the medical datasets UMLS and DBpedia50, along with two other benchmark datasets.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   SEConv excels in learning more expressive and discriminative feature representations.\n        *   It achieves a substantial improvement compared with baseline models.\n        *   The results verify its applicability for healthcare prediction tasks and smart healthcare treatments.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly focuses on the efficiency aspect by introducing a \"less resource-consuming\" self-attention mechanism, suggesting an assumption or design goal for deployment on resource-limited consumer electronics. Specific explicit limitations are not detailed in the provided text.\n    *   **Scope of Applicability**: Primarily focused on medical knowledge graph completion, link prediction for medical decision-making, disease prediction, and smart healthcare treatments. Its design also considers deployment on resource-limited consumer electronics.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: SEConv advances the technical state-of-the-art in KGE by effectively addressing the challenge of capturing complex structural features in medical KGs, which traditional models often neglect \\cite{yang2025}. It also contributes to making KGE models more deployable on resource-constrained devices through its efficient design.\n    *   **Potential Impact on Future Research**: This work opens avenues for future research in developing more sophisticated and resource-efficient KGE models for specialized domains like healthcare, particularly in integrating AIGC principles for enhanced feature learning and enabling smart healthcare applications on consumer electronics.",
      "intriguing_abstract": "Unlocking the full potential of medical knowledge graphs for precise disease prediction and smart healthcare demands models capable of discerning intricate structural patterns, a challenge traditional Knowledge Graph Embedding (KGE) approaches often fail to meet. This paper introduces SEConv, a novel KGE model inspired by AI-generated content (AIGC) principles, specifically designed to overcome these limitations. SEConv innovatively integrates a resource-efficient self-attention mechanism to generate highly expressive embedding representations, crucial for deployment on consumer electronics. Simultaneously, it employs a multilayer convolutional neural network to meticulously extract deeper, more informative structural features from medical triplets. Our experiments on benchmark medical datasets like UMLS and DBpedia50 demonstrate SEConv's superior ability to learn discriminative features, achieving substantial performance improvements over baselines. This breakthrough significantly advances link prediction capabilities, paving the way for enhanced medical decision-making and accessible smart healthcare solutions on resource-constrained devices.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "medical knowledge graphs",
        "link prediction",
        "SEConv model",
        "self-attention mechanism",
        "multilayer convolutional neural network (CNN)",
        "AI-generated content (AIGC) principles",
        "complex structural features",
        "resource-limited consumer electronics",
        "medical decision-making",
        "disease prediction",
        "expressive embedding representations",
        "smart healthcare treatments"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/33f3f53c957c4a8832b1dcb095a4ac967bd89897.pdf",
      "citation_key": "yang2025",
      "metadata": {
        "title": "A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction",
        "authors": [
          "Qingqing Yang",
          "Min He",
          "Zhongwen Li",
          "Tao He",
          "Seunggil Jeon"
        ],
        "published_date": "2025",
        "abstract": "AI technology has been often employed to establish knowledge graph embedding (KGE) model, which can be used for link prediction on medical knowledge graph to help medical decision-making and disease prediction. However, traditional knowledge graph completion models usually focus on exploiting simple structural features during the phase of feature learning while neglecting the complex structural feature. Considering AI-generated content (AIGC) has shown great potentials for healthcare electronics (HE), a knowledge graph embedding model with AIGC called SEConv is proposed for medical knowledge graph completion. Firstly, a less resource-consuming model of self-attention mechanism is introduced to generate more expressive embedding representations, which contributes to deploying on resource-limited consumer electronics. Secondly, in order to extract more informative features from the triplets, a multilayer convolutional neural network is adopted to learn deeper structural features. Experiments have been implemented on the medical dataset of UMLS and DBpedia50, and other two benchmark datasets. And the results show that SEConv excels in learning more expressive and discriminative feature representations. Compared with the baseline models, SEConv achieves a substantial improvement, which verifies it can be used for healthcare prediction task and smart healthcare treatments.",
        "file_path": "paper_data/knowledge_graph_embedding/33f3f53c957c4a8832b1dcb095a4ac967bd89897.pdf",
        "venue": "IEEE transactions on consumer electronics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional Knowledge Graph Embedding (KGE) models for medical knowledge graphs (KGs) primarily exploit simple structural features, neglecting complex structural information during feature learning. This limits their effectiveness for link prediction in medical decision-making and disease prediction.\n    *   **Importance & Challenge**: Accurate link prediction on medical KGs is crucial for enhancing medical decision-making and disease prediction. The challenge lies in effectively capturing complex, deeper structural features within these KGs, especially for deployment on resource-limited consumer electronics, while leveraging the potential of AI-generated content (AIGC) in healthcare electronics (HE).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon existing KGE models that use AI technology for link prediction.\n    *   **Limitations of Previous Solutions**: Traditional KGE completion models are limited by their focus on exploiting only simple structural features, failing to capture more informative, deeper structural patterns within triplets.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**: The paper proposes SEConv, a Knowledge Graph Embedding model enhanced with AIGC principles for medical knowledge graph completion.\n        *   It integrates a less resource-consuming self-attention mechanism to generate more expressive embedding representations.\n        *   It employs a multilayer convolutional neural network (CNN) to learn deeper and more informative structural features from triplets.\n    *   **Novelty/Difference**: SEConv's novelty lies in its dual approach: combining a resource-efficient self-attention mechanism for expressive embeddings with a multilayer CNN for extracting complex, deeper structural features. This addresses both the expressiveness of embeddings and the depth of feature learning, specifically tailored for medical KGs and resource-constrained environments, drawing inspiration from AIGC's potential.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of a less resource-consuming self-attention mechanism for generating highly expressive embedding representations, suitable for resource-limited consumer electronics.\n        *   Adoption of a multilayer convolutional neural network to extract deeper and more informative structural features from knowledge graph triplets.\n    *   **System Design/Architectural Innovations**: The SEConv model integrates these two components (self-attention and multilayer CNN) into a cohesive architecture for medical knowledge graph completion.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were implemented on the medical datasets UMLS and DBpedia50, along with two other benchmark datasets.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   SEConv excels in learning more expressive and discriminative feature representations.\n        *   It achieves a substantial improvement compared with baseline models.\n        *   The results verify its applicability for healthcare prediction tasks and smart healthcare treatments.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly focuses on the efficiency aspect by introducing a \"less resource-consuming\" self-attention mechanism, suggesting an assumption or design goal for deployment on resource-limited consumer electronics. Specific explicit limitations are not detailed in the provided text.\n    *   **Scope of Applicability**: Primarily focused on medical knowledge graph completion, link prediction for medical decision-making, disease prediction, and smart healthcare treatments. Its design also considers deployment on resource-limited consumer electronics.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: SEConv advances the technical state-of-the-art in KGE by effectively addressing the challenge of capturing complex structural features in medical KGs, which traditional models often neglect \\cite{yang2025}. It also contributes to making KGE models more deployable on resource-constrained devices through its efficient design.\n    *   **Potential Impact on Future Research**: This work opens avenues for future research in developing more sophisticated and resource-efficient KGE models for specialized domains like healthcare, particularly in integrating AIGC principles for enhanced feature learning and enabling smart healthcare applications on consumer electronics.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "medical knowledge graphs",
          "link prediction",
          "SEConv model",
          "self-attention mechanism",
          "multilayer convolutional neural network (CNN)",
          "AI-generated content (AIGC) principles",
          "complex structural features",
          "resource-limited consumer electronics",
          "medical decision-making",
          "disease prediction",
          "expressive embedding representations",
          "smart healthcare treatments"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "33f3f53c957c4a8832b1dcb095a4ac967bd89897.pdf"
    },
    {
      "success": true,
      "doc_id": "22b5caca0b97af919ab2cc42f55511f5",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### CoKE: Contextualized Knowledge Graph Embedding \\cite{wang2019}\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Traditional Knowledge Graph Embedding (KGE) methods assign a single, static vector representation to each entity and relation, ignoring their intrinsic contextual nature.\n    *   **Motivation:** Entities and relations exhibit different meanings and properties depending on the specific graph context (e.g., edges, paths, subgraphs) they appear in. For instance, \"Barack Obama\" has distinct political and family roles, and the relation \"HasPart\" can imply composition or location. Learning dynamic representations that capture these context-dependent meanings is a significant and challenging problem for KGE.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Traditional KGE:** Most models learn static, global representations solely from individual subject-relation-object triples (e.g., TransE, ComplEx).\n        *   **Beyond Triples:** Some methods incorporate richer graph structures like multi-hop paths or k-degree neighborhoods, but *still learn static global representations* for entities/relations.\n        *   **Previous Notions of Context:** Earlier work touched upon related phenomena, such as relation-specific entity projections (to handle 1-to-N relations) or polysemous relations (modeled as mixtures of Gaussians).\n    *   **Limitations of Previous Solutions:**\n        *   They fail to learn *dynamic, fully contextualized* representations where an entity's or relation's embedding adapts to its specific input graph context.\n        *   Previous \"contextual\" approaches were limited (e.g., relation-specific projections are static per relation, not dynamic per instance) and lacked a formal discussion of the intrinsic contextual nature of KGs.\n        *   While inspired by contextualized word embeddings, most graph embedding methods drawing from NLP still produce static embeddings.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** CoKE models entity and relation representations as a function of their *individual graph context*. It unifies graph contexts (edges and paths) as sequences of entities and relations. A stack of Transformer encoder blocks is then used to process these sequences.\n        *   Input representations for each element (`x_i`) in a sequence are formed by summing an element embedding (`x_ele_i`) and a position embedding (`x_pos_i`).\n        *   The Transformer's multi-head self-attention mechanism allows each element to attend to all other elements in the sequence, effectively capturing contextual dependencies.\n    *   **Novelty/Difference:**\n        *   **Dynamic, Contextualized Embeddings:** Unlike static embeddings, CoKE's representations are naturally adaptive to the input sequence, capturing the specific contextual meanings of entities and relations within that sequence.\n        *   **Sequence-based Context Modeling:** Formulating edges and paths as sequences allows leveraging powerful sequence modeling architectures like Transformer, inspired by advancements in contextualized word embeddings.\n        *   **Unified Training Task:** The model is trained via an entity prediction task (predicting a masked entity in an edge or path sequence), which directly aligns with downstream tasks like link prediction and path query answering, avoiding training-test discrepancy.\n\n4.  **Key Technical Contributions**\n    *   **Novel Paradigm:** Introduction of the concept of \"contextualized Knowledge Graph Embedding\" to explicitly address the dynamic, context-dependent nature of entities and relations in KGs \\cite{wang2019}.\n    *   **Algorithmic Innovation:** Devising CoKE, which leverages Transformer encoders to learn dynamic, flexible, and fully contextualized embeddings by treating graph contexts (edges and paths) as input sequences \\cite{wang2019}.\n    *   **System Design:** A unified framework that processes both single-hop (edges) and multi-hop (paths) contexts as sequences, enabling a consistent training and evaluation methodology for various KG tasks \\cite{wang2019}.\n    *   **Training Strategy:** An entity prediction task that directly mirrors downstream applications, ensuring learned representations are highly relevant and effective for tasks like link prediction and path query answering \\cite{wang2019}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Link Prediction:** Completing missing entities in triples (`?!r!o` or `s!r!?`).\n        *   **Path Query Answering:** Answering multi-hop queries (`s!r1!...!rk!?`).\n        *   **Parameter Efficiency Analysis:** Comparison of parameter counts with state-of-the-art (SOTA) models.\n        *   **Visualization:** Demonstrating CoKE's ability to discern fine-grained contextual meanings.\n    *   **Datasets:** Four widely used benchmarks: FB15k, WN18, FB15k-237, and WN18RR.\n    *   **Key Performance Metrics:** Mean Reciprocal Rank (MRR) and Hits@n (H@1, H@3, H@10) in a filtered setting.\n    *   **Comparison Results:**\n        *   **Link Prediction:** CoKE consistently outperforms or performs equally well as current SOTA methods (e.g., RotatE, TuckER, ConvR) on three out of four datasets (FB15k, FB15k-237, WN18RR) across almost all metrics, and achieves near-best results on WN18. It demonstrates superior stability compared to baselines.\n        *   **Path Query Answering:** Achieves a significant absolute improvement of up to 21.0% in H@10, highlighting its superior capability for multi-hop reasoning.\n        *   **Parameter Efficiency:** CoKE is parameter-efficient, achieving better or comparable results with fewer parameters than SOTA models like RotatE and TuckER, partly due to its ability to work well with a smaller embedding size (D=256).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:**\n        *   The current path formulation excludes intermediate entities from the path components, focusing on relations between the start and end entities. This simplifies the relationship to Horn clauses but might limit the richness of path contexts.\n        *   The model's maximum sequence length `K` for paths is a hyperparameter, which might limit the length of paths it can effectively model without increased computational cost.\n    *   **Scope of Applicability:**\n        *   Primarily focused on structured graph contexts (edges and paths).\n        *   Applicable to tasks that can be framed as entity prediction within a sequence, such as link prediction and path query answering.\n        *   The approach is generalizable to other sequence-based graph contexts, but the paper specifically investigates edges and paths.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:** CoKE establishes new state-of-the-art results in link prediction on several benchmarks and significantly improves performance in multi-hop path query answering, demonstrating superior reasoning capabilities.\n    *   **Paradigm Shift:** Introduces a novel paradigm for KG embedding by explicitly modeling the contextual nature of entities and relations, moving beyond static representations. This aligns KG embedding more closely with advancements in contextualized representations in NLP.\n    *   **Potential Impact on Future Research:**\n        *   Opens avenues for exploring more complex graph contexts (e.g., subgraphs, temporal contexts) using sequence-based or other context-aware neural architectures.\n        *   Encourages the development of KG embedding models that are more sensitive to the nuanced meanings of entities and relations in different scenarios.\n        *   The parameter efficiency of CoKE, despite using a Transformer, suggests that powerful contextual models can be developed without excessive computational overhead, making them practical for large KGs.\n        *   The success in path query answering highlights its potential for advanced knowledge graph reasoning and question answering systems.",
      "intriguing_abstract": "The static nature of traditional Knowledge Graph Embedding (KGE) models fundamentally limits their ability to capture the nuanced, context-dependent meanings of entities and relations within complex knowledge graphs. We introduce CoKE (Contextualized Knowledge Graph Embedding), a novel paradigm that moves beyond fixed representations by learning dynamic, fully contextualized embeddings.\n\nInspired by advancements in contextualized word embeddings, CoKE unifies diverse graph contextsfrom single edges to multi-hop pathsas sequences. It then leverages powerful Transformer encoder blocks with multi-head self-attention to dynamically adapt entity and relation representations based on their specific input sequence. This innovative approach enables CoKE to achieve new state-of-the-art performance in link prediction across multiple benchmarks and deliver substantial improvements in multi-hop path query answering, demonstrating superior reasoning capabilities. Crucially, CoKE achieves these gains with remarkable parameter efficiency. CoKE represents a significant paradigm shift, aligning KGE with the expressive power of contextualized representations. It paves the way for more sophisticated knowledge graph reasoning, question answering, and a deeper understanding of semantic nuances, making it a critical advancement for the field.",
      "keywords": [
        "Contextualized Knowledge Graph Embedding (CoKE)",
        "dynamic entity and relation representations",
        "Transformer encoder blocks",
        "multi-head self-attention",
        "sequence-based context modeling",
        "entity prediction task",
        "link prediction",
        "path query answering",
        "multi-hop reasoning",
        "state-of-the-art performance",
        "parameter efficiency",
        "unified framework",
        "context-dependent meanings"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/2e925a02db26a60ee1cc022f3923e09f3fae7b39.pdf",
      "citation_key": "wang2019",
      "metadata": {
        "title": "CoKE: Contextualized Knowledge Graph Embedding",
        "authors": [
          "Quan Wang",
          "Pingping Huang",
          "Haifeng Wang",
          "Songtai Dai",
          "Wenbin Jiang",
          "Jing Liu",
          "Yajuan Lyu",
          "Yong Zhu",
          "Hua Wu"
        ],
        "published_date": "2019",
        "abstract": "Knowledge graph embedding, which projects symbolic entities and relations into continuous vector spaces, is gaining increasing attention. Previous methods allow a single static embedding for each entity or relation, ignoring their intrinsic contextual nature, i.e., entities and relations may appear in different graph contexts, and accordingly, exhibit different properties. This work presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm that takes into account such contextual nature, and learns dynamic, flexible, and fully contextualized entity and relation embeddings. Two types of graph contexts are studied: edges and paths, both formulated as sequences of entities and relations. CoKE takes a sequence as input and uses a Transformer encoder to obtain contextualized representations. These representations are hence naturally adaptive to the input, capturing contextual meanings of entities and relations therein. Evaluation on a wide variety of public benchmarks verifies the superiority of CoKE in link prediction and path query answering. It performs consistently better than, or at least equally well as current state-of-the-art in almost every case, in particular offering an absolute improvement of 19.7% in H@10 on path query answering. Our code is available at \\url{this https URL}.",
        "file_path": "paper_data/knowledge_graph_embedding/2e925a02db26a60ee1cc022f3923e09f3fae7b39.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### CoKE: Contextualized Knowledge Graph Embedding \\cite{wang2019}\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Traditional Knowledge Graph Embedding (KGE) methods assign a single, static vector representation to each entity and relation, ignoring their intrinsic contextual nature.\n    *   **Motivation:** Entities and relations exhibit different meanings and properties depending on the specific graph context (e.g., edges, paths, subgraphs) they appear in. For instance, \"Barack Obama\" has distinct political and family roles, and the relation \"HasPart\" can imply composition or location. Learning dynamic representations that capture these context-dependent meanings is a significant and challenging problem for KGE.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Traditional KGE:** Most models learn static, global representations solely from individual subject-relation-object triples (e.g., TransE, ComplEx).\n        *   **Beyond Triples:** Some methods incorporate richer graph structures like multi-hop paths or k-degree neighborhoods, but *still learn static global representations* for entities/relations.\n        *   **Previous Notions of Context:** Earlier work touched upon related phenomena, such as relation-specific entity projections (to handle 1-to-N relations) or polysemous relations (modeled as mixtures of Gaussians).\n    *   **Limitations of Previous Solutions:**\n        *   They fail to learn *dynamic, fully contextualized* representations where an entity's or relation's embedding adapts to its specific input graph context.\n        *   Previous \"contextual\" approaches were limited (e.g., relation-specific projections are static per relation, not dynamic per instance) and lacked a formal discussion of the intrinsic contextual nature of KGs.\n        *   While inspired by contextualized word embeddings, most graph embedding methods drawing from NLP still produce static embeddings.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** CoKE models entity and relation representations as a function of their *individual graph context*. It unifies graph contexts (edges and paths) as sequences of entities and relations. A stack of Transformer encoder blocks is then used to process these sequences.\n        *   Input representations for each element (`x_i`) in a sequence are formed by summing an element embedding (`x_ele_i`) and a position embedding (`x_pos_i`).\n        *   The Transformer's multi-head self-attention mechanism allows each element to attend to all other elements in the sequence, effectively capturing contextual dependencies.\n    *   **Novelty/Difference:**\n        *   **Dynamic, Contextualized Embeddings:** Unlike static embeddings, CoKE's representations are naturally adaptive to the input sequence, capturing the specific contextual meanings of entities and relations within that sequence.\n        *   **Sequence-based Context Modeling:** Formulating edges and paths as sequences allows leveraging powerful sequence modeling architectures like Transformer, inspired by advancements in contextualized word embeddings.\n        *   **Unified Training Task:** The model is trained via an entity prediction task (predicting a masked entity in an edge or path sequence), which directly aligns with downstream tasks like link prediction and path query answering, avoiding training-test discrepancy.\n\n4.  **Key Technical Contributions**\n    *   **Novel Paradigm:** Introduction of the concept of \"contextualized Knowledge Graph Embedding\" to explicitly address the dynamic, context-dependent nature of entities and relations in KGs \\cite{wang2019}.\n    *   **Algorithmic Innovation:** Devising CoKE, which leverages Transformer encoders to learn dynamic, flexible, and fully contextualized embeddings by treating graph contexts (edges and paths) as input sequences \\cite{wang2019}.\n    *   **System Design:** A unified framework that processes both single-hop (edges) and multi-hop (paths) contexts as sequences, enabling a consistent training and evaluation methodology for various KG tasks \\cite{wang2019}.\n    *   **Training Strategy:** An entity prediction task that directly mirrors downstream applications, ensuring learned representations are highly relevant and effective for tasks like link prediction and path query answering \\cite{wang2019}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Link Prediction:** Completing missing entities in triples (`?!r!o` or `s!r!?`).\n        *   **Path Query Answering:** Answering multi-hop queries (`s!r1!...!rk!?`).\n        *   **Parameter Efficiency Analysis:** Comparison of parameter counts with state-of-the-art (SOTA) models.\n        *   **Visualization:** Demonstrating CoKE's ability to discern fine-grained contextual meanings.\n    *   **Datasets:** Four widely used benchmarks: FB15k, WN18, FB15k-237, and WN18RR.\n    *   **Key Performance Metrics:** Mean Reciprocal Rank (MRR) and Hits@n (H@1, H@3, H@10) in a filtered setting.\n    *   **Comparison Results:**\n        *   **Link Prediction:** CoKE consistently outperforms or performs equally well as current SOTA methods (e.g., RotatE, TuckER, ConvR) on three out of four datasets (FB15k, FB15k-237, WN18RR) across almost all metrics, and achieves near-best results on WN18. It demonstrates superior stability compared to baselines.\n        *   **Path Query Answering:** Achieves a significant absolute improvement of up to 21.0% in H@10, highlighting its superior capability for multi-hop reasoning.\n        *   **Parameter Efficiency:** CoKE is parameter-efficient, achieving better or comparable results with fewer parameters than SOTA models like RotatE and TuckER, partly due to its ability to work well with a smaller embedding size (D=256).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:**\n        *   The current path formulation excludes intermediate entities from the path components, focusing on relations between the start and end entities. This simplifies the relationship to Horn clauses but might limit the richness of path contexts.\n        *   The model's maximum sequence length `K` for paths is a hyperparameter, which might limit the length of paths it can effectively model without increased computational cost.\n    *   **Scope of Applicability:**\n        *   Primarily focused on structured graph contexts (edges and paths).\n        *   Applicable to tasks that can be framed as entity prediction within a sequence, such as link prediction and path query answering.\n        *   The approach is generalizable to other sequence-based graph contexts, but the paper specifically investigates edges and paths.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:** CoKE establishes new state-of-the-art results in link prediction on several benchmarks and significantly improves performance in multi-hop path query answering, demonstrating superior reasoning capabilities.\n    *   **Paradigm Shift:** Introduces a novel paradigm for KG embedding by explicitly modeling the contextual nature of entities and relations, moving beyond static representations. This aligns KG embedding more closely with advancements in contextualized representations in NLP.\n    *   **Potential Impact on Future Research:**\n        *   Opens avenues for exploring more complex graph contexts (e.g., subgraphs, temporal contexts) using sequence-based or other context-aware neural architectures.\n        *   Encourages the development of KG embedding models that are more sensitive to the nuanced meanings of entities and relations in different scenarios.\n        *   The parameter efficiency of CoKE, despite using a Transformer, suggests that powerful contextual models can be developed without excessive computational overhead, making them practical for large KGs.\n        *   The success in path query answering highlights its potential for advanced knowledge graph reasoning and question answering systems.",
        "keywords": [
          "Contextualized Knowledge Graph Embedding (CoKE)",
          "dynamic entity and relation representations",
          "Transformer encoder blocks",
          "multi-head self-attention",
          "sequence-based context modeling",
          "entity prediction task",
          "link prediction",
          "path query answering",
          "multi-hop reasoning",
          "state-of-the-art performance",
          "parameter efficiency",
          "unified framework",
          "context-dependent meanings"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "2e925a02db26a60ee1cc022f3923e09f3fae7b39.pdf"
    },
    {
      "success": true,
      "doc_id": "0952b1a058ad62ab998217f934264b70",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the limitations of existing Knowledge Graph (KG) embedding model searching methods \\cite{di2023}.\n    *   Current KG searching methods are restricted to a single KG form (e.g., only KGs, not NRD or HKG) and can only search within a single type of embedding model \\cite{di2023}.\n    *   Existing message function designs in Graph Neural Networks (GNNs) fix structures and operators, making them inflexible for handling diverse KG forms and datasets \\cite{di2023}. This inflexibility hinders data adaptability and performance.\n\n*   **Related Work & Positioning**\n    *   Existing KG searching methods aim to find suitable embedding models for a given KG dataset \\cite{di2023}.\n    *   This work positions itself as an advancement by overcoming the limitations of these methods, which are restricted to a single KG form and a single type of embedding model \\cite{di2023}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach is to build a search space for the message function within Graph Neural Networks (GNNs) \\cite{di2023}.\n    *   The innovation lies in designing a novel message function space that allows *both structures and operators* to be searched \\cite{di2023}. This contrasts with existing GNN message functions that fix these elements.\n\n*   **Key Technical Contributions**\n    *   **Novel Message Function Space:** Introduction of a flexible message function space that can adapt to different KG forms (KG, NRD, HKG) and specific datasets \\cite{di2023}.\n    *   **Unified Framework:** The proposed space is expressive enough to search for different types of embedding models, and can even instantiate existing message function designs and classic KG embedding models as special cases \\cite{di2023}.\n    *   **Data Adaptability:** The design enables the search for data-dependent message functions, promoting better adaptability and performance \\cite{di2023}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted on benchmark datasets for KGs, NRD (n-ary relational data), and HKGs (hyper-relational KGs) \\cite{di2023}.\n    *   **Key Results:** The empirically searched message functions were shown to be data-dependent and achieved leading performance across these diverse benchmark datasets \\cite{di2023}.\n\n*   **Limitations & Scope**\n    *   The paper implicitly addresses the limitations of prior KG searching methods and fixed GNN message functions by proposing a more flexible alternative \\cite{di2023}.\n    *   The scope of applicability covers various KG forms: traditional KGs, n-ary relational data (NRD), and hyper-relational KGs (HKG) \\cite{di2023}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing a unified and flexible framework for searching GNN message functions that can adapt to diverse knowledge graph structures \\cite{di2023}.\n    *   It offers a path towards more adaptable and high-performing KG embedding models by enabling the discovery of data-specific message functions, potentially impacting future research in automated machine learning for graph data and robust KG representation learning \\cite{di2023}.",
      "intriguing_abstract": "The promise of Knowledge Graph (KG) embeddings is often constrained by the rigid architectures of existing models, which struggle to adapt to the rich diversity of real-world knowledge. Current KG embedding search methods are confined to single KG forms and fixed Graph Neural Network (GNN) message functions, limiting their adaptability and performance. We introduce a groundbreaking paradigm: a novel search space for GNN message functions that, for the first time, allows for the dynamic discovery of *both structures and operators*. This unified framework transcends previous limitations, seamlessly adapting to traditional KGs, n-ary relational data (NRD), and hyper-relational KGs (HKG). It empowers the automatic discovery of data-dependent message functions, significantly enhancing model flexibility and performance. Empirical validation across diverse benchmark datasets demonstrates that our searched functions achieve leading performance, proving their superior adaptability. This work represents a significant leap in automated machine learning for graph data, paving the way for more robust and high-performing KG representation learning by enabling models to truly learn from the data's inherent complexity.",
      "keywords": [
        "Knowledge Graph embedding models",
        "Graph Neural Networks (GNNs)",
        "message function search space",
        "novel message function design",
        "searchable structures and operators",
        "unified framework",
        "data adaptability",
        "n-ary relational data (NRD)",
        "hyper-relational KGs (HKG)",
        "data-dependent message functions",
        "leading performance",
        "automated machine learning for graph data",
        "KG representation learning"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/040fe47af8f4870bf681f34861c42b3ea46d76cf.pdf",
      "citation_key": "di2023",
      "metadata": {
        "title": "Message Function Search for Knowledge Graph Embedding",
        "authors": [
          "Shimin Di",
          "Lei Chen"
        ],
        "published_date": "2023",
        "abstract": "Recently, many promising embedding models have been proposed to embed knowledge graphs (KGs) and their more general forms, such as n-ary relational data (NRD) and hyper-relational KG (HKG). To promote the data adaptability and performance of embedding models, KG searching methods propose to search for suitable models for a given KG data set. But they are restricted to a single KG form, and the searched models are restricted to a single type of embedding model. To tackle such issues, we propose to build a search space for the message function in graph neural networks (GNNs). However, it is a non-trivial task. Existing message function designs fix the structures and operators, which makes them difficult to handle different KG forms and data sets. Therefore, we first design a novel message function space, which enables both structures and operators to be searched for the given KG form (including KG, NRD, and HKG) and data. The proposed space can flexibly take different KG forms as inputs and is expressive to search for different types of embedding models. Especially, some existing message function designs and some classic KG embedding models can be instantiated as special cases of our space. We empirically show that the searched message functions are data-dependent, and can achieve leading performance on benchmark KGs, NRD, and HKGs.",
        "file_path": "paper_data/knowledge_graph_embedding/040fe47af8f4870bf681f34861c42b3ea46d76cf.pdf",
        "venue": "The Web Conference",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the limitations of existing Knowledge Graph (KG) embedding model searching methods \\cite{di2023}.\n    *   Current KG searching methods are restricted to a single KG form (e.g., only KGs, not NRD or HKG) and can only search within a single type of embedding model \\cite{di2023}.\n    *   Existing message function designs in Graph Neural Networks (GNNs) fix structures and operators, making them inflexible for handling diverse KG forms and datasets \\cite{di2023}. This inflexibility hinders data adaptability and performance.\n\n*   **Related Work & Positioning**\n    *   Existing KG searching methods aim to find suitable embedding models for a given KG dataset \\cite{di2023}.\n    *   This work positions itself as an advancement by overcoming the limitations of these methods, which are restricted to a single KG form and a single type of embedding model \\cite{di2023}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach is to build a search space for the message function within Graph Neural Networks (GNNs) \\cite{di2023}.\n    *   The innovation lies in designing a novel message function space that allows *both structures and operators* to be searched \\cite{di2023}. This contrasts with existing GNN message functions that fix these elements.\n\n*   **Key Technical Contributions**\n    *   **Novel Message Function Space:** Introduction of a flexible message function space that can adapt to different KG forms (KG, NRD, HKG) and specific datasets \\cite{di2023}.\n    *   **Unified Framework:** The proposed space is expressive enough to search for different types of embedding models, and can even instantiate existing message function designs and classic KG embedding models as special cases \\cite{di2023}.\n    *   **Data Adaptability:** The design enables the search for data-dependent message functions, promoting better adaptability and performance \\cite{di2023}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted on benchmark datasets for KGs, NRD (n-ary relational data), and HKGs (hyper-relational KGs) \\cite{di2023}.\n    *   **Key Results:** The empirically searched message functions were shown to be data-dependent and achieved leading performance across these diverse benchmark datasets \\cite{di2023}.\n\n*   **Limitations & Scope**\n    *   The paper implicitly addresses the limitations of prior KG searching methods and fixed GNN message functions by proposing a more flexible alternative \\cite{di2023}.\n    *   The scope of applicability covers various KG forms: traditional KGs, n-ary relational data (NRD), and hyper-relational KGs (HKG) \\cite{di2023}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing a unified and flexible framework for searching GNN message functions that can adapt to diverse knowledge graph structures \\cite{di2023}.\n    *   It offers a path towards more adaptable and high-performing KG embedding models by enabling the discovery of data-specific message functions, potentially impacting future research in automated machine learning for graph data and robust KG representation learning \\cite{di2023}.",
        "keywords": [
          "Knowledge Graph embedding models",
          "Graph Neural Networks (GNNs)",
          "message function search space",
          "novel message function design",
          "searchable structures and operators",
          "unified framework",
          "data adaptability",
          "n-ary relational data (NRD)",
          "hyper-relational KGs (HKG)",
          "data-dependent message functions",
          "leading performance",
          "automated machine learning for graph data",
          "KG representation learning"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "040fe47af8f4870bf681f34861c42b3ea46d76cf.pdf"
    },
    {
      "success": true,
      "doc_id": "6a876685204f8774eb918ff3c140c17f",
      "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Existing knowledge graph embedding methods (e.g., TransE, TransH, TransR) suffer from limited performance because they rely on a global, fixed margin in their loss functions. This margin is determined experimentally from a closed set of candidates.\n*   **Importance & Challenge:** Knowledge graphs are fundamental for applications like link prediction, web search, and question answering. The challenge lies in developing embedding methods that can adapt to the unique characteristics (locality) of different knowledge graphs and remain up-to-date as graphs evolve, rather than using a one-size-fits-all, static approach. The fixed global margin ignores the local properties of different graphs, leading to suboptimal embedding performance.\n\n**2. Related Work & Positioning**\n*   **Existing Approaches:** The paper positions its work against existing knowledge graph embedding methods such as TransE, TransH, and TransR. These methods learn embeddings by defining a global margin-based loss function.\n*   **Limitations of Previous Solutions:**\n    *   The loss function's parameters (specifically the margin) are determined through experiments, chosen from a predefined, closed set of candidates.\n    *   These methods apply the same set of candidates and a global margin across different knowledge graphs, disregarding the unique entities and relations (i.e., the \"locality\") of each graph.\n    *   This lack of adaptability and locality awareness results in limited performance for embedding-related applications.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper proposes TransA (Locally Adaptive Translation method for Knowledge Graph Embedding). TransA aims to find an optimal loss function by adaptively determining its margin based on the specific characteristics of different knowledge graphs.\n*   **Novelty/Difference:**\n    *   **Adaptive Margin Determination:** Unlike prior methods with fixed, global margins, TransA dynamically adjusts the margin of the loss function, making it locally adaptive to the specific knowledge graph.\n    *   **Locality Awareness:** It explicitly addresses the limitation of ignoring graph locality by adapting the margin over different knowledge graphs.\n    *   **Incremental Learning (iTransA):** To handle dynamic knowledge graphs, an incremental algorithm called iTransA is introduced. iTransA adaptively adjusts the optimal margin over time, allowing the embedding methods to remain current when new vertices and edges are added without full re-training.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods:**\n    *   **TransA:** A novel locally adaptive translation method for knowledge graph embedding that adaptively determines the margin of the loss function, improving performance by considering graph locality \\cite{jia2017}.\n    *   **iTransA:** An incremental algorithm for TransA, designed to efficiently update knowledge graph embeddings in dynamic environments where new entities and relations are frequently added \\cite{jia2017}.\n*   **Theoretical Insights:**\n    *   The convergence of TransA is formally verified through an analysis of its uniform stability \\cite{jia2017}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:** The proposed method, TransA, was evaluated through experiments on four benchmark datasets.\n*   **Key Performance Metrics & Comparison Results:** The experiments demonstrated the \"superiority\" of TransA when compared to state-of-the-art knowledge graph embedding methods \\cite{jia2017}. While specific metrics are not detailed in the provided text, \"superiority\" implies improved performance on standard evaluation tasks such as link prediction or triple classification.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:** The provided text does not explicitly state technical limitations or assumptions of TransA or iTransA.\n*   **Scope of Applicability:** The methods are primarily applicable to large-scale knowledge graph embedding tasks, particularly where the adaptability of the embedding model to different graph structures and the ability to handle dynamic updates are crucial.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** TransA significantly advances the technical state-of-the-art in knowledge graph embedding by introducing an adaptive margin mechanism, directly addressing a key limitation of previous translation-based models that relied on fixed, global margins \\cite{jia2017}.\n*   **Potential Impact on Future Research:** The introduction of iTransA, an incremental learning algorithm, is particularly impactful. It paves the way for more practical and efficient knowledge graph embedding in real-world, dynamic scenarios where knowledge graphs are constantly evolving, reducing the computational cost of re-training and enabling continuous learning. This could inspire further research into adaptive and incremental learning for other graph-based machine learning tasks.",
      "intriguing_abstract": "The efficacy of knowledge graph embedding (KGE) is often hampered by a critical oversight: the static, global margin used in conventional loss functions. Existing translation-based methods like TransE, TransH, and TransR, while foundational, fail to account for the unique local characteristics of diverse knowledge graphs, leading to suboptimal performance. We introduce **TransA**, a novel Locally Adaptive Translation method that fundamentally redefines KGE by dynamically determining the optimal margin for its loss function, making it inherently locality-aware. This adaptive mechanism significantly enhances embedding quality.\n\nFurthermore, to tackle the challenge of evolving knowledge graphs, we propose **iTransA**, an incremental algorithm that efficiently updates embeddings and adjusts the margin over time, eliminating the need for costly full re-training when new entities and relations emerge. Our theoretical analysis verifies TransA's convergence, and extensive experiments on benchmark datasets demonstrate its superior performance against state-of-the-art KGE models. TransA and iTransA offer a powerful, practical paradigm for robust, efficient, and continuously learning knowledge graph embeddings, paving the way for more accurate and adaptable AI systems in dynamic real-world environments.",
      "keywords": [
        "Knowledge graph embedding",
        "adaptive margin",
        "TransA",
        "iTransA",
        "locality awareness",
        "dynamic knowledge graphs",
        "incremental learning",
        "loss function optimization",
        "uniform stability",
        "link prediction",
        "state-of-the-art advancement",
        "continuous learning"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/c762e198b0239313ee50476021b1939390c4ef9d.pdf",
      "citation_key": "jia2017",
      "metadata": {
        "title": "Knowledge Graph Embedding",
        "authors": [
          "Yantao Jia",
          "Yuanzhuo Wang",
          "Xiaolong Jin",
          "Hailun Lin",
          "Xueqi Cheng"
        ],
        "published_date": "2017",
        "abstract": "A knowledge graph is a graph with entities of different types as nodes and various relations among them as edges. The construction of knowledge graphs in the past decades facilitates many applications, such as link prediction, web search analysis, question answering, and so on. Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, for example, TransE, TransH, and TransR, learn the embedding representation by defining a global margin-based loss function over the data. However, the loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidates, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this article, a locally adaptive translation method for knowledge graph embedding, called TransA, is proposed to find the loss function by adaptively determining its margin over different knowledge graphs. Then the convergence of TransA is verified from the aspect of its uniform stability. To make the embedding methods up-to-date when new vertices and edges are added into the knowledge graph, the incremental algorithm for TransA, called iTransA, is proposed by adaptively adjusting the optimal margin over time. Experiments on four benchmark data sets demonstrate the superiority of the proposed method, as compared to the state-of-the-art ones.",
        "file_path": "paper_data/knowledge_graph_embedding/c762e198b0239313ee50476021b1939390c4ef9d.pdf",
        "venue": "ACM Transactions on the Web",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Existing knowledge graph embedding methods (e.g., TransE, TransH, TransR) suffer from limited performance because they rely on a global, fixed margin in their loss functions. This margin is determined experimentally from a closed set of candidates.\n*   **Importance & Challenge:** Knowledge graphs are fundamental for applications like link prediction, web search, and question answering. The challenge lies in developing embedding methods that can adapt to the unique characteristics (locality) of different knowledge graphs and remain up-to-date as graphs evolve, rather than using a one-size-fits-all, static approach. The fixed global margin ignores the local properties of different graphs, leading to suboptimal embedding performance.\n\n**2. Related Work & Positioning**\n*   **Existing Approaches:** The paper positions its work against existing knowledge graph embedding methods such as TransE, TransH, and TransR. These methods learn embeddings by defining a global margin-based loss function.\n*   **Limitations of Previous Solutions:**\n    *   The loss function's parameters (specifically the margin) are determined through experiments, chosen from a predefined, closed set of candidates.\n    *   These methods apply the same set of candidates and a global margin across different knowledge graphs, disregarding the unique entities and relations (i.e., the \"locality\") of each graph.\n    *   This lack of adaptability and locality awareness results in limited performance for embedding-related applications.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper proposes TransA (Locally Adaptive Translation method for Knowledge Graph Embedding). TransA aims to find an optimal loss function by adaptively determining its margin based on the specific characteristics of different knowledge graphs.\n*   **Novelty/Difference:**\n    *   **Adaptive Margin Determination:** Unlike prior methods with fixed, global margins, TransA dynamically adjusts the margin of the loss function, making it locally adaptive to the specific knowledge graph.\n    *   **Locality Awareness:** It explicitly addresses the limitation of ignoring graph locality by adapting the margin over different knowledge graphs.\n    *   **Incremental Learning (iTransA):** To handle dynamic knowledge graphs, an incremental algorithm called iTransA is introduced. iTransA adaptively adjusts the optimal margin over time, allowing the embedding methods to remain current when new vertices and edges are added without full re-training.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods:**\n    *   **TransA:** A novel locally adaptive translation method for knowledge graph embedding that adaptively determines the margin of the loss function, improving performance by considering graph locality \\cite{jia2017}.\n    *   **iTransA:** An incremental algorithm for TransA, designed to efficiently update knowledge graph embeddings in dynamic environments where new entities and relations are frequently added \\cite{jia2017}.\n*   **Theoretical Insights:**\n    *   The convergence of TransA is formally verified through an analysis of its uniform stability \\cite{jia2017}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:** The proposed method, TransA, was evaluated through experiments on four benchmark datasets.\n*   **Key Performance Metrics & Comparison Results:** The experiments demonstrated the \"superiority\" of TransA when compared to state-of-the-art knowledge graph embedding methods \\cite{jia2017}. While specific metrics are not detailed in the provided text, \"superiority\" implies improved performance on standard evaluation tasks such as link prediction or triple classification.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:** The provided text does not explicitly state technical limitations or assumptions of TransA or iTransA.\n*   **Scope of Applicability:** The methods are primarily applicable to large-scale knowledge graph embedding tasks, particularly where the adaptability of the embedding model to different graph structures and the ability to handle dynamic updates are crucial.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** TransA significantly advances the technical state-of-the-art in knowledge graph embedding by introducing an adaptive margin mechanism, directly addressing a key limitation of previous translation-based models that relied on fixed, global margins \\cite{jia2017}.\n*   **Potential Impact on Future Research:** The introduction of iTransA, an incremental learning algorithm, is particularly impactful. It paves the way for more practical and efficient knowledge graph embedding in real-world, dynamic scenarios where knowledge graphs are constantly evolving, reducing the computational cost of re-training and enabling continuous learning. This could inspire further research into adaptive and incremental learning for other graph-based machine learning tasks.",
        "keywords": [
          "Knowledge graph embedding",
          "adaptive margin",
          "TransA",
          "iTransA",
          "locality awareness",
          "dynamic knowledge graphs",
          "incremental learning",
          "loss function optimization",
          "uniform stability",
          "link prediction",
          "state-of-the-art advancement",
          "continuous learning"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "c762e198b0239313ee50476021b1939390c4ef9d.pdf"
    },
    {
      "success": true,
      "doc_id": "dd87980381f345088c35bb6c4ae68b2a",
      "summary": "Here is a focused summary of the survey paper by \\cite{choudhary2021} for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey paper covers the domain of Knowledge Graph Embedding (KGE) techniques and their diverse applications.\n    *   Its main objectives are to trace the evolution of KGE models from simple translation-based approaches to more enriched representations, including those incorporating textual and multi-modal data, and to highlight their utility in various real-world applications.\n\n2.  **Literature Coverage**\n    *   The paper reviews literature spanning from \"earlier works\" in Knowledge Graph creation (e.g., YAGO, Freebase) and the foundational concepts of KGE (e.g., TransE) up to \"recent research\" incorporating multi-modal and contextual information (e.g., Graph Attention Networks, multi-modal graphs, papers from 2019-2020).\n    *   The selection criteria focus on categorizing KGE methods into translation-based and semantic matching models, and further discussing their extension to include enriched representations from textual and multi-modal data.\n\n3.  **Classification Framework**\n    *   **Translation Models**: These models represent relations as translations between entity embeddings in a continuous vector space, using distance-based scoring functions (e.g., TransE, TransH, TransR, RotatE, HakE).\n    *   **Semantic Matching Models**: These models employ similarity-based scoring functions, often leveraging tensor factorization or bilinear forms to capture interactions between entities and relations (e.g., RESCAL, TATEC, DistMult, HolE, ComplEx, ANALOGY).\n    *   **Enriched Representations**: The survey also discusses the integration of textual, image-based, and contextual information to enhance entity embeddings, moving beyond purely structure-based approaches.\n\n4.  **Key Findings & Insights**\n    *   The field of KGE has evolved significantly, addressing limitations of early models (e.g., TransE's struggle with complex relations) through more sophisticated approaches that handle diverse relational patterns (e.g., RotatE for symmetric/compositional relations) and semantic hierarchies (e.g., HakE).\n    *   Semantic matching models have progressed from computationally intensive tensor factorization (RESCAL) to more efficient and robust methods that mitigate overfitting (TATEC) and effectively model asymmetric relations (HolE, ComplEx).\n    *   A major trend is the shift from solely structure-based embeddings to incorporating richer, multi-modal information (text, images) and contextual data, leading to more comprehensive entity representations.\n    *   Different KGE models present trade-offs between expressivity, computational complexity, and their ability to capture specific types of relational semantics.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey identifies a historical gap where most KGE methods primarily focused on structure-based information, often neglecting other rich data modalities.\n    *   Future research directions emphasize the need for more effective integration of text-based, image-based, and other multi-modal information into entity embeddings.\n    *   Enhancing representations with context information is highlighted as a crucial area for further development to improve the performance of KGE in real-world applications.\n\n6.  **Survey Contribution**\n    *   This survey by \\cite{choudhary2021} provides a valuable, structured overview of the growth and evolution of Knowledge Graph Embedding, systematically categorizing models from foundational translation-based approaches to advanced semantic matching and enriched representation techniques.\n    *   It offers a comprehensive perspective on how KGE methods have evolved to address limitations and incorporate diverse data types, making it a useful resource for understanding the current landscape and future trajectory of KGE research.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graphs (KGs) hinges on their effective representation. As cornerstones of modern AI, KGs demand sophisticated embedding techniques to capture their rich, interconnected data. This comprehensive survey meticulously traces the evolution of Knowledge Graph Embedding (KGE) models, systematically categorizing them from foundational translation-based approaches (e.g., TransE, RotatE) to sophisticated semantic matching models (e.g., ComplEx, HolE).\n\nCrucially, we highlight the paradigm shift towards enriched entity embeddings that seamlessly integrate textual, image-based, and contextual information, moving beyond purely structural data. By dissecting their strengths, limitations, and trade-offs, we reveal how KGEs have advanced to model complex relational patterns and semantic hierarchies. We identify a historical gap in multi-modal integration and underscore the urgent need for future research to enhance context-aware and multi-modal KGEs, paving the way for more robust and intelligent AI applications. This paper offers a vital resource for understanding the current landscape and future trajectory of KGE research.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Translation models",
        "Semantic matching models",
        "Entity embeddings",
        "Multi-modal data integration",
        "Enriched representations",
        "Evolution of KGE models",
        "Relational patterns",
        "Structure-based embeddings",
        "Research gaps and future directions",
        "Textual and contextual information",
        "Scoring functions",
        "Real-world applications"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/1f20378d2820fdf1c1bb09ce22f739ab77b14e82.pdf",
      "citation_key": "choudhary2021",
      "metadata": {
        "title": "A Survey of Knowledge Graph Embedding and Their Applications",
        "authors": [
          "Shivani Choudhary",
          "Tarun Luthra",
          "Ashima Mittal",
          "Rajat Singh"
        ],
        "published_date": "2021",
        "abstract": "Knowledge Graph embedding provides a versatile technique for representing knowledge. These techniques can be used in a variety of applications such as completion of knowledge graph to predict missing information, recommender systems, question answering, query expansion, etc. The information embedded in Knowledge graph though being structured is challenging to consume in a real-world application. Knowledge graph embedding enables the real-world application to consume information to improve performance. Knowledge graph embedding is an active research area. Most of the embedding methods focus on structure-based information. Recent research has extended the boundary to include text-based information and image-based information in entity embedding. Efforts have been made to enhance the representation with context information. This paper introduces growth in the field of KG embedding from simple translation-based models to enrichment-based models. This paper includes the utility of the Knowledge graph in real-world applications.",
        "file_path": "paper_data/knowledge_graph_embedding/1f20378d2820fdf1c1bb09ce22f739ab77b14e82.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper by \\cite{choudhary2021} for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey paper covers the domain of Knowledge Graph Embedding (KGE) techniques and their diverse applications.\n    *   Its main objectives are to trace the evolution of KGE models from simple translation-based approaches to more enriched representations, including those incorporating textual and multi-modal data, and to highlight their utility in various real-world applications.\n\n2.  **Literature Coverage**\n    *   The paper reviews literature spanning from \"earlier works\" in Knowledge Graph creation (e.g., YAGO, Freebase) and the foundational concepts of KGE (e.g., TransE) up to \"recent research\" incorporating multi-modal and contextual information (e.g., Graph Attention Networks, multi-modal graphs, papers from 2019-2020).\n    *   The selection criteria focus on categorizing KGE methods into translation-based and semantic matching models, and further discussing their extension to include enriched representations from textual and multi-modal data.\n\n3.  **Classification Framework**\n    *   **Translation Models**: These models represent relations as translations between entity embeddings in a continuous vector space, using distance-based scoring functions (e.g., TransE, TransH, TransR, RotatE, HakE).\n    *   **Semantic Matching Models**: These models employ similarity-based scoring functions, often leveraging tensor factorization or bilinear forms to capture interactions between entities and relations (e.g., RESCAL, TATEC, DistMult, HolE, ComplEx, ANALOGY).\n    *   **Enriched Representations**: The survey also discusses the integration of textual, image-based, and contextual information to enhance entity embeddings, moving beyond purely structure-based approaches.\n\n4.  **Key Findings & Insights**\n    *   The field of KGE has evolved significantly, addressing limitations of early models (e.g., TransE's struggle with complex relations) through more sophisticated approaches that handle diverse relational patterns (e.g., RotatE for symmetric/compositional relations) and semantic hierarchies (e.g., HakE).\n    *   Semantic matching models have progressed from computationally intensive tensor factorization (RESCAL) to more efficient and robust methods that mitigate overfitting (TATEC) and effectively model asymmetric relations (HolE, ComplEx).\n    *   A major trend is the shift from solely structure-based embeddings to incorporating richer, multi-modal information (text, images) and contextual data, leading to more comprehensive entity representations.\n    *   Different KGE models present trade-offs between expressivity, computational complexity, and their ability to capture specific types of relational semantics.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey identifies a historical gap where most KGE methods primarily focused on structure-based information, often neglecting other rich data modalities.\n    *   Future research directions emphasize the need for more effective integration of text-based, image-based, and other multi-modal information into entity embeddings.\n    *   Enhancing representations with context information is highlighted as a crucial area for further development to improve the performance of KGE in real-world applications.\n\n6.  **Survey Contribution**\n    *   This survey by \\cite{choudhary2021} provides a valuable, structured overview of the growth and evolution of Knowledge Graph Embedding, systematically categorizing models from foundational translation-based approaches to advanced semantic matching and enriched representation techniques.\n    *   It offers a comprehensive perspective on how KGE methods have evolved to address limitations and incorporate diverse data types, making it a useful resource for understanding the current landscape and future trajectory of KGE research.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Translation models",
          "Semantic matching models",
          "Entity embeddings",
          "Multi-modal data integration",
          "Enriched representations",
          "Evolution of KGE models",
          "Relational patterns",
          "Structure-based embeddings",
          "Research gaps and future directions",
          "Textual and contextual information",
          "Scoring functions",
          "Real-world applications"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "1f20378d2820fdf1c1bb09ce22f739ab77b14e82.pdf"
    },
    {
      "success": true,
      "doc_id": "e4e8faa94a48d909727d06640bf6cf46",
      "summary": "Here's a focused summary of the technical paper \\cite{xiao2015} for a literature review:\n\n### Analysis of \"From One Point to A Manifold: Knowledge Graph Embedding For Precise Link Prediction\" \\cite{xiao2015}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing knowledge graph embedding (KGE) methods struggle with *precise link prediction*, which aims to find the *exact* missing entity given a head entity and a relation, rather than just a list of plausible candidates.\n    *   **Importance & Challenge:** Precise link prediction is critical for improving knowledge completion, enhancing knowledge reasoning, and boosting the performance of various knowledge-related AI tasks. The challenge arises from two fundamental issues in current KGE models:\n        *   **Ill-posed algebraic system:** Translation-based models (e.g., `h+r=t`) result in an algebraic system with significantly more equations (facts * embedding dimension) than free variables (entities + relations * embedding dimension), leading to imprecise and unstable solutions.\n        *   **Over-strict geometric form:** Existing methods typically map a \"golden\" (true) triple to a single point in the embedding space, which is too rigid, especially for complex relations like one-to-many or many-to-many, where multiple tail entities might be valid for a given head and relation.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** \\cite{xiao2015} primarily discusses translation-based methods like TransE \\cite{bordes2013}, TransH \\cite{wang2014}, TransR \\cite{lin2015b}, PTransE \\cite{lin2015a}, and KG2E \\cite{he2015}, which are state-of-the-art for general link prediction. Other methods like UM, SE, SME, SLM, LFM, NTN, and RESCAL are also mentioned.\n    *   **Limitations of Previous Solutions:** While these methods have achieved success, \\cite{xiao2015} argues that none of them explicitly address the problem of *precise* link prediction. Even methods like TransH and TransR, which project entities into relation-specific subspaces, still maintain an over-strict \"one-point\" geometric form within those subspaces, failing to adequately represent complex relations or resolve the ill-posed algebraic system.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{xiao2015} proposes **ManifoldE**, a manifold-based embedding principle that replaces the point-wise translation principle (`h+r=t`) with a manifold-wise principle `M(h,r,t) = D_r^2`. The score function measures the distance of a triple from this relation-specific manifold: `f_r(h,t) = ||M(h,r,t) - D_r^2||^2`.\n    *   **Novelty/Difference:**\n        *   **Manifold-based Modeling:** Instead of mapping true triples to a single point, \\cite{xiao2015} maps them to a *manifold* (e.g., a high-dimensional sphere or hyperplane) defined by the head entity and relation. This provides a more flexible geometric representation.\n        *   **Addressing Ill-posedness:** By treating `M(h,r,t) = D_r^2` as a single equation per fact, \\cite{xiao2015} transforms the embedding problem into a nearly well-posed algebraic system, especially when the embedding dimension `d` is chosen such that `d >= T/(E+R)` (where T is number of facts, E entities, R relations). This leads to more stable and precise solutions.\n        *   **Specific Manifold Implementations:**\n            *   **Sphere:** `M(h,r,t) = ||h+r - t||^2`, where `h+r` is the center and `D_r` is the radius. This is a direct generalization of TransE (where `D_r=0`). Kernel functions (Linear, Gaussian, Polynomial) can be applied for more expressive representations in a Reproducing Kernel Hilbert Space (RKHS).\n            *   **Hyperplane:** `M(h,r,t) = (h+r_head) * (t+r_tail)`. This allows for more intersections between manifolds, potentially offering more solutions. An \"absolute operator\" (`|h+r_head| * |t+r_tail|`) is introduced to further increase solution flexibility. Kernel functions can also be applied here.\n\n4.  **Key Technical Contributions**\n    *   **Novel Principle:** Introduction of the manifold-based embedding principle (ManifoldE) to address the limitations of point-wise modeling in KGE.\n    *   **Algebraic System Re-formulation:** Identification and mitigation of the ill-posed algebraic system problem in KGE by transforming it into a nearly well-posed system, leading to more stable and precise embeddings.\n    *   **Flexible Geometric Forms:** Expansion of the \"golden position\" from a single point to a high-dimensional manifold (sphere, hyperplane), better accommodating complex relation types (e.g., 1-N, N-N).\n    *   **Kernel Integration:** Application of kernel functions (e.g., Linear, Polynomial) within the manifold framework to enhance expressiveness.\n    *   **Absolute Operator for Hyperplane:** A novel operator to increase the number of potential solutions and improve embedding flexibility for the Hyperplane manifold.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** \\cite{xiao2015} evaluated ManifoldE on two tasks: Link Prediction and Triple Classification, using four public benchmark datasets: WN18, FB15K, WN11, and FB13 (subsets of Wordnet and Freebase). Visualization comparisons were also performed.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Link Prediction (HITS@N):**\n            *   **Overall Performance (HITS@10 Filter):** ManifoldE (both Sphere and Hyperplane variants) consistently achieved substantial improvements over state-of-the-art baselines (SE, TransE, TransH, TransR, KG2E). For instance, on WN18, ManifoldE Sphere reached 94.4% (vs. KG2E 92.8%), and on FB15K, ManifoldE Sphere reached 79.5% (vs. KG2E 74.0%).\n            *   **Precise Prediction (HITS@1 Filter):** This metric specifically validated the paper's core claim. ManifoldE showed significant gains, particularly for complex relations. On FB15K, for N-N relations, ManifoldE Hyperplane achieved 53.0% (predicting head) and 55.9% (predicting tail) HITS@1, dramatically outperforming TransE (18.1% head, 20.3% tail) and TransR (14.5% head, 16.2% tail).\n        *   **Efficiency:** ManifoldE demonstrated high efficiency, with training times comparable to TransE (the most efficient baseline) and significantly faster than TransR and KG2E. For example, on FB15K, ManifoldE Sphere took 0.7s, comparable to TransE's 0.7s, but much faster than TransR's 29.1s and KG2E's 44.2s.\n        *   **Visualization:** Visualizations (e.g., Figure 1) illustrated that ManifoldE effectively separates true and false triples, with true triples clustering within the defined manifold, unlike the chaotic distribution observed with TransE.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** While ManifoldE addresses the ill-posed algebraic system by making it \"nearly well-posed,\" it doesn't necessarily guarantee a perfectly well-posed system in all scenarios. The choice of manifold (Sphere vs. Hyperplane) and kernel function can impact performance, requiring careful tuning. The absolute operator, while increasing flexibility, might also introduce additional complexity in optimization.\n    *   **Scope of Applicability:** The method is primarily designed for knowledge graph embedding tasks, particularly link prediction and triple classification. Its direct applicability to other types of graph data or embedding problems might require adaptation. The focus is on improving precision for existing (h,r,t) triple structures.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{xiao2015} significantly advances the technical state-of-the-art in knowledge graph embedding by formally identifying and addressing the critical issues of ill-posed algebraic systems and over-strict geometric forms, which limit precise link prediction in previous models. The proposed ManifoldE principle offers a more robust and flexible framework.\n    *   **Potential Impact on Future Research:** This work opens new avenues for research in KGE by shifting the paradigm from point-wise to manifold-wise modeling. It encourages further exploration of different manifold types, advanced kernel functions, and more sophisticated algebraic treatments to achieve even higher precision and stability in knowledge representation. The emphasis on precise link prediction could also inspire new evaluation metrics and benchmarks for KGE models.",
      "intriguing_abstract": "Precise link prediction in knowledge graphs remains a critical challenge, hindering the full potential of knowledge completion and AI reasoning. Current knowledge graph embedding (KGE) methods, often relying on a rigid point-wise translation principle (e.g., h+r=t), suffer from an ill-posed algebraic system and an over-strict geometric form that fails to adequately capture complex relation types. We introduce **ManifoldE**, a novel manifold-based embedding principle that fundamentally re-imagines knowledge representation. Instead of mapping true triples to a single point, ManifoldE embeds them onto flexible, high-dimensional manifolds (e.g., spheres or hyperplanes) defined by the head entity and relation. This innovative approach transforms the embedding problem into a nearly well-posed algebraic system, leading to significantly more stable and precise solutions. By integrating kernel functions, ManifoldE further enhances expressiveness. Extensive experiments on WN18 and FB15K demonstrate ManifoldE's superior performance, achieving substantial improvements in precise link prediction (HITS@1) and overall link prediction compared to state-of-the-art baselines like TransE, TransH, and KG2E, particularly for complex N-N relations. ManifoldE offers a robust and efficient paradigm shift for knowledge graph embedding, paving the way for more accurate and reliable knowledge-driven AI applications.",
      "keywords": [
        "Knowledge graph embedding (KGE)",
        "precise link prediction",
        "ManifoldE",
        "manifold-based embedding",
        "ill-posed algebraic system",
        "over-strict geometric form",
        "Sphere manifold",
        "Hyperplane manifold",
        "kernel functions",
        "flexible geometric representation",
        "HITS@1",
        "knowledge completion",
        "triple classification",
        "state-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/991b64748dfeecf026a27030c16fe1743aa20167.pdf",
      "citation_key": "xiao2015",
      "metadata": {
        "title": "From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction",
        "authors": [
          "Han Xiao",
          "Minlie Huang",
          "Xiaoyan Zhu"
        ],
        "published_date": "2015",
        "abstract": "Knowledge graph embedding aims at offering a numerical knowledge representation paradigm by transforming the entities and relations into continuous vector space. However, existing methods could not characterize the knowledge graph in a fine degree to make a precise prediction. There are two reasons: being an ill-posed algebraic system and applying an overstrict geometric form. As precise prediction is critical, we propose an manifold-based embedding principle (\\textbf{ManifoldE}) which could be treated as a well-posed algebraic system that expands the position of golden triples from one point in current models to a manifold in ours. Extensive experiments show that the proposed models achieve substantial improvements against the state-of-the-art baselines especially for the precise prediction task, and yet maintain high efficiency.",
        "file_path": "paper_data/knowledge_graph_embedding/991b64748dfeecf026a27030c16fe1743aa20167.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper \\cite{xiao2015} for a literature review:\n\n### Analysis of \"From One Point to A Manifold: Knowledge Graph Embedding For Precise Link Prediction\" \\cite{xiao2015}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing knowledge graph embedding (KGE) methods struggle with *precise link prediction*, which aims to find the *exact* missing entity given a head entity and a relation, rather than just a list of plausible candidates.\n    *   **Importance & Challenge:** Precise link prediction is critical for improving knowledge completion, enhancing knowledge reasoning, and boosting the performance of various knowledge-related AI tasks. The challenge arises from two fundamental issues in current KGE models:\n        *   **Ill-posed algebraic system:** Translation-based models (e.g., `h+r=t`) result in an algebraic system with significantly more equations (facts * embedding dimension) than free variables (entities + relations * embedding dimension), leading to imprecise and unstable solutions.\n        *   **Over-strict geometric form:** Existing methods typically map a \"golden\" (true) triple to a single point in the embedding space, which is too rigid, especially for complex relations like one-to-many or many-to-many, where multiple tail entities might be valid for a given head and relation.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** \\cite{xiao2015} primarily discusses translation-based methods like TransE \\cite{bordes2013}, TransH \\cite{wang2014}, TransR \\cite{lin2015b}, PTransE \\cite{lin2015a}, and KG2E \\cite{he2015}, which are state-of-the-art for general link prediction. Other methods like UM, SE, SME, SLM, LFM, NTN, and RESCAL are also mentioned.\n    *   **Limitations of Previous Solutions:** While these methods have achieved success, \\cite{xiao2015} argues that none of them explicitly address the problem of *precise* link prediction. Even methods like TransH and TransR, which project entities into relation-specific subspaces, still maintain an over-strict \"one-point\" geometric form within those subspaces, failing to adequately represent complex relations or resolve the ill-posed algebraic system.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{xiao2015} proposes **ManifoldE**, a manifold-based embedding principle that replaces the point-wise translation principle (`h+r=t`) with a manifold-wise principle `M(h,r,t) = D_r^2`. The score function measures the distance of a triple from this relation-specific manifold: `f_r(h,t) = ||M(h,r,t) - D_r^2||^2`.\n    *   **Novelty/Difference:**\n        *   **Manifold-based Modeling:** Instead of mapping true triples to a single point, \\cite{xiao2015} maps them to a *manifold* (e.g., a high-dimensional sphere or hyperplane) defined by the head entity and relation. This provides a more flexible geometric representation.\n        *   **Addressing Ill-posedness:** By treating `M(h,r,t) = D_r^2` as a single equation per fact, \\cite{xiao2015} transforms the embedding problem into a nearly well-posed algebraic system, especially when the embedding dimension `d` is chosen such that `d >= T/(E+R)` (where T is number of facts, E entities, R relations). This leads to more stable and precise solutions.\n        *   **Specific Manifold Implementations:**\n            *   **Sphere:** `M(h,r,t) = ||h+r - t||^2`, where `h+r` is the center and `D_r` is the radius. This is a direct generalization of TransE (where `D_r=0`). Kernel functions (Linear, Gaussian, Polynomial) can be applied for more expressive representations in a Reproducing Kernel Hilbert Space (RKHS).\n            *   **Hyperplane:** `M(h,r,t) = (h+r_head) * (t+r_tail)`. This allows for more intersections between manifolds, potentially offering more solutions. An \"absolute operator\" (`|h+r_head| * |t+r_tail|`) is introduced to further increase solution flexibility. Kernel functions can also be applied here.\n\n4.  **Key Technical Contributions**\n    *   **Novel Principle:** Introduction of the manifold-based embedding principle (ManifoldE) to address the limitations of point-wise modeling in KGE.\n    *   **Algebraic System Re-formulation:** Identification and mitigation of the ill-posed algebraic system problem in KGE by transforming it into a nearly well-posed system, leading to more stable and precise embeddings.\n    *   **Flexible Geometric Forms:** Expansion of the \"golden position\" from a single point to a high-dimensional manifold (sphere, hyperplane), better accommodating complex relation types (e.g., 1-N, N-N).\n    *   **Kernel Integration:** Application of kernel functions (e.g., Linear, Polynomial) within the manifold framework to enhance expressiveness.\n    *   **Absolute Operator for Hyperplane:** A novel operator to increase the number of potential solutions and improve embedding flexibility for the Hyperplane manifold.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** \\cite{xiao2015} evaluated ManifoldE on two tasks: Link Prediction and Triple Classification, using four public benchmark datasets: WN18, FB15K, WN11, and FB13 (subsets of Wordnet and Freebase). Visualization comparisons were also performed.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Link Prediction (HITS@N):**\n            *   **Overall Performance (HITS@10 Filter):** ManifoldE (both Sphere and Hyperplane variants) consistently achieved substantial improvements over state-of-the-art baselines (SE, TransE, TransH, TransR, KG2E). For instance, on WN18, ManifoldE Sphere reached 94.4% (vs. KG2E 92.8%), and on FB15K, ManifoldE Sphere reached 79.5% (vs. KG2E 74.0%).\n            *   **Precise Prediction (HITS@1 Filter):** This metric specifically validated the paper's core claim. ManifoldE showed significant gains, particularly for complex relations. On FB15K, for N-N relations, ManifoldE Hyperplane achieved 53.0% (predicting head) and 55.9% (predicting tail) HITS@1, dramatically outperforming TransE (18.1% head, 20.3% tail) and TransR (14.5% head, 16.2% tail).\n        *   **Efficiency:** ManifoldE demonstrated high efficiency, with training times comparable to TransE (the most efficient baseline) and significantly faster than TransR and KG2E. For example, on FB15K, ManifoldE Sphere took 0.7s, comparable to TransE's 0.7s, but much faster than TransR's 29.1s and KG2E's 44.2s.\n        *   **Visualization:** Visualizations (e.g., Figure 1) illustrated that ManifoldE effectively separates true and false triples, with true triples clustering within the defined manifold, unlike the chaotic distribution observed with TransE.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** While ManifoldE addresses the ill-posed algebraic system by making it \"nearly well-posed,\" it doesn't necessarily guarantee a perfectly well-posed system in all scenarios. The choice of manifold (Sphere vs. Hyperplane) and kernel function can impact performance, requiring careful tuning. The absolute operator, while increasing flexibility, might also introduce additional complexity in optimization.\n    *   **Scope of Applicability:** The method is primarily designed for knowledge graph embedding tasks, particularly link prediction and triple classification. Its direct applicability to other types of graph data or embedding problems might require adaptation. The focus is on improving precision for existing (h,r,t) triple structures.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{xiao2015} significantly advances the technical state-of-the-art in knowledge graph embedding by formally identifying and addressing the critical issues of ill-posed algebraic systems and over-strict geometric forms, which limit precise link prediction in previous models. The proposed ManifoldE principle offers a more robust and flexible framework.\n    *   **Potential Impact on Future Research:** This work opens new avenues for research in KGE by shifting the paradigm from point-wise to manifold-wise modeling. It encourages further exploration of different manifold types, advanced kernel functions, and more sophisticated algebraic treatments to achieve even higher precision and stability in knowledge representation. The emphasis on precise link prediction could also inspire new evaluation metrics and benchmarks for KGE models.",
        "keywords": [
          "Knowledge graph embedding (KGE)",
          "precise link prediction",
          "ManifoldE",
          "manifold-based embedding",
          "ill-posed algebraic system",
          "over-strict geometric form",
          "Sphere manifold",
          "Hyperplane manifold",
          "kernel functions",
          "flexible geometric representation",
          "HITS@1",
          "knowledge completion",
          "triple classification",
          "state-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "991b64748dfeecf026a27030c16fe1743aa20167.pdf"
    },
    {
      "success": true,
      "doc_id": "cfc8bbc38fcb629f8b8295d033bafb95",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Accurately predicting natural-language spatial relations between geographic entities (geoentities), which are influenced by diverse perceptions (location, culture, linguistic conventions) \\cite{hu2024}.\n    *   **Importance:** These relations are crucial for supporting various geospatial tasks, such as question answering and cognitive reasoning \\cite{hu2024}.\n    *   **Challenge:** Prior studies on spatial relations often focused on a limited set of human-selected spatial terms and geometric attributes, critically overlooking essential semantic attributes \\cite{hu2024}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing:** This work builds upon and addresses limitations of \"prior studies\" that investigated natural-language spatial relations \\cite{hu2024}.\n    *   **Limitations of previous solutions:** Previous approaches were constrained by focusing on a \"limited set of human-selected spatial terms and geometric attributes\" and \"often overlooked essential semantic attributes\" \\cite{hu2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper introduces a \"Spatial Relation-based Knowledge Graph Embedding framework, SR-KGE,\" designed to predict spatial relation terms among distinct geoentities \\cite{hu2024}.\n    *   **Novelty:**\n        *   Incorporates \"new KG fusion functions\" to enhance embedding and learning \\cite{hu2024}.\n        *   Considers both \"graph structures and the diversity of natural language expressions\" during the embedding and learning process \\cite{hu2024}.\n        *   Integrates \"geoentity types as a constraint\" to more accurately capture both spatial and semantic relations \\cite{hu2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms/methods:** Development of the SR-KGE framework, which includes novel KG fusion functions \\cite{hu2024}.\n    *   **Techniques:** Introduction of geoentity types as a crucial constraint for more accurate spatial and semantic relation capture, alongside considering graph structures and natural language diversity in embeddings \\cite{hu2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Evaluated SR-KGE on \"two knowledge graph datasets, one small-scale and one large-scale\" \\cite{hu2024}.\n    *   **Key performance metrics and comparison results:** Experiments demonstrated SR-KGE's \"superior performance in spatial relation inference\" compared to popular Knowledge Graph Embedding (KGE) models, including TransE, RotatE, and HAKE \\cite{hu2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions:** The abstract does not explicitly state limitations of the SR-KGE framework itself, but it implicitly addresses the limitations of prior work by proposing a more comprehensive approach \\cite{hu2024}.\n    *   **Scope of applicability:** The method is specifically designed for predicting natural-language spatial relations between geographic entities within knowledge graphs \\cite{hu2024}.\n\n7.  **Technical Significance**\n    *   **Advance state-of-the-art:** SR-KGE advances the classic study of natural language described spatial relations by offering a more automated and intelligent inference mechanism \\cite{hu2024}.\n    *   **Potential impact on future research:** By more accurately capturing diverse spatial and semantic relations, it can significantly improve performance in geospatial tasks like question answering and cognitive reasoning, paving the way for more sophisticated geospatial AI applications \\cite{hu2024}.",
      "intriguing_abstract": "Unraveling the complexities of natural-language spatial relations between geographic entities (geoentities) remains a formidable challenge, yet it is indispensable for advancing geospatial AI, question answering, and cognitive reasoning. Existing approaches often fall short by focusing solely on geometric attributes, neglecting the rich semantic diversity inherent in human perception and linguistic expression. Here, we introduce SR-KGE, a novel Spatial Relation-based Knowledge Graph Embedding framework designed to accurately predict these nuanced spatial relation terms. SR-KGE innovatively incorporates new KG fusion functions and integrates geoentity types as a crucial constraint, enabling a more comprehensive capture of both spatial and semantic relations. Our framework uniquely considers both underlying graph structures and the diversity of natural language expressions during the embedding process. Evaluated on two distinct knowledge graph datasets, SR-KGE demonstrates superior performance in spatial relation inference compared to state-of-the-art Knowledge Graph Embedding models such as TransE, RotatE, and HAKE. This work significantly advances the automated inference of natural language described spatial relations, paving the way for more intelligent and robust geospatial reasoning systems.",
      "keywords": [
        "natural-language spatial relations",
        "geographic entities (geoentities)",
        "Spatial Relation-based Knowledge Graph Embedding (SR-KGE)",
        "Knowledge Graph Embedding (KGE)",
        "KG fusion functions",
        "geoentity types as constraint",
        "graph structures",
        "natural language diversity",
        "semantic attributes",
        "geospatial tasks",
        "spatial relation inference",
        "superior performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/6a2f26cece133b0aa52843be0f149a65e78374f7.pdf",
      "citation_key": "hu2024",
      "metadata": {
        "title": "GeoEntity-type constrained knowledge graph embedding for predicting natural-language spatial relations",
        "authors": [
          "Lei Hu",
          "Wenwen Li",
          "Jun Xu",
          "Yunqiang Zhu"
        ],
        "published_date": "2024",
        "abstract": "Abstract Natural-language spatial relations between geographic entities (geoentities) reflect diverse perceptions influenced by factors like location, culture, and linguistic conventions. These relations play a crucial role in supporting geospatial tasks, such as question answering and cognitive reasoning. While prior studies focused on a limited set of human-selected spatial terms and geometric attributes, they often overlooked essential semantic attributes. To overcome this limitation, we developed a Spatial Relation-based Knowledge Graph Embedding framework, SR-KGE, with new KG fusion functions to predict spatial relation terms among distinct geoentities. This method not only considers graph structures and the diversity of natural language expressions in the embedding and learning process, but also incorporates geoentity types as a constraint to capture spatial and semantic relations more accurately. Our experiments on two knowledge graph datasets, one small-scale and one large-scale, have both shown its superior performance in spatial relation inference compared to popular KGE models, including TransE, RotatE, and HAKE. We hope our research will advance the classic study of natural language described spatial relations in a more automated and intelligent way.",
        "file_path": "paper_data/knowledge_graph_embedding/6a2f26cece133b0aa52843be0f149a65e78374f7.pdf",
        "venue": "International Journal of Geographical Information Science",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Accurately predicting natural-language spatial relations between geographic entities (geoentities), which are influenced by diverse perceptions (location, culture, linguistic conventions) \\cite{hu2024}.\n    *   **Importance:** These relations are crucial for supporting various geospatial tasks, such as question answering and cognitive reasoning \\cite{hu2024}.\n    *   **Challenge:** Prior studies on spatial relations often focused on a limited set of human-selected spatial terms and geometric attributes, critically overlooking essential semantic attributes \\cite{hu2024}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing:** This work builds upon and addresses limitations of \"prior studies\" that investigated natural-language spatial relations \\cite{hu2024}.\n    *   **Limitations of previous solutions:** Previous approaches were constrained by focusing on a \"limited set of human-selected spatial terms and geometric attributes\" and \"often overlooked essential semantic attributes\" \\cite{hu2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper introduces a \"Spatial Relation-based Knowledge Graph Embedding framework, SR-KGE,\" designed to predict spatial relation terms among distinct geoentities \\cite{hu2024}.\n    *   **Novelty:**\n        *   Incorporates \"new KG fusion functions\" to enhance embedding and learning \\cite{hu2024}.\n        *   Considers both \"graph structures and the diversity of natural language expressions\" during the embedding and learning process \\cite{hu2024}.\n        *   Integrates \"geoentity types as a constraint\" to more accurately capture both spatial and semantic relations \\cite{hu2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms/methods:** Development of the SR-KGE framework, which includes novel KG fusion functions \\cite{hu2024}.\n    *   **Techniques:** Introduction of geoentity types as a crucial constraint for more accurate spatial and semantic relation capture, alongside considering graph structures and natural language diversity in embeddings \\cite{hu2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Evaluated SR-KGE on \"two knowledge graph datasets, one small-scale and one large-scale\" \\cite{hu2024}.\n    *   **Key performance metrics and comparison results:** Experiments demonstrated SR-KGE's \"superior performance in spatial relation inference\" compared to popular Knowledge Graph Embedding (KGE) models, including TransE, RotatE, and HAKE \\cite{hu2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions:** The abstract does not explicitly state limitations of the SR-KGE framework itself, but it implicitly addresses the limitations of prior work by proposing a more comprehensive approach \\cite{hu2024}.\n    *   **Scope of applicability:** The method is specifically designed for predicting natural-language spatial relations between geographic entities within knowledge graphs \\cite{hu2024}.\n\n7.  **Technical Significance**\n    *   **Advance state-of-the-art:** SR-KGE advances the classic study of natural language described spatial relations by offering a more automated and intelligent inference mechanism \\cite{hu2024}.\n    *   **Potential impact on future research:** By more accurately capturing diverse spatial and semantic relations, it can significantly improve performance in geospatial tasks like question answering and cognitive reasoning, paving the way for more sophisticated geospatial AI applications \\cite{hu2024}.",
        "keywords": [
          "natural-language spatial relations",
          "geographic entities (geoentities)",
          "Spatial Relation-based Knowledge Graph Embedding (SR-KGE)",
          "Knowledge Graph Embedding (KGE)",
          "KG fusion functions",
          "geoentity types as constraint",
          "graph structures",
          "natural language diversity",
          "semantic attributes",
          "geospatial tasks",
          "spatial relation inference",
          "superior performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "6a2f26cece133b0aa52843be0f149a65e78374f7.pdf"
    },
    {
      "success": true,
      "doc_id": "87f9385e1ba59f2266df298d32382c89",
      "summary": "Here's a focused summary of the paper \"Knowledge Graph Embedding by Translating on Hyperplanes\" \\cite{wang2014} for a literature review:\n\n### Technical Paper Analysis: Knowledge Graph Embedding by Translating on Hyperplanes \\cite{wang2014}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of embedding large-scale knowledge graphs (composed of entities and relations) into a continuous vector space, specifically focusing on the limitations of existing efficient models like TransE \\cite{wang2014} in handling complex relation mapping properties. TransE \\cite{wang2014} struggles with reflexive, one-to-many, many-to-one, and many-to-many relations because it assumes a single, fixed representation for an entity regardless of the relation it participates in.\n    *   **Importance and Challenge:** Knowledge graphs are vital for AI applications (e.g., web search, Q&A). Key challenges include bridging symbolic/logical systems with numerical computing and aggregating global knowledge. While TransE \\cite{wang2014} is efficient and performs well on many tasks, its inability to model complex relation types accurately limits its applicability. More complex models can handle these properties but sacrifice efficiency and often overall predictive performance. The challenge is to achieve a good trade-off between model capacity (handling complex relations) and computational efficiency.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds directly upon translation-based embedding models, particularly TransE \\cite{wang2014}, which represents relations as translation vectors. It positions itself as an improvement over TransE \\cite{wang2014} by addressing its specific flaws while retaining its efficiency. The paper also briefly compares against other embedding models like Unstructured, Distant Model, Bilinear Model, Single Layer Model, and NTN, highlighting their varying complexities and performance.\n    *   **Limitations of Previous Solutions:**\n        *   **TransE \\cite{wang2014}:** While efficient and achieving state-of-the-art performance in many scenarios, TransE \\cite{wang2014} fails to adequately model relations with mapping properties such as reflexive, one-to-many, many-to-one, and many-to-many. This is because it enforces a single representation for an entity across all relations, leading to problematic consequences (e.g., `h=t` for reflexive relations, or `h_0=...=h_m` for many-to-one relations in an ideal error-free embedding).\n        *   **More Complex Models (e.g., NTN):** These models are capable of preserving complex mapping properties but incur significantly higher model complexity and running time, often resulting in worse overall predictive performance compared to TransE \\cite{wang2014}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **TransH (Translation on Hyperplanes)**. In TransH, a relation `r` is modeled by two components: a relation-specific hyperplane (defined by its normal vector `w_r`) and a translation vector `d_r` that lies *within* this hyperplane.\n        *   For a given triplet `(h, r, t)`, the entity embeddings `h` and `t` are first projected onto the relation-specific hyperplane `w_r`. These projections are denoted as `h_perp` and `t_perp`.\n        *   The scoring function `f_r(h,t)` measures the plausibility of the triplet by calculating the squared L2-norm of the difference between `h_perp + d_r` and `t_perp`: `f_r(h,t) = ||(h - w_r^T h w_r) + d_r - (t - w_r^T t w_r)||_2^2`. A lower score indicates higher plausibility.\n        *   Constraints are applied during training: `||w_r||_2 = 1` (unit normal vector) and `w_r^T d_r = 0` (ensuring `d_r` is orthogonal to `w_r`, thus lying in the hyperplane).\n    *   **Novelty/Difference:**\n        *   **Distributed Entity Representations:** By projecting entities onto relation-specific hyperplanes, TransH \\cite{wang2014} implicitly allows an entity to have different \"roles\" or distributed representations depending on the relation it is involved in. This directly addresses the core limitation of TransE \\cite{wang2014}.\n        *   **Efficiency and Capacity Trade-off:** TransH \\cite{wang2014} achieves this enhanced modeling capacity with almost the same model complexity as TransE \\cite{wang2014} (O(nek + 2nrk) vs. O(nek + nrk)), offering a better balance than previous complex models.\n        *   **Improved Negative Sampling:** Introduces a novel strategy for constructing negative examples during training. It uses a Bernoulli distribution to decide whether to corrupt the head or tail entity, with probabilities based on the relation's `tph` (tails per head) and `hpt` (heads per tail) statistics. This reduces the likelihood of generating false negative labels, which is crucial for incomplete knowledge graphs.\n\n4.  **Key Technical Contributions**\n    *   **TransH Model:** A novel knowledge graph embedding model that represents relations as hyperplanes with translation vectors on them, enabling relation-specific entity representations.\n    *   **Scoring Function:** A new scoring function `f_r(h,t)` that incorporates entity projections onto relation-specific hyperplanes before applying translation.\n    *   **Orthogonality Constraint:** The introduction of a constraint `w_r^T d_r = 0` to ensure the translation vector `d_r` lies within the relation's hyperplane.\n    *   **Bernoulli Negative Sampling:** A practical and effective method for constructing negative training examples that leverages relation mapping properties to reduce false negative labels.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on three tasks: link prediction, triplet classification, and fact extraction.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Datasets:** WN18 (a subset of WordNet) and FB15k (a dense subgraph of Freebase).\n        *   **Metrics (Link Prediction):** Mean rank (lower is better) and Hits@10 (proportion of correct entities ranked in the top 10, higher is better). Both \"raw\" and \"filt\" (filtered out existing valid triplets) settings were used.\n        *   **Results:**\n            *   TransH \\cite{wang2014} consistently and significantly outperforms TransE \\cite{wang2014} on predictive accuracy, especially on the larger and more complex FB15k dataset.\n            *   Detailed analysis on FB15k shows TransH \\cite{wang2014} brings substantial improvements to TransE \\cite{wang2014} for one-to-many, many-to-one, and many-to-many relations, and surprisingly, also significantly improves performance on one-to-one relations (>60% improvement).\n            *   TransH \\cite{wang2014} demonstrates comparable running time and scalability to TransE \\cite{wang2014}.\n            *   The proposed Bernoulli negative sampling strategy (\"bern.\") consistently yields better performance than uniform sampling (\"unif\").\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The model still operates within a translation-based geometric framework. While it addresses TransE's \\cite{wang2014} specific limitations, it doesn't explore fundamentally different embedding paradigms. The effectiveness is demonstrated on specific benchmark datasets, and its performance on highly sparse or different types of knowledge graphs might vary.\n    *   **Scope of Applicability:** Primarily focused on knowledge graph embedding for tasks like link prediction, triplet classification, and fact extraction on large-scale, multi-relational graphs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** TransH \\cite{wang2014} significantly advances the state-of-the-art in knowledge graph embedding by providing a more expressive and robust model that effectively handles complex relation mapping properties (reflexive, one-to-many, many-to-one, many-to-many) which were problematic for previous efficient models like TransE \\cite{wang2014}. It achieves this improved capacity with almost the same model complexity and efficiency as TransE \\cite{wang2014}, offering a superior trade-off between model capacity and computational cost.\n    *   **Potential Impact on Future Research:** This work highlights the critical importance of considering relation mapping properties and sophisticated negative sampling strategies in knowledge graph embedding. It provides a strong, efficient, and highly performant baseline for future research, encouraging the development of models that can capture richer relational semantics without sacrificing scalability. It paves the way for more accurate and reliable knowledge graph completion and reasoning in various AI applications.",
      "intriguing_abstract": "The quest to accurately embed vast knowledge graphs into continuous vector spaces is paramount for modern AI, yet a critical bottleneck persists: effectively modeling complex relational semantics. While translation-based models like TransE offer efficiency, they fundamentally struggle with intricate relation mapping properties (e.g., one-to-many, many-to-many), forcing entities into rigid, context-agnostic representations.\n\nThis paper unveils **TransH**, a pioneering knowledge graph embedding model that elegantly resolves this dilemma. TransH models each relation not as a simple translation, but as a translation vector *within a relation-specific hyperplane*. By projecting entities onto these hyperplanes, TransH implicitly allows for *distributed entity representations*, enabling an entity to adopt different 'roles' depending on the relation it participates in.\n\nThis novel geometric approach dramatically enhances model capacity, robustly handling complex relation types that previously challenged state-of-the-art methods. Crucially, TransH achieves this superior expressiveness with computational complexity nearly identical to TransE, offering an unprecedented balance between accuracy and efficiency. Coupled with an innovative Bernoulli negative sampling strategy, our extensive experiments on WN18 and FB15k demonstrate significant performance gains in **link prediction** and **triplet classification**, particularly for intricate many-to-many relations. TransH sets a new benchmark for knowledge graph embedding, paving the way for more accurate knowledge completion and reasoning in real-world AI applications.",
      "keywords": [
        "Knowledge graph embedding",
        "TransH (Translation on Hyperplanes)",
        "TransE",
        "relation mapping properties",
        "relation-specific entity representations",
        "hyperplanes",
        "translation vectors",
        "Bernoulli negative sampling",
        "link prediction",
        "triplet classification",
        "computational efficiency",
        "model capacity",
        "AI applications"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/2a3f862199883ceff5e3c74126f0c80770653e05.pdf",
      "citation_key": "wang2014",
      "metadata": {
        "title": "Knowledge Graph Embedding by Translating on Hyperplanes",
        "authors": [
          "Zhen Wang",
          "Jianwen Zhang",
          "Jianlin Feng",
          "Zheng Chen"
        ],
        "published_date": "2014",
        "abstract": "\n \n We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.\n \n",
        "file_path": "paper_data/knowledge_graph_embedding/2a3f862199883ceff5e3c74126f0c80770653e05.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Knowledge Graph Embedding by Translating on Hyperplanes\" \\cite{wang2014} for a literature review:\n\n### Technical Paper Analysis: Knowledge Graph Embedding by Translating on Hyperplanes \\cite{wang2014}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of embedding large-scale knowledge graphs (composed of entities and relations) into a continuous vector space, specifically focusing on the limitations of existing efficient models like TransE \\cite{wang2014} in handling complex relation mapping properties. TransE \\cite{wang2014} struggles with reflexive, one-to-many, many-to-one, and many-to-many relations because it assumes a single, fixed representation for an entity regardless of the relation it participates in.\n    *   **Importance and Challenge:** Knowledge graphs are vital for AI applications (e.g., web search, Q&A). Key challenges include bridging symbolic/logical systems with numerical computing and aggregating global knowledge. While TransE \\cite{wang2014} is efficient and performs well on many tasks, its inability to model complex relation types accurately limits its applicability. More complex models can handle these properties but sacrifice efficiency and often overall predictive performance. The challenge is to achieve a good trade-off between model capacity (handling complex relations) and computational efficiency.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds directly upon translation-based embedding models, particularly TransE \\cite{wang2014}, which represents relations as translation vectors. It positions itself as an improvement over TransE \\cite{wang2014} by addressing its specific flaws while retaining its efficiency. The paper also briefly compares against other embedding models like Unstructured, Distant Model, Bilinear Model, Single Layer Model, and NTN, highlighting their varying complexities and performance.\n    *   **Limitations of Previous Solutions:**\n        *   **TransE \\cite{wang2014}:** While efficient and achieving state-of-the-art performance in many scenarios, TransE \\cite{wang2014} fails to adequately model relations with mapping properties such as reflexive, one-to-many, many-to-one, and many-to-many. This is because it enforces a single representation for an entity across all relations, leading to problematic consequences (e.g., `h=t` for reflexive relations, or `h_0=...=h_m` for many-to-one relations in an ideal error-free embedding).\n        *   **More Complex Models (e.g., NTN):** These models are capable of preserving complex mapping properties but incur significantly higher model complexity and running time, often resulting in worse overall predictive performance compared to TransE \\cite{wang2014}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **TransH (Translation on Hyperplanes)**. In TransH, a relation `r` is modeled by two components: a relation-specific hyperplane (defined by its normal vector `w_r`) and a translation vector `d_r` that lies *within* this hyperplane.\n        *   For a given triplet `(h, r, t)`, the entity embeddings `h` and `t` are first projected onto the relation-specific hyperplane `w_r`. These projections are denoted as `h_perp` and `t_perp`.\n        *   The scoring function `f_r(h,t)` measures the plausibility of the triplet by calculating the squared L2-norm of the difference between `h_perp + d_r` and `t_perp`: `f_r(h,t) = ||(h - w_r^T h w_r) + d_r - (t - w_r^T t w_r)||_2^2`. A lower score indicates higher plausibility.\n        *   Constraints are applied during training: `||w_r||_2 = 1` (unit normal vector) and `w_r^T d_r = 0` (ensuring `d_r` is orthogonal to `w_r`, thus lying in the hyperplane).\n    *   **Novelty/Difference:**\n        *   **Distributed Entity Representations:** By projecting entities onto relation-specific hyperplanes, TransH \\cite{wang2014} implicitly allows an entity to have different \"roles\" or distributed representations depending on the relation it is involved in. This directly addresses the core limitation of TransE \\cite{wang2014}.\n        *   **Efficiency and Capacity Trade-off:** TransH \\cite{wang2014} achieves this enhanced modeling capacity with almost the same model complexity as TransE \\cite{wang2014} (O(nek + 2nrk) vs. O(nek + nrk)), offering a better balance than previous complex models.\n        *   **Improved Negative Sampling:** Introduces a novel strategy for constructing negative examples during training. It uses a Bernoulli distribution to decide whether to corrupt the head or tail entity, with probabilities based on the relation's `tph` (tails per head) and `hpt` (heads per tail) statistics. This reduces the likelihood of generating false negative labels, which is crucial for incomplete knowledge graphs.\n\n4.  **Key Technical Contributions**\n    *   **TransH Model:** A novel knowledge graph embedding model that represents relations as hyperplanes with translation vectors on them, enabling relation-specific entity representations.\n    *   **Scoring Function:** A new scoring function `f_r(h,t)` that incorporates entity projections onto relation-specific hyperplanes before applying translation.\n    *   **Orthogonality Constraint:** The introduction of a constraint `w_r^T d_r = 0` to ensure the translation vector `d_r` lies within the relation's hyperplane.\n    *   **Bernoulli Negative Sampling:** A practical and effective method for constructing negative training examples that leverages relation mapping properties to reduce false negative labels.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on three tasks: link prediction, triplet classification, and fact extraction.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Datasets:** WN18 (a subset of WordNet) and FB15k (a dense subgraph of Freebase).\n        *   **Metrics (Link Prediction):** Mean rank (lower is better) and Hits@10 (proportion of correct entities ranked in the top 10, higher is better). Both \"raw\" and \"filt\" (filtered out existing valid triplets) settings were used.\n        *   **Results:**\n            *   TransH \\cite{wang2014} consistently and significantly outperforms TransE \\cite{wang2014} on predictive accuracy, especially on the larger and more complex FB15k dataset.\n            *   Detailed analysis on FB15k shows TransH \\cite{wang2014} brings substantial improvements to TransE \\cite{wang2014} for one-to-many, many-to-one, and many-to-many relations, and surprisingly, also significantly improves performance on one-to-one relations (>60% improvement).\n            *   TransH \\cite{wang2014} demonstrates comparable running time and scalability to TransE \\cite{wang2014}.\n            *   The proposed Bernoulli negative sampling strategy (\"bern.\") consistently yields better performance than uniform sampling (\"unif\").\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The model still operates within a translation-based geometric framework. While it addresses TransE's \\cite{wang2014} specific limitations, it doesn't explore fundamentally different embedding paradigms. The effectiveness is demonstrated on specific benchmark datasets, and its performance on highly sparse or different types of knowledge graphs might vary.\n    *   **Scope of Applicability:** Primarily focused on knowledge graph embedding for tasks like link prediction, triplet classification, and fact extraction on large-scale, multi-relational graphs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** TransH \\cite{wang2014} significantly advances the state-of-the-art in knowledge graph embedding by providing a more expressive and robust model that effectively handles complex relation mapping properties (reflexive, one-to-many, many-to-one, many-to-many) which were problematic for previous efficient models like TransE \\cite{wang2014}. It achieves this improved capacity with almost the same model complexity and efficiency as TransE \\cite{wang2014}, offering a superior trade-off between model capacity and computational cost.\n    *   **Potential Impact on Future Research:** This work highlights the critical importance of considering relation mapping properties and sophisticated negative sampling strategies in knowledge graph embedding. It provides a strong, efficient, and highly performant baseline for future research, encouraging the development of models that can capture richer relational semantics without sacrificing scalability. It paves the way for more accurate and reliable knowledge graph completion and reasoning in various AI applications.",
        "keywords": [
          "Knowledge graph embedding",
          "TransH (Translation on Hyperplanes)",
          "TransE",
          "relation mapping properties",
          "relation-specific entity representations",
          "hyperplanes",
          "translation vectors",
          "Bernoulli negative sampling",
          "link prediction",
          "triplet classification",
          "computational efficiency",
          "model capacity",
          "AI applications"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "2a3f862199883ceff5e3c74126f0c80770653e05.pdf"
    },
    {
      "success": true,
      "doc_id": "d661add2b2ff381a642aa1e2f9e81362",
      "summary": "Here's a focused summary of the paper \\cite{zhu2020} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: High-dimensional Knowledge Graph Embeddings (KGEs) offer superior reasoning capabilities but demand substantial storage and computing resources, making them unsuitable for resource-limited or time-constrained applications. Directly training low-dimensional KGEs typically results in poor performance.\n    *   **Importance and Challenge**: There is a critical need for faster and cheaper KGE reasoning, especially for deployment on edge devices, mobile platforms, or in real-time online prediction systems. The challenge lies in reducing KGE dimensionality and resource consumption while preserving high reasoning accuracy.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon Knowledge Distillation (KD) techniques, which transfer knowledge from a large \"teacher\" model to a smaller \"student\" model.\n    *   **Limitations of Previous Solutions**:\n        *   **KGE Compression**: Prior methods like quantization (e.g., \\cite{zhu2020} [23]) reduce size but do not improve inference speed and can complicate model convergence.\n        *   **Knowledge Distillation for KGEs**: MulDE \\cite{zhu2020} [13] was an early attempt but required pre-training multiple teacher models.\n        *   **General KD**: Many existing KD methods treat all soft labels from the teacher equally, failing to account for their varying quality, and do not sufficiently explore the student's influence on the teacher's learning process.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: DualDE is a novel knowledge distillation framework that constructs a low-dimensional student KGE from a pre-trained high-dimensional teacher KGE. It considers the \"dual-influence\" between the teacher and the student.\n    *   **Novelty**:\n        *   **Soft Label Evaluation Mechanism**: Adaptively assigns different soft and hard label weights to triples based on the perceived quality (reliability) of the teacher's soft labels. This prevents negative impacts from unreliable teacher scores.\n        *   **Two-Stage Distillation Approach**: Improves the student's acceptance of the teacher. In the first stage, the teacher is static. In the second stage, the teacher is unfrozen and adjusted by learning from the student's output, making the teacher more \"acceptable\" and aligned with the student's learning state.\n        *   **Distillation Objective**: The student learns both the \"credibility\" (score difference) and the \"embedding structure\" (length ratio and angle between entity embeddings) of triples from the teacher.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of a soft label evaluation mechanism that dynamically weights distillation loss components based on the teacher's confidence in its predictions for individual triples.\n        *   A two-stage distillation strategy where the teacher model is adaptively refined in the second stage based on the student's learning progress.\n    *   **System Design/Architectural Innovations**: A general framework for KGE distillation that is applicable to various KGE models (e.g., TransE, ComplEx, RotatE, SimplE).\n    *   **Theoretical Insights**: Proposes and validates the concept of \"dual-influence\" in knowledge distillation, emphasizing that both teacher-to-student and student-to-teacher interactions are crucial for optimal distillation.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated DualDE on standard KG datasets (WN18RR, FB15k-237) using several typical KGE models (ComplEx, RotatE, TransE, SimplE). Ablation studies were performed to confirm the effectiveness of the soft label evaluation mechanism and the two-stage distillation approach.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Successfully reduced embedding parameters by 7x-15x.\n        *   Increased inference speed by 2x-6x.\n        *   Maintained high performance, showing only a little or no loss of accuracy compared to the original high-dimensional teacher KGEs.\n        *   Significantly outperformed low-dimensional KGEs trained directly from scratch.\n        *   Ablation studies confirmed that both the soft label evaluation mechanism and the two-stage distillation approach contribute positively to the distillation results.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper focuses on distilling from an *already pre-trained* high-dimensional KGE. The quality of the initial teacher model is assumed to be high.\n    *   **Scope of Applicability**: Primarily targets KGEs for faster and cheaper reasoning in scenarios with limited computing resources (e.g., edge computing, mobile devices) or strict time constraints (e.g., online financial predictions).\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: DualDE significantly advances the practical applicability of KGEs by providing an effective method to compress models and accelerate inference without substantial performance degradation. It introduces novel, adaptive mechanisms for knowledge distillation.\n    *   **Potential Impact**: This work enables the widespread deployment of KGEs in real-world, resource-constrained environments, broadening their utility beyond high-performance computing settings. The proposed dual-influence perspective and adaptive distillation mechanisms could inspire future research in model compression and knowledge transfer across various AI domains.",
      "intriguing_abstract": "The formidable reasoning power of high-dimensional Knowledge Graph Embeddings (KGEs) is often bottlenecked by their substantial computational and storage demands, severely limiting their deployment in resource-constrained or real-time environments. Directly training low-dimensional KGEs typically yields poor performance. We introduce DualDE, a novel **knowledge distillation** framework that efficiently compresses high-dimensional KGEs into lightweight, **low-dimensional** models without sacrificing accuracy.\n\nDualDE pioneers a \"dual-influence\" distillation paradigm, where the student not only learns from the teacher but also adaptively refines the teacher's knowledge. This is achieved through a dynamic **soft label evaluation mechanism** that weights distillation loss based on teacher confidence, preventing negative transfer. Furthermore, a two-stage distillation approach allows the teacher to adjust to the student's learning state, fostering optimal knowledge transfer. DualDE also uniquely distills both the credibility (score differences) and the **embedding structure** (length ratios, angles) of triples. Our experiments demonstrate DualDE's remarkable ability to reduce KGE parameters by 7x-15x and accelerate **inference speed** by 2x-6x, all while maintaining competitive accuracy. This breakthrough enables the widespread deployment of sophisticated KGE reasoning on **edge devices**, mobile platforms, and in critical real-time applications, pushing the boundaries of practical AI.",
      "keywords": [
        "Knowledge Graph Embeddings (KGEs)",
        "Knowledge Distillation (KD)",
        "DualDE framework",
        "high-dimensional KGEs",
        "low-dimensional KGEs",
        "soft label evaluation mechanism",
        "two-stage distillation",
        "dual-influence",
        "model compression",
        "inference acceleration",
        "resource-constrained environments",
        "high reasoning accuracy",
        "adaptive teacher refinement",
        "embedding structure distillation"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3.pdf",
      "citation_key": "zhu2020",
      "metadata": {
        "title": "DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning",
        "authors": [
          "Yushan Zhu",
          "Wen Zhang",
          "Mingyang Chen",
          "Hui Chen",
          "Xu-Xin Cheng",
          "Wei Zhang",
          "Huajun Chen Zhejiang University",
          "Alibaba Group",
          "Cetc Big Data Research Institute"
        ],
        "published_date": "2020",
        "abstract": "Knowledge Graph Embedding (KGE) is a popular method for KG reasoning and training KGEs with higher dimension are usually preferred since they have better reasoning capability. However, high-dimensional KGEs pose huge challenges to storage and computing resources and are not suitable for resource-limited or time-constrained applications, for which faster and cheaper reasoning is necessary. To address this problem, we propose DualDE, a knowledge distillation method to build low-dimensional student KGE from pre-trained high-dimensional teacher KGE. DualDE considers the dual-influence between the teacher and the student. In DualDE, we propose a soft label evaluation mechanism to adaptively assign different soft label and hard label weights to different triples, and a two-stage distillation approach to improve the student's acceptance of the teacher. Our DualDE is general enough to be applied to various KGEs. Experimental results show that our method can successfully reduce the embedding parameters of a high-dimensional KGE by 7 - 15 and increase the inference speed by 2 - 6 while retaining a high performance. We also experimentally prove the effectiveness of our soft label evaluation mechanism and two-stage distillation approach via ablation study.",
        "file_path": "paper_data/knowledge_graph_embedding/21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3.pdf",
        "venue": "Web Search and Data Mining",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{zhu2020} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: High-dimensional Knowledge Graph Embeddings (KGEs) offer superior reasoning capabilities but demand substantial storage and computing resources, making them unsuitable for resource-limited or time-constrained applications. Directly training low-dimensional KGEs typically results in poor performance.\n    *   **Importance and Challenge**: There is a critical need for faster and cheaper KGE reasoning, especially for deployment on edge devices, mobile platforms, or in real-time online prediction systems. The challenge lies in reducing KGE dimensionality and resource consumption while preserving high reasoning accuracy.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon Knowledge Distillation (KD) techniques, which transfer knowledge from a large \"teacher\" model to a smaller \"student\" model.\n    *   **Limitations of Previous Solutions**:\n        *   **KGE Compression**: Prior methods like quantization (e.g., \\cite{zhu2020} [23]) reduce size but do not improve inference speed and can complicate model convergence.\n        *   **Knowledge Distillation for KGEs**: MulDE \\cite{zhu2020} [13] was an early attempt but required pre-training multiple teacher models.\n        *   **General KD**: Many existing KD methods treat all soft labels from the teacher equally, failing to account for their varying quality, and do not sufficiently explore the student's influence on the teacher's learning process.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: DualDE is a novel knowledge distillation framework that constructs a low-dimensional student KGE from a pre-trained high-dimensional teacher KGE. It considers the \"dual-influence\" between the teacher and the student.\n    *   **Novelty**:\n        *   **Soft Label Evaluation Mechanism**: Adaptively assigns different soft and hard label weights to triples based on the perceived quality (reliability) of the teacher's soft labels. This prevents negative impacts from unreliable teacher scores.\n        *   **Two-Stage Distillation Approach**: Improves the student's acceptance of the teacher. In the first stage, the teacher is static. In the second stage, the teacher is unfrozen and adjusted by learning from the student's output, making the teacher more \"acceptable\" and aligned with the student's learning state.\n        *   **Distillation Objective**: The student learns both the \"credibility\" (score difference) and the \"embedding structure\" (length ratio and angle between entity embeddings) of triples from the teacher.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of a soft label evaluation mechanism that dynamically weights distillation loss components based on the teacher's confidence in its predictions for individual triples.\n        *   A two-stage distillation strategy where the teacher model is adaptively refined in the second stage based on the student's learning progress.\n    *   **System Design/Architectural Innovations**: A general framework for KGE distillation that is applicable to various KGE models (e.g., TransE, ComplEx, RotatE, SimplE).\n    *   **Theoretical Insights**: Proposes and validates the concept of \"dual-influence\" in knowledge distillation, emphasizing that both teacher-to-student and student-to-teacher interactions are crucial for optimal distillation.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated DualDE on standard KG datasets (WN18RR, FB15k-237) using several typical KGE models (ComplEx, RotatE, TransE, SimplE). Ablation studies were performed to confirm the effectiveness of the soft label evaluation mechanism and the two-stage distillation approach.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Successfully reduced embedding parameters by 7x-15x.\n        *   Increased inference speed by 2x-6x.\n        *   Maintained high performance, showing only a little or no loss of accuracy compared to the original high-dimensional teacher KGEs.\n        *   Significantly outperformed low-dimensional KGEs trained directly from scratch.\n        *   Ablation studies confirmed that both the soft label evaluation mechanism and the two-stage distillation approach contribute positively to the distillation results.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper focuses on distilling from an *already pre-trained* high-dimensional KGE. The quality of the initial teacher model is assumed to be high.\n    *   **Scope of Applicability**: Primarily targets KGEs for faster and cheaper reasoning in scenarios with limited computing resources (e.g., edge computing, mobile devices) or strict time constraints (e.g., online financial predictions).\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: DualDE significantly advances the practical applicability of KGEs by providing an effective method to compress models and accelerate inference without substantial performance degradation. It introduces novel, adaptive mechanisms for knowledge distillation.\n    *   **Potential Impact**: This work enables the widespread deployment of KGEs in real-world, resource-constrained environments, broadening their utility beyond high-performance computing settings. The proposed dual-influence perspective and adaptive distillation mechanisms could inspire future research in model compression and knowledge transfer across various AI domains.",
        "keywords": [
          "Knowledge Graph Embeddings (KGEs)",
          "Knowledge Distillation (KD)",
          "DualDE framework",
          "high-dimensional KGEs",
          "low-dimensional KGEs",
          "soft label evaluation mechanism",
          "two-stage distillation",
          "dual-influence",
          "model compression",
          "inference acceleration",
          "resource-constrained environments",
          "high reasoning accuracy",
          "adaptive teacher refinement",
          "embedding structure distillation"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3.pdf"
    },
    {
      "success": true,
      "doc_id": "9e005535accc23832b316d7a6a91b34d",
      "summary": "Here's a focused summary of the empirical study by \\cite{ali2020} for a literature review:\n\n1.  **Research Questions & Hypotheses**\n    *   The study investigates the reproducibility of previously reported results for Knowledge Graph Embedding Models (KGEMs) and aims to identify factors contributing to performance variability.\n    *   It empirically questions how model architecture, training approaches, loss functions, optimizers, and the explicit modeling of inverse relations influence KGEM performance.\n    *   Implicit hypotheses include that performance is not solely determined by model architecture, and that careful configuration can enable various architectures to achieve state-of-the-art results.\n\n2.  **Study Design & Methodology**\n    *   The study employed a two-phase design: a reproducibility study and a large-scale benchmarking study.\n    *   Researchers re-implemented 21 KGEMs, along with their training pipelines, loss functions, and evaluation metrics, within a unified, open-source PyKEEN framework to ensure fair and consistent comparison.\n    *   The benchmarking systematically varied hyper-parameters, training approaches (local closed world assumption, stochastic local closed world assumption), loss functions, optimizers, and the explicit modeling of inverse relations.\n\n3.  **Data & Participants**\n    *   The study evaluated 21 distinct Knowledge Graph Embedding Models (KGEMs).\n    *   Experiments were conducted on four benchmark datasets (specific names not provided in the snippet).\n    *   The large-scale benchmarking involved \"several thousands of experiments\" consuming 24,804 GPU hours.\n\n4.  **Key Empirical Findings**\n    *   Reproducibility varied significantly: some previously reported results were reproduced with original hyper-parameters, others only with alternate hyper-parameters, and some could not be reproduced at all.\n    *   Model performance is crucially determined by the combination of model architecture, training approach, loss function, and the explicit modeling of inverse relations, rather than by architecture alone.\n    *   Several KGEM architectures, when carefully configured and optimized, can achieve performance competitive with state-of-the-art models.\n    *   The study provided insights into best practices and optimal configurations for each evaluated model, identifying improvements over previously published best configurations.\n\n5.  **Statistical Analysis**\n    *   The study involved a large-scale empirical comparison of KGEM performance, with thousands of experiments.\n    *   Performance was assessed using standard link prediction evaluation metrics (e.g., plausibility scores, likely including MRR and Hits@k, though not explicitly named in the snippet).\n    *   The analysis focused on identifying significant patterns and relationships between model components and overall performance, leveraging extensive computational resources for comprehensive exploration.\n\n6.  **Validity & Limitations**\n    *   The study enhanced external validity by re-implementing models in a unified framework, directly addressing the heterogeneity and lack of precise hyper-parameter specifications that plagued prior research.\n    *   A limitation is its focus on shallow embedding approaches, excluding graph neural network (GNN)-based or temporal KGEMs.\n\n7.  **Empirical Contribution**\n    *   The study provides novel empirical evidence on the reproducibility crisis in KGEM research and offers the most comprehensive, fair benchmark of 21 models under a unified framework to date.\n    *   It contributes new knowledge by demonstrating the critical impact of training configurations and inverse relation modeling on KGEM performance, suggesting that careful optimization can elevate various architectures to state-of-the-art competitiveness.",
      "intriguing_abstract": "The reproducibility crisis casts a long shadow over machine learning research, particularly in the complex domain of Knowledge Graph Embedding Models (KGEMs). This paper presents an unprecedented, large-scale empirical study to unravel the true performance landscape of 21 diverse KGEMs. We re-implemented these models within a unified, open-source PyKEEN framework, systematically varying critical components including model architecture, training approaches, loss functions, optimizers, and the explicit modeling of inverse relations across thousands of experiments.\n\nOur findings reveal significant reproducibility challenges, with many previously reported results proving elusive without extensive re-optimization. Crucially, we demonstrate that KGEM performance is not solely dictated by architecture, but by the intricate synergy of its entire configuration stack. We show that numerous established KGEMs, when meticulously configured, can achieve state-of-the-art link prediction performance, often surpassing their published bests. This work provides a vital benchmark, demystifies performance variability, and offers actionable best practices, fundamentally shifting how researchers approach KGEM development and evaluation.",
      "keywords": [
        "Knowledge Graph Embedding Models (KGEMs)",
        "reproducibility study",
        "performance variability",
        "model architecture",
        "training configurations",
        "inverse relations modeling",
        "large-scale benchmarking",
        "unified framework (PyKEEN)",
        "hyper-parameter optimization",
        "reproducibility crisis",
        "state-of-the-art competitiveness",
        "empirical comparison",
        "shallow embedding approaches"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/acc855d74431537b98de5185e065e4eacbab7b26.pdf",
      "citation_key": "ali2020",
      "metadata": {
        "title": "Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework",
        "authors": [
          "Mehdi Ali",
          "M. Berrendorf",
          "Charles Tapley Hoyt",
          "Laurent Vermue",
          "Mikhail Galkin",
          "Sahand Sharifzadeh",
          "Asja Fischer",
          "Volker Tresp",
          "Jens Lehmann"
        ],
        "published_date": "2020",
        "abstract": "The heterogeneity in recently published knowledge graph embedding models implementations, training, and evaluation has made fair and thorough comparisons difficult. To assess the reproducibility of previously published results, we re-implemented and evaluated 21 models in the PyKEEN software package. In this paper, we outline which results could be reproduced with their reported hyper-parameters, which could only be reproduced with alternate hyper-parameters, and which could not be reproduced at all, as well as provide insight as to why this might be the case. We then performed a large-scale benchmarking on four datasets with several thousands of experiments and 24,804 GPU hours of computation time. We present insights gained as to best practices, best configurations for each model, and where improvements could be made over previously published best configurations. Our results highlight that the combination of model architecture, training approach, loss function, and the explicit modeling of inverse relations is crucial for a models performance and is not only determined by its architecture. We provide evidence that several architectures can obtain results competitive to the state of the art when configured carefully. We have made all code, experimental configurations, results, and analyses available at https://github.com/pykeen/pykeen and https://github.com/pykeen/benchmarking.",
        "file_path": "paper_data/knowledge_graph_embedding/acc855d74431537b98de5185e065e4eacbab7b26.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the empirical study by \\cite{ali2020} for a literature review:\n\n1.  **Research Questions & Hypotheses**\n    *   The study investigates the reproducibility of previously reported results for Knowledge Graph Embedding Models (KGEMs) and aims to identify factors contributing to performance variability.\n    *   It empirically questions how model architecture, training approaches, loss functions, optimizers, and the explicit modeling of inverse relations influence KGEM performance.\n    *   Implicit hypotheses include that performance is not solely determined by model architecture, and that careful configuration can enable various architectures to achieve state-of-the-art results.\n\n2.  **Study Design & Methodology**\n    *   The study employed a two-phase design: a reproducibility study and a large-scale benchmarking study.\n    *   Researchers re-implemented 21 KGEMs, along with their training pipelines, loss functions, and evaluation metrics, within a unified, open-source PyKEEN framework to ensure fair and consistent comparison.\n    *   The benchmarking systematically varied hyper-parameters, training approaches (local closed world assumption, stochastic local closed world assumption), loss functions, optimizers, and the explicit modeling of inverse relations.\n\n3.  **Data & Participants**\n    *   The study evaluated 21 distinct Knowledge Graph Embedding Models (KGEMs).\n    *   Experiments were conducted on four benchmark datasets (specific names not provided in the snippet).\n    *   The large-scale benchmarking involved \"several thousands of experiments\" consuming 24,804 GPU hours.\n\n4.  **Key Empirical Findings**\n    *   Reproducibility varied significantly: some previously reported results were reproduced with original hyper-parameters, others only with alternate hyper-parameters, and some could not be reproduced at all.\n    *   Model performance is crucially determined by the combination of model architecture, training approach, loss function, and the explicit modeling of inverse relations, rather than by architecture alone.\n    *   Several KGEM architectures, when carefully configured and optimized, can achieve performance competitive with state-of-the-art models.\n    *   The study provided insights into best practices and optimal configurations for each evaluated model, identifying improvements over previously published best configurations.\n\n5.  **Statistical Analysis**\n    *   The study involved a large-scale empirical comparison of KGEM performance, with thousands of experiments.\n    *   Performance was assessed using standard link prediction evaluation metrics (e.g., plausibility scores, likely including MRR and Hits@k, though not explicitly named in the snippet).\n    *   The analysis focused on identifying significant patterns and relationships between model components and overall performance, leveraging extensive computational resources for comprehensive exploration.\n\n6.  **Validity & Limitations**\n    *   The study enhanced external validity by re-implementing models in a unified framework, directly addressing the heterogeneity and lack of precise hyper-parameter specifications that plagued prior research.\n    *   A limitation is its focus on shallow embedding approaches, excluding graph neural network (GNN)-based or temporal KGEMs.\n\n7.  **Empirical Contribution**\n    *   The study provides novel empirical evidence on the reproducibility crisis in KGEM research and offers the most comprehensive, fair benchmark of 21 models under a unified framework to date.\n    *   It contributes new knowledge by demonstrating the critical impact of training configurations and inverse relation modeling on KGEM performance, suggesting that careful optimization can elevate various architectures to state-of-the-art competitiveness.",
        "keywords": [
          "Knowledge Graph Embedding Models (KGEMs)",
          "reproducibility study",
          "performance variability",
          "model architecture",
          "training configurations",
          "inverse relations modeling",
          "large-scale benchmarking",
          "unified framework (PyKEEN)",
          "hyper-parameter optimization",
          "reproducibility crisis",
          "state-of-the-art competitiveness",
          "empirical comparison",
          "shallow embedding approaches"
        ],
        "is_new_direction": "0",
        "paper_type": "empirical"
      },
      "file_name": "acc855d74431537b98de5185e065e4eacbab7b26.pdf"
    },
    {
      "success": true,
      "doc_id": "00eeea5ad707b7209ab6f461ac3d2287",
      "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey \\cite{mohamed2020} focuses on the application of Knowledge Graph Embedding (KGE) models within the domain of biological knowledge graphs. Its main objectives are to demonstrate how KGE models are a natural fit for representing complex biological knowledge, discuss their predictive and analytical capabilities across various biological applications, and analyze practical considerations, opportunities, and challenges associated with their adoption.\n\n2.  **Literature Coverage**\n    The paper reviews approaches that have emerged \"in recent years,\" specifically focusing on Knowledge Graph Embedding (KGE) models. While specific timeframes or detailed selection criteria for included literature are not explicitly provided, the scope is centered on advanced computational methods for graph modeling and mining.\n\n3.  **Classification Framework**\n    *   Examines KGE models specifically within the context of biological knowledge graphs.\n    *   Categorizes discussions by their predictive and analytical capabilities across various biological applications, supported by concrete case studies.\n    *   Analyzes practical considerations, opportunities, and challenges related to the adoption of KGEs for biological system modeling.\n\n4.  **Key Findings & Insights**\n    *   Traditional graph exploratory approaches for biological knowledge graphs, despite high accuracy, suffer from limited scalability due to time-consuming path exploration.\n    *   Knowledge Graph Embedding (KGE) models offer superior performance, accuracy, and scalability by learning low-rank vector representations of graph entities.\n    *   KGE models are identified as a \"natural fit\" for representing and analyzing complex biological knowledge graphs.\n    *   Their capabilities are demonstrated through applications like drug-target interaction prediction and polypharmacy side effect prediction.\n\n5.  **Research Gaps & Future Directions**\n    The survey \\cite{mohamed2020} identifies a need to address practical considerations and challenges associated with the adoption of KGE models in biological systems. It points towards future research opportunities in leveraging KGEs for more effective modeling of complex biological knowledge.\n\n6.  **Survey Contribution**\n    This survey \\cite{mohamed2020} uniquely contributes by providing a focused analysis of Knowledge Graph Embedding models specifically for biological knowledge graphs, highlighting their suitability and practical implications. It serves as a valuable resource for researchers considering KGEs for complex biological system modeling.",
      "intriguing_abstract": "Unraveling the intricate web of biological knowledge presents a formidable challenge, often hampered by the scalability limitations of traditional graph exploration methods. This survey illuminates the transformative potential of **Knowledge Graph Embedding (KGE) models** as a powerful paradigm for representing and analyzing complex **biological knowledge graphs**. We demonstrate why KGEs are a natural fit, offering superior performance, accuracy, and unprecedented scalability by learning efficient **low-rank vector representations** of biological entities and relations.\n\nThrough a comprehensive review, we showcase KGE models' remarkable predictive and analytical capabilities across critical biological applications, including precise **drug-target interaction prediction** and crucial **polypharmacy side effect prediction**. Beyond highlighting their successes, this paper provides a unique, focused analysis of the practical considerations, opportunities, and inherent challenges in adopting KGEs for biological system modeling. By bridging the gap between advanced computational methods and complex biological data, this work serves as an indispensable resource, guiding researchers toward leveraging KGEs for more effective and scalable discovery in the life sciences.",
      "keywords": [
        "Knowledge Graph Embedding (KGE) models",
        "biological knowledge graphs",
        "predictive and analytical capabilities",
        "drug-target interaction prediction",
        "polypharmacy side effect prediction",
        "scalability and accuracy",
        "low-rank vector representations",
        "complex biological system modeling",
        "practical considerations and challenges",
        "graph exploratory approaches",
        "survey analysis"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/2a25540e3ce0baba56ee71da7ca938f0264f790d.pdf",
      "citation_key": "mohamed2020",
      "metadata": {
        "title": "Biological applications of knowledge graph embedding models",
        "authors": [
          "Sameh K. Mohamed",
          "A. Nounu",
          "V. Novek"
        ],
        "published_date": "2020",
        "abstract": "Complex biological systems are traditionally modelled as graphs of interconnected biological entities. These graphs, i.e. biological knowledge graphs, are then processed using graph exploratory approaches to perform different types of analytical and predictive tasks. Despite the high predictive accuracy of these approaches, they have limited scalability due to their dependency on time-consuming path exploratory procedures. In recent years, owing to the rapid advances of computational technologies, new approaches for modelling graphs and mining them with high accuracy and scalability have emerged. These approaches, i.e. knowledge graph embedding (KGE) models, operate by learning low-rank vector representations of graph nodes and edges that preserve the graph's inherent structure. These approaches were used to analyse knowledge graphs from different domains where they showed superior performance and accuracy compared to previous graph exploratory approaches. In this work, we study this class of models in the context of biological knowledge graphs and their different applications. We then show how KGE models can be a natural fit for representing complex biological knowledge modelled as graphs. We also discuss their predictive and analytical capabilities in different biology applications. In this regard, we present two example case studies that demonstrate the capabilities of KGE models: prediction of drug-target interactions and polypharmacy side effects. Finally, we analyse different practical considerations for KGEs, and we discuss possible opportunities and challenges related to adopting them for modelling biological systems.",
        "file_path": "paper_data/knowledge_graph_embedding/2a25540e3ce0baba56ee71da7ca938f0264f790d.pdf",
        "venue": "Briefings Bioinform.",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey \\cite{mohamed2020} focuses on the application of Knowledge Graph Embedding (KGE) models within the domain of biological knowledge graphs. Its main objectives are to demonstrate how KGE models are a natural fit for representing complex biological knowledge, discuss their predictive and analytical capabilities across various biological applications, and analyze practical considerations, opportunities, and challenges associated with their adoption.\n\n2.  **Literature Coverage**\n    The paper reviews approaches that have emerged \"in recent years,\" specifically focusing on Knowledge Graph Embedding (KGE) models. While specific timeframes or detailed selection criteria for included literature are not explicitly provided, the scope is centered on advanced computational methods for graph modeling and mining.\n\n3.  **Classification Framework**\n    *   Examines KGE models specifically within the context of biological knowledge graphs.\n    *   Categorizes discussions by their predictive and analytical capabilities across various biological applications, supported by concrete case studies.\n    *   Analyzes practical considerations, opportunities, and challenges related to the adoption of KGEs for biological system modeling.\n\n4.  **Key Findings & Insights**\n    *   Traditional graph exploratory approaches for biological knowledge graphs, despite high accuracy, suffer from limited scalability due to time-consuming path exploration.\n    *   Knowledge Graph Embedding (KGE) models offer superior performance, accuracy, and scalability by learning low-rank vector representations of graph entities.\n    *   KGE models are identified as a \"natural fit\" for representing and analyzing complex biological knowledge graphs.\n    *   Their capabilities are demonstrated through applications like drug-target interaction prediction and polypharmacy side effect prediction.\n\n5.  **Research Gaps & Future Directions**\n    The survey \\cite{mohamed2020} identifies a need to address practical considerations and challenges associated with the adoption of KGE models in biological systems. It points towards future research opportunities in leveraging KGEs for more effective modeling of complex biological knowledge.\n\n6.  **Survey Contribution**\n    This survey \\cite{mohamed2020} uniquely contributes by providing a focused analysis of Knowledge Graph Embedding models specifically for biological knowledge graphs, highlighting their suitability and practical implications. It serves as a valuable resource for researchers considering KGEs for complex biological system modeling.",
        "keywords": [
          "Knowledge Graph Embedding (KGE) models",
          "biological knowledge graphs",
          "predictive and analytical capabilities",
          "drug-target interaction prediction",
          "polypharmacy side effect prediction",
          "scalability and accuracy",
          "low-rank vector representations",
          "complex biological system modeling",
          "practical considerations and challenges",
          "graph exploratory approaches",
          "survey analysis"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "2a25540e3ce0baba56ee71da7ca938f0264f790d.pdf"
    },
    {
      "success": true,
      "doc_id": "bafc184077782c68d7b9afa5b2472219",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   Knowledge graph embedding (KGE) models need to effectively learn low-dimensional representations of entities and relations to support various applications \\cite{gao2020}.\n    *   A critical challenge is for KGE models to accurately capture and infer diverse relation patterns, including symmetry/antisymmetry, inversion, and especially the non-commutative composition pattern \\cite{gao2020}.\n    *   Modeling non-commutative composition is crucial for robust multi-hop reasoning within knowledge graphs \\cite{gao2020}.\n\n2.  **Related Work & Positioning**\n    *   Most existing KGE methods struggle or fail to adequately model the non-commutative composition pattern of relations \\cite{gao2020}.\n    *   This work positions itself as a solution to this specific limitation, particularly for multi-hop reasoning where relation order matters \\cite{gao2020}.\n\n3.  **Technical Approach & Innovation**\n    *   The paper proposes a novel model called **Rotate3D** \\cite{gao2020}.\n    *   **Core Method**: Rotate3D maps entities into a three-dimensional (3D) space \\cite{gao2020}.\n    *   **Relation Definition**: Relations are defined as rotations that transform a head entity's embedding to a tail entity's embedding in this 3D space \\cite{gao2020}.\n    *   **Novelty**: The key innovation lies in leveraging the inherent non-commutative composition property of rotations in 3D space. This allows Rotate3D to naturally preserve the order of relation composition, directly addressing the limitations of previous models \\cite{gao2020}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of Rotate3D, a KGE model that embeds entities in 3D space and models relations as rotations \\cite{gao2020}.\n    *   **Mechanism for Composition**: A novel mechanism for modeling non-commutative relation composition by exploiting the mathematical properties of 3D rotations \\cite{gao2020}.\n    *   **Pattern Capture**: Demonstrated ability to effectively capture various relation patterns, with a significant improvement in modeling composition \\cite{gao2020}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed on standard tasks including link prediction and path query answering \\cite{gao2020}.\n    *   **Performance Metrics**: Performance was evaluated against existing state-of-the-art models \\cite{gao2020}.\n    *   **Key Results**: Rotate3D consistently outperforms existing state-of-the-art models on both link prediction and path query answering tasks \\cite{gao2020}.\n    *   **Case Studies**: Further qualitative case studies confirmed Rotate3D's effectiveness in capturing diverse relation patterns, particularly highlighting its marked improvement in modeling the composition pattern \\cite{gao2020}.\n\n6.  **Limitations & Scope**\n    *   The provided abstract does not explicitly state technical limitations or assumptions of Rotate3D.\n    *   The scope of applicability is primarily knowledge graph embedding tasks requiring robust modeling of relation patterns, especially non-commutative composition for multi-hop reasoning \\cite{gao2020}.\n\n7.  **Technical Significance**\n    *   Rotate3D significantly advances the technical state-of-the-art in knowledge graph embedding by providing a robust and natural way to model non-commutative relation composition \\cite{gao2020}.\n    *   This innovation has potential impact on future research in multi-hop reasoning, complex query answering, and other applications where the order of relations is crucial \\cite{gao2020}.\n    *   It demonstrates the power of geometric transformations (rotations) in capturing complex semantic properties within knowledge graphs \\cite{gao2020}.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graph Embedding (KGE) for complex reasoning hinges on accurately modeling diverse relation patterns, particularly the elusive non-commutative composition. Existing KGE models often falter in capturing the sequential order inherent in multi-hop reasoning, limiting their inferential power. We introduce **Rotate3D**, a novel KGE model that redefines how relations are understood. By embedding entities into a three-dimensional (3D) space and defining relations as precise 3D rotations, Rotate3D inherently leverages the non-commutative properties of rotations. This elegant geometric approach naturally preserves the order of relation composition, a critical advancement for robust multi-hop reasoning. Our extensive experiments on standard benchmarks demonstrate that Rotate3D consistently outperforms state-of-the-art models in both link prediction and path query answering. Furthermore, qualitative analyses confirm its superior ability to capture complex relation patterns, especially composition. Rotate3D represents a significant leap forward, offering a powerful paradigm for knowledge graph reasoning and paving the way for more sophisticated AI applications.",
      "keywords": [
        "Knowledge graph embedding (KGE)",
        "non-commutative composition pattern",
        "multi-hop reasoning",
        "Rotate3D model",
        "3D space entity embedding",
        "relations as 3D rotations",
        "geometric transformations",
        "link prediction",
        "path query answering",
        "state-of-the-art performance",
        "diverse relation patterns",
        "complex query answering"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4.pdf",
      "citation_key": "gao2020",
      "metadata": {
        "title": "Rotate3D: Representing Relations as Rotations in Three-Dimensional Space for Knowledge Graph Embedding",
        "authors": [
          "Chang Gao",
          "Chengjie Sun",
          "Lili Shan",
          "Lei Lin",
          "Mingjiang Wang"
        ],
        "published_date": "2020",
        "abstract": "Knowledge graph embedding, which aims to learn low-dimensional embeddings of entities and relations, plays a vital role in a wide range of applications. It is crucial for knowledge graph embedding models to model and infer various relation patterns, such as symmetry/antisymmetry, inversion, and composition. However, most existing methods fail to model the non-commutative composition pattern, which is essential, especially for multi-hop reasoning. To address this issue, we propose a new model called Rotate3D, which maps entities to the three-dimensional space and defines relations as rotations from head entities to tail entities. By using the non-commutative composition property of rotations in the three-dimensional space, Rotate3D can naturally preserve the order of the composition of relations. Experiments show that Rotate3D outperforms existing state-of-the-art models for link prediction and path query answering. Further case studies demonstrate that Rotate3D can effectively capture various relation patterns with a marked improvement in modeling the composition pattern.",
        "file_path": "paper_data/knowledge_graph_embedding/d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   Knowledge graph embedding (KGE) models need to effectively learn low-dimensional representations of entities and relations to support various applications \\cite{gao2020}.\n    *   A critical challenge is for KGE models to accurately capture and infer diverse relation patterns, including symmetry/antisymmetry, inversion, and especially the non-commutative composition pattern \\cite{gao2020}.\n    *   Modeling non-commutative composition is crucial for robust multi-hop reasoning within knowledge graphs \\cite{gao2020}.\n\n2.  **Related Work & Positioning**\n    *   Most existing KGE methods struggle or fail to adequately model the non-commutative composition pattern of relations \\cite{gao2020}.\n    *   This work positions itself as a solution to this specific limitation, particularly for multi-hop reasoning where relation order matters \\cite{gao2020}.\n\n3.  **Technical Approach & Innovation**\n    *   The paper proposes a novel model called **Rotate3D** \\cite{gao2020}.\n    *   **Core Method**: Rotate3D maps entities into a three-dimensional (3D) space \\cite{gao2020}.\n    *   **Relation Definition**: Relations are defined as rotations that transform a head entity's embedding to a tail entity's embedding in this 3D space \\cite{gao2020}.\n    *   **Novelty**: The key innovation lies in leveraging the inherent non-commutative composition property of rotations in 3D space. This allows Rotate3D to naturally preserve the order of relation composition, directly addressing the limitations of previous models \\cite{gao2020}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of Rotate3D, a KGE model that embeds entities in 3D space and models relations as rotations \\cite{gao2020}.\n    *   **Mechanism for Composition**: A novel mechanism for modeling non-commutative relation composition by exploiting the mathematical properties of 3D rotations \\cite{gao2020}.\n    *   **Pattern Capture**: Demonstrated ability to effectively capture various relation patterns, with a significant improvement in modeling composition \\cite{gao2020}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed on standard tasks including link prediction and path query answering \\cite{gao2020}.\n    *   **Performance Metrics**: Performance was evaluated against existing state-of-the-art models \\cite{gao2020}.\n    *   **Key Results**: Rotate3D consistently outperforms existing state-of-the-art models on both link prediction and path query answering tasks \\cite{gao2020}.\n    *   **Case Studies**: Further qualitative case studies confirmed Rotate3D's effectiveness in capturing diverse relation patterns, particularly highlighting its marked improvement in modeling the composition pattern \\cite{gao2020}.\n\n6.  **Limitations & Scope**\n    *   The provided abstract does not explicitly state technical limitations or assumptions of Rotate3D.\n    *   The scope of applicability is primarily knowledge graph embedding tasks requiring robust modeling of relation patterns, especially non-commutative composition for multi-hop reasoning \\cite{gao2020}.\n\n7.  **Technical Significance**\n    *   Rotate3D significantly advances the technical state-of-the-art in knowledge graph embedding by providing a robust and natural way to model non-commutative relation composition \\cite{gao2020}.\n    *   This innovation has potential impact on future research in multi-hop reasoning, complex query answering, and other applications where the order of relations is crucial \\cite{gao2020}.\n    *   It demonstrates the power of geometric transformations (rotations) in capturing complex semantic properties within knowledge graphs \\cite{gao2020}.",
        "keywords": [
          "Knowledge graph embedding (KGE)",
          "non-commutative composition pattern",
          "multi-hop reasoning",
          "Rotate3D model",
          "3D space entity embedding",
          "relations as 3D rotations",
          "geometric transformations",
          "link prediction",
          "path query answering",
          "state-of-the-art performance",
          "diverse relation patterns",
          "complex query answering"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4.pdf"
    },
    {
      "success": true,
      "doc_id": "776ec7f51b9d20e50cecfa9aed7239f6",
      "summary": "Here's a focused summary of the paper \"Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis\" \\cite{peng2021} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Existing Knowledge Graph Embedding (KGE) studies primarily focus on improving model performance, often overlooking the significant computational cost in terms of execution time and environmental impact (carbon footprint).\n    *   This problem is critical due to the widespread application of KGEs in NLP tasks (e.g., question answering, search engines) and the increasing energy requirements of modern AI models, necessitating more computationally cheap and eco-friendly approaches.\n\n*   **Related Work & Positioning**\n    *   Previous efforts to reduce computational cost in KGEs often focused on reducing model parameters (e.g., using quaternions).\n    *   Existing neural KGE frameworks typically rely on random mini-batches, which are difficult to parallelize efficiently due to potential synchronization errors when updating relation embeddings.\n    *   Orthogonal constraints in KGEs (e.g., RotatE, OTE) either limit modeling capacity (RotatE's 2D relations) or are computationally expensive due to gradient descent and iterative orthogonalization (OTE's Gram-Schmidt).\n    *   Most KGE methods employ negative sampling, which, while reducing training time for gradient-based updates, can become a bandwidth bottleneck when gradient computation is no longer the primary constraint.\n\n*   **Technical Approach & Innovation**\n    *   **PROCRUSTES** is a lightweight, fast, and eco-friendly KGE training technique built upon three core innovations:\n        *   **Full Batch Learning via Relational Matrices**: Instead of random batches, tuples are grouped by their relations. This transforms tuple-level computation into matrix-level arithmetic, ensures each relation embedding is accessed by only one process (avoiding data corruption), and enables robust parallelization of KGE training. The objective function is formulated as `L = sum(i=1 to m) sum(j=1 to d/ds) ||H_i,j R_i,j - T_i,j||^2`.\n        *   **Closed-Form Orthogonal Procrustes Analysis for KGEs**: To minimize the Euclidean distance between head and tail entity matrices while enforcing orthogonality on relation matrices (`R_i,j`), \\cite{peng2021} leverages a closed-form solution derived from Singular Value Decomposition (SVD). Specifically, `R*_i,j = UV^T` where `SVD(H_i,j^T T_i,j) = U S V^T`. This allows for instant, globally optimal updates of relation embeddings in each iteration, drastically speeding up training compared to gradient-descent methods.\n        *   **Non-Negative-Sampling Training**: With the closed-form solution making gradient computation no longer a bottleneck, the paper identifies negative sampling as a new bandwidth bottleneck. PROCRUSTES eliminates negative sampling, updating all embeddings with positive samples only, further optimizing training speed.\n    *   **Segmented Embeddings**: The model is built upon segmented embeddings, where entity representation space is divided into multiple independent sub-spaces, allowing parallel processing and enhancing expressiveness.\n    *   **Spherisation Constraints**: To prevent the model from collapsing into a trivial optimum (all zeros), two spherisation steps are applied per epoch: centring (column-wise sum of matrices becomes zero) and length normalization (row-wise Euclidean norm of entity sub-vectors is one).\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of a KGE framework that integrates full batch learning based on relational matrices, a closed-form solution for Orthogonal Procrustes Analysis, and non-negative-sampling training.\n    *   **System Design/Architectural Innovations**: A parallelizable KGE training architecture where computation is decomposed into `m * (d/ds)` independent processes, significantly enhancing training speed and stability.\n    *   **Theoretical Insights**: Demonstrates that by restructuring the KGE optimization problem, a computationally expensive gradient-descent task can be transformed into an instantly solvable closed-form problem, leading to orders-of-magnitude efficiency gains.\n    *   **Semantic Richness**: For the first time, entity embeddings are shown to encode full relation information, allowing direct restoration of relation embeddings and leading to highly interpretable and semantically rich entity representations.\n\n*   **Experimental Validation**\n    *   **Experiments**: Conducted multi-relational link prediction experiments on two standard benchmark datasets: WN18RR and FB15k-237.\n    *   **Baselines**: Compared against 13 strong baselines, including classical methods (TransE, DistMult, ComplEx) and recent state-of-the-art approaches (RotatE, OTE, SACN, TuckER).\n    *   **Performance Metrics**: Evaluated using Mean Reciprocal Rank (MRR) and Hit Ratio (H1, H3, H10).\n    *   **Efficiency Metrics**: Measured training time (minutes) and carbon dioxide production (grams).\n    *   **Key Results**:\n        *   **Effectiveness**: PROCRUSTES achieves competitive performance, often matching or exceeding state-of-the-art models. On WN18RR, it outperforms RotatE and OTE in MRR. Ablation studies show that variants with negative sampling and traditional batching can achieve even higher performance, sometimes reaching SOTA, albeit with increased computational cost.\n        *   **Efficiency**: PROCRUSTES significantly reduces training time (e.g., 14 minutes for WN18RR, 9 minutes for FB15k-237) by up to 98.4% compared to baselines. It also drastically lowers the carbon footprint (e.g., 37g CO2 for WN18RR, 42g for FB15k-237), representing up to a 99.3% reduction.\n        *   **Interpretability**: Visualizations demonstrate that PROCRUSTES's entity embeddings cluster semantically related entities, confirming their richer information content and interpretability.\n\n*   **Limitations & Scope**\n    *   The base PROCRUSTES model, while highly efficient, might not always achieve the absolute highest performance compared to *all* SOTA models, especially on FB15k-237, where its variants with negative sampling and traditional batching perform better but are less efficient. This suggests a potential trade-off between extreme efficiency and peak performance in some scenarios.\n    *   The model requires specific spherisation constraints (centring and length normalization) to prevent collapse to a trivial optimum, indicating a potential instability without these safeguards.\n    *   The scope of applicability is primarily KGEs for link prediction, particularly in scenarios where computational resources and environmental impact are critical considerations.\n\n*   **Technical Significance**\n    *   \\cite{peng2021} significantly advances the technical state-of-the-art in KGE training by providing an algorithmically efficient framework that drastically reduces computational cost and environmental impact without sacrificing competitive performance.\n    *   It introduces a novel paradigm for KGE optimization by leveraging closed-form solutions and rethinking batching strategies, offering a blueprint for \"green AI\" in KGEs.\n    *   The ability to encode full relation information within entity embeddings and achieve high interpretability opens new avenues for understanding and utilizing KGEs.\n    *   This work has the potential to impact future research by encouraging the development of more sustainable and efficient AI models, particularly in resource-constrained environments or for large-scale knowledge graphs.",
      "intriguing_abstract": "As Knowledge Graph Embeddings (KGEs) become indispensable for AI applications, their escalating computational cost and environmental footprint demand urgent attention. We introduce PROCRUSTES, a groundbreaking KGE training framework that redefines efficiency. Diverging from iterative gradient-descent, PROCRUSTES leverages a novel closed-form solution derived from Singular Value Decomposition (SVD) for Orthogonal Procrustes Analysis, enabling instant, globally optimal updates of relation embeddings. This innovation is coupled with full batch learning via relational matrices, ensuring robust parallelization, and non-negative-sampling training, which eliminates a critical bandwidth bottleneck. Our approach drastically accelerates the learning process.\n\nExperiments on WN18RR and FB15k-237 demonstrate unprecedented efficiency, reducing training time by up to 98.4% and carbon emissions by 99.3%, while maintaining competitive link prediction performance. PROCRUSTES not only offers a blueprint for sustainable, \"green AI\" in KGEs but also yields highly interpretable entity embeddings encoding full relational information. This work transforms KGE optimization, paving the way for scalable and eco-conscious AI development.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "PROCRUSTES",
        "Orthogonal Procrustes Analysis",
        "Closed-Form Solution",
        "Full Batch Learning",
        "Non-Negative-Sampling Training",
        "Parallelizable KGE Training",
        "Computational Efficiency",
        "Carbon Footprint Reduction",
        "Semantically Rich Entity Representations",
        "Link Prediction",
        "Green AI",
        "Singular Value Decomposition (SVD)"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/d7ef14459674b75807cd9be549f1e12d53849ead.pdf",
      "citation_key": "peng2021",
      "metadata": {
        "title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis",
        "authors": [
          "Xutan Peng",
          "Guanyi Chen",
          "Chenghua Lin",
          "Mark Stevenson"
        ],
        "published_date": "2021",
        "abstract": "Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm.",
        "file_path": "paper_data/knowledge_graph_embedding/d7ef14459674b75807cd9be549f1e12d53849ead.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis\" \\cite{peng2021} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Existing Knowledge Graph Embedding (KGE) studies primarily focus on improving model performance, often overlooking the significant computational cost in terms of execution time and environmental impact (carbon footprint).\n    *   This problem is critical due to the widespread application of KGEs in NLP tasks (e.g., question answering, search engines) and the increasing energy requirements of modern AI models, necessitating more computationally cheap and eco-friendly approaches.\n\n*   **Related Work & Positioning**\n    *   Previous efforts to reduce computational cost in KGEs often focused on reducing model parameters (e.g., using quaternions).\n    *   Existing neural KGE frameworks typically rely on random mini-batches, which are difficult to parallelize efficiently due to potential synchronization errors when updating relation embeddings.\n    *   Orthogonal constraints in KGEs (e.g., RotatE, OTE) either limit modeling capacity (RotatE's 2D relations) or are computationally expensive due to gradient descent and iterative orthogonalization (OTE's Gram-Schmidt).\n    *   Most KGE methods employ negative sampling, which, while reducing training time for gradient-based updates, can become a bandwidth bottleneck when gradient computation is no longer the primary constraint.\n\n*   **Technical Approach & Innovation**\n    *   **PROCRUSTES** is a lightweight, fast, and eco-friendly KGE training technique built upon three core innovations:\n        *   **Full Batch Learning via Relational Matrices**: Instead of random batches, tuples are grouped by their relations. This transforms tuple-level computation into matrix-level arithmetic, ensures each relation embedding is accessed by only one process (avoiding data corruption), and enables robust parallelization of KGE training. The objective function is formulated as `L = sum(i=1 to m) sum(j=1 to d/ds) ||H_i,j R_i,j - T_i,j||^2`.\n        *   **Closed-Form Orthogonal Procrustes Analysis for KGEs**: To minimize the Euclidean distance between head and tail entity matrices while enforcing orthogonality on relation matrices (`R_i,j`), \\cite{peng2021} leverages a closed-form solution derived from Singular Value Decomposition (SVD). Specifically, `R*_i,j = UV^T` where `SVD(H_i,j^T T_i,j) = U S V^T`. This allows for instant, globally optimal updates of relation embeddings in each iteration, drastically speeding up training compared to gradient-descent methods.\n        *   **Non-Negative-Sampling Training**: With the closed-form solution making gradient computation no longer a bottleneck, the paper identifies negative sampling as a new bandwidth bottleneck. PROCRUSTES eliminates negative sampling, updating all embeddings with positive samples only, further optimizing training speed.\n    *   **Segmented Embeddings**: The model is built upon segmented embeddings, where entity representation space is divided into multiple independent sub-spaces, allowing parallel processing and enhancing expressiveness.\n    *   **Spherisation Constraints**: To prevent the model from collapsing into a trivial optimum (all zeros), two spherisation steps are applied per epoch: centring (column-wise sum of matrices becomes zero) and length normalization (row-wise Euclidean norm of entity sub-vectors is one).\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of a KGE framework that integrates full batch learning based on relational matrices, a closed-form solution for Orthogonal Procrustes Analysis, and non-negative-sampling training.\n    *   **System Design/Architectural Innovations**: A parallelizable KGE training architecture where computation is decomposed into `m * (d/ds)` independent processes, significantly enhancing training speed and stability.\n    *   **Theoretical Insights**: Demonstrates that by restructuring the KGE optimization problem, a computationally expensive gradient-descent task can be transformed into an instantly solvable closed-form problem, leading to orders-of-magnitude efficiency gains.\n    *   **Semantic Richness**: For the first time, entity embeddings are shown to encode full relation information, allowing direct restoration of relation embeddings and leading to highly interpretable and semantically rich entity representations.\n\n*   **Experimental Validation**\n    *   **Experiments**: Conducted multi-relational link prediction experiments on two standard benchmark datasets: WN18RR and FB15k-237.\n    *   **Baselines**: Compared against 13 strong baselines, including classical methods (TransE, DistMult, ComplEx) and recent state-of-the-art approaches (RotatE, OTE, SACN, TuckER).\n    *   **Performance Metrics**: Evaluated using Mean Reciprocal Rank (MRR) and Hit Ratio (H1, H3, H10).\n    *   **Efficiency Metrics**: Measured training time (minutes) and carbon dioxide production (grams).\n    *   **Key Results**:\n        *   **Effectiveness**: PROCRUSTES achieves competitive performance, often matching or exceeding state-of-the-art models. On WN18RR, it outperforms RotatE and OTE in MRR. Ablation studies show that variants with negative sampling and traditional batching can achieve even higher performance, sometimes reaching SOTA, albeit with increased computational cost.\n        *   **Efficiency**: PROCRUSTES significantly reduces training time (e.g., 14 minutes for WN18RR, 9 minutes for FB15k-237) by up to 98.4% compared to baselines. It also drastically lowers the carbon footprint (e.g., 37g CO2 for WN18RR, 42g for FB15k-237), representing up to a 99.3% reduction.\n        *   **Interpretability**: Visualizations demonstrate that PROCRUSTES's entity embeddings cluster semantically related entities, confirming their richer information content and interpretability.\n\n*   **Limitations & Scope**\n    *   The base PROCRUSTES model, while highly efficient, might not always achieve the absolute highest performance compared to *all* SOTA models, especially on FB15k-237, where its variants with negative sampling and traditional batching perform better but are less efficient. This suggests a potential trade-off between extreme efficiency and peak performance in some scenarios.\n    *   The model requires specific spherisation constraints (centring and length normalization) to prevent collapse to a trivial optimum, indicating a potential instability without these safeguards.\n    *   The scope of applicability is primarily KGEs for link prediction, particularly in scenarios where computational resources and environmental impact are critical considerations.\n\n*   **Technical Significance**\n    *   \\cite{peng2021} significantly advances the technical state-of-the-art in KGE training by providing an algorithmically efficient framework that drastically reduces computational cost and environmental impact without sacrificing competitive performance.\n    *   It introduces a novel paradigm for KGE optimization by leveraging closed-form solutions and rethinking batching strategies, offering a blueprint for \"green AI\" in KGEs.\n    *   The ability to encode full relation information within entity embeddings and achieve high interpretability opens new avenues for understanding and utilizing KGEs.\n    *   This work has the potential to impact future research by encouraging the development of more sustainable and efficient AI models, particularly in resource-constrained environments or for large-scale knowledge graphs.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "PROCRUSTES",
          "Orthogonal Procrustes Analysis",
          "Closed-Form Solution",
          "Full Batch Learning",
          "Non-Negative-Sampling Training",
          "Parallelizable KGE Training",
          "Computational Efficiency",
          "Carbon Footprint Reduction",
          "Semantically Rich Entity Representations",
          "Link Prediction",
          "Green AI",
          "Singular Value Decomposition (SVD)"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "d7ef14459674b75807cd9be549f1e12d53849ead.pdf"
    },
    {
      "success": true,
      "doc_id": "c31a46857a5552efe5cd2d330f26580f",
      "summary": "Here's a focused summary of the technical paper for literature review, adhering to the specified citation and format requirements:\n\n### Focused Summary for Literature Review\n\n#### 1. Research Problem & Motivation\n*   **Specific Technical Problem**: The paper addresses the challenge of knowledge graph embedding (KGE) for reasoning over known facts and inferring missing links, specifically focusing on improving the representation learning of entities and relations.\n*   **Importance and Challenge**: KGE is crucial for various AI tasks. Existing methods struggle because they either ignore the inherent graph structure (triplet-based) or overlook crucial contextual information of nodes (graph-based), leading to an inability to discern valuable entity/relation information and accurately predict missing links.\n\n#### 2. Related Work & Positioning\n*   **Existing Approaches**: Previous KGE methods are broadly categorized into triplet-based and graph-based approaches.\n*   **Limitations of Previous Solutions**:\n    *   **Triplet-based approaches**: Learn embeddings from single triples, neglecting the broader graph structure and inter-triplet relationships \\cite{shi2025}.\n    *   **Graph-based methods**: While considering graph structure, they fail to incorporate contextual information of nodes, hindering their ability to capture nuanced entity/relation semantics \\cite{shi2025}.\n\n#### 3. Technical Approach & Innovation\n*   **Core Technical Method**: The paper proposes a general **Graph Transformer Framework for Knowledge Graph Embedding (TGformer)** \\cite{shi2025}.\n    *   It constructs a **context-level subgraph** for each predicted triplet, explicitly modeling relationships between triplets sharing the same entity \\cite{shi2025}.\n    *   It employs a **Knowledge Graph Transformer Network (KGTN)** designed to comprehensively explore multi-structural features (triplet-level and graph-level) within knowledge graphs \\cite{shi2025}.\n    *   Finally, **semantic matching** is used to select the entity with the highest score for link prediction \\cite{shi2025}.\n*   **Novelty**: TGformer is presented as the first framework to leverage a graph transformer for building knowledge embeddings by integrating both triplet-level and graph-level structural features across static and temporal knowledge graphs \\cite{shi2025}. This multi-structural and contextual understanding is a key differentiator.\n\n#### 4. Key Technical Contributions\n*   **Novel Algorithms/Methods**:\n    *   Introduction of **TGformer**, a novel graph transformer framework for KGE \\cite{shi2025}.\n    *   Design of a **context-level subgraph construction** mechanism to capture inter-triplet relationships based on shared entities \\cite{shi2025}.\n    *   Development of the **Knowledge Graph Transformer Network (KGTN)**, specifically tailored to explore multi-structural features (triplet-level and graph-level) and contextual information \\cite{shi2025}.\n*   **Architectural Innovations**: The framework uniquely integrates graph transformer capabilities to process both static and temporal knowledge graphs, considering both fine-grained triplet-level and broader graph-level structural information \\cite{shi2025}.\n\n#### 5. Experimental Validation\n*   **Experiments Conducted**: The method was evaluated on the task of link prediction \\cite{shi2025}.\n*   **Key Performance Metrics & Results**: Experimental results on several public knowledge graph datasets demonstrate that TGformer achieves **state-of-the-art performance** in link prediction \\cite{shi2025}.\n\n#### 6. Limitations & Scope\n*   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the limitations of prior triplet-based and graph-based methods by integrating contextual and multi-structural features. It does not explicitly detail specific technical limitations of the TGformer itself within the provided abstract.\n*   **Scope of Applicability**: The framework is designed for knowledge graph embedding in both static and temporal knowledge graphs, with a primary application demonstrated in link prediction \\cite{shi2025}.\n\n#### 7. Technical Significance\n*   **Advancement of State-of-the-Art**: TGformer significantly advances the technical state-of-the-art in KGE by being the first to effectively apply graph transformers to integrate both triplet-level and graph-level structural features, along with contextual information, for robust entity and relation understanding \\cite{shi2025}.\n*   **Potential Impact**: This work provides a novel paradigm for KGE, potentially influencing future research in graph neural networks for knowledge representation, especially in scenarios requiring a deep understanding of multi-faceted structural and contextual information within complex knowledge graphs. It opens avenues for more sophisticated reasoning and inference tasks beyond link prediction.",
      "intriguing_abstract": "The intricate challenge of Knowledge Graph Embedding (KGE) lies in robustly representing entities and relations while inferring missing links, a task often hampered by methods that either neglect crucial graph structure or overlook vital contextual information. We introduce **TGformer**, a pioneering Graph Transformer Framework designed to revolutionize KGE by comprehensively addressing these limitations.\n\nTGformer's innovation stems from two core components: a novel **context-level subgraph** construction that explicitly models inter-triplet relationships, and a sophisticated **Knowledge Graph Transformer Network (KGTN)**. This unique architecture is the first to seamlessly integrate both fine-grained **triplet-level** and broader **graph-level multi-structural features**, alongside rich contextual information, across both **static and temporal knowledge graphs**. Our framework significantly enhances entity and relation representation learning.\n\nEvaluated on the critical task of **link prediction**, TGformer achieves state-of-the-art performance across multiple public datasets. This work not only advances the technical frontier of KGE but also establishes a novel paradigm for leveraging graph transformers in knowledge representation, paving the way for more sophisticated reasoning and inference capabilities in complex AI systems.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Graph Transformer Framework (TGformer)",
        "Link Prediction",
        "Entity/Relation Representation Learning",
        "Context-level Subgraph Construction",
        "Knowledge Graph Transformer Network (KGTN)",
        "Multi-structural Features",
        "Contextual Information",
        "Static and Temporal Knowledge Graphs",
        "State-of-the-Art Performance",
        "Novel Paradigm for KGE"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/3f170af3566f055e758fa3bdf2bfd3a0e8787e58.pdf",
      "citation_key": "shi2025",
      "metadata": {
        "title": "TGformer: A Graph Transformer Framework for Knowledge Graph Embedding",
        "authors": [
          "Fobo Shi",
          "Duantengchuan Li",
          "Xiaoguang Wang",
          "Bing Li",
          "Xindong Wu"
        ],
        "published_date": "2025",
        "abstract": "Knowledge graph embedding is efficient method for reasoning over known facts and inferring missing links. Existing methods are mainly triplet-based or graph-based. Triplet-based approaches learn the embedding of missing entities by a single triple only. They ignore the fact that the knowledge graph is essentially a graph structure. Graph-based methods consider graph structure information but ignore the contextual information of nodes in the knowledge graph, making them unable to discern valuable entity (relation) information. In response to the above limitations, we propose a general graph transformer framework for knowledge graph embedding (TGformer). It is the first to use a graph transformer to build knowledge embeddings with triplet-level and graph-level structural features in the static and temporal knowledge graph. Specifically, a context-level subgraph is constructed for each predicted triplet, which models the relation between triplets with the same entity. Afterward, we design a knowledge graph transformer network (KGTN) to fully explore multi-structural features in knowledge graphs, including triplet-level and graph-level, boosting the model to understand entities (relations) in different contexts. Finally, semantic matching is adopted to select the entity with the highest score. Experimental results on several public knowledge graph datasets show that our method can achieve state-of-the-art performance in link prediction.",
        "file_path": "paper_data/knowledge_graph_embedding/3f170af3566f055e758fa3bdf2bfd3a0e8787e58.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review, adhering to the specified citation and format requirements:\n\n### Focused Summary for Literature Review\n\n#### 1. Research Problem & Motivation\n*   **Specific Technical Problem**: The paper addresses the challenge of knowledge graph embedding (KGE) for reasoning over known facts and inferring missing links, specifically focusing on improving the representation learning of entities and relations.\n*   **Importance and Challenge**: KGE is crucial for various AI tasks. Existing methods struggle because they either ignore the inherent graph structure (triplet-based) or overlook crucial contextual information of nodes (graph-based), leading to an inability to discern valuable entity/relation information and accurately predict missing links.\n\n#### 2. Related Work & Positioning\n*   **Existing Approaches**: Previous KGE methods are broadly categorized into triplet-based and graph-based approaches.\n*   **Limitations of Previous Solutions**:\n    *   **Triplet-based approaches**: Learn embeddings from single triples, neglecting the broader graph structure and inter-triplet relationships \\cite{shi2025}.\n    *   **Graph-based methods**: While considering graph structure, they fail to incorporate contextual information of nodes, hindering their ability to capture nuanced entity/relation semantics \\cite{shi2025}.\n\n#### 3. Technical Approach & Innovation\n*   **Core Technical Method**: The paper proposes a general **Graph Transformer Framework for Knowledge Graph Embedding (TGformer)** \\cite{shi2025}.\n    *   It constructs a **context-level subgraph** for each predicted triplet, explicitly modeling relationships between triplets sharing the same entity \\cite{shi2025}.\n    *   It employs a **Knowledge Graph Transformer Network (KGTN)** designed to comprehensively explore multi-structural features (triplet-level and graph-level) within knowledge graphs \\cite{shi2025}.\n    *   Finally, **semantic matching** is used to select the entity with the highest score for link prediction \\cite{shi2025}.\n*   **Novelty**: TGformer is presented as the first framework to leverage a graph transformer for building knowledge embeddings by integrating both triplet-level and graph-level structural features across static and temporal knowledge graphs \\cite{shi2025}. This multi-structural and contextual understanding is a key differentiator.\n\n#### 4. Key Technical Contributions\n*   **Novel Algorithms/Methods**:\n    *   Introduction of **TGformer**, a novel graph transformer framework for KGE \\cite{shi2025}.\n    *   Design of a **context-level subgraph construction** mechanism to capture inter-triplet relationships based on shared entities \\cite{shi2025}.\n    *   Development of the **Knowledge Graph Transformer Network (KGTN)**, specifically tailored to explore multi-structural features (triplet-level and graph-level) and contextual information \\cite{shi2025}.\n*   **Architectural Innovations**: The framework uniquely integrates graph transformer capabilities to process both static and temporal knowledge graphs, considering both fine-grained triplet-level and broader graph-level structural information \\cite{shi2025}.\n\n#### 5. Experimental Validation\n*   **Experiments Conducted**: The method was evaluated on the task of link prediction \\cite{shi2025}.\n*   **Key Performance Metrics & Results**: Experimental results on several public knowledge graph datasets demonstrate that TGformer achieves **state-of-the-art performance** in link prediction \\cite{shi2025}.\n\n#### 6. Limitations & Scope\n*   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the limitations of prior triplet-based and graph-based methods by integrating contextual and multi-structural features. It does not explicitly detail specific technical limitations of the TGformer itself within the provided abstract.\n*   **Scope of Applicability**: The framework is designed for knowledge graph embedding in both static and temporal knowledge graphs, with a primary application demonstrated in link prediction \\cite{shi2025}.\n\n#### 7. Technical Significance\n*   **Advancement of State-of-the-Art**: TGformer significantly advances the technical state-of-the-art in KGE by being the first to effectively apply graph transformers to integrate both triplet-level and graph-level structural features, along with contextual information, for robust entity and relation understanding \\cite{shi2025}.\n*   **Potential Impact**: This work provides a novel paradigm for KGE, potentially influencing future research in graph neural networks for knowledge representation, especially in scenarios requiring a deep understanding of multi-faceted structural and contextual information within complex knowledge graphs. It opens avenues for more sophisticated reasoning and inference tasks beyond link prediction.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Graph Transformer Framework (TGformer)",
          "Link Prediction",
          "Entity/Relation Representation Learning",
          "Context-level Subgraph Construction",
          "Knowledge Graph Transformer Network (KGTN)",
          "Multi-structural Features",
          "Contextual Information",
          "Static and Temporal Knowledge Graphs",
          "State-of-the-Art Performance",
          "Novel Paradigm for KGE"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "3f170af3566f055e758fa3bdf2bfd3a0e8787e58.pdf"
    },
    {
      "success": true,
      "doc_id": "f7989f301fa3bfd898dfe8fa4bdfd91d",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review: Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph\n\nThis paper, \"Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph\" by Zhang et al. \\cite{zhang2024}, introduces a novel approach to address the challenges of semantic disparity in Federated Knowledge Graph Embedding (FKGE).\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing FKGE methods typically use a simple arithmetic mean of entity embeddings from all clients as global supplementary knowledge and learn a replica of global consensus entity embeddings. This approach neglects the inherent semantic disparities among distinct clients, leading to:\n        *   Globally shared complementary knowledge being \"inundated with too much noise\" when tailored to a specific client \\cite{zhang2024}.\n        *   A discrepancy between local and global optimization objectives, compromising the quality of learned embeddings \\cite{zhang2024}.\n    *   **Importance and Challenge**: With the rise of data privacy regulations like GDPR, KGs are increasingly distributed across multiple clients (Federated Knowledge Graphs, FKG). FKGE aims to collaboratively learn embeddings from these distributed KGs while preserving privacy. The core challenge is that KGs from different clients often have varying relation sets and thus diverse semantics for shared entities (semantic disparity), making a \"one-size-fits-all\" global aggregation ineffective \\cite{zhang2024}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Current FKGE methods primarily fall into client-server (e.g., FedE, FedEC, FedR) or peer-to-peer (e.g., FKGE) architectures \\cite{zhang2024}.\n        *   FedE \\cite{zhang2024} is a pioneering model using simple averaging for aggregation. FedEC \\cite{zhang2024} enhances FedE with embedding-contrastive learning. FedR \\cite{zhang2024} focuses on scenarios with shared entities and relations.\n    *   **Limitations of Previous Solutions**:\n        *   FedE and FedEC \\cite{zhang2024} ignore the client-wise relation graph during aggregation, leading to suboptimal embedding quality. Their averaging strategy provides a universally shared global supplementary knowledge that may contain too much irrelevant information for specific clients due to semantic disparities.\n        *   These methods typically learn a global consensus entity embedding for all clients, which can lead to a divergence between local and global optimization objectives, especially for KGs with few shared relations \\cite{zhang2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Personalized Federated knowledge graph Embedding with client-wise relation Graph (PFedEG)** \\cite{zhang2024}.\n        *   **Personalized Supplementary Knowledge**: The server generates personalized supplementary knowledge for each client by aggregating entity embeddings from *neighboring* clients. This aggregation is based on their \"affinity\" within a client-wise relation graph \\cite{zhang2024}.\n        *   **Client-Wise Relation Graph**: The \"affinity\" between two clients, representing their semantic relevance, is quantified by a relation weight on this graph. The paper introduces two strategies for learning these relation weights: based on shared entities and shared relations \\cite{zhang2024}.\n        *   **Personalized Embedding Learning**: Each client then conducts personalized embedding learning using its local triples and its specific personalized supplementary knowledge.\n        *   **Client Update Objective**: The local training objective for each client incorporates a KGE loss function with self-adversarial negative sampling and a regularization term `D(Et_c, Kt_c)` \\cite{zhang2024}. This term constrains the updated local entity embeddings from drifting too far from the personalized supplementary knowledge, which also initializes the local entity embeddings at the start of each round \\cite{zhang2024}.\n    *   **Novelty/Differentiation**:\n        *   PFedEG is the first approach to aggregate entity embeddings as *personalized* supplementary knowledge for each client based on a client-wise relation graph, harnessing more relevant information from semantically proximate KGs \\cite{zhang2024}.\n        *   It is the first to propose conducting personalized embedding learning for individual KGs by leveraging personalized supplementary knowledge from other KGs, directly addressing the semantic disparity challenge \\cite{zhang2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The PFedEG framework for personalized federated knowledge graph embedding \\cite{zhang2024}.\n        *   A novel mechanism for generating personalized supplementary knowledge for each client by aggregating embeddings based on a learned client-wise relation graph and inter-client \"affinity\" \\cite{zhang2024}.\n        *   Introduction of two strategies for dynamically learning the \"affinity\" (relation weights) between clients based on shared entities and shared relations \\cite{zhang2024}.\n        *   A personalized client-side optimization objective that integrates personalized supplementary knowledge through both initialization and a regularization term \\cite{zhang2024}.\n    *   **Theoretical Insights/Analysis**: The paper highlights the critical impact of semantic disparity in FKG and provides a principled approach to mitigate it by tailoring external knowledge to each client's specific semantic context \\cite{zhang2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted to evaluate PFedEG against state-of-the-art models \\cite{zhang2024}. These experiments were performed on four benchmark datasets \\cite{zhang2024}.\n    *   **Key Performance Metrics and Comparison Results**: The evaluation used four metrics that assess the accuracy of FKGE, including Mean Reciprocal Rank (MRR) as indicated in the algorithm \\cite{zhang2024}. The results consistently demonstrate the \"superiority\" and \"significant improvement in performance\" of PFedEG over existing state-of-the-art methods across all evaluated metrics \\cite{zhang2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The method assumes that information about aligned entities (shared entities) is provided via a private set intersection and kept privately on the server \\cite{zhang2024}.\n        *   The paper states that privacy is not its primary research focus, but existing privacy-preserving methods (e.g., Differential Privacy) can be incorporated \\cite{zhang2024}.\n        *   PFedEG adopts a client-server architecture for communication efficiency, implying that peer-to-peer architectures are outside its current scope \\cite{zhang2024}.\n    *   **Scope of Applicability**: PFedEG is applicable to federated learning scenarios where KGs are distributed across multiple clients, exhibit semantic disparities, and require personalized embedding learning while maintaining data privacy \\cite{zhang2024}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: PFedEG significantly advances the technical state-of-the-art in FKGE by effectively addressing the long-standing challenge of semantic disparity among clients, which previous methods largely overlooked \\cite{zhang2024}. By providing personalized supplementary knowledge, it leads to higher quality and more relevant embeddings.\n    *   **Potential Impact on Future Research**: This work establishes a new paradigm for personalized knowledge aggregation in federated learning. It could inspire future research in personalized federated learning across various domains beyond KGE, particularly where data heterogeneity and semantic relevance are critical factors \\cite{zhang2024}. The concept of a client-wise relation graph and affinity-based aggregation offers a promising direction for more intelligent and context-aware federated learning systems.",
      "intriguing_abstract": "Unlocking the full potential of distributed Knowledge Graphs (KGs) in privacy-preserving federated settings is severely challenged by inherent semantic disparities among clients. Existing Federated Knowledge Graph Embedding (FKGE) methods often aggregate a \"one-size-fits-all\" global knowledge, leading to noisy, irrelevant information and suboptimal local embeddings. We introduce **PFedEG**, a novel Personalized Federated Knowledge Graph Embedding framework designed to overcome this critical limitation. PFedEG pioneers the generation of *personalized supplementary knowledge* for each client by leveraging a dynamically learned *client-wise relation graph*. This graph quantifies semantic \"affinity\" between clients based on shared entities and relations, enabling intelligent aggregation of relevant entity embeddings from semantically proximate neighbors. Each client then performs personalized embedding learning, guided by this tailored knowledge. Extensive experiments on benchmark datasets demonstrate PFedEG's superior performance over state-of-the-art FKGE approaches, significantly enhancing embedding quality. Our work establishes a new paradigm for personalized knowledge aggregation in federated learning, paving the way for more effective and context-aware distributed AI systems.",
      "keywords": [
        "Personalized Federated Knowledge Graph Embedding (PFedEG)",
        "Federated Knowledge Graph Embedding (FKGE)",
        "semantic disparity",
        "client-wise relation graph",
        "personalized supplementary knowledge",
        "affinity-based aggregation",
        "distributed Knowledge Graphs",
        "entity embeddings",
        "personalized embedding learning",
        "relation weights",
        "client-server architecture",
        "superior performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/5b5b3face4be1cf131d0cb9c40ae5adcd0c16408.pdf",
      "citation_key": "zhang2024",
      "metadata": {
        "title": "Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph",
        "authors": [
          "Xiaoxiong Zhang",
          "Zhiwei Zeng",
          "Xin Zhou",
          "D. Niyato",
          "Zhiqi Shen"
        ],
        "published_date": "2024",
        "abstract": "Federated Knowledge Graph Embedding (FKGE) has recently garnered considerable interest due to its capacity to extract expressive representations from distributed knowledge graphs, while concurrently safeguarding the privacy of individual clients. Existing FKGE methods typically harness the arithmetic mean of entity embeddings from all clients as the global supplementary knowledge, and learn a replica of global consensus entities embeddings for each client. However, these methods usually neglect the inherent semantic disparities among distinct clients. This oversight not only results in the globally shared complementary knowledge being inundated with too much noise when tailored to a specific client, but also instigates a discrepancy between local and global optimization objectives. Consequently, the quality of the learned embeddings is compromised. To address this, we propose Personalized Federated knowledge graph Embedding with client-wise relation Graph (PFedEG), a novel approach that employs a client-wise relation graph to learn personalized embeddings by discerning the semantic relevance of embeddings from other clients. Specifically, PFedEG learns personalized supplementary knowledge for each client by amalgamating entity embedding from its neighboring clients based on their\"affinity\"on the client-wise relation graph. Each client then conducts personalized embedding learning based on its local triples and personalized supplementary knowledge. We conduct extensive experiments on four benchmark datasets to evaluate our method against state-of-the-art models and results demonstrate the superiority of our method.",
        "file_path": "paper_data/knowledge_graph_embedding/5b5b3face4be1cf131d0cb9c40ae5adcd0c16408.pdf",
        "venue": "Applied intelligence (Boston)",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review: Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph\n\nThis paper, \"Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph\" by Zhang et al. \\cite{zhang2024}, introduces a novel approach to address the challenges of semantic disparity in Federated Knowledge Graph Embedding (FKGE).\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing FKGE methods typically use a simple arithmetic mean of entity embeddings from all clients as global supplementary knowledge and learn a replica of global consensus entity embeddings. This approach neglects the inherent semantic disparities among distinct clients, leading to:\n        *   Globally shared complementary knowledge being \"inundated with too much noise\" when tailored to a specific client \\cite{zhang2024}.\n        *   A discrepancy between local and global optimization objectives, compromising the quality of learned embeddings \\cite{zhang2024}.\n    *   **Importance and Challenge**: With the rise of data privacy regulations like GDPR, KGs are increasingly distributed across multiple clients (Federated Knowledge Graphs, FKG). FKGE aims to collaboratively learn embeddings from these distributed KGs while preserving privacy. The core challenge is that KGs from different clients often have varying relation sets and thus diverse semantics for shared entities (semantic disparity), making a \"one-size-fits-all\" global aggregation ineffective \\cite{zhang2024}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Current FKGE methods primarily fall into client-server (e.g., FedE, FedEC, FedR) or peer-to-peer (e.g., FKGE) architectures \\cite{zhang2024}.\n        *   FedE \\cite{zhang2024} is a pioneering model using simple averaging for aggregation. FedEC \\cite{zhang2024} enhances FedE with embedding-contrastive learning. FedR \\cite{zhang2024} focuses on scenarios with shared entities and relations.\n    *   **Limitations of Previous Solutions**:\n        *   FedE and FedEC \\cite{zhang2024} ignore the client-wise relation graph during aggregation, leading to suboptimal embedding quality. Their averaging strategy provides a universally shared global supplementary knowledge that may contain too much irrelevant information for specific clients due to semantic disparities.\n        *   These methods typically learn a global consensus entity embedding for all clients, which can lead to a divergence between local and global optimization objectives, especially for KGs with few shared relations \\cite{zhang2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Personalized Federated knowledge graph Embedding with client-wise relation Graph (PFedEG)** \\cite{zhang2024}.\n        *   **Personalized Supplementary Knowledge**: The server generates personalized supplementary knowledge for each client by aggregating entity embeddings from *neighboring* clients. This aggregation is based on their \"affinity\" within a client-wise relation graph \\cite{zhang2024}.\n        *   **Client-Wise Relation Graph**: The \"affinity\" between two clients, representing their semantic relevance, is quantified by a relation weight on this graph. The paper introduces two strategies for learning these relation weights: based on shared entities and shared relations \\cite{zhang2024}.\n        *   **Personalized Embedding Learning**: Each client then conducts personalized embedding learning using its local triples and its specific personalized supplementary knowledge.\n        *   **Client Update Objective**: The local training objective for each client incorporates a KGE loss function with self-adversarial negative sampling and a regularization term `D(Et_c, Kt_c)` \\cite{zhang2024}. This term constrains the updated local entity embeddings from drifting too far from the personalized supplementary knowledge, which also initializes the local entity embeddings at the start of each round \\cite{zhang2024}.\n    *   **Novelty/Differentiation**:\n        *   PFedEG is the first approach to aggregate entity embeddings as *personalized* supplementary knowledge for each client based on a client-wise relation graph, harnessing more relevant information from semantically proximate KGs \\cite{zhang2024}.\n        *   It is the first to propose conducting personalized embedding learning for individual KGs by leveraging personalized supplementary knowledge from other KGs, directly addressing the semantic disparity challenge \\cite{zhang2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The PFedEG framework for personalized federated knowledge graph embedding \\cite{zhang2024}.\n        *   A novel mechanism for generating personalized supplementary knowledge for each client by aggregating embeddings based on a learned client-wise relation graph and inter-client \"affinity\" \\cite{zhang2024}.\n        *   Introduction of two strategies for dynamically learning the \"affinity\" (relation weights) between clients based on shared entities and shared relations \\cite{zhang2024}.\n        *   A personalized client-side optimization objective that integrates personalized supplementary knowledge through both initialization and a regularization term \\cite{zhang2024}.\n    *   **Theoretical Insights/Analysis**: The paper highlights the critical impact of semantic disparity in FKG and provides a principled approach to mitigate it by tailoring external knowledge to each client's specific semantic context \\cite{zhang2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were conducted to evaluate PFedEG against state-of-the-art models \\cite{zhang2024}. These experiments were performed on four benchmark datasets \\cite{zhang2024}.\n    *   **Key Performance Metrics and Comparison Results**: The evaluation used four metrics that assess the accuracy of FKGE, including Mean Reciprocal Rank (MRR) as indicated in the algorithm \\cite{zhang2024}. The results consistently demonstrate the \"superiority\" and \"significant improvement in performance\" of PFedEG over existing state-of-the-art methods across all evaluated metrics \\cite{zhang2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The method assumes that information about aligned entities (shared entities) is provided via a private set intersection and kept privately on the server \\cite{zhang2024}.\n        *   The paper states that privacy is not its primary research focus, but existing privacy-preserving methods (e.g., Differential Privacy) can be incorporated \\cite{zhang2024}.\n        *   PFedEG adopts a client-server architecture for communication efficiency, implying that peer-to-peer architectures are outside its current scope \\cite{zhang2024}.\n    *   **Scope of Applicability**: PFedEG is applicable to federated learning scenarios where KGs are distributed across multiple clients, exhibit semantic disparities, and require personalized embedding learning while maintaining data privacy \\cite{zhang2024}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: PFedEG significantly advances the technical state-of-the-art in FKGE by effectively addressing the long-standing challenge of semantic disparity among clients, which previous methods largely overlooked \\cite{zhang2024}. By providing personalized supplementary knowledge, it leads to higher quality and more relevant embeddings.\n    *   **Potential Impact on Future Research**: This work establishes a new paradigm for personalized knowledge aggregation in federated learning. It could inspire future research in personalized federated learning across various domains beyond KGE, particularly where data heterogeneity and semantic relevance are critical factors \\cite{zhang2024}. The concept of a client-wise relation graph and affinity-based aggregation offers a promising direction for more intelligent and context-aware federated learning systems.",
        "keywords": [
          "Personalized Federated Knowledge Graph Embedding (PFedEG)",
          "Federated Knowledge Graph Embedding (FKGE)",
          "semantic disparity",
          "client-wise relation graph",
          "personalized supplementary knowledge",
          "affinity-based aggregation",
          "distributed Knowledge Graphs",
          "entity embeddings",
          "personalized embedding learning",
          "relation weights",
          "client-server architecture",
          "superior performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408.pdf"
    },
    {
      "success": true,
      "doc_id": "05291bcd6887ab7d2975b9cc9854fde5",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **CITATION**: \\cite{rosso2020}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph (KG) embedding techniques oversimplify hyper-relational facts. They typically represent KGs as simple triplets (h, r, t), ignoring the associated key-value pairs (k, v) that are integral to hyper-relational facts.\n    *   **Importance and Challenge**: Hyper-relational facts contain richer, more complex information than simple triplets. While some recent methods attempt to learn from hyper-relational data by transforming it into an n-ary representation (only key-value pairs), these approaches are suboptimal because they discard the fundamental triplet structure, which is crucial for preserving essential information and effective link prediction in modern KGs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   Traditional KG embedding methods: Learn from triplet representations (h, r, t), preserving the basic structural information.\n        *   Recent hyper-relational methods: Transform hyper-relational facts into n-ary representations (sets of key-value pairs) for learning.\n    *   **Limitations of Previous Solutions**:\n        *   Triplet-only methods: Fail to capture the additional, complex information present in key-value pairs of hyper-relational facts.\n        *   N-ary representation methods: Lead to suboptimal models because they are \"unaware of the triplet structure,\" which is a fundamental and essential component for link prediction in KGs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes HINGE, a hyper-relational KG embedding model that directly learns from hyper-relational facts in a KG \\cite{rosso2020}.\n    *   **Novelty/Difference**: HINGE's innovation lies in its ability to simultaneously capture two critical aspects:\n        1.  The primary structural information of the KG encoded in the base triplets (h, r, t).\n        2.  The correlation between each triplet and its associated key-value pairs (k, v) \\cite{rosso2020}.\n        This direct, integrated learning from the full hyper-relational fact, without oversimplification or loss of structure, distinguishes it from prior work.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of HINGE, a novel hyper-relational KG embedding model specifically designed to handle the complexity of hyper-relational facts.\n    *   **Algorithmic Innovation**: HINGE's core contribution is its mechanism to directly learn from hyper-relational facts by capturing both the fundamental triplet structure and the associated key-value pair correlations, thereby overcoming the limitations of existing methods \\cite{rosso2020}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive evaluation was performed on various link prediction tasks across different KGs \\cite{rosso2020}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   HINGE consistently demonstrated superiority over KG embedding methods that learn from triplets only, showing performance improvements ranging from 0.81% to 41.45% (depending on the specific link prediction tasks and settings) \\cite{rosso2020}.\n        *   HINGE also consistently outperformed methods that learn from hyper-relational facts using n-ary representations, with improvements ranging from 13.2% to 84.1% \\cite{rosso2020}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract does not explicitly state technical limitations of HINGE itself. Its primary assumption is the presence of hyper-relational facts (triplets with associated key-value pairs) in the KG.\n    *   **Scope of Applicability**: HINGE is specifically applicable to Knowledge Graphs that contain hyper-relational facts, where each fact is composed of a base triplet and additional key-value attributes. Its main utility is in improving link prediction accuracy in such complex KGs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HINGE significantly advances the technical state-of-the-art in KG embeddings by providing a robust and effective solution for directly learning from hyper-relational data, which was previously either oversimplified or inadequately handled \\cite{rosso2020}.\n    *   **Potential Impact**: This work has the potential to lead to more accurate and comprehensive knowledge discovery and reasoning in real-world applications that rely on KGs with complex, hyper-relational structures, by enabling models to leverage all available information within a fact.",
      "intriguing_abstract": "The intricate tapestry of Knowledge Graphs (KGs) often hides a critical challenge: effectively embedding hyper-relational facts. Traditional methods either oversimplify by ignoring vital key-value pairs or discard the fundamental triplet structure, severely limiting the accuracy of link prediction. We present HINGE, a novel hyper-relational KG embedding model that directly confronts this dilemma. HINGE innovatively learns from the complete hyper-relational fact, simultaneously capturing the primary structural information of the base triplet (h, r, t) and its rich associated key-value (k, v) correlations. This integrated approach ensures no crucial data is lost, providing a holistic representation of complex knowledge. Our extensive evaluations demonstrate HINGE's remarkable superiority, achieving performance improvements of up to 41.45% over triplet-only methods and an astounding 84.1% over n-ary approaches in diverse link prediction tasks. HINGE marks a significant leap forward in KG embedding, enabling unprecedented accuracy and comprehensive knowledge discovery in real-world, hyper-relational datasets.",
      "keywords": [
        "Knowledge Graph (KG) embedding",
        "hyper-relational facts",
        "triplet structure (h",
        "r",
        "t)",
        "key-value pairs (k",
        "v)",
        "n-ary representation",
        "link prediction",
        "HINGE model",
        "direct learning from hyper-relational facts",
        "simultaneous capture of triplet and key-value correlations",
        "improved link prediction accuracy",
        "state-of-the-art advancement",
        "complex KGs"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/f4e39a4f8fd8f8453372b74fda17047b9860d870.pdf",
      "citation_key": "rosso2020",
      "metadata": {
        "title": "Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction",
        "authors": [
          "Paolo Rosso",
          "Dingqi Yang",
          "P. Cudr-Mauroux"
        ],
        "published_date": "2020",
        "abstract": "Knowledge Graph (KG) embeddings are a powerful tool for predicting missing links in KGs. Existing techniques typically represent a KG as a set of triplets, where each triplet (h, r, t) links two entities h and t through a relation r, and learn entity/relation embeddings from such triplets while preserving such a structure. However, this triplet representation oversimplifies the complex nature of the data stored in the KG, in particular for hyper-relational facts, where each fact contains not only a base triplet (h, r, t), but also the associated key-value pairs (k, v). Even though a few recent techniques tried to learn from such data by transforming a hyper-relational fact into an n-ary representation (i.e., a set of key-value pairs only without triplets), they result in suboptimal models as they are unaware of the triplet structure, which serves as the fundamental data structure in modern KGs and preserves the essential information for link prediction. To address this issue, we propose HINGE, a hyper-relational KG embedding model, which directly learns from hyper-relational facts in a KG. HINGE captures not only the primary structural information of the KG encoded in the triplets, but also the correlation between each triplet and its associated key-value pairs. Our extensive evaluation shows the superiority of HINGE on various link prediction tasks over KGs. In particular, HINGE consistently outperforms not only the KG embedding methods learning from triplets only (by 0.81-41.45% depending on the link prediction tasks and settings), but also the methods learning from hyper-relational facts using the n-ary representation (by 13.2-84.1%).",
        "file_path": "paper_data/knowledge_graph_embedding/f4e39a4f8fd8f8453372b74fda17047b9860d870.pdf",
        "venue": "The Web Conference",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **CITATION**: \\cite{rosso2020}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph (KG) embedding techniques oversimplify hyper-relational facts. They typically represent KGs as simple triplets (h, r, t), ignoring the associated key-value pairs (k, v) that are integral to hyper-relational facts.\n    *   **Importance and Challenge**: Hyper-relational facts contain richer, more complex information than simple triplets. While some recent methods attempt to learn from hyper-relational data by transforming it into an n-ary representation (only key-value pairs), these approaches are suboptimal because they discard the fundamental triplet structure, which is crucial for preserving essential information and effective link prediction in modern KGs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   Traditional KG embedding methods: Learn from triplet representations (h, r, t), preserving the basic structural information.\n        *   Recent hyper-relational methods: Transform hyper-relational facts into n-ary representations (sets of key-value pairs) for learning.\n    *   **Limitations of Previous Solutions**:\n        *   Triplet-only methods: Fail to capture the additional, complex information present in key-value pairs of hyper-relational facts.\n        *   N-ary representation methods: Lead to suboptimal models because they are \"unaware of the triplet structure,\" which is a fundamental and essential component for link prediction in KGs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes HINGE, a hyper-relational KG embedding model that directly learns from hyper-relational facts in a KG \\cite{rosso2020}.\n    *   **Novelty/Difference**: HINGE's innovation lies in its ability to simultaneously capture two critical aspects:\n        1.  The primary structural information of the KG encoded in the base triplets (h, r, t).\n        2.  The correlation between each triplet and its associated key-value pairs (k, v) \\cite{rosso2020}.\n        This direct, integrated learning from the full hyper-relational fact, without oversimplification or loss of structure, distinguishes it from prior work.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of HINGE, a novel hyper-relational KG embedding model specifically designed to handle the complexity of hyper-relational facts.\n    *   **Algorithmic Innovation**: HINGE's core contribution is its mechanism to directly learn from hyper-relational facts by capturing both the fundamental triplet structure and the associated key-value pair correlations, thereby overcoming the limitations of existing methods \\cite{rosso2020}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive evaluation was performed on various link prediction tasks across different KGs \\cite{rosso2020}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   HINGE consistently demonstrated superiority over KG embedding methods that learn from triplets only, showing performance improvements ranging from 0.81% to 41.45% (depending on the specific link prediction tasks and settings) \\cite{rosso2020}.\n        *   HINGE also consistently outperformed methods that learn from hyper-relational facts using n-ary representations, with improvements ranging from 13.2% to 84.1% \\cite{rosso2020}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided abstract does not explicitly state technical limitations of HINGE itself. Its primary assumption is the presence of hyper-relational facts (triplets with associated key-value pairs) in the KG.\n    *   **Scope of Applicability**: HINGE is specifically applicable to Knowledge Graphs that contain hyper-relational facts, where each fact is composed of a base triplet and additional key-value attributes. Its main utility is in improving link prediction accuracy in such complex KGs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HINGE significantly advances the technical state-of-the-art in KG embeddings by providing a robust and effective solution for directly learning from hyper-relational data, which was previously either oversimplified or inadequately handled \\cite{rosso2020}.\n    *   **Potential Impact**: This work has the potential to lead to more accurate and comprehensive knowledge discovery and reasoning in real-world applications that rely on KGs with complex, hyper-relational structures, by enabling models to leverage all available information within a fact.",
        "keywords": [
          "Knowledge Graph (KG) embedding",
          "hyper-relational facts",
          "triplet structure (h",
          "r",
          "t)",
          "key-value pairs (k",
          "v)",
          "n-ary representation",
          "link prediction",
          "HINGE model",
          "direct learning from hyper-relational facts",
          "simultaneous capture of triplet and key-value correlations",
          "improved link prediction accuracy",
          "state-of-the-art advancement",
          "complex KGs"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "f4e39a4f8fd8f8453372b74fda17047b9860d870.pdf"
    },
    {
      "success": true,
      "doc_id": "b221ef13225dc063278ef7fc396c58f9",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **1. Research Problem & Motivation** \\cite{zhou2024}\n    *   **Specific Technical Problem**: Addressing poisoning attacks in Federated Knowledge Graph Embedding (FKGE).\n    *   **Importance & Challenge**: This problem is critical because poisoning attacks can lead to biased decisions in downstream applications, yet it remains largely unexplored in FKGE. It is challenging due to FKGE's distributed nature, where client KGs are maintained locally, making direct injection of poisoned data difficult. Attackers must create poisoned data without direct access to the victim's KG and inject it indirectly through FKGE aggregation.\n\n*   **2. Related Work & Positioning** \\cite{zhou2024}\n    *   **Relation to Existing Approaches**: This work is positioned as the first to systematize the risks of FKGE poisoning attacks.\n    *   **Limitations of Previous Solutions**: Unlike centralized KGEs where direct data injection might be feasible, FKGE's local KG maintenance presents a unique challenge that previous attack methodologies have not addressed.\n\n*   **3. Technical Approach & Innovation** \\cite{zhou2024}\n    *   **Core Technical Method**: The paper develops a novel framework for poisoning attacks designed to force a victim client to predict specific false facts.\n    *   **Novelty**:\n        *   **KG Component Inference Attack**: A new method for attackers to infer targeted relations within the victim's local KG without direct access.\n        *   **Shadow Model Training**: Attackers locally train a \"shadow model\" using the created poisoned data.\n        *   **Optimized Dynamic Poisoning Scheme**: This scheme dynamically adjusts the shadow model and generates progressive poisoned updates, which are then injected through FKGE aggregation to accurately mislead the victim's embeddings.\n\n*   **4. Key Technical Contributions** \\cite{zhou2024}\n    *   **Novel Attack Framework**: The first comprehensive framework for poisoning attacks in FKGE.\n    *   **KG Component Inference Attack**: A novel technique for inferring victim KG components in a distributed setting.\n    *   **Optimized Dynamic Poisoning Scheme**: An innovative method for generating progressive poisoned updates to effectively manipulate FKGE models via aggregation.\n    *   **Systematization of Risks**: The first work to systematically analyze and categorize the risks associated with FKGE poisoning attacks.\n\n*   **5. Experimental Validation** \\cite{zhou2024}\n    *   **Experiments Conducted**: The authors conducted experiments to demonstrate the effectiveness of their proposed attack framework.\n    *   **Key Performance Metrics & Results**: The attack achieved a remarkable success rate on various KGE models (e.g., 100% on TransE with WN18RR). Crucially, it managed to achieve this high success rate while keeping the original task's performance nearly unchanged, indicating stealth and efficacy.\n\n*   **6. Limitations & Scope** \\cite{zhou2024}\n    *   **Technical Limitations/Assumptions**: The paper focuses on a specific type of attack: forcing the victim client to predict *specific false facts*. While highly effective for this goal, the scope does not explicitly cover other potential attack objectives (e.g., general model degradation without specific false fact prediction).\n    *   **Scope of Applicability**: The framework is specifically designed for poisoning attacks within Federated Knowledge Graph Embedding (FKGE) systems.\n\n*   **7. Technical Significance** \\cite{zhou2024}\n    *   **Advance State-of-the-Art**: This work significantly advances the technical state-of-the-art by being the first to systematically explore and demonstrate the feasibility and effectiveness of poisoning attacks in FKGE.\n    *   **Potential Impact**: It highlights critical security vulnerabilities in emerging FKGE systems, paving the way for future research into robust defense mechanisms and secure FKGE design.",
      "intriguing_abstract": "The promise of Federated Knowledge Graph Embedding (FKGE) for collaborative intelligence is shadowed by an unexplored, yet critical, threat: poisoning attacks. This paper unveils the first comprehensive framework to systematically analyze and execute such attacks, demonstrating a profound vulnerability in these emerging distributed systems. Our novel approach bypasses the inherent challenges of FKGE's local data maintenance by introducing a sophisticated **KG Component Inference Attack** that deduces victim knowledge without direct access. Coupled with an **optimized dynamic poisoning scheme** and **shadow model training**, attackers can progressively inject malicious updates through federated aggregation, forcing victim clients to predict specific false facts with alarming precision. Experiments show remarkable success rates, achieving 100% on models like TransE (WN18RR) while maintaining original task performance  a testament to its stealth and efficacy. This work not only systematizes the risks of FKGE poisoning but also provides an urgent foundation for developing robust defense mechanisms, fundamentally reshaping the discourse on secure and trustworthy FKGE design.",
      "keywords": [
        "Federated Knowledge Graph Embedding (FKGE)",
        "poisoning attacks",
        "KG Component Inference Attack",
        "Optimized Dynamic Poisoning Scheme",
        "shadow model training",
        "novel attack framework",
        "systematization of FKGE risks",
        "distributed KGE systems",
        "security vulnerabilities",
        "false fact prediction",
        "high attack success rate",
        "stealthy attacks",
        "KGE model manipulation"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/6a86594566fc9fa2e92afb6f0229d63a45fe25e6.pdf",
      "citation_key": "zhou2024",
      "metadata": {
        "title": "Poisoning Attack on Federated Knowledge Graph Embedding",
        "authors": [
          "Enyuan Zhou",
          "Song Guo",
          "Zhixiu Ma",
          "Zicong Hong",
          "Tao Guo",
          "Peiran Dong"
        ],
        "published_date": "2024",
        "abstract": "Federated Knowledge Graph Embedding (FKGE) is an emerging collaborative learning technique for deriving expressive representations (i.e., embeddings) from client-maintained distributed knowledge graphs (KGs). However, poisoning attacks in FKGE, which lead to biased decisions by downstream applications, remain unexplored. This paper is the first work to systematize the risks of FKGE poisoning attacks, from which we develop a novel framework for poisoning attacks that force the victim client to predict specific false facts. Unlike centralized KGEs, FKGE maintains KGs locally, making direct injection of poisoned data challenging. Instead, attackers must create poisoned data without access to the victim's KG and inject it indirectly through FKGE aggregation. Specifically, to create poisoned data, the attacker first infers the targeted relations in the victim's local KG via a new KG component inference attack. Then, to accurately mislead the victim's embeddings via aggregation, the attacker locally trains a shadow model using the poisoned data and uses an optimized dynamic poisoning scheme to adjust the model and generate progressive poisoned updates. Our experimental results demonstrate the attack's effectiveness, achieving a remarkable success rate on various KGE models (e.g., 100% on TransE with WN18RR) while keeping the original task's performance nearly unchanged.",
        "file_path": "paper_data/knowledge_graph_embedding/6a86594566fc9fa2e92afb6f0229d63a45fe25e6.pdf",
        "venue": "The Web Conference",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **1. Research Problem & Motivation** \\cite{zhou2024}\n    *   **Specific Technical Problem**: Addressing poisoning attacks in Federated Knowledge Graph Embedding (FKGE).\n    *   **Importance & Challenge**: This problem is critical because poisoning attacks can lead to biased decisions in downstream applications, yet it remains largely unexplored in FKGE. It is challenging due to FKGE's distributed nature, where client KGs are maintained locally, making direct injection of poisoned data difficult. Attackers must create poisoned data without direct access to the victim's KG and inject it indirectly through FKGE aggregation.\n\n*   **2. Related Work & Positioning** \\cite{zhou2024}\n    *   **Relation to Existing Approaches**: This work is positioned as the first to systematize the risks of FKGE poisoning attacks.\n    *   **Limitations of Previous Solutions**: Unlike centralized KGEs where direct data injection might be feasible, FKGE's local KG maintenance presents a unique challenge that previous attack methodologies have not addressed.\n\n*   **3. Technical Approach & Innovation** \\cite{zhou2024}\n    *   **Core Technical Method**: The paper develops a novel framework for poisoning attacks designed to force a victim client to predict specific false facts.\n    *   **Novelty**:\n        *   **KG Component Inference Attack**: A new method for attackers to infer targeted relations within the victim's local KG without direct access.\n        *   **Shadow Model Training**: Attackers locally train a \"shadow model\" using the created poisoned data.\n        *   **Optimized Dynamic Poisoning Scheme**: This scheme dynamically adjusts the shadow model and generates progressive poisoned updates, which are then injected through FKGE aggregation to accurately mislead the victim's embeddings.\n\n*   **4. Key Technical Contributions** \\cite{zhou2024}\n    *   **Novel Attack Framework**: The first comprehensive framework for poisoning attacks in FKGE.\n    *   **KG Component Inference Attack**: A novel technique for inferring victim KG components in a distributed setting.\n    *   **Optimized Dynamic Poisoning Scheme**: An innovative method for generating progressive poisoned updates to effectively manipulate FKGE models via aggregation.\n    *   **Systematization of Risks**: The first work to systematically analyze and categorize the risks associated with FKGE poisoning attacks.\n\n*   **5. Experimental Validation** \\cite{zhou2024}\n    *   **Experiments Conducted**: The authors conducted experiments to demonstrate the effectiveness of their proposed attack framework.\n    *   **Key Performance Metrics & Results**: The attack achieved a remarkable success rate on various KGE models (e.g., 100% on TransE with WN18RR). Crucially, it managed to achieve this high success rate while keeping the original task's performance nearly unchanged, indicating stealth and efficacy.\n\n*   **6. Limitations & Scope** \\cite{zhou2024}\n    *   **Technical Limitations/Assumptions**: The paper focuses on a specific type of attack: forcing the victim client to predict *specific false facts*. While highly effective for this goal, the scope does not explicitly cover other potential attack objectives (e.g., general model degradation without specific false fact prediction).\n    *   **Scope of Applicability**: The framework is specifically designed for poisoning attacks within Federated Knowledge Graph Embedding (FKGE) systems.\n\n*   **7. Technical Significance** \\cite{zhou2024}\n    *   **Advance State-of-the-Art**: This work significantly advances the technical state-of-the-art by being the first to systematically explore and demonstrate the feasibility and effectiveness of poisoning attacks in FKGE.\n    *   **Potential Impact**: It highlights critical security vulnerabilities in emerging FKGE systems, paving the way for future research into robust defense mechanisms and secure FKGE design.",
        "keywords": [
          "Federated Knowledge Graph Embedding (FKGE)",
          "poisoning attacks",
          "KG Component Inference Attack",
          "Optimized Dynamic Poisoning Scheme",
          "shadow model training",
          "novel attack framework",
          "systematization of FKGE risks",
          "distributed KGE systems",
          "security vulnerabilities",
          "false fact prediction",
          "high attack success rate",
          "stealthy attacks",
          "KGE model manipulation"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "6a86594566fc9fa2e92afb6f0229d63a45fe25e6.pdf"
    },
    {
      "success": true,
      "doc_id": "796b3b5aa393520b7dddc233643848ef",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem**: The paper addresses the challenge of learning expressive, low-dimensional vector representations (embeddings) for entities and relations in knowledge graphs (KGE) from observed triples.\n*   **Importance & Challenge**: Conventional shallow KGE models suffer from limited expressiveness. While CNN-based models like ConvE improve interaction learning, they lack the ability to incorporate structural information from the knowledge graph, and their performance is still constrained by the number of interactions.\n\n**2. Related Work & Positioning**\n*   **Existing Approaches**:\n    *   **Conventional shallow models**: Identified as having limited expressiveness.\n    *   **ConvE (Dettmers et al., 2018)**: Utilizes Convolutional Neural Networks (CNNs) to increase interactions between head and relation embeddings, offering parameter-efficient expressiveness.\n    *   **KBGAT (Nathani et al., 2019)**: Learns embeddings by adaptively leveraging structural information within the knowledge graph.\n*   **Limitations of Previous Solutions**:\n    *   ConvE lacks structural information in its embedding space, and its performance is limited by the number of interactions it can capture.\n    *   Shallow models are inherently limited in their expressive power.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method**: The paper proposes ReInceptionE \\cite{xie2020}, a novel model that integrates the benefits of ConvE (enhanced interaction) and KBGAT (structural information).\n    *   **Inception Network for Query Embedding**: An Inception network is employed to learn query embeddings, specifically designed to further increase the interactions between head and relation embeddings.\n    *   **Relation-aware Attention Mechanism**: A novel attention mechanism is introduced to enrich the query embedding by incorporating both local neighborhood and global entity information from the knowledge graph structure.\n*   **Novelty**: ReInceptionE \\cite{xie2020} is novel in its synergistic combination of an Inception network for deep interaction learning and a relation-aware attention mechanism for integrating comprehensive structural context (local and global) into KGE.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods**:\n    *   The application of an Inception network architecture to enhance the learning of interactions between head and relation embeddings for KGE.\n    *   The development of a relation-aware attention mechanism that effectively enriches query embeddings with both local neighborhood and global entity structural information.\n*   **System Design/Architectural Innovations**: A unified architecture, ReInceptionE \\cite{xie2020}, that combines these two distinct mechanisms to overcome the limitations of prior KGE models by simultaneously improving interaction learning and structural awareness.\n\n**5. Experimental Validation**\n*   **Experiments Conducted**: The proposed ReInceptionE \\cite{xie2020} model was evaluated against state-of-the-art methods.\n*   **Key Performance Metrics & Comparison Results**: Experiments were conducted on two widely used benchmark datasets: WN18RR and FB15k-237. The results demonstrate that ReInceptionE \\cite{xie2020} achieves competitive performance compared to existing state-of-the-art approaches.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions**: The provided abstract does not explicitly state specific technical limitations or assumptions of ReInceptionE \\cite{xie2020}.\n*   **Scope of Applicability**: The model is primarily applicable to knowledge graph embedding tasks, particularly for scenarios requiring highly expressive embeddings that capture both intricate interactions and rich structural context.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art**: ReInceptionE \\cite{xie2020} advances the technical state-of-the-art in KGE by providing a more comprehensive and powerful framework that addresses the shortcomings of previous models, specifically by integrating enhanced interaction learning with explicit structural information.\n*   **Potential Impact on Future Research**: This work highlights the benefits of combining advanced neural network architectures (like Inception) with attention mechanisms for structural awareness in KGE. It could inspire future research into hybrid models that leverage diverse architectural strengths to learn richer and more robust knowledge graph representations, potentially improving performance in downstream tasks such as link prediction, knowledge graph completion, and question answering.",
      "intriguing_abstract": "Unlocking the full potential of knowledge graphs hinges on learning highly expressive, low-dimensional entity and relation embeddings. While convolutional neural networks (CNNs) have enhanced interaction learning in models like ConvE, they often overlook crucial structural information inherent in the graph, limiting their representational power. We introduce ReInceptionE, a novel knowledge graph embedding (KGE) model that synergistically integrates deep interaction learning with comprehensive structural awareness.\n\nReInceptionE leverages an Inception network architecture to significantly increase the interactions between head and relation embeddings, forming richer query representations. Crucially, we propose a novel relation-aware attention mechanism that enriches these query embeddings by adaptively incorporating both local neighborhood and global entity structural context from the knowledge graph. This unified architecture overcomes the expressiveness limitations of prior models, simultaneously deepening interaction capture and integrating explicit structural knowledge. Extensive experiments on WN18RR and FB15k-237 demonstrate ReInceptionE's competitive, state-of-the-art performance. This work advances representation learning for knowledge graphs, paving the way for more robust link prediction and knowledge graph completion by effectively fusing intricate semantic interactions with rich structural context.",
      "keywords": [
        "Knowledge Graph Embeddings (KGE)",
        "ReInceptionE",
        "Inception Network",
        "Relation-aware Attention Mechanism",
        "Structural Information Integration",
        "Enhanced Interaction Learning",
        "Query Embeddings",
        "Deep Learning Architecture",
        "Hybrid Models",
        "Link Prediction",
        "Knowledge Graph Completion",
        "State-of-the-Art Advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/1620a20881b572b5ffc6f9cb3cf39f6090cee19f.pdf",
      "citation_key": "xie2020",
      "metadata": {
        "title": "ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding",
        "authors": [
          "Zhiwen Xie",
          "Guangyou Zhou",
          "Jin Liu",
          "Xiangji Huang"
        ],
        "published_date": "2020",
        "abstract": "The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples. The conventional shallow models are limited to their expressiveness. ConvE (Dettmers et al., 2018) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings. However, there is no structural information in the embedding space of ConvE, and the performance is still limited by the number of interactions. The recent KBGAT (Nathani et al., 2019) provides another way to learn embeddings by adaptively utilizing structural information. In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE). Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings. Then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information. Experimental results on both WN18RR and FB15k-237 datasets demonstrate that ReInceptionE achieves competitive performance compared with state-of-the-art methods.",
        "file_path": "paper_data/knowledge_graph_embedding/1620a20881b572b5ffc6f9cb3cf39f6090cee19f.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem**: The paper addresses the challenge of learning expressive, low-dimensional vector representations (embeddings) for entities and relations in knowledge graphs (KGE) from observed triples.\n*   **Importance & Challenge**: Conventional shallow KGE models suffer from limited expressiveness. While CNN-based models like ConvE improve interaction learning, they lack the ability to incorporate structural information from the knowledge graph, and their performance is still constrained by the number of interactions.\n\n**2. Related Work & Positioning**\n*   **Existing Approaches**:\n    *   **Conventional shallow models**: Identified as having limited expressiveness.\n    *   **ConvE (Dettmers et al., 2018)**: Utilizes Convolutional Neural Networks (CNNs) to increase interactions between head and relation embeddings, offering parameter-efficient expressiveness.\n    *   **KBGAT (Nathani et al., 2019)**: Learns embeddings by adaptively leveraging structural information within the knowledge graph.\n*   **Limitations of Previous Solutions**:\n    *   ConvE lacks structural information in its embedding space, and its performance is limited by the number of interactions it can capture.\n    *   Shallow models are inherently limited in their expressive power.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method**: The paper proposes ReInceptionE \\cite{xie2020}, a novel model that integrates the benefits of ConvE (enhanced interaction) and KBGAT (structural information).\n    *   **Inception Network for Query Embedding**: An Inception network is employed to learn query embeddings, specifically designed to further increase the interactions between head and relation embeddings.\n    *   **Relation-aware Attention Mechanism**: A novel attention mechanism is introduced to enrich the query embedding by incorporating both local neighborhood and global entity information from the knowledge graph structure.\n*   **Novelty**: ReInceptionE \\cite{xie2020} is novel in its synergistic combination of an Inception network for deep interaction learning and a relation-aware attention mechanism for integrating comprehensive structural context (local and global) into KGE.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods**:\n    *   The application of an Inception network architecture to enhance the learning of interactions between head and relation embeddings for KGE.\n    *   The development of a relation-aware attention mechanism that effectively enriches query embeddings with both local neighborhood and global entity structural information.\n*   **System Design/Architectural Innovations**: A unified architecture, ReInceptionE \\cite{xie2020}, that combines these two distinct mechanisms to overcome the limitations of prior KGE models by simultaneously improving interaction learning and structural awareness.\n\n**5. Experimental Validation**\n*   **Experiments Conducted**: The proposed ReInceptionE \\cite{xie2020} model was evaluated against state-of-the-art methods.\n*   **Key Performance Metrics & Comparison Results**: Experiments were conducted on two widely used benchmark datasets: WN18RR and FB15k-237. The results demonstrate that ReInceptionE \\cite{xie2020} achieves competitive performance compared to existing state-of-the-art approaches.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions**: The provided abstract does not explicitly state specific technical limitations or assumptions of ReInceptionE \\cite{xie2020}.\n*   **Scope of Applicability**: The model is primarily applicable to knowledge graph embedding tasks, particularly for scenarios requiring highly expressive embeddings that capture both intricate interactions and rich structural context.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art**: ReInceptionE \\cite{xie2020} advances the technical state-of-the-art in KGE by providing a more comprehensive and powerful framework that addresses the shortcomings of previous models, specifically by integrating enhanced interaction learning with explicit structural information.\n*   **Potential Impact on Future Research**: This work highlights the benefits of combining advanced neural network architectures (like Inception) with attention mechanisms for structural awareness in KGE. It could inspire future research into hybrid models that leverage diverse architectural strengths to learn richer and more robust knowledge graph representations, potentially improving performance in downstream tasks such as link prediction, knowledge graph completion, and question answering.",
        "keywords": [
          "Knowledge Graph Embeddings (KGE)",
          "ReInceptionE",
          "Inception Network",
          "Relation-aware Attention Mechanism",
          "Structural Information Integration",
          "Enhanced Interaction Learning",
          "Query Embeddings",
          "Deep Learning Architecture",
          "Hybrid Models",
          "Link Prediction",
          "Knowledge Graph Completion",
          "State-of-the-Art Advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "1620a20881b572b5ffc6f9cb3cf39f6090cee19f.pdf"
    },
    {
      "success": true,
      "doc_id": "9e0b69b5e3f56ebcbf7900870acf288c",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem:** Existing Knowledge Graph Embedding (KGE) models struggle to effectively model and infer the *transitivity* relation pattern, despite its commonality and importance for link prediction \\cite{song2021}. While models like RotatE can handle symmetry, asymmetry, inversion, and composition, none fully support transitivity without forcing entity embeddings in a transitive chain to be identical, which limits expressiveness.\n    *   **Importance and challenge:** Transitivity (if (a,r,b) and (b,r,c), then (a,r,c)) is a fundamental logical pattern crucial for robust reasoning and inferring missing links in knowledge graphs. The challenge lies in designing a KGE model that can represent this property without collapsing distinct entity embeddings, while also maintaining the ability to model other complex relation patterns.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches:** The work builds upon and extends previous KGE models, particularly RotatE \\cite{song2021}. It categorizes existing models into \"Trans-series\" (e.g., TransE, TransH, TransR, BoxE) and \"Bilinear models\" (e.g., DistMult, ComplEx, RotatE, QuatE).\n    *   **Limitations of previous solutions:**\n        *   TransE and its variants, while good for composition and inversion, often require the translation vector for transitive relations to be zero, forcing entities in a transitive chain to have identical embeddings \\cite{song2021}.\n        *   RotatE, a state-of-the-art model, can infer symmetry, asymmetry, inversion, and composition, but similarly fails to model transitivity without forcing entity embeddings in a transitive chain to be the same (requiring rotation phases of `2n*pi`) \\cite{song2021}.\n        *   BoxE, a recent Trans-series model, can express composition and inversion but explicitly \"cannot express transitivity\" \\cite{song2021}.\n        *   The paper highlights that \"none of existing models is capable of modeling transitivity relation pattern\" alongside other patterns \\cite{song2021}.\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method:** The proposed Rot-Pro model combines *projection* and *relational rotation* in a complex vector space \\cite{song2021}. For a triple `(h,r,t)`, it requires `rot(pr(eh(k)); r(k)) = pr(et(k))`, where `pr` is a projection and `rot` is a rotation.\n    *   **Novelty/Difference:**\n        *   **Projection for Transitivity:** The key innovation is the theoretical demonstration that transitive relations can be modeled using *idempotent transformations (projections)* \\cite{song2021}. A relation `r` is represented by a projection `pr(k)` defined by an idempotent matrix `Mr(k) = Sr(k)^-1 * diag(ar(k), br(k)) * Sr(k)`, where `ar(k), br(k)  {0,1}`. `Sr(k)` is simplified to a rotation matrix. This allows entities in a transitive chain to have distinct embeddings but share the same *projected vector*, overcoming the limitations of previous models that forced identical embeddings \\cite{song2021}.\n        *   **Unified Framework:** Rot-Pro integrates this projection mechanism for transitivity with the relational rotation mechanism (inspired by RotatE) to model symmetry, asymmetry, inversion, and composition, creating a single model capable of inferring all five patterns \\cite{song2021}.\n        *   **Projection Penalty Loss:** A novel `Lp` loss function is introduced during optimization to enforce the `0` or `1` constraint on the `a` and `b` parameters of the projection matrix, using a weighted penalty to encourage binary values \\cite{song2021}.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insight:** Provides the first theoretical proof that transitive relations can be effectively modeled using idempotent transformations (projections) within a KGE framework \\cite{song2021}.\n    *   **Novel Algorithm:** Introduces Rot-Pro, a new KGE model that uniquely combines projection-based transitivity modeling with relational rotation, enabling comprehensive support for all five major relation patterns \\cite{song2021}.\n    *   **Comprehensive Pattern Support:** Theoretically proves that Rot-Pro can infer symmetry, asymmetry, inversion, composition, and transitivity patterns, a capability unmatched by prior KGE models \\cite{song2021}.\n    *   **Optimization Technique:** Develops a specialized projection penalty loss to ensure the learned projection matrices maintain their idempotent properties, crucial for the model's theoretical foundation \\cite{song2021}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted:** Link prediction experiments were performed on four benchmark datasets: FB15k-237, WN18RR, YAGO3-10, and Countries \\cite{song2021}. The latter two datasets (YAGO3-10 and Countries) were specifically chosen for their \"abundant relation patterns including transitivity\" \\cite{song2021}.\n    *   **Key performance metrics and comparison results:**\n        *   Evaluated using Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hit@k (Hit@1, Hit@3, Hit@10) in a filtered setting \\cite{song2021}.\n        *   Rot-Pro achieved state-of-the-art results on datasets containing transitive relations (YAGO3-10 and Countries) \\cite{song2021}.\n        *   On the Countries dataset, Rot-Pro significantly outperformed RotatE and other baselines, achieving near-perfect AUC-PR scores (1.00, 1.00, 0.998) across three composition tasks, demonstrating its effective inference of transitivity, symmetry, and composition \\cite{song2021}.\n        *   It also outperformed baseline models on most metrics for FB15k-237 and WN18RR, even though these datasets have fewer explicit transitive relations \\cite{song2021}.\n        *   The results empirically validate that Rot-Pro effectively learns the transitivity pattern \\cite{song2021}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations/assumptions:** The primary advantage of Rot-Pro is most pronounced in datasets rich in transitive relations. Its improvement over RotatE on general datasets like FB15k-237 and WN18RR is noted as \"limited\" due to the insufficient presence of transitive relations in those benchmarks \\cite{song2021}.\n    *   **Scope of applicability:** The model is specifically designed for KGE tasks, particularly link prediction, with a strong focus on accurately modeling complex relation patterns, especially transitivity.\n\n*   **Technical Significance**\n    *   **Advancement of state-of-the-art:** Rot-Pro significantly advances the technical state-of-the-art in KGE by providing a unified and theoretically grounded framework that can model *all five* fundamental relation patterns (symmetry, asymmetry, inversion, composition, and transitivity) \\cite{song2021}. This addresses a critical gap in existing KGE models.\n    *   **Potential impact on future research:** The introduction of projection-based modeling for transitivity opens new avenues for designing more expressive and logically consistent KGE models. It provides a strong foundation for future research into comprehensive relation pattern modeling and more sophisticated reasoning capabilities within knowledge graphs \\cite{song2021}.",
      "intriguing_abstract": "Despite their power, current Knowledge Graph Embedding (KGE) models falter at a fundamental logical pattern: transitivity. Existing approaches, including state-of-the-art models like RotatE, struggle to infer transitive relations without collapsing distinct entity embeddings, severely limiting their expressiveness and reasoning capabilities crucial for robust link prediction.\n\nWe introduce Rot-Pro, a novel KGE model that resolves this critical challenge. Rot-Pro ingeniously combines *relational rotation* with a theoretically grounded *projection* mechanism in a complex vector space. Our core innovation lies in demonstrating that transitive relations can be precisely modeled using *idempotent transformations*, allowing entities in a transitive chain to maintain distinct, expressive embeddings while sharing a common projected vector. This unified framework not only supports transitivity but also comprehensively infers symmetry, asymmetry, inversion, and composition  a capability unmatched by any prior KGE model. Empirical validation on benchmark datasets, particularly those rich in transitive patterns like YAGO3-10 and Countries, shows Rot-Pro achieving state-of-the-art performance. Our model significantly enhances the logical consistency and reasoning power of knowledge graphs, paving the way for more robust and intelligent AI systems.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Transitivity relation pattern",
        "Rot-Pro model",
        "Projection-based transitivity modeling",
        "Relational rotation",
        "Unified relation pattern modeling",
        "Link prediction",
        "Entity embeddings",
        "Projection penalty loss",
        "Theoretical proof",
        "State-of-the-art results",
        "Complex vector space"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/83a46afaeb520abcd9b0138507a253f6d4d8bff7.pdf",
      "citation_key": "song2021",
      "metadata": {
        "title": "Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding",
        "authors": [
          "Tengwei Song",
          "Jie Luo",
          "Lei Huang"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding models learn the representations of entities and relations in the knowledge graphs for predicting missing links (relations) between entities. Their effectiveness are deeply affected by the ability of modeling and inferring different relation patterns such as symmetry, asymmetry, inversion, composition and transitivity. Although existing models are already able to model many of these relations patterns, transitivity, a very common relation pattern, is still not been fully supported. In this paper, we first theoretically show that the transitive relations can be modeled with projections. We then propose the Rot-Pro model which combines the projection and relational rotation together. We prove that Rot-Pro can infer all the above relation patterns. Experimental results show that the proposed Rot-Pro model effectively learns the transitivity pattern and achieves the state-of-the-art results on the link prediction task in the datasets containing transitive relations.",
        "file_path": "paper_data/knowledge_graph_embedding/83a46afaeb520abcd9b0138507a253f6d4d8bff7.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem:** Existing Knowledge Graph Embedding (KGE) models struggle to effectively model and infer the *transitivity* relation pattern, despite its commonality and importance for link prediction \\cite{song2021}. While models like RotatE can handle symmetry, asymmetry, inversion, and composition, none fully support transitivity without forcing entity embeddings in a transitive chain to be identical, which limits expressiveness.\n    *   **Importance and challenge:** Transitivity (if (a,r,b) and (b,r,c), then (a,r,c)) is a fundamental logical pattern crucial for robust reasoning and inferring missing links in knowledge graphs. The challenge lies in designing a KGE model that can represent this property without collapsing distinct entity embeddings, while also maintaining the ability to model other complex relation patterns.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches:** The work builds upon and extends previous KGE models, particularly RotatE \\cite{song2021}. It categorizes existing models into \"Trans-series\" (e.g., TransE, TransH, TransR, BoxE) and \"Bilinear models\" (e.g., DistMult, ComplEx, RotatE, QuatE).\n    *   **Limitations of previous solutions:**\n        *   TransE and its variants, while good for composition and inversion, often require the translation vector for transitive relations to be zero, forcing entities in a transitive chain to have identical embeddings \\cite{song2021}.\n        *   RotatE, a state-of-the-art model, can infer symmetry, asymmetry, inversion, and composition, but similarly fails to model transitivity without forcing entity embeddings in a transitive chain to be the same (requiring rotation phases of `2n*pi`) \\cite{song2021}.\n        *   BoxE, a recent Trans-series model, can express composition and inversion but explicitly \"cannot express transitivity\" \\cite{song2021}.\n        *   The paper highlights that \"none of existing models is capable of modeling transitivity relation pattern\" alongside other patterns \\cite{song2021}.\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method:** The proposed Rot-Pro model combines *projection* and *relational rotation* in a complex vector space \\cite{song2021}. For a triple `(h,r,t)`, it requires `rot(pr(eh(k)); r(k)) = pr(et(k))`, where `pr` is a projection and `rot` is a rotation.\n    *   **Novelty/Difference:**\n        *   **Projection for Transitivity:** The key innovation is the theoretical demonstration that transitive relations can be modeled using *idempotent transformations (projections)* \\cite{song2021}. A relation `r` is represented by a projection `pr(k)` defined by an idempotent matrix `Mr(k) = Sr(k)^-1 * diag(ar(k), br(k)) * Sr(k)`, where `ar(k), br(k)  {0,1}`. `Sr(k)` is simplified to a rotation matrix. This allows entities in a transitive chain to have distinct embeddings but share the same *projected vector*, overcoming the limitations of previous models that forced identical embeddings \\cite{song2021}.\n        *   **Unified Framework:** Rot-Pro integrates this projection mechanism for transitivity with the relational rotation mechanism (inspired by RotatE) to model symmetry, asymmetry, inversion, and composition, creating a single model capable of inferring all five patterns \\cite{song2021}.\n        *   **Projection Penalty Loss:** A novel `Lp` loss function is introduced during optimization to enforce the `0` or `1` constraint on the `a` and `b` parameters of the projection matrix, using a weighted penalty to encourage binary values \\cite{song2021}.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insight:** Provides the first theoretical proof that transitive relations can be effectively modeled using idempotent transformations (projections) within a KGE framework \\cite{song2021}.\n    *   **Novel Algorithm:** Introduces Rot-Pro, a new KGE model that uniquely combines projection-based transitivity modeling with relational rotation, enabling comprehensive support for all five major relation patterns \\cite{song2021}.\n    *   **Comprehensive Pattern Support:** Theoretically proves that Rot-Pro can infer symmetry, asymmetry, inversion, composition, and transitivity patterns, a capability unmatched by prior KGE models \\cite{song2021}.\n    *   **Optimization Technique:** Develops a specialized projection penalty loss to ensure the learned projection matrices maintain their idempotent properties, crucial for the model's theoretical foundation \\cite{song2021}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted:** Link prediction experiments were performed on four benchmark datasets: FB15k-237, WN18RR, YAGO3-10, and Countries \\cite{song2021}. The latter two datasets (YAGO3-10 and Countries) were specifically chosen for their \"abundant relation patterns including transitivity\" \\cite{song2021}.\n    *   **Key performance metrics and comparison results:**\n        *   Evaluated using Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hit@k (Hit@1, Hit@3, Hit@10) in a filtered setting \\cite{song2021}.\n        *   Rot-Pro achieved state-of-the-art results on datasets containing transitive relations (YAGO3-10 and Countries) \\cite{song2021}.\n        *   On the Countries dataset, Rot-Pro significantly outperformed RotatE and other baselines, achieving near-perfect AUC-PR scores (1.00, 1.00, 0.998) across three composition tasks, demonstrating its effective inference of transitivity, symmetry, and composition \\cite{song2021}.\n        *   It also outperformed baseline models on most metrics for FB15k-237 and WN18RR, even though these datasets have fewer explicit transitive relations \\cite{song2021}.\n        *   The results empirically validate that Rot-Pro effectively learns the transitivity pattern \\cite{song2021}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations/assumptions:** The primary advantage of Rot-Pro is most pronounced in datasets rich in transitive relations. Its improvement over RotatE on general datasets like FB15k-237 and WN18RR is noted as \"limited\" due to the insufficient presence of transitive relations in those benchmarks \\cite{song2021}.\n    *   **Scope of applicability:** The model is specifically designed for KGE tasks, particularly link prediction, with a strong focus on accurately modeling complex relation patterns, especially transitivity.\n\n*   **Technical Significance**\n    *   **Advancement of state-of-the-art:** Rot-Pro significantly advances the technical state-of-the-art in KGE by providing a unified and theoretically grounded framework that can model *all five* fundamental relation patterns (symmetry, asymmetry, inversion, composition, and transitivity) \\cite{song2021}. This addresses a critical gap in existing KGE models.\n    *   **Potential impact on future research:** The introduction of projection-based modeling for transitivity opens new avenues for designing more expressive and logically consistent KGE models. It provides a strong foundation for future research into comprehensive relation pattern modeling and more sophisticated reasoning capabilities within knowledge graphs \\cite{song2021}.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Transitivity relation pattern",
          "Rot-Pro model",
          "Projection-based transitivity modeling",
          "Relational rotation",
          "Unified relation pattern modeling",
          "Link prediction",
          "Entity embeddings",
          "Projection penalty loss",
          "Theoretical proof",
          "State-of-the-art results",
          "Complex vector space"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "83a46afaeb520abcd9b0138507a253f6d4d8bff7.pdf"
    },
    {
      "success": true,
      "doc_id": "2e89b76614c90730dfd1e22005c709e8",
      "summary": "Here's a focused summary of the paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   Knowledge graphs (KGs) are often incomplete, requiring methods to predict missing relations between entities.\n    *   The core technical problem is that existing knowledge graph embedding models struggle to learn sufficiently expressive features and effectively handle complex relation types (e.g., 1-to-N, N-to-1, N-to-N) \\cite{zhang2020}.\n\n*   **Related Work & Positioning**\n    *   Previous embedding models typically represent (subject, relation, object) triplets using translational distance or semantic matching in vector space \\cite{zhang2020}.\n    *   These prior approaches are limited by their inability to learn a wide range of expressive features, which hinders their performance on complex relation patterns \\cite{zhang2020}.\n\n*   **Technical Approach & Innovation**\n    *   The paper introduces a multi-scale dynamic convolutional network (M-DCN) for knowledge graph embedding \\cite{zhang2020}.\n    *   **Input Layer Innovation**: Subject entity and relation embeddings are composed in an alternating pattern, designed to extract richer feature interactions and enhance expressiveness \\cite{zhang2020}.\n    *   **Convolution Layer Innovation**: Multi-scale filters are generated to capture diverse characteristics from the input embeddings \\cite{zhang2020}.\n    *   **Dynamic Filter Weights**: A key innovation is that the weights of these multi-scale filters are dynamically related to each specific relation, enabling the model to effectively handle and learn complex relation patterns \\cite{zhang2020}.\n\n*   **Key Technical Contributions**\n    *   A novel input composition strategy using an alternating pattern for subject entity and relation embeddings to enhance feature interaction \\cite{zhang2020}.\n    *   The development of multi-scale convolutional filters for learning varied characteristics within embeddings \\cite{zhang2020}.\n    *   The introduction of dynamic, relation-specific filter weights, which is crucial for modeling and addressing complex relation types (1-to-N, N-to-1, N-to-N) \\cite{zhang2020}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted to test M-DCN's performance on five benchmark datasets \\cite{zhang2020}.\n    *   The primary task evaluated was link prediction \\cite{zhang2020}.\n    *   Results demonstrate that M-DCN achieves state-of-the-art link prediction results on most evaluation metrics and effectively handles complex relations \\cite{zhang2020}.\n\n*   **Limitations & Scope**\n    *   The paper primarily focuses on addressing the limitations of previous models in handling complex relation types (1-to-N, N-to-1, N-to-N) \\cite{zhang2020}.\n    *   While the paper highlights the model's ability to generate richer and more expressive features, specific technical limitations of M-DCN itself are not explicitly detailed in the provided abstract.\n\n*   **Technical Significance**\n    *   M-DCN advances the state-of-the-art in knowledge graph embedding by generating richer and more expressive feature embeddings than prior models \\cite{zhang2020}.\n    *   Its innovative dynamic convolutional architecture significantly improves the ability to model and predict complex relations, which is a long-standing challenge in KG completion \\cite{zhang2020}.\n    *   This work provides a strong foundation for future research into more sophisticated, relation-aware convolutional architectures for knowledge graph analysis.",
      "intriguing_abstract": "Knowledge graphs (KGs) are powerful semantic repositories, yet their inherent incompleteness and the struggle of existing embedding models to capture complex relation patterns (1-to-N, N-to-1, N-to-N) significantly hinder their full potential. We introduce M-DCN, a novel multi-scale dynamic convolutional network designed to overcome these critical limitations. M-DCN pioneers an innovative input composition strategy, alternating subject entity and relation embeddings to foster richer, more expressive feature interactions. Crucially, it employs multi-scale convolutional filters whose weights are dynamically generated and uniquely tailored to each specific relation. This dynamic, relation-specific mechanism empowers M-DCN to adaptively learn and model the intricate characteristics of diverse relation types, particularly excelling at complex patterns that confound prior approaches. Our extensive experiments demonstrate that M-DCN achieves state-of-the-art link prediction results across five benchmark datasets, generating significantly more expressive feature embeddings. This work marks a substantial advance in knowledge graph embedding, offering a robust, relation-aware architecture for superior KG completion.",
      "keywords": [
        "Knowledge graph embedding",
        "complex relation types",
        "multi-scale dynamic convolutional network (M-DCN)",
        "link prediction",
        "novel input composition strategy",
        "dynamic relation-specific filter weights",
        "multi-scale convolutional filters",
        "richer feature interactions",
        "expressive feature embeddings",
        "state-of-the-art performance",
        "knowledge graph completion",
        "relation-aware convolutional architectures"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/f44ee7932aacd054101b00f37d4c26c27630c557.pdf",
      "citation_key": "zhang2020",
      "metadata": {
        "title": "Multi-Scale Dynamic Convolutional Network for Knowledge Graph Embedding",
        "authors": [
          "Zhaoli Zhang",
          "Zhifei Li",
          "Hai Liu",
          "N. Xiong"
        ],
        "published_date": "2020",
        "abstract": "Knowledge graphs are large graph-structured knowledge bases with incomplete or partial information. Numerous studies have focused on knowledge graph embedding to identify the embedded representation of entities and relations, thereby predicting missing relations between entities. Previous embedding models primarily regard (subject entity, relation, and object entity) triplet as translational distance or semantic matching in vector space. However, these models only learn a few expressive features and hard to handle complex relations, i.e., 1-to-N, N-to-1, and N-to-N, in knowledge graphs. To overcome these issues, we introduce a multi-scale dynamic convolutional network (M-DCN) model for knowledge graph embedding. This model features topnotch performance and an ability to generate richer and more expressive feature embeddings than its counterparts. The subject entity and relation embeddings in M-DCN are composed in an alternating pattern in the input layer, which helps extract additional feature interactions and increase the expressiveness. Multi-scale filters are generated in the convolution layer to learn different characteristics among input embeddings. Specifically, the weights of these filters are dynamically related to each relation to model complex relations. The performance of M-DCN on the five benchmark datasets is tested via experiments. Results show that the model can effectively handle complex relations and achieve state-of-the-art link prediction results on most evaluation metrics.",
        "file_path": "paper_data/knowledge_graph_embedding/f44ee7932aacd054101b00f37d4c26c27630c557.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   Knowledge graphs (KGs) are often incomplete, requiring methods to predict missing relations between entities.\n    *   The core technical problem is that existing knowledge graph embedding models struggle to learn sufficiently expressive features and effectively handle complex relation types (e.g., 1-to-N, N-to-1, N-to-N) \\cite{zhang2020}.\n\n*   **Related Work & Positioning**\n    *   Previous embedding models typically represent (subject, relation, object) triplets using translational distance or semantic matching in vector space \\cite{zhang2020}.\n    *   These prior approaches are limited by their inability to learn a wide range of expressive features, which hinders their performance on complex relation patterns \\cite{zhang2020}.\n\n*   **Technical Approach & Innovation**\n    *   The paper introduces a multi-scale dynamic convolutional network (M-DCN) for knowledge graph embedding \\cite{zhang2020}.\n    *   **Input Layer Innovation**: Subject entity and relation embeddings are composed in an alternating pattern, designed to extract richer feature interactions and enhance expressiveness \\cite{zhang2020}.\n    *   **Convolution Layer Innovation**: Multi-scale filters are generated to capture diverse characteristics from the input embeddings \\cite{zhang2020}.\n    *   **Dynamic Filter Weights**: A key innovation is that the weights of these multi-scale filters are dynamically related to each specific relation, enabling the model to effectively handle and learn complex relation patterns \\cite{zhang2020}.\n\n*   **Key Technical Contributions**\n    *   A novel input composition strategy using an alternating pattern for subject entity and relation embeddings to enhance feature interaction \\cite{zhang2020}.\n    *   The development of multi-scale convolutional filters for learning varied characteristics within embeddings \\cite{zhang2020}.\n    *   The introduction of dynamic, relation-specific filter weights, which is crucial for modeling and addressing complex relation types (1-to-N, N-to-1, N-to-N) \\cite{zhang2020}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted to test M-DCN's performance on five benchmark datasets \\cite{zhang2020}.\n    *   The primary task evaluated was link prediction \\cite{zhang2020}.\n    *   Results demonstrate that M-DCN achieves state-of-the-art link prediction results on most evaluation metrics and effectively handles complex relations \\cite{zhang2020}.\n\n*   **Limitations & Scope**\n    *   The paper primarily focuses on addressing the limitations of previous models in handling complex relation types (1-to-N, N-to-1, N-to-N) \\cite{zhang2020}.\n    *   While the paper highlights the model's ability to generate richer and more expressive features, specific technical limitations of M-DCN itself are not explicitly detailed in the provided abstract.\n\n*   **Technical Significance**\n    *   M-DCN advances the state-of-the-art in knowledge graph embedding by generating richer and more expressive feature embeddings than prior models \\cite{zhang2020}.\n    *   Its innovative dynamic convolutional architecture significantly improves the ability to model and predict complex relations, which is a long-standing challenge in KG completion \\cite{zhang2020}.\n    *   This work provides a strong foundation for future research into more sophisticated, relation-aware convolutional architectures for knowledge graph analysis.",
        "keywords": [
          "Knowledge graph embedding",
          "complex relation types",
          "multi-scale dynamic convolutional network (M-DCN)",
          "link prediction",
          "novel input composition strategy",
          "dynamic relation-specific filter weights",
          "multi-scale convolutional filters",
          "richer feature interactions",
          "expressive feature embeddings",
          "state-of-the-art performance",
          "knowledge graph completion",
          "relation-aware convolutional architectures"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "f44ee7932aacd054101b00f37d4c26c27630c557.pdf"
    },
    {
      "success": true,
      "doc_id": "3b3ad4666d5c5e7be0a21a09b9dc0f07",
      "summary": "Here's a focused summary of the paper \\cite{ge2022} for a literature review:\n\n### Technical Paper Analysis: CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations \\cite{ge2022}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of effectively modeling complex relation types (e.g., 1-N, N-1, N-N, symmetric, antisymmetric, transitive, non-commutative, sub-relations) in Knowledge Graph Embedding (KGE) models, especially in low-dimensional settings and for large-scale KGs.\n    *   **Importance and Challenge**: Real-world KGs contain a vast number of entities and complex relations, making it difficult for existing KGE models to achieve high performance without high-dimensional embeddings or to handle diverse relation patterns. Many existing models (like TransE, RotatE) are limited to a single geometric operation, which restricts their expressive power and ability to distinguish complex relation compositions.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: CompoundE builds upon distance-based KGE models like TransE (translation) and RotatE (rotation) by integrating a broader set of geometric operations. It positions itself as a generalization of several distance-based models, including TransE, RotatE, PairRE, and LinearRE, by showing they can be derived as special cases of CompoundE.\n    *   **Limitations of Previous Solutions**:\n        *   TransE struggles with 1-N, N-1, N-N, and symmetric relations.\n        *   Models based on single operations (translation, rotation, scaling) lack the expressive power to capture the full complexity of relations, particularly non-commutative compositions or hierarchical structures.\n        *   Many models may require high-dimensional embeddings, leading to memory constraints and computational costs, especially for large KGs.\n        *   Existing models often have specific strengths but also weaknesses, and a unified approach leveraging multiple strengths is lacking.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: CompoundE proposes a novel KGE model that leverages a cascade of three fundamental geometric manipulation operations: translation, rotation, and scaling. These operations are applied to entity embeddings to model relations.\n        *   It defines three forms of scoring functions: CompoundE-Head (applies compound operation to head entity), CompoundE-Tail (applies to tail entity), and CompoundE-Full (applies to both).\n        *   The constituent operators (translation, rotation, scaling) are relation-specific and can be cascaded in any order or subset, offering significant design flexibility.\n        *   The model uses a self-adversarial negative sampling loss function, similar to RotatE.\n    *   **Novelty/Difference**:\n        *   **Compound Operations**: It is the first KGE model to systematically combine all three geometric operations (translation, rotation, scaling) into a \"compound operation,\" inspired by their successful use in image processing.\n        *   **Affine Group Framework**: CompoundE is formally cast within the framework of the affine group, demonstrating its mathematical properties and showing that it is a more general transformation than those restricted to the Special Euclidean Group (which includes only translation and rotation). This mathematical foundation explains its enhanced capability to model complex relations.\n        *   **Generalization**: It mathematically proves that several existing distance-based KGE models are special cases of CompoundE, providing a unifying perspective.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of CompoundE, a novel KGE model that integrates translation, rotation, and scaling operations in a cascaded, relation-specific manner.\n    *   **Theoretical Insights/Analysis**: Mathematical proof that CompoundE belongs to the affine group, which is a more general group than the Special Euclidean Group, enabling it to model a richer set of complex relation types (e.g., symmetric/antisymmetric, inversion, transitive, commutative/non-commutative, sub-relations).\n    *   **System Design/Architectural Innovations**: The flexible design allows for various permutations and subsets of the three operations, enabling adaptation to different dataset characteristics.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive link prediction experiments were conducted on three popular KG completion datasets.\n    *   **Datasets**: ogbl-wikikg2 (large-scale, challenging for scalability), FB15k-237, and WN18RR (challenging for modeling symmetry/antisymmetry and composition relation patterns).\n    *   **Key Performance Metrics**: Mean Reciprocal Rank (MRR) and Hits@k (Hits@1, Hits@3, Hits@10) using filtered ranking protocol.\n    *   **Comparison Results**:\n        *   CompoundE consistently achieved state-of-the-art performance across all three datasets, outperforming numerous benchmarking models including TransE, DistMult, ComplEx, RotatE, TuckER, AutoSF, PairRE, and GIE.\n        *   On the large-scale ogbl-wikikg2 dataset, CompoundE significantly outperformed previous KGE models with *fewer parameters* and *lower embedding dimensions*, implying reduced computation and memory costs while achieving higher accuracy.\n        *   The results confirm that cascading geometric transformations is an effective strategy, as CompoundE showed significant improvement over models using single operations.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on distance-based scoring functions and does not explicitly discuss limitations of CompoundE itself, but rather highlights its advantages over previous models. The detailed proofs for modeling various relation types are stated to be in the appendix (Section 6.3), which is not provided in the excerpt.\n    *   **Scope of Applicability**: The model is primarily validated for the link prediction (KG completion) task. Its applicability to other downstream tasks (e.g., multi-hop reasoning, entity classification) is implied but not directly demonstrated in the provided text.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: CompoundE significantly advances the technical state-of-the-art in KGE by introducing a more powerful and generalized geometric transformation framework. It demonstrates that combining translation, rotation, and scaling operations is highly effective for modeling complex relations and achieving superior performance.\n    *   **Potential Impact on Future Research**:\n        *   Offers a unifying perspective for many existing distance-based KGE models, potentially guiding the design of future models.\n        *   Its ability to handle complex relations and scale efficiently to large KGs with fewer parameters opens avenues for developing more robust and resource-efficient KGE solutions.\n        *   The affine group theoretical foundation provides a strong mathematical basis for further exploration of geometric transformations in KGE.",
      "intriguing_abstract": "Unlocking the full expressive power of Knowledge Graphs (KGs) remains a grand challenge, particularly in modeling their intricate and diverse relation types with limited embedding dimensions. Existing Knowledge Graph Embedding (KGE) models often struggle with complex patterns like 1-N, N-N, symmetric, or non-commutative relations, limiting their real-world applicability. We introduce CompoundE, a novel KGE model that systematically integrates a cascade of three fundamental geometric operations: translation, rotation, and scaling. Unlike prior approaches restricted to single operations, CompoundE leverages this compound transformation, inspired by its success in image processing, to capture a significantly richer spectrum of relational semantics.\n\nThe theoretical foundation of CompoundE is rooted in the affine group, mathematically proving its superior generality over models based on the Special Euclidean Group. This framework not only unifies several existing distance-based KGE models as special cases but also enables CompoundE to inherently model a broader range of complex relation types. Extensive link prediction experiments on challenging datasets (FB15k-237, WN18RR, ogbl-wikikg2) demonstrate CompoundE's state-of-the-art performance. Crucially, it achieves superior accuracy with fewer parameters and lower embedding dimensions, offering a highly efficient and robust solution for large-scale KGs. CompoundE represents a significant leap forward, providing a powerful, generalized framework for understanding and embedding complex relational data.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "CompoundE",
        "Compound Geometric Operations (Translation",
        "Rotation",
        "Scaling)",
        "Affine Group Framework",
        "Modeling Complex Relations",
        "Link Prediction",
        "State-of-the-art Performance",
        "Generalization of KGE Models",
        "Low-dimensional Embeddings",
        "Large-scale Knowledge Graphs",
        "Distance-based KGE models",
        "Self-adversarial Negative Sampling"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/44ce738296c3148c6593324773706cdc228614d4.pdf",
      "citation_key": "ge2022",
      "metadata": {
        "title": "CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations",
        "authors": [
          "Xiou Ge",
          "Yun Cheng Wang",
          "Bin Wang",
          "C.-C. Jay Kuo"
        ],
        "published_date": "2022",
        "abstract": "Translation, rotation, and scaling are three commonly used geometric manipulation operations in image processing. Besides, some of them are successfully used in developing effective knowledge graph embedding (KGE) models such as TransE and RotatE. Inspired by the synergy, we propose a new KGE model by leveraging all three operations in this work. Since translation, rotation, and scaling operations are cascaded to form a compound one, the new model is named CompoundE. By casting CompoundE in the framework of group theory, we show that quite a few scoring-function-based KGE models are special cases of CompoundE. CompoundE extends the simple distance-based relation to relation-dependent compound operations on head and/or tail entities. To demonstrate the effectiveness of CompoundE, we conduct experiments on three popular KG completion datasets. Experimental results show that CompoundE consistently achieves the state of-the-art performance.",
        "file_path": "paper_data/knowledge_graph_embedding/44ce738296c3148c6593324773706cdc228614d4.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{ge2022} for a literature review:\n\n### Technical Paper Analysis: CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations \\cite{ge2022}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of effectively modeling complex relation types (e.g., 1-N, N-1, N-N, symmetric, antisymmetric, transitive, non-commutative, sub-relations) in Knowledge Graph Embedding (KGE) models, especially in low-dimensional settings and for large-scale KGs.\n    *   **Importance and Challenge**: Real-world KGs contain a vast number of entities and complex relations, making it difficult for existing KGE models to achieve high performance without high-dimensional embeddings or to handle diverse relation patterns. Many existing models (like TransE, RotatE) are limited to a single geometric operation, which restricts their expressive power and ability to distinguish complex relation compositions.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: CompoundE builds upon distance-based KGE models like TransE (translation) and RotatE (rotation) by integrating a broader set of geometric operations. It positions itself as a generalization of several distance-based models, including TransE, RotatE, PairRE, and LinearRE, by showing they can be derived as special cases of CompoundE.\n    *   **Limitations of Previous Solutions**:\n        *   TransE struggles with 1-N, N-1, N-N, and symmetric relations.\n        *   Models based on single operations (translation, rotation, scaling) lack the expressive power to capture the full complexity of relations, particularly non-commutative compositions or hierarchical structures.\n        *   Many models may require high-dimensional embeddings, leading to memory constraints and computational costs, especially for large KGs.\n        *   Existing models often have specific strengths but also weaknesses, and a unified approach leveraging multiple strengths is lacking.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: CompoundE proposes a novel KGE model that leverages a cascade of three fundamental geometric manipulation operations: translation, rotation, and scaling. These operations are applied to entity embeddings to model relations.\n        *   It defines three forms of scoring functions: CompoundE-Head (applies compound operation to head entity), CompoundE-Tail (applies to tail entity), and CompoundE-Full (applies to both).\n        *   The constituent operators (translation, rotation, scaling) are relation-specific and can be cascaded in any order or subset, offering significant design flexibility.\n        *   The model uses a self-adversarial negative sampling loss function, similar to RotatE.\n    *   **Novelty/Difference**:\n        *   **Compound Operations**: It is the first KGE model to systematically combine all three geometric operations (translation, rotation, scaling) into a \"compound operation,\" inspired by their successful use in image processing.\n        *   **Affine Group Framework**: CompoundE is formally cast within the framework of the affine group, demonstrating its mathematical properties and showing that it is a more general transformation than those restricted to the Special Euclidean Group (which includes only translation and rotation). This mathematical foundation explains its enhanced capability to model complex relations.\n        *   **Generalization**: It mathematically proves that several existing distance-based KGE models are special cases of CompoundE, providing a unifying perspective.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of CompoundE, a novel KGE model that integrates translation, rotation, and scaling operations in a cascaded, relation-specific manner.\n    *   **Theoretical Insights/Analysis**: Mathematical proof that CompoundE belongs to the affine group, which is a more general group than the Special Euclidean Group, enabling it to model a richer set of complex relation types (e.g., symmetric/antisymmetric, inversion, transitive, commutative/non-commutative, sub-relations).\n    *   **System Design/Architectural Innovations**: The flexible design allows for various permutations and subsets of the three operations, enabling adaptation to different dataset characteristics.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive link prediction experiments were conducted on three popular KG completion datasets.\n    *   **Datasets**: ogbl-wikikg2 (large-scale, challenging for scalability), FB15k-237, and WN18RR (challenging for modeling symmetry/antisymmetry and composition relation patterns).\n    *   **Key Performance Metrics**: Mean Reciprocal Rank (MRR) and Hits@k (Hits@1, Hits@3, Hits@10) using filtered ranking protocol.\n    *   **Comparison Results**:\n        *   CompoundE consistently achieved state-of-the-art performance across all three datasets, outperforming numerous benchmarking models including TransE, DistMult, ComplEx, RotatE, TuckER, AutoSF, PairRE, and GIE.\n        *   On the large-scale ogbl-wikikg2 dataset, CompoundE significantly outperformed previous KGE models with *fewer parameters* and *lower embedding dimensions*, implying reduced computation and memory costs while achieving higher accuracy.\n        *   The results confirm that cascading geometric transformations is an effective strategy, as CompoundE showed significant improvement over models using single operations.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on distance-based scoring functions and does not explicitly discuss limitations of CompoundE itself, but rather highlights its advantages over previous models. The detailed proofs for modeling various relation types are stated to be in the appendix (Section 6.3), which is not provided in the excerpt.\n    *   **Scope of Applicability**: The model is primarily validated for the link prediction (KG completion) task. Its applicability to other downstream tasks (e.g., multi-hop reasoning, entity classification) is implied but not directly demonstrated in the provided text.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: CompoundE significantly advances the technical state-of-the-art in KGE by introducing a more powerful and generalized geometric transformation framework. It demonstrates that combining translation, rotation, and scaling operations is highly effective for modeling complex relations and achieving superior performance.\n    *   **Potential Impact on Future Research**:\n        *   Offers a unifying perspective for many existing distance-based KGE models, potentially guiding the design of future models.\n        *   Its ability to handle complex relations and scale efficiently to large KGs with fewer parameters opens avenues for developing more robust and resource-efficient KGE solutions.\n        *   The affine group theoretical foundation provides a strong mathematical basis for further exploration of geometric transformations in KGE.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "CompoundE",
          "Compound Geometric Operations (Translation",
          "Rotation",
          "Scaling)",
          "Affine Group Framework",
          "Modeling Complex Relations",
          "Link Prediction",
          "State-of-the-art Performance",
          "Generalization of KGE Models",
          "Low-dimensional Embeddings",
          "Large-scale Knowledge Graphs",
          "Distance-based KGE models",
          "Self-adversarial Negative Sampling"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "44ce738296c3148c6593324773706cdc228614d4.pdf"
    },
    {
      "success": true,
      "doc_id": "05cddc7305aef6218d368ecf770f5bd1",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the challenge of developing effective Knowledge Graph Embedding (KGE) methods that overcome the complexity and computational cost of existing deep neural network (DNN) based approaches \\cite{ren2020}.\n    *   While DNN-based KGEs achieve state-of-the-art performance, they are often \"very complex and need much time for training and inference,\" hindering their use in real-time applications \\cite{ren2020}.\n    *   Additionally, deep convolutional neural network (DCNN) based methods often suffer from a \"reduced feature resolution issue\" due to repeated max-pooling and down-sampling operations \\cite{ren2020}.\n    *   The core motivation is to find a better trade-off between model complexity (number of parameters) and model expressiveness (performance in capturing semantic information) \\cite{ren2020}.\n\n*   **Related Work & Positioning**\n    *   The work positions itself against two main categories of KGE methods:\n        *   **Translation-based and Bilinear models:** Such as TransE, TransH, ComplEx, HolE, RotatE, which define relations as translation operations or combination operators \\cite{ren2020}.\n        *   **Deep Neural Network (DNN) and Graph Neural Network (GNN) based models:** Including ConvE, ConvKB, R-GCN, CompGCN, which have pushed KGE performance but are criticized for their \"very complex and time-consuming\" nature \\cite{ren2020}.\n    *   The paper highlights that existing DCNN methods suffer from reduced feature resolution, a problem that atrous convolution aims to solve \\cite{ren2020}. It also notes that many DNN/GNN methods are too complex for online/real-time scenarios \\cite{ren2020}.\n\n*   **Technical Approach & Innovation**\n    *   The paper proposes **AcrE (Atrous Convolution and Residual Embedding)**, a simple yet effective KGE method \\cite{ren2020}.\n    *   **Atrous Convolution:** This is the core innovation, allowing the model to \"effectively enlarge the field of view of filters almost without increasing the number of parameters or the amount of computations\" \\cite{ren2020}. It addresses the reduced feature resolution issue of standard DCNNs.\n    *   **Residual Learning:** Introduced to combat the \"original information forgotten issue\" (where features become increasingly detached from initial input with more convolutions) and the \"vanishing/exploding gradient issue\" inherent in deep networks \\cite{ren2020}. It adds original input information back to the processed features.\n    *   **Two Learning Structures:**\n        *   **Serial AcrE:** Standard convolution followed by multiple atrous convolutions in sequence, with a residual connection combining the final output with the initial embeddings \\cite{ren2020}.\n        *   **Parallel AcrE:** Standard and multiple atrous convolutions are performed simultaneously, their results are integrated (via element-add or concatenation), and then combined with initial embeddings via residual learning \\cite{ren2020}.\n    *   **2D Embedding Representation:** Similar to ConvE, entity and relation embeddings are reshaped into a 2D representation before convolution to increase expressiveness \\cite{ren2020}.\n    *   **Loss Function:** Uses a listwise binary cross-entropy loss, similar to ConvE, which contributes to fast training and inference \\cite{ren2020}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of atrous convolutions to KGE, enabling a larger receptive field and richer feature interactions without increasing model complexity or parameters \\cite{ren2020}.\n    *   **Architectural Innovations:** Design of two distinct architectures (Serial AcrE and Parallel AcrE) that effectively integrate standard and atrous convolutions with residual learning \\cite{ren2020}.\n    *   **Problem Mitigation:** Effectively addresses the \"reduced feature resolution\" problem in DCNNs and the \"original information forgotten\" and \"vanishing/exploding gradient\" issues in deep KGE models through atrous convolution and residual learning, respectively \\cite{ren2020}.\n    *   **Efficiency:** Achieves a simpler structure and higher parameter efficiency compared to many existing complex DNN/GNN KGE methods \\cite{ren2020}.\n\n*   **Experimental Validation**\n    *   **Experiments:** Conducted link prediction tasks to evaluate the model's ability to predict missing entities in triplets \\cite{ren2020}.\n    *   **Datasets:** Evaluated on six benchmark datasets: WN18, FB15k, WN18RR, FB15k-237, Alyawarra Kinship, and DB100K \\cite{ren2020}.\n    *   **Metrics:** Used standard KGE evaluation metrics: Mean Reciprocal Rank (MRR) and Hits@k (k=1, 3, 10) \\cite{ren2020}.\n    *   **Key Results:**\n        *   AcrE \"significantly outperforms the compared state-of-the-art results under all the evaluation metrics on all datasets except for WN18RR\" \\cite{ren2020}.\n        *   Achieved substantial improvements on DB100K, FB15k, and Kinship datasets, often by a large margin \\cite{ren2020}. For instance, on DB100K, AcrE (Parallel) achieved an MRR of 0.413, outperforming the SOTA SEEK (0.338) \\cite{ren2020}.\n        *   AcrE (Parallel) generally showed better performance than AcrE (Serial) \\cite{ren2020}.\n        *   Even on WN18RR, AcrE achieved competitive results and significantly outperformed other DCNN-based KGE methods like ConvE and ConvKB \\cite{ren2020}.\n\n*   **Limitations & Scope**\n    *   The paper primarily focuses on link prediction as the evaluation task \\cite{ren2020}.\n    *   While generally superior, AcrE's performance on WN18RR was competitive rather than universally superior to *all* baselines, though it still outperformed other DCNN-based methods \\cite{ren2020}.\n    *   The scope is limited to KGE for structured knowledge graphs, not explicitly addressing textual or multimodal KGE.\n    *   The paper does not explicitly state technical limitations of AcrE itself, but rather positions it as a solution to limitations of prior work.\n\n*   **Technical Significance**\n    *   AcrE advances the technical state-of-the-art by demonstrating that simpler, more parameter-efficient deep learning architectures can achieve superior performance in KGE \\cite{ren2020}.\n    *   It provides a practical solution to the trade-off between model complexity and expressiveness, making KGE models more viable for \"on-line or real-time application scenarios\" \\cite{ren2020}.\n    *   The successful integration of atrous convolutions and residual learning offers a novel paradigm for designing efficient and effective convolutional KGE models, potentially inspiring future research into lightweight yet powerful architectures for knowledge representation \\cite{ren2020}.",
      "intriguing_abstract": "Deep neural networks have propelled Knowledge Graph Embedding (KGE) to new performance heights, yet their inherent complexity, computational cost, and susceptibility to reduced feature resolution in DCNNs often preclude real-time applications. We introduce AcrE (Atrous Convolution and Residual Embedding), a novel and remarkably efficient KGE method designed to overcome these limitations. AcrE innovatively integrates **atrous convolution** to dramatically enlarge the receptive field and capture richer relational patterns without increasing model parameters, directly addressing the feature resolution challenge. Furthermore, **residual learning** is incorporated to combat information forgetting and stabilize gradient flow in deep architectures. Through two distinct architectures, Serial and Parallel AcrE, our model achieves an exceptional balance of expressiveness and efficiency. Extensive **link prediction** experiments on six benchmark datasets demonstrate that AcrE significantly outperforms state-of-the-art KGE models, achieving substantial gains on datasets like DB100K and FB15k, while maintaining a simpler, more parameter-efficient structure. This breakthrough offers a practical solution for robust and scalable **real-time knowledge graph applications**, proving that superior KGE performance can be achieved without excessive complexity.",
      "keywords": [
        "AcrE (Atrous Convolution and Residual Embedding)",
        "Knowledge Graph Embedding (KGE)",
        "Atrous Convolution",
        "Residual Learning",
        "reduced feature resolution",
        "model complexity and expressiveness",
        "real-time applications",
        "link prediction",
        "parameter efficiency",
        "state-of-the-art performance",
        "Deep Convolutional Neural Networks (DCNNs)",
        "Serial and Parallel AcrE architectures",
        "vanishing/exploding gradient"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/bcdb8914550df02bfe1f69348c9830d775f6590a.pdf",
      "citation_key": "ren2020",
      "metadata": {
        "title": "Knowledge Graph Embedding with Atrous Convolution and Residual Learning",
        "authors": [
          "Feiliang Ren",
          "Jucheng Li",
          "Huihui Zhang",
          "Shilei Liu",
          "Bochao Li",
          "Ruicheng Ming",
          "Yujia Bai"
        ],
        "published_date": "2020",
        "abstract": "Knowledge graph embedding is an important task and it will benefit lots of downstream applications. Currently, deep neural networks based methods achieve state-of-the-art performance. However, most of these existing methods are very complex and need much time for training and inference. To address this issue, we propose a simple but effective atrous convolution based knowledge graph embedding method. Compared with existing state-of-the-art methods, our method has following main characteristics. First, it effectively increases feature interactions by using atrous convolutions. Second, to address the original information forgotten issue and vanishing/exploding gradient issue, it uses the residual learning method. Third, it has simpler structure but much higher parameter efficiency. We evaluate our method on six benchmark datasets with different evaluation metrics. Extensive experiments show that our model is very effective. On these diverse datasets, it achieves better results than the compared state-of-the-art methods on most of evaluation metrics. The source codes of our model could be found at https://github.com/neukg/AcrE.",
        "file_path": "paper_data/knowledge_graph_embedding/bcdb8914550df02bfe1f69348c9830d775f6590a.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the challenge of developing effective Knowledge Graph Embedding (KGE) methods that overcome the complexity and computational cost of existing deep neural network (DNN) based approaches \\cite{ren2020}.\n    *   While DNN-based KGEs achieve state-of-the-art performance, they are often \"very complex and need much time for training and inference,\" hindering their use in real-time applications \\cite{ren2020}.\n    *   Additionally, deep convolutional neural network (DCNN) based methods often suffer from a \"reduced feature resolution issue\" due to repeated max-pooling and down-sampling operations \\cite{ren2020}.\n    *   The core motivation is to find a better trade-off between model complexity (number of parameters) and model expressiveness (performance in capturing semantic information) \\cite{ren2020}.\n\n*   **Related Work & Positioning**\n    *   The work positions itself against two main categories of KGE methods:\n        *   **Translation-based and Bilinear models:** Such as TransE, TransH, ComplEx, HolE, RotatE, which define relations as translation operations or combination operators \\cite{ren2020}.\n        *   **Deep Neural Network (DNN) and Graph Neural Network (GNN) based models:** Including ConvE, ConvKB, R-GCN, CompGCN, which have pushed KGE performance but are criticized for their \"very complex and time-consuming\" nature \\cite{ren2020}.\n    *   The paper highlights that existing DCNN methods suffer from reduced feature resolution, a problem that atrous convolution aims to solve \\cite{ren2020}. It also notes that many DNN/GNN methods are too complex for online/real-time scenarios \\cite{ren2020}.\n\n*   **Technical Approach & Innovation**\n    *   The paper proposes **AcrE (Atrous Convolution and Residual Embedding)**, a simple yet effective KGE method \\cite{ren2020}.\n    *   **Atrous Convolution:** This is the core innovation, allowing the model to \"effectively enlarge the field of view of filters almost without increasing the number of parameters or the amount of computations\" \\cite{ren2020}. It addresses the reduced feature resolution issue of standard DCNNs.\n    *   **Residual Learning:** Introduced to combat the \"original information forgotten issue\" (where features become increasingly detached from initial input with more convolutions) and the \"vanishing/exploding gradient issue\" inherent in deep networks \\cite{ren2020}. It adds original input information back to the processed features.\n    *   **Two Learning Structures:**\n        *   **Serial AcrE:** Standard convolution followed by multiple atrous convolutions in sequence, with a residual connection combining the final output with the initial embeddings \\cite{ren2020}.\n        *   **Parallel AcrE:** Standard and multiple atrous convolutions are performed simultaneously, their results are integrated (via element-add or concatenation), and then combined with initial embeddings via residual learning \\cite{ren2020}.\n    *   **2D Embedding Representation:** Similar to ConvE, entity and relation embeddings are reshaped into a 2D representation before convolution to increase expressiveness \\cite{ren2020}.\n    *   **Loss Function:** Uses a listwise binary cross-entropy loss, similar to ConvE, which contributes to fast training and inference \\cite{ren2020}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of atrous convolutions to KGE, enabling a larger receptive field and richer feature interactions without increasing model complexity or parameters \\cite{ren2020}.\n    *   **Architectural Innovations:** Design of two distinct architectures (Serial AcrE and Parallel AcrE) that effectively integrate standard and atrous convolutions with residual learning \\cite{ren2020}.\n    *   **Problem Mitigation:** Effectively addresses the \"reduced feature resolution\" problem in DCNNs and the \"original information forgotten\" and \"vanishing/exploding gradient\" issues in deep KGE models through atrous convolution and residual learning, respectively \\cite{ren2020}.\n    *   **Efficiency:** Achieves a simpler structure and higher parameter efficiency compared to many existing complex DNN/GNN KGE methods \\cite{ren2020}.\n\n*   **Experimental Validation**\n    *   **Experiments:** Conducted link prediction tasks to evaluate the model's ability to predict missing entities in triplets \\cite{ren2020}.\n    *   **Datasets:** Evaluated on six benchmark datasets: WN18, FB15k, WN18RR, FB15k-237, Alyawarra Kinship, and DB100K \\cite{ren2020}.\n    *   **Metrics:** Used standard KGE evaluation metrics: Mean Reciprocal Rank (MRR) and Hits@k (k=1, 3, 10) \\cite{ren2020}.\n    *   **Key Results:**\n        *   AcrE \"significantly outperforms the compared state-of-the-art results under all the evaluation metrics on all datasets except for WN18RR\" \\cite{ren2020}.\n        *   Achieved substantial improvements on DB100K, FB15k, and Kinship datasets, often by a large margin \\cite{ren2020}. For instance, on DB100K, AcrE (Parallel) achieved an MRR of 0.413, outperforming the SOTA SEEK (0.338) \\cite{ren2020}.\n        *   AcrE (Parallel) generally showed better performance than AcrE (Serial) \\cite{ren2020}.\n        *   Even on WN18RR, AcrE achieved competitive results and significantly outperformed other DCNN-based KGE methods like ConvE and ConvKB \\cite{ren2020}.\n\n*   **Limitations & Scope**\n    *   The paper primarily focuses on link prediction as the evaluation task \\cite{ren2020}.\n    *   While generally superior, AcrE's performance on WN18RR was competitive rather than universally superior to *all* baselines, though it still outperformed other DCNN-based methods \\cite{ren2020}.\n    *   The scope is limited to KGE for structured knowledge graphs, not explicitly addressing textual or multimodal KGE.\n    *   The paper does not explicitly state technical limitations of AcrE itself, but rather positions it as a solution to limitations of prior work.\n\n*   **Technical Significance**\n    *   AcrE advances the technical state-of-the-art by demonstrating that simpler, more parameter-efficient deep learning architectures can achieve superior performance in KGE \\cite{ren2020}.\n    *   It provides a practical solution to the trade-off between model complexity and expressiveness, making KGE models more viable for \"on-line or real-time application scenarios\" \\cite{ren2020}.\n    *   The successful integration of atrous convolutions and residual learning offers a novel paradigm for designing efficient and effective convolutional KGE models, potentially inspiring future research into lightweight yet powerful architectures for knowledge representation \\cite{ren2020}.",
        "keywords": [
          "AcrE (Atrous Convolution and Residual Embedding)",
          "Knowledge Graph Embedding (KGE)",
          "Atrous Convolution",
          "Residual Learning",
          "reduced feature resolution",
          "model complexity and expressiveness",
          "real-time applications",
          "link prediction",
          "parameter efficiency",
          "state-of-the-art performance",
          "Deep Convolutional Neural Networks (DCNNs)",
          "Serial and Parallel AcrE architectures",
          "vanishing/exploding gradient"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "bcdb8914550df02bfe1f69348c9830d775f6590a.pdf"
    },
    {
      "success": true,
      "doc_id": "568d74a4b943e6f06f2eb62230b8aea3",
      "summary": "Here's a focused summary of the paper \"TransGate: Knowledge Graph Embedding with Shared Gate Structure\" \\cite{yuan2019} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) models, particularly \"discriminate models\" that aim for higher accuracy, suffer from high time complexity and large parameter counts. This is primarily because they assume independence between relations and learn unique, relation-specific parameter sets, making them inefficient for real-world, large-scale Knowledge Graphs (KGs).\n    *   **Importance and Challenge**: KGs are crucial for many AI applications, but they are often incomplete. Knowledge Graph Completion (KGC) requires effective and scalable solutions. The challenge lies in developing models that can learn expressive, relation-specific features without incurring prohibitive computational costs and parameter sizes, which often lead to the need for complex feature engineering, more hyperparameters, and pre-training.\n\n*   **Related Work & Positioning**\n    *   **Indiscriminate Models (e.g., TransE, DistMult, ComplEx)**: These models are scalable due to limited parameters and low computational costs but often learn less expressive features, leading to lower accuracy.\n    *   **Discriminate Models (e.g., TransH, TransR, TransD, NTN, ConvE, R-GCN)**: These models aim to improve precision by discriminating relation-specific information.\n    *   **Limitations of Previous Solutions**:\n        *   **Large Parameters**: Discriminate models learn unique parameter sets for each relation, leading to massive parameter counts (e.g., TransD on Freebase can have >33GB parameters).\n        *   **High Time Complexity**: They often employ increasingly complex feature engineering, resulting in high computational costs.\n        *   **Overfitting & Hyperparameters**: Due to large parameter sizes and complex designs, they frequently require more hyperparameters and pre-training to prevent overfitting.\n    *   **Positioning**: \\cite{yuan2019} positions TransGate as a solution that addresses these limitations by introducing parameter sharing, aiming to simultaneously learn more expressive features, reduce parameters, and avoid complex feature engineering, thus finding a better trade-off between complexity and expressivity.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{yuan2019} proposes **TransGate**, a novel KGE model that leverages a **shared gate structure** (inspired by LSTM) to discriminate relation-specific information. Instead of learning unique parameters for each relation, TransGate uses only two shared gates for all relations.\n    *   **Novelty/Difference**:\n        *   **Parameter Sharing**: The key innovation is the concept of parameter sharing across relations. By using shared gates, TransGate exploits the inherent relevance and semantic sharing between relations, allowing it to learn expressive features with significantly fewer parameters.\n        *   **Adaptive Non-linear Discrimination**: The gate structure, composed of a sigmoid activation function and a Hadamard product, enables adaptive and non-linear filtering of entity embeddings based on both the entity and relation, generating relation-specific entity representations (`hr`, `tr`).\n        *   **Two Variants for Scalability**:\n            *   `TransGate(fc)`: Uses standard fully connected layers within the gates for precise discrimination.\n            *   `TransGate(wv)`: Reconstructs the gate with **weight vectors** instead of weight matrices. This crucial modification avoids matrix-vector multiplication operations, drastically reducing calculation and decreasing time complexity to the same order as indiscriminate models like TransE, making it highly scalable.\n        *   **Translation-based Scoring**: After discrimination, a translation-based score function `fr(h;t) = ||hr + r - tr||L1/L2` is used, similar to TransE, but applied to the relation-specific discriminated embeddings.\n\n*   **Key Technical Contributions**\n    *   **Novel Mechanism**: Identified and leveraged the significance of inherent relevance/semantic sharing between relations, which was largely overlooked by previous discriminate models.\n    *   **Shared Gate Structure**: Proposed TransGate, a novel architecture based on LSTM's gate structure, to implement a shared discriminate mechanism, leading to a substantial reduction in discriminate parameters.\n    *   **Efficiency Innovation**: Introduced the `TransGate(wv)` variant, which reconstructs the gate with weight vectors to achieve comparable time complexity to indiscriminate models (like TransE) while maintaining expressivity, making it highly effective and scalable.\n    *   **Generalization**: Demonstrated that TransGate embraces TransE, suggesting it is a more general KGE framework.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on two standard KGE tasks: link prediction and triplet classification.\n    *   **Datasets**: Large-scale public knowledge graphs, namely Freebase and WordNet.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   TransGate not only outperforms state-of-the-art baselines (e.g., ConvE, R-GCN) in terms of accuracy but also achieves significant parameter reduction.\n        *   For example, TransGate outperforms ConvE and R-GCN with 6x and 17x fewer parameters, respectively.\n        *   The model is self-contained and does not require pre-training or extra hyperparameters to prevent overfitting, unlike many related models.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily highlights its strengths in overcoming prior limitations. For `TransGate(wv)`, it assumes that \"every dimension should be independent from each other in a well-trained embedding model\" to justify the use of weight vectors over matrices.\n    *   **Scope of Applicability**: TransGate is designed for and applicable to large-scale KGs, particularly for tasks like link prediction and triplet classification, where efficiency and scalability are critical. It is a shallow model without using additional information, which contributes to its scalability.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: TransGate advances the technical state-of-the-art by demonstrating that parameter sharing, implemented through a shared gate structure, can simultaneously enhance model expressivity and drastically reduce both parameter count and time complexity in KGE.\n    *   **Potential Impact on Future Research**: It introduces a new paradigm for designing KGE models that achieve a superior trade-off between complexity and expressivity. This could inspire future research into more sophisticated parameter sharing mechanisms, adaptive gating, and other techniques to build highly scalable and accurate KGE models for real-world applications, moving away from the traditional approach of learning independent parameters for each relation.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) is fundamental for Knowledge Graph Completion (KGC), yet state-of-the-art discriminate models face a critical dilemma: achieving high accuracy often demands prohibitive time complexity and massive parameter counts. This stems from their assumption of independent, relation-specific parameters, severely hindering scalability for real-world, large-scale Knowledge Graphs.\n\nWe introduce **TransGate**, a novel KGE model that revolutionizes relation-specific discrimination through an innovative **shared gate structure**. Inspired by LSTMs, TransGate leverages **parameter sharing** across all relations, exploiting inherent semantic commonalities. This groundbreaking approach drastically reduces the parameter count by up to 17x compared to leading models, while simultaneously achieving superior accuracy on tasks like **link prediction** and **triplet classification**. Crucially, our `TransGate(wv)` variant reconstructs the gate with weight vectors, achieving a **time complexity** comparable to simpler, indiscriminate models like TransE, making it exceptionally **scalable**. TransGate offers a superior trade-off between expressivity and efficiency, advancing the state-of-the-art by enabling robust and scalable KGE without complex feature engineering or pre-training. This paradigm shift paves the way for more efficient and effective KGC in diverse AI applications.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Knowledge Graph Completion (KGC)",
        "TransGate",
        "shared gate structure",
        "parameter sharing",
        "discriminate models",
        "high time complexity",
        "large parameter counts",
        "weight vectors",
        "efficiency and scalability",
        "link prediction",
        "triplet classification",
        "state-of-the-art performance",
        "reduced parameters"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/77dc07c92c37586f94a6f5ac3de103b218931578.pdf",
      "citation_key": "yuan2019",
      "metadata": {
        "title": "TransGate: Knowledge Graph Embedding with Shared Gate Structure",
        "authors": [
          "Jun Yuan",
          "Neng Gao",
          "Ji Xiang"
        ],
        "published_date": "2019",
        "abstract": "Embedding knowledge graphs (KGs) into continuous vector space is an essential problem in knowledge extraction. Current models continue to improve embedding by focusing on discriminating relation-specific information from entities with increasingly complex feature engineering. We noted that they ignored the inherent relevance between relations and tried to learn unique discriminate parameter set for each relation. Thus, these models potentially suffer from high time complexity and large parameters, preventing them from efficiently applying on real-world KGs. In this paper, we follow the thought of parameter sharing to simultaneously learn more expressive features, reduce parameters and avoid complex feature engineering. Based on gate structure from LSTM, we propose a novel model TransGate and develop shared discriminate mechanism, resulting in almost same space complexity as indiscriminate models. Furthermore, to develop a more effective and scalable model, we reconstruct the gate with weight vectors making our method has comparative time complexity against indiscriminate model. We conduct extensive experiments on link prediction and triplets classification. Experiments show that TransGate not only outperforms state-of-art baselines, but also reduces parameters greatly. For example, TransGate outperforms ConvE and RGCN with 6x and 17x fewer parameters, respectively. These results indicate that parameter sharing is a superior way to further optimize embedding and TransGate finds a better trade-off between complexity and expressivity.",
        "file_path": "paper_data/knowledge_graph_embedding/77dc07c92c37586f94a6f5ac3de103b218931578.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"TransGate: Knowledge Graph Embedding with Shared Gate Structure\" \\cite{yuan2019} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) models, particularly \"discriminate models\" that aim for higher accuracy, suffer from high time complexity and large parameter counts. This is primarily because they assume independence between relations and learn unique, relation-specific parameter sets, making them inefficient for real-world, large-scale Knowledge Graphs (KGs).\n    *   **Importance and Challenge**: KGs are crucial for many AI applications, but they are often incomplete. Knowledge Graph Completion (KGC) requires effective and scalable solutions. The challenge lies in developing models that can learn expressive, relation-specific features without incurring prohibitive computational costs and parameter sizes, which often lead to the need for complex feature engineering, more hyperparameters, and pre-training.\n\n*   **Related Work & Positioning**\n    *   **Indiscriminate Models (e.g., TransE, DistMult, ComplEx)**: These models are scalable due to limited parameters and low computational costs but often learn less expressive features, leading to lower accuracy.\n    *   **Discriminate Models (e.g., TransH, TransR, TransD, NTN, ConvE, R-GCN)**: These models aim to improve precision by discriminating relation-specific information.\n    *   **Limitations of Previous Solutions**:\n        *   **Large Parameters**: Discriminate models learn unique parameter sets for each relation, leading to massive parameter counts (e.g., TransD on Freebase can have >33GB parameters).\n        *   **High Time Complexity**: They often employ increasingly complex feature engineering, resulting in high computational costs.\n        *   **Overfitting & Hyperparameters**: Due to large parameter sizes and complex designs, they frequently require more hyperparameters and pre-training to prevent overfitting.\n    *   **Positioning**: \\cite{yuan2019} positions TransGate as a solution that addresses these limitations by introducing parameter sharing, aiming to simultaneously learn more expressive features, reduce parameters, and avoid complex feature engineering, thus finding a better trade-off between complexity and expressivity.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{yuan2019} proposes **TransGate**, a novel KGE model that leverages a **shared gate structure** (inspired by LSTM) to discriminate relation-specific information. Instead of learning unique parameters for each relation, TransGate uses only two shared gates for all relations.\n    *   **Novelty/Difference**:\n        *   **Parameter Sharing**: The key innovation is the concept of parameter sharing across relations. By using shared gates, TransGate exploits the inherent relevance and semantic sharing between relations, allowing it to learn expressive features with significantly fewer parameters.\n        *   **Adaptive Non-linear Discrimination**: The gate structure, composed of a sigmoid activation function and a Hadamard product, enables adaptive and non-linear filtering of entity embeddings based on both the entity and relation, generating relation-specific entity representations (`hr`, `tr`).\n        *   **Two Variants for Scalability**:\n            *   `TransGate(fc)`: Uses standard fully connected layers within the gates for precise discrimination.\n            *   `TransGate(wv)`: Reconstructs the gate with **weight vectors** instead of weight matrices. This crucial modification avoids matrix-vector multiplication operations, drastically reducing calculation and decreasing time complexity to the same order as indiscriminate models like TransE, making it highly scalable.\n        *   **Translation-based Scoring**: After discrimination, a translation-based score function `fr(h;t) = ||hr + r - tr||L1/L2` is used, similar to TransE, but applied to the relation-specific discriminated embeddings.\n\n*   **Key Technical Contributions**\n    *   **Novel Mechanism**: Identified and leveraged the significance of inherent relevance/semantic sharing between relations, which was largely overlooked by previous discriminate models.\n    *   **Shared Gate Structure**: Proposed TransGate, a novel architecture based on LSTM's gate structure, to implement a shared discriminate mechanism, leading to a substantial reduction in discriminate parameters.\n    *   **Efficiency Innovation**: Introduced the `TransGate(wv)` variant, which reconstructs the gate with weight vectors to achieve comparable time complexity to indiscriminate models (like TransE) while maintaining expressivity, making it highly effective and scalable.\n    *   **Generalization**: Demonstrated that TransGate embraces TransE, suggesting it is a more general KGE framework.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on two standard KGE tasks: link prediction and triplet classification.\n    *   **Datasets**: Large-scale public knowledge graphs, namely Freebase and WordNet.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   TransGate not only outperforms state-of-the-art baselines (e.g., ConvE, R-GCN) in terms of accuracy but also achieves significant parameter reduction.\n        *   For example, TransGate outperforms ConvE and R-GCN with 6x and 17x fewer parameters, respectively.\n        *   The model is self-contained and does not require pre-training or extra hyperparameters to prevent overfitting, unlike many related models.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily highlights its strengths in overcoming prior limitations. For `TransGate(wv)`, it assumes that \"every dimension should be independent from each other in a well-trained embedding model\" to justify the use of weight vectors over matrices.\n    *   **Scope of Applicability**: TransGate is designed for and applicable to large-scale KGs, particularly for tasks like link prediction and triplet classification, where efficiency and scalability are critical. It is a shallow model without using additional information, which contributes to its scalability.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: TransGate advances the technical state-of-the-art by demonstrating that parameter sharing, implemented through a shared gate structure, can simultaneously enhance model expressivity and drastically reduce both parameter count and time complexity in KGE.\n    *   **Potential Impact on Future Research**: It introduces a new paradigm for designing KGE models that achieve a superior trade-off between complexity and expressivity. This could inspire future research into more sophisticated parameter sharing mechanisms, adaptive gating, and other techniques to build highly scalable and accurate KGE models for real-world applications, moving away from the traditional approach of learning independent parameters for each relation.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Knowledge Graph Completion (KGC)",
          "TransGate",
          "shared gate structure",
          "parameter sharing",
          "discriminate models",
          "high time complexity",
          "large parameter counts",
          "weight vectors",
          "efficiency and scalability",
          "link prediction",
          "triplet classification",
          "state-of-the-art performance",
          "reduced parameters"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "77dc07c92c37586f94a6f5ac3de103b218931578.pdf"
    },
    {
      "success": true,
      "doc_id": "8ffe3ddd354fb3fd5ad2e00c3ca5d203",
      "summary": "Here's a focused summary of the paper \"TransA: An Adaptive Approach for Knowledge Graph Embedding\" by `\\cite{xiao2015}` for a literature review:\n\n### Analysis of `\\cite{xiao2015}`: TransA: An Adaptive Approach for Knowledge Graph Embedding\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: Existing translation-based knowledge graph embedding methods (e.g., TransE, TransH, TransR) suffer from an oversimplified loss metric, typically Euclidean distance. This leads to:\n        *   **Inflexible spherical equipotential surfaces**: Incompetent to model diverse and complex relation topologies (e.g., one-to-many, many-to-one, many-to-many relations), making it difficult to distinguish plausible from implausible triples.\n        *   **Identical treatment of dimensions**: Each embedding dimension is treated equally, leading to noise from unrelated dimensions degrading performance.\n    *   **Motivation**: To develop a more flexible and adaptive embedding method that can effectively model various and complex entities/relations in knowledge bases by addressing the limitations of the oversimplified loss metric.\n\n2.  **Related Work & Positioning**\n    *   `\\cite{xiao2015}` builds upon the successful **translation-based embedding methods** (e.g., TransE, TransH, TransR) which follow the principle `h + r  t`.\n    *   **Limitations of previous translation-based solutions**: While these methods differ in how they project entities into relation-specific spaces (e.g., hyperplanes in TransH, matrices in TransR), they all apply the same oversimplified Euclidean distance metric `||h_r + r - t_r||^2_2`. This fundamental flaw makes them \"incompetent to model various and complex entities/relations\" and unable to suppress noise from irrelevant dimensions.\n    *   **Positioning**: `\\cite{xiao2015}` introduces an **adaptive metric** to the translation-based paradigm, directly addressing the metric's inflexibility, which is a core limitation not fully resolved by previous projection-based translation models. It also differentiates from TransM by learning weights adaptively from data and applying feature transformation.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{xiao2015}` proposes **TransA**, which replaces the inflexible Euclidean distance with an **adaptive Mahalanobis distance of absolute loss**.\n        *   The score function is defined as `fr(h,t) = (|h+r-t|)^T * Wr * (|h+r-t|)`.\n        *   `|h+r-t|` represents the element-wise absolute value of the translation loss vector.\n        *   `Wr` is a **relation-specific symmetric non-negative weight matrix** that captures the adaptive metric.\n    *   **Novelty/Difference**:\n        *   **Adaptive Metric**: Unlike previous methods using a fixed Euclidean metric, TransA learns a relation-specific adaptive metric `Wr` from the data.\n        *   **Elliptical Equipotential Surfaces**: By using Mahalanobis distance, TransA allows for elliptical equipotential hyper-surfaces instead of spherical ones, providing greater flexibility to characterize complex embedding topologies induced by complex relations (e.g., one-to-many).\n        *   **Feature Weighting**: The `Wr` matrix implicitly weights different dimensions of the loss vector, effectively suppressing noise from unrelated dimensions and highlighting relevant ones. This is achieved through LDL decomposition of `Wr`, where `Dr` becomes a diagonal matrix of weights.\n        *   **Absolute Operator**: The use of the absolute operator `|...|` is critical for ensuring the score function forms a well-defined norm under the non-negative condition of `Wr` entries and for correctly measuring absolute loss, preventing undesired reductions in overall loss due to negative components.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of TransA, an adaptive metric approach for knowledge graph embedding.\n    *   **Adaptive Metric Learning**: Proposes learning a relation-specific symmetric non-negative weight matrix `Wr` to adapt the distance metric to different relations.\n    *   **Enhanced Representation**: Enables the modeling of complex relation topologies (one-to-many, many-to-one, many-to-many) through flexible elliptical equipotential surfaces.\n    *   **Noise Suppression**: Achieves effective noise suppression by weighting transformed feature dimensions, allowing the model to focus on relevant dimensions for each relation.\n    *   **Theoretical Justification**: Provides geometric explanations (elliptical surfaces) and algebraic interpretations (feature weighting) for the adaptive metric.\n    *   **Efficient Training**: The weight matrix `Wr` has a closed-form solution during training, contributing to computational efficiency.\n\n5.  **Experimental Validation**\n    *   **Tasks**: Evaluated on two benchmark tasks: **link prediction** and **triples classification** (though only link prediction results are detailed in the provided text).\n    *   **Datasets**: Conducted experiments on four public datasets, subsets of Wordnet and Freebase: **WN18** and **FB15K** for link prediction, and WN11 and FB13 for triple classification.\n    *   **Metrics**:\n        *   **Link Prediction**: Averaged rank (Mean Rank) and HITS@10 (proportion of correct triples ranked within the top 10). Both \"Raw\" and \"Filter\" settings were used, with \"Filter\" being preferred to exclude existing corrupted triples.\n    *   **Key Results & Comparison**:\n        *   **Significant and Consistent Improvements**: TransA consistently and significantly outperforms all state-of-the-art baselines, including SE, SME, LFM, TransE, TransH, and TransR, on both WN18 and FB15K datasets for link prediction.\n        *   **Example (FB15K Filtered)**: TransA achieved a Mean Rank of 74 and HITS@10 of 80.4%, compared to TransR's 77 Mean Rank and 68.7% HITS@10, demonstrating substantial gains.\n        *   The results validate the effectiveness of TransA, particularly in handling complex knowledge graph structures (datasets with higher ATPE - Averaged Triple number Per Entity).\n\n6.  **Limitations & Scope**\n    *   **Technical Assumptions**: The primary assumption is that a relation-specific adaptive metric, specifically Mahalanobis distance with a non-negative weight matrix, is superior to a fixed Euclidean metric for capturing complex relation topologies.\n    *   **Non-negative Condition for `Wr`**: While the paper argues that the non-negative condition for `Wr` entries is \"easy-to-achieve\" and generalizes common metric learning forms, it's a specific constraint on the weight matrix, potentially different from a positive semi-definite (PSD) constraint often seen in Mahalanobis metric learning. The \"Adaptive Metric (PSD)\" baseline suggests this choice was deliberate.\n    *   **Scope of Applicability**: Primarily focused on knowledge graph embedding for tasks like link prediction and triple classification. The benefits are most pronounced for knowledge bases with \"various and complex entities/relations.\"\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: `\\cite{xiao2015}` significantly advances the technical state-of-the-art in knowledge graph embedding by introducing a more flexible and adaptive metric learning approach within the translation-based paradigm.\n    *   **Improved Modeling of Complex Relations**: It provides a robust solution for modeling complex relation topologies (one-to-many, many-to-one, many-to-many) that previous methods struggled with due to their oversimplified metrics.\n    *   **Enhanced Robustness**: The ability to weight embedding dimensions and suppress noise makes the embeddings more robust and discriminative.\n    *   **Potential Impact**: TransA's adaptive metric learning paradigm could inspire future research in developing more sophisticated and context-aware distance metrics for various representation learning tasks beyond knowledge graphs, especially where data exhibits complex, non-uniform structures.",
      "intriguing_abstract": "The Achilles' heel of translation-based **knowledge graph embedding (KGE)** methods lies in their rigid Euclidean distance metric, which imposes inflexible spherical equipotential surfaces and treats all embedding dimensions equally. This fundamental limitation severely hinders their ability to model diverse and **complex relation topologies** (e.g., one-to-many, many-to-many) and effectively suppress noise.\n\nWe introduce **TransA**, a novel and adaptive approach that revolutionizes KGE by replacing the oversimplified Euclidean distance with an **adaptive Mahalanobis distance of absolute loss**. TransA learns a relation-specific symmetric non-negative weight matrix, enabling the formation of flexible **elliptical equipotential surfaces** that precisely capture intricate relation patterns. This **adaptive metric** inherently performs **feature weighting**, dynamically suppressing noise from irrelevant dimensions and enhancing discriminative power. Extensive experiments on benchmark datasets (WN18, FB15K) demonstrate TransA's significant and consistent superiority in **link prediction** and **triple classification**, consistently outperforming state-of-the-art baselines. TransA represents a critical advancement, offering a robust framework for modeling complex knowledge bases and paving the way for more nuanced representation learning.",
      "keywords": [
        "TransA",
        "Knowledge Graph Embedding",
        "Adaptive Metric Learning",
        "Mahalanobis Distance",
        "Relation-Specific Weight Matrix",
        "Complex Relation Topologies",
        "Elliptical Equipotential Surfaces",
        "Noise Suppression",
        "Translation-Based Embedding Methods",
        "Link Prediction",
        "Triples Classification",
        "Oversimplified Loss Metric",
        "Absolute Loss Operator",
        "Outperforms State-of-the-Art"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/d1a525c16a53b94200029df1037f2c9c7c244d7b.pdf",
      "citation_key": "xiao2015",
      "metadata": {
        "title": "TransA: An Adaptive Approach for Knowledge Graph Embedding",
        "authors": [
          "Han Xiao",
          "Minlie Huang",
          "Yu Hao",
          "Xiaoyan Zhu"
        ],
        "published_date": "2015",
        "abstract": "Knowledge representation is a major topic in AI, and many studies attempt to represent entities and relations of knowledge base in a continuous vector space. Among these attempts, translation-based methods build entity and relation vectors by minimizing the translation loss from a head entity to a tail one. In spite of the success of these methods, translation-based methods also suffer from the oversimplified loss metric, and are not competitive enough to model various and complex entities/relations in knowledge bases. To address this issue, we propose \\textbf{TransA}, an adaptive metric approach for embedding, utilizing the metric learning ideas to provide a more flexible embedding method. Experiments are conducted on the benchmark datasets and our proposed method makes significant and consistent improvements over the state-of-the-art baselines.",
        "file_path": "paper_data/knowledge_graph_embedding/d1a525c16a53b94200029df1037f2c9c7c244d7b.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"TransA: An Adaptive Approach for Knowledge Graph Embedding\" by `\\cite{xiao2015}` for a literature review:\n\n### Analysis of `\\cite{xiao2015}`: TransA: An Adaptive Approach for Knowledge Graph Embedding\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: Existing translation-based knowledge graph embedding methods (e.g., TransE, TransH, TransR) suffer from an oversimplified loss metric, typically Euclidean distance. This leads to:\n        *   **Inflexible spherical equipotential surfaces**: Incompetent to model diverse and complex relation topologies (e.g., one-to-many, many-to-one, many-to-many relations), making it difficult to distinguish plausible from implausible triples.\n        *   **Identical treatment of dimensions**: Each embedding dimension is treated equally, leading to noise from unrelated dimensions degrading performance.\n    *   **Motivation**: To develop a more flexible and adaptive embedding method that can effectively model various and complex entities/relations in knowledge bases by addressing the limitations of the oversimplified loss metric.\n\n2.  **Related Work & Positioning**\n    *   `\\cite{xiao2015}` builds upon the successful **translation-based embedding methods** (e.g., TransE, TransH, TransR) which follow the principle `h + r  t`.\n    *   **Limitations of previous translation-based solutions**: While these methods differ in how they project entities into relation-specific spaces (e.g., hyperplanes in TransH, matrices in TransR), they all apply the same oversimplified Euclidean distance metric `||h_r + r - t_r||^2_2`. This fundamental flaw makes them \"incompetent to model various and complex entities/relations\" and unable to suppress noise from irrelevant dimensions.\n    *   **Positioning**: `\\cite{xiao2015}` introduces an **adaptive metric** to the translation-based paradigm, directly addressing the metric's inflexibility, which is a core limitation not fully resolved by previous projection-based translation models. It also differentiates from TransM by learning weights adaptively from data and applying feature transformation.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{xiao2015}` proposes **TransA**, which replaces the inflexible Euclidean distance with an **adaptive Mahalanobis distance of absolute loss**.\n        *   The score function is defined as `fr(h,t) = (|h+r-t|)^T * Wr * (|h+r-t|)`.\n        *   `|h+r-t|` represents the element-wise absolute value of the translation loss vector.\n        *   `Wr` is a **relation-specific symmetric non-negative weight matrix** that captures the adaptive metric.\n    *   **Novelty/Difference**:\n        *   **Adaptive Metric**: Unlike previous methods using a fixed Euclidean metric, TransA learns a relation-specific adaptive metric `Wr` from the data.\n        *   **Elliptical Equipotential Surfaces**: By using Mahalanobis distance, TransA allows for elliptical equipotential hyper-surfaces instead of spherical ones, providing greater flexibility to characterize complex embedding topologies induced by complex relations (e.g., one-to-many).\n        *   **Feature Weighting**: The `Wr` matrix implicitly weights different dimensions of the loss vector, effectively suppressing noise from unrelated dimensions and highlighting relevant ones. This is achieved through LDL decomposition of `Wr`, where `Dr` becomes a diagonal matrix of weights.\n        *   **Absolute Operator**: The use of the absolute operator `|...|` is critical for ensuring the score function forms a well-defined norm under the non-negative condition of `Wr` entries and for correctly measuring absolute loss, preventing undesired reductions in overall loss due to negative components.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of TransA, an adaptive metric approach for knowledge graph embedding.\n    *   **Adaptive Metric Learning**: Proposes learning a relation-specific symmetric non-negative weight matrix `Wr` to adapt the distance metric to different relations.\n    *   **Enhanced Representation**: Enables the modeling of complex relation topologies (one-to-many, many-to-one, many-to-many) through flexible elliptical equipotential surfaces.\n    *   **Noise Suppression**: Achieves effective noise suppression by weighting transformed feature dimensions, allowing the model to focus on relevant dimensions for each relation.\n    *   **Theoretical Justification**: Provides geometric explanations (elliptical surfaces) and algebraic interpretations (feature weighting) for the adaptive metric.\n    *   **Efficient Training**: The weight matrix `Wr` has a closed-form solution during training, contributing to computational efficiency.\n\n5.  **Experimental Validation**\n    *   **Tasks**: Evaluated on two benchmark tasks: **link prediction** and **triples classification** (though only link prediction results are detailed in the provided text).\n    *   **Datasets**: Conducted experiments on four public datasets, subsets of Wordnet and Freebase: **WN18** and **FB15K** for link prediction, and WN11 and FB13 for triple classification.\n    *   **Metrics**:\n        *   **Link Prediction**: Averaged rank (Mean Rank) and HITS@10 (proportion of correct triples ranked within the top 10). Both \"Raw\" and \"Filter\" settings were used, with \"Filter\" being preferred to exclude existing corrupted triples.\n    *   **Key Results & Comparison**:\n        *   **Significant and Consistent Improvements**: TransA consistently and significantly outperforms all state-of-the-art baselines, including SE, SME, LFM, TransE, TransH, and TransR, on both WN18 and FB15K datasets for link prediction.\n        *   **Example (FB15K Filtered)**: TransA achieved a Mean Rank of 74 and HITS@10 of 80.4%, compared to TransR's 77 Mean Rank and 68.7% HITS@10, demonstrating substantial gains.\n        *   The results validate the effectiveness of TransA, particularly in handling complex knowledge graph structures (datasets with higher ATPE - Averaged Triple number Per Entity).\n\n6.  **Limitations & Scope**\n    *   **Technical Assumptions**: The primary assumption is that a relation-specific adaptive metric, specifically Mahalanobis distance with a non-negative weight matrix, is superior to a fixed Euclidean metric for capturing complex relation topologies.\n    *   **Non-negative Condition for `Wr`**: While the paper argues that the non-negative condition for `Wr` entries is \"easy-to-achieve\" and generalizes common metric learning forms, it's a specific constraint on the weight matrix, potentially different from a positive semi-definite (PSD) constraint often seen in Mahalanobis metric learning. The \"Adaptive Metric (PSD)\" baseline suggests this choice was deliberate.\n    *   **Scope of Applicability**: Primarily focused on knowledge graph embedding for tasks like link prediction and triple classification. The benefits are most pronounced for knowledge bases with \"various and complex entities/relations.\"\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: `\\cite{xiao2015}` significantly advances the technical state-of-the-art in knowledge graph embedding by introducing a more flexible and adaptive metric learning approach within the translation-based paradigm.\n    *   **Improved Modeling of Complex Relations**: It provides a robust solution for modeling complex relation topologies (one-to-many, many-to-one, many-to-many) that previous methods struggled with due to their oversimplified metrics.\n    *   **Enhanced Robustness**: The ability to weight embedding dimensions and suppress noise makes the embeddings more robust and discriminative.\n    *   **Potential Impact**: TransA's adaptive metric learning paradigm could inspire future research in developing more sophisticated and context-aware distance metrics for various representation learning tasks beyond knowledge graphs, especially where data exhibits complex, non-uniform structures.",
        "keywords": [
          "TransA",
          "Knowledge Graph Embedding",
          "Adaptive Metric Learning",
          "Mahalanobis Distance",
          "Relation-Specific Weight Matrix",
          "Complex Relation Topologies",
          "Elliptical Equipotential Surfaces",
          "Noise Suppression",
          "Translation-Based Embedding Methods",
          "Link Prediction",
          "Triples Classification",
          "Oversimplified Loss Metric",
          "Absolute Loss Operator",
          "Outperforms State-of-the-Art"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "d1a525c16a53b94200029df1037f2c9c7c244d7b.pdf"
    },
    {
      "success": true,
      "doc_id": "2a50b36977bd7d624278ba8e68e0587f",
      "summary": "Here is a focused summary of the paper \"ROTAT E: K NOWLEDGE GRAPH EMBEDDING BY RELA-TIONAL ROTATION IN COMPLEX SPACE\" by Sun et al. \\cite{sun2018} for a literature review:\n\n---\n\n### Analysis of \"ROTAT E: K NOWLEDGE GRAPH EMBEDDING BY RELA-TIONAL ROTATION IN COMPLEX SPACE\" \\cite{sun2018}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Learning effective low-dimensional representations (embeddings) of entities and relations in knowledge graphs to predict missing links.\n    *   **Importance and Challenge**: The accuracy of missing link prediction heavily relies on the ability to model and infer various fundamental relation patterns, including symmetry/antisymmetry, inversion, and composition. Existing knowledge graph embedding (KGE) models struggle to capture all these patterns simultaneously.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: This work builds upon the extensive research in knowledge graph embedding, which typically defines a score function for triplets (h, r, t).\n    *   **Limitations of previous solutions**:\n        *   Models like TransE \\cite{sun2018} can model inversion and composition but fail to represent symmetric relations effectively.\n        *   DistMult \\cite{sun2018} and ComplEx \\cite{sun2018} (which extends DistMult) can model symmetric/antisymmetric relations and inversion, but ComplEx cannot infer composition patterns.\n        *   No single existing state-of-the-art model (e.g., TransE, TransX, DistMult, ComplEx, HolE, ConvE) is capable of modeling and inferring *all three* crucial relation patterns (symmetry/antisymmetry, inversion, and composition) simultaneously.\n        *   Concurrent work like TorusE \\cite{sun2018} is a special case of RotatE with fixed embedding moduli, limiting its representation capacity, especially for composition patterns.\n        *   Relational path approaches are often less scalable and do not provide meaningful entity/relation embeddings.\n        *   Previous negative sampling techniques (e.g., GAN-based) are computationally expensive and difficult to optimize.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: RotatE maps entities and relations to a complex vector space. It defines each relation `r` as an element-wise rotation from the head entity `h` to the tail entity `t`.\n        *   For a given triplet `(h, r, t)`, the model expects `t = h * r`, where `h, r, t` are complex embeddings, `*` denotes the Hadamard (element-wise) product, and the modulus of each element of `r` is constrained to `|r_i| = 1`.\n        *   The distance function for a triplet is `dr(h,t) = ||h * r - t||`.\n    *   **Novelty**:\n        *   **Relational Rotation in Complex Space**: Inspired by Euler's identity, representing relations as rotations in complex space provides an elegant and unified mechanism to inherently model symmetry/antisymmetry, inversion, and composition patterns simultaneously, a capability lacking in prior models.\n        *   **Self-Adversarial Negative Sampling**: A novel training technique that samples negative triples based on the current embedding model's scores, making the negative samples more informative and challenging. This approach is more efficient than uniform sampling and avoids the complexities of adversarial training frameworks.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of RotatE, a knowledge graph embedding model that defines relations as element-wise rotations in complex vector space, demonstrably capable of modeling and inferring symmetry/antisymmetry, inversion, and composition patterns.\n    *   **Novel Training Technique**: Proposal of self-adversarial negative sampling, an efficient and effective method for generating informative negative samples during KGE model training.\n    *   **Theoretical Insights**: Mathematical proofs (provided in the appendix) demonstrating RotatE's inherent ability to capture all three key relation patterns.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: Link prediction tasks on four widely used benchmark knowledge graphs.\n    *   **Datasets**: FB15k, WN18 (emphasizing symmetry/antisymmetry and inversion), FB15k-237, and WN18RR (emphasizing symmetry/antisymmetry and composition due to removed inverse relations). Also tested on the \"Countries\" dataset, specifically designed for composition pattern inference.\n    *   **Key performance metrics**: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@N (H@1, H@3, H@10).\n    *   **Comparison results**:\n        *   RotatE significantly outperforms existing state-of-the-art models (TransE, DistMult, ComplEx, HolE, ConvE) across all four benchmark datasets.\n        *   It achieves state-of-the-art performance on all benchmarks, including the \"Countries\" dataset, demonstrating its superior ability to infer composition patterns.\n        *   A variant, pRotatE (which constrains entity embedding moduli), performs similarly on datasets dominated by inversion but shows a larger performance gap on datasets emphasizing composition, highlighting the importance of the full complex space representation for complex patterns.\n        *   The model is shown to be scalable to large knowledge graphs.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions**: The paper primarily focuses on the strengths of RotatE in overcoming previous limitations. While a variant (pRotatE) with constrained entity moduli shows slightly reduced performance on composition-heavy datasets, the core RotatE model itself does not present explicit technical limitations within the paper's scope.\n    *   **Scope of applicability**: RotatE is designed for link prediction in knowledge graphs. The proposed self-adversarial negative sampling technique is general and can be applied to other distance-based KGE models.\n\n7.  **Technical Significance**\n    *   **Advances state-of-the-art**: RotatE is presented as the first model to achieve state-of-the-art performance across benchmarks that require modeling and inferring *all three* major relation patterns (symmetry/antisymmetry, inversion, and composition) simultaneously.\n    *   **Potential impact**: Offers a novel, unified, and mathematically elegant framework for understanding and modeling diverse relational patterns in knowledge graphs. The self-adversarial negative sampling technique provides a more efficient and effective training paradigm. This work could inspire future research into complex-space embeddings and geometric transformations for relational learning.",
      "intriguing_abstract": "Knowledge graph embedding (KGE) models are crucial for predicting missing links, yet a fundamental challenge persists: simultaneously capturing diverse relational patterns like symmetry, inversion, and composition. We introduce RotatE, a novel KGE model that elegantly addresses this by mapping entities and relations into a complex vector space. In RotatE, each relation is defined as an element-wise rotation from the head to the tail entity, where the modulus of relation elements is constrained to one. This innovative geometric interpretation inherently and simultaneously models all three critical relational patterns, a capability previously lacking in state-of-the-art models. Furthermore, we propose a self-adversarial negative sampling technique, significantly enhancing training efficiency and effectiveness. Extensive experiments on benchmark datasets including FB15k, WN18, and Countries demonstrate that RotatE achieves new state-of-the-art performance, outperforming existing methods by a substantial margin. RotatE offers a unified, mathematically elegant framework that advances the understanding and modeling of complex relational structures, paving the way for more robust and intelligent knowledge systems.",
      "keywords": [
        "RotatE model",
        "knowledge graph embedding",
        "relational rotation",
        "complex vector space",
        "symmetry/antisymmetry",
        "inversion",
        "composition",
        "self-adversarial negative sampling",
        "link prediction",
        "unified relation pattern modeling",
        "state-of-the-art performance",
        "low-dimensional representations",
        "Hadamard product"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/8f096071a09701012c9c279aee2a88143a295935.pdf",
      "citation_key": "sun2018",
      "metadata": {
        "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
        "authors": [
          "Zhiqing Sun",
          "Zhihong Deng",
          "Jian-Yun Nie",
          "Jian Tang"
        ],
        "published_date": "2018",
        "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",
        "file_path": "paper_data/knowledge_graph_embedding/8f096071a09701012c9c279aee2a88143a295935.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the paper \"ROTAT E: K NOWLEDGE GRAPH EMBEDDING BY RELA-TIONAL ROTATION IN COMPLEX SPACE\" by Sun et al. \\cite{sun2018} for a literature review:\n\n---\n\n### Analysis of \"ROTAT E: K NOWLEDGE GRAPH EMBEDDING BY RELA-TIONAL ROTATION IN COMPLEX SPACE\" \\cite{sun2018}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Learning effective low-dimensional representations (embeddings) of entities and relations in knowledge graphs to predict missing links.\n    *   **Importance and Challenge**: The accuracy of missing link prediction heavily relies on the ability to model and infer various fundamental relation patterns, including symmetry/antisymmetry, inversion, and composition. Existing knowledge graph embedding (KGE) models struggle to capture all these patterns simultaneously.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: This work builds upon the extensive research in knowledge graph embedding, which typically defines a score function for triplets (h, r, t).\n    *   **Limitations of previous solutions**:\n        *   Models like TransE \\cite{sun2018} can model inversion and composition but fail to represent symmetric relations effectively.\n        *   DistMult \\cite{sun2018} and ComplEx \\cite{sun2018} (which extends DistMult) can model symmetric/antisymmetric relations and inversion, but ComplEx cannot infer composition patterns.\n        *   No single existing state-of-the-art model (e.g., TransE, TransX, DistMult, ComplEx, HolE, ConvE) is capable of modeling and inferring *all three* crucial relation patterns (symmetry/antisymmetry, inversion, and composition) simultaneously.\n        *   Concurrent work like TorusE \\cite{sun2018} is a special case of RotatE with fixed embedding moduli, limiting its representation capacity, especially for composition patterns.\n        *   Relational path approaches are often less scalable and do not provide meaningful entity/relation embeddings.\n        *   Previous negative sampling techniques (e.g., GAN-based) are computationally expensive and difficult to optimize.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: RotatE maps entities and relations to a complex vector space. It defines each relation `r` as an element-wise rotation from the head entity `h` to the tail entity `t`.\n        *   For a given triplet `(h, r, t)`, the model expects `t = h * r`, where `h, r, t` are complex embeddings, `*` denotes the Hadamard (element-wise) product, and the modulus of each element of `r` is constrained to `|r_i| = 1`.\n        *   The distance function for a triplet is `dr(h,t) = ||h * r - t||`.\n    *   **Novelty**:\n        *   **Relational Rotation in Complex Space**: Inspired by Euler's identity, representing relations as rotations in complex space provides an elegant and unified mechanism to inherently model symmetry/antisymmetry, inversion, and composition patterns simultaneously, a capability lacking in prior models.\n        *   **Self-Adversarial Negative Sampling**: A novel training technique that samples negative triples based on the current embedding model's scores, making the negative samples more informative and challenging. This approach is more efficient than uniform sampling and avoids the complexities of adversarial training frameworks.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of RotatE, a knowledge graph embedding model that defines relations as element-wise rotations in complex vector space, demonstrably capable of modeling and inferring symmetry/antisymmetry, inversion, and composition patterns.\n    *   **Novel Training Technique**: Proposal of self-adversarial negative sampling, an efficient and effective method for generating informative negative samples during KGE model training.\n    *   **Theoretical Insights**: Mathematical proofs (provided in the appendix) demonstrating RotatE's inherent ability to capture all three key relation patterns.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: Link prediction tasks on four widely used benchmark knowledge graphs.\n    *   **Datasets**: FB15k, WN18 (emphasizing symmetry/antisymmetry and inversion), FB15k-237, and WN18RR (emphasizing symmetry/antisymmetry and composition due to removed inverse relations). Also tested on the \"Countries\" dataset, specifically designed for composition pattern inference.\n    *   **Key performance metrics**: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@N (H@1, H@3, H@10).\n    *   **Comparison results**:\n        *   RotatE significantly outperforms existing state-of-the-art models (TransE, DistMult, ComplEx, HolE, ConvE) across all four benchmark datasets.\n        *   It achieves state-of-the-art performance on all benchmarks, including the \"Countries\" dataset, demonstrating its superior ability to infer composition patterns.\n        *   A variant, pRotatE (which constrains entity embedding moduli), performs similarly on datasets dominated by inversion but shows a larger performance gap on datasets emphasizing composition, highlighting the importance of the full complex space representation for complex patterns.\n        *   The model is shown to be scalable to large knowledge graphs.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions**: The paper primarily focuses on the strengths of RotatE in overcoming previous limitations. While a variant (pRotatE) with constrained entity moduli shows slightly reduced performance on composition-heavy datasets, the core RotatE model itself does not present explicit technical limitations within the paper's scope.\n    *   **Scope of applicability**: RotatE is designed for link prediction in knowledge graphs. The proposed self-adversarial negative sampling technique is general and can be applied to other distance-based KGE models.\n\n7.  **Technical Significance**\n    *   **Advances state-of-the-art**: RotatE is presented as the first model to achieve state-of-the-art performance across benchmarks that require modeling and inferring *all three* major relation patterns (symmetry/antisymmetry, inversion, and composition) simultaneously.\n    *   **Potential impact**: Offers a novel, unified, and mathematically elegant framework for understanding and modeling diverse relational patterns in knowledge graphs. The self-adversarial negative sampling technique provides a more efficient and effective training paradigm. This work could inspire future research into complex-space embeddings and geometric transformations for relational learning.",
        "keywords": [
          "RotatE model",
          "knowledge graph embedding",
          "relational rotation",
          "complex vector space",
          "symmetry/antisymmetry",
          "inversion",
          "composition",
          "self-adversarial negative sampling",
          "link prediction",
          "unified relation pattern modeling",
          "state-of-the-art performance",
          "low-dimensional representations",
          "Hadamard product"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "8f096071a09701012c9c279aee2a88143a295935.pdf"
    },
    {
      "success": true,
      "doc_id": "f036ae7d449a09c9ecc8b4db909bcee0",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n1.  **Research Problem & Motivation** \\cite{ji2015}\n    *   **Problem**: Knowledge graphs (KGs) are incomplete, limiting their utility in various AI applications.\n    *   **Motivation**: Addressing KG incompleteness is crucial for enhancing the performance of AI systems that rely on these valuable resources.\n\n2.  **Related Work & Positioning** \\cite{ji2015}\n    *   **Related Work**: Builds upon previous translational models like TransE, TransH, and TransR/CTransR, which model relations as translations between head and tail entities.\n    *   **Limitations of Previous Solutions**: While CTransR achieved state-of-the-art performance, it primarily focused on relation diversity. Previous models did not adequately capture the diversity of *both* entities and relations in a fine-grained manner, and some might have higher computational complexity (e.g., matrix-vector multiplications).\n\n3.  **Technical Approach & Innovation** \\cite{ji2015}\n    *   **Core Technical Method**: Proposes TransD, a \"more fine-grained model\" that improves upon TransR/CTransR.\n    *   **Novelty**: TransD represents each named symbol object (entity and relation) using *two* distinct vectors:\n        *   One vector represents the intrinsic meaning of the entity or relation.\n        *   The second vector is used to dynamically construct a mapping matrix for that specific entity or relation. This dynamic construction allows for a more flexible and fine-grained projection.\n\n4.  **Key Technical Contributions** \\cite{ji2015}\n    *   **Novel Algorithm**: Introduces TransD, a novel knowledge graph embedding model that considers the diversity of *both* relations and entities, unlike prior models that primarily focused on relation diversity.\n    *   **Architectural Innovation**: Employs a dual-vector representation for entities and relations, enabling dynamic construction of mapping matrices without explicit matrix-vector multiplication.\n    *   **Efficiency**: Achieves reduced parameter count and avoids computationally intensive matrix-vector multiplication operations, making it more scalable for large-scale knowledge graphs.\n\n5.  **Experimental Validation** \\cite{ji2015}\n    *   **Experiments Conducted**: Evaluated the model on two standard tasks for knowledge graph embedding:\n        *   Triplet Classification\n        *   Link Prediction\n    *   **Key Performance Metrics & Results**: The evaluation results demonstrate that TransD consistently outperforms existing state-of-the-art methods on these tasks.\n\n6.  **Limitations & Scope** \\cite{ji2015}\n    *   **Scope of Applicability**: Primarily designed for large-scale knowledge graphs due to its efficiency and reduced parameter count.\n    *   **Implicit Limitations**: While it improves upon previous translational models, it still operates within the translational embedding paradigm. The specific details of how the dynamic mapping matrix is constructed and its theoretical properties are not fully detailed in the provided abstract.\n\n7.  **Technical Significance** \\cite{ji2015}\n    *   **Advancement of State-of-the-Art**: TransD significantly advances the technical state-of-the-art in knowledge graph embedding by achieving superior performance on key tasks while offering improved efficiency and scalability.\n    *   **Potential Impact**: Its ability to model the diversity of both entities and relations in a fine-grained and efficient manner could influence future research in scalable and accurate knowledge graph completion and representation learning, particularly for very large and complex KGs.",
      "intriguing_abstract": "The pervasive incompleteness of knowledge graphs (KGs) remains a critical bottleneck, hindering the full potential of AI applications. While prior translational models like TransE and TransR advanced KG embedding by addressing relation diversity, they often overlooked the fine-grained diversity of *both* entities and relations, or incurred significant computational overhead.\n\nWe introduce **TransD**, a novel knowledge graph embedding model that fundamentally rethinks how entities and relations are represented. Our core innovation lies in a dual-vector representation for each symbol: one capturing intrinsic meaning, and another dynamically constructing a specific mapping matrix. This architectural breakthrough enables TransD to capture the rich, fine-grained diversity of *both* entities and relations with unprecedented flexibility. Crucially, this dynamic projection avoids computationally intensive matrix-vector multiplications, leading to significantly reduced parameter counts and superior scalability for large-scale KGs.\n\nExtensive experiments on standard link prediction and triplet classification benchmarks demonstrate that TransD consistently outperforms existing state-of-the-art methods. TransD not only pushes the boundaries of KG completion accuracy but also offers a highly efficient and scalable paradigm, paving the way for more robust and intelligent AI systems reliant on comprehensive knowledge.",
      "keywords": [
        "Knowledge graphs (KGs)",
        "KG incompleteness",
        "knowledge graph embedding",
        "TransD",
        "translational models",
        "entity and relation diversity",
        "dual-vector representation",
        "dynamic mapping matrix",
        "computational efficiency",
        "scalability",
        "triplet classification",
        "link prediction",
        "state-of-the-art performance",
        "AI applications"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/18bd7cd489874ed9976b4f87a6a558f9533316e0.pdf",
      "citation_key": "ji2015",
      "metadata": {
        "title": "Knowledge Graph Embedding via Dynamic Mapping Matrix",
        "authors": [
          "Guoliang Ji",
          "Shizhu He",
          "Liheng Xu",
          "Kang Liu",
          "Jun Zhao"
        ],
        "published_date": "2015",
        "abstract": "Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms state-of-the-art methods.",
        "file_path": "paper_data/knowledge_graph_embedding/18bd7cd489874ed9976b4f87a6a558f9533316e0.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n1.  **Research Problem & Motivation** \\cite{ji2015}\n    *   **Problem**: Knowledge graphs (KGs) are incomplete, limiting their utility in various AI applications.\n    *   **Motivation**: Addressing KG incompleteness is crucial for enhancing the performance of AI systems that rely on these valuable resources.\n\n2.  **Related Work & Positioning** \\cite{ji2015}\n    *   **Related Work**: Builds upon previous translational models like TransE, TransH, and TransR/CTransR, which model relations as translations between head and tail entities.\n    *   **Limitations of Previous Solutions**: While CTransR achieved state-of-the-art performance, it primarily focused on relation diversity. Previous models did not adequately capture the diversity of *both* entities and relations in a fine-grained manner, and some might have higher computational complexity (e.g., matrix-vector multiplications).\n\n3.  **Technical Approach & Innovation** \\cite{ji2015}\n    *   **Core Technical Method**: Proposes TransD, a \"more fine-grained model\" that improves upon TransR/CTransR.\n    *   **Novelty**: TransD represents each named symbol object (entity and relation) using *two* distinct vectors:\n        *   One vector represents the intrinsic meaning of the entity or relation.\n        *   The second vector is used to dynamically construct a mapping matrix for that specific entity or relation. This dynamic construction allows for a more flexible and fine-grained projection.\n\n4.  **Key Technical Contributions** \\cite{ji2015}\n    *   **Novel Algorithm**: Introduces TransD, a novel knowledge graph embedding model that considers the diversity of *both* relations and entities, unlike prior models that primarily focused on relation diversity.\n    *   **Architectural Innovation**: Employs a dual-vector representation for entities and relations, enabling dynamic construction of mapping matrices without explicit matrix-vector multiplication.\n    *   **Efficiency**: Achieves reduced parameter count and avoids computationally intensive matrix-vector multiplication operations, making it more scalable for large-scale knowledge graphs.\n\n5.  **Experimental Validation** \\cite{ji2015}\n    *   **Experiments Conducted**: Evaluated the model on two standard tasks for knowledge graph embedding:\n        *   Triplet Classification\n        *   Link Prediction\n    *   **Key Performance Metrics & Results**: The evaluation results demonstrate that TransD consistently outperforms existing state-of-the-art methods on these tasks.\n\n6.  **Limitations & Scope** \\cite{ji2015}\n    *   **Scope of Applicability**: Primarily designed for large-scale knowledge graphs due to its efficiency and reduced parameter count.\n    *   **Implicit Limitations**: While it improves upon previous translational models, it still operates within the translational embedding paradigm. The specific details of how the dynamic mapping matrix is constructed and its theoretical properties are not fully detailed in the provided abstract.\n\n7.  **Technical Significance** \\cite{ji2015}\n    *   **Advancement of State-of-the-Art**: TransD significantly advances the technical state-of-the-art in knowledge graph embedding by achieving superior performance on key tasks while offering improved efficiency and scalability.\n    *   **Potential Impact**: Its ability to model the diversity of both entities and relations in a fine-grained and efficient manner could influence future research in scalable and accurate knowledge graph completion and representation learning, particularly for very large and complex KGs.",
        "keywords": [
          "Knowledge graphs (KGs)",
          "KG incompleteness",
          "knowledge graph embedding",
          "TransD",
          "translational models",
          "entity and relation diversity",
          "dual-vector representation",
          "dynamic mapping matrix",
          "computational efficiency",
          "scalability",
          "triplet classification",
          "link prediction",
          "state-of-the-art performance",
          "AI applications"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "18bd7cd489874ed9976b4f87a6a558f9533316e0.pdf"
    },
    {
      "success": true,
      "doc_id": "a7f64a40fb284f0e451763fc8b5d55fe",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Addresses the challenges of sparse data and computational efficiency in Knowledge Graphs (KGs) by representing semantic information as dense, low-dimensional vectors.\n    *   Highlights the critical limitation of conventional KG representation methods, which primarily focus on static data and fail to accommodate facts that evolve over time or are only valid for specific periods. This temporal aspect is crucial for real-world KGs.\n\n*   **Related Work & Positioning**\n    *   Positions itself against existing KG embedding methods that are designed for static data.\n    *   Identifies the key limitation of previous solutions as their inability to effectively model and represent the temporal dynamics of facts within a KG.\n\n*   **Technical Approach & Innovation**\n    *   Proposes a novel temporal KG embedding model based on tensor decomposition.\n    *   The core technical method involves representing the fact set of a KG as a fourth-order tensor, explicitly incorporating head entities, relations, tail entities, and the time dimension.\n    *   This approach is innovative because it extends traditional tensor decomposition methods to inherently capture temporal information, making it suitable for dynamic KGs.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of a temporal KG embedding model that leverages tensor decomposition to integrate the time dimension directly into the representation learning process \\cite{lin2020}.\n    *   **System Design/Architectural Innovation**: Conceptualizing KG facts as a fourth-order tensor (head, relation, tail, time) for comprehensive temporal modeling.\n    *   **Generalizability**: The proposed method can be generalized to enhance other static KG embedding approaches that are also based on tensor decomposition.\n\n*   **Experimental Validation**\n    *   Experiments were conducted using temporal datasets derived from real-world KGs.\n    *   Key performance metrics (though not explicitly detailed in the provided text) demonstrate that the proposed approach significantly outperforms state-of-the-art methods in KG embedding.\n\n*   **Limitations & Scope**\n    *   The paper's primary scope is addressing the temporal evolution of facts within Knowledge Graphs.\n    *   The provided text does not explicitly state technical limitations or assumptions of the *proposed* method, but rather highlights the limitations of *prior* static approaches.\n\n*   **Technical Significance**\n    *   Advances the technical state-of-the-art by providing an effective solution for modeling temporal dynamics in KGs, moving beyond static representations.\n    *   Has significant potential impact on future research by enabling more accurate and realistic representations of evolving knowledge, which is crucial for applications requiring up-to-date and context-aware information.",
      "intriguing_abstract": "The static nature of conventional Knowledge Graphs (KGs) severely limits their ability to capture the dynamic, evolving landscape of real-world information, where facts change over time or are valid only for specific periods. This fundamental limitation hinders the development of truly intelligent systems. We introduce a novel **temporal KG embedding model** that directly addresses this challenge, moving beyond static representations to unlock the full potential of dynamic knowledge.\n\nOur innovative approach leverages a sophisticated **tensor decomposition** framework, conceptualizing KG facts as a **fourth-order tensor** that explicitly integrates head entities, relations, tail entities, and, crucially, the **time dimension**. This direct incorporation of temporal dynamics into the **representation learning** process significantly advances the state-of-the-art, offering a powerful and generalizable method to model evolving knowledge. Experimental validation on real-world temporal datasets demonstrates that our model significantly outperforms existing static and temporal baselines. This work paves the way for more accurate, context-aware, and up-to-date knowledge representation, essential for applications demanding a nuanced understanding of dynamic information.",
      "keywords": [
        "Knowledge Graphs (KGs)",
        "temporal dynamics",
        "KG embedding",
        "novel temporal KG embedding model",
        "tensor decomposition",
        "fourth-order tensor representation",
        "time dimension integration",
        "sparse data challenges",
        "computational efficiency",
        "representation learning",
        "dynamic KGs",
        "state-of-the-art performance",
        "evolving knowledge"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/0364e17da01358e2705524cd781ef8cc928256f5.pdf",
      "citation_key": "lin2020",
      "metadata": {
        "title": "Tensor Decomposition-Based Temporal Knowledge Graph Embedding",
        "authors": [
          "Lifan Lin",
          "Kun She"
        ],
        "published_date": "2020",
        "abstract": "In order to meet the problems caused by sparse data and computational efficiency, knowledge graph (KG) is adopted to represent the semantic information of entities and relations as dense and low-dimensional vectors. While conventional KG representation methods mainly focuse on static data. These methods fail to deal with data that evolves with time which may only be valid for a certain period of time. To accommodate this problem, a temporal KG embedding model based on tensor decomposition is proposed in this paper, which regards the fact set in the KG as a fourth-order tensor including head entities, relations, tail entities and time dimensions. This method can be further generalized to other static KG embedding based on tensor decomposition. With experiments on temporal datasets extracted from real-world KG, extensive experiment results show that our approach outperforms state-of-the-art methods of KG embedding.",
        "file_path": "paper_data/knowledge_graph_embedding/0364e17da01358e2705524cd781ef8cc928256f5.pdf",
        "venue": "IEEE International Conference on Tools with Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Addresses the challenges of sparse data and computational efficiency in Knowledge Graphs (KGs) by representing semantic information as dense, low-dimensional vectors.\n    *   Highlights the critical limitation of conventional KG representation methods, which primarily focus on static data and fail to accommodate facts that evolve over time or are only valid for specific periods. This temporal aspect is crucial for real-world KGs.\n\n*   **Related Work & Positioning**\n    *   Positions itself against existing KG embedding methods that are designed for static data.\n    *   Identifies the key limitation of previous solutions as their inability to effectively model and represent the temporal dynamics of facts within a KG.\n\n*   **Technical Approach & Innovation**\n    *   Proposes a novel temporal KG embedding model based on tensor decomposition.\n    *   The core technical method involves representing the fact set of a KG as a fourth-order tensor, explicitly incorporating head entities, relations, tail entities, and the time dimension.\n    *   This approach is innovative because it extends traditional tensor decomposition methods to inherently capture temporal information, making it suitable for dynamic KGs.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of a temporal KG embedding model that leverages tensor decomposition to integrate the time dimension directly into the representation learning process \\cite{lin2020}.\n    *   **System Design/Architectural Innovation**: Conceptualizing KG facts as a fourth-order tensor (head, relation, tail, time) for comprehensive temporal modeling.\n    *   **Generalizability**: The proposed method can be generalized to enhance other static KG embedding approaches that are also based on tensor decomposition.\n\n*   **Experimental Validation**\n    *   Experiments were conducted using temporal datasets derived from real-world KGs.\n    *   Key performance metrics (though not explicitly detailed in the provided text) demonstrate that the proposed approach significantly outperforms state-of-the-art methods in KG embedding.\n\n*   **Limitations & Scope**\n    *   The paper's primary scope is addressing the temporal evolution of facts within Knowledge Graphs.\n    *   The provided text does not explicitly state technical limitations or assumptions of the *proposed* method, but rather highlights the limitations of *prior* static approaches.\n\n*   **Technical Significance**\n    *   Advances the technical state-of-the-art by providing an effective solution for modeling temporal dynamics in KGs, moving beyond static representations.\n    *   Has significant potential impact on future research by enabling more accurate and realistic representations of evolving knowledge, which is crucial for applications requiring up-to-date and context-aware information.",
        "keywords": [
          "Knowledge Graphs (KGs)",
          "temporal dynamics",
          "KG embedding",
          "novel temporal KG embedding model",
          "tensor decomposition",
          "fourth-order tensor representation",
          "time dimension integration",
          "sparse data challenges",
          "computational efficiency",
          "representation learning",
          "dynamic KGs",
          "state-of-the-art performance",
          "evolving knowledge"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "0364e17da01358e2705524cd781ef8cc928256f5.pdf"
    },
    {
      "success": true,
      "doc_id": "6f6f032eb2b074172c879f0d1f307e9c",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Analysis of \"Molecularevaluated and explainable drug repurposing for COVID19 using ensemble knowledge graph embedding\" \\cite{islam2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The urgent need for effective COVID-19 drugs, as no clinically proven drug is available, coupled with the high cost, long timelines, and high failure rates of traditional drug development. Existing virtual screening methods (e.g., molecular docking) suffer from high false positive rates, and current Knowledge Graph (KG)-based drug repurposing approaches often rely on single embedding models and lack robust validation beyond in-trial drug matching.\n    *   **Importance and Challenge:** Drug repurposing offers a faster, more cost-effective alternative. The challenge lies in accurately predicting novel drug-disease associations from complex biological data, reducing false positives, and providing reliable, explainable predictions to build confidence in out-of-trial candidates.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon existing KG embedding methods for drug repurposing, which formulate the task as link prediction (e.g., (Compound, Treat, Disease) triples) on COVID-19 centric KGs like DRKG.\n    *   **Limitations of Previous Solutions:**\n        *   Most studies depend on a *single KG embedding model*, which may not effectively capture the diverse types of relations within a complex biological KG \\cite{islam2023}.\n        *   Existing approaches primarily *assess predictions only against in-trial drugs*, lacking molecular-level validation (e.g., docking or structural similarity) for predicted compounds \\cite{islam2023}.\n        *   *KG-derived explanations for predictions are largely missing*, making it difficult to understand the rationale behind the recommendations and hindering reliability \\cite{islam2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes an integrated pipeline for COVID-19 drug repurposing, evaluation, and explanation.\n        *   **Ensemble KG Embedding:** It generates \"ensemble embeddings\" by combining multiple complementary traditional KG embedding methods (TransE, TransH, DistMult) and then reducing their dimensionality using Principal Component Analysis (PCA) \\cite{islam2023}. This aims to create a more robust latent representation of KG entities and relations.\n        *   **Deep Neural Network (DNN) Prediction:** These ensemble embeddings are fed into a DNN-based prediction model to compute the probability of a `Treat` relation between compounds and COVID-19 disease targets \\cite{islam2023}.\n        *   **Multi-faceted Evaluation:** Predictions are evaluated through cross-matching with in-trial drugs and, uniquely, through *molecular evaluation* (ligand-based structural similarity clustering and target-based molecular docking) \\cite{islam2023}.\n        *   **Explainability:** Rule-based explanations are extracted from the KG and instantiated with KG-derived explanatory paths for specific predictions \\cite{islam2023}.\n    *   **Novelty/Difference:**\n        *   **Ensemble Embedding:** First to propose an ensemble approach combining multiple KG embedding models to capture diverse relation types, overcoming the limitations of single models \\cite{islam2023}.\n        *   **Molecular Evaluation Integration:** First to integrate molecular docking and ligand structural similarity as a post-prediction evaluation step for KG embedding-based drug repurposing, significantly enhancing confidence in novel predictions \\cite{islam2023}.\n        *   **Rule-based Explainability:** Provides rule-based explanations extracted from the KG, offering transparency and improving the reliability of predictions, a feature often lacking in other KG-based repurposing methods \\cite{islam2023}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A novel ensemble KG embedding generation method that combines outputs from multiple traditional embedding models (TransE, TransH, DistMult) and uses PCA for dimensionality reduction, leading to higher quality and more compact representations \\cite{islam2023}.\n        *   The integration of molecular docking and ligand-based structural clustering as a complementary and reusable method for evaluating KG-based drug repurposing predictions, providing molecular-level validation \\cite{islam2023}.\n        *   A methodology for extracting and instantiating rule-based explanations from the KG to provide transparent justifications for predicted drug-disease associations \\cite{islam2023}.\n    *   **System Design/Architectural Innovations:** An integrated pipeline that seamlessly combines KG data cleaning, ensemble embedding generation, DNN-based prediction, multi-modal evaluation (in-trial matching and molecular), and rule-based explanation, offering a comprehensive solution for drug repurposing \\cite{islam2023}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Utilized a cleaned COVID-19 centric Drug Repurposing Knowledge Graph (DRKG) containing 98,000 entities, 102 relation types, and 5.8 million triples \\cite{islam2023}.\n        *   Trained a DNN prediction model using 10-fold cross-validation on 261,080 training pairs and 5800 test pairs \\cite{islam2023}.\n        *   Evaluated top-100 predicted compounds by cross-matching with 31 known in-trial drugs for COVID-19 \\cite{islam2023}.\n        *   Conducted molecular evaluations specifically for the SARS-CoV-2-nsp13 target:\n            *   Ligand-based evaluation: Clustered 38 predicted compounds with 86 known ligands based on structural similarity \\cite{islam2023}.\n            *   Target-based evaluation: Performed molecular docking of 38 predicted and 86 known ligands into the nsp13 active site using GOLD software \\cite{islam2023}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The DNN model achieved an average Mean Squared Error (MSE) of 0.09 and an average AUC of 0.96 for link prediction \\cite{islam2023}.\n        *   **Improved In-Trial Drug Retrieval:** The approach identified 10 out of 31 in-trial drugs within its top-100 predictions, outperforming state-of-the-art methods like Tex-Graph, TransE-DRKG, ENSIGN, and PERM in terms of top-ranked in-trial drugs (e.g., Dexamethasone ranked 1st, Methylprednisolone 2nd, Ruxolitinib 3rd) \\cite{islam2023}.\n        *   **Molecular Validation Success:**\n            *   Ligand-based: Identified 18 novel compounds for nsp13 in clusters showing high molecular similarity with known ligands \\cite{islam2023}.\n            *   Target-based (Docking): Fosinopril, a predicted drug, was ranked 2nd among 124 ligands for nsp13 with a high docking score (78.86), very close to the top-ranked known ligand Diosmine (79.04) \\cite{islam2023}. Macitentan, Eprosartan, and Dinoprostone also appeared in the top-20 docked ligands \\cite{islam2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The ensemble embedding relies on combining existing traditional KG embedding methods; future work could explore more advanced or specialized models.\n        *   Molecular evaluation was focused on a single SARS-CoV-2 target (nsp13) due to computational and resource constraints \\cite{islam2023}.\n        *   The quality and comprehensiveness of the rule-based explanations are dependent on the richness and accuracy of the underlying KG and the rule extraction process \\cite{islam2023}.\n    *   **Scope of Applicability:** While primarily demonstrated for COVID-19 drug repurposing, the proposed integrated pipeline (ensemble embeddings, DNN prediction, molecular evaluation, and explainability) is designed to be generalizable and applicable to drug repurposing for other diseases and broader drug discovery tasks \\cite{islam2023}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:**\n        *   Introduces a more robust and comprehensive KG embedding strategy through ensemble learning, addressing the inherent limitations of single embedding models in capturing diverse relational semantics \\cite{islam2023}.\n        *   Establishes a novel, multi-faceted validation framework for KG-based drug repurposing by integrating molecular evaluation (docking and structural similarity) alongside traditional in-trial drug matching. This significantly enhances the confidence in predictions, especially for out-of-trial candidates \\cite{islam2023}.\n        *   Pioneers the provision of rule-based explanations for KG-based drug repurposing predictions, moving beyond black-box models and improving the transparency and reliability of the results \\cite{islam2023}.\n    *   **Potential Impact on Future Research:**\n        *   Provides a more reliable and explainable methodology for identifying drug repurposing candidates, potentially accelerating drug discovery for emerging diseases and reducing development costs and time \\cite{islam2023}.\n        *   The integrated evaluation and explanation methods are reusable and complementary, setting a new standard for assessing and validating KG-based drug repurposing results in the field \\cite{islam2023}.\n        *   Highlights specific promising candidates like Fosinopril for SARS-CoV-2-nsp13, providing strong evidence for further experimental investigation and potentially leading to new therapeutic options \\cite{islam2023}.",
      "intriguing_abstract": "The urgent global need for effective COVID-19 therapeutics demands innovative drug discovery strategies. Traditional methods are slow and costly, while existing Knowledge Graph (KG)-based drug repurposing often suffers from high false positives, reliance on single embedding models, and a critical lack of molecular-level validation and explainability. We introduce a novel, integrated pipeline that revolutionizes drug repurposing by combining robust **ensemble KG embedding** with multi-faceted **molecular evaluation** and transparent **rule-based explanations**. Our approach leverages an ensemble of TransE, TransH, and DistMult embeddings, refined by PCA, to generate superior latent representations for a Deep Neural Network (DNN) prediction model. Crucially, we integrate ligand-based structural similarity clustering and target-based **molecular docking** (e.g., for SARS-CoV-2-nsp13) to provide unprecedented molecular validation for predicted candidates. Achieving an AUC of 0.96 and outperforming state-of-the-art methods in retrieving in-trial drugs, our pipeline identified promising novel candidates like Fosinopril, which demonstrated exceptional docking scores. This work sets a new standard for **explainable AI** in drug repurposing, accelerating the identification of reliable therapeutic options for COVID-19 and beyond.",
      "keywords": [
        "COVID-19 drug repurposing",
        "Knowledge Graph embedding",
        "Ensemble KG embedding",
        "Deep Neural Network",
        "Molecular evaluation",
        "Molecular docking",
        "Ligand structural similarity",
        "Rule-based explanations",
        "Multi-faceted validation",
        "Link prediction",
        "SARS-CoV-2-nsp13",
        "Fosinopril"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/fda63b289d4c0c332f88975994114fb61b514ced.pdf",
      "citation_key": "islam2023",
      "metadata": {
        "title": "Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding",
        "authors": [
          "M. Islam",
          "Diego Amaya-Ramirez",
          "B. Maigret",
          "M. Devignes",
          "Sabeur Aridhi",
          "Malika Smal-Tabbone"
        ],
        "published_date": "2023",
        "abstract": "The search for an effective drug is still urgent for COVID-19 as no drug with proven clinical efficacy is available. Finding the new purpose of an approved or investigational drug, known as drug repurposing, has become increasingly popular in recent years. We propose here a new drug repurposing approach for COVID-19, based on knowledge graph (KG) embeddings. Our approach learns ensemble embeddings of entities and relations in a COVID-19 centric KG, in order to get a better latent representation of the graph elements. Ensemble KG-embeddings are subsequently used in a deep neural network trained for discovering potential drugs for COVID-19. Compared to related works, we retrieve more in-trial drugs among our top-ranked predictions, thus giving greater confidence in our prediction for out-of-trial drugs. For the first time to our knowledge, molecular docking is then used to evaluate the predictions obtained from drug repurposing using KG embedding. We show that Fosinopril is a potential ligand for the SARS-CoV-2 nsp13 target. We also provide explanations of our predictions thanks to rules extracted from the KG and instanciated by KG-derived explanatory paths. Molecular evaluation and explanatory paths bring reliability to our results and constitute new complementary and reusable methods for assessing KG-based drug repurposing.",
        "file_path": "paper_data/knowledge_graph_embedding/fda63b289d4c0c332f88975994114fb61b514ced.pdf",
        "venue": "Scientific Reports",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Analysis of \"Molecularevaluated and explainable drug repurposing for COVID19 using ensemble knowledge graph embedding\" \\cite{islam2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The urgent need for effective COVID-19 drugs, as no clinically proven drug is available, coupled with the high cost, long timelines, and high failure rates of traditional drug development. Existing virtual screening methods (e.g., molecular docking) suffer from high false positive rates, and current Knowledge Graph (KG)-based drug repurposing approaches often rely on single embedding models and lack robust validation beyond in-trial drug matching.\n    *   **Importance and Challenge:** Drug repurposing offers a faster, more cost-effective alternative. The challenge lies in accurately predicting novel drug-disease associations from complex biological data, reducing false positives, and providing reliable, explainable predictions to build confidence in out-of-trial candidates.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon existing KG embedding methods for drug repurposing, which formulate the task as link prediction (e.g., (Compound, Treat, Disease) triples) on COVID-19 centric KGs like DRKG.\n    *   **Limitations of Previous Solutions:**\n        *   Most studies depend on a *single KG embedding model*, which may not effectively capture the diverse types of relations within a complex biological KG \\cite{islam2023}.\n        *   Existing approaches primarily *assess predictions only against in-trial drugs*, lacking molecular-level validation (e.g., docking or structural similarity) for predicted compounds \\cite{islam2023}.\n        *   *KG-derived explanations for predictions are largely missing*, making it difficult to understand the rationale behind the recommendations and hindering reliability \\cite{islam2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes an integrated pipeline for COVID-19 drug repurposing, evaluation, and explanation.\n        *   **Ensemble KG Embedding:** It generates \"ensemble embeddings\" by combining multiple complementary traditional KG embedding methods (TransE, TransH, DistMult) and then reducing their dimensionality using Principal Component Analysis (PCA) \\cite{islam2023}. This aims to create a more robust latent representation of KG entities and relations.\n        *   **Deep Neural Network (DNN) Prediction:** These ensemble embeddings are fed into a DNN-based prediction model to compute the probability of a `Treat` relation between compounds and COVID-19 disease targets \\cite{islam2023}.\n        *   **Multi-faceted Evaluation:** Predictions are evaluated through cross-matching with in-trial drugs and, uniquely, through *molecular evaluation* (ligand-based structural similarity clustering and target-based molecular docking) \\cite{islam2023}.\n        *   **Explainability:** Rule-based explanations are extracted from the KG and instantiated with KG-derived explanatory paths for specific predictions \\cite{islam2023}.\n    *   **Novelty/Difference:**\n        *   **Ensemble Embedding:** First to propose an ensemble approach combining multiple KG embedding models to capture diverse relation types, overcoming the limitations of single models \\cite{islam2023}.\n        *   **Molecular Evaluation Integration:** First to integrate molecular docking and ligand structural similarity as a post-prediction evaluation step for KG embedding-based drug repurposing, significantly enhancing confidence in novel predictions \\cite{islam2023}.\n        *   **Rule-based Explainability:** Provides rule-based explanations extracted from the KG, offering transparency and improving the reliability of predictions, a feature often lacking in other KG-based repurposing methods \\cite{islam2023}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A novel ensemble KG embedding generation method that combines outputs from multiple traditional embedding models (TransE, TransH, DistMult) and uses PCA for dimensionality reduction, leading to higher quality and more compact representations \\cite{islam2023}.\n        *   The integration of molecular docking and ligand-based structural clustering as a complementary and reusable method for evaluating KG-based drug repurposing predictions, providing molecular-level validation \\cite{islam2023}.\n        *   A methodology for extracting and instantiating rule-based explanations from the KG to provide transparent justifications for predicted drug-disease associations \\cite{islam2023}.\n    *   **System Design/Architectural Innovations:** An integrated pipeline that seamlessly combines KG data cleaning, ensemble embedding generation, DNN-based prediction, multi-modal evaluation (in-trial matching and molecular), and rule-based explanation, offering a comprehensive solution for drug repurposing \\cite{islam2023}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Utilized a cleaned COVID-19 centric Drug Repurposing Knowledge Graph (DRKG) containing 98,000 entities, 102 relation types, and 5.8 million triples \\cite{islam2023}.\n        *   Trained a DNN prediction model using 10-fold cross-validation on 261,080 training pairs and 5800 test pairs \\cite{islam2023}.\n        *   Evaluated top-100 predicted compounds by cross-matching with 31 known in-trial drugs for COVID-19 \\cite{islam2023}.\n        *   Conducted molecular evaluations specifically for the SARS-CoV-2-nsp13 target:\n            *   Ligand-based evaluation: Clustered 38 predicted compounds with 86 known ligands based on structural similarity \\cite{islam2023}.\n            *   Target-based evaluation: Performed molecular docking of 38 predicted and 86 known ligands into the nsp13 active site using GOLD software \\cite{islam2023}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The DNN model achieved an average Mean Squared Error (MSE) of 0.09 and an average AUC of 0.96 for link prediction \\cite{islam2023}.\n        *   **Improved In-Trial Drug Retrieval:** The approach identified 10 out of 31 in-trial drugs within its top-100 predictions, outperforming state-of-the-art methods like Tex-Graph, TransE-DRKG, ENSIGN, and PERM in terms of top-ranked in-trial drugs (e.g., Dexamethasone ranked 1st, Methylprednisolone 2nd, Ruxolitinib 3rd) \\cite{islam2023}.\n        *   **Molecular Validation Success:**\n            *   Ligand-based: Identified 18 novel compounds for nsp13 in clusters showing high molecular similarity with known ligands \\cite{islam2023}.\n            *   Target-based (Docking): Fosinopril, a predicted drug, was ranked 2nd among 124 ligands for nsp13 with a high docking score (78.86), very close to the top-ranked known ligand Diosmine (79.04) \\cite{islam2023}. Macitentan, Eprosartan, and Dinoprostone also appeared in the top-20 docked ligands \\cite{islam2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The ensemble embedding relies on combining existing traditional KG embedding methods; future work could explore more advanced or specialized models.\n        *   Molecular evaluation was focused on a single SARS-CoV-2 target (nsp13) due to computational and resource constraints \\cite{islam2023}.\n        *   The quality and comprehensiveness of the rule-based explanations are dependent on the richness and accuracy of the underlying KG and the rule extraction process \\cite{islam2023}.\n    *   **Scope of Applicability:** While primarily demonstrated for COVID-19 drug repurposing, the proposed integrated pipeline (ensemble embeddings, DNN prediction, molecular evaluation, and explainability) is designed to be generalizable and applicable to drug repurposing for other diseases and broader drug discovery tasks \\cite{islam2023}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:**\n        *   Introduces a more robust and comprehensive KG embedding strategy through ensemble learning, addressing the inherent limitations of single embedding models in capturing diverse relational semantics \\cite{islam2023}.\n        *   Establishes a novel, multi-faceted validation framework for KG-based drug repurposing by integrating molecular evaluation (docking and structural similarity) alongside traditional in-trial drug matching. This significantly enhances the confidence in predictions, especially for out-of-trial candidates \\cite{islam2023}.\n        *   Pioneers the provision of rule-based explanations for KG-based drug repurposing predictions, moving beyond black-box models and improving the transparency and reliability of the results \\cite{islam2023}.\n    *   **Potential Impact on Future Research:**\n        *   Provides a more reliable and explainable methodology for identifying drug repurposing candidates, potentially accelerating drug discovery for emerging diseases and reducing development costs and time \\cite{islam2023}.\n        *   The integrated evaluation and explanation methods are reusable and complementary, setting a new standard for assessing and validating KG-based drug repurposing results in the field \\cite{islam2023}.\n        *   Highlights specific promising candidates like Fosinopril for SARS-CoV-2-nsp13, providing strong evidence for further experimental investigation and potentially leading to new therapeutic options \\cite{islam2023}.",
        "keywords": [
          "COVID-19 drug repurposing",
          "Knowledge Graph embedding",
          "Ensemble KG embedding",
          "Deep Neural Network",
          "Molecular evaluation",
          "Molecular docking",
          "Ligand structural similarity",
          "Rule-based explanations",
          "Multi-faceted validation",
          "Link prediction",
          "SARS-CoV-2-nsp13",
          "Fosinopril"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "fda63b289d4c0c332f88975994114fb61b514ced.pdf"
    },
    {
      "success": true,
      "doc_id": "9fd21d1f7da047289be46125ac3ef072",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Existing Knowledge Graph Embedding (KGE) methods struggle with large-scale knowledge graphs \\cite{wang2021}.\n    *   This problem is critical because KGEs are widely used for capturing semantics and various downstream applications (e.g., similar entity extraction), but their applicability to emerging large-scale KGs is limited by storage and inference efficiency constraints.\n\n*   **Related Work & Positioning**\n    *   This work directly addresses the limitations of previous KGE solutions, which \"cannot work well on emerging knowledge graphs that are large-scale due to the constraints in storage and inference efficiency\" \\cite{wang2021}.\n    *   It positions itself as a lightweight alternative designed to overcome these efficiency bottlenecks.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is LightKG, a lightweight KGE model that significantly reduces storage and inference time \\cite{wang2021}.\n    *   Instead of storing continuous vectors for each entity, LightKG stores a few codebooks (containing codewords representing embedding representatives) and indices for codeword selections.\n    *   Querying efficiency is boosted by calculating relevance scores via a quick look-up table between queries and codewords.\n    *   LightKG is an end-to-end framework that automatically infers codebooks, codewords, and generates approximated entity embeddings.\n    *   **Novelty**:\n        *   A residual module is incorporated to induce diversity among codebooks.\n        *   A continuous function is adopted to approximate the non-differentiable codeword selection process.\n        *   A novel dynamic negative sampling method based on quantization is proposed, which is applicable to LightKG and other KGE methods.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   LightKG model: A novel KGE architecture leveraging codebooks and codewords for highly efficient storage and inference \\cite{wang2021}.\n        *   Residual module: Integrated into LightKG to enhance the diversity of learned codebooks.\n        *   Continuous approximation: A technique to handle the non-differentiable codeword selection, enabling end-to-end training.\n        *   Dynamic negative sampling: A quantization-based method to improve KGE performance, applicable broadly.\n    *   **System Design**: An end-to-end framework for automatic codebook inference and approximated embedding generation.\n\n*   **Experimental Validation**\n    *   **Experiments**: Extensive experiments were conducted on five public datasets \\cite{wang2021}.\n    *   **Key Performance Metrics & Results**:\n        *   LightKG demonstrated high search and memory efficiency while maintaining high approximate search accuracy.\n        *   The proposed dynamic negative sampling method dramatically improved model performance, achieving over 19% improvement on average.\n\n*   **Limitations & Scope**\n    *   The primary scope of LightKG is to address the storage and inference efficiency challenges of KGEs on large-scale knowledge graphs \\cite{wang2021}.\n    *   While the paper highlights the efficiency gains, it implicitly assumes that the approximation introduced by codebooks and codewords is acceptable for the target applications.\n\n*   **Technical Significance**\n    *   LightKG significantly advances the technical state-of-the-art by providing a highly efficient KGE model that drastically reduces storage and inference time, making KGEs practical for large-scale knowledge graphs \\cite{wang2021}.\n    *   The dynamic negative sampling method offers a general improvement for KGE performance, potentially impacting future research across various KGE models.",
      "intriguing_abstract": "The pervasive challenge of deploying Knowledge Graph Embeddings (KGEs) on massive, real-world knowledge graphs stems from prohibitive storage and inference efficiency. We introduce LightKG, a pioneering lightweight KGE model designed to shatter these barriers. Unlike traditional methods storing continuous vectors, LightKG drastically reduces memory footprint and accelerates inference by leveraging a compact set of codebooks and indices, enabling rapid relevance score look-ups.\n\nOur end-to-end framework innovates with a residual module to ensure diverse codebook learning and a continuous function to approximate the non-differentiable codeword selection, facilitating seamless training. Crucially, we propose a novel quantization-based dynamic negative sampling method, which not only significantly boosts LightKG's performance (over 19% average improvement) but is also broadly applicable to other KGE models. Extensive experiments on five public datasets confirm LightKG's superior search and memory efficiency while preserving high approximate search accuracy. LightKG makes high-performance KGEs practical for the largest knowledge graphs, offering a transformative solution for semantic understanding and downstream applications.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "large-scale knowledge graphs",
        "storage and inference efficiency",
        "LightKG model",
        "codebooks and codewords",
        "approximate entity embeddings",
        "residual module",
        "continuous approximation for codeword selection",
        "quantization-based dynamic negative sampling",
        "end-to-end framework",
        "memory efficiency",
        "search accuracy"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/3f0d5aa7a637d2c0bb3d768c99cc203430b4481e.pdf",
      "citation_key": "wang2021",
      "metadata": {
        "title": "A Lightweight Knowledge Graph Embedding Framework for Efficient Inference and Storage",
        "authors": [
          "Haoyu Wang",
          "Yaqing Wang",
          "Defu Lian",
          "Jing Gao"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graphs, which consist of entities and their relations, have become a popular way to store structured knowledge. Knowledge graph embedding (KGE), which derives a representation for each entity and relation, has been widely used to capture the semantics of the information in the knowledge graphs, and has demonstrated great success in many downstream applications, such as the extraction of similar entities in response to a query entity. However, existing KGE methods cannot work well on emerging knowledge graphs that are large-scale due to the constraints in storage and inference efficiency. In this paper, we propose a lightweight KGE model, LightKG, which significantly reduces storage as well as running time needed for inference. Instead of storing a continuous vector for every entity, LightKG only needs to store a few codebooks, each of which contains some codewords that correspond to the representatives among the embeddings, and the indices that correspond to the codeword selections for entities. Hence LightKG can achieve highly efficient storage. The efficiency of the downstream querying process can be significantly boosted too with the proposed LightKG model as the relevance score between the query and an entity can be efficiently calculated via a quick look-up in a table that contains the scores between the query and codewords. The storage and inference efficiency of LightKG is achieved by its novel design. LightKG is an end-to-end framework that automatically infers codebooks and codewords and generates an approximated embedding for each entity. A residual module is included in LightKG to induce the diversity among codebooks, and a continuous function is adopted to approximate codeword selection, which is non-differential. In addition, to further improve the performance of KGE, we propose a novel dynamic negative sampling method based on quantization, which can be applied to the proposed LightKG or other KGE methods. We conduct extensive experiments on five public datasets. The experiments show that LightKG is search and memory efficient with high approximate search accuracy. Also, the dynamic negative sampling can dramatically improve model performance with over 19% improvement on average.",
        "file_path": "paper_data/knowledge_graph_embedding/3f0d5aa7a637d2c0bb3d768c99cc203430b4481e.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Existing Knowledge Graph Embedding (KGE) methods struggle with large-scale knowledge graphs \\cite{wang2021}.\n    *   This problem is critical because KGEs are widely used for capturing semantics and various downstream applications (e.g., similar entity extraction), but their applicability to emerging large-scale KGs is limited by storage and inference efficiency constraints.\n\n*   **Related Work & Positioning**\n    *   This work directly addresses the limitations of previous KGE solutions, which \"cannot work well on emerging knowledge graphs that are large-scale due to the constraints in storage and inference efficiency\" \\cite{wang2021}.\n    *   It positions itself as a lightweight alternative designed to overcome these efficiency bottlenecks.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is LightKG, a lightweight KGE model that significantly reduces storage and inference time \\cite{wang2021}.\n    *   Instead of storing continuous vectors for each entity, LightKG stores a few codebooks (containing codewords representing embedding representatives) and indices for codeword selections.\n    *   Querying efficiency is boosted by calculating relevance scores via a quick look-up table between queries and codewords.\n    *   LightKG is an end-to-end framework that automatically infers codebooks, codewords, and generates approximated entity embeddings.\n    *   **Novelty**:\n        *   A residual module is incorporated to induce diversity among codebooks.\n        *   A continuous function is adopted to approximate the non-differentiable codeword selection process.\n        *   A novel dynamic negative sampling method based on quantization is proposed, which is applicable to LightKG and other KGE methods.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   LightKG model: A novel KGE architecture leveraging codebooks and codewords for highly efficient storage and inference \\cite{wang2021}.\n        *   Residual module: Integrated into LightKG to enhance the diversity of learned codebooks.\n        *   Continuous approximation: A technique to handle the non-differentiable codeword selection, enabling end-to-end training.\n        *   Dynamic negative sampling: A quantization-based method to improve KGE performance, applicable broadly.\n    *   **System Design**: An end-to-end framework for automatic codebook inference and approximated embedding generation.\n\n*   **Experimental Validation**\n    *   **Experiments**: Extensive experiments were conducted on five public datasets \\cite{wang2021}.\n    *   **Key Performance Metrics & Results**:\n        *   LightKG demonstrated high search and memory efficiency while maintaining high approximate search accuracy.\n        *   The proposed dynamic negative sampling method dramatically improved model performance, achieving over 19% improvement on average.\n\n*   **Limitations & Scope**\n    *   The primary scope of LightKG is to address the storage and inference efficiency challenges of KGEs on large-scale knowledge graphs \\cite{wang2021}.\n    *   While the paper highlights the efficiency gains, it implicitly assumes that the approximation introduced by codebooks and codewords is acceptable for the target applications.\n\n*   **Technical Significance**\n    *   LightKG significantly advances the technical state-of-the-art by providing a highly efficient KGE model that drastically reduces storage and inference time, making KGEs practical for large-scale knowledge graphs \\cite{wang2021}.\n    *   The dynamic negative sampling method offers a general improvement for KGE performance, potentially impacting future research across various KGE models.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "large-scale knowledge graphs",
          "storage and inference efficiency",
          "LightKG model",
          "codebooks and codewords",
          "approximate entity embeddings",
          "residual module",
          "continuous approximation for codeword selection",
          "quantization-based dynamic negative sampling",
          "end-to-end framework",
          "memory efficiency",
          "search accuracy"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "3f0d5aa7a637d2c0bb3d768c99cc203430b4481e.pdf"
    },
    {
      "success": true,
      "doc_id": "e311bf42b1774eb8b94f7a612e2db61f",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the challenges in knowledge graph embedding (KGE) research related to reproducibility, the difficulty of conducting comprehensive experimental studies, and the lack of tools to easily analyze the individual contributions of different components (training methods, model architectures, evaluation methods).\n    *   **Importance:** Reproducibility is crucial for scientific validity, and comprehensive studies are essential for a thorough understanding and advancement of KGE models. Isolating component contributions helps in targeted improvements.\n\n*   **Related Work & Positioning**\n    *   **Relation:** This work relates to existing knowledge graph embedding models and training methods.\n    *   **Limitations of previous solutions (implied):** Existing research practices and tools often lack the necessary features for easy reproducibility, systematic comprehensive experimentation, and granular analysis of model components.\n    *   **Positioning:** LibKGE \\cite{broscheit2020} positions itself as a robust, open-source framework designed to overcome these limitations, providing a platform for reproducible and comprehensive KGE research that achieves competitive to state-of-the-art performance.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** LibKGE \\cite{broscheit2020} is an open-source, PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models specifically for link prediction.\n    *   **Novelty:**\n        *   **Reproducibility:** Experiments are fully reproducible via a single, highly configurable configuration file.\n        *   **Modularity:** Individual components (models, training methods, evaluation methods) are decoupled, allowing for flexible mix-and-match experimentation.\n        *   **Efficiency:** Implementations prioritize efficiency within the standard Python/Numpy/PyTorch ecosystem.\n        *   **Analysis:** Includes a comprehensive logging mechanism and tooling to facilitate in-depth analysis of experiments.\n\n*   **Key Technical Contributions**\n    *   **System Design:** A highly configurable, modular, and extensible PyTorch-based library architecture for KGE research.\n    *   **Novel Techniques:**\n        *   A robust mechanism for ensuring full experiment reproducibility through detailed configuration files.\n        *   A decoupled component design that enables flexible combination and testing of different KGE model architectures, training methods, and evaluation strategies.\n        *   Integrated comprehensive logging and analysis tools to support detailed experimental insights.\n        *   Efficient implementations of common KGE models and training methods, with easy extensibility for new additions.\n\n*   **Experimental Validation**\n    *   **Experiments:** A comparative study (Ruffinelli et al., 2020) was conducted using LibKGE \\cite{broscheit2020}.\n    *   **Key Results:** The study demonstrated that LibKGE achieves competitive to state-of-the-art performance for many KGE models, even with only a modest amount of automatic hyperparameter tuning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The efficiency of implementations is maintained \"without leaving the scope of Python/Numpy/PyTorch,\" indicating adherence to these frameworks' inherent performance characteristics.\n    *   **Scope:** Primarily focused on knowledge graph embedding models for the specific task of link prediction, covering training, hyperparameter optimization, and evaluation.\n\n*   **Technical Significance**\n    *   **Advancement:** LibKGE \\cite{broscheit2020} significantly advances the technical state-of-the-art by providing a standardized, reproducible, and analyzable framework for KGE research, addressing critical methodological gaps in the field.\n    *   **Potential Impact:** It has the potential to become a foundational tool for KGE researchers, fostering more reliable and comparable research, accelerating the development of new models, and enabling deeper insights into the behavior of existing ones.",
      "intriguing_abstract": "The rapid evolution of Knowledge Graph Embedding (KGE) models is often hampered by a critical lack of reproducibility and the immense difficulty in conducting systematic, comprehensive experimental studies. Disentangling the true contributions of various model architectures, training methods, and evaluation strategies remains a significant challenge. We introduce LibKGE, an open-source, PyTorch-based framework designed to revolutionize KGE research for link prediction.\n\nLibKGE ensures full experiment reproducibility through a single, highly configurable file, a groundbreaking feature for the field. Its modular architecture decouples components, enabling researchers to effortlessly mix-and-match models, training regimes, and evaluation metrics. Coupled with efficient implementations and comprehensive logging tools for in-depth analysis, LibKGE achieves competitive to state-of-the-art performance with minimal hyperparameter tuning. This framework not only standardizes KGE experimentation but also empowers researchers to accelerate model development and gain unprecedented insights into model behavior, fostering a new era of reliable and comparable KGE research.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Reproducibility",
        "LibKGE framework",
        "Link prediction",
        "PyTorch-based library",
        "Modular design",
        "Hyperparameter optimization",
        "Comprehensive experimental studies",
        "Analysis tools",
        "State-of-the-art performance",
        "Configurable experiments",
        "Methodological gaps",
        "Standardized framework"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83.pdf",
      "citation_key": "broscheit2020",
      "metadata": {
        "title": "LibKGE - A knowledge graph embedding library for reproducible research",
        "authors": [
          "Samuel Broscheit",
          "Daniel Ruffinelli",
          "Adrian Kochsiek",
          "Patrick Betz",
          "Rainer Gemulla"
        ],
        "published_date": "2020",
        "abstract": "LibKGE ( https://github.com/uma-pi1/kge ) is an open-source PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models for link prediction. The key goals of LibKGE are to enable reproducible research, to provide a framework for comprehensive experimental studies, and to facilitate analyzing the contributions of individual components of training methods, model architectures, and evaluation methods. LibKGE is highly configurable and every experiment can be fully reproduced with a single configuration file. Individual components are decoupled to the extent possible so that they can be mixed and matched with each other. Implementations in LibKGE aim to be as efficient as possible without leaving the scope of Python/Numpy/PyTorch. A comprehensive logging mechanism and tooling facilitates in-depth analysis. LibKGE provides implementations of common knowledge graph embedding models and training methods, and new ones can be easily added. A comparative study (Ruffinelli et al., 2020) showed that LibKGE reaches competitive to state-of-the-art performance for many models with a modest amount of automatic hyperparameter tuning.",
        "file_path": "paper_data/knowledge_graph_embedding/2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the challenges in knowledge graph embedding (KGE) research related to reproducibility, the difficulty of conducting comprehensive experimental studies, and the lack of tools to easily analyze the individual contributions of different components (training methods, model architectures, evaluation methods).\n    *   **Importance:** Reproducibility is crucial for scientific validity, and comprehensive studies are essential for a thorough understanding and advancement of KGE models. Isolating component contributions helps in targeted improvements.\n\n*   **Related Work & Positioning**\n    *   **Relation:** This work relates to existing knowledge graph embedding models and training methods.\n    *   **Limitations of previous solutions (implied):** Existing research practices and tools often lack the necessary features for easy reproducibility, systematic comprehensive experimentation, and granular analysis of model components.\n    *   **Positioning:** LibKGE \\cite{broscheit2020} positions itself as a robust, open-source framework designed to overcome these limitations, providing a platform for reproducible and comprehensive KGE research that achieves competitive to state-of-the-art performance.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** LibKGE \\cite{broscheit2020} is an open-source, PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models specifically for link prediction.\n    *   **Novelty:**\n        *   **Reproducibility:** Experiments are fully reproducible via a single, highly configurable configuration file.\n        *   **Modularity:** Individual components (models, training methods, evaluation methods) are decoupled, allowing for flexible mix-and-match experimentation.\n        *   **Efficiency:** Implementations prioritize efficiency within the standard Python/Numpy/PyTorch ecosystem.\n        *   **Analysis:** Includes a comprehensive logging mechanism and tooling to facilitate in-depth analysis of experiments.\n\n*   **Key Technical Contributions**\n    *   **System Design:** A highly configurable, modular, and extensible PyTorch-based library architecture for KGE research.\n    *   **Novel Techniques:**\n        *   A robust mechanism for ensuring full experiment reproducibility through detailed configuration files.\n        *   A decoupled component design that enables flexible combination and testing of different KGE model architectures, training methods, and evaluation strategies.\n        *   Integrated comprehensive logging and analysis tools to support detailed experimental insights.\n        *   Efficient implementations of common KGE models and training methods, with easy extensibility for new additions.\n\n*   **Experimental Validation**\n    *   **Experiments:** A comparative study (Ruffinelli et al., 2020) was conducted using LibKGE \\cite{broscheit2020}.\n    *   **Key Results:** The study demonstrated that LibKGE achieves competitive to state-of-the-art performance for many KGE models, even with only a modest amount of automatic hyperparameter tuning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The efficiency of implementations is maintained \"without leaving the scope of Python/Numpy/PyTorch,\" indicating adherence to these frameworks' inherent performance characteristics.\n    *   **Scope:** Primarily focused on knowledge graph embedding models for the specific task of link prediction, covering training, hyperparameter optimization, and evaluation.\n\n*   **Technical Significance**\n    *   **Advancement:** LibKGE \\cite{broscheit2020} significantly advances the technical state-of-the-art by providing a standardized, reproducible, and analyzable framework for KGE research, addressing critical methodological gaps in the field.\n    *   **Potential Impact:** It has the potential to become a foundational tool for KGE researchers, fostering more reliable and comparable research, accelerating the development of new models, and enabling deeper insights into the behavior of existing ones.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Reproducibility",
          "LibKGE framework",
          "Link prediction",
          "PyTorch-based library",
          "Modular design",
          "Hyperparameter optimization",
          "Comprehensive experimental studies",
          "Analysis tools",
          "State-of-the-art performance",
          "Configurable experiments",
          "Methodological gaps",
          "Standardized framework"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83.pdf"
    },
    {
      "success": true,
      "doc_id": "281023f9bf971748bd0e0edeb2ef03ba",
      "summary": "Here's a focused summary of the empirical study by \\cite{fanourakis2022} for a literature review:\n\n1.  **Research Questions & Hypotheses**\n    *   The study investigates critical factors affecting relation-based and attribute-based embedding methods (Q1), the effectiveness improvement from combining structural and attribute information (Q2), the trade-off between effectiveness and efficiency (Q3), and method sensitivity to dataset characteristics (Q4) \\cite{fanourakis2022}.\n    *   It implicitly hypothesizes statistically significant correlations between embedding methods and KG meta-features, a statistically significant ranking of methods by effectiveness, and the existence of interesting effectiveness vs. efficiency trade-offs \\cite{fanourakis2022}.\n\n2.  **Study Design & Methodology**\n    *   The study employs a meta-level analysis, conducting a fair empirical comparison of state-of-the-art embedding-based entity alignment methods across an extended testbed of real-world knowledge graphs \\cite{fanourakis2022}. Data collection involved evaluating methods on diverse KG characteristics, and analysis used a statistically sound methodology including non-parametric tests for ranking and correlation analysis \\cite{fanourakis2022}.\n\n3.  **Data & Participants**\n    *   The study utilized an extended testbed of real-world knowledge graph (KG) datasets, including five additional datasets beyond those in prior benchmarks, featuring diverse characteristics like KG density, entity naming, and textual descriptions \\cite{fanourakis2022}. It evaluated a range of popular supervised, unsupervised, and semi-supervised embedding methods, such as MTransE, RDGCN, AttrE, KDCoE, and BERT INT, alongside a non-embedding baseline \\cite{fanourakis2022}.\n\n4.  **Key Empirical Findings**\n    *   The analysis established a statistically significant ranking of embedding methods based on their effectiveness across all real-world KGs in the testbed \\cite{fanourakis2022}.\n    *   Statistically significant correlations were discovered between method performance and various meta-features of the datasets, such as KG density and factual information richness \\cite{fanourakis2022}.\n    *   Unsupervised (AttrE) and semi-supervised (KDCoE) methods exploiting literal similarity outperformed supervised relation-based methods (RDGCN) on datasets with decreasing density but rich factual information \\cite{fanourakis2022}.\n    *   The study identified interesting trade-offs between the effectiveness and efficiency (runtime overhead) of different entity alignment methods \\cite{fanourakis2022}.\n\n5.  **Statistical Analysis**\n    *   The researchers applied a meta-level analysis to identify statistically significant correlations between method performance and dataset characteristics \\cite{fanourakis2022}. For method ranking, they used the non-parametric Friedman test followed by the post-hoc Nemenyi test for pairwise comparisons, ensuring a statistically sound assessment of effectiveness across datasets \\cite{fanourakis2022}.\n\n6.  **Validity & Limitations**\n    *   The study enhanced external validity by using an extended testbed of diverse real-world KGs and a broader selection of state-of-the-art methods compared to previous benchmarks \\cite{fanourakis2022}. A potential limitation is that the findings are specific to the evaluated set of \"popular\" embedding methods and KG characteristics, though chosen to be representative \\cite{fanourakis2022}.\n\n7.  **Empirical Contribution**\n    *   This work provides the first meta-level analysis of popular KG embedding methods for entity alignment, offering new empirical knowledge on their strengths, weaknesses, and sensitivities to KG characteristics \\cite{fanourakis2022}. These findings contribute to theory by clarifying method applicability and inform practice by guiding the selection of appropriate EA methods based on dataset properties and efficiency requirements \\cite{fanourakis2022}.",
      "intriguing_abstract": "Navigating the complex landscape of Knowledge Graph (KG) entity alignment (EA) methods is paramount for effective data integration. This paper presents the *first meta-level empirical analysis* of state-of-the-art KG embedding methods, moving beyond conventional benchmarks to provide a deeper understanding of their real-world performance. We rigorously evaluate popular supervised, unsupervised, and semi-supervised techniques (e.g., RDGCN, AttrE, KDCoE) across an *extended testbed of diverse real-world KGs*, employing a statistically sound methodology.\n\nOur findings reveal a *statistically significant ranking* of methods and, crucially, *uncover significant correlations* between method performance and KG meta-features like density and factual richness. Intriguingly, we demonstrate that *unsupervised and semi-supervised methods leveraging literal similarity can outperform supervised relation-based approaches* on datasets with decreasing density but rich factual information. Furthermore, we delineate critical effectiveness-efficiency trade-offs. This work provides pivotal empirical knowledge, clarifying method applicability and offering practical guidance for selecting optimal EA strategies based on specific dataset characteristics and computational constraints.",
      "keywords": [
        "entity alignment",
        "knowledge graphs (KGs)",
        "embedding methods",
        "relation-based and attribute-based embeddings",
        "meta-level analysis",
        "empirical comparison",
        "effectiveness and efficiency trade-offs",
        "KG meta-features",
        "statistically significant ranking",
        "non-parametric tests",
        "supervised",
        "unsupervised",
        "semi-supervised methods",
        "method sensitivity to dataset characteristics",
        "guiding EA method selection"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/84aa127dc5ca3080385439cb10edc50b5d2c04e4.pdf",
      "citation_key": "fanourakis2022",
      "metadata": {
        "title": "Knowledge graph embedding methods for entity alignment: experimental review",
        "authors": [
          "N. Fanourakis",
          "Vasilis Efthymiou",
          "D. Kotzinos",
          "V. Christophides"
        ],
        "published_date": "2022",
        "abstract": "In recent years, we have witnessed the proliferation of knowledge graphs (KG) in various domains, aiming to support applications like question answering, recommendations, etc. A frequent task when integrating knowledge from different KGs is to find which subgraphs refer to the same real-world entity, a task largely known as the Entity Alignment. Recently, embedding methods have been used for entity alignment tasks, that learn a vector-space representation of entities which preserves their similarity in the original KGs. A wide variety of supervised, unsupervised, and semi-supervised methods have been proposed that exploit both factual (attribute based) and structural information (relation based) of entities in the KGs. Still, a quantitative assessment of their strengths and weaknesses in real-world KGs according to different performance metrics and KG characteristics is missing from the literature. In this work, we conduct the first meta-level analysis of popular embedding methods for entity alignment, based on a statistically sound methodology. Our analysis reveals statistically significant correlations of different embedding methods with various meta-features extracted by KGs and rank them in a statistically significant way according to their effectiveness across all real-world KGs of our testbed. Finally, we study interesting trade-offs in terms of methods effectiveness and efficiency.",
        "file_path": "paper_data/knowledge_graph_embedding/84aa127dc5ca3080385439cb10edc50b5d2c04e4.pdf",
        "venue": "Data mining and knowledge discovery",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the empirical study by \\cite{fanourakis2022} for a literature review:\n\n1.  **Research Questions & Hypotheses**\n    *   The study investigates critical factors affecting relation-based and attribute-based embedding methods (Q1), the effectiveness improvement from combining structural and attribute information (Q2), the trade-off between effectiveness and efficiency (Q3), and method sensitivity to dataset characteristics (Q4) \\cite{fanourakis2022}.\n    *   It implicitly hypothesizes statistically significant correlations between embedding methods and KG meta-features, a statistically significant ranking of methods by effectiveness, and the existence of interesting effectiveness vs. efficiency trade-offs \\cite{fanourakis2022}.\n\n2.  **Study Design & Methodology**\n    *   The study employs a meta-level analysis, conducting a fair empirical comparison of state-of-the-art embedding-based entity alignment methods across an extended testbed of real-world knowledge graphs \\cite{fanourakis2022}. Data collection involved evaluating methods on diverse KG characteristics, and analysis used a statistically sound methodology including non-parametric tests for ranking and correlation analysis \\cite{fanourakis2022}.\n\n3.  **Data & Participants**\n    *   The study utilized an extended testbed of real-world knowledge graph (KG) datasets, including five additional datasets beyond those in prior benchmarks, featuring diverse characteristics like KG density, entity naming, and textual descriptions \\cite{fanourakis2022}. It evaluated a range of popular supervised, unsupervised, and semi-supervised embedding methods, such as MTransE, RDGCN, AttrE, KDCoE, and BERT INT, alongside a non-embedding baseline \\cite{fanourakis2022}.\n\n4.  **Key Empirical Findings**\n    *   The analysis established a statistically significant ranking of embedding methods based on their effectiveness across all real-world KGs in the testbed \\cite{fanourakis2022}.\n    *   Statistically significant correlations were discovered between method performance and various meta-features of the datasets, such as KG density and factual information richness \\cite{fanourakis2022}.\n    *   Unsupervised (AttrE) and semi-supervised (KDCoE) methods exploiting literal similarity outperformed supervised relation-based methods (RDGCN) on datasets with decreasing density but rich factual information \\cite{fanourakis2022}.\n    *   The study identified interesting trade-offs between the effectiveness and efficiency (runtime overhead) of different entity alignment methods \\cite{fanourakis2022}.\n\n5.  **Statistical Analysis**\n    *   The researchers applied a meta-level analysis to identify statistically significant correlations between method performance and dataset characteristics \\cite{fanourakis2022}. For method ranking, they used the non-parametric Friedman test followed by the post-hoc Nemenyi test for pairwise comparisons, ensuring a statistically sound assessment of effectiveness across datasets \\cite{fanourakis2022}.\n\n6.  **Validity & Limitations**\n    *   The study enhanced external validity by using an extended testbed of diverse real-world KGs and a broader selection of state-of-the-art methods compared to previous benchmarks \\cite{fanourakis2022}. A potential limitation is that the findings are specific to the evaluated set of \"popular\" embedding methods and KG characteristics, though chosen to be representative \\cite{fanourakis2022}.\n\n7.  **Empirical Contribution**\n    *   This work provides the first meta-level analysis of popular KG embedding methods for entity alignment, offering new empirical knowledge on their strengths, weaknesses, and sensitivities to KG characteristics \\cite{fanourakis2022}. These findings contribute to theory by clarifying method applicability and inform practice by guiding the selection of appropriate EA methods based on dataset properties and efficiency requirements \\cite{fanourakis2022}.",
        "keywords": [
          "entity alignment",
          "knowledge graphs (KGs)",
          "embedding methods",
          "relation-based and attribute-based embeddings",
          "meta-level analysis",
          "empirical comparison",
          "effectiveness and efficiency trade-offs",
          "KG meta-features",
          "statistically significant ranking",
          "non-parametric tests",
          "supervised",
          "unsupervised",
          "semi-supervised methods",
          "method sensitivity to dataset characteristics",
          "guiding EA method selection"
        ],
        "is_new_direction": "1",
        "paper_type": "empirical"
      },
      "file_name": "84aa127dc5ca3080385439cb10edc50b5d2c04e4.pdf"
    },
    {
      "success": true,
      "doc_id": "604e1f114928eef51e575b84bfa88f54",
      "summary": "Here's a focused summary of the paper \"Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding\" \\cite{wang2018} for a literature review:\n\n---\n\n### Focused Summary for Literature Review: Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding \\cite{wang2018}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of **inductive knowledge graph embedding**, specifically how to model and embed *emerging entities* (new entities not seen during training) in knowledge graphs (KGs).\n    *   **Importance and Challenge**: KGs are dynamic, with new entities emerging daily (e.g., 200 new entities on DBpedia daily). Traditional KG embedding models are *transductive*, requiring all entities to be present during training, making retraining from scratch for every new entity infeasible. Existing inductive methods that use neighborhood aggregators often neglect the *unordered* and *unequal* nature of an entity's neighbors, leading to suboptimal embeddings for new entities.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Transductive KG Embedding**: Acknowledges the success of models like TransE, Distmult, Complex, but highlights their fundamental limitation in handling emerging entities.\n        *   **Inductive KG Embedding (Text/Image-based)**: Mentions approaches using description text or images (e.g., Xie et al. 2016b), but notes their limitations in inferring implicit facts or when only partial facts (not text/images) are available.\n        *   **GNNs for KGs**: Refers to Hamaguchi et al. (2017) which applies Graph Neural Networks (GNNs) to KGs for inductive embedding.\n        *   **Node Representation for Homogeneous Graphs**: Draws inspiration from works like Hamilton, Ying, and Leskovec (2017a) that use neighborhood aggregation for inductive node embedding in homogeneous graphs.\n    *   **Limitations of Previous Solutions**:\n        *   Transductive models cannot generalize to unseen entities.\n        *   Text/image-based inductive models may not capture implicit facts or handle partial fact inputs.\n        *   GNN-based aggregators for KGs (e.g., Hamaguchi et al. 2017) use simple pooling functions, *neglecting the differences and importance of individual neighbors*.\n        *   Homogeneous graph aggregators either treat neighbors equally or require them to be ordered, violating permutation invariance, and cannot be directly applied to multi-relational KGs.\n        *   Previous aggregators generally fail to satisfy all three desired properties: Permutation Invariance, Redundancy Awareness, and Query Relation Awareness.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{wang2018} proposes a novel neighborhood aggregator called **Logic Attention Network (LAN)** within an encoder-decoder framework for inductive KG embedding. The encoder uses LAN to generate entity embeddings by aggregating neighbor information, and a decoder measures triplet plausibility.\n    *   **Novelty/Difference**:\n        *   **Addresses Desired Properties**: LAN is designed to inherently satisfy three key properties for effective KG aggregators:\n            1.  **Permutation Invariant**: Aggregates neighbors via a weighted combination, making the order irrelevant.\n            2.  **Redundancy Aware**: Exploits dependencies between facts and relations in the neighborhood.\n            3.  **Query Relation Aware**: Utilizes the target query relation to focus on relevant facts in the neighborhood.\n        *   **Double-View Attention Mechanism**: Estimates attention weights for neighbors using two complementary mechanisms:\n            *   **Logic Rule Mechanism**: Assigns weights based on global statistical dependencies between neighboring relations and the query relation (e.g., `P(r -> q)`), promoting relations strongly implying `q` and demoting redundant ones. This operates at a *coarse, relation-level granularity*.\n            *   **Neural Network Mechanism**: Uses a standard attention network (Bahdanau, Cho, and Bengio 2015) to compute fine-grained attention weights based on transformed neighbor embeddings and a relation-specific attention parameter. This operates at a *fine, neighbor-level granularity*.\n        *   The final attention weight for each neighbor is a sum of contributions from both mechanisms, allowing for a comprehensive understanding of neighbor importance.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Formalization of three crucial desired properties for effective neighborhood aggregators in inductive KG embedding: Permutation Invariance, Redundancy Awareness, and Query Relation Awareness.\n        *   Introduction of the **Logic Attention Network (LAN)**, a novel neighborhood aggregator.\n        *   Development of a **double-view attention mechanism** combining a Logic Rule Mechanism (for relation-level, redundancy-aware, query-aware weighting) and a Neural Network Mechanism (for fine-grained, neighbor-level weighting).\n    *   **System Design/Architectural Innovations**: An encoder-decoder framework where LAN serves as the core encoder for generating inductive entity embeddings.\n    *   **Theoretical Insights/Analysis**: The paper provides a principled way to incorporate logical dependencies and fine-grained neural attention into neighborhood aggregation for KGs.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{wang2018} conducted extensive comparisons with conventional aggregators. The evaluation was performed on **two knowledge graph completion tasks**. (Specific datasets and detailed metrics are not fully provided in the abstract, but typically involve link prediction and triplet classification).\n    *   **Key Performance Metrics and Comparison Results**: The experimental results **validate LAN's superiority** in terms of the desired properties (Permutation Invariance, Redundancy Awareness, Query Relation Awareness) when compared against conventional aggregators.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper focuses on inductive embedding for *emerging entities* that have *some existing neighbors*. It assumes the availability of partial facts for new entities to leverage their neighborhood. The specific datasets used for evaluation are not fully detailed in the provided abstract, which might imply a scope limited to certain KG characteristics.\n    *   **Scope of Applicability**: Primarily applicable to scenarios where KGs are dynamic and new entities frequently emerge, requiring efficient inductive embedding without full retraining. It is designed for multi-relational KGs, differentiating it from homogeneous graph embedding methods.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{wang2018} significantly advances the state-of-the-art in inductive KG embedding by proposing a principled and effective neighborhood aggregation mechanism that explicitly addresses critical properties often overlooked by previous methods. By incorporating both logical rules and neural attention, LAN provides a more robust and intelligent way to represent emerging entities.\n    *   **Potential Impact on Future Research**: This work opens avenues for future research in:\n        *   Developing more sophisticated attention mechanisms for multi-relational graphs.\n        *   Exploring how to integrate other forms of external knowledge (beyond logical rules) into inductive embedding.\n        *   Improving the handling of very sparse neighborhoods for emerging entities.\n        *   Applying similar attention-based aggregation strategies to other graph-structured data problems beyond KGs.",
      "intriguing_abstract": "Knowledge graphs are dynamic, with new entities emerging daily, yet traditional transductive embedding models are inherently incapable of generalizing to these unseen entities without costly retraining. Existing inductive methods, often relying on simplistic neighborhood aggregators, fail to capture the nuanced, multi-relational context of an entity's neighborhood.\n\nWe introduce the **Logic Attention Network (LAN)**, a novel neighborhood aggregator designed for robust **inductive knowledge graph embedding** of **emerging entities**. LAN uniquely addresses three critical properties for effective aggregation: **Permutation Invariance**, **Redundancy Awareness**, and **Query Relation Awareness**. Our core innovation is a **double-view attention mechanism** that synergistically combines a **Logic Rule Mechanism**, which leverages global statistical dependencies between relations, with a **Neural Network Mechanism** for fine-grained, neighbor-level weighting. This dual approach allows LAN to intelligently discern and aggregate the most relevant neighborhood information. Experiments on **knowledge graph completion** tasks demonstrate LAN's superior performance against conventional aggregators, marking a significant advancement in handling dynamic KGs and paving the way for more intelligent graph representation learning.",
      "keywords": [
        "inductive knowledge graph embedding",
        "emerging entities",
        "Logic Attention Network (LAN)",
        "neighborhood aggregation",
        "double-view attention mechanism",
        "Permutation Invariance",
        "Redundancy Awareness",
        "Query Relation Awareness",
        "Logic Rule Mechanism",
        "Neural Network Mechanism",
        "encoder-decoder framework"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf",
      "citation_key": "wang2018",
      "metadata": {
        "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
        "authors": [
          "Peifeng Wang",
          "Jialong Han",
          "Chenliang Li",
          "Rong Pan"
        ],
        "published_date": "2018",
        "abstract": "Knowledge graph embedding aims at modeling entities and relations with low-dimensional vectors. Most previous methods require that all entities should be seen during training, which is unpractical for real-world knowledge graphs with new entities emerging on a daily basis. Recent efforts on this issue suggest training a neighborhood aggregator in conjunction with the conventional entity and relation embeddings, which may help embed new entities inductively via their existing neighbors. However, their neighborhood aggregators neglect the unordered and unequal natures of an entitys neighbors. To this end, we summarize the desired properties that may lead to effective neighborhood aggregators. We also introduce a novel aggregator, namely, Logic Attention Network (LAN), which addresses the properties by aggregating neighbors with both rules- and network-based attention weights. By comparing with conventional aggregators on two knowledge graph completion tasks, we experimentally validate LANs superiority in terms of the desired properties.",
        "file_path": "paper_data/knowledge_graph_embedding/727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding\" \\cite{wang2018} for a literature review:\n\n---\n\n### Focused Summary for Literature Review: Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding \\cite{wang2018}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of **inductive knowledge graph embedding**, specifically how to model and embed *emerging entities* (new entities not seen during training) in knowledge graphs (KGs).\n    *   **Importance and Challenge**: KGs are dynamic, with new entities emerging daily (e.g., 200 new entities on DBpedia daily). Traditional KG embedding models are *transductive*, requiring all entities to be present during training, making retraining from scratch for every new entity infeasible. Existing inductive methods that use neighborhood aggregators often neglect the *unordered* and *unequal* nature of an entity's neighbors, leading to suboptimal embeddings for new entities.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Transductive KG Embedding**: Acknowledges the success of models like TransE, Distmult, Complex, but highlights their fundamental limitation in handling emerging entities.\n        *   **Inductive KG Embedding (Text/Image-based)**: Mentions approaches using description text or images (e.g., Xie et al. 2016b), but notes their limitations in inferring implicit facts or when only partial facts (not text/images) are available.\n        *   **GNNs for KGs**: Refers to Hamaguchi et al. (2017) which applies Graph Neural Networks (GNNs) to KGs for inductive embedding.\n        *   **Node Representation for Homogeneous Graphs**: Draws inspiration from works like Hamilton, Ying, and Leskovec (2017a) that use neighborhood aggregation for inductive node embedding in homogeneous graphs.\n    *   **Limitations of Previous Solutions**:\n        *   Transductive models cannot generalize to unseen entities.\n        *   Text/image-based inductive models may not capture implicit facts or handle partial fact inputs.\n        *   GNN-based aggregators for KGs (e.g., Hamaguchi et al. 2017) use simple pooling functions, *neglecting the differences and importance of individual neighbors*.\n        *   Homogeneous graph aggregators either treat neighbors equally or require them to be ordered, violating permutation invariance, and cannot be directly applied to multi-relational KGs.\n        *   Previous aggregators generally fail to satisfy all three desired properties: Permutation Invariance, Redundancy Awareness, and Query Relation Awareness.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{wang2018} proposes a novel neighborhood aggregator called **Logic Attention Network (LAN)** within an encoder-decoder framework for inductive KG embedding. The encoder uses LAN to generate entity embeddings by aggregating neighbor information, and a decoder measures triplet plausibility.\n    *   **Novelty/Difference**:\n        *   **Addresses Desired Properties**: LAN is designed to inherently satisfy three key properties for effective KG aggregators:\n            1.  **Permutation Invariant**: Aggregates neighbors via a weighted combination, making the order irrelevant.\n            2.  **Redundancy Aware**: Exploits dependencies between facts and relations in the neighborhood.\n            3.  **Query Relation Aware**: Utilizes the target query relation to focus on relevant facts in the neighborhood.\n        *   **Double-View Attention Mechanism**: Estimates attention weights for neighbors using two complementary mechanisms:\n            *   **Logic Rule Mechanism**: Assigns weights based on global statistical dependencies between neighboring relations and the query relation (e.g., `P(r -> q)`), promoting relations strongly implying `q` and demoting redundant ones. This operates at a *coarse, relation-level granularity*.\n            *   **Neural Network Mechanism**: Uses a standard attention network (Bahdanau, Cho, and Bengio 2015) to compute fine-grained attention weights based on transformed neighbor embeddings and a relation-specific attention parameter. This operates at a *fine, neighbor-level granularity*.\n        *   The final attention weight for each neighbor is a sum of contributions from both mechanisms, allowing for a comprehensive understanding of neighbor importance.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Formalization of three crucial desired properties for effective neighborhood aggregators in inductive KG embedding: Permutation Invariance, Redundancy Awareness, and Query Relation Awareness.\n        *   Introduction of the **Logic Attention Network (LAN)**, a novel neighborhood aggregator.\n        *   Development of a **double-view attention mechanism** combining a Logic Rule Mechanism (for relation-level, redundancy-aware, query-aware weighting) and a Neural Network Mechanism (for fine-grained, neighbor-level weighting).\n    *   **System Design/Architectural Innovations**: An encoder-decoder framework where LAN serves as the core encoder for generating inductive entity embeddings.\n    *   **Theoretical Insights/Analysis**: The paper provides a principled way to incorporate logical dependencies and fine-grained neural attention into neighborhood aggregation for KGs.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{wang2018} conducted extensive comparisons with conventional aggregators. The evaluation was performed on **two knowledge graph completion tasks**. (Specific datasets and detailed metrics are not fully provided in the abstract, but typically involve link prediction and triplet classification).\n    *   **Key Performance Metrics and Comparison Results**: The experimental results **validate LAN's superiority** in terms of the desired properties (Permutation Invariance, Redundancy Awareness, Query Relation Awareness) when compared against conventional aggregators.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper focuses on inductive embedding for *emerging entities* that have *some existing neighbors*. It assumes the availability of partial facts for new entities to leverage their neighborhood. The specific datasets used for evaluation are not fully detailed in the provided abstract, which might imply a scope limited to certain KG characteristics.\n    *   **Scope of Applicability**: Primarily applicable to scenarios where KGs are dynamic and new entities frequently emerge, requiring efficient inductive embedding without full retraining. It is designed for multi-relational KGs, differentiating it from homogeneous graph embedding methods.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{wang2018} significantly advances the state-of-the-art in inductive KG embedding by proposing a principled and effective neighborhood aggregation mechanism that explicitly addresses critical properties often overlooked by previous methods. By incorporating both logical rules and neural attention, LAN provides a more robust and intelligent way to represent emerging entities.\n    *   **Potential Impact on Future Research**: This work opens avenues for future research in:\n        *   Developing more sophisticated attention mechanisms for multi-relational graphs.\n        *   Exploring how to integrate other forms of external knowledge (beyond logical rules) into inductive embedding.\n        *   Improving the handling of very sparse neighborhoods for emerging entities.\n        *   Applying similar attention-based aggregation strategies to other graph-structured data problems beyond KGs.",
        "keywords": [
          "inductive knowledge graph embedding",
          "emerging entities",
          "Logic Attention Network (LAN)",
          "neighborhood aggregation",
          "double-view attention mechanism",
          "Permutation Invariance",
          "Redundancy Awareness",
          "Query Relation Awareness",
          "Logic Rule Mechanism",
          "Neural Network Mechanism",
          "encoder-decoder framework"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf"
    },
    {
      "success": true,
      "doc_id": "5f04caed0d252452f88062aadd83da1a",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Knowledge Graph Embedding (KGE) models, despite their widespread use, overlook the problem of probability calibration \\cite{tabacof2019}.\n    *   Existing KGE models are shown to be uncalibrated, meaning their predicted probabilities for triples are unreliable (e.g., a prediction of 80% confidence doesn't correspond to being correct 80% of the time) \\cite{tabacof2019}.\n    *   This unreliability is critical in high-stakes applications (e.g., drug-target discovery) where trustworthy and interpretable decisions are needed \\cite{tabacof2019}.\n    *   Uncalibrated models necessitate defining relation-specific decision thresholds for triple classification, which is cumbersome for graphs with many relation types \\cite{tabacof2019}.\n\n*   **Related Work & Positioning**\n    *   The paper acknowledges extensive research in KGE models (e.g., TransE, DistMult, ComplEx, HolE, ConvE, RotatE, etc.) but highlights that these models do not address the reliability of their predictions or probability calibration \\cite{tabacof2019}.\n    *   General probability calibration techniques like Platt scaling and isotonic regression are well-established, and recent work has applied them to modern neural networks in classification and regression (e.g., temperature scaling for classification, Platt scaling for deep regression) \\cite{tabacof2019}.\n    *   However, systematic application of these methods to KGE models, especially in the common scenario where ground truth negatives are unavailable, has been largely overlooked. Previous work that mentions calibration (e.g., Knowledge Vault, KG2E, Krompa & Tresp (2015)) either doesn't apply it directly to KGE models' outputs or lacks details on handling the absence of negatives \\cite{tabacof2019}.\n    *   This work is presented as the first to specifically focus on calibration for knowledge graph embeddings, particularly addressing the challenge of missing ground truth negatives \\cite{tabacof2019}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes two scenario-dependent calibration techniques:\n        1.  **Calibration with Ground Truth Negatives**: For datasets where ground truth negatives are available (e.g., triple classification datasets), standard Platt scaling and isotonic regression are directly applied \\cite{tabacof2019}.\n        2.  **Calibration with Synthetic Negatives (Main Contribution)**: For the more common scenario in KGs (e.g., link prediction tasks) where ground truth negatives are absent, a novel calibration heuristic is introduced \\cite{tabacof2019}.\n    *   **Innovation for Synthetic Negatives**: This heuristic combines Platt scaling or isotonic regression with synthetically generated corrupted triples as negatives \\cite{tabacof2019}.\n    *   **Weighting Scheme**: To ensure the calibrated model adheres to the true population positive base rate ($\\pi$), a novel weighting scheme is proposed for positive and synthetic negative triples:\n        *   Weight for positive triples ($w_+$) = $\\pi$\n        *   Weight for negative triples ($w_-$) = $(1-\\pi) / (\\text{corruption rate})$\n        *   This scheme ensures that the base rate in the calibration process matches the user-specified population base rate, preventing it from being arbitrarily influenced by the number of generated synthetic negatives \\cite{tabacof2019}.\n\n*   **Key Technical Contributions**\n    *   **Novel Calibration Heuristic**: A method to calibrate KGE models when ground truth negatives are not available, which is a common challenge in knowledge graphs \\cite{tabacof2019}.\n    *   **Weighted Synthetic Negatives**: Introduction of a specific weighting scheme for synthetically generated negatives to ensure the calibration process respects the true population positive base rate \\cite{tabacof2019}.\n    *   **Empirical Demonstration of Miscalibration**: First systematic demonstration that popular KGE models (TransE, ComplEx, DistMult, HolE) are indeed uncalibrated across various loss functions and datasets \\cite{tabacof2019}.\n\n*   **Experimental Validation**\n    *   **Datasets**: Experiments were conducted on three triple classification datasets with ground truth negatives (WN11, FB13, YAGO39K) and two link prediction datasets without ground truth negatives (WN18RR, FB15K-237) \\cite{tabacof2019}.\n    *   **Models**: Four popular KGE models were used: TransE, DistMult, ComplEx, and HolE \\cite{tabacof2019}.\n    *   **Loss Functions**: Models were trained with four different loss functions: Self-adversarial, pairwise, NLL, and Multiclass-NLL \\cite{tabacof2019}.\n    *   **Metrics**: Calibration quality was assessed using Brier scores and log losses. Triple classification performance was evaluated using accuracy \\cite{tabacof2019}.\n    *   **Key Results**:\n        *   All proposed calibration methods significantly improved calibration quality (lower Brier scores and log losses) compared to uncalibrated models across all datasets and models \\cite{tabacof2019}.\n        *   The synthetic negative calibration method performed remarkably well, often approaching the performance of calibration with ground truth negatives, demonstrating its effectiveness in real-world scenarios \\cite{tabacof2019}.\n        *   Isotonic regression generally offered the best calibration performance, although with practical trade-offs (non-convex/differentiable) compared to Platt scaling \\cite{tabacof2019}.\n        *   Calibrated models achieved state-of-the-art accuracy in triple classification *without the need for relation-specific decision thresholds*, simplifying the classification process \\cite{tabacof2019}.\n        *   Self-adversarial loss generally yielded the best calibration results among the tested loss functions \\cite{tabacof2019}.\n\n*   **Limitations & Scope**\n    *   Isotonic regression, while performing better, is not a convex or differentiable algorithm, making its integration into mini-batch based deep learning optimization challenging compared to Platt scaling \\cite{tabacof2019}.\n    *   The synthetic calibration method requires a user-specified positive base rate ($\\pi$), which might not always be precisely known \\cite{tabacof2019}.\n    *   The scope of the analysis was limited to four popular KGE models (TransE, DistMult, ComplEx, HolE) and specific corruption strategies for synthetic negatives \\cite{tabacof2019}.\n\n*   **Technical Significance**\n    *   This work addresses a fundamental oversight in KGE research, significantly improving the reliability and interpretability of KGE model predictions \\cite{tabacof2019}.\n    *   It provides a practical solution for calibrating KGE models even in the common absence of ground truth negatives, expanding the applicability of calibrated predictions \\cite{tabacof2019}.\n    *   By enabling calibrated probabilities, the paper eliminates the need for cumbersome relation-specific decision thresholds in triple classification, streamlining the deployment of KGE models in downstream tasks \\cite{tabacof2019}.\n    *   The findings pave the way for more trustworthy AI systems built upon knowledge graphs, particularly in sensitive domains where prediction confidence is paramount \\cite{tabacof2019}.",
      "intriguing_abstract": "Despite their widespread adoption, Knowledge Graph Embedding (KGE) models suffer from a critical, often overlooked, flaw: their predicted probabilities are uncalibrated. This means a KGE model's 80% confidence might not correspond to being correct 80% of the time, undermining trustworthiness in high-stakes applications like drug discovery and necessitating cumbersome relation-specific decision thresholds.\n\nThis paper systematically demonstrates the pervasive uncalibration across popular KGE models (TransE, ComplEx, DistMult, HolE) and loss functions. Crucially, we introduce the first dedicated probability calibration framework for KGEs, addressing the fundamental challenge of missing ground truth negatives in real-world knowledge graphs. Our novel heuristic combines established techniques like Platt scaling and isotonic regression with synthetically generated negatives, employing a unique weighting scheme to preserve the true population positive base rate.\n\nExtensive experiments on diverse datasets show our methods dramatically improve calibration quality, evidenced by significantly lower Brier scores and log losses. Remarkably, our synthetic negative approach rivals calibration with actual ground truth negatives. This breakthrough not only delivers unprecedented reliability and interpretability for KGE predictions but also enables state-of-the-art triple classification accuracy without the need for complex, relation-specific thresholds, paving the way for truly trustworthy AI systems built on knowledge graphs.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "probability calibration",
        "uncalibrated KGE predictions",
        "absence of ground truth negatives",
        "synthetic negatives",
        "novel calibration heuristic",
        "weighted synthetic negatives scheme",
        "Platt scaling",
        "isotonic regression",
        "empirical miscalibration demonstration",
        "improved calibration quality",
        "triple classification",
        "relation-specific decision thresholds",
        "trustworthy AI systems"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/19a672bdf29367b7509586a4be27c6843af903b1.pdf",
      "citation_key": "tabacof2019",
      "metadata": {
        "title": "Probability Calibration for Knowledge Graph Embedding Models",
        "authors": [
          "Pedro Tabacof",
          "Luca Costabello"
        ],
        "published_date": "2019",
        "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well-calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.",
        "file_path": "paper_data/knowledge_graph_embedding/19a672bdf29367b7509586a4be27c6843af903b1.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Knowledge Graph Embedding (KGE) models, despite their widespread use, overlook the problem of probability calibration \\cite{tabacof2019}.\n    *   Existing KGE models are shown to be uncalibrated, meaning their predicted probabilities for triples are unreliable (e.g., a prediction of 80% confidence doesn't correspond to being correct 80% of the time) \\cite{tabacof2019}.\n    *   This unreliability is critical in high-stakes applications (e.g., drug-target discovery) where trustworthy and interpretable decisions are needed \\cite{tabacof2019}.\n    *   Uncalibrated models necessitate defining relation-specific decision thresholds for triple classification, which is cumbersome for graphs with many relation types \\cite{tabacof2019}.\n\n*   **Related Work & Positioning**\n    *   The paper acknowledges extensive research in KGE models (e.g., TransE, DistMult, ComplEx, HolE, ConvE, RotatE, etc.) but highlights that these models do not address the reliability of their predictions or probability calibration \\cite{tabacof2019}.\n    *   General probability calibration techniques like Platt scaling and isotonic regression are well-established, and recent work has applied them to modern neural networks in classification and regression (e.g., temperature scaling for classification, Platt scaling for deep regression) \\cite{tabacof2019}.\n    *   However, systematic application of these methods to KGE models, especially in the common scenario where ground truth negatives are unavailable, has been largely overlooked. Previous work that mentions calibration (e.g., Knowledge Vault, KG2E, Krompa & Tresp (2015)) either doesn't apply it directly to KGE models' outputs or lacks details on handling the absence of negatives \\cite{tabacof2019}.\n    *   This work is presented as the first to specifically focus on calibration for knowledge graph embeddings, particularly addressing the challenge of missing ground truth negatives \\cite{tabacof2019}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes two scenario-dependent calibration techniques:\n        1.  **Calibration with Ground Truth Negatives**: For datasets where ground truth negatives are available (e.g., triple classification datasets), standard Platt scaling and isotonic regression are directly applied \\cite{tabacof2019}.\n        2.  **Calibration with Synthetic Negatives (Main Contribution)**: For the more common scenario in KGs (e.g., link prediction tasks) where ground truth negatives are absent, a novel calibration heuristic is introduced \\cite{tabacof2019}.\n    *   **Innovation for Synthetic Negatives**: This heuristic combines Platt scaling or isotonic regression with synthetically generated corrupted triples as negatives \\cite{tabacof2019}.\n    *   **Weighting Scheme**: To ensure the calibrated model adheres to the true population positive base rate ($\\pi$), a novel weighting scheme is proposed for positive and synthetic negative triples:\n        *   Weight for positive triples ($w_+$) = $\\pi$\n        *   Weight for negative triples ($w_-$) = $(1-\\pi) / (\\text{corruption rate})$\n        *   This scheme ensures that the base rate in the calibration process matches the user-specified population base rate, preventing it from being arbitrarily influenced by the number of generated synthetic negatives \\cite{tabacof2019}.\n\n*   **Key Technical Contributions**\n    *   **Novel Calibration Heuristic**: A method to calibrate KGE models when ground truth negatives are not available, which is a common challenge in knowledge graphs \\cite{tabacof2019}.\n    *   **Weighted Synthetic Negatives**: Introduction of a specific weighting scheme for synthetically generated negatives to ensure the calibration process respects the true population positive base rate \\cite{tabacof2019}.\n    *   **Empirical Demonstration of Miscalibration**: First systematic demonstration that popular KGE models (TransE, ComplEx, DistMult, HolE) are indeed uncalibrated across various loss functions and datasets \\cite{tabacof2019}.\n\n*   **Experimental Validation**\n    *   **Datasets**: Experiments were conducted on three triple classification datasets with ground truth negatives (WN11, FB13, YAGO39K) and two link prediction datasets without ground truth negatives (WN18RR, FB15K-237) \\cite{tabacof2019}.\n    *   **Models**: Four popular KGE models were used: TransE, DistMult, ComplEx, and HolE \\cite{tabacof2019}.\n    *   **Loss Functions**: Models were trained with four different loss functions: Self-adversarial, pairwise, NLL, and Multiclass-NLL \\cite{tabacof2019}.\n    *   **Metrics**: Calibration quality was assessed using Brier scores and log losses. Triple classification performance was evaluated using accuracy \\cite{tabacof2019}.\n    *   **Key Results**:\n        *   All proposed calibration methods significantly improved calibration quality (lower Brier scores and log losses) compared to uncalibrated models across all datasets and models \\cite{tabacof2019}.\n        *   The synthetic negative calibration method performed remarkably well, often approaching the performance of calibration with ground truth negatives, demonstrating its effectiveness in real-world scenarios \\cite{tabacof2019}.\n        *   Isotonic regression generally offered the best calibration performance, although with practical trade-offs (non-convex/differentiable) compared to Platt scaling \\cite{tabacof2019}.\n        *   Calibrated models achieved state-of-the-art accuracy in triple classification *without the need for relation-specific decision thresholds*, simplifying the classification process \\cite{tabacof2019}.\n        *   Self-adversarial loss generally yielded the best calibration results among the tested loss functions \\cite{tabacof2019}.\n\n*   **Limitations & Scope**\n    *   Isotonic regression, while performing better, is not a convex or differentiable algorithm, making its integration into mini-batch based deep learning optimization challenging compared to Platt scaling \\cite{tabacof2019}.\n    *   The synthetic calibration method requires a user-specified positive base rate ($\\pi$), which might not always be precisely known \\cite{tabacof2019}.\n    *   The scope of the analysis was limited to four popular KGE models (TransE, DistMult, ComplEx, HolE) and specific corruption strategies for synthetic negatives \\cite{tabacof2019}.\n\n*   **Technical Significance**\n    *   This work addresses a fundamental oversight in KGE research, significantly improving the reliability and interpretability of KGE model predictions \\cite{tabacof2019}.\n    *   It provides a practical solution for calibrating KGE models even in the common absence of ground truth negatives, expanding the applicability of calibrated predictions \\cite{tabacof2019}.\n    *   By enabling calibrated probabilities, the paper eliminates the need for cumbersome relation-specific decision thresholds in triple classification, streamlining the deployment of KGE models in downstream tasks \\cite{tabacof2019}.\n    *   The findings pave the way for more trustworthy AI systems built upon knowledge graphs, particularly in sensitive domains where prediction confidence is paramount \\cite{tabacof2019}.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "probability calibration",
          "uncalibrated KGE predictions",
          "absence of ground truth negatives",
          "synthetic negatives",
          "novel calibration heuristic",
          "weighted synthetic negatives scheme",
          "Platt scaling",
          "isotonic regression",
          "empirical miscalibration demonstration",
          "improved calibration quality",
          "triple classification",
          "relation-specific decision thresholds",
          "trustworthy AI systems"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "19a672bdf29367b7509586a4be27c6843af903b1.pdf"
    },
    {
      "success": true,
      "doc_id": "26a17bf60c412d744187208e444d60e7",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of entity alignment across different knowledge graphs, which involves identifying semantically identical entities.\n    *   **Importance & Challenge:**\n        *   Current knowledge graph embedding (KGE)-based solutions for entity alignment are limited by the scarcity and high cost of acquiring labeled (priorly aligned) entity pairs, leading to underutilization of abundant unlabeled data.\n        *   KGE performance is adversely affected by the degree difference among entities (i.e., high-frequency vs. low-frequency entities), which complicates accurate alignment.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Most recent entity alignment solutions rely on knowledge graph embedding, which maps entities into a low-dimensional space guided by known aligned pairs.\n    *   **Limitations of Previous Solutions:**\n        *   Existing methods heavily depend on expensive and difficult-to-acquire labeled data, neglecting the potential of abundant unlabeled information.\n        *   Previous KGE approaches struggle with entity degree differences, leading to inconsistent alignment accuracy across entities with varying frequencies.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a semi-supervised entity alignment method (SEA) \\cite{pei2019}.\n    *   **Novelty:**\n        *   SEA innovatively leverages both limited labeled entity pairs and abundant unlabeled entity information for alignment.\n        *   It enhances knowledge graph embedding by explicitly incorporating awareness of entity degree differences through the application of adversarial training.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of the Semi-supervised Entity Alignment (SEA) method \\cite{pei2019}.\n    *   **Techniques:** Improvement of knowledge graph embedding by integrating degree difference awareness, achieved through an adversarial training framework.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on real-world datasets.\n    *   **Key Performance Metrics & Comparison Results:** The proposed SEA model consistently and significantly outperforms state-of-the-art methods in terms of alignment accuracy \\cite{pei2019}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The provided abstract does not detail specific technical limitations or assumptions of the proposed SEA method itself.\n    *   **Scope of Applicability:** The method is designed for entity alignment in knowledge graphs, validated on real-world datasets.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** The work advances the technical state-of-the-art by addressing two critical limitations of existing KGE-based entity alignment methods: data scarcity and sensitivity to entity degree differences. It achieves superior alignment accuracy compared to current methods \\cite{pei2019}.\n    *   **Potential Impact:** This research offers a more robust and data-efficient approach to entity alignment, which can significantly benefit knowledge graph construction, integration, and overall knowledge management by improving the quality and completeness of linked data.",
      "intriguing_abstract": "Integrating disparate knowledge graphs hinges on accurate entity alignment, a task severely hampered by the scarcity of labeled data and the inconsistent performance of knowledge graph embedding (KGE) methods across entities with varying degrees. We present SEA, a novel **semi-supervised entity alignment** method that fundamentally redefines how KGEs leverage information. SEA innovatively combines limited labeled pairs with abundant unlabeled data, overcoming the critical data bottleneck. Crucially, it enhances KGE by explicitly integrating **degree difference awareness** through a sophisticated **adversarial training** framework. This unique approach mitigates performance disparities between high- and low-frequency entities, leading to more robust and consistent alignments. Extensive experiments on real-world datasets demonstrate that SEA consistently and significantly outperforms state-of-the-art methods in alignment accuracy. Our work offers a data-efficient and highly effective solution, advancing the state-of-the-art in **knowledge graph integration** and promising substantial impact on **knowledge management** and linked data quality.",
      "keywords": [
        "entity alignment",
        "knowledge graphs",
        "knowledge graph embedding (KGE)",
        "semi-supervised entity alignment (SEA)",
        "adversarial training",
        "entity degree differences",
        "unlabeled data utilization",
        "labeled data scarcity",
        "alignment accuracy",
        "state-of-the-art performance",
        "robust entity alignment",
        "knowledge management"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/ecc04e9285f016090697a1a8f9e96ce01e94e742.pdf",
      "citation_key": "pei2019",
      "metadata": {
        "title": "Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference",
        "authors": [
          "Shichao Pei",
          "Lu Yu",
          "R. Hoehndorf",
          "Xiangliang Zhang"
        ],
        "published_date": "2019",
        "abstract": "Entity alignment associates entities in different knowledge graphs if they are semantically same, and has been successfully used in the knowledge graph construction and connection. Most of the recent solutions for entity alignment are based on knowledge graph embedding, which maps knowledge entities in a low-dimension space where entities are connected with the guidance of prior aligned entity pairs. The study in this paper focuses on two important issues that limit the accuracy of current entity alignment solutions: 1) labeled data of priorly aligned entity pairs are difficult and expensive to acquire, whereas abundant of unlabeled data are not used; and 2) knowledge graph embedding is affected by entity's degree difference, which brings challenges to align high frequent and low frequent entities. We propose a semi-supervised entity alignment method (SEA) to leverage both labeled entities and the abundant unlabeled entity information for the alignment. Furthermore, we improve the knowledge graph embedding with awareness of the degree difference by performing the adversarial training. To evaluate our proposed model, we conduct extensive experiments on real-world datasets. The experimental results show that our model consistently outperforms the state-of-the-art methods with significant improvement on alignment accuracy.",
        "file_path": "paper_data/knowledge_graph_embedding/ecc04e9285f016090697a1a8f9e96ce01e94e742.pdf",
        "venue": "The Web Conference",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of entity alignment across different knowledge graphs, which involves identifying semantically identical entities.\n    *   **Importance & Challenge:**\n        *   Current knowledge graph embedding (KGE)-based solutions for entity alignment are limited by the scarcity and high cost of acquiring labeled (priorly aligned) entity pairs, leading to underutilization of abundant unlabeled data.\n        *   KGE performance is adversely affected by the degree difference among entities (i.e., high-frequency vs. low-frequency entities), which complicates accurate alignment.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Most recent entity alignment solutions rely on knowledge graph embedding, which maps entities into a low-dimensional space guided by known aligned pairs.\n    *   **Limitations of Previous Solutions:**\n        *   Existing methods heavily depend on expensive and difficult-to-acquire labeled data, neglecting the potential of abundant unlabeled information.\n        *   Previous KGE approaches struggle with entity degree differences, leading to inconsistent alignment accuracy across entities with varying frequencies.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a semi-supervised entity alignment method (SEA) \\cite{pei2019}.\n    *   **Novelty:**\n        *   SEA innovatively leverages both limited labeled entity pairs and abundant unlabeled entity information for alignment.\n        *   It enhances knowledge graph embedding by explicitly incorporating awareness of entity degree differences through the application of adversarial training.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Introduction of the Semi-supervised Entity Alignment (SEA) method \\cite{pei2019}.\n    *   **Techniques:** Improvement of knowledge graph embedding by integrating degree difference awareness, achieved through an adversarial training framework.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were performed on real-world datasets.\n    *   **Key Performance Metrics & Comparison Results:** The proposed SEA model consistently and significantly outperforms state-of-the-art methods in terms of alignment accuracy \\cite{pei2019}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The provided abstract does not detail specific technical limitations or assumptions of the proposed SEA method itself.\n    *   **Scope of Applicability:** The method is designed for entity alignment in knowledge graphs, validated on real-world datasets.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art:** The work advances the technical state-of-the-art by addressing two critical limitations of existing KGE-based entity alignment methods: data scarcity and sensitivity to entity degree differences. It achieves superior alignment accuracy compared to current methods \\cite{pei2019}.\n    *   **Potential Impact:** This research offers a more robust and data-efficient approach to entity alignment, which can significantly benefit knowledge graph construction, integration, and overall knowledge management by improving the quality and completeness of linked data.",
        "keywords": [
          "entity alignment",
          "knowledge graphs",
          "knowledge graph embedding (KGE)",
          "semi-supervised entity alignment (SEA)",
          "adversarial training",
          "entity degree differences",
          "unlabeled data utilization",
          "labeled data scarcity",
          "alignment accuracy",
          "state-of-the-art performance",
          "robust entity alignment",
          "knowledge management"
        ],
        "is_new_direction": "0",
        "paper_type": "technical"
      },
      "file_name": "ecc04e9285f016090697a1a8f9e96ce01e94e742.pdf"
    },
    {
      "success": true,
      "doc_id": "63d39a32b598ecb01e1fd17b1091d137",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding \\cite{zhang2018}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Existing negative sampling methods (e.g., uniform, Bernoulli) for Knowledge Graph (KG) embedding suffer from the \"vanishing gradient problem\" \\cite{zhang2018}. This occurs because they frequently sample \"easy\" negative triplets (those with small scores), which provide little gradient signal, impeding effective training. While GAN-based methods (IGAN, KBGAN) address this by dynamically sampling high-quality (large score) negative triplets, they introduce significant complexity, instability, and require reinforcement learning and pretraining \\cite{zhang2018}.\n    *   **Importance and challenge**: KG embedding is fundamental for various downstream applications like link prediction and question answering. High-quality negative triplets are crucial for robust model training. The challenge lies in efficiently capturing the dynamic and highly skewed distribution of these \"hard\" negative triplets (where only a few have large scores) without adding substantial model complexity or training instability \\cite{zhang2018}.\n\n2.  **Related Work & Positioning**\n    *   **Existing approaches**:\n        *   **Fixed sampling schemes**: Uniform sampling (simple, efficient but prone to vanishing gradients) and Bernoulli sampling (improves uniform by considering relation types, but still fixed) \\cite{zhang2018}.\n        *   **Dynamic sampling schemes (GAN-based)**: IGAN and KBGAN utilize Generative Adversarial Networks (GANs) to dynamically generate high-quality negative triplets by modeling their distribution \\cite{zhang2018}.\n    *   **Limitations of previous solutions**:\n        *   Fixed schemes: Cannot adapt to the dynamic changes in negative triplet distributions during training, leading to vanishing gradients \\cite{zhang2018}.\n        *   GAN-based schemes: Increase model complexity with an extra generator, suffer from training instability and degeneracy, require high-variance REINFORCE gradients, necessitate pretraining, and waste computational resources learning the full (skewed) distribution of negative triplets, including the many \"useless\" small-score ones \\cite{zhang2018}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: NSCaching \\cite{zhang2018} proposes a simple and efficient cache-based negative sampling method. Motivated by the observation that high-score negative triplets are rare, it directly maintains and samples from a cache of these \"hard\" negative triplets for each positive triplet.\n    *   **Novelty/Difference**:\n        *   **Direct caching of hard negatives**: Unlike GANs that model the full distribution, NSCaching directly stores and updates a small set of high-quality negative triplets in a cache \\cite{zhang2018}.\n        *   **Importance Sampling (IS) for cache update**: A carefully designed Importance Sampling strategy is used to dynamically update the cache, ensuring it captures the evolving distribution of hard negatives efficiently \\cite{zhang2018}.\n        *   **Exploration-exploitation balance**: The method incorporates mechanisms to balance exploring new potential hard negatives and exploiting the currently known hard negatives stored in the cache \\cite{zhang2018}.\n        *   **\"Distilled\" GAN alternative**: NSCaching acts as a \"distilled\" version of GAN-based methods, achieving similar benefits (sampling hard negatives) without the additional parameters, training complexity, or reliance on reinforcement learning. It can be trained with standard gradient descent \\cite{zhang2018}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms/methods**:\n        *   NSCaching: A novel cache-based negative sampling scheme that is general and compatible with various KG embedding models \\cite{zhang2018}.\n        *   A uniform sampling strategy for selecting negative triplets directly from the cache.\n        *   An Importance Sampling (IS) strategy for dynamically updating the cache of hard negative triplets \\cite{zhang2018}.\n        *   A mechanism to balance exploration (finding new hard negatives) and exploitation (sampling from existing hard negatives in the cache) \\cite{zhang2018}.\n    *   **Theoretical insights/analysis**:\n        *   Empirical observation and analysis of the highly skewed score distribution of negative triplets, highlighting that only a few have large scores \\cite{zhang2018}.\n        *   Analysis connecting NSCaching to self-paced learning, demonstrating its ability to first learn from easily classified samples and then gradually switch to harder ones \\cite{zhang2018}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments were performed to evaluate NSCaching's effectiveness and efficiency. It was applied to various popular KG embedding models (e.g., TransE, TransH, TransD, DistMult, ComplEx, HolE) \\cite{zhang2018}.\n    *   **Key performance metrics and comparison results**:\n        *   **Datasets**: WN18, FB15K, and their variants WN18RR, FB15K237 \\cite{zhang2018}.\n        *   **Results**: NSCaching demonstrated significant performance improvements across various KG embedding models. It consistently outperformed state-of-the-art GAN-based negative sampling methods (IGAN and KBGAN) in terms of effectiveness, while also being more efficient \\cite{zhang2018}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions**: The core assumption is that high-score negative triplets are rare and can be effectively managed by a cache. The performance relies on the efficiency of the cache update mechanism and the balance between exploration and exploitation. The optimal cache size and update frequency might be hyper-parameters requiring tuning.\n    *   **Scope of applicability**: NSCaching is designed as a general negative sampling scheme that can be injected into all popularly used KG embedding models \\cite{zhang2018}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art**: NSCaching provides a simpler, more efficient, and more stable alternative to complex GAN-based methods for dynamic negative sampling in KG embedding \\cite{zhang2018}. It effectively addresses the vanishing gradient problem without introducing the training instabilities and parameter overhead associated with GANs, achieving superior performance.\n    *   **Potential impact on future research**: The work highlights the effectiveness of directly managing \"hard\" negative samples through caching, offering a practical paradigm shift from complex generative models. This approach could inspire similar efficient sampling strategies in other machine learning domains where identifying and leveraging challenging negative examples is crucial for model performance. Its simplicity and general applicability make it a strong candidate for widespread adoption in KG embedding research and applications \\cite{zhang2018}.",
      "intriguing_abstract": "Effective negative sampling is paramount for robust Knowledge Graph Embedding (KGE), yet current methods face significant hurdles. Fixed sampling schemes often succumb to the \"vanishing gradient problem\" by selecting trivial negatives, while complex GAN-based approaches, though dynamic, introduce substantial training instability, computational overhead, and reliance on reinforcement learning.\n\nWe introduce **NSCaching**, a novel and remarkably simple cache-based negative sampling scheme that elegantly resolves these challenges. Instead of modeling the entire negative triplet distribution, NSCaching directly maintains and dynamically updates a small, highly efficient cache of \"hard\" negative triplets using a carefully designed Importance Sampling strategy. This innovative approach strikes a crucial balance between exploring new challenging negatives and exploiting known ones, effectively acting as a 'distilled' GAN alternative without the need for complex generators, pretraining, or the instabilities associated with GANs.\n\nNSCaching significantly outperforms state-of-the-art GAN-based methods across various KGE models (e.g., TransE, DistMult) and datasets (WN18RR, FB15K237), demonstrating superior performance and computational efficiency. Its general applicability and stability make it a powerful tool for enhancing KGE training, offering a paradigm shift towards simpler, more effective dynamic negative sampling. This work provides a critical advancement for link prediction and other downstream applications, paving the way for more robust and scalable knowledge graph analysis.",
      "keywords": [
        "Knowledge Graph Embedding",
        "Negative Sampling",
        "Vanishing Gradient Problem",
        "NSCaching",
        "Cache-based negative sampling",
        "Hard negative triplets",
        "Importance Sampling",
        "Dynamic sampling",
        "Generative Adversarial Networks (GANs)",
        "Training instability",
        "Exploration-exploitation balance",
        "Self-paced learning",
        "Link prediction",
        "Efficiency and stability",
        "Superior performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/beade097ff41c62a8d8d29065be0e1339be39f30.pdf",
      "citation_key": "zhang2018",
      "metadata": {
        "title": "NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding",
        "authors": [
          "Yongqi Zhang",
          "Quanming Yao",
          "Yingxia Shao",
          "Lei Chen"
        ],
        "published_date": "2018",
        "abstract": "Knowledge graph (KG) embedding is a fundamental problem in data mining research with many real-world applications. It aims to encode the entities and relations in the graph into low dimensional vector space, which can be used for subsequent algorithms. Negative sampling, which samples negative triplets from non-observed ones in the training data, is an important step in KG embedding. Recently, generative adversarial network (GAN), has been introduced in negative sampling. By sampling negative triplets with large scores, these methods avoid the problem of vanishing gradient and thus obtain better performance. However, using GAN makes the original model more complex and harder to train, where reinforcement learning must be used. In this paper, motivated by the observation that negative triplets with large scores are important but rare, we propose to directly keep track of them with cache. However, how to sample from and update the cache are two important questions. We carefully design the solutions, which are not only efficient but also achieve good balance between exploration and exploitation. In this way, our method acts as a \"distilled\" version of previous GAN-based methods, which does not waste training time on additional parameters to fit the full distribution of negative triplets. The extensive experiments show that our method can gain significant improvement on various KG embedding models, and outperform the state-of-the-arts negative sampling methods based on GAN.",
        "file_path": "paper_data/knowledge_graph_embedding/beade097ff41c62a8d8d29065be0e1339be39f30.pdf",
        "venue": "IEEE International Conference on Data Engineering",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding \\cite{zhang2018}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Existing negative sampling methods (e.g., uniform, Bernoulli) for Knowledge Graph (KG) embedding suffer from the \"vanishing gradient problem\" \\cite{zhang2018}. This occurs because they frequently sample \"easy\" negative triplets (those with small scores), which provide little gradient signal, impeding effective training. While GAN-based methods (IGAN, KBGAN) address this by dynamically sampling high-quality (large score) negative triplets, they introduce significant complexity, instability, and require reinforcement learning and pretraining \\cite{zhang2018}.\n    *   **Importance and challenge**: KG embedding is fundamental for various downstream applications like link prediction and question answering. High-quality negative triplets are crucial for robust model training. The challenge lies in efficiently capturing the dynamic and highly skewed distribution of these \"hard\" negative triplets (where only a few have large scores) without adding substantial model complexity or training instability \\cite{zhang2018}.\n\n2.  **Related Work & Positioning**\n    *   **Existing approaches**:\n        *   **Fixed sampling schemes**: Uniform sampling (simple, efficient but prone to vanishing gradients) and Bernoulli sampling (improves uniform by considering relation types, but still fixed) \\cite{zhang2018}.\n        *   **Dynamic sampling schemes (GAN-based)**: IGAN and KBGAN utilize Generative Adversarial Networks (GANs) to dynamically generate high-quality negative triplets by modeling their distribution \\cite{zhang2018}.\n    *   **Limitations of previous solutions**:\n        *   Fixed schemes: Cannot adapt to the dynamic changes in negative triplet distributions during training, leading to vanishing gradients \\cite{zhang2018}.\n        *   GAN-based schemes: Increase model complexity with an extra generator, suffer from training instability and degeneracy, require high-variance REINFORCE gradients, necessitate pretraining, and waste computational resources learning the full (skewed) distribution of negative triplets, including the many \"useless\" small-score ones \\cite{zhang2018}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: NSCaching \\cite{zhang2018} proposes a simple and efficient cache-based negative sampling method. Motivated by the observation that high-score negative triplets are rare, it directly maintains and samples from a cache of these \"hard\" negative triplets for each positive triplet.\n    *   **Novelty/Difference**:\n        *   **Direct caching of hard negatives**: Unlike GANs that model the full distribution, NSCaching directly stores and updates a small set of high-quality negative triplets in a cache \\cite{zhang2018}.\n        *   **Importance Sampling (IS) for cache update**: A carefully designed Importance Sampling strategy is used to dynamically update the cache, ensuring it captures the evolving distribution of hard negatives efficiently \\cite{zhang2018}.\n        *   **Exploration-exploitation balance**: The method incorporates mechanisms to balance exploring new potential hard negatives and exploiting the currently known hard negatives stored in the cache \\cite{zhang2018}.\n        *   **\"Distilled\" GAN alternative**: NSCaching acts as a \"distilled\" version of GAN-based methods, achieving similar benefits (sampling hard negatives) without the additional parameters, training complexity, or reliance on reinforcement learning. It can be trained with standard gradient descent \\cite{zhang2018}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms/methods**:\n        *   NSCaching: A novel cache-based negative sampling scheme that is general and compatible with various KG embedding models \\cite{zhang2018}.\n        *   A uniform sampling strategy for selecting negative triplets directly from the cache.\n        *   An Importance Sampling (IS) strategy for dynamically updating the cache of hard negative triplets \\cite{zhang2018}.\n        *   A mechanism to balance exploration (finding new hard negatives) and exploitation (sampling from existing hard negatives in the cache) \\cite{zhang2018}.\n    *   **Theoretical insights/analysis**:\n        *   Empirical observation and analysis of the highly skewed score distribution of negative triplets, highlighting that only a few have large scores \\cite{zhang2018}.\n        *   Analysis connecting NSCaching to self-paced learning, demonstrating its ability to first learn from easily classified samples and then gradually switch to harder ones \\cite{zhang2018}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments were performed to evaluate NSCaching's effectiveness and efficiency. It was applied to various popular KG embedding models (e.g., TransE, TransH, TransD, DistMult, ComplEx, HolE) \\cite{zhang2018}.\n    *   **Key performance metrics and comparison results**:\n        *   **Datasets**: WN18, FB15K, and their variants WN18RR, FB15K237 \\cite{zhang2018}.\n        *   **Results**: NSCaching demonstrated significant performance improvements across various KG embedding models. It consistently outperformed state-of-the-art GAN-based negative sampling methods (IGAN and KBGAN) in terms of effectiveness, while also being more efficient \\cite{zhang2018}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions**: The core assumption is that high-score negative triplets are rare and can be effectively managed by a cache. The performance relies on the efficiency of the cache update mechanism and the balance between exploration and exploitation. The optimal cache size and update frequency might be hyper-parameters requiring tuning.\n    *   **Scope of applicability**: NSCaching is designed as a general negative sampling scheme that can be injected into all popularly used KG embedding models \\cite{zhang2018}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art**: NSCaching provides a simpler, more efficient, and more stable alternative to complex GAN-based methods for dynamic negative sampling in KG embedding \\cite{zhang2018}. It effectively addresses the vanishing gradient problem without introducing the training instabilities and parameter overhead associated with GANs, achieving superior performance.\n    *   **Potential impact on future research**: The work highlights the effectiveness of directly managing \"hard\" negative samples through caching, offering a practical paradigm shift from complex generative models. This approach could inspire similar efficient sampling strategies in other machine learning domains where identifying and leveraging challenging negative examples is crucial for model performance. Its simplicity and general applicability make it a strong candidate for widespread adoption in KG embedding research and applications \\cite{zhang2018}.",
        "keywords": [
          "Knowledge Graph Embedding",
          "Negative Sampling",
          "Vanishing Gradient Problem",
          "NSCaching",
          "Cache-based negative sampling",
          "Hard negative triplets",
          "Importance Sampling",
          "Dynamic sampling",
          "Generative Adversarial Networks (GANs)",
          "Training instability",
          "Exploration-exploitation balance",
          "Self-paced learning",
          "Link prediction",
          "Efficiency and stability",
          "Superior performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "beade097ff41c62a8d8d29065be0e1339be39f30.pdf"
    },
    {
      "success": true,
      "doc_id": "c5ac17389f4e9d583d2daecbc8c41716",
      "summary": "Here's a focused summary of the provided technical paper for a literature review:\n\n*   **CITATION**: \\cite{li2021}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most Knowledge Graph Embedding (KGE) models rely on negative sampling during training. This approach leads to unstable model performance due to the inherent uncertainty in the sampling procedure and can result in suboptimal prediction accuracy because only a subset of negative instances is considered. While a non-sampling approach (considering all negative instances) could improve accuracy and stability, it dramatically increases computational and space complexity, making it impractical for real-world KGs.\n    *   **Importance and Challenge**: KGE is a critical technique for representing and manipulating large-scale, heterogeneous knowledge graphs, powering applications like search engines, recommendation systems, and question answering. The challenge lies in developing a KGE training framework that can leverage all available data (non-sampling) to achieve higher accuracy and stability, without incurring prohibitive computational and memory costs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Negative Sampling KGE Models**: The majority of current KGE methods (e.g., DistMult \\cite{li2021}, SimplE \\cite{li2021}, ComplEx \\cite{li2021}, TransE \\cite{li2021}, RESCAL \\cite{li2021}) use negative sampling to reduce training time.\n        *   **Improved Negative Sampling Strategies**: Some research attempts to mitigate the drawbacks of random sampling by employing carefully designed strategies, such as dynamic negative sampling \\cite{li2021} or GAN-based generation of high-quality negative samples \\cite{li2021}.\n        *   **Non-Sampling in Other Domains**: Whole-data based approaches have been explored in recommendation systems and factorization machines \\cite{li2021} to improve accuracy.\n    *   **Limitations of Previous Solutions**:\n        *   **Negative Sampling KGEs**: Suffer from weakened prediction accuracy due to incomplete information from negative instances and unstable training results across different runs \\cite{li2021}. Some models require a large number of negative samples, increasing training time.\n        *   **Improved Negative Sampling**: Still fundamentally rely on sampled instances, thus not fully addressing the limitations of partial information and potential fluctuations \\cite{li2021}.\n        *   **Non-Sampling in Other Domains**: These methods are typically not generalizable to KGE models (especially square-loss based ones) and often focus only on time complexity, neglecting space efficiency, which necessitates batch learning \\cite{li2021}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes the **Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE)** framework.\n        *   It aims to train KGE models by considering *all* positive and negative instances, thereby eliminating the need for negative sampling.\n        *   The framework is applicable to KGE models whose loss function is a square loss or can be converted into one.\n        *   To overcome the dramatic increase in computational and space complexity from non-sampling, the core innovation is a **mathematical re-derivation and re-organization of the non-sampling square loss function**.\n        *   The loss function is initially formulated as a sum over all possible triplets (Eq. 1) and then re-organized into terms for positive instances (`L_P`), all entities (`L_A`), and a constant (Eq. 3).\n        *   For factorization-based KGE models, the scoring function `f_r(h,t)` (e.g., `e_h^T (r  e_t)`) is manipulated to express its square `f_r(h,t)^2` in a way that **disentangles the head entity, relation, and tail entity embeddings**. This disentanglement allows for a more efficient calculation of the `L_A` term, which is the most computationally expensive part.\n    *   **Novelty/Difference**: The primary novelty is the development of a **general and efficient non-sampling framework for KGE** that simultaneously addresses both the time and space complexity bottlenecks. This is achieved through a sophisticated mathematical re-organization of the square loss function, enabling full-data training without sacrificing computational tractability, a significant departure from prevalent negative sampling methods.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of the **Non-Sampling Knowledge Graph Embedding (NS-KGE) framework**.\n        *   A novel mathematical derivation that transforms the computationally intensive non-sampling square loss into an efficient form by disentangling entity and relation parameters, thereby mitigating time and space bottlenecks.\n    *   **System Design/Architectural Innovations**: Provides a general framework applicable to a broad class of square-loss based KGE models.\n    *   **Theoretical Insights/Analysis**: Demonstrates that the full non-sampling loss, traditionally considered intractable, can be made computationally efficient through algebraic manipulation, offering a new paradigm for KGE training.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The NS-KGE framework was applied to four representative KGE models: DistMult \\cite{li2021}, SimplE \\cite{li2021}, ComplEx \\cite{li2021}, and TransE \\cite{li2021}. Experiments were conducted on \"benchmark datasets\" (specific names not provided in the excerpt).\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Accuracy**: The NS-KGE framework is reported to achieve \"better prediction accuracy\" compared to traditional negative sampling based models.\n        *   **Efficiency**: NS-KGE demonstrates \"better efficiency\" (shorter running time) and \"better space efficiency\" than existing negative sampling models.\n        *   Overall, the framework \"outperforms most of the models in terms of both prediction accuracy and learning efficiency\" \\cite{li2021}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The NS-KGE framework is specifically designed for and applicable to **square-loss based KGE models** or models whose loss functions can be mathematically converted into a square loss format \\cite{li2021}.\n    *   **Scope of Applicability**: The framework is generalizable to a \"large class of knowledge graph embedding models\" that meet the square-loss criterion, particularly factorization-based models where the scoring function can be expressed in a separable form (e.g., `e_h^T (r  e_t)`).\n\n7.  **Technical Significance**\n    *   **Advance State-of-the-Art**: NS-KGE significantly advances the state-of-the-art in KGE by providing a robust and efficient method to overcome the long-standing issues of instability and suboptimal accuracy associated with negative sampling. It enables KGE models to learn from all available data, leading to more reliable and accurate embeddings.\n    *   **Potential Impact on Future Research**: This work opens new research directions for developing KGE models that do not rely on sampling, potentially leading to more stable and higher-quality embeddings. The mathematical approach to optimize computational and space complexity for full-data training could also inspire similar non-sampling methodologies in other machine learning domains where sampling is currently a necessary compromise.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) models are fundamental for AI applications, yet their pervasive reliance on negative sampling for training often compromises prediction accuracy and introduces performance instability. The computational and space complexity of training with *all* available data has historically rendered non-sampling approaches impractical. This paper introduces **NS-KGE (Non-Sampling Knowledge Graph Embedding)**, a novel framework that revolutionizes KGE training by enabling efficient learning from every positive and negative instance. Our core innovation lies in a sophisticated mathematical re-derivation and re-organization of the square loss function, disentangling entity and relation parameters to overcome the prohibitive computational and space bottlenecks. NS-KGE is applicable to a broad class of square-loss based KGE models, demonstrating superior prediction accuracy, enhanced stability, and remarkable learning efficiency compared to traditional negative sampling methods. This work establishes a new paradigm for KGE, paving the way for more robust and precise knowledge representation and opening new avenues for non-sampling methodologies across machine learning.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "negative sampling",
        "non-sampling KGE framework (NS-KGE)",
        "computational complexity",
        "space complexity",
        "square-loss based KGE models",
        "mathematical loss function re-derivation",
        "disentanglement of embeddings",
        "full-data training",
        "prediction accuracy",
        "model stability",
        "learning efficiency",
        "factorization-based KGE models"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/bbb89d88ad5b8279709ff089d3c00cd2750cd26b.pdf",
      "citation_key": "li2021",
      "metadata": {
        "title": "Efficient Non-Sampling Knowledge Graph Embedding",
        "authors": [
          "Zelong Li",
          "Jianchao Ji",
          "Zuohui Fu",
          "Yingqiang Ge",
          "Shuyuan Xu",
          "Chong Chen",
          "Yongfeng Zhang"
        ],
        "published_date": "2021",
        "abstract": "Knowledge Graph (KG) is a flexible structure that is able to describe the complex relationship between data entities. Currently, most KG embedding models are trained based on negative sampling, i.e., the model aims to maximize some similarity of the connected entities in the KG, while minimizing the similarity of the sampled disconnected entities. Negative sampling helps to reduce the time complexity of model learning by only considering a subset of negative instances, which may fail to deliver stable model performance due to the uncertainty in the sampling procedure. To avoid such deficiency, we propose a new framework for KG embeddingEfficient Non-Sampling Knowledge Graph Embedding (NS-KGE). The basic idea is to consider all of the negative instances in the KG for model learning, and thus to avoid negative sampling. The framework can be applied to square-loss based knowledge graph embedding models or models whose loss can be converted to a square loss. A natural side-effect of this non-sampling strategy is the increased computational complexity of model learning. To solve the problem, we leverage mathematical derivations to reduce the complexity of non-sampling loss function, which eventually provides us both better efficiency and better accuracy in KG embedding compared with existing models. Experiments on benchmark datasets show that our NS-KGE framework can achieve a better performance on efficiency and accuracy over traditional negative sampling based models, and that the framework is applicable to a large class of knowledge graph embedding models.",
        "file_path": "paper_data/knowledge_graph_embedding/bbb89d88ad5b8279709ff089d3c00cd2750cd26b.pdf",
        "venue": "The Web Conference",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the provided technical paper for a literature review:\n\n*   **CITATION**: \\cite{li2021}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Most Knowledge Graph Embedding (KGE) models rely on negative sampling during training. This approach leads to unstable model performance due to the inherent uncertainty in the sampling procedure and can result in suboptimal prediction accuracy because only a subset of negative instances is considered. While a non-sampling approach (considering all negative instances) could improve accuracy and stability, it dramatically increases computational and space complexity, making it impractical for real-world KGs.\n    *   **Importance and Challenge**: KGE is a critical technique for representing and manipulating large-scale, heterogeneous knowledge graphs, powering applications like search engines, recommendation systems, and question answering. The challenge lies in developing a KGE training framework that can leverage all available data (non-sampling) to achieve higher accuracy and stability, without incurring prohibitive computational and memory costs.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Negative Sampling KGE Models**: The majority of current KGE methods (e.g., DistMult \\cite{li2021}, SimplE \\cite{li2021}, ComplEx \\cite{li2021}, TransE \\cite{li2021}, RESCAL \\cite{li2021}) use negative sampling to reduce training time.\n        *   **Improved Negative Sampling Strategies**: Some research attempts to mitigate the drawbacks of random sampling by employing carefully designed strategies, such as dynamic negative sampling \\cite{li2021} or GAN-based generation of high-quality negative samples \\cite{li2021}.\n        *   **Non-Sampling in Other Domains**: Whole-data based approaches have been explored in recommendation systems and factorization machines \\cite{li2021} to improve accuracy.\n    *   **Limitations of Previous Solutions**:\n        *   **Negative Sampling KGEs**: Suffer from weakened prediction accuracy due to incomplete information from negative instances and unstable training results across different runs \\cite{li2021}. Some models require a large number of negative samples, increasing training time.\n        *   **Improved Negative Sampling**: Still fundamentally rely on sampled instances, thus not fully addressing the limitations of partial information and potential fluctuations \\cite{li2021}.\n        *   **Non-Sampling in Other Domains**: These methods are typically not generalizable to KGE models (especially square-loss based ones) and often focus only on time complexity, neglecting space efficiency, which necessitates batch learning \\cite{li2021}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes the **Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE)** framework.\n        *   It aims to train KGE models by considering *all* positive and negative instances, thereby eliminating the need for negative sampling.\n        *   The framework is applicable to KGE models whose loss function is a square loss or can be converted into one.\n        *   To overcome the dramatic increase in computational and space complexity from non-sampling, the core innovation is a **mathematical re-derivation and re-organization of the non-sampling square loss function**.\n        *   The loss function is initially formulated as a sum over all possible triplets (Eq. 1) and then re-organized into terms for positive instances (`L_P`), all entities (`L_A`), and a constant (Eq. 3).\n        *   For factorization-based KGE models, the scoring function `f_r(h,t)` (e.g., `e_h^T (r  e_t)`) is manipulated to express its square `f_r(h,t)^2` in a way that **disentangles the head entity, relation, and tail entity embeddings**. This disentanglement allows for a more efficient calculation of the `L_A` term, which is the most computationally expensive part.\n    *   **Novelty/Difference**: The primary novelty is the development of a **general and efficient non-sampling framework for KGE** that simultaneously addresses both the time and space complexity bottlenecks. This is achieved through a sophisticated mathematical re-organization of the square loss function, enabling full-data training without sacrificing computational tractability, a significant departure from prevalent negative sampling methods.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of the **Non-Sampling Knowledge Graph Embedding (NS-KGE) framework**.\n        *   A novel mathematical derivation that transforms the computationally intensive non-sampling square loss into an efficient form by disentangling entity and relation parameters, thereby mitigating time and space bottlenecks.\n    *   **System Design/Architectural Innovations**: Provides a general framework applicable to a broad class of square-loss based KGE models.\n    *   **Theoretical Insights/Analysis**: Demonstrates that the full non-sampling loss, traditionally considered intractable, can be made computationally efficient through algebraic manipulation, offering a new paradigm for KGE training.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The NS-KGE framework was applied to four representative KGE models: DistMult \\cite{li2021}, SimplE \\cite{li2021}, ComplEx \\cite{li2021}, and TransE \\cite{li2021}. Experiments were conducted on \"benchmark datasets\" (specific names not provided in the excerpt).\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Accuracy**: The NS-KGE framework is reported to achieve \"better prediction accuracy\" compared to traditional negative sampling based models.\n        *   **Efficiency**: NS-KGE demonstrates \"better efficiency\" (shorter running time) and \"better space efficiency\" than existing negative sampling models.\n        *   Overall, the framework \"outperforms most of the models in terms of both prediction accuracy and learning efficiency\" \\cite{li2021}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The NS-KGE framework is specifically designed for and applicable to **square-loss based KGE models** or models whose loss functions can be mathematically converted into a square loss format \\cite{li2021}.\n    *   **Scope of Applicability**: The framework is generalizable to a \"large class of knowledge graph embedding models\" that meet the square-loss criterion, particularly factorization-based models where the scoring function can be expressed in a separable form (e.g., `e_h^T (r  e_t)`).\n\n7.  **Technical Significance**\n    *   **Advance State-of-the-Art**: NS-KGE significantly advances the state-of-the-art in KGE by providing a robust and efficient method to overcome the long-standing issues of instability and suboptimal accuracy associated with negative sampling. It enables KGE models to learn from all available data, leading to more reliable and accurate embeddings.\n    *   **Potential Impact on Future Research**: This work opens new research directions for developing KGE models that do not rely on sampling, potentially leading to more stable and higher-quality embeddings. The mathematical approach to optimize computational and space complexity for full-data training could also inspire similar non-sampling methodologies in other machine learning domains where sampling is currently a necessary compromise.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "negative sampling",
          "non-sampling KGE framework (NS-KGE)",
          "computational complexity",
          "space complexity",
          "square-loss based KGE models",
          "mathematical loss function re-derivation",
          "disentanglement of embeddings",
          "full-data training",
          "prediction accuracy",
          "model stability",
          "learning efficiency",
          "factorization-based KGE models"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "bbb89d88ad5b8279709ff089d3c00cd2750cd26b.pdf"
    },
    {
      "success": true,
      "doc_id": "434068e9c13bb45a484efa4d1f5ddc10",
      "summary": "This paper by \\cite{li2022} introduces a novel approach to operationalize and measure knowledge proximity within the US Patent Database using knowledge graphs and embedding techniques.\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the challenge of measuring \"knowledge proximity\"  the strength of association between entities in a knowledge base  particularly for heterogeneous pairs of entities (e.g., inventor-domain, patent-assignee) within the complex US Patent Database.\n    *   **Importance and challenge**: Existing quantitative measures for knowledge proximity are often limited to homogeneous entity pairs (e.g., inventor-inventor, patent-patent) and capture only specific, isolated aspects of the patent database through individual relations (e.g., <patent, cite, patent>). This lack of interoperability across different entity types makes it difficult to gain a holistic understanding of knowledge relationships and innovation dynamics. A unified structural form that embodies all types of entities and relations is needed.\n\n2.  **Related Work & Positioning**\n    *   **Existing approaches**:\n        *   **Homogeneous Proximity Measures**: Previous work measured proximity for patent-patent (e.g., shared citations, Euclidean distance on classification digits, LSA/SVD cosine similarity), assignee-assignee (e.g., overlapping patent classes, vector representations from patent distribution), and domain-domain (e.g., co-occurrence, citation distribution cosine similarity).\n        *   **Knowledge Graph Embedding (KGE) Techniques**: Reviewed translational distance models (TransE, TransR, RotateE) and semantic matching models (RESCAL, DistMult, ComplEx), which learn low-rank vector representations of entities and relations.\n    *   **Limitations of previous solutions**:\n        *   The reviewed proximity measures lack interoperability across different entity types, making them unsuitable for associating heterogeneous pairs (e.g., patent-inventor).\n        *   They capture only limited aspects of the patent database through individual relations.\n        *   Graph Neural Networks (GNNs), while powerful, are largely applicable to homogeneous graphs, whereas this work requires embedding a heterogeneous knowledge graph.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**:\n        *   **Knowledge Graph Construction**: Built a heterogeneous knowledge graph called 'PatNet' from the US Patent Database (1976-2020). PatNet comprises 10,273,843 entities (patents, inventors, assignees, groups, subsections) and 106,882,276 links, representing five types of relations: <patent, cite, patent>, <inventor, write, patent>, <assignee, own, patent>, <group, contain, patent>, and <subsection, comprise, groups>.\n        *   **Knowledge Graph Embedding**: Trained various graph embedding models (TransE_l1, TransE_l2, TransR, RESCAL, DistMult, ComplEx, RotateE) on PatNet to obtain low-dimensional vector embeddings for all entities and relations.\n        *   **Knowledge Proximity Measurement**: Defined knowledge proximity as the cosine similarity between the corresponding (or transformed) embeddings of entities.\n    *   **Novelty or difference**:\n        *   The primary innovation is the creation of a unified, comprehensive knowledge graph (PatNet) that integrates diverse patent metadata and relations into a single structural form.\n        *   This allows for the application of state-of-the-art knowledge graph embedding techniques to learn rich, interoperable representations for all entities, enabling the measurement of knowledge proximity for both homogeneous and, crucially, heterogeneous entity pairs in a consistent manner.\n\n4.  **Key Technical Contributions**\n    *   **System Design/Architectural Innovations**: Development of 'PatNet', a large-scale, heterogeneous knowledge graph specifically designed for patent metadata, integrating five distinct entity types and five relation types from the USPTO database.\n    *   **Novel Algorithms/Methods**: Application and comparative assessment of a suite of established knowledge graph embedding models (TransE, TransR, RESCAL, DistMult, ComplEx, RotateE) on the complex PatNet structure to derive meaningful entity and relation embeddings.\n    *   **Theoretical Insights/Analysis**: Demonstrated that knowledge graph embeddings can effectively operationalize knowledge proximity for both homogeneous and heterogeneous entity pairs, overcoming the limitations of prior, siloed proximity measures.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**:\n        *   **Predicting Target Entities (Link Prediction)**: Evaluated the embedding models on their ability to predict missing entities in triples (e.g., identifying a missing patent, inventor, or domain) using a 10% test set of PatNet triples.\n        *   **Assessing Domain Expansion Profiles**: Examined how well the knowledge proximity measure (derived from embeddings) explains the historical domain expansion of 76,326 inventors and 15,283 assignees, based on the premise that agents tend to explore technologically \"less distant\" domains.\n    *   **Key performance metrics and comparison results**:\n        *   **Link Prediction**: Models were evaluated using Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@k (k=1, 3, 10). RESCAL, ComplEx, DistMult, and TransE_l2 showed the best performance, with RESCAL achieving the highest MRR (0.928) and Hits@10 (0.958).\n        *   **Domain Expansion**: Models were evaluated using the Area Under the Curve (AUC) of the cumulative distribution of proximity percentiles and \"explainability\" (proportion of agents for whom a model exhibits higher AUC). TransE_l2 significantly outperformed other models, exhibiting the highest AUC for the combined expansion profiles of both assignees and inventors, and the highest explainability (highest AUC for nearly 70% of agents).\n        *   **Overall**: TransE_l2 was identified as the best-preferred model, demonstrating strong performance in both capturing the structure and semantics of PatNet (link prediction) and forming meaningful associations that explain real-world phenomena (domain expansion).\n\n6.  **Limitations & Scope**\n    *   **Technical limitations**:\n        *   The study focused on a specific set of knowledge graph embedding models; other advanced models, particularly those designed for heterogeneous graphs or GNNs, were not extensively explored.\n        *   The embedding dimension was set to 500 due to hardware and time constraints, though it falls within a generally accepted range.\n        *   PatNet, by design, does not include cycles or two-way relations due to the unidirectional nature of the captured facts.\n    *   **Scope of applicability**: The methodology is primarily applied to the US Patent Database (1976-2020). While the approach is generalizable, the specific PatNet construction and empirical validation are tied to this dataset.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art**: This work provides a robust and unified framework for measuring knowledge proximity across diverse, heterogeneous entities within a complex knowledge base like patent data. It overcomes the limitations of previous siloed approaches by integrating all relevant metadata into a single knowledge graph and leveraging advanced embedding techniques.\n    *   **Potential impact on future research**:\n        *   Enables more comprehensive and nuanced analysis of innovation dynamics, technological evolution, and strategic decision-making in R&D.\n        *   Facilitates applications such as identifying emerging technologies, predicting technological trajectories, and recommending collaborations between inventors or assignees.\n        *   Offers a generalizable methodology for operationalizing knowledge proximity in other domains with rich, heterogeneous relational data, extending beyond patent analysis.",
      "intriguing_abstract": "Unlocking the intricate web of innovation within vast datasets like the US Patent Database demands a holistic understanding of knowledge relationships. Existing measures of \"knowledge proximity\" are often limited to homogeneous entity pairs, failing to capture the rich, heterogeneous connections between inventors, patents, assignees, and technological domains. This paper introduces a pioneering framework to overcome this challenge. We construct PatNet, an unprecedented large-scale **heterogeneous knowledge graph** integrating over 10 million entities and 100 million links from the USPTO database. By applying advanced **knowledge graph embedding (KGE)** techniques, we derive low-dimensional vector representations for all entities and relations, enabling the consistent measurement of **knowledge proximity** via **cosine similarity** for both homogeneous and, crucially, heterogeneous entity pairs. Our rigorous experimental validation, including **link prediction** and the explanation of real-world **domain expansion** profiles, demonstrates the superior performance of models like **TransE_l2**. This work provides a robust, generalizable methodology for operationalizing knowledge proximity, offering profound implications for analyzing **innovation dynamics**, predicting technological trajectories, and informing strategic R&D decisions.",
      "keywords": [
        "Knowledge proximity measurement",
        "Knowledge graphs",
        "Knowledge graph embedding techniques",
        "US Patent Database",
        "Heterogeneous entity pairs",
        "PatNet (knowledge graph)",
        "Unified framework",
        "Interoperable representations",
        "Link prediction",
        "Domain expansion analysis",
        "TransE_l2 model",
        "Innovation dynamics"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/d605a7628b2a7ff8ce04fc27111626e2d734cab4.pdf",
      "citation_key": "li2022",
      "metadata": {
        "title": "Embedding knowledge graph of patent metadata to measure knowledge proximity",
        "authors": [
          "Guangtong Li",
          "L. Siddharth",
          "Jianxi Luo"
        ],
        "published_date": "2022",
        "abstract": "Knowledge proximity refers to the strength of association between any two entities in a structural form that embodies certain aspects of a knowledge base. In this work, we operationalize knowledge proximity within the context of the US Patent Database (knowledge base) using a knowledge graph (structural form) named PatNet built using patent metadata, including citations, inventors, assignees, and domain classifications. We train various graph embedding models using PatNet to obtain the embeddings of entities and relations. The cosine similarity between the corresponding (or transformed) embeddings of entities denotes the knowledge proximity between these. We compare the embedding models in terms of their performances in predicting target entities and explaining domain expansion profiles of inventors and assignees. We then apply the embeddings of the bestpreferred model to associate homogeneous (e.g., patentpatent) and heterogeneous (e.g., inventorassignee) pairs of entities.",
        "file_path": "paper_data/knowledge_graph_embedding/d605a7628b2a7ff8ce04fc27111626e2d734cab4.pdf",
        "venue": "J. Assoc. Inf. Sci. Technol.",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper by \\cite{li2022} introduces a novel approach to operationalize and measure knowledge proximity within the US Patent Database using knowledge graphs and embedding techniques.\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the challenge of measuring \"knowledge proximity\"  the strength of association between entities in a knowledge base  particularly for heterogeneous pairs of entities (e.g., inventor-domain, patent-assignee) within the complex US Patent Database.\n    *   **Importance and challenge**: Existing quantitative measures for knowledge proximity are often limited to homogeneous entity pairs (e.g., inventor-inventor, patent-patent) and capture only specific, isolated aspects of the patent database through individual relations (e.g., <patent, cite, patent>). This lack of interoperability across different entity types makes it difficult to gain a holistic understanding of knowledge relationships and innovation dynamics. A unified structural form that embodies all types of entities and relations is needed.\n\n2.  **Related Work & Positioning**\n    *   **Existing approaches**:\n        *   **Homogeneous Proximity Measures**: Previous work measured proximity for patent-patent (e.g., shared citations, Euclidean distance on classification digits, LSA/SVD cosine similarity), assignee-assignee (e.g., overlapping patent classes, vector representations from patent distribution), and domain-domain (e.g., co-occurrence, citation distribution cosine similarity).\n        *   **Knowledge Graph Embedding (KGE) Techniques**: Reviewed translational distance models (TransE, TransR, RotateE) and semantic matching models (RESCAL, DistMult, ComplEx), which learn low-rank vector representations of entities and relations.\n    *   **Limitations of previous solutions**:\n        *   The reviewed proximity measures lack interoperability across different entity types, making them unsuitable for associating heterogeneous pairs (e.g., patent-inventor).\n        *   They capture only limited aspects of the patent database through individual relations.\n        *   Graph Neural Networks (GNNs), while powerful, are largely applicable to homogeneous graphs, whereas this work requires embedding a heterogeneous knowledge graph.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**:\n        *   **Knowledge Graph Construction**: Built a heterogeneous knowledge graph called 'PatNet' from the US Patent Database (1976-2020). PatNet comprises 10,273,843 entities (patents, inventors, assignees, groups, subsections) and 106,882,276 links, representing five types of relations: <patent, cite, patent>, <inventor, write, patent>, <assignee, own, patent>, <group, contain, patent>, and <subsection, comprise, groups>.\n        *   **Knowledge Graph Embedding**: Trained various graph embedding models (TransE_l1, TransE_l2, TransR, RESCAL, DistMult, ComplEx, RotateE) on PatNet to obtain low-dimensional vector embeddings for all entities and relations.\n        *   **Knowledge Proximity Measurement**: Defined knowledge proximity as the cosine similarity between the corresponding (or transformed) embeddings of entities.\n    *   **Novelty or difference**:\n        *   The primary innovation is the creation of a unified, comprehensive knowledge graph (PatNet) that integrates diverse patent metadata and relations into a single structural form.\n        *   This allows for the application of state-of-the-art knowledge graph embedding techniques to learn rich, interoperable representations for all entities, enabling the measurement of knowledge proximity for both homogeneous and, crucially, heterogeneous entity pairs in a consistent manner.\n\n4.  **Key Technical Contributions**\n    *   **System Design/Architectural Innovations**: Development of 'PatNet', a large-scale, heterogeneous knowledge graph specifically designed for patent metadata, integrating five distinct entity types and five relation types from the USPTO database.\n    *   **Novel Algorithms/Methods**: Application and comparative assessment of a suite of established knowledge graph embedding models (TransE, TransR, RESCAL, DistMult, ComplEx, RotateE) on the complex PatNet structure to derive meaningful entity and relation embeddings.\n    *   **Theoretical Insights/Analysis**: Demonstrated that knowledge graph embeddings can effectively operationalize knowledge proximity for both homogeneous and heterogeneous entity pairs, overcoming the limitations of prior, siloed proximity measures.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**:\n        *   **Predicting Target Entities (Link Prediction)**: Evaluated the embedding models on their ability to predict missing entities in triples (e.g., identifying a missing patent, inventor, or domain) using a 10% test set of PatNet triples.\n        *   **Assessing Domain Expansion Profiles**: Examined how well the knowledge proximity measure (derived from embeddings) explains the historical domain expansion of 76,326 inventors and 15,283 assignees, based on the premise that agents tend to explore technologically \"less distant\" domains.\n    *   **Key performance metrics and comparison results**:\n        *   **Link Prediction**: Models were evaluated using Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@k (k=1, 3, 10). RESCAL, ComplEx, DistMult, and TransE_l2 showed the best performance, with RESCAL achieving the highest MRR (0.928) and Hits@10 (0.958).\n        *   **Domain Expansion**: Models were evaluated using the Area Under the Curve (AUC) of the cumulative distribution of proximity percentiles and \"explainability\" (proportion of agents for whom a model exhibits higher AUC). TransE_l2 significantly outperformed other models, exhibiting the highest AUC for the combined expansion profiles of both assignees and inventors, and the highest explainability (highest AUC for nearly 70% of agents).\n        *   **Overall**: TransE_l2 was identified as the best-preferred model, demonstrating strong performance in both capturing the structure and semantics of PatNet (link prediction) and forming meaningful associations that explain real-world phenomena (domain expansion).\n\n6.  **Limitations & Scope**\n    *   **Technical limitations**:\n        *   The study focused on a specific set of knowledge graph embedding models; other advanced models, particularly those designed for heterogeneous graphs or GNNs, were not extensively explored.\n        *   The embedding dimension was set to 500 due to hardware and time constraints, though it falls within a generally accepted range.\n        *   PatNet, by design, does not include cycles or two-way relations due to the unidirectional nature of the captured facts.\n    *   **Scope of applicability**: The methodology is primarily applied to the US Patent Database (1976-2020). While the approach is generalizable, the specific PatNet construction and empirical validation are tied to this dataset.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art**: This work provides a robust and unified framework for measuring knowledge proximity across diverse, heterogeneous entities within a complex knowledge base like patent data. It overcomes the limitations of previous siloed approaches by integrating all relevant metadata into a single knowledge graph and leveraging advanced embedding techniques.\n    *   **Potential impact on future research**:\n        *   Enables more comprehensive and nuanced analysis of innovation dynamics, technological evolution, and strategic decision-making in R&D.\n        *   Facilitates applications such as identifying emerging technologies, predicting technological trajectories, and recommending collaborations between inventors or assignees.\n        *   Offers a generalizable methodology for operationalizing knowledge proximity in other domains with rich, heterogeneous relational data, extending beyond patent analysis.",
        "keywords": [
          "Knowledge proximity measurement",
          "Knowledge graphs",
          "Knowledge graph embedding techniques",
          "US Patent Database",
          "Heterogeneous entity pairs",
          "PatNet (knowledge graph)",
          "Unified framework",
          "Interoperable representations",
          "Link prediction",
          "Domain expansion analysis",
          "TransE_l2 model",
          "Innovation dynamics"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "d605a7628b2a7ff8ce04fc27111626e2d734cab4.pdf"
    },
    {
      "success": true,
      "doc_id": "4419ae4e0e969ec543ccaea21b7e40b2",
      "summary": "Here's a focused summary of the technical paper \\cite{ding2018} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Improving Knowledge Graph Embedding (KGE) by learning more predictive, compact, and interpretable representations for entities and relations. Existing KGE methods either rely on complex triple scoring models or incorporate external information, often leading to increased complexity or requiring extensive manual effort.\n    *   **Motivation:** The paper argues that simple, universal constraints can effectively enhance KGE without significantly increasing model complexity or requiring laborious rule grounding, addressing the limitations of prior approaches that are often inefficient or restricted to strict, hard rules.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   Early KGE works used simple models over KG triples (e.g., TransE, RESCAL).\n        *   Later works focused on designing more complicated triple scoring models (e.g., TransE/RESCAL extensions, neural networks) or incorporating extra information beyond triples (e.g., entity types, relation paths, textual descriptions).\n        *   A line of research integrates logical background knowledge, but most require grounding first-order logic rules, which is time and space inefficient.\n        *   Methods avoiding grounding (e.g., Demeester et al., Minervini et al.) often handle only strict, hard rules, create representations for entity pairs rather than individual entities, or have limited scope for rule types and confidence levels.\n    *   **Limitations of Previous Solutions:** Inefficiency due to grounding, inability to handle unpaired entities, reliance on strict/hard rules requiring manual effort, or limited flexibility in modeling rule uncertainty.\n    *   **Positioning:** \\cite{ding2018} differentiates itself by investigating *very simple constraints* applied directly to entity and relation representations, avoiding grounding, being universally applicable, and automatically derivable, thus offering a more efficient and scalable solution compared to complex models or rule-grounding approaches.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper extends ComplEx \\cite{trouillon2016} (a state-of-the-art complex-valued embedding model) by introducing two types of simple constraints during training:\n        1.  **Non-negativity and Boundedness Constraints on Entity Representations (NNE):** Entity embeddings (both real and imaginary components) are constrained to be within the hypercube `[0,1]^d`. This is motivated by the idea that entities are primarily described by positive properties, and non-negativity induces sparsity and interpretability.\n        2.  **Approximate Entailment Constraints on Relation Representations (AER):** These constraints encode logical entailment regularities between relations (e.g., `BornInCountry` approximately entails `Nationality`). The entailments are automatically derived from the KG using rule mining systems (e.g., AMIE+) and associated with confidence levels. The constraints are formulated to ensure that if `rp` entails `rq`, then `Re(rp)  Re(rq)` and `Im(rp) = Im(rq)` (for strict entailment), with slack variables and confidence levels introduced for approximate entailment.\n    *   **Novelty/Difference:**\n        *   Imposes constraints *directly* on entity and relation representations, eliminating the need for computationally expensive rule grounding.\n        *   The constraints (non-negativity and approximate entailment) are *universal* and *automatically derivable* from statistical properties of the KG, requiring no manual effort.\n        *   Integrates uncertainty into relation entailment through confidence levels and slack variables.\n        *   Achieves improved performance and interpretability without increasing the model's space or time complexity compared to the base ComplEx model.\n\n*   **Key Technical Contributions**\n    *   **Novel Methods:**\n        *   Introduction of non-negativity and boundedness constraints (`0  Re(e), Im(e)  1`) for entity embeddings to promote compactness and interpretability.\n        *   Formulation of approximate entailment constraints on relation embeddings that directly model logical regularities (` * [Re(rp) - Re(rq)]+  `, ` * [Im(rp) - Im(rq)]+  `) without grounding.\n    *   **Theoretical Insights:** Demonstrated that for ComplEx, with non-negative entity representations, `Re(rp)  Re(rq)` and `Im(rp) = Im(rq)` is a sufficient condition for strict entailment `(ei, rp, ej)  (ei, rq, ej)`.\n    *   **System Design:** The overall model integrates these constraints into the ComplEx optimization objective, converting approximate entailment into penalty terms and enforcing non-negativity via projection after each gradient step.\n\n*   **Experimental Validation**\n    *   **Experiments:** Evaluated on the link prediction task (predicting missing head or tail entities).\n    *   **Datasets:** WN18 (WordNet), FB15K (Freebase), and DB100K (DBpedia). Approximate entailments were extracted from training sets using AMIE+.\n    *   **Performance Metrics:** Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits@10, Hits@3, Hits@1.\n    *   **Comparison Results:**\n        *   The proposed model, ComplEx-NNE_AER, consistently and significantly outperformed competitive baselines (TransE, DistMult, and the base ComplEx model) across all datasets and metrics.\n        *   Ablation studies showed that both non-negativity constraints (ComplEx-NNE) and approximate entailment constraints (ComplEx-AER) individually improve performance over ComplEx, and their combination yields the best results.\n        *   **Interpretability:** Demonstrated that non-negativity leads to sparser and more interpretable entity representations (e.g., \"cat\" vector highlights positive properties). Approximate entailment constraints resulted in a more structured relation embedding space, where entailed relations exhibit expected vectorial relationships.\n        *   **Efficiency:** Confirmed that the approach maintains efficiency and scalability, with space and time complexity on par with the base ComplEx model.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The paper does not explicitly state technical limitations beyond the inherent challenges of KGE. The approximate entailment constraints are derived statistically, which might not capture all nuances of logical rules. The sufficiency condition for strict entailment relies on the non-negativity of entity representations.\n    *   **Scope of Applicability:** Applicable to KGE tasks where interpretability and logical consistency are desired. The method is general enough to be applied to other complex-valued embedding models beyond ComplEx. The automatic derivation of approximate entailments makes it broadly applicable to various KGs without manual rule engineering.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{ding2018} demonstrates that simple, universally applicable constraints can significantly improve KGE performance and interpretability without increasing model complexity, offering a novel direction distinct from designing more complex models or relying on external, often manually curated, information.\n    *   **Potential Impact:** Provides a practical and efficient method for incorporating prior beliefs about entity and relation structures into KGE. This can lead to more robust, interpretable, and logically consistent knowledge representations, benefiting downstream NLP and AI tasks that rely on KGs. The approach of using automatically derived, approximate constraints without grounding offers a scalable paradigm for integrating soft logical knowledge.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graph Embeddings (KGE) demands representations that are not only predictive but also interpretable and efficient. Existing KGE methods often resort to complex models or laborious rule grounding, hindering scalability and practical utility. This paper introduces a novel paradigm: enhancing KGE through **simple, universal, and automatically derivable constraints** applied directly to entity and relation representations, circumventing the computational overhead of traditional rule grounding.\n\nWe extend the state-of-the-art ComplEx model by integrating two powerful constraint types: **Non-negativity and Boundedness Constraints on Entity Representations (NNE)**, which promote sparsity and interpretability by confining embeddings to a hypercube, and **Approximate Entailment Constraints on Relation Representations (AER)**. These AER constraints directly encode logical regularities, automatically mined from the knowledge graph with associated confidence levels, into the embedding space. Our approach significantly outperforms competitive baselines in **link prediction** tasks across diverse datasets, yielding more structured and interpretable entity and relation embeddings without increasing model complexity. This work offers a scalable, efficient, and robust method for injecting soft logical knowledge into KGE, paving the way for more reliable and transparent AI systems.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "ComplEx model extension",
        "Non-negativity and boundedness constraints",
        "Approximate entailment constraints",
        "Entity and relation representations",
        "Eliminating rule grounding",
        "Automatically derivable constraints",
        "Constraint-based learning",
        "Link prediction task",
        "Improved interpretability",
        "Scalable KGE",
        "Soft logical knowledge integration",
        "Complex-valued embeddings"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/322aa32b2a409d2e135dbb14736d9aeb497f1c52.pdf",
      "citation_key": "ding2018",
      "metadata": {
        "title": "Improving Knowledge Graph Embedding Using Simple Constraints",
        "authors": [
          "Boyang Ding",
          "Quan Wang",
          "Bin Wang",
          "Li Guo"
        ],
        "published_date": "2018",
        "abstract": "Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER.",
        "file_path": "paper_data/knowledge_graph_embedding/322aa32b2a409d2e135dbb14736d9aeb497f1c52.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper \\cite{ding2018} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Improving Knowledge Graph Embedding (KGE) by learning more predictive, compact, and interpretable representations for entities and relations. Existing KGE methods either rely on complex triple scoring models or incorporate external information, often leading to increased complexity or requiring extensive manual effort.\n    *   **Motivation:** The paper argues that simple, universal constraints can effectively enhance KGE without significantly increasing model complexity or requiring laborious rule grounding, addressing the limitations of prior approaches that are often inefficient or restricted to strict, hard rules.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   Early KGE works used simple models over KG triples (e.g., TransE, RESCAL).\n        *   Later works focused on designing more complicated triple scoring models (e.g., TransE/RESCAL extensions, neural networks) or incorporating extra information beyond triples (e.g., entity types, relation paths, textual descriptions).\n        *   A line of research integrates logical background knowledge, but most require grounding first-order logic rules, which is time and space inefficient.\n        *   Methods avoiding grounding (e.g., Demeester et al., Minervini et al.) often handle only strict, hard rules, create representations for entity pairs rather than individual entities, or have limited scope for rule types and confidence levels.\n    *   **Limitations of Previous Solutions:** Inefficiency due to grounding, inability to handle unpaired entities, reliance on strict/hard rules requiring manual effort, or limited flexibility in modeling rule uncertainty.\n    *   **Positioning:** \\cite{ding2018} differentiates itself by investigating *very simple constraints* applied directly to entity and relation representations, avoiding grounding, being universally applicable, and automatically derivable, thus offering a more efficient and scalable solution compared to complex models or rule-grounding approaches.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper extends ComplEx \\cite{trouillon2016} (a state-of-the-art complex-valued embedding model) by introducing two types of simple constraints during training:\n        1.  **Non-negativity and Boundedness Constraints on Entity Representations (NNE):** Entity embeddings (both real and imaginary components) are constrained to be within the hypercube `[0,1]^d`. This is motivated by the idea that entities are primarily described by positive properties, and non-negativity induces sparsity and interpretability.\n        2.  **Approximate Entailment Constraints on Relation Representations (AER):** These constraints encode logical entailment regularities between relations (e.g., `BornInCountry` approximately entails `Nationality`). The entailments are automatically derived from the KG using rule mining systems (e.g., AMIE+) and associated with confidence levels. The constraints are formulated to ensure that if `rp` entails `rq`, then `Re(rp)  Re(rq)` and `Im(rp) = Im(rq)` (for strict entailment), with slack variables and confidence levels introduced for approximate entailment.\n    *   **Novelty/Difference:**\n        *   Imposes constraints *directly* on entity and relation representations, eliminating the need for computationally expensive rule grounding.\n        *   The constraints (non-negativity and approximate entailment) are *universal* and *automatically derivable* from statistical properties of the KG, requiring no manual effort.\n        *   Integrates uncertainty into relation entailment through confidence levels and slack variables.\n        *   Achieves improved performance and interpretability without increasing the model's space or time complexity compared to the base ComplEx model.\n\n*   **Key Technical Contributions**\n    *   **Novel Methods:**\n        *   Introduction of non-negativity and boundedness constraints (`0  Re(e), Im(e)  1`) for entity embeddings to promote compactness and interpretability.\n        *   Formulation of approximate entailment constraints on relation embeddings that directly model logical regularities (` * [Re(rp) - Re(rq)]+  `, ` * [Im(rp) - Im(rq)]+  `) without grounding.\n    *   **Theoretical Insights:** Demonstrated that for ComplEx, with non-negative entity representations, `Re(rp)  Re(rq)` and `Im(rp) = Im(rq)` is a sufficient condition for strict entailment `(ei, rp, ej)  (ei, rq, ej)`.\n    *   **System Design:** The overall model integrates these constraints into the ComplEx optimization objective, converting approximate entailment into penalty terms and enforcing non-negativity via projection after each gradient step.\n\n*   **Experimental Validation**\n    *   **Experiments:** Evaluated on the link prediction task (predicting missing head or tail entities).\n    *   **Datasets:** WN18 (WordNet), FB15K (Freebase), and DB100K (DBpedia). Approximate entailments were extracted from training sets using AMIE+.\n    *   **Performance Metrics:** Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits@10, Hits@3, Hits@1.\n    *   **Comparison Results:**\n        *   The proposed model, ComplEx-NNE_AER, consistently and significantly outperformed competitive baselines (TransE, DistMult, and the base ComplEx model) across all datasets and metrics.\n        *   Ablation studies showed that both non-negativity constraints (ComplEx-NNE) and approximate entailment constraints (ComplEx-AER) individually improve performance over ComplEx, and their combination yields the best results.\n        *   **Interpretability:** Demonstrated that non-negativity leads to sparser and more interpretable entity representations (e.g., \"cat\" vector highlights positive properties). Approximate entailment constraints resulted in a more structured relation embedding space, where entailed relations exhibit expected vectorial relationships.\n        *   **Efficiency:** Confirmed that the approach maintains efficiency and scalability, with space and time complexity on par with the base ComplEx model.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The paper does not explicitly state technical limitations beyond the inherent challenges of KGE. The approximate entailment constraints are derived statistically, which might not capture all nuances of logical rules. The sufficiency condition for strict entailment relies on the non-negativity of entity representations.\n    *   **Scope of Applicability:** Applicable to KGE tasks where interpretability and logical consistency are desired. The method is general enough to be applied to other complex-valued embedding models beyond ComplEx. The automatic derivation of approximate entailments makes it broadly applicable to various KGs without manual rule engineering.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{ding2018} demonstrates that simple, universally applicable constraints can significantly improve KGE performance and interpretability without increasing model complexity, offering a novel direction distinct from designing more complex models or relying on external, often manually curated, information.\n    *   **Potential Impact:** Provides a practical and efficient method for incorporating prior beliefs about entity and relation structures into KGE. This can lead to more robust, interpretable, and logically consistent knowledge representations, benefiting downstream NLP and AI tasks that rely on KGs. The approach of using automatically derived, approximate constraints without grounding offers a scalable paradigm for integrating soft logical knowledge.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "ComplEx model extension",
          "Non-negativity and boundedness constraints",
          "Approximate entailment constraints",
          "Entity and relation representations",
          "Eliminating rule grounding",
          "Automatically derivable constraints",
          "Constraint-based learning",
          "Link prediction task",
          "Improved interpretability",
          "Scalable KGE",
          "Soft logical knowledge integration",
          "Complex-valued embeddings"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "322aa32b2a409d2e135dbb14736d9aeb497f1c52.pdf"
    },
    {
      "success": true,
      "doc_id": "62674181992e2834c91a08b2366e047f",
      "summary": "Here's a focused summary of the paper \"TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation\" \\cite{zhang2022} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Problem**: Traditional transition-based Knowledge Graph Embedding (KGE) methods, like TransE, use a single relation vector to translate a head entity to a tail entity. This scoring pattern is inadequate for complex scenarios, especially when the *same entity pair* can have *multiple different relations* (e.g., \"professor\" and \"employer\" between a person and a university, or multiple roles for a person in a film) \\cite{zhang2022}.\n    *   **Importance & Challenge**: Knowledge graphs often contain such complex relationships (one-to-many, many-to-one, many-to-many). Previous models (e.g., TransH/R/D) focused on improving entity representations for these complex relations but still relied on a single, static relation vector, which limits their ability to capture the nuances of multiple relationships between the same entities \\cite{zhang2022}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work is positioned within the family of transition-based KGE methods, which are popular due to their simplicity and effectiveness \\cite{zhang2022}. It builds upon the foundational idea of TransE (h + r  t).\n    *   **Limitations of Previous Solutions**:\n        *   TransE struggles with complex relations like one-to-many/many-to-one/many-to-many \\cite{zhang2022}.\n        *   Subsequent models (e.g., TransH, TransR, TransD) improved entity representations (e.g., using hyperplanes or multiple embedding spaces) to handle complex relation types, but critically, they *still used a single relation vector* (`r`) in their scoring patterns. This single vector cannot differentiate between multiple relationships for the same entity pair \\cite{zhang2022}.\n        *   Compared to InterHT, TranS uses a sum of multiple relation vectors for its relation part, rather than a single vector. Compared to TripleRE, TranS applies synthetic relation vectors *only* to the relation part of the scoring function, unlike TripleRE which applies three relations to three different parts (head, tail, relation) \\cite{zhang2022}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The proposed model, TranS, replaces the traditional single relation vector (`r`) with a \"synthetic relation representation\" \\cite{zhang2022}.\n    *   **Scoring Function**: The core scoring pattern is `||R_h - R_t + R_r||`, where `R_h` is the head representation, `R_t` is the tail representation, and `R_r` is the novel synthetic relation representation \\cite{zhang2022}.\n    *   **Synthetic Relation Representation**: `R_r` is defined as the sum of three distinct relation vectors: `~r  h + r + ^r  t`.\n        *   `r`: a main relation vector (similar to traditional models).\n        *   `~r  h`: an auxiliary relation vector dynamically combined with the head entity (`h`) using a Hadamard product (``).\n        *   `^r  t`: an auxiliary relation vector dynamically combined with the tail entity (`t`) using a Hadamard product (``).\n    *   **Context-Aware Entity Representations**: `R_h` and `R_t` are also made context-aware using Hadamard products: `R_h = h  ~t` and `R_t = t  ~h`, where `~h` and `~t` are auxiliary entity vectors \\cite{zhang2022}.\n    *   **OOV Handling**: Incorporates NodePiece \\cite{zhang2022} to learn a fixed-size entity vocabulary, addressing out-of-vocabulary (OOV) issues for large KGs.\n    *   **Loss Function**: Utilizes self-adversarial negative sampling loss during training \\cite{zhang2022}.\n\n*   **Key Technical Contributions**\n    *   **Novel Synthetic Relation Representation**: The primary innovation is the introduction of a dynamic, synthetic relation vector (`~r  h + r + ^r  t`) that adapts based on the specific head and tail entities, effectively addressing the limitation of single relation vectors for complex scenarios \\cite{zhang2022}.\n    *   **Contextualized Entity and Relation Embeddings**: The use of Hadamard products to create `R_h = h  ~t`, `R_t = t  ~h`, and the components of `R_r` allows for richer, context-dependent representations of entities and relations within a triplet \\cite{zhang2022}.\n    *   **Improved Handling of Complex Relations**: Provides a more robust mechanism for modeling multiple distinct relationships between the same entity pair, a significant challenge for previous transition-based models \\cite{zhang2022}.\n    *   **Parameter Efficiency**: Achieves state-of-the-art performance with a significantly reduced number of parameters compared to leading baselines \\cite{zhang2022}.\n\n*   **Experimental Validation**\n    *   **Dataset**: Evaluated on the large-scale ogbl-wikikg2 dataset \\cite{zhang2022}, which contains 2.5 million entities, 535 relation types, and over 17 million edges. The dataset uses a time-based split to simulate realistic KG completion.\n    *   **Metrics**: Performance is measured using Mean Reciprocal Rank (MRR) with the standard filtered metric \\cite{zhang2022}.\n    *   **Key Results**:\n        *   TranS + NodePiece achieved state-of-the-art results with 0.6988 MRR on the validation set and 0.6882 MRR on the test set \\cite{zhang2022}.\n        *   It outperformed the previous best model, TripleREv3 + NodePiece (0.6955 val MRR, 0.6866 test MRR), while using approximately half the parameters (19.2M vs. 36.4M) \\cite{zhang2022}.\n        *   A larger TranS model (38.4M parameters) further improved performance to 0.7101 val MRR and 0.6992 test MRR \\cite{zhang2022}.\n    *   **Implementation**: Adam optimizer, learning rate 0.0005, batch size 512, dropout 0.1, negative sampling size 128, embedding dimension 200, 20k NodePiece anchors \\cite{zhang2022}.\n\n*   **Limitations & Scope**\n    *   **Focus on Transition-based Models**: The primary scope of the paper is to improve transition-based KGE models, and its direct comparison to other KGE paradigms (e.g., semantic matching, neural networks) is not extensively explored \\cite{zhang2022}.\n    *   **Dependency on NodePiece**: The reported state-of-the-art results are achieved in conjunction with NodePiece for entity representation, suggesting that the core TranS model benefits from this external component for large-scale KGs with OOV entities \\cite{zhang2022}.\n    *   **Lack of Ablation Study**: The paper does not include an ablation study to individually quantify the contribution of each component of the synthetic relation representation (`~r  h`, `r`, `^r  t`) or the Hadamard product for entity representations \\cite{zhang2022}.\n\n*   **Technical Significance**\n    *   **Advancement in KGEs**: TranS significantly advances the state-of-the-art in transition-based KGEs by providing an effective solution to the long-standing problem of modeling complex relations, particularly when multiple relations exist between the same entity pair \\cite{zhang2022}.\n    *   **High Performance with Efficiency**: Achieves superior performance on a challenging large-scale benchmark while maintaining or improving parameter efficiency, which is crucial for practical applications \\cite{zhang2022}.\n    *   **Future Research Direction**: The concept of synthetic, context-aware relation representation opens new avenues for research into more dynamic and adaptive modeling of relationships in knowledge graphs and other graph-structured data \\cite{zhang2022}.",
      "intriguing_abstract": "Traditional transition-based Knowledge Graph Embedding (KGE) models often falter when faced with the pervasive challenge of complex relations, particularly when multiple distinct relationships exist between the same entity pair. Their reliance on static, single relation vectors proves fundamentally inadequate for capturing such intricate semantic nuances. We introduce **TranS**, a novel transition-based KGE model that revolutionizes relation modeling through a dynamic, **synthetic relation representation**.\n\nUnlike prior approaches, TranS replaces the static relation vector with a sophisticated combination of a main relation vector and two auxiliary relation vectors dynamically modulated by the head and tail entities via **Hadamard products**. This innovative approach also extends to **context-aware entity embeddings**, allowing for richer, triplet-specific representations. Evaluated on the large-scale **ogbl-wikikg2** dataset, TranS achieves **state-of-the-art (SOTA) performance** with remarkable **parameter efficiency**, outperforming leading baselines while using significantly fewer parameters. TranS offers a robust and efficient solution for modeling complex relations, pushing the boundaries of KGE research and opening new avenues for dynamic knowledge graph understanding.",
      "keywords": [
        "Knowledge Graph Embedding",
        "Transition-based KGE methods",
        "Synthetic relation representation",
        "Complex relations modeling",
        "Context-aware entity representations",
        "Hadamard product",
        "Parameter efficiency",
        "State-of-the-art performance",
        "NodePiece",
        "Self-adversarial negative sampling",
        "ogbl-wikikg2 dataset",
        "Multiple relations between entity pairs"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/b2d2ad9a458bdcb0523d22be659eb013ca2d3c67.pdf",
      "citation_key": "zhang2022",
      "metadata": {
        "title": "TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation",
        "authors": [
          "Xuanyu Zhang",
          "Qing Yang",
          "Dongliang Xu"
        ],
        "published_date": "2022",
        "abstract": "Knowledge graph embedding (KGE) aims to learn continuous vectors of relations and entities in knowledge graph. Recently, transition-based KGE methods have achieved promising performance, where the single relation vector learns to translate head entity to tail entity. However, this scoring pattern is not suitable for complex scenarios where the same entity pair has different relations. Previous models usually focus on the improvement of entity representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single relation vector. In this paper, we propose a novel transition-based method, TranS, for knowledge graph embedding. The single relation vector in traditional scoring patterns is replaced with synthetic relation representation, which can solve these issues effectively and efficiently. Experiments on a large knowledge graph dataset, ogbl-wikikg2, show that our model achieves state-of-the-art results.",
        "file_path": "paper_data/knowledge_graph_embedding/b2d2ad9a458bdcb0523d22be659eb013ca2d3c67.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation\" \\cite{zhang2022} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Problem**: Traditional transition-based Knowledge Graph Embedding (KGE) methods, like TransE, use a single relation vector to translate a head entity to a tail entity. This scoring pattern is inadequate for complex scenarios, especially when the *same entity pair* can have *multiple different relations* (e.g., \"professor\" and \"employer\" between a person and a university, or multiple roles for a person in a film) \\cite{zhang2022}.\n    *   **Importance & Challenge**: Knowledge graphs often contain such complex relationships (one-to-many, many-to-one, many-to-many). Previous models (e.g., TransH/R/D) focused on improving entity representations for these complex relations but still relied on a single, static relation vector, which limits their ability to capture the nuances of multiple relationships between the same entities \\cite{zhang2022}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work is positioned within the family of transition-based KGE methods, which are popular due to their simplicity and effectiveness \\cite{zhang2022}. It builds upon the foundational idea of TransE (h + r  t).\n    *   **Limitations of Previous Solutions**:\n        *   TransE struggles with complex relations like one-to-many/many-to-one/many-to-many \\cite{zhang2022}.\n        *   Subsequent models (e.g., TransH, TransR, TransD) improved entity representations (e.g., using hyperplanes or multiple embedding spaces) to handle complex relation types, but critically, they *still used a single relation vector* (`r`) in their scoring patterns. This single vector cannot differentiate between multiple relationships for the same entity pair \\cite{zhang2022}.\n        *   Compared to InterHT, TranS uses a sum of multiple relation vectors for its relation part, rather than a single vector. Compared to TripleRE, TranS applies synthetic relation vectors *only* to the relation part of the scoring function, unlike TripleRE which applies three relations to three different parts (head, tail, relation) \\cite{zhang2022}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The proposed model, TranS, replaces the traditional single relation vector (`r`) with a \"synthetic relation representation\" \\cite{zhang2022}.\n    *   **Scoring Function**: The core scoring pattern is `||R_h - R_t + R_r||`, where `R_h` is the head representation, `R_t` is the tail representation, and `R_r` is the novel synthetic relation representation \\cite{zhang2022}.\n    *   **Synthetic Relation Representation**: `R_r` is defined as the sum of three distinct relation vectors: `~r  h + r + ^r  t`.\n        *   `r`: a main relation vector (similar to traditional models).\n        *   `~r  h`: an auxiliary relation vector dynamically combined with the head entity (`h`) using a Hadamard product (``).\n        *   `^r  t`: an auxiliary relation vector dynamically combined with the tail entity (`t`) using a Hadamard product (``).\n    *   **Context-Aware Entity Representations**: `R_h` and `R_t` are also made context-aware using Hadamard products: `R_h = h  ~t` and `R_t = t  ~h`, where `~h` and `~t` are auxiliary entity vectors \\cite{zhang2022}.\n    *   **OOV Handling**: Incorporates NodePiece \\cite{zhang2022} to learn a fixed-size entity vocabulary, addressing out-of-vocabulary (OOV) issues for large KGs.\n    *   **Loss Function**: Utilizes self-adversarial negative sampling loss during training \\cite{zhang2022}.\n\n*   **Key Technical Contributions**\n    *   **Novel Synthetic Relation Representation**: The primary innovation is the introduction of a dynamic, synthetic relation vector (`~r  h + r + ^r  t`) that adapts based on the specific head and tail entities, effectively addressing the limitation of single relation vectors for complex scenarios \\cite{zhang2022}.\n    *   **Contextualized Entity and Relation Embeddings**: The use of Hadamard products to create `R_h = h  ~t`, `R_t = t  ~h`, and the components of `R_r` allows for richer, context-dependent representations of entities and relations within a triplet \\cite{zhang2022}.\n    *   **Improved Handling of Complex Relations**: Provides a more robust mechanism for modeling multiple distinct relationships between the same entity pair, a significant challenge for previous transition-based models \\cite{zhang2022}.\n    *   **Parameter Efficiency**: Achieves state-of-the-art performance with a significantly reduced number of parameters compared to leading baselines \\cite{zhang2022}.\n\n*   **Experimental Validation**\n    *   **Dataset**: Evaluated on the large-scale ogbl-wikikg2 dataset \\cite{zhang2022}, which contains 2.5 million entities, 535 relation types, and over 17 million edges. The dataset uses a time-based split to simulate realistic KG completion.\n    *   **Metrics**: Performance is measured using Mean Reciprocal Rank (MRR) with the standard filtered metric \\cite{zhang2022}.\n    *   **Key Results**:\n        *   TranS + NodePiece achieved state-of-the-art results with 0.6988 MRR on the validation set and 0.6882 MRR on the test set \\cite{zhang2022}.\n        *   It outperformed the previous best model, TripleREv3 + NodePiece (0.6955 val MRR, 0.6866 test MRR), while using approximately half the parameters (19.2M vs. 36.4M) \\cite{zhang2022}.\n        *   A larger TranS model (38.4M parameters) further improved performance to 0.7101 val MRR and 0.6992 test MRR \\cite{zhang2022}.\n    *   **Implementation**: Adam optimizer, learning rate 0.0005, batch size 512, dropout 0.1, negative sampling size 128, embedding dimension 200, 20k NodePiece anchors \\cite{zhang2022}.\n\n*   **Limitations & Scope**\n    *   **Focus on Transition-based Models**: The primary scope of the paper is to improve transition-based KGE models, and its direct comparison to other KGE paradigms (e.g., semantic matching, neural networks) is not extensively explored \\cite{zhang2022}.\n    *   **Dependency on NodePiece**: The reported state-of-the-art results are achieved in conjunction with NodePiece for entity representation, suggesting that the core TranS model benefits from this external component for large-scale KGs with OOV entities \\cite{zhang2022}.\n    *   **Lack of Ablation Study**: The paper does not include an ablation study to individually quantify the contribution of each component of the synthetic relation representation (`~r  h`, `r`, `^r  t`) or the Hadamard product for entity representations \\cite{zhang2022}.\n\n*   **Technical Significance**\n    *   **Advancement in KGEs**: TranS significantly advances the state-of-the-art in transition-based KGEs by providing an effective solution to the long-standing problem of modeling complex relations, particularly when multiple relations exist between the same entity pair \\cite{zhang2022}.\n    *   **High Performance with Efficiency**: Achieves superior performance on a challenging large-scale benchmark while maintaining or improving parameter efficiency, which is crucial for practical applications \\cite{zhang2022}.\n    *   **Future Research Direction**: The concept of synthetic, context-aware relation representation opens new avenues for research into more dynamic and adaptive modeling of relationships in knowledge graphs and other graph-structured data \\cite{zhang2022}.",
        "keywords": [
          "Knowledge Graph Embedding",
          "Transition-based KGE methods",
          "Synthetic relation representation",
          "Complex relations modeling",
          "Context-aware entity representations",
          "Hadamard product",
          "Parameter efficiency",
          "State-of-the-art performance",
          "NodePiece",
          "Self-adversarial negative sampling",
          "ogbl-wikikg2 dataset",
          "Multiple relations between entity pairs"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67.pdf"
    },
    {
      "success": true,
      "doc_id": "5ff0a12a016628efcc3282a51d08153d",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Conventional Knowledge Graph Embedding (KGE) methods struggle to efficiently update incremental knowledge in dynamic service ecosystems \\cite{sun2024}.\n    *   **Importance & Challenge**: This inefficiency significantly hinders the effectiveness of intelligent web applications that rely on continuously updated service knowledge \\cite{sun2024}.\n\n*   **Related Work & Positioning**\n    *   **Relation**: The work builds upon meta-learning strategies for KGE \\cite{sun2024}.\n    *   **Limitations of Previous Solutions**: Existing meta-learning KGE studies primarily focus on local entity information, which can lead to issues like spatial deformation and less effective representation of unseen entities \\cite{sun2024}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper introduces MetaHG, a meta-learning strategy for KGE designed to handle continuous updates of service knowledge \\cite{sun2024}.\n    *   **Novelty**:\n        *   Unlike prior work, MetaHG incorporates *both local and potential global structural information* from current knowledge graph snapshots \\cite{sun2024}.\n        *   It initializes entity embeddings using 'in' and 'out' relationship matrices \\cite{sun2024}.\n        *   Embeddings are refined through a novel *hybrid Graph Neural Network (GNN) framework* comprising a GNN layer for local information and a Hypergraph Neural Network (HGNN) layer for potential global information \\cite{sun2024}.\n        *   The meta-learning strategy facilitates the transfer of meta-knowledge for accurate representation of emerging entities \\cite{sun2024}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: MetaHG, a meta-learning strategy specifically designed for dynamic KGE updates, integrating both local and global structural information \\cite{sun2024}.\n    *   **System Design/Architectural Innovations**: A hybrid GNN framework combining a standard GNN layer with an HGNN layer to capture multi-faceted graph information \\cite{sun2024}.\n    *   **Techniques**: Initialization of entity embeddings using 'in' and 'out' relationship matrices to provide a robust starting point \\cite{sun2024}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on a self-collected clothing industry service dataset and two publicly available open-source KG datasets \\cite{sun2024}.\n    *   **Key Performance Metrics & Results**: Compared against several baselines, MetaHG demonstrated superior performance in generating high-quality embeddings for emerging entities and effectively updating service knowledge dynamically \\cite{sun2024}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly assumes the availability of current snapshot KGs for extracting local and global structural information \\cite{sun2024}. While it mitigates spatial deformation, the extent of its robustness to highly sparse or rapidly evolving graphs is not explicitly detailed as a limitation.\n    *   **Scope of Applicability**: Primarily focused on dynamic service ecosystems and intelligent web applications requiring continuous knowledge updates \\cite{sun2024}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: MetaHG significantly advances the state-of-the-art in KGE by providing an effective solution for continuously updating incremental knowledge, particularly for emerging entities, and by mitigating issues like spatial deformation \\cite{sun2024}.\n    *   **Potential Impact**: This work has the potential to enhance the adaptability and effectiveness of intelligent web applications and other systems operating in dynamic knowledge environments, by enabling more accurate and timely representation of evolving service knowledge \\cite{sun2024}.",
      "intriguing_abstract": "Dynamic service ecosystems demand Knowledge Graph Embedding (KGE) methods capable of efficiently updating incremental knowledge, a critical challenge that conventional approaches struggle to address, hindering the adaptability of intelligent web applications. Existing meta-learning KGE solutions, often limited to local entity information, suffer from spatial deformation and inadequate representation of unseen entities. We introduce MetaHG, a novel meta-learning strategy designed for continuous KGE updates. MetaHG uniquely integrates *both local and potential global structural information* from current knowledge graph snapshots. It initializes entity embeddings using 'in' and 'out' relationship matrices, then refines them through an innovative *hybrid Graph Neural Network (GNN) framework* combining a standard GNN layer for local context and a Hypergraph Neural Network (HGNN) layer for capturing global insights. This meta-learning approach facilitates robust meta-knowledge transfer, enabling accurate and timely representation of emerging entities. Extensive experiments on diverse datasets demonstrate MetaHG's superior performance in dynamically updating service knowledge and generating high-quality embeddings. MetaHG significantly advances the state-of-the-art in KGE, offering a powerful and adaptable solution for intelligent systems operating in rapidly evolving knowledge environments.",
      "keywords": [
        "MetaHG",
        "Knowledge Graph Embedding (KGE)",
        "meta-learning strategy",
        "dynamic knowledge updates",
        "hybrid Graph Neural Network (GNN) framework",
        "Hypergraph Neural Network (HGNN)",
        "local and global structural information",
        "emerging entities",
        "spatial deformation mitigation",
        "'in' and 'out' relationship matrices",
        "dynamic service ecosystems",
        "intelligent web applications",
        "high-quality embeddings",
        "continuous service knowledge updates"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/ce7291c5cd919a97ced6369ca697db9849848688.pdf",
      "citation_key": "sun2024",
      "metadata": {
        "title": "Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning",
        "authors": [
          "Hongliang Sun",
          "Jinlan Liu",
          "Can Wang",
          "Dianbo Sui",
          "Zhiying Tu",
          "Xiaofei Xu"
        ],
        "published_date": "2024",
        "abstract": "In the context of dynamic service ecosystems, the inability of conventional knowledge graph embedding (KGE) methods to efficiently update incremental knowledge poses a significant challenge for the effectiveness of intelligent web applications. To address the continuous updating challenges of service knowledge, this paper introduces MetaHG, a meta-learning strategy for KGE. Unlike existing meta-learning KGE studies that focus solely on local entity information, MetaHG incorporates both local and potential global structural information from current snapshots seen knowledge graphs (KGs) to mitigate issues such as spatial deformation and enhance the representation of unseen entities. Our approach initializes entity embeddings using in and out relationship matrices and refines them through a hybrid graph neural network (GNN) framework, which includes a GNN layer for local information and a hypergraph neural network (HGNN) layer for potential global information. The meta-learning strategy embedded in MetaHG effectively transfers meta-knowledge for the accurate representation of emerging entities. Extensive experiments are conducted on a self-collected clothing industry service dataset and two publicly available open-source KG datasets. By comparing with several baselines, experiment results demonstrate the superior performance of MetaHG in generating high-quality embeddings for emerging entities and dynamically updating service knowledge.",
        "file_path": "paper_data/knowledge_graph_embedding/ce7291c5cd919a97ced6369ca697db9849848688.pdf",
        "venue": "2024 IEEE International Conference on Web Services (ICWS)",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Conventional Knowledge Graph Embedding (KGE) methods struggle to efficiently update incremental knowledge in dynamic service ecosystems \\cite{sun2024}.\n    *   **Importance & Challenge**: This inefficiency significantly hinders the effectiveness of intelligent web applications that rely on continuously updated service knowledge \\cite{sun2024}.\n\n*   **Related Work & Positioning**\n    *   **Relation**: The work builds upon meta-learning strategies for KGE \\cite{sun2024}.\n    *   **Limitations of Previous Solutions**: Existing meta-learning KGE studies primarily focus on local entity information, which can lead to issues like spatial deformation and less effective representation of unseen entities \\cite{sun2024}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper introduces MetaHG, a meta-learning strategy for KGE designed to handle continuous updates of service knowledge \\cite{sun2024}.\n    *   **Novelty**:\n        *   Unlike prior work, MetaHG incorporates *both local and potential global structural information* from current knowledge graph snapshots \\cite{sun2024}.\n        *   It initializes entity embeddings using 'in' and 'out' relationship matrices \\cite{sun2024}.\n        *   Embeddings are refined through a novel *hybrid Graph Neural Network (GNN) framework* comprising a GNN layer for local information and a Hypergraph Neural Network (HGNN) layer for potential global information \\cite{sun2024}.\n        *   The meta-learning strategy facilitates the transfer of meta-knowledge for accurate representation of emerging entities \\cite{sun2024}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: MetaHG, a meta-learning strategy specifically designed for dynamic KGE updates, integrating both local and global structural information \\cite{sun2024}.\n    *   **System Design/Architectural Innovations**: A hybrid GNN framework combining a standard GNN layer with an HGNN layer to capture multi-faceted graph information \\cite{sun2024}.\n    *   **Techniques**: Initialization of entity embeddings using 'in' and 'out' relationship matrices to provide a robust starting point \\cite{sun2024}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on a self-collected clothing industry service dataset and two publicly available open-source KG datasets \\cite{sun2024}.\n    *   **Key Performance Metrics & Results**: Compared against several baselines, MetaHG demonstrated superior performance in generating high-quality embeddings for emerging entities and effectively updating service knowledge dynamically \\cite{sun2024}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly assumes the availability of current snapshot KGs for extracting local and global structural information \\cite{sun2024}. While it mitigates spatial deformation, the extent of its robustness to highly sparse or rapidly evolving graphs is not explicitly detailed as a limitation.\n    *   **Scope of Applicability**: Primarily focused on dynamic service ecosystems and intelligent web applications requiring continuous knowledge updates \\cite{sun2024}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: MetaHG significantly advances the state-of-the-art in KGE by providing an effective solution for continuously updating incremental knowledge, particularly for emerging entities, and by mitigating issues like spatial deformation \\cite{sun2024}.\n    *   **Potential Impact**: This work has the potential to enhance the adaptability and effectiveness of intelligent web applications and other systems operating in dynamic knowledge environments, by enabling more accurate and timely representation of evolving service knowledge \\cite{sun2024}.",
        "keywords": [
          "MetaHG",
          "Knowledge Graph Embedding (KGE)",
          "meta-learning strategy",
          "dynamic knowledge updates",
          "hybrid Graph Neural Network (GNN) framework",
          "Hypergraph Neural Network (HGNN)",
          "local and global structural information",
          "emerging entities",
          "spatial deformation mitigation",
          "'in' and 'out' relationship matrices",
          "dynamic service ecosystems",
          "intelligent web applications",
          "high-quality embeddings",
          "continuous service knowledge updates"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "ce7291c5cd919a97ced6369ca697db9849848688.pdf"
    },
    {
      "success": true,
      "doc_id": "322c2a8947459854eeade0874993cef3",
      "summary": "Here's a focused summary of the paper \\cite{wang2024} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Temporal Knowledge Graph Completion (TKGC) methods struggle to effectively capture the complex and diverse geometric structures (e.g., ring, hierarchical, chain) inherent in Temporal Knowledge Graphs (TKGs). This is primarily because they either model TKGs in a single embedding space or neglect the heterogeneity and \"spatial gap\" between different curvature spaces. Additionally, current feature fusion mechanisms are often computationally complex or use fixed pooling strategies that fail to retain important information.\n    *   **Importance and Challenge**: TKGs are crucial for capturing the dynamic evolution of real-world knowledge, but their incompleteness hinders knowledge-driven systems. The challenge lies in developing a TKGC model that can simultaneously represent diverse geometric patterns, bridge the semantic gaps between different embedding spaces, and efficiently fuse information to make accurate predictions.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon both Euclidean embedding-based (e.g., TransE, RotatE, ConvE, QDN) and Non-Euclidean embedding-based (e.g., ATTH, MuRMP, DyERNIE, BiQCap) KGC methods. It extends the concept of multi-curvature embeddings, previously explored in static KGC and some TKGC methods, by introducing explicit mechanisms to manage inter-space relationships and adaptive pooling.\n    *   **Limitations of Previous Solutions**:\n        *   Most TKGC methods model TKGs in a singular space, failing to capture the intricate geometric structures (e.g., tree-like, ring-like) that often coexist within TKGs.\n        *   Existing multi-curvature TKGC methods typically overlook the \"spatial gap\" and heterogeneity among different curvature spaces, limiting their expressive capacity.\n        *   Prior feature fusion methods either incur high computational complexity (sophisticated mechanisms) or use fixed pooling strategies (average/max pooling) that may not effectively preserve important information.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes the **Integrating Multi-curvature shared and specific Embedding (IME)** model for TKGC.\n        *   IME simultaneously models TKGs in **multi-curvature spaces**: hyperspherical, hyperbolic, and Euclidean spaces, recognizing their distinct strengths in capturing different geometric structures.\n        *   It incorporates a **quadruplet distributor** within each space to facilitate information aggregation and distribution among entities, relations, and timestamps.\n        *   IME learns two key properties:\n            *   **Space-shared property**: Captures commonalities across different curvature spaces using shared parameters, aiming to mitigate the \"spatial gap.\"\n            *   **Space-specific property**: Captures characteristic features unique to each curvature space using specific parameters.\n        *   It introduces an **Adjustable Multi-curvature Pooling (AMP)** approach, which learns appropriate pooling weights to achieve a superior pooling strategy, effectively retaining important information.\n        *   IME innovatively designs **similarity, difference, and structure loss functions** to guide the learning process.\n    *   **Novelty/Difference**:\n        *   First to explicitly integrate both \"space-shared\" and \"space-specific\" properties in multi-curvature TKGC to simultaneously bridge spatial gaps and capture unique features.\n        *   Proposes an adaptive pooling mechanism (AMP) that learns optimal pooling weights, moving beyond fixed pooling strategies.\n        *   Introduces the concept of \"structure loss\" into TKGC tasks to ensure structural similarity of quadruplets across various curvature spaces.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The IME model, which integrates multi-curvature embeddings with space-shared and space-specific property learning.\n        *   The Adjustable Multi-curvature Pooling (AMP) module for adaptive information fusion.\n        *   Novel similarity, difference, and structure loss functions specifically designed for multi-curvature TKGC.\n    *   **System Design/Architectural Innovations**: Adaptation of the quadruplet distributor for information aggregation and distribution within each of the multi-curvature spaces.\n    *   **Theoretical Insights/Analysis**: The explicit recognition and modeling of the \"spatial gap\" between different curvature spaces and the proposal of mechanisms (space-shared property, structure loss) to address it.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were conducted on \"several widely used datasets\" to evaluate IME's performance against existing state-of-the-art TKGC models.\n    *   **Key Performance Metrics and Comparison Results**: The paper states that experimental results \"clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models\" and that IME \"achieves competitive performance.\" While specific metrics (e.g., MRR, Hits@k) are not detailed in the abstract, these are standard for TKGC tasks.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The abstract does not explicitly state technical limitations or assumptions of IME itself, but rather focuses on addressing the limitations of prior work.\n    *   **Scope of Applicability**: The model is designed for Temporal Knowledge Graph Completion tasks, specifically predicting missing entities, relations, or temporal attributes in dynamic knowledge graphs.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: IME significantly advances the technical state-of-the-art in TKGC by providing a more comprehensive and nuanced approach to modeling the complex geometric structures of TKGs. By effectively integrating multi-curvature spaces, bridging spatial gaps, and employing adaptive information fusion, it improves the accuracy and completeness of TKG predictions.\n    *   **Potential Impact on Future Research**: The novel concepts of space-shared/specific properties, adjustable pooling, and structure loss could inspire future research in multi-modal or multi-space embeddings for other complex data structures, adaptive fusion mechanisms, and the design of more sophisticated loss functions that account for structural consistency across different representations.",
      "intriguing_abstract": "The dynamic evolution of real-world knowledge, captured by Temporal Knowledge Graphs (TKGs), is crucial for intelligent systems, yet their inherent incompleteness and complex geometric structures present a formidable challenge for Temporal Knowledge Graph Completion (TKGC). Existing methods often struggle to effectively model the diverse geometric patterns (e.g., hierarchical, ring-like) within TKGs, either by confining representations to a single embedding space or by neglecting the critical \"spatial gap\" and heterogeneity between different curvature spaces.\n\nWe introduce the **Integrating Multi-curvature shared and specific Embedding (IME)** model, a novel paradigm that simultaneously leverages hyperspherical, hyperbolic, and Euclidean spaces to capture the full spectrum of TKG geometries. IME innovatively learns both **space-shared properties** to bridge inter-space semantic gaps and **space-specific properties** to capture unique features inherent to each curvature. Furthermore, our **Adjustable Multi-curvature Pooling (AMP)** adaptively fuses information, moving beyond fixed pooling strategies, complemented by a novel **structure loss** to ensure cross-space consistency. Extensive experiments demonstrate IME's superior performance over state-of-the-art TKGC models, offering a robust solution for dynamic knowledge inference and paving the way for advanced multi-space representation learning.",
      "keywords": [
        "Temporal Knowledge Graph Completion (TKGC)",
        "Multi-curvature embedding spaces",
        "Integrating Multi-curvature shared and specific Embedding (IME)",
        "Space-shared and space-specific properties",
        "Adjustable Multi-curvature Pooling (AMP)",
        "Spatial gap bridging",
        "Structure loss function",
        "Adaptive information fusion",
        "Diverse geometric structures",
        "Quadruplet distributor",
        "Dynamic knowledge evolution"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/780bc77fac1aaf460ba191daa218f3c111119092.pdf",
      "citation_key": "wang2024",
      "metadata": {
        "title": "IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion",
        "authors": [
          "Jiapu Wang",
          "Zheng Cui",
          "Boyue Wang",
          "Shirui Pan",
          "Junbin Gao",
          "Baocai Yin",
          "Wen Gao"
        ],
        "published_date": "2024",
        "abstract": "Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing for a precise capture of the evolution of knowledge and reflecting the dynamic nature of the real world. Typically, TKGs contain complex geometric structures, with various geometric structures interwoven. However, existing Temporal Knowledge Graph Completion (TKGC) methods either model TKGs in a single space or neglect the heterogeneity of different curvature spaces, thus constraining their capacity to capture these intricate geometric structures. In this paper, we propose a novel Integrating Multi-curvature shared and specific Embedding (IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature spaces, including hyperspherical, hyperbolic, and Euclidean spaces. Subsequently, IME incorporates two key properties, namely space-shared property and space-specific property. The space-shared property facilitates the learning of commonalities across different curvature spaces and alleviates the spatial gap caused by the heterogeneous nature of multi-curvature spaces, while the space-specific property captures characteristic features. Meanwhile, IME proposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively retain important information. Furthermore, IME innovatively designs similarity, difference, and structure loss functions to attain the stated objective. Experimental results clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models.",
        "file_path": "paper_data/knowledge_graph_embedding/780bc77fac1aaf460ba191daa218f3c111119092.pdf",
        "venue": "The Web Conference",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{wang2024} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Temporal Knowledge Graph Completion (TKGC) methods struggle to effectively capture the complex and diverse geometric structures (e.g., ring, hierarchical, chain) inherent in Temporal Knowledge Graphs (TKGs). This is primarily because they either model TKGs in a single embedding space or neglect the heterogeneity and \"spatial gap\" between different curvature spaces. Additionally, current feature fusion mechanisms are often computationally complex or use fixed pooling strategies that fail to retain important information.\n    *   **Importance and Challenge**: TKGs are crucial for capturing the dynamic evolution of real-world knowledge, but their incompleteness hinders knowledge-driven systems. The challenge lies in developing a TKGC model that can simultaneously represent diverse geometric patterns, bridge the semantic gaps between different embedding spaces, and efficiently fuse information to make accurate predictions.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon both Euclidean embedding-based (e.g., TransE, RotatE, ConvE, QDN) and Non-Euclidean embedding-based (e.g., ATTH, MuRMP, DyERNIE, BiQCap) KGC methods. It extends the concept of multi-curvature embeddings, previously explored in static KGC and some TKGC methods, by introducing explicit mechanisms to manage inter-space relationships and adaptive pooling.\n    *   **Limitations of Previous Solutions**:\n        *   Most TKGC methods model TKGs in a singular space, failing to capture the intricate geometric structures (e.g., tree-like, ring-like) that often coexist within TKGs.\n        *   Existing multi-curvature TKGC methods typically overlook the \"spatial gap\" and heterogeneity among different curvature spaces, limiting their expressive capacity.\n        *   Prior feature fusion methods either incur high computational complexity (sophisticated mechanisms) or use fixed pooling strategies (average/max pooling) that may not effectively preserve important information.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes the **Integrating Multi-curvature shared and specific Embedding (IME)** model for TKGC.\n        *   IME simultaneously models TKGs in **multi-curvature spaces**: hyperspherical, hyperbolic, and Euclidean spaces, recognizing their distinct strengths in capturing different geometric structures.\n        *   It incorporates a **quadruplet distributor** within each space to facilitate information aggregation and distribution among entities, relations, and timestamps.\n        *   IME learns two key properties:\n            *   **Space-shared property**: Captures commonalities across different curvature spaces using shared parameters, aiming to mitigate the \"spatial gap.\"\n            *   **Space-specific property**: Captures characteristic features unique to each curvature space using specific parameters.\n        *   It introduces an **Adjustable Multi-curvature Pooling (AMP)** approach, which learns appropriate pooling weights to achieve a superior pooling strategy, effectively retaining important information.\n        *   IME innovatively designs **similarity, difference, and structure loss functions** to guide the learning process.\n    *   **Novelty/Difference**:\n        *   First to explicitly integrate both \"space-shared\" and \"space-specific\" properties in multi-curvature TKGC to simultaneously bridge spatial gaps and capture unique features.\n        *   Proposes an adaptive pooling mechanism (AMP) that learns optimal pooling weights, moving beyond fixed pooling strategies.\n        *   Introduces the concept of \"structure loss\" into TKGC tasks to ensure structural similarity of quadruplets across various curvature spaces.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The IME model, which integrates multi-curvature embeddings with space-shared and space-specific property learning.\n        *   The Adjustable Multi-curvature Pooling (AMP) module for adaptive information fusion.\n        *   Novel similarity, difference, and structure loss functions specifically designed for multi-curvature TKGC.\n    *   **System Design/Architectural Innovations**: Adaptation of the quadruplet distributor for information aggregation and distribution within each of the multi-curvature spaces.\n    *   **Theoretical Insights/Analysis**: The explicit recognition and modeling of the \"spatial gap\" between different curvature spaces and the proposal of mechanisms (space-shared property, structure loss) to address it.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were conducted on \"several widely used datasets\" to evaluate IME's performance against existing state-of-the-art TKGC models.\n    *   **Key Performance Metrics and Comparison Results**: The paper states that experimental results \"clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models\" and that IME \"achieves competitive performance.\" While specific metrics (e.g., MRR, Hits@k) are not detailed in the abstract, these are standard for TKGC tasks.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The abstract does not explicitly state technical limitations or assumptions of IME itself, but rather focuses on addressing the limitations of prior work.\n    *   **Scope of Applicability**: The model is designed for Temporal Knowledge Graph Completion tasks, specifically predicting missing entities, relations, or temporal attributes in dynamic knowledge graphs.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: IME significantly advances the technical state-of-the-art in TKGC by providing a more comprehensive and nuanced approach to modeling the complex geometric structures of TKGs. By effectively integrating multi-curvature spaces, bridging spatial gaps, and employing adaptive information fusion, it improves the accuracy and completeness of TKG predictions.\n    *   **Potential Impact on Future Research**: The novel concepts of space-shared/specific properties, adjustable pooling, and structure loss could inspire future research in multi-modal or multi-space embeddings for other complex data structures, adaptive fusion mechanisms, and the design of more sophisticated loss functions that account for structural consistency across different representations.",
        "keywords": [
          "Temporal Knowledge Graph Completion (TKGC)",
          "Multi-curvature embedding spaces",
          "Integrating Multi-curvature shared and specific Embedding (IME)",
          "Space-shared and space-specific properties",
          "Adjustable Multi-curvature Pooling (AMP)",
          "Spatial gap bridging",
          "Structure loss function",
          "Adaptive information fusion",
          "Diverse geometric structures",
          "Quadruplet distributor",
          "Dynamic knowledge evolution"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "780bc77fac1aaf460ba191daa218f3c111119092.pdf"
    },
    {
      "success": true,
      "doc_id": "c329295f22c1e9205d7a2f2ada925fd3",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### CPa-WAC : Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding \\cite{modak2024}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Addressing the significant computational and memory costs, and long training times associated with Graph Neural Network (GNN) models for Knowledge Graph Embedding (KGE), especially for large-scale Knowledge Graphs (KGs).\n    *   **Importance & Challenge:**\n        *   Scalability and training time are crucial for real-world KG applications (e.g., fraud detection, drug interaction prediction).\n        *   Existing state-of-the-art GNN-based KGE models (e.g., Comp-GCN, RAGAT, SEGNN) require high memory (GPU) and immense training time due to millions of trainable parameters, often limiting them to small batch sizes.\n        *   While partitioning KGs can reduce training time and enable parallel processing, it often leads to a significant reduction in prediction accuracy compared to training on the whole graph.\n        *   A challenge lies in effectively partitioning KGs with minimal cross-partition edges and developing a framework to merge individual embeddings for global inference without losing structural information.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Traditional KGE:** TransE, TransH, TransR, TransD, TransG (for link prediction, node classification).\n        *   **Semantic KGE:** Conv2D, RESCAL, ComplEX, TuckER, HAKE, SimplE (capture complex semantic relationships but require high embedding dimensionality).\n        *   **GNN-based KGE:** RGCN, GAT, and their integrated models (e.g., Comp-GCN, RAGAT, SEGNN) achieve high accuracy but suffer from high trainable parameters and long training times.\n        *   **Scalability Solutions:** KG augmentation (GreenKGC), feature pruning, partitioning, parallel training, multi-GPU training (DGL-KE).\n        *   **KG Partitioning:** Ontology-based partitioning \\cite{bai2023}, METIS \\cite{karypis1998}, k-means clustering \\cite{wang2022b, zheng2020}, edge-cut partitioning \\cite{sheikh2022}, workload-aware partitioning \\cite{priyadarshi2021}.\n    *   **Limitations of Previous Solutions:**\n        *   GNN-based models have high computational and memory costs.\n        *   Existing libraries (e.g., Pytorch-Biggraph, DGL-KE) do not fully address the scalability of GNN-based KGE algorithms.\n        *   Properly partitioning KGs with the least cross-partition edges remains challenging.\n        *   A framework is often lacking to effectively merge individual embeddings from partitioned subgraphs into a complete graph structure for global inference.\n        *   Many partitioning methods require node or edge features, which might not always be available or suitable for preserving topological structure.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (CPa-WAC):** A lightweight architecture that combines graph convolutional networks with modularity maximization-based constellation partitioning. It consists of three main stages:\n        1.  **Constellation Partitioning (CPa):**\n            *   Utilizes Louvain clustering \\cite{blondel2008} (or Leiden algorithm for comparison) to partition the KG into topological clusters based on edge density, without relying on node/edge attributes.\n            *   Constructs a symmetric weighted adjacency matrix from KG triples.\n            *   Employs a hierarchical merging strategy with multiple thresholds (, =, ) to merge small outlier clusters with larger, denser \"nearest linked neighbors\" (NLN) while capping cluster size to avoid entity explosion. This ensures a relatively similar number of entities per cluster and preserves overall graph modularity.\n        2.  **Weighted Aggregation Composition (WAC) Convolution:**\n            *   An improved compositional message-passing GCN algorithm.\n            *   Harnesses graph attention layers \\cite{liu2021} and two distinct composition functions (Eq. 2 & 3) for message aggregation, incorporating entity, relation, and learnable weight vectors.\n            *   Uses GELU activation function and an attention layer () after normalizing messages with the degree matrix (G).\n            *   Updates relation embeddings with a separate learnable weight vector (Eq. 5).\n            *   Decodes embeddings using a 1D Convolutional Neural Network (1D-CNN) with a multiplication operation similar to SimplE \\cite{kazemi2018}, followed by batch normalization.\n        3.  **Global Decoder (GD) Framework:**\n            *   A separate framework for global-level inference after cluster-specific embeddings are learned.\n            *   Concatenates upscaled feature vectors for all nodes and relations (e.g., `ec_u` from dimension `1xs` to `1xCs` with zero padding for other clusters).\n            *   These features are projected to lower dimensions using trainable weight matrices (We, Wr) and fed into a Multi-Layer Perceptron (MLP).\n            *   Trained end-to-end using a multiclass Binary Cross-Entropy (BCE) loss for link prediction.\n    *   **Novelty/Difference:**\n        *   Introduces a dedicated, topology-preserving KG partitioning algorithm (CPa) that does not require node/edge features and minimizes cross-cluster links through hierarchical merging and NLN strategy.\n        *   Proposes an enhanced compositional GCN (WAC) that integrates attention mechanisms and a 1D-CNN for robust embedding learning.\n        *   Develops a novel Global Decoder framework to effectively combine embeddings from independently trained partitions for global inference, overcoming a major challenge in partitioned KGE.\n\n4.  **Key Technical Contributions** \\cite{modak2024}\n    *   **Novel Algorithms/Methods:**\n        *   **CPa (Constellation Partitioning):** A novel KG partitioning algorithm utilizing fast Louvain clustering and a hierarchical merging strategy to create topological clusters while minimizing lost links between them.\n        *   **WAC (Weighted Aggregation Composition) Convolution:** An improved compositional-GCN algorithm that couples a multiplication operation with a 1D convolutional network, leveraging feature, entity, and relation-specific weights for effective embedding learning.\n    *   **System Design/Architectural Innovations:**\n        *   A modular, three-stage architecture (CPa, WAC, GD) that enables scalable KGE by decoupling partitioning, local embedding learning, and global inference.\n        *   **Global Decoder Framework:** A unique framework designed to aggregate and utilize node and relationship embeddings from different clusters to achieve global-level inference, addressing the challenge of combining partitioned results.\n    *   **Theoretical Insights/Analysis:**\n        *   Empirical verification that partitioning can speed up KGE without destroying the KG structure or jumbling inference logic, building on the idea that semantic features are locally contained \\cite{jain2021}.\n        *   Comparison and empirical verification of Louvain vs. Leiden algorithms for KG partitioning.\n\n5.  **Experimental Validation** \\cite{modak2024}\n    *   **Experiments Conducted:**\n        *   Training and evaluation of CPa-WAC on standard KGE benchmarks.\n        *   Comparison of CPa-WAC against several state-of-the-art KGE methods.\n        *   Analysis of the impact of partitioning on training time and prediction accuracy.\n        *   Empirical comparison of Louvain and Leiden algorithms for partitioning.\n    *   **Datasets:** WN18, WN18RR (Wordnet), FB15K, FB15K-237 (Freebase).\n    *   **Hardware:** I7-13700, 32 GB RAM, NVIDIA RTX A2000 12 GB GPU.\n    *   **Optimizer:** AdamW.\n    *   **Key Performance Metrics:** Prediction accuracy (implied by \"similar performance\" and \"outperforms\"), training time.\n    *   **Comparison Results:**\n        *   **Training Time:** CPa-WAC reduces training time by up to five times compared to training a GCN on the whole KG.\n        *   **Prediction Accuracy:** Achieves similar prediction performance to training a GCN on the entire KG, demonstrating that meaningful partitioning can retain accuracy.\n        *   **State-of-the-Art Comparison:** CPa-WAC outperforms several other state-of-the-art KGE methods in terms of prediction accuracy.\n\n6.  **Limitations & Scope** \\cite{modak2024}\n    *   **Technical Limitations/Assumptions:**\n        *   The Louvain clustering algorithm, while effective, has limitations in directly partitioning heterogeneous directed graphs, necessitating the hierarchical merging strategy.\n        *   Assumes that semantic features are primarily contained locally within graph partitions, allowing for effective partitioning without significant loss of global semantic information \\cite{jain2021}.\n    *   **Scope of Applicability:**\n        *   Primarily focused on scalable KGE for link prediction, node classification, and reasoning tasks.\n        *   The CPa partitioning method is particularly suited for KGs without explicit node or edge attributes, as it relies on graph topology and edge density.\n\n7.  **Technical Significance** \\cite{modak2024}\n    *   **Advancement of State-of-the-Art:** CPa-WAC significantly advances the technical state-of-the-art by effectively addressing the critical trade-off between scalability (training time, memory cost) and prediction accuracy in GNN-based KGE. It demonstrates that meaningful partitioning can lead to substantial speed-ups without compromising performance.\n    *   **Potential Impact on Future Research:**\n        *   Provides a robust and efficient framework for processing large-scale KGs, making GNN-based KGE more practical for real-world applications.\n        *   The novel partitioning strategy (CPa) and the Global Decoder framework offer new avenues for research into distributed and scalable graph learning.\n        *   Encourages further exploration of topology-aware partitioning methods that do not rely on feature information, broadening applicability.\n        *   The lightweight WAC convolution could inspire more efficient GCN designs for various graph-based tasks.",
      "intriguing_abstract": "The promise of Graph Neural Networks (GNNs) for Knowledge Graph Embedding (KGE) is often hampered by prohibitive computational and memory costs, rendering them impractical for large-scale knowledge graphs. Existing partitioning strategies typically sacrifice prediction accuracy for scalability. We introduce CPa-WAC, a novel, lightweight architecture that decisively resolves this critical trade-off.\n\nCPa-WAC comprises three innovative stages: **Constellation Partitioning (CPa)**, a topology-preserving algorithm leveraging Louvain clustering and hierarchical merging to minimize cross-partition links without relying on node/edge features; **Weighted Aggregation Composition (WAC)**, an enhanced compositional GCN integrating attention mechanisms and a 1D-CNN for robust local embedding; and a unique **Global Decoder (GD)** framework designed to seamlessly aggregate cluster-specific embeddings for accurate global inference. Our empirical evaluation demonstrates CPa-WAC achieves up to a 5x reduction in training time while maintaining or even surpassing the prediction accuracy of full-graph GNN models and outperforming state-of-the-art KGE methods on benchmark datasets. CPa-WAC offers a transformative solution, making high-performance GNN-based KGE truly scalable and practical for real-world applications.",
      "keywords": [
        "CPa-WAC",
        "Knowledge Graph Embedding (KGE)",
        "Graph Neural Networks (GNN)",
        "Scalability",
        "Constellation Partitioning (CPa)",
        "Weighted Aggregation Composition (WAC)",
        "Global Decoder Framework",
        "Topology-preserving partitioning",
        "Louvain clustering",
        "Reduced training time",
        "Prediction accuracy",
        "Link prediction",
        "Computational and memory costs"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/6205f75cb6db1503c94386441ca68c63c9cbd456.pdf",
      "citation_key": "modak2024",
      "metadata": {
        "title": "CPa-WAC: Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding",
        "authors": [
          "S. Modak",
          "Aakarsh Malhotra",
          "Sarthak Malik",
          "Anil Surisetty",
          "Esam Abdel-Raheem"
        ],
        "published_date": "2024",
        "abstract": "Scalability and training time are crucial for any graph neural network model processing a knowledge graph (KG). While partitioning knowledge graphs helps reduce the training time, the prediction accuracy reduces significantly compared to training the model on the whole graph. In this paper, we propose CPa-WAC: a lightweight architecture that incorporates graph convolutional networks and modularity maximization-based constellation partitioning to harness the power of local graph topology. The proposed CPa-WAC method reduces the training time and memory cost of knowledge graph embedding, making the learning model scalable. The results from our experiments on standard databases, such as Wordnet and Freebase, show that by achieving meaningful partitioning, any knowledge graph can be broken down into subgraphs and processed separately to learn embeddings. Furthermore, these learned embeddings can be used for knowledge graph completion, retaining similar performance compared to training a GCN on the whole KG, while speeding up the training process by upto five times. Additionally, the proposed CPa-WAC method outperforms several other state-of-the-art KG in terms of prediction accuracy.",
        "file_path": "paper_data/knowledge_graph_embedding/6205f75cb6db1503c94386441ca68c63c9cbd456.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### CPa-WAC : Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding \\cite{modak2024}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Addressing the significant computational and memory costs, and long training times associated with Graph Neural Network (GNN) models for Knowledge Graph Embedding (KGE), especially for large-scale Knowledge Graphs (KGs).\n    *   **Importance & Challenge:**\n        *   Scalability and training time are crucial for real-world KG applications (e.g., fraud detection, drug interaction prediction).\n        *   Existing state-of-the-art GNN-based KGE models (e.g., Comp-GCN, RAGAT, SEGNN) require high memory (GPU) and immense training time due to millions of trainable parameters, often limiting them to small batch sizes.\n        *   While partitioning KGs can reduce training time and enable parallel processing, it often leads to a significant reduction in prediction accuracy compared to training on the whole graph.\n        *   A challenge lies in effectively partitioning KGs with minimal cross-partition edges and developing a framework to merge individual embeddings for global inference without losing structural information.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Traditional KGE:** TransE, TransH, TransR, TransD, TransG (for link prediction, node classification).\n        *   **Semantic KGE:** Conv2D, RESCAL, ComplEX, TuckER, HAKE, SimplE (capture complex semantic relationships but require high embedding dimensionality).\n        *   **GNN-based KGE:** RGCN, GAT, and their integrated models (e.g., Comp-GCN, RAGAT, SEGNN) achieve high accuracy but suffer from high trainable parameters and long training times.\n        *   **Scalability Solutions:** KG augmentation (GreenKGC), feature pruning, partitioning, parallel training, multi-GPU training (DGL-KE).\n        *   **KG Partitioning:** Ontology-based partitioning \\cite{bai2023}, METIS \\cite{karypis1998}, k-means clustering \\cite{wang2022b, zheng2020}, edge-cut partitioning \\cite{sheikh2022}, workload-aware partitioning \\cite{priyadarshi2021}.\n    *   **Limitations of Previous Solutions:**\n        *   GNN-based models have high computational and memory costs.\n        *   Existing libraries (e.g., Pytorch-Biggraph, DGL-KE) do not fully address the scalability of GNN-based KGE algorithms.\n        *   Properly partitioning KGs with the least cross-partition edges remains challenging.\n        *   A framework is often lacking to effectively merge individual embeddings from partitioned subgraphs into a complete graph structure for global inference.\n        *   Many partitioning methods require node or edge features, which might not always be available or suitable for preserving topological structure.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (CPa-WAC):** A lightweight architecture that combines graph convolutional networks with modularity maximization-based constellation partitioning. It consists of three main stages:\n        1.  **Constellation Partitioning (CPa):**\n            *   Utilizes Louvain clustering \\cite{blondel2008} (or Leiden algorithm for comparison) to partition the KG into topological clusters based on edge density, without relying on node/edge attributes.\n            *   Constructs a symmetric weighted adjacency matrix from KG triples.\n            *   Employs a hierarchical merging strategy with multiple thresholds (, =, ) to merge small outlier clusters with larger, denser \"nearest linked neighbors\" (NLN) while capping cluster size to avoid entity explosion. This ensures a relatively similar number of entities per cluster and preserves overall graph modularity.\n        2.  **Weighted Aggregation Composition (WAC) Convolution:**\n            *   An improved compositional message-passing GCN algorithm.\n            *   Harnesses graph attention layers \\cite{liu2021} and two distinct composition functions (Eq. 2 & 3) for message aggregation, incorporating entity, relation, and learnable weight vectors.\n            *   Uses GELU activation function and an attention layer () after normalizing messages with the degree matrix (G).\n            *   Updates relation embeddings with a separate learnable weight vector (Eq. 5).\n            *   Decodes embeddings using a 1D Convolutional Neural Network (1D-CNN) with a multiplication operation similar to SimplE \\cite{kazemi2018}, followed by batch normalization.\n        3.  **Global Decoder (GD) Framework:**\n            *   A separate framework for global-level inference after cluster-specific embeddings are learned.\n            *   Concatenates upscaled feature vectors for all nodes and relations (e.g., `ec_u` from dimension `1xs` to `1xCs` with zero padding for other clusters).\n            *   These features are projected to lower dimensions using trainable weight matrices (We, Wr) and fed into a Multi-Layer Perceptron (MLP).\n            *   Trained end-to-end using a multiclass Binary Cross-Entropy (BCE) loss for link prediction.\n    *   **Novelty/Difference:**\n        *   Introduces a dedicated, topology-preserving KG partitioning algorithm (CPa) that does not require node/edge features and minimizes cross-cluster links through hierarchical merging and NLN strategy.\n        *   Proposes an enhanced compositional GCN (WAC) that integrates attention mechanisms and a 1D-CNN for robust embedding learning.\n        *   Develops a novel Global Decoder framework to effectively combine embeddings from independently trained partitions for global inference, overcoming a major challenge in partitioned KGE.\n\n4.  **Key Technical Contributions** \\cite{modak2024}\n    *   **Novel Algorithms/Methods:**\n        *   **CPa (Constellation Partitioning):** A novel KG partitioning algorithm utilizing fast Louvain clustering and a hierarchical merging strategy to create topological clusters while minimizing lost links between them.\n        *   **WAC (Weighted Aggregation Composition) Convolution:** An improved compositional-GCN algorithm that couples a multiplication operation with a 1D convolutional network, leveraging feature, entity, and relation-specific weights for effective embedding learning.\n    *   **System Design/Architectural Innovations:**\n        *   A modular, three-stage architecture (CPa, WAC, GD) that enables scalable KGE by decoupling partitioning, local embedding learning, and global inference.\n        *   **Global Decoder Framework:** A unique framework designed to aggregate and utilize node and relationship embeddings from different clusters to achieve global-level inference, addressing the challenge of combining partitioned results.\n    *   **Theoretical Insights/Analysis:**\n        *   Empirical verification that partitioning can speed up KGE without destroying the KG structure or jumbling inference logic, building on the idea that semantic features are locally contained \\cite{jain2021}.\n        *   Comparison and empirical verification of Louvain vs. Leiden algorithms for KG partitioning.\n\n5.  **Experimental Validation** \\cite{modak2024}\n    *   **Experiments Conducted:**\n        *   Training and evaluation of CPa-WAC on standard KGE benchmarks.\n        *   Comparison of CPa-WAC against several state-of-the-art KGE methods.\n        *   Analysis of the impact of partitioning on training time and prediction accuracy.\n        *   Empirical comparison of Louvain and Leiden algorithms for partitioning.\n    *   **Datasets:** WN18, WN18RR (Wordnet), FB15K, FB15K-237 (Freebase).\n    *   **Hardware:** I7-13700, 32 GB RAM, NVIDIA RTX A2000 12 GB GPU.\n    *   **Optimizer:** AdamW.\n    *   **Key Performance Metrics:** Prediction accuracy (implied by \"similar performance\" and \"outperforms\"), training time.\n    *   **Comparison Results:**\n        *   **Training Time:** CPa-WAC reduces training time by up to five times compared to training a GCN on the whole KG.\n        *   **Prediction Accuracy:** Achieves similar prediction performance to training a GCN on the entire KG, demonstrating that meaningful partitioning can retain accuracy.\n        *   **State-of-the-Art Comparison:** CPa-WAC outperforms several other state-of-the-art KGE methods in terms of prediction accuracy.\n\n6.  **Limitations & Scope** \\cite{modak2024}\n    *   **Technical Limitations/Assumptions:**\n        *   The Louvain clustering algorithm, while effective, has limitations in directly partitioning heterogeneous directed graphs, necessitating the hierarchical merging strategy.\n        *   Assumes that semantic features are primarily contained locally within graph partitions, allowing for effective partitioning without significant loss of global semantic information \\cite{jain2021}.\n    *   **Scope of Applicability:**\n        *   Primarily focused on scalable KGE for link prediction, node classification, and reasoning tasks.\n        *   The CPa partitioning method is particularly suited for KGs without explicit node or edge attributes, as it relies on graph topology and edge density.\n\n7.  **Technical Significance** \\cite{modak2024}\n    *   **Advancement of State-of-the-Art:** CPa-WAC significantly advances the technical state-of-the-art by effectively addressing the critical trade-off between scalability (training time, memory cost) and prediction accuracy in GNN-based KGE. It demonstrates that meaningful partitioning can lead to substantial speed-ups without compromising performance.\n    *   **Potential Impact on Future Research:**\n        *   Provides a robust and efficient framework for processing large-scale KGs, making GNN-based KGE more practical for real-world applications.\n        *   The novel partitioning strategy (CPa) and the Global Decoder framework offer new avenues for research into distributed and scalable graph learning.\n        *   Encourages further exploration of topology-aware partitioning methods that do not rely on feature information, broadening applicability.\n        *   The lightweight WAC convolution could inspire more efficient GCN designs for various graph-based tasks.",
        "keywords": [
          "CPa-WAC",
          "Knowledge Graph Embedding (KGE)",
          "Graph Neural Networks (GNN)",
          "Scalability",
          "Constellation Partitioning (CPa)",
          "Weighted Aggregation Composition (WAC)",
          "Global Decoder Framework",
          "Topology-preserving partitioning",
          "Louvain clustering",
          "Reduced training time",
          "Prediction accuracy",
          "Link prediction",
          "Computational and memory costs"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "6205f75cb6db1503c94386441ca68c63c9cbd456.pdf"
    },
    {
      "success": true,
      "doc_id": "2b3e66c64be4bf914abd2b53a022dcca",
      "summary": "Here's a focused summary of the paper \"SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions\" by \\cite{xiao2016} for a literature review:\n\n---\n\n### Analysis of \"SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions\" \\cite{xiao2016}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Most existing knowledge graph embedding (KGE) models primarily focus on symbolic fact triples (h, r, t) and do not fully leverage the rich, supplementary semantic information available in textual descriptions of entities and relations.\n    *   **Importance & Challenge:**\n        *   **Discovering Semantic Relevance:** Textual descriptions can help infer true triples that are difficult to deduce from symbolic triples alone (e.g., identifying family relations through shared keywords).\n        *   **Offering Precise Semantic Expression:** Textual data can enhance the discriminative ability between similar triples, refining entity topics and making more precise distinctions (e.g., distinguishing between \"politician\" and \"lawyer\" for an entity based on descriptive keywords).\n        *   **Weak-correlation modeling issue:** Previous text-aware models (like DKRL and \"Jointly\") often apply first-order constraints, which are weak in capturing the strong, intricate correlations between texts and triples, limiting the semantic effects.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Triple-only Embedding Models:** The paper acknowledges pioneering works like TransE \\cite{xiao2016} and its variants (TransH, TransR, ManifoldE, PTransE, KG2E, etc.) that focus solely on symbolic triples.\n        *   **Text-Aware Embedding Models:** It positions itself against models like NTN, \"Jointly\" \\cite{xiao2016}, and DKRL \\cite{xiao2016} which attempt to incorporate textual information.\n    *   **Limitations of Previous Solutions:**\n        *   **Weak Correlation Modeling:** Existing text-aware models (e.g., DKRL, \"Jointly\") use first-order constraints, which are insufficient to characterize the strong correlations between textual descriptions and symbolic triples. They often concatenate vectors or generate coherent embeddings without deeply integrating the semantic interaction.\n        *   **Limited Semantic Interaction:** In these models, triple embedding remains the main procedure, and textual descriptions do not sufficiently interact with triples to fully realize their semantic potential.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes the **Semantic Space Projection (SSP)** model, which jointly learns from symbolic triples and textual descriptions. It restricts the embedding procedure of a specific triple within a semantic subspace, specifically a hyperplane.\n    *   **Novelty/Difference:**\n        *   **Strong Correlation Modeling:** Unlike previous methods, SSP models the *strong correlations* between texts and triples by projecting the loss vector `e = h + r - t` onto a semantic hyperplane. This is achieved through a quadratic constraint.\n        *   **Semantic Hyperplane:** A semantic hyperplane, defined by a normal vector `s` composed from head-specific (`sh`) and tail-specific (`st`) semantic vectors, guides the embedding process.\n        *   **Score Function:** The plausibility of a triple is measured by `fr(h;t) =  ||e - s^T e s||^2_2 + ||e||^2_2`, where `` balances the projection component (loss inside the hyperplane) and the overall loss norm. A smaller score indicates higher plausibility.\n        *   **Semantic Vector Generation:** Semantic vectors (`sh`, `st`) are generated using a topic model (NMF) from entity descriptions, capturing topic distributions.\n        *   **Joint Learning (Optional):** SSP offers a \"Joint\" setting where the topic model and embedding model are trained simultaneously, allowing symbolic triples to positively influence textual semantics, in addition to a \"Standard\" setting where semantic vectors are pre-trained and fixed.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** Introduction of the Semantic Space Projection (SSP) model that integrates textual descriptions into knowledge graph embedding by projecting triple loss vectors onto a dynamically generated semantic hyperplane.\n    *   **Strong Correlation Modeling:** A novel approach to model strong correlations between symbolic triples and textual descriptions using a quadratic constraint, ensuring embedding topologies are semantics-specific.\n    *   **Enhanced Semantic Effects:** The model effectively leverages textual descriptions to improve both semantic relevance discovery and precise semantic expression, addressing limitations of prior text-aware models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Evaluated on two tasks: knowledge graph completion (link prediction) and entity classification.\n    *   **Datasets:** Three benchmark datasets: WN18 (Wordnet subset) and FB15K, FB20K (Freebase subsets). Textual information for FB datasets comes from wiki-pages, and for WN18 from Wordnet definitions. FB20K is used for zero-shot learning.\n    *   **Key Performance Metrics:**\n        *   **Knowledge Graph Completion:** Mean Rank (average rank of true triples) and HITS@10 (proportion of true triples ranked within top 10), both in \"Raw\" and \"Filter\" settings.\n        *   **Comparison Results:** \\cite{xiao2016} consistently outperforms all baselines (including TransE, TransH, TransR, PTransE, KG2E, DKRL, \"Jointly\") with remarkable improvements across both tasks and datasets. For instance, in link prediction, SSP significantly improves HITS@10 and Mean Rank compared to state-of-the-art text-aware models like DKRL.\n    *   **Efficiency:** Computation complexity is comparable to TransE (O( * O(TransE))), with a small constant factor , demonstrating practical efficiency.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The \"Standard\" setting fixes pre-trained semantic vectors, as jointly adapting all parameters could \"refill the semantic vectors and flush the semantics out.\" This implies a challenge in fully end-to-end joint optimization of textual semantics and embeddings without careful regularization.\n        *   The choice of topic model (NMF) for semantic vector generation is highlighted as suitable, but the paper notes word embedding could also work, suggesting potential for exploring other semantic representation methods.\n    *   **Scope of Applicability:** Primarily focused on knowledge graph embedding for tasks like knowledge graph completion and entity classification, leveraging entity textual descriptions. The method is generalizable to other KGE tasks where semantic precision is crucial.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{xiao2016} significantly advances the state-of-the-art in knowledge graph embedding by introducing a novel mechanism to model strong correlations between symbolic triples and textual descriptions. It demonstrates that a deeper, geometrically-inspired interaction between these two information sources leads to superior performance.\n    *   **Potential Impact on Future Research:**\n        *   Encourages further exploration of sophisticated interaction mechanisms between heterogeneous data sources (symbolic and textual) in representation learning.\n        *   Provides a strong foundation for developing more semantically precise and discriminative knowledge graph embeddings, which can benefit downstream NLP tasks such as question answering, semantic search, and reasoning.\n        *   Highlights the importance of not just *using* textual data, but *how* it interacts with symbolic data to achieve meaningful semantic effects.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graph Embedding (KGE) demands a sophisticated integration of symbolic triples and rich textual descriptions. While prior text-aware models attempted this, they often suffered from weak correlation modeling, failing to capture the intricate semantic interplay. We introduce **Semantic Space Projection (SSP)**, a novel KGE model that fundamentally redefines this interaction. SSP models *strong correlations* by projecting the triple loss vector onto a dynamically generated *semantic hyperplane*, enforced by a *quadratic constraint*. This innovative approach ensures that embedding topologies are deeply informed by entity and relation semantics derived from text. Our experiments demonstrate SSP's remarkable superiority, outperforming state-of-the-art baselines, including TransE and DKRL, across *knowledge graph completion* (link prediction) and *entity classification* tasks on WN18, FB15K, and FB20K. SSP significantly advances KGE, offering more precise and discriminative embeddings, paving the way for enhanced reasoning and downstream NLP applications.",
      "keywords": [
        "Knowledge Graph Embedding",
        "Textual Descriptions",
        "Semantic Space Projection",
        "Strong Correlation Modeling",
        "Semantic Hyperplane",
        "Joint Learning",
        "Knowledge Graph Completion",
        "Entity Classification",
        "Quadratic Constraint",
        "Symbolic Fact Triples",
        "Enhanced Semantic Effects",
        "Heterogeneous Data Integration",
        "Topic Model (NMF)"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/e379f7c85441df5d8ddc1565cabf4b4290c22f1f.pdf",
      "citation_key": "xiao2016",
      "metadata": {
        "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions",
        "authors": [
          "Han Xiao",
          "Minlie Huang",
          "Lian Meng",
          "Xiaoyan Zhu"
        ],
        "published_date": "2016",
        "abstract": "\n \n Knowledge graph embedding represents entities and relations in knowledge graph as low-dimensional, continuous vectors, and thus enables knowledge graph compatible with machine learning models. Though there have been a variety of models for knowledge graph embedding, most methods merely concentrate on the fact triples, while supplementary textual descriptions of entities and relations have not been fully employed. To this end, this paper proposes the semantic space projection (SSP) model which jointly learns from the symbolic triples and textual descriptions. Our model builds interaction between the two information sources, and employs textual descriptions to discover semantic relevance and offer precise semantic embedding. Extensive experiments show that our method achieves substantial improvements against baselines on the tasks of knowledge graph completion and entity classification.\n \n",
        "file_path": "paper_data/knowledge_graph_embedding/e379f7c85441df5d8ddc1565cabf4b4290c22f1f.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions\" by \\cite{xiao2016} for a literature review:\n\n---\n\n### Analysis of \"SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions\" \\cite{xiao2016}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Most existing knowledge graph embedding (KGE) models primarily focus on symbolic fact triples (h, r, t) and do not fully leverage the rich, supplementary semantic information available in textual descriptions of entities and relations.\n    *   **Importance & Challenge:**\n        *   **Discovering Semantic Relevance:** Textual descriptions can help infer true triples that are difficult to deduce from symbolic triples alone (e.g., identifying family relations through shared keywords).\n        *   **Offering Precise Semantic Expression:** Textual data can enhance the discriminative ability between similar triples, refining entity topics and making more precise distinctions (e.g., distinguishing between \"politician\" and \"lawyer\" for an entity based on descriptive keywords).\n        *   **Weak-correlation modeling issue:** Previous text-aware models (like DKRL and \"Jointly\") often apply first-order constraints, which are weak in capturing the strong, intricate correlations between texts and triples, limiting the semantic effects.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Triple-only Embedding Models:** The paper acknowledges pioneering works like TransE \\cite{xiao2016} and its variants (TransH, TransR, ManifoldE, PTransE, KG2E, etc.) that focus solely on symbolic triples.\n        *   **Text-Aware Embedding Models:** It positions itself against models like NTN, \"Jointly\" \\cite{xiao2016}, and DKRL \\cite{xiao2016} which attempt to incorporate textual information.\n    *   **Limitations of Previous Solutions:**\n        *   **Weak Correlation Modeling:** Existing text-aware models (e.g., DKRL, \"Jointly\") use first-order constraints, which are insufficient to characterize the strong correlations between textual descriptions and symbolic triples. They often concatenate vectors or generate coherent embeddings without deeply integrating the semantic interaction.\n        *   **Limited Semantic Interaction:** In these models, triple embedding remains the main procedure, and textual descriptions do not sufficiently interact with triples to fully realize their semantic potential.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes the **Semantic Space Projection (SSP)** model, which jointly learns from symbolic triples and textual descriptions. It restricts the embedding procedure of a specific triple within a semantic subspace, specifically a hyperplane.\n    *   **Novelty/Difference:**\n        *   **Strong Correlation Modeling:** Unlike previous methods, SSP models the *strong correlations* between texts and triples by projecting the loss vector `e = h + r - t` onto a semantic hyperplane. This is achieved through a quadratic constraint.\n        *   **Semantic Hyperplane:** A semantic hyperplane, defined by a normal vector `s` composed from head-specific (`sh`) and tail-specific (`st`) semantic vectors, guides the embedding process.\n        *   **Score Function:** The plausibility of a triple is measured by `fr(h;t) =  ||e - s^T e s||^2_2 + ||e||^2_2`, where `` balances the projection component (loss inside the hyperplane) and the overall loss norm. A smaller score indicates higher plausibility.\n        *   **Semantic Vector Generation:** Semantic vectors (`sh`, `st`) are generated using a topic model (NMF) from entity descriptions, capturing topic distributions.\n        *   **Joint Learning (Optional):** SSP offers a \"Joint\" setting where the topic model and embedding model are trained simultaneously, allowing symbolic triples to positively influence textual semantics, in addition to a \"Standard\" setting where semantic vectors are pre-trained and fixed.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** Introduction of the Semantic Space Projection (SSP) model that integrates textual descriptions into knowledge graph embedding by projecting triple loss vectors onto a dynamically generated semantic hyperplane.\n    *   **Strong Correlation Modeling:** A novel approach to model strong correlations between symbolic triples and textual descriptions using a quadratic constraint, ensuring embedding topologies are semantics-specific.\n    *   **Enhanced Semantic Effects:** The model effectively leverages textual descriptions to improve both semantic relevance discovery and precise semantic expression, addressing limitations of prior text-aware models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Evaluated on two tasks: knowledge graph completion (link prediction) and entity classification.\n    *   **Datasets:** Three benchmark datasets: WN18 (Wordnet subset) and FB15K, FB20K (Freebase subsets). Textual information for FB datasets comes from wiki-pages, and for WN18 from Wordnet definitions. FB20K is used for zero-shot learning.\n    *   **Key Performance Metrics:**\n        *   **Knowledge Graph Completion:** Mean Rank (average rank of true triples) and HITS@10 (proportion of true triples ranked within top 10), both in \"Raw\" and \"Filter\" settings.\n        *   **Comparison Results:** \\cite{xiao2016} consistently outperforms all baselines (including TransE, TransH, TransR, PTransE, KG2E, DKRL, \"Jointly\") with remarkable improvements across both tasks and datasets. For instance, in link prediction, SSP significantly improves HITS@10 and Mean Rank compared to state-of-the-art text-aware models like DKRL.\n    *   **Efficiency:** Computation complexity is comparable to TransE (O( * O(TransE))), with a small constant factor , demonstrating practical efficiency.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The \"Standard\" setting fixes pre-trained semantic vectors, as jointly adapting all parameters could \"refill the semantic vectors and flush the semantics out.\" This implies a challenge in fully end-to-end joint optimization of textual semantics and embeddings without careful regularization.\n        *   The choice of topic model (NMF) for semantic vector generation is highlighted as suitable, but the paper notes word embedding could also work, suggesting potential for exploring other semantic representation methods.\n    *   **Scope of Applicability:** Primarily focused on knowledge graph embedding for tasks like knowledge graph completion and entity classification, leveraging entity textual descriptions. The method is generalizable to other KGE tasks where semantic precision is crucial.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{xiao2016} significantly advances the state-of-the-art in knowledge graph embedding by introducing a novel mechanism to model strong correlations between symbolic triples and textual descriptions. It demonstrates that a deeper, geometrically-inspired interaction between these two information sources leads to superior performance.\n    *   **Potential Impact on Future Research:**\n        *   Encourages further exploration of sophisticated interaction mechanisms between heterogeneous data sources (symbolic and textual) in representation learning.\n        *   Provides a strong foundation for developing more semantically precise and discriminative knowledge graph embeddings, which can benefit downstream NLP tasks such as question answering, semantic search, and reasoning.\n        *   Highlights the importance of not just *using* textual data, but *how* it interacts with symbolic data to achieve meaningful semantic effects.",
        "keywords": [
          "Knowledge Graph Embedding",
          "Textual Descriptions",
          "Semantic Space Projection",
          "Strong Correlation Modeling",
          "Semantic Hyperplane",
          "Joint Learning",
          "Knowledge Graph Completion",
          "Entity Classification",
          "Quadratic Constraint",
          "Symbolic Fact Triples",
          "Enhanced Semantic Effects",
          "Heterogeneous Data Integration",
          "Topic Model (NMF)"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f.pdf"
    },
    {
      "success": true,
      "doc_id": "2f08a817f71a2e3a324623332e24f416",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) methods struggle with the data imbalance issue prevalent in Knowledge Graphs (KGs), where entities and relations follow a long-tail distribution. This means a small fraction of entities/relations are frequent, while the vast majority are infrequent and have few training samples \\cite{zhang2023}.\n    *   **Importance and Challenge**: Current KGE methods assign equal weights to all entities and relations during training. This leads to long-tail entities and relations being insufficiently trained, resulting in unreliable and poor-quality representations for them. Learning robust embeddings for these infrequent elements is crucial for the overall utility and accuracy of KGEs \\cite{zhang2023}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work directly addresses a fundamental limitation of \"existing KGE methods\" which, by assigning equal weights, fail to adequately train long-tail entities and relations \\cite{zhang2023}.\n    *   **Limitations of Previous Solutions**: The primary limitation is the uniform weighting scheme, which overlooks the inherent data imbalance in KGs. This leads to undertrained representations for the majority of entities and relations that fall into the long-tail, making their embeddings unreliable \\cite{zhang2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **WeightE**, a novel KGE approach that differentially attends to various entities and relations during training \\cite{zhang2023}.\n    *   **Novelty**: WeightE innovatively assigns lower weights to frequent entities/relations and higher weights to infrequent (long-tail) ones. This is achieved by tailoring a **bilevel optimization** framework for the KGE task \\cite{zhang2023}.\n        *   The **inner level** of this optimization focuses on learning reliable entity and relation embeddings.\n        *   The **outer level** is responsible for adaptively assigning appropriate weights to each entity and relation \\cite{zhang2023}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of WeightE, a KGE algorithm specifically designed to mitigate the data imbalance problem by dynamically weighting entities and relations \\cite{zhang2023}.\n    *   **Methodological Innovation**: The pioneering application of a bilevel optimization framework to the KGE task, allowing for simultaneous optimization of embeddings and their corresponding training weights \\cite{zhang2023}.\n    *   **Generality**: The proposed weighting technique is highlighted as general and flexible, capable of being integrated with and enhancing a number of existing KGE models \\cite{zhang2023}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper states that extensive validation was performed \\cite{zhang2023}.\n    *   **Key Performance Metrics and Comparison Results**: WeightE demonstrated \"superiority\" against various state-of-the-art baselines, indicating improved performance on standard KGE evaluation metrics (though specific metrics are not detailed in the provided abstract) \\cite{zhang2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided text does not explicitly state technical limitations or assumptions of WeightE itself, beyond addressing the data imbalance problem.\n    *   **Scope of Applicability**: The weighting technique developed in WeightE is described as general and flexible, implying broad applicability across different existing KGE models \\cite{zhang2023}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: WeightE significantly advances the technical state-of-the-art in KGE by effectively addressing the long-standing data imbalance issue. By learning more reliable representations for long-tail entities and relations, it improves the overall quality and robustness of KGEs \\cite{zhang2023}.\n    *   **Potential Impact on Future Research**: The introduction of a bilevel optimization framework for adaptive weighting provides a novel paradigm for KGE. Its general applicability suggests it could serve as a foundational component or inspiration for future KGE models, leading to more robust and accurate embeddings across diverse knowledge graph applications \\cite{zhang2023}.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are indispensable, yet their pervasive long-tail distribution, where a few entities and relations are frequent while the vast majority are infrequent, severely cripples the reliability of traditional Knowledge Graph Embedding (KGE) methods. Existing approaches, by uniformly weighting all elements, leave these critical long-tail entities and relations undertrained, leading to unreliable representations and hindering overall KGE utility.\n\nWe introduce **WeightE**, a novel KGE framework designed to revolutionize how we learn from imbalanced KGs. WeightE pioneers the application of a **bilevel optimization** framework to KGE, enabling a sophisticated, adaptive weighting mechanism. Its outer level dynamically assigns higher training weights to infrequent, long-tail entities and relations, while the inner level focuses on learning robust embeddings. This differential attention ensures that even the scarcest elements receive sufficient training, yielding significantly more reliable and higher-quality representations.\n\nExtensive experiments demonstrate WeightE's superior performance against state-of-the-art baselines, validating its ability to overcome data imbalance. Our general and flexible weighting technique not only advances the state-of-the-art in KGE but also provides a powerful new paradigm for learning robust embeddings, paving the way for more accurate and trustworthy AI applications built upon KGs.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "data imbalance",
        "long-tail distribution",
        "WeightE",
        "bilevel optimization framework",
        "adaptive weighting",
        "entity and relation embeddings",
        "robust representations",
        "differential attention",
        "Knowledge Graphs (KGs)",
        "state-of-the-art advancement",
        "general weighting technique"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/c180564160d0788a82df203f9e5f61380d9846aa.pdf",
      "citation_key": "zhang2023",
      "metadata": {
        "title": "Weighted Knowledge Graph Embedding",
        "authors": [
          "Zhao Zhang",
          "Zhanpeng Guan",
          "Fuwei Zhang",
          "Fuzhen Zhuang",
          "Zhulin An",
          "Fei Wang",
          "Yongjun Xu"
        ],
        "published_date": "2023",
        "abstract": "Knowledge graph embedding (KGE) aims to project both entities and relations in a knowledge graph (KG) into low-dimensional vectors. Indeed, existing KGs suffer from the data imbalance issue, i.e., entities and relations conform to a long-tail distribution, only a small portion of entities and relations occur frequently, while the vast majority of entities and relations only have a few training samples. Existing KGE methods assign equal weights to each entity and relation during the training process. Under this setting, long-tail entities and relations are not fully trained during training, leading to unreliable representations. In this paper, we propose WeightE, which attends differentially to different entities and relations. Specifically, WeightE is able to endow lower weights to frequent entities and relations, and higher weights to infrequent ones. In such manner, WeightE is capable of increasing the weights of long-tail entities and relations, and learning better representations for them. In particular, WeightE tailors bilevel optimization for the KGE task, where the inner level aims to learn reliable entity and relation embeddings, and the outer level attempts to assign appropriate weights for each entity and relation. Moreover, it is worth noting that our technique of applying weights to different entities and relations is general and flexible, which can be applied to a number of existing KGE models. Finally, we extensively validate the superiority of WeightE against various state-of-the-art baselines.",
        "file_path": "paper_data/knowledge_graph_embedding/c180564160d0788a82df203f9e5f61380d9846aa.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) methods struggle with the data imbalance issue prevalent in Knowledge Graphs (KGs), where entities and relations follow a long-tail distribution. This means a small fraction of entities/relations are frequent, while the vast majority are infrequent and have few training samples \\cite{zhang2023}.\n    *   **Importance and Challenge**: Current KGE methods assign equal weights to all entities and relations during training. This leads to long-tail entities and relations being insufficiently trained, resulting in unreliable and poor-quality representations for them. Learning robust embeddings for these infrequent elements is crucial for the overall utility and accuracy of KGEs \\cite{zhang2023}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work directly addresses a fundamental limitation of \"existing KGE methods\" which, by assigning equal weights, fail to adequately train long-tail entities and relations \\cite{zhang2023}.\n    *   **Limitations of Previous Solutions**: The primary limitation is the uniform weighting scheme, which overlooks the inherent data imbalance in KGs. This leads to undertrained representations for the majority of entities and relations that fall into the long-tail, making their embeddings unreliable \\cite{zhang2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **WeightE**, a novel KGE approach that differentially attends to various entities and relations during training \\cite{zhang2023}.\n    *   **Novelty**: WeightE innovatively assigns lower weights to frequent entities/relations and higher weights to infrequent (long-tail) ones. This is achieved by tailoring a **bilevel optimization** framework for the KGE task \\cite{zhang2023}.\n        *   The **inner level** of this optimization focuses on learning reliable entity and relation embeddings.\n        *   The **outer level** is responsible for adaptively assigning appropriate weights to each entity and relation \\cite{zhang2023}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of WeightE, a KGE algorithm specifically designed to mitigate the data imbalance problem by dynamically weighting entities and relations \\cite{zhang2023}.\n    *   **Methodological Innovation**: The pioneering application of a bilevel optimization framework to the KGE task, allowing for simultaneous optimization of embeddings and their corresponding training weights \\cite{zhang2023}.\n    *   **Generality**: The proposed weighting technique is highlighted as general and flexible, capable of being integrated with and enhancing a number of existing KGE models \\cite{zhang2023}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper states that extensive validation was performed \\cite{zhang2023}.\n    *   **Key Performance Metrics and Comparison Results**: WeightE demonstrated \"superiority\" against various state-of-the-art baselines, indicating improved performance on standard KGE evaluation metrics (though specific metrics are not detailed in the provided abstract) \\cite{zhang2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The provided text does not explicitly state technical limitations or assumptions of WeightE itself, beyond addressing the data imbalance problem.\n    *   **Scope of Applicability**: The weighting technique developed in WeightE is described as general and flexible, implying broad applicability across different existing KGE models \\cite{zhang2023}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: WeightE significantly advances the technical state-of-the-art in KGE by effectively addressing the long-standing data imbalance issue. By learning more reliable representations for long-tail entities and relations, it improves the overall quality and robustness of KGEs \\cite{zhang2023}.\n    *   **Potential Impact on Future Research**: The introduction of a bilevel optimization framework for adaptive weighting provides a novel paradigm for KGE. Its general applicability suggests it could serve as a foundational component or inspiration for future KGE models, leading to more robust and accurate embeddings across diverse knowledge graph applications \\cite{zhang2023}.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "data imbalance",
          "long-tail distribution",
          "WeightE",
          "bilevel optimization framework",
          "adaptive weighting",
          "entity and relation embeddings",
          "robust representations",
          "differential attention",
          "Knowledge Graphs (KGs)",
          "state-of-the-art advancement",
          "general weighting technique"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "c180564160d0788a82df203f9e5f61380d9846aa.pdf"
    },
    {
      "success": true,
      "doc_id": "3b5a8b8bfac84ef6a6f3cf03db212f8c",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Embedding Knowledge Graphs (KGs) into low-dimensional vector spaces.\n    *   **Motivation**: Existing methods primarily rely solely on observed facts, neglecting the intrinsic geometric structure of the embedding space. This limits their ability to capture richer relationships and semantic regularities.\n\n*   **Related Work & Positioning**\n    *   **Relation**: This work extends existing KG embedding approaches.\n    *   **Limitations of Previous Solutions**: Prior methods only require learned embeddings to be compatible within individual facts, without leveraging additional semantic information or enforcing structural properties like semantic smoothness across the embedding space.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes Semantically Smooth Embedding (SSE) \\cite{guo2015}.\n    *   **Novelty**: SSE's key innovation is to leverage additional semantic information (e.g., entity categories) to enforce a \"semantically smooth\" embedding space. This means entities belonging to the same semantic category are constrained to lie close to each other in the learned vector space.\n    *   **Mechanism**: This smoothness assumption is modeled using two manifold learning algorithms: Laplacian Eigenmaps and Locally Linear Embedding (LLE). These algorithms are formulated as geometrically based regularization terms that constrain the embedding task.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the Semantically Smooth Embedding (SSE) framework for KG embedding.\n    *   **Technique**: Integration of manifold learning algorithms (Laplacian Eigenmaps, LLE) as geometrically based regularization terms to enforce semantic smoothness in the embedding space.\n    *   **Conceptual Innovation**: The idea of explicitly enforcing semantic smoothness based on external semantic categories to improve KG embeddings.\n\n*   **Experimental Validation**\n    *   **Experiments**: SSE was empirically evaluated on two benchmark tasks: link prediction and triple classification.\n    *   **Results**: The proposed SSE method achieved significant and consistent improvements over state-of-the-art methods in both evaluation tasks.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: SSE is presented as a general framework. The smoothness assumption can be applied to a wide variety of existing embedding models.\n    *   **Flexibility**: The framework is flexible enough to construct the smoothness assumption using other types of information beyond just entities semantic categories.\n\n*   **Technical Significance**\n    *   **Advancement**: SSE advances the technical state-of-the-art in KG embedding by demonstrating that incorporating external semantic information and enforcing geometric smoothness significantly improves embedding quality.\n    *   **Potential Impact**: Its general framework nature suggests it can be widely adopted to enhance various existing and future KG embedding models, potentially leading to more semantically rich and geometrically structured representations.",
      "intriguing_abstract": "Current Knowledge Graph (KG) embedding methods often fall short by solely relying on observed facts and neglecting the intrinsic geometric structure of the embedding space. This oversight limits their capacity to capture richer relationships and semantic regularities. We introduce Semantically Smooth Embedding (SSE), a novel framework that fundamentally redefines Knowledge Graph (KG) embedding by explicitly enforcing semantic smoothness. SSE leverages external semantic information, such as entity categories, to constrain entities from the same category to lie close in the learned vector space. This is achieved by integrating powerful manifold learning algorithmsLaplacian Eigenmaps and Locally Linear Embedding (LLE)as geometrically based regularization terms. Our approach transforms the embedding task, yielding a more structured and semantically coherent embedding space. Empirical evaluations on benchmark link prediction and triple classification tasks demonstrate that SSE consistently and significantly outperforms state-of-the-art methods. As a general and flexible framework, SSE offers a powerful paradigm to enhance a wide array of existing and future KG embedding models, paving the way for truly semantically rich and geometrically structured knowledge representations.",
      "keywords": [
        "Knowledge Graphs (KGs)",
        "KG embedding",
        "Semantically Smooth Embedding (SSE)",
        "semantic smoothness",
        "manifold learning",
        "Laplacian Eigenmaps",
        "Locally Linear Embedding (LLE)",
        "geometrically based regularization",
        "external semantic information",
        "link prediction",
        "triple classification",
        "state-of-the-art improvements",
        "semantically rich representations"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/69418ff5d4eac106c72130e152b807004e2b979c.pdf",
      "citation_key": "guo2015",
      "metadata": {
        "title": "Semantically Smooth Knowledge Graph Embedding",
        "authors": [
          "Shu Guo",
          "Quan Wang",
          "Bin Wang",
          "Lihong Wang",
          "Li Guo"
        ],
        "published_date": "2015",
        "abstract": "This paper considers the problem of embedding Knowledge Graphs (KGs) consisting of entities and relations into lowdimensional vector spaces. Most of the existing methods perform this task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. In this paper, aiming at further discovering the intrinsic geometric structure of the embedding space, we propose Semantically Smooth Embedding (SSE). The key idea of SSE is to take full advantage of additional semantic information and enforce the embedding space to be semantically smooth, i.e., entities belonging to the same semantic category will lie close to each other in the embedding space. Two manifold learning algorithms Laplacian Eigenmaps and Locally Linear Embedding are used to model the smoothness assumption. Both are formulated as geometrically based regularization terms to constrain the embedding task. We empirically evaluate SSE in two benchmark tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods. Furthermore, SSE is a general framework. The smoothness assumption can be imposed to a wide variety of embedding models, and it can also be constructed using other information besides entities semantic categories.",
        "file_path": "paper_data/knowledge_graph_embedding/69418ff5d4eac106c72130e152b807004e2b979c.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Embedding Knowledge Graphs (KGs) into low-dimensional vector spaces.\n    *   **Motivation**: Existing methods primarily rely solely on observed facts, neglecting the intrinsic geometric structure of the embedding space. This limits their ability to capture richer relationships and semantic regularities.\n\n*   **Related Work & Positioning**\n    *   **Relation**: This work extends existing KG embedding approaches.\n    *   **Limitations of Previous Solutions**: Prior methods only require learned embeddings to be compatible within individual facts, without leveraging additional semantic information or enforcing structural properties like semantic smoothness across the embedding space.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes Semantically Smooth Embedding (SSE) \\cite{guo2015}.\n    *   **Novelty**: SSE's key innovation is to leverage additional semantic information (e.g., entity categories) to enforce a \"semantically smooth\" embedding space. This means entities belonging to the same semantic category are constrained to lie close to each other in the learned vector space.\n    *   **Mechanism**: This smoothness assumption is modeled using two manifold learning algorithms: Laplacian Eigenmaps and Locally Linear Embedding (LLE). These algorithms are formulated as geometrically based regularization terms that constrain the embedding task.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the Semantically Smooth Embedding (SSE) framework for KG embedding.\n    *   **Technique**: Integration of manifold learning algorithms (Laplacian Eigenmaps, LLE) as geometrically based regularization terms to enforce semantic smoothness in the embedding space.\n    *   **Conceptual Innovation**: The idea of explicitly enforcing semantic smoothness based on external semantic categories to improve KG embeddings.\n\n*   **Experimental Validation**\n    *   **Experiments**: SSE was empirically evaluated on two benchmark tasks: link prediction and triple classification.\n    *   **Results**: The proposed SSE method achieved significant and consistent improvements over state-of-the-art methods in both evaluation tasks.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: SSE is presented as a general framework. The smoothness assumption can be applied to a wide variety of existing embedding models.\n    *   **Flexibility**: The framework is flexible enough to construct the smoothness assumption using other types of information beyond just entities semantic categories.\n\n*   **Technical Significance**\n    *   **Advancement**: SSE advances the technical state-of-the-art in KG embedding by demonstrating that incorporating external semantic information and enforcing geometric smoothness significantly improves embedding quality.\n    *   **Potential Impact**: Its general framework nature suggests it can be widely adopted to enhance various existing and future KG embedding models, potentially leading to more semantically rich and geometrically structured representations.",
        "keywords": [
          "Knowledge Graphs (KGs)",
          "KG embedding",
          "Semantically Smooth Embedding (SSE)",
          "semantic smoothness",
          "manifold learning",
          "Laplacian Eigenmaps",
          "Locally Linear Embedding (LLE)",
          "geometrically based regularization",
          "external semantic information",
          "link prediction",
          "triple classification",
          "state-of-the-art improvements",
          "semantically rich representations"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "69418ff5d4eac106c72130e152b807004e2b979c.pdf"
    },
    {
      "success": true,
      "doc_id": "eb4081f68fde450030be04c9e6ae2849",
      "summary": "Here's a focused summary of the paper \"TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation\" by Xu et al. \\cite{xu2020} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional Knowledge Graph Embedding (KGE) models disregard time information, making them ineffective for Temporal Knowledge Graphs (TKGs) which contain time-aware facts (quadruples `(s, r, o, t)`). Existing TKG embedding (TKGE) models, often extensions of TransE or DistMult, inherit limitations of their base models, struggling to capture various complex relation patterns (e.g., temporary, asymmetric, reflexive relations) over time. Furthermore, many existing TKGE models do not robustly handle diverse time annotations, such as time intervals.\n    *   **Importance and Challenge**: The increasing availability of TKGs necessitates models that can effectively characterize and reason over their complex temporal dynamics and multi-relational nature. Accurately modeling temporal evolution and diverse relation patterns is crucial for tasks like link prediction, where time-awareness can significantly refine predictions (e.g., `(Barack Obama, visits, ?, 2014-07-08)`). The challenge lies in developing a unified framework that is expressive enough to capture temporal dynamics, various relation patterns, and different forms of time annotations without excessive complexity.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon KGE models, particularly distance-based models like RotatE, which uses rotations in complex space. It positions itself as an advancement over existing TKGE models that are primarily temporal extensions of TransE (e.g., TTransE, HyTE, ATiSE) and DistMult (e.g., Know-Evolve, TDistMult).\n    *   **Limitations of Previous Solutions**:\n        *   **Static KGEs**: Cannot model temporary relations (e.g., `visits` being valid at `t1` but not `t2`).\n        *   **TransE-based TKGEs**: Struggle with multiple reflexive relations (e.g., `equalTo`, `subsetOf`) as they tend to enforce relation embeddings to zero for such cases.\n        *   **DistMult-based TKGEs**: Cannot capture asymmetric relations (e.g., `parentOf`) because their scoring functions are symmetric (`score(s,r,o,t) = score(o,r,s,t)`).\n        *   **DE-SimplE**: While capable of modeling various patterns, it focuses only on event-based TKGs and cannot model facts involving time intervals.\n        *   **General TKGEs**: Many previous works use fixed or specific time granularities, and the effect of time granularity on performance has not been thoroughly investigated.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: TeRo defines the temporal evolution of an entity embedding as an element-wise rotation from its initial, time-independent state to its current time-specific state in a complex vector space.\n        *   For a quadruple `(s, r, o, t)`, time-specific entity embeddings `s_t` and `o_t` are derived from time-independent `s` and `o` by `s_t = s * phi_t` and `o_t = o * phi_t`, where `phi_t` is a complex vector representing the rotation for time `t`. Each element of `phi_t` has a modulus of 1, acting as a rotation in the complex plane.\n        *   The plausibility score for a fact `(s, r, o, t)` is `f_TeRo(s,r,o,t) = ||s_t + r - o_t||`, where `r` is the relation embedding.\n    *   **Novelty/Differentiation**:\n        *   **Temporal Rotation**: The core innovation is modeling temporal evolution via rotation in complex space, inspired by Euler's identity, which allows for dynamic changes in entity embeddings over time while preserving their underlying structure.\n        *   **Handling Time Intervals**: For facts with time intervals `[t_b, t_e]`, TeRo introduces a pair of dual complex relation embeddings (`r_b` for beginning, `r_e` for end). The score is the mean of scores for `(s, r_b, o, t_b)` and `(s, r_e, o, t_e)`. This allows TeRo to adapt to various time annotations: time points, beginning/end times, and full intervals.\n        *   **Expressiveness for Relation Patterns**: By leveraging complex embeddings and temporal rotations, TeRo inherently supports temporary, asymmetric, and reflexive relations, overcoming the limitations of TransE and DistMult extensions.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of TeRo, a novel TKG embedding model that uses temporal rotation in complex vector space to capture time-aware entity evolution.\n    *   **Method for Time Intervals**: A unique approach to model facts with time intervals by employing dual relation embeddings (`r_b`, `r_e`), enabling robust handling of diverse temporal annotations.\n    *   **Enhanced Expressiveness**: Demonstrated capability to learn and infer temporary, asymmetric, and reflexive relation patterns, which are challenging for many existing TKGE models.\n    *   **Empirical Analysis of Time Granularity**: First investigation into the effect of time granularity (length of time steps) on link prediction performance over TKGs, providing insights into dataset-specific temporal modeling.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Link prediction tasks were performed on four diverse TKGs. An additional analysis was conducted on the effect of time granularity.\n    *   **Datasets**:\n        *   ICEWS14, ICEWS05-15: Event-based datasets with time points.\n        *   YAGO11k, Wikidata12k: Datasets with mixed time annotations, including time points, beginning/end times, and time intervals.\n    *   **Key Performance Metrics**: Mean Reciprocal Rank (MRR) and Hits@k (Hits@1, Hits@3, Hits@10) under a time-wise filtered setting.\n    *   **Comparison Results**: TeRo significantly outperformed several state-of-the-art KGE models (TransE, DistMult, ComplEx-N3, RotatE, QuatE) and existing TKGE models (TTransE, TA-TransE, TA-DistMult, DE-SimplE, ATiSE) across all four datasets for link prediction.\n    *   **Time Granularity Analysis**: Experiments showed that tuning time granularity (e.g., `u` days for ICEWS, `thre` minimum triples per interval for YAGO/Wikidata) impacts performance, suggesting optimal granularities exist for different datasets.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The space complexity of TeRo is `O(ned + nrd + n_phi * d)`, where `n_phi` is the number of time steps. While this can be managed by tuning time granularity (`n_phi < ne`), a very fine granularity could increase memory requirements. The model assumes that temporal evolution can be effectively modeled as a rotation in complex space.\n    *   **Scope of Applicability**: TeRo is applicable to TKGs with various forms of time annotations, including discrete time points and continuous time intervals. It is particularly well-suited for datasets where relations exhibit temporary, asymmetric, or reflexive properties.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: TeRo advances the technical state-of-the-art in TKGEs by introducing a more expressive and robust model that effectively captures temporal dynamics and diverse relation patterns, overcoming key limitations of previous TransE/DistMult-based approaches.\n    *   **Potential Impact on Future Research**:\n        *   Provides a strong baseline and a novel perspective (temporal rotation) for future TKGE research.\n        *   The dual relation embedding mechanism for time intervals offers a valuable technique for handling complex temporal annotations.\n        *   The investigation into time granularity highlights a critical, yet underexplored, aspect of TKG modeling, opening avenues for research into adaptive or learned time granularity.\n        *   Its ability to model various relation patterns makes it a versatile tool for reasoning over complex real-world TKGs.",
      "intriguing_abstract": "The static nature of traditional Knowledge Graph Embeddings (KGEs) fundamentally limits their utility for dynamic Temporal Knowledge Graphs (TKGs), which demand models capable of capturing intricate temporal evolution. We present TeRo, a novel Temporal Knowledge Graph Embedding (TKGE) model that revolutionizes how time-aware facts are understood. TeRo introduces the innovative concept of **temporal rotation**, modeling entity evolution as element-wise rotations in a complex vector space. This elegant mechanism allows TeRo to inherently capture diverse and challenging relation patternsincluding temporary, asymmetric, and reflexive relationsa significant advancement over prior TransE and DistMult-based TKGEs.\n\nFurthermore, TeRo uniquely addresses the critical challenge of diverse time annotations by employing dual relation embeddings for facts involving **time intervals**, providing robust adaptability. Extensive **link prediction** experiments on four benchmark TKGs demonstrate TeRo's superior performance against state-of-the-art models. We also offer the first comprehensive analysis of **time granularity**'s impact on TKGE performance. TeRo provides an expressive, robust, and versatile framework, pushing the boundaries of temporal reasoning and opening new avenues for understanding dynamic knowledge.",
      "keywords": [
        "Temporal Knowledge Graphs (TKGs)",
        "Temporal Knowledge Graph Embedding (TKGE)",
        "TeRo model",
        "Temporal Rotation",
        "Complex Vector Space",
        "Time-aware Entity Evolution",
        "Handling Time Intervals",
        "Dual Relation Embeddings",
        "Complex Relation Patterns",
        "Link Prediction",
        "Time Granularity Analysis",
        "State-of-the-Art Advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/552bfaca30af29647c083993fbe406867fc70d4c.pdf",
      "citation_key": "xu2020",
      "metadata": {
        "title": "TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation",
        "authors": [
          "Chengjin Xu",
          "M. Nayyeri",
          "Fouad Alkhoury",
          "H. S. Yazdi",
          "Jens Lehmann"
        ],
        "published_date": "2020",
        "abstract": "In the last few years, there has been a surge of interest in learning representations of entities and relations in knowledge graph (KG). However, the recent availability of temporal knowledge graphs (TKGs) that contain time information for each fact created the need for reasoning over time in such TKGs. In this regard, we present a new approach of TKG embedding, TeRo, which defines the temporal evolution of entity embedding as a rotation from the initial time to the current time in the complex vector space. Specially, for facts involving time intervals, each relation is represented as a pair of dual complex embeddings to handle the beginning and the end of the relation, respectively. We show our proposed model overcomes the limitations of the existing KG embedding models and TKG embedding models and has the ability of learning and inferring various relation patterns over time. Experimental results on three different TKGs show that TeRo significantly outperforms existing state-of-the-art models for link prediction. In addition, we analyze the effect of time granularity on link prediction over TKGs, which as far as we know has not been investigated in previous literature.",
        "file_path": "paper_data/knowledge_graph_embedding/552bfaca30af29647c083993fbe406867fc70d4c.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation\" by Xu et al. \\cite{xu2020} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional Knowledge Graph Embedding (KGE) models disregard time information, making them ineffective for Temporal Knowledge Graphs (TKGs) which contain time-aware facts (quadruples `(s, r, o, t)`). Existing TKG embedding (TKGE) models, often extensions of TransE or DistMult, inherit limitations of their base models, struggling to capture various complex relation patterns (e.g., temporary, asymmetric, reflexive relations) over time. Furthermore, many existing TKGE models do not robustly handle diverse time annotations, such as time intervals.\n    *   **Importance and Challenge**: The increasing availability of TKGs necessitates models that can effectively characterize and reason over their complex temporal dynamics and multi-relational nature. Accurately modeling temporal evolution and diverse relation patterns is crucial for tasks like link prediction, where time-awareness can significantly refine predictions (e.g., `(Barack Obama, visits, ?, 2014-07-08)`). The challenge lies in developing a unified framework that is expressive enough to capture temporal dynamics, various relation patterns, and different forms of time annotations without excessive complexity.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon KGE models, particularly distance-based models like RotatE, which uses rotations in complex space. It positions itself as an advancement over existing TKGE models that are primarily temporal extensions of TransE (e.g., TTransE, HyTE, ATiSE) and DistMult (e.g., Know-Evolve, TDistMult).\n    *   **Limitations of Previous Solutions**:\n        *   **Static KGEs**: Cannot model temporary relations (e.g., `visits` being valid at `t1` but not `t2`).\n        *   **TransE-based TKGEs**: Struggle with multiple reflexive relations (e.g., `equalTo`, `subsetOf`) as they tend to enforce relation embeddings to zero for such cases.\n        *   **DistMult-based TKGEs**: Cannot capture asymmetric relations (e.g., `parentOf`) because their scoring functions are symmetric (`score(s,r,o,t) = score(o,r,s,t)`).\n        *   **DE-SimplE**: While capable of modeling various patterns, it focuses only on event-based TKGs and cannot model facts involving time intervals.\n        *   **General TKGEs**: Many previous works use fixed or specific time granularities, and the effect of time granularity on performance has not been thoroughly investigated.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: TeRo defines the temporal evolution of an entity embedding as an element-wise rotation from its initial, time-independent state to its current time-specific state in a complex vector space.\n        *   For a quadruple `(s, r, o, t)`, time-specific entity embeddings `s_t` and `o_t` are derived from time-independent `s` and `o` by `s_t = s * phi_t` and `o_t = o * phi_t`, where `phi_t` is a complex vector representing the rotation for time `t`. Each element of `phi_t` has a modulus of 1, acting as a rotation in the complex plane.\n        *   The plausibility score for a fact `(s, r, o, t)` is `f_TeRo(s,r,o,t) = ||s_t + r - o_t||`, where `r` is the relation embedding.\n    *   **Novelty/Differentiation**:\n        *   **Temporal Rotation**: The core innovation is modeling temporal evolution via rotation in complex space, inspired by Euler's identity, which allows for dynamic changes in entity embeddings over time while preserving their underlying structure.\n        *   **Handling Time Intervals**: For facts with time intervals `[t_b, t_e]`, TeRo introduces a pair of dual complex relation embeddings (`r_b` for beginning, `r_e` for end). The score is the mean of scores for `(s, r_b, o, t_b)` and `(s, r_e, o, t_e)`. This allows TeRo to adapt to various time annotations: time points, beginning/end times, and full intervals.\n        *   **Expressiveness for Relation Patterns**: By leveraging complex embeddings and temporal rotations, TeRo inherently supports temporary, asymmetric, and reflexive relations, overcoming the limitations of TransE and DistMult extensions.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of TeRo, a novel TKG embedding model that uses temporal rotation in complex vector space to capture time-aware entity evolution.\n    *   **Method for Time Intervals**: A unique approach to model facts with time intervals by employing dual relation embeddings (`r_b`, `r_e`), enabling robust handling of diverse temporal annotations.\n    *   **Enhanced Expressiveness**: Demonstrated capability to learn and infer temporary, asymmetric, and reflexive relation patterns, which are challenging for many existing TKGE models.\n    *   **Empirical Analysis of Time Granularity**: First investigation into the effect of time granularity (length of time steps) on link prediction performance over TKGs, providing insights into dataset-specific temporal modeling.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Link prediction tasks were performed on four diverse TKGs. An additional analysis was conducted on the effect of time granularity.\n    *   **Datasets**:\n        *   ICEWS14, ICEWS05-15: Event-based datasets with time points.\n        *   YAGO11k, Wikidata12k: Datasets with mixed time annotations, including time points, beginning/end times, and time intervals.\n    *   **Key Performance Metrics**: Mean Reciprocal Rank (MRR) and Hits@k (Hits@1, Hits@3, Hits@10) under a time-wise filtered setting.\n    *   **Comparison Results**: TeRo significantly outperformed several state-of-the-art KGE models (TransE, DistMult, ComplEx-N3, RotatE, QuatE) and existing TKGE models (TTransE, TA-TransE, TA-DistMult, DE-SimplE, ATiSE) across all four datasets for link prediction.\n    *   **Time Granularity Analysis**: Experiments showed that tuning time granularity (e.g., `u` days for ICEWS, `thre` minimum triples per interval for YAGO/Wikidata) impacts performance, suggesting optimal granularities exist for different datasets.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The space complexity of TeRo is `O(ned + nrd + n_phi * d)`, where `n_phi` is the number of time steps. While this can be managed by tuning time granularity (`n_phi < ne`), a very fine granularity could increase memory requirements. The model assumes that temporal evolution can be effectively modeled as a rotation in complex space.\n    *   **Scope of Applicability**: TeRo is applicable to TKGs with various forms of time annotations, including discrete time points and continuous time intervals. It is particularly well-suited for datasets where relations exhibit temporary, asymmetric, or reflexive properties.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: TeRo advances the technical state-of-the-art in TKGEs by introducing a more expressive and robust model that effectively captures temporal dynamics and diverse relation patterns, overcoming key limitations of previous TransE/DistMult-based approaches.\n    *   **Potential Impact on Future Research**:\n        *   Provides a strong baseline and a novel perspective (temporal rotation) for future TKGE research.\n        *   The dual relation embedding mechanism for time intervals offers a valuable technique for handling complex temporal annotations.\n        *   The investigation into time granularity highlights a critical, yet underexplored, aspect of TKG modeling, opening avenues for research into adaptive or learned time granularity.\n        *   Its ability to model various relation patterns makes it a versatile tool for reasoning over complex real-world TKGs.",
        "keywords": [
          "Temporal Knowledge Graphs (TKGs)",
          "Temporal Knowledge Graph Embedding (TKGE)",
          "TeRo model",
          "Temporal Rotation",
          "Complex Vector Space",
          "Time-aware Entity Evolution",
          "Handling Time Intervals",
          "Dual Relation Embeddings",
          "Complex Relation Patterns",
          "Link Prediction",
          "Time Granularity Analysis",
          "State-of-the-Art Advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "552bfaca30af29647c083993fbe406867fc70d4c.pdf"
    },
    {
      "success": true,
      "doc_id": "276b9feb7dca4742844b4319560c96dd",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Technical Paper Analysis: GE2: A General and Efficient System for Graph Embedding Learning \\cite{zheng2024}\n\n1.  **Research Problem & Motivation**\n    *   Existing graph embedding systems (e.g., PBG, DGL-KE, Marius) suffer from long CPU time and high CPU-GPU communication overhead, particularly when utilizing multiple GPUs.\n    *   Implementing various negative sampling algorithms, which are critical for model quality, is cumbersome and lacks generality in current systems.\n    *   The core problem is to achieve both generality (for diverse negative sampling) and efficiency (reducing CPU time and communication) in graph embedding learning systems.\n\n2.  **Related Work & Positioning**\n    *   The work positions itself against state-of-the-art graph embedding systems such as PBG, DGL-KE, and Marius.\n    *   Limitations of previous solutions include:\n        *   Inefficient resource utilization leading to long CPU times.\n        *   High CPU-GPU communication overhead, especially in multi-GPU setups.\n        *   Lack of a general and user-friendly mechanism for implementing the diverse variants of negative sampling algorithms.\n\n3.  **Technical Approach & Innovation**\n    *   The paper proposes GE2, a new system designed for general and efficient graph embedding learning.\n    *   **Core Method**: A general execution model is introduced that can encompass various negative sampling algorithms.\n    *   **User-Friendly API**: Based on this execution model, a user-friendly API is designed to simplify the expression and implementation of negative sampling algorithms.\n    *   **Efficiency Enhancements**: Operations are offloaded from the CPU to the GPU to leverage high parallelism and reduce CPU processing time.\n    *   **Multi-GPU Data Management**: The novel COVER algorithm is introduced, which is presented as the first algorithm specifically for managing data swap between the CPU and multiple GPUs with minimal communication costs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Execution Model**: A general execution model that unifies and supports various negative sampling algorithms.\n    *   **User-Friendly API**: An API built upon the execution model, significantly simplifying the implementation of complex negative sampling strategies.\n    *   **CPU-to-GPU Offloading**: A strategy to offload computationally intensive operations from CPU to GPU, enhancing parallelism and reducing CPU bottlenecks.\n    *   **COVER Algorithm**: A novel algorithm for efficient data swap management between CPU and multiple GPUs, specifically designed to minimize communication overhead.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed comparing GE2 against state-of-the-art graph embedding systems.\n    *   **Key Performance Metrics**: Training speed and efficiency were the primary metrics.\n    *   **Comparison Results**: GE2 consistently demonstrated faster training across different models and datasets.\n    *   **Speedup**: Achieved speedups were typically over 2x, reaching up to 7.5x compared to existing systems.\n\n6.  **Limitations & Scope**\n    *   The provided abstract does not explicitly state technical limitations or assumptions of GE2 itself.\n    *   The scope of applicability is focused on graph embedding learning, particularly addressing challenges related to negative sampling and multi-GPU efficiency.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: GE2 significantly advances the technical state-of-the-art in graph embedding systems by providing a more general and substantially more efficient platform.\n    *   **Impact on Future Research**: The general execution model and user-friendly API for negative sampling could simplify future research and development of new graph embedding models. The COVER algorithm's approach to multi-GPU data management could influence the design of other distributed machine learning systems requiring efficient data movement.\n    *   **Practical Impact**: The substantial speedups (2x to 7.5x) translate directly into faster model development and deployment for real-world applications in social networks, e-commerce, and medicine.",
      "intriguing_abstract": "The promise of graph embedding learning is often hampered by system inefficiencies and a lack of generality. Existing platforms suffer from debilitating CPU time and excessive CPU-GPU communication overhead, especially in multi-GPU environments, while the critical diversity of negative sampling algorithms remains difficult to implement. We present GE2, a groundbreaking system engineered for both general and highly efficient graph embedding learning.\n\nGE2 introduces a novel execution model and an intuitive API that unify and simplify the expression of diverse negative sampling strategies. Its unparalleled efficiency is achieved by intelligently offloading computationally intensive operations to the GPU, harnessing massive parallelism. A core innovation is the **COVER algorithm**, the first of its kind to manage multi-GPU data swap with unprecedentedly minimal communication costs. Rigorous evaluations show GE2 delivers up to 7.5x speedups against state-of-the-art systems. GE2 not only dramatically accelerates graph embedding research and real-world deployment but also establishes a new paradigm for scalable, general, and efficient distributed machine learning.",
      "keywords": [
        "GE2 system",
        "graph embedding learning",
        "negative sampling algorithms",
        "multi-GPU systems",
        "CPU-GPU communication overhead",
        "general execution model",
        "user-friendly API",
        "CPU-to-GPU offloading",
        "COVER algorithm",
        "data swap management",
        "training efficiency",
        "significant training speedup"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/33a7b7abf006d22de24c1471e6f6c93842a497b6.pdf",
      "citation_key": "zheng2024",
      "metadata": {
        "title": "GE2: A General and Efficient Knowledge Graph Embedding Learning System",
        "authors": [
          "Chenguang Zheng",
          "Guanxian Jiang",
          "Xiao Yan",
          "Peiqi Yin",
          "Qihui Zhou",
          "James Cheng"
        ],
        "published_date": "2024",
        "abstract": "Graph embedding learning computes an embedding vector for each node in a graph and finds many applications in areas such as social networks, e-commerce, and medicine. We observe that existing graph embedding systems (e.g., PBG, DGL-KE, and Marius) have long CPU time and high CPU-GPU communication overhead, especially when using multiple GPUs. Moreover, it is cumbersome to implement negative sampling algorithms on them, which have many variants and are crucial for model quality. We propose a new system called GE2, which achieves both generality and efficiency for graph embedding learning. In particular, we propose a general execution model that encompasses various negative sampling algorithms. Based on the execution model, we design a user-friendly API that allows users to easily express negative sampling algorithms. To support efficient training, we offload operations from CPU to GPU to enjoy high parallelism and reduce CPU time. We also design COVER, which, to our knowledge, is the first algorithm to manage data swap between CPU and multiple GPUs for small communication costs. Extensive experimental results show that, comparing with the state-of-the-art graph embedding systems, GE2 trains consistently faster across different models and datasets, where the speedup is usually over 2x and can be up to 7.5x.",
        "file_path": "paper_data/knowledge_graph_embedding/33a7b7abf006d22de24c1471e6f6c93842a497b6.pdf",
        "venue": "Proc. ACM Manag. Data",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Technical Paper Analysis: GE2: A General and Efficient System for Graph Embedding Learning \\cite{zheng2024}\n\n1.  **Research Problem & Motivation**\n    *   Existing graph embedding systems (e.g., PBG, DGL-KE, Marius) suffer from long CPU time and high CPU-GPU communication overhead, particularly when utilizing multiple GPUs.\n    *   Implementing various negative sampling algorithms, which are critical for model quality, is cumbersome and lacks generality in current systems.\n    *   The core problem is to achieve both generality (for diverse negative sampling) and efficiency (reducing CPU time and communication) in graph embedding learning systems.\n\n2.  **Related Work & Positioning**\n    *   The work positions itself against state-of-the-art graph embedding systems such as PBG, DGL-KE, and Marius.\n    *   Limitations of previous solutions include:\n        *   Inefficient resource utilization leading to long CPU times.\n        *   High CPU-GPU communication overhead, especially in multi-GPU setups.\n        *   Lack of a general and user-friendly mechanism for implementing the diverse variants of negative sampling algorithms.\n\n3.  **Technical Approach & Innovation**\n    *   The paper proposes GE2, a new system designed for general and efficient graph embedding learning.\n    *   **Core Method**: A general execution model is introduced that can encompass various negative sampling algorithms.\n    *   **User-Friendly API**: Based on this execution model, a user-friendly API is designed to simplify the expression and implementation of negative sampling algorithms.\n    *   **Efficiency Enhancements**: Operations are offloaded from the CPU to the GPU to leverage high parallelism and reduce CPU processing time.\n    *   **Multi-GPU Data Management**: The novel COVER algorithm is introduced, which is presented as the first algorithm specifically for managing data swap between the CPU and multiple GPUs with minimal communication costs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Execution Model**: A general execution model that unifies and supports various negative sampling algorithms.\n    *   **User-Friendly API**: An API built upon the execution model, significantly simplifying the implementation of complex negative sampling strategies.\n    *   **CPU-to-GPU Offloading**: A strategy to offload computationally intensive operations from CPU to GPU, enhancing parallelism and reducing CPU bottlenecks.\n    *   **COVER Algorithm**: A novel algorithm for efficient data swap management between CPU and multiple GPUs, specifically designed to minimize communication overhead.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed comparing GE2 against state-of-the-art graph embedding systems.\n    *   **Key Performance Metrics**: Training speed and efficiency were the primary metrics.\n    *   **Comparison Results**: GE2 consistently demonstrated faster training across different models and datasets.\n    *   **Speedup**: Achieved speedups were typically over 2x, reaching up to 7.5x compared to existing systems.\n\n6.  **Limitations & Scope**\n    *   The provided abstract does not explicitly state technical limitations or assumptions of GE2 itself.\n    *   The scope of applicability is focused on graph embedding learning, particularly addressing challenges related to negative sampling and multi-GPU efficiency.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: GE2 significantly advances the technical state-of-the-art in graph embedding systems by providing a more general and substantially more efficient platform.\n    *   **Impact on Future Research**: The general execution model and user-friendly API for negative sampling could simplify future research and development of new graph embedding models. The COVER algorithm's approach to multi-GPU data management could influence the design of other distributed machine learning systems requiring efficient data movement.\n    *   **Practical Impact**: The substantial speedups (2x to 7.5x) translate directly into faster model development and deployment for real-world applications in social networks, e-commerce, and medicine.",
        "keywords": [
          "GE2 system",
          "graph embedding learning",
          "negative sampling algorithms",
          "multi-GPU systems",
          "CPU-GPU communication overhead",
          "general execution model",
          "user-friendly API",
          "CPU-to-GPU offloading",
          "COVER algorithm",
          "data swap management",
          "training efficiency",
          "significant training speedup"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "33a7b7abf006d22de24c1471e6f6c93842a497b6.pdf"
    },
    {
      "success": true,
      "doc_id": "d8361abacbeadea08c34acb6748943ea",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Knowledge Graphs (KGs) like Freebase and WordNet suffer from incompleteness, despite their large size.\n    *   **Motivation**: This incompleteness leads to performance degradation in AI-related applications, highlighting the need for more complete and robust KG representations.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Most current research focuses on Knowledge Graph Embedding (KGE) models.\n    *   **Limitations of Previous Solutions**: These KGE models typically embed entities and relations into latent vectors without effectively leveraging the rich information inherent in the relation structure itself.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes leveraging a \"three-layer hierarchical relation structure (HRS)\" to learn knowledge representations. This HRS categorizes relations into:\n        *   Top layer: Relation clusters (semantically similar relations).\n        *   Middle layer: Individual relations.\n        *   Bottom layer: Fine-grained sub-relations.\n    *   **Novelty**: The approach innovatively extends existing KGE models (specifically TransE, TransH, and DistMult) by integrating this HRS information, which is a novel way to enrich the embedding process beyond simple latent vector representations. The method is designed to be extensible to other KGE models \\cite{zhang2018}.\n\n*   **Key Technical Contributions**\n    *   **Novel Method**: Introduction and formalization of the three-layer Hierarchical Relation Structure (HRS) for KGs.\n    *   **Algorithmic Innovation**: A framework for extending existing KGE models (e.g., TransE, TransH, DistMult) to incorporate and benefit from the HRS information during the embedding learning process.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed to evaluate the proposed approach.\n    *   **Key Results**: The experiment results \"clearly validate the effectiveness of the proposed approach against baselines\" \\cite{zhang2018}. While specific metrics are not detailed in the provided text, the validation confirms performance improvement.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The proposed approach is designed to be generalizable and capable of extending various other KGE models beyond those explicitly tested (TransE, TransH, DistMult) \\cite{zhang2018}.\n    *   **Technical Limitations**: The provided text does not explicitly state technical limitations or assumptions of the HRS model itself.\n\n*   **Technical Significance**\n    *   **Advancement**: This work advances the technical state-of-the-art in KGE by introducing a structured way to incorporate hierarchical relation information, moving beyond flat relation embeddings.\n    *   **Potential Impact**: It offers a new paradigm for improving the completeness and quality of knowledge graph embeddings, potentially leading to more accurate and robust AI applications that rely on KGs. Future research could explore more complex hierarchical structures or integrate this approach with other KG completion techniques.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are indispensable for advanced AI, yet their pervasive incompleteness severely degrades application performance. Current Knowledge Graph Embedding (KGE) models often fall short by treating relations as flat, atomic entities, overlooking their rich, inherent structure. This paper introduces a groundbreaking paradigm shift: a novel three-layer Hierarchical Relation Structure (HRS) designed to unlock deeper semantic understanding within KGs.\n\nWe formalize HRS, categorizing relations into top-layer clusters, individual relations, and fine-grained sub-relations. Our innovative framework seamlessly extends existing KGE models, including TransE, TransH, and DistMult, by integrating this hierarchical information directly into the embedding learning process. This approach moves beyond simple latent vector representations, enabling KGE models to capture intricate relational semantics previously ignored. Experimental results unequivocally validate the effectiveness of our HRS-enhanced models, demonstrating significant improvements in knowledge representation and completeness. This work offers a powerful new direction for building more robust and accurate KGs, promising a substantial impact on the reliability and performance of AI systems reliant on structured knowledge.",
      "keywords": [
        "Knowledge Graphs (KGs)",
        "Knowledge Graph Embedding (KGE) models",
        "KG incompleteness",
        "three-layer hierarchical relation structure (HRS)",
        "extensible KGE framework",
        "embedding learning process",
        "AI applications",
        "algorithmic innovation",
        "validated effectiveness",
        "state-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/86ac98157da100a529ca65fe6e1da064b0a651e8.pdf",
      "citation_key": "zhang2018",
      "metadata": {
        "title": "Knowledge Graph Embedding with Hierarchical Relation Structure",
        "authors": [
          "Zhao Zhang",
          "Fuzhen Zhuang",
          "Meng Qu",
          "Fen Lin",
          "Qing He"
        ],
        "published_date": "2018",
        "abstract": "The rapid development of knowledge graphs (KGs), such as Freebase and WordNet, has changed the paradigm for AI-related applications. However, even though these KGs are impressively large, most of them are suffering from incompleteness, which leads to performance degradation of AI applications. Most existing researches are focusing on knowledge graph embedding (KGE) models. Nevertheless, those models simply embed entities and relations into latent vectors without leveraging the rich information from the relation structure. Indeed, relations in KGs conform to a three-layer hierarchical relation structure (HRS), i.e., semantically similar relations can make up relation clusters and some relations can be further split into several fine-grained sub-relations. Relation clusters, relations and sub-relations can fit in the top, the middle and the bottom layer of three-layer HRS respectively. To this end, in this paper, we extend existing KGE models TransE, TransH and DistMult, to learn knowledge representations by leveraging the information from the HRS. Particularly, our approach is capable to extend other KGE models. Finally, the experiment results clearly validate the effectiveness of the proposed approach against baselines.",
        "file_path": "paper_data/knowledge_graph_embedding/86ac98157da100a529ca65fe6e1da064b0a651e8.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Knowledge Graphs (KGs) like Freebase and WordNet suffer from incompleteness, despite their large size.\n    *   **Motivation**: This incompleteness leads to performance degradation in AI-related applications, highlighting the need for more complete and robust KG representations.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Most current research focuses on Knowledge Graph Embedding (KGE) models.\n    *   **Limitations of Previous Solutions**: These KGE models typically embed entities and relations into latent vectors without effectively leveraging the rich information inherent in the relation structure itself.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes leveraging a \"three-layer hierarchical relation structure (HRS)\" to learn knowledge representations. This HRS categorizes relations into:\n        *   Top layer: Relation clusters (semantically similar relations).\n        *   Middle layer: Individual relations.\n        *   Bottom layer: Fine-grained sub-relations.\n    *   **Novelty**: The approach innovatively extends existing KGE models (specifically TransE, TransH, and DistMult) by integrating this HRS information, which is a novel way to enrich the embedding process beyond simple latent vector representations. The method is designed to be extensible to other KGE models \\cite{zhang2018}.\n\n*   **Key Technical Contributions**\n    *   **Novel Method**: Introduction and formalization of the three-layer Hierarchical Relation Structure (HRS) for KGs.\n    *   **Algorithmic Innovation**: A framework for extending existing KGE models (e.g., TransE, TransH, DistMult) to incorporate and benefit from the HRS information during the embedding learning process.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed to evaluate the proposed approach.\n    *   **Key Results**: The experiment results \"clearly validate the effectiveness of the proposed approach against baselines\" \\cite{zhang2018}. While specific metrics are not detailed in the provided text, the validation confirms performance improvement.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The proposed approach is designed to be generalizable and capable of extending various other KGE models beyond those explicitly tested (TransE, TransH, DistMult) \\cite{zhang2018}.\n    *   **Technical Limitations**: The provided text does not explicitly state technical limitations or assumptions of the HRS model itself.\n\n*   **Technical Significance**\n    *   **Advancement**: This work advances the technical state-of-the-art in KGE by introducing a structured way to incorporate hierarchical relation information, moving beyond flat relation embeddings.\n    *   **Potential Impact**: It offers a new paradigm for improving the completeness and quality of knowledge graph embeddings, potentially leading to more accurate and robust AI applications that rely on KGs. Future research could explore more complex hierarchical structures or integrate this approach with other KG completion techniques.",
        "keywords": [
          "Knowledge Graphs (KGs)",
          "Knowledge Graph Embedding (KGE) models",
          "KG incompleteness",
          "three-layer hierarchical relation structure (HRS)",
          "extensible KGE framework",
          "embedding learning process",
          "AI applications",
          "algorithmic innovation",
          "validated effectiveness",
          "state-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "86ac98157da100a529ca65fe6e1da064b0a651e8.pdf"
    },
    {
      "success": true,
      "doc_id": "8a2c649401965eca9643b41021bcad52",
      "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey focuses on knowledge graph entity alignment (EA) research, specifically methods based on representation learning and graph embedding \\cite{zhu2024}. Its main objectives are to summarize and analyze representative EA approaches, propose a new comprehensive framework, and identify future research directions.\n\n2.  **Literature Coverage**\n    The survey covers representative and advanced knowledge graph entity alignment (EA) models, particularly those based on representation learning and graph embedding, up to the latest research trends \\cite{zhu2024}. It includes both unimodal and multimodal EA, as well as Chinese EA, aiming to fill gaps in existing reviews by incorporating recent developments.\n\n3.  **Classification Framework**\n    *   The survey proposes a novel three-module framework: Information Aggregation, Alignment, and Post-Alignment modules, detailing technologies within each \\cite{zhu2024}.\n    *   Within the Information Aggregation module, it refines categories into global structure embedding and local semantic information, emphasizing their interaction.\n    *   For experimental analysis, models are categorized by alignment inference strategy, noise filtering strategy, utilization of global structure, and combination of global structure and local semantics.\n\n4.  **Key Findings & Insights**\n    *   Representation learning-based EA methods significantly outperform traditional approaches in performance and efficiency \\cite{zhu2024}.\n    *   The direction of entity alignment significantly impacts model performance, offering a crucial optimization reference for researchers.\n    *   The integration of global structural embedding with local semantic information (e.g., attributes, images) is crucial for enhancing alignment accuracy.\n    *   The survey provides a comparative analysis of unimodal and multimodal EA, classifying models based on global alignment, noise filtering, and information utilization strategies.\n\n5.  **Research Gaps & Future Directions**\n    The survey identifies gaps in existing EA methods, particularly regarding the integration of diverse modalities and the robustness of current models \\cite{zhu2024}. Future research should focus on incorporating additional features like video for multimodal EA, constructing more realistic multi-dimensional datasets, exploring complex vector spaces for embeddings, and comprehensively considering spatial and temporal dimensions to handle dynamic knowledge graphs.\n\n6.  **Survey Contribution**\n    This survey offers a comprehensive and authoritative analysis of representation learning-based entity alignment, filling gaps in existing literature by incorporating the latest models and proposing a novel three-module framework \\cite{zhu2024}. It provides unique value through detailed discussions on global-local information interaction, alignment optimization, non-alignable entity prediction, and innovative future research directions.",
      "intriguing_abstract": "Unlocking the full potential of interconnected knowledge graphs demands robust **entity alignment (EA)**. This comprehensive survey critically analyzes the latest advancements in **representation learning** and **graph embedding** based EA, addressing critical gaps in existing literature by incorporating cutting-edge models and multimodal approaches. We introduce a novel, three-module framework**Information Aggregation, Alignment, and Post-Alignment**to systematically categorize and understand diverse EA methodologies. Our analysis reveals the profound impact of integrating **global structure embedding** with **local semantic information** for superior accuracy, and provides crucial insights into alignment direction and noise filtering strategies. Covering both **unimodal** and **multimodal EA**, including recent developments in Chinese EA, this paper offers a comparative analysis of state-of-the-art models. We identify pressing research gaps, particularly in incorporating diverse modalities like video, constructing realistic multi-dimensional datasets, and handling **dynamic knowledge graphs**. This survey serves as an authoritative guide, providing researchers with a crucial reference for optimizing current EA models and charting innovative future directions, including **non-alignable entity prediction** and complex vector spaces.",
      "keywords": [
        "knowledge graph entity alignment",
        "representation learning",
        "graph embedding",
        "novel three-module framework",
        "unimodal and multimodal EA",
        "global-local information interaction",
        "alignment optimization",
        "non-alignable entity prediction",
        "direction of entity alignment",
        "integration of diverse modalities",
        "dynamic knowledge graphs",
        "complex vector spaces",
        "comprehensive survey"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/52b167a90a10cde25309e40d7f6e6b5e14ec3261.pdf",
      "citation_key": "zhu2024",
      "metadata": {
        "title": "A survey: knowledge graph entity alignment research based on graph embedding",
        "authors": [
          "Beibei Zhu",
          "Ruolin Wang",
          "Junyi Wang",
          "Fei Shao",
          "Kerun Wang"
        ],
        "published_date": "2024",
        "abstract": "Entity alignment (EA) aims to automatically match entities in different knowledge graphs, which is beneficial to the development of knowledge-driven applications. Representation learning has powerful feature capture capability and it is widely used in the field of natural language processing. Compared with traditional EA methods, EA methods based on representation learning have better performance and efficiency. Hence, we summarize and analyze the representative EA approaches based on representation learning in this paper. We present the problem description and data preprocessing for EA and other related fundamental knowledge. We propose a new EA framework for the latest models, which includes information aggregation module, entity alignment module, and post-alignment module. Based on these three modules, the various technologies are described in detail. In the experimental part, we first explore the effect of EA direction on model performance. Then, we classify the models into different categories in terms of alignment inference strategy, noise filtering strategy, and whether additional information is utilized. To ensure fairness, we perform the comparative analysis of the performance of the models within the categories separately on different datasets. We investigate both unimodal and multimodal EA. Finally, we present future research perspectives based on the shortcomings of existing EA methods.",
        "file_path": "paper_data/knowledge_graph_embedding/52b167a90a10cde25309e40d7f6e6b5e14ec3261.pdf",
        "venue": "Artificial Intelligence Review",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey focuses on knowledge graph entity alignment (EA) research, specifically methods based on representation learning and graph embedding \\cite{zhu2024}. Its main objectives are to summarize and analyze representative EA approaches, propose a new comprehensive framework, and identify future research directions.\n\n2.  **Literature Coverage**\n    The survey covers representative and advanced knowledge graph entity alignment (EA) models, particularly those based on representation learning and graph embedding, up to the latest research trends \\cite{zhu2024}. It includes both unimodal and multimodal EA, as well as Chinese EA, aiming to fill gaps in existing reviews by incorporating recent developments.\n\n3.  **Classification Framework**\n    *   The survey proposes a novel three-module framework: Information Aggregation, Alignment, and Post-Alignment modules, detailing technologies within each \\cite{zhu2024}.\n    *   Within the Information Aggregation module, it refines categories into global structure embedding and local semantic information, emphasizing their interaction.\n    *   For experimental analysis, models are categorized by alignment inference strategy, noise filtering strategy, utilization of global structure, and combination of global structure and local semantics.\n\n4.  **Key Findings & Insights**\n    *   Representation learning-based EA methods significantly outperform traditional approaches in performance and efficiency \\cite{zhu2024}.\n    *   The direction of entity alignment significantly impacts model performance, offering a crucial optimization reference for researchers.\n    *   The integration of global structural embedding with local semantic information (e.g., attributes, images) is crucial for enhancing alignment accuracy.\n    *   The survey provides a comparative analysis of unimodal and multimodal EA, classifying models based on global alignment, noise filtering, and information utilization strategies.\n\n5.  **Research Gaps & Future Directions**\n    The survey identifies gaps in existing EA methods, particularly regarding the integration of diverse modalities and the robustness of current models \\cite{zhu2024}. Future research should focus on incorporating additional features like video for multimodal EA, constructing more realistic multi-dimensional datasets, exploring complex vector spaces for embeddings, and comprehensively considering spatial and temporal dimensions to handle dynamic knowledge graphs.\n\n6.  **Survey Contribution**\n    This survey offers a comprehensive and authoritative analysis of representation learning-based entity alignment, filling gaps in existing literature by incorporating the latest models and proposing a novel three-module framework \\cite{zhu2024}. It provides unique value through detailed discussions on global-local information interaction, alignment optimization, non-alignable entity prediction, and innovative future research directions.",
        "keywords": [
          "knowledge graph entity alignment",
          "representation learning",
          "graph embedding",
          "novel three-module framework",
          "unimodal and multimodal EA",
          "global-local information interaction",
          "alignment optimization",
          "non-alignable entity prediction",
          "direction of entity alignment",
          "integration of diverse modalities",
          "dynamic knowledge graphs",
          "complex vector spaces",
          "comprehensive survey"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "52b167a90a10cde25309e40d7f6e6b5e14ec3261.pdf"
    },
    {
      "success": true,
      "doc_id": "affa1fb3cc061e6998d65797fe9773cc",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis: Multi-domain Item-Item Recommendation based on Cross-domain Knowledge Graph Embedding \\cite{liu2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional knowledge graph (KG)-based recommender systems, while effective for sparsity and cold start within a single domain, struggle with the \"cross-domain cold start problem\" and are unable to provide \"multi-domain recommendations\" \\cite{liu2023}.\n    *   **Importance & Challenge**: Addressing these limitations is crucial for improving user experience by enabling accurate and efficient recommendations across diverse domains, which is challenging due to the need to model complex interactions between items from different domains \\cite{liu2023}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon existing knowledge graph-based recommender systems, acknowledging their success in mitigating sparsity and single-domain cold start issues \\cite{liu2023}.\n    *   **Limitations of Previous Solutions**: Previous KG-based systems are limited by their inability to effectively handle item interactions and cold start scenarios *across* different domains, thus failing to provide comprehensive multi-domain recommendations \\cite{liu2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a multi-domain item-item (I2I) recommendation approach based on cross-domain knowledge graph embedding \\cite{liu2023}. This involves analyzing both homo-domain item associations and hetero-domain item interactions within a rich knowledge graph \\cite{liu2023}.\n    *   **Novelty**:\n        *   A novel \"cross-domain knowledge graph chiasmal embedding approach\" is introduced to efficiently interact all items across multiple domains \\cite{liu2023}.\n        *   A \"binding rule\" is put forward to facilitate both homo-domain and hetero-domain embedding of items \\cite{liu2023}.\n        *   The multi-domain I2I recommendation is framed as a \"link prediction\" problem within the knowledge graph \\cite{liu2023}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Proposed a \"cross-domain knowledge graph chiasmal embedding approach\" for efficient multi-domain item interaction \\cite{liu2023}.\n        *   Introduced a \"binding rule\" to enable effective homo-domain and hetero-domain item embeddings \\cite{liu2023}.\n        *   Developed a \"multi-domain I2I recommendation method\" formulated as a knowledge graph link prediction task \\cite{liu2023}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The proposed methods were compared and analyzed against several benchmark methods \\cite{liu2023}.\n    *   **Key Performance Metrics & Results**: Experiments were conducted using two datasets. The results demonstrated that the proposed methods achieved superior performance in both \"link prediction results\" and \"multi-domain recommendation results\" compared to the benchmarks \\cite{liu2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper does not explicitly state technical limitations or assumptions beyond the scope of its problem definition.\n    *   **Scope of Applicability**: The methods are specifically designed for multi-domain item-item recommendation, leveraging cross-domain knowledge graphs and focusing on link prediction for recommendation \\cite{liu2023}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the state-of-the-art by providing a robust solution to the cross-domain cold start problem and enabling effective multi-domain recommendations, which were limitations of prior KG-based systems \\cite{liu2023}.\n    *   **Potential Impact**: The proposed chiasmal embedding and binding rule offer novel mechanisms for integrating and leveraging cross-domain information, potentially paving the way for more sophisticated and comprehensive multi-domain recommender systems in future research \\cite{liu2023}.",
      "intriguing_abstract": "Recommender systems face a critical hurdle: the pervasive \"cross-domain cold start problem\" and the inability to provide truly comprehensive \"multi-domain recommendations.\" While traditional knowledge graph (KG)-based approaches excel within single domains, they struggle to model the intricate, complex interactions between items from diverse domains. This paper introduces a groundbreaking multi-domain item-item (I2I) recommendation framework built upon a novel \"cross-domain knowledge graph embedding\" paradigm.\n\nOur core innovation lies in a unique \"cross-domain knowledge graph chiasmal embedding approach\" that efficiently captures intricate relationships across multiple domains, coupled with a powerful \"binding rule\" facilitating both homo-domain and hetero-domain item embeddings. By reframing multi-domain I2I recommendation as a \"link prediction\" task within this enriched KG, our method significantly overcomes the limitations of prior systems. Experimental results on two datasets demonstrate superior performance in both link prediction and multi-domain recommendation, marking a significant advancement in mitigating cross-domain cold start and paving the way for more intelligent, interconnected recommender systems.",
      "keywords": [
        "Multi-domain Item-Item Recommendation",
        "Cross-domain Knowledge Graph Embedding",
        "cross-domain cold start problem",
        "knowledge graph-based recommender systems",
        "cross-domain knowledge graph chiasmal embedding",
        "binding rule",
        "link prediction",
        "homo-domain item associations",
        "hetero-domain item interactions",
        "superior performance",
        "state-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/145fa4ea1567a6b9d981fdea0e183140d99aeb97.pdf",
      "citation_key": "liu2023",
      "metadata": {
        "title": "Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation",
        "authors": [
          "Jia Liu",
          "Wei Huang",
          "Tianrui Li",
          "Shenggong Ji",
          "Junbo Zhang"
        ],
        "published_date": "2023",
        "abstract": "Recommender system can provide users with the required information accurately and efficiently, playing a very important role in improving users life experience. Although knowledge graph-based recommender system can solve the sparsity and cold start problems faced by traditional recommender system, it cannot handle the cross-domain cold start problem and cannot provide multi-domain recommendations. Therefore, this paper focuses on multi-domain item-item (I2I) recommendation based on cross-domain knowledge graph embedding by analyzing the association between items of the same domain and the interaction between items of diverse domains with the aid of knowledge graph that contains rich information. First, a cross-domain knowledge graph chiasmal embedding approach is proposed to efficiently interact all items in multiple domains. To help achieve both homo-domain embedding and hetero-domain embedding of items, a binding rule is put forward. Second, a multi-domain I2I recommendation method is presented to efficiently recommend items in multiple domains, which is a recommendation method based on link prediction of knowledge graph. Finally, the proposed methods are compared and analyzed with some benchmark methods using two datasets. The experimental results show that the proposed methods achieve better link prediction results and multi-domain recommendation results.",
        "file_path": "paper_data/knowledge_graph_embedding/145fa4ea1567a6b9d981fdea0e183140d99aeb97.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n### Technical Paper Analysis: Multi-domain Item-Item Recommendation based on Cross-domain Knowledge Graph Embedding \\cite{liu2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Traditional knowledge graph (KG)-based recommender systems, while effective for sparsity and cold start within a single domain, struggle with the \"cross-domain cold start problem\" and are unable to provide \"multi-domain recommendations\" \\cite{liu2023}.\n    *   **Importance & Challenge**: Addressing these limitations is crucial for improving user experience by enabling accurate and efficient recommendations across diverse domains, which is challenging due to the need to model complex interactions between items from different domains \\cite{liu2023}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon existing knowledge graph-based recommender systems, acknowledging their success in mitigating sparsity and single-domain cold start issues \\cite{liu2023}.\n    *   **Limitations of Previous Solutions**: Previous KG-based systems are limited by their inability to effectively handle item interactions and cold start scenarios *across* different domains, thus failing to provide comprehensive multi-domain recommendations \\cite{liu2023}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a multi-domain item-item (I2I) recommendation approach based on cross-domain knowledge graph embedding \\cite{liu2023}. This involves analyzing both homo-domain item associations and hetero-domain item interactions within a rich knowledge graph \\cite{liu2023}.\n    *   **Novelty**:\n        *   A novel \"cross-domain knowledge graph chiasmal embedding approach\" is introduced to efficiently interact all items across multiple domains \\cite{liu2023}.\n        *   A \"binding rule\" is put forward to facilitate both homo-domain and hetero-domain embedding of items \\cite{liu2023}.\n        *   The multi-domain I2I recommendation is framed as a \"link prediction\" problem within the knowledge graph \\cite{liu2023}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Proposed a \"cross-domain knowledge graph chiasmal embedding approach\" for efficient multi-domain item interaction \\cite{liu2023}.\n        *   Introduced a \"binding rule\" to enable effective homo-domain and hetero-domain item embeddings \\cite{liu2023}.\n        *   Developed a \"multi-domain I2I recommendation method\" formulated as a knowledge graph link prediction task \\cite{liu2023}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The proposed methods were compared and analyzed against several benchmark methods \\cite{liu2023}.\n    *   **Key Performance Metrics & Results**: Experiments were conducted using two datasets. The results demonstrated that the proposed methods achieved superior performance in both \"link prediction results\" and \"multi-domain recommendation results\" compared to the benchmarks \\cite{liu2023}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper does not explicitly state technical limitations or assumptions beyond the scope of its problem definition.\n    *   **Scope of Applicability**: The methods are specifically designed for multi-domain item-item recommendation, leveraging cross-domain knowledge graphs and focusing on link prediction for recommendation \\cite{liu2023}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the state-of-the-art by providing a robust solution to the cross-domain cold start problem and enabling effective multi-domain recommendations, which were limitations of prior KG-based systems \\cite{liu2023}.\n    *   **Potential Impact**: The proposed chiasmal embedding and binding rule offer novel mechanisms for integrating and leveraging cross-domain information, potentially paving the way for more sophisticated and comprehensive multi-domain recommender systems in future research \\cite{liu2023}.",
        "keywords": [
          "Multi-domain Item-Item Recommendation",
          "Cross-domain Knowledge Graph Embedding",
          "cross-domain cold start problem",
          "knowledge graph-based recommender systems",
          "cross-domain knowledge graph chiasmal embedding",
          "binding rule",
          "link prediction",
          "homo-domain item associations",
          "hetero-domain item interactions",
          "superior performance",
          "state-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "145fa4ea1567a6b9d981fdea0e183140d99aeb97.pdf"
    },
    {
      "success": true,
      "doc_id": "aec8d90b0fd730f9e270e2ad3376cb45",
      "summary": "Here is a focused summary of the technical paper for literature review, adhering to your requirements:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Knowledge bases (e.g., Freebase, YAGO, DBPedia, Nell) are inherently incomplete, containing numerous missing facts.\n    *   **Importance and Challenge**: This incompleteness severely limits their utility in diverse natural language processing applications. While knowledge graph embedding (KGE) is a promising approach, any single KGE model is insufficient to achieve comprehensive knowledge base completion \\cite{choi2020}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon the foundation of knowledge graph embedding (KGE) models, which map entities and relations into a low-dimensional vector space to infer missing facts.\n    *   **Limitations of Previous Solutions**: Previous solutions, primarily single KGE models, are deemed insufficient for robust knowledge base completion due to each model's inherent \"idiosyncrasy\" and limited perspective \\cite{choi2020}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper defines knowledge base completion as a ranking task and proposes a novel \"committee-based knowledge graph embedding model\" \\cite{choi2020}.\n    *   **Novelty**: The innovation lies in forming a \"committee of various knowledge graph embeddings.\" This committee aggregates diverse perspectives from different KGE models to compute the plausibility of candidate facts more comprehensively. Candidate facts are then ranked by this committee-computed plausibility, and the top-k facts are selected as missing facts \\cite{choi2020}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of a committee-based knowledge graph embedding model for enhanced knowledge base completion.\n    *   **System Design/Architectural Innovations**: A framework that integrates multiple, diverse KGE models into a unified committee to leverage their individual strengths and overcome their limitations.\n    *   **Theoretical Insights**: The implicit insight that combining models with \"idiosyncrasies\" leads to a more robust and accurate measure of fact plausibility by considering various perspectives \\cite{choi2020}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The proposed model was evaluated through experiments on \"two data sets\" \\cite{choi2020}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   The proposed committee-based model achieved \"higher performance than any single knowledge graph embedding\" \\cite{choi2020}.\n        *   It demonstrated \"robust performances regardless of k\" (the number of top facts chosen), indicating its stability and effectiveness across different completion thresholds \\cite{choi2020}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily addresses the limitations of *single* KGE models. While it doesn't explicitly state limitations of its *own* committee model, it assumes that combining diverse KGEs will inherently lead to superior performance. The specific KGE models chosen for the committee and their weighting (if any) are not detailed in the provided abstract.\n    *   **Scope of Applicability**: The method is applicable to knowledge base completion tasks where missing facts need to be identified and ranked.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The work significantly advances the state-of-the-art in knowledge base completion by demonstrating that ensemble or committee-based approaches can overcome the inherent limitations of individual KGE models \\cite{choi2020}.\n    *   **Potential Impact on Future Research**: It opens avenues for future research into optimal strategies for combining diverse KGE models, exploring different committee formation techniques, and understanding how various model \"idiosyncrasies\" contribute to overall performance. The robust performance regardless of 'k' also highlights its practical utility.",
      "intriguing_abstract": "Incomplete knowledge bases (KBs) critically impede the utility of AI systems, yet current knowledge graph embedding (KGE) models, despite their individual strengths, offer only fragmented solutions due to their inherent \"idiosyncrasies.\" We present a groundbreaking solution: a novel **committee-based knowledge graph embedding model** for robust knowledge base completion. Our approach redefines KBC as a sophisticated ranking task, where a dynamic committee of diverse KGE models collaboratively assesses the plausibility of candidate facts.\n\nThis innovative framework aggregates multiple, distinct perspectives, effectively overcoming the limitations of any single KGE model and yielding a far more comprehensive and accurate measure of fact plausibility. Through rigorous experiments on two datasets, our committee-based model consistently outperforms individual KGEs, demonstrating significantly higher accuracy and remarkable robustness across varying completion thresholds. This work not only advances the state-of-the-art in **knowledge base completion** but also establishes a powerful paradigm for leveraging ensemble intelligence in **knowledge graph embedding**, paving the way for more complete and reliable AI knowledge systems.",
      "keywords": [
        "Knowledge base completion",
        "Knowledge graph embedding (KGE)",
        "Incomplete knowledge bases",
        "Committee-based KGE model",
        "Aggregating diverse KGE perspectives",
        "Fact plausibility",
        "Ranking task",
        "Natural language processing",
        "Ensemble approaches",
        "Overcoming single KGE limitations",
        "Higher performance",
        "Robust performance",
        "State-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c.pdf",
      "citation_key": "choi2020",
      "metadata": {
        "title": "An Approach to Knowledge Base Completion by a Committee-Based Knowledge Graph Embedding",
        "authors": [
          "S. Choi",
          "Hyun-Je Song",
          "Seong-Bae Park"
        ],
        "published_date": "2020",
        "abstract": "Knowledge bases such as Freebase, YAGO, DBPedia, and Nell contain a number of facts with various entities and relations. Since they store many facts, they are regarded as core resources for many natural language processing tasks. Nevertheless, they are not normally complete and have many missing facts. Such missing facts keep them from being used in diverse applications in spite of their usefulness. Therefore, it is significant to complete knowledge bases. Knowledge graph embedding is one of the promising approaches to completing a knowledge base and thus many variants of knowledge graph embedding have been proposed. It maps all entities and relations in knowledge base onto a low dimensional vector space. Then, candidate facts that are plausible in the space are determined as missing facts. However, any single knowledge graph embedding is insufficient to complete a knowledge base. As a solution to this problem, this paper defines knowledge base completion as a ranking task and proposes a committee-based knowledge graph embedding model for improving the performance of knowledge base completion. Since each knowledge graph embedding has its own idiosyncrasy, we make up a committee of various knowledge graph embeddings to reflect various perspectives. After ranking all candidate facts according to their plausibility computed by the committee, the top-k facts are chosen as missing facts. Our experimental results on two data sets show that the proposed model achieves higher performance than any single knowledge graph embedding and shows robust performances regardless of k. These results prove that the proposed model considers various perspectives in measuring the plausibility of candidate facts.",
        "file_path": "paper_data/knowledge_graph_embedding/e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c.pdf",
        "venue": "Applied Sciences",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the technical paper for literature review, adhering to your requirements:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Knowledge bases (e.g., Freebase, YAGO, DBPedia, Nell) are inherently incomplete, containing numerous missing facts.\n    *   **Importance and Challenge**: This incompleteness severely limits their utility in diverse natural language processing applications. While knowledge graph embedding (KGE) is a promising approach, any single KGE model is insufficient to achieve comprehensive knowledge base completion \\cite{choi2020}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon the foundation of knowledge graph embedding (KGE) models, which map entities and relations into a low-dimensional vector space to infer missing facts.\n    *   **Limitations of Previous Solutions**: Previous solutions, primarily single KGE models, are deemed insufficient for robust knowledge base completion due to each model's inherent \"idiosyncrasy\" and limited perspective \\cite{choi2020}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper defines knowledge base completion as a ranking task and proposes a novel \"committee-based knowledge graph embedding model\" \\cite{choi2020}.\n    *   **Novelty**: The innovation lies in forming a \"committee of various knowledge graph embeddings.\" This committee aggregates diverse perspectives from different KGE models to compute the plausibility of candidate facts more comprehensively. Candidate facts are then ranked by this committee-computed plausibility, and the top-k facts are selected as missing facts \\cite{choi2020}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of a committee-based knowledge graph embedding model for enhanced knowledge base completion.\n    *   **System Design/Architectural Innovations**: A framework that integrates multiple, diverse KGE models into a unified committee to leverage their individual strengths and overcome their limitations.\n    *   **Theoretical Insights**: The implicit insight that combining models with \"idiosyncrasies\" leads to a more robust and accurate measure of fact plausibility by considering various perspectives \\cite{choi2020}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The proposed model was evaluated through experiments on \"two data sets\" \\cite{choi2020}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   The proposed committee-based model achieved \"higher performance than any single knowledge graph embedding\" \\cite{choi2020}.\n        *   It demonstrated \"robust performances regardless of k\" (the number of top facts chosen), indicating its stability and effectiveness across different completion thresholds \\cite{choi2020}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily addresses the limitations of *single* KGE models. While it doesn't explicitly state limitations of its *own* committee model, it assumes that combining diverse KGEs will inherently lead to superior performance. The specific KGE models chosen for the committee and their weighting (if any) are not detailed in the provided abstract.\n    *   **Scope of Applicability**: The method is applicable to knowledge base completion tasks where missing facts need to be identified and ranked.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The work significantly advances the state-of-the-art in knowledge base completion by demonstrating that ensemble or committee-based approaches can overcome the inherent limitations of individual KGE models \\cite{choi2020}.\n    *   **Potential Impact on Future Research**: It opens avenues for future research into optimal strategies for combining diverse KGE models, exploring different committee formation techniques, and understanding how various model \"idiosyncrasies\" contribute to overall performance. The robust performance regardless of 'k' also highlights its practical utility.",
        "keywords": [
          "Knowledge base completion",
          "Knowledge graph embedding (KGE)",
          "Incomplete knowledge bases",
          "Committee-based KGE model",
          "Aggregating diverse KGE perspectives",
          "Fact plausibility",
          "Ranking task",
          "Natural language processing",
          "Ensemble approaches",
          "Overcoming single KGE limitations",
          "Higher performance",
          "Robust performance",
          "State-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c.pdf"
    },
    {
      "success": true,
      "doc_id": "c88ee9ad5c1182bf9763b035d64665aa",
      "summary": "Here's a focused summary of the paper \"Knowledge Graph Embedding with 3D Compound Geometric Transformations\" \\cite{ge2023} for a literature review:\n\n---\n\n### Analysis of \"Knowledge Graph Embedding with 3D Compound Geometric Transformations\" \\cite{ge2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of \"missing link prediction\" in Knowledge Graphs (KGs), which are often incomplete. It specifically aims to develop more expressive and effective Knowledge Graph Embedding (KGE) models for this task.\n    *   **Importance and Challenge:** KGs are crucial for various AI applications (knowledge management, recommendation, chatbots). Existing distance-based KGE models, while effective, often rely on single 2D geometric transformations (e.g., translation, rotation, scaling) or limited 2D compound transformations, which may not fully capture the rich and complex underlying characteristics of diverse relations in KGs. Modeling non-commutative relations and achieving better parameterization are also challenges.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon and extends previous geometric transformation-based KGE models:\n        *   **TransE \\cite{ge2023}, RotatE \\cite{ge2023}, PairRE \\cite{ge2023}:** These models use single 2D geometric transformations (translation, rotation, scaling, respectively). \\cite{ge2023} positions them as degenerate cases of more complex compound models.\n        *   **CompoundE \\cite{ge2023}:** This model exploited the cascade of multiple 2D geometric transformations (translation, rotation, scaling). \\cite{ge2023} extends CompoundE by moving to 3D transformations and including more affine operations.\n        *   **Rotate3D \\cite{ge2023}:** This model leveraged 3D rotation for KGE, demonstrating better modeling power for non-commutative relations than 2D rotation (RotatE). \\cite{ge2023} is inspired by Rotate3D's use of 3D space.\n    *   **Limitations of Previous Solutions:**\n        *   Single 2D transformations are often insufficient to model the diversity and complexity of relations in KGs.\n        *   CompoundE, while powerful, was limited to 2D transformations and a specific set of operations.\n        *   Previous approaches lacked a systematic way to explore and combine a wider range of geometric transformations, especially in 3D, or to effectively ensemble multiple model variants.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{ge2023} proposes **CompoundE3D**, a family of KGE models that leverage 3D compound geometric transformations to model relations between entities.\n        *   It incorporates five 3D affine operations: **Translation (T), Scaling (S), Rotation (R), Reflection (F), and Shear (H)**. These operations are represented as 4x4 matrices in homogeneous coordinates and can be cascaded.\n        *   Relations are modeled by applying these compound operators to head entities, tail entities, or both, leading to three scoring functions: `CompoundE3D-Head`, `CompoundE3D-Tail`, and `CompoundE3D-Complete`.\n        *   A high-dimensional relation operator is represented as a block diagonal matrix of these compound operators.\n    *   **Novelty/Differentiation:**\n        *   **Expanded Transformation Space:** It extends beyond 2D transformations and the limited set of operations in CompoundE by including Reflection and Shear in 3D space, significantly enlarging the design space for relation representations.\n        *   **Adapted Beam Search Algorithm:** To navigate the \"huge search space\" of possible CompoundE3D variants (combinations of operations and their application points), \\cite{ge2023} introduces an adapted beam search algorithm. This algorithm gradually builds more complex scoring functions from simpler ones, optimizing for performance while managing complexity.\n        *   **Model Ensemble Strategies:** To further boost performance and mitigate errors from individual variants, \\cite{ge2023} explores two ensemble strategies:\n            *   **Weighted-Distances-Sum (WDS):** Combines scores from top-k variants using uniform, geometric, or learnable weights.\n            *   **Rank Fusion:** Applies unsupervised rank aggregation functions to unify rank predictions from individual model variants.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduction of 3D affine operations (Translation, Scaling, Rotation, Reflection, Shear) for KGE, allowing for more versatile relation representations.\n        *   Development of the CompoundE3D framework, which systematically combines these 3D operations.\n        *   An adapted beam search algorithm for efficient discovery of optimal CompoundE3D model variants, balancing complexity and performance.\n        *   Exploration and application of two ensemble strategies (Weighted-Distances-Sum and Rank Fusion) to aggregate decisions from multiple CompoundE3D variants.\n    *   **Theoretical Insights/Analysis:** Analysis of the properties of each geometric operation and its advantages in modeling different relation types, backed by empirical results.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The effectiveness and flexibility of CompoundE3D were experimentally verified through link prediction tasks.\n    *   **Key Performance Metrics & Results:** Experiments were conducted on four popular link prediction datasets. The paper claims that CompoundE3D, especially with ensemble strategies, yields \"superior performance\" and that its effectiveness is \"experimentally verified.\" (Specific metrics like MRR, Hits@N are implied for link prediction, though not detailed in the provided abstract/intro).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The inherent complexity of the search space for optimal compound operations is a significant challenge, which the beam search algorithm aims to address but doesn't eliminate.\n        *   The choice of the total number of stages for compounding operations is a user-selected hyper-parameter.\n        *   The effectiveness of ensemble methods relies on the diversity and quality of the individual variants.\n    *   **Scope of Applicability:** Primarily focused on distance-based KGE models for link prediction in general KGs. The framework is designed to be flexible enough to match rich underlying characteristics of various KG datasets.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{ge2023} significantly advances the state-of-the-art in geometric transformation-based KGE by:\n        *   Expanding the modeling power through a richer set of 3D affine transformations.\n        *   Providing a systematic and efficient method (beam search) to explore and construct complex KGE models.\n        *   Demonstrating the effectiveness of ensemble learning for KGE, which has been under-explored, to boost link prediction performance.\n    *   **Potential Impact on Future Research:** This work opens avenues for:\n        *   Further exploration of other advanced geometric or algebraic structures for KGE.\n        *   Developing more sophisticated search algorithms for optimal KGE model architectures.\n        *   Encouraging the wider adoption and development of ensemble methods in KGE and other graph-based machine learning tasks.\n        *   Inspiring deeper analysis into which specific geometric transformations are best suited for different types of relations in KGs.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graphs (KGs) hinges on models that can truly capture the intricate, diverse nature of relations for robust **link prediction**. Existing **Knowledge Graph Embedding (KGE)** models often fall short, relying on limited 2D geometric transformations. We introduce **CompoundE3D**, a groundbreaking KGE framework that pioneers the use of **3D compound geometric transformations** to achieve unprecedented relational expressiveness.\n\nCompoundE3D systematically combines five fundamental **3D affine operations****Translation, Scaling, Rotation, Reflection, and Shear**to model complex relational patterns, vastly expanding beyond previous 2D approaches. To navigate the immense design space of these compound operations, we develop an **adapted beam search algorithm** that efficiently discovers optimal model configurations. Furthermore, we introduce novel **ensemble learning strategies**, including Weighted-Distances-Sum and Rank Fusion, to aggregate predictions and boost performance. Extensive experiments demonstrate CompoundE3D's superior performance in **link prediction** across diverse datasets, significantly advancing the state-of-the-art and establishing a new paradigm for geometric KGE. This work opens exciting avenues for modeling complex relational data with unparalleled fidelity.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "missing link prediction",
        "3D compound geometric transformations",
        "CompoundE3D",
        "3D affine operations",
        "expanded transformation space",
        "adapted beam search algorithm",
        "model ensemble strategies",
        "Weighted-Distances-Sum (WDS)",
        "Rank Fusion",
        "non-commutative relations",
        "homogeneous coordinates",
        "state-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/4085a5cf49c193fe3d3ff19ff2d696fe20a5a596.pdf",
      "citation_key": "ge2023",
      "metadata": {
        "title": "Knowledge Graph Embedding with 3D Compound Geometric Transformations",
        "authors": [
          "Xiou Ge",
          "Yun Cheng Wang",
          "Bin Wang",
          "C. J. Kuo"
        ],
        "published_date": "2023",
        "abstract": "The cascade of 2D geometric transformations were exploited to model relations between entities in a knowledge graph (KG), leading to an effective KG embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was proposed as a new KGE model, Rotate3D, by leveraging its non-commutative property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear and propose a family of KGE models, named CompoundE3D, in this work. CompoundE3D allows multiple design variants to match rich underlying characteristics of a KG. Since each variant has its own advantages on a subset of relations, an ensemble of multiple variants can yield superior performance. The effectiveness and flexibility of CompoundE3D are experimentally verified on four popular link prediction datasets.",
        "file_path": "paper_data/knowledge_graph_embedding/4085a5cf49c193fe3d3ff19ff2d696fe20a5a596.pdf",
        "venue": "APSIPA Transactions on Signal and Information Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Knowledge Graph Embedding with 3D Compound Geometric Transformations\" \\cite{ge2023} for a literature review:\n\n---\n\n### Analysis of \"Knowledge Graph Embedding with 3D Compound Geometric Transformations\" \\cite{ge2023}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of \"missing link prediction\" in Knowledge Graphs (KGs), which are often incomplete. It specifically aims to develop more expressive and effective Knowledge Graph Embedding (KGE) models for this task.\n    *   **Importance and Challenge:** KGs are crucial for various AI applications (knowledge management, recommendation, chatbots). Existing distance-based KGE models, while effective, often rely on single 2D geometric transformations (e.g., translation, rotation, scaling) or limited 2D compound transformations, which may not fully capture the rich and complex underlying characteristics of diverse relations in KGs. Modeling non-commutative relations and achieving better parameterization are also challenges.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon and extends previous geometric transformation-based KGE models:\n        *   **TransE \\cite{ge2023}, RotatE \\cite{ge2023}, PairRE \\cite{ge2023}:** These models use single 2D geometric transformations (translation, rotation, scaling, respectively). \\cite{ge2023} positions them as degenerate cases of more complex compound models.\n        *   **CompoundE \\cite{ge2023}:** This model exploited the cascade of multiple 2D geometric transformations (translation, rotation, scaling). \\cite{ge2023} extends CompoundE by moving to 3D transformations and including more affine operations.\n        *   **Rotate3D \\cite{ge2023}:** This model leveraged 3D rotation for KGE, demonstrating better modeling power for non-commutative relations than 2D rotation (RotatE). \\cite{ge2023} is inspired by Rotate3D's use of 3D space.\n    *   **Limitations of Previous Solutions:**\n        *   Single 2D transformations are often insufficient to model the diversity and complexity of relations in KGs.\n        *   CompoundE, while powerful, was limited to 2D transformations and a specific set of operations.\n        *   Previous approaches lacked a systematic way to explore and combine a wider range of geometric transformations, especially in 3D, or to effectively ensemble multiple model variants.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** \\cite{ge2023} proposes **CompoundE3D**, a family of KGE models that leverage 3D compound geometric transformations to model relations between entities.\n        *   It incorporates five 3D affine operations: **Translation (T), Scaling (S), Rotation (R), Reflection (F), and Shear (H)**. These operations are represented as 4x4 matrices in homogeneous coordinates and can be cascaded.\n        *   Relations are modeled by applying these compound operators to head entities, tail entities, or both, leading to three scoring functions: `CompoundE3D-Head`, `CompoundE3D-Tail`, and `CompoundE3D-Complete`.\n        *   A high-dimensional relation operator is represented as a block diagonal matrix of these compound operators.\n    *   **Novelty/Differentiation:**\n        *   **Expanded Transformation Space:** It extends beyond 2D transformations and the limited set of operations in CompoundE by including Reflection and Shear in 3D space, significantly enlarging the design space for relation representations.\n        *   **Adapted Beam Search Algorithm:** To navigate the \"huge search space\" of possible CompoundE3D variants (combinations of operations and their application points), \\cite{ge2023} introduces an adapted beam search algorithm. This algorithm gradually builds more complex scoring functions from simpler ones, optimizing for performance while managing complexity.\n        *   **Model Ensemble Strategies:** To further boost performance and mitigate errors from individual variants, \\cite{ge2023} explores two ensemble strategies:\n            *   **Weighted-Distances-Sum (WDS):** Combines scores from top-k variants using uniform, geometric, or learnable weights.\n            *   **Rank Fusion:** Applies unsupervised rank aggregation functions to unify rank predictions from individual model variants.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduction of 3D affine operations (Translation, Scaling, Rotation, Reflection, Shear) for KGE, allowing for more versatile relation representations.\n        *   Development of the CompoundE3D framework, which systematically combines these 3D operations.\n        *   An adapted beam search algorithm for efficient discovery of optimal CompoundE3D model variants, balancing complexity and performance.\n        *   Exploration and application of two ensemble strategies (Weighted-Distances-Sum and Rank Fusion) to aggregate decisions from multiple CompoundE3D variants.\n    *   **Theoretical Insights/Analysis:** Analysis of the properties of each geometric operation and its advantages in modeling different relation types, backed by empirical results.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The effectiveness and flexibility of CompoundE3D were experimentally verified through link prediction tasks.\n    *   **Key Performance Metrics & Results:** Experiments were conducted on four popular link prediction datasets. The paper claims that CompoundE3D, especially with ensemble strategies, yields \"superior performance\" and that its effectiveness is \"experimentally verified.\" (Specific metrics like MRR, Hits@N are implied for link prediction, though not detailed in the provided abstract/intro).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The inherent complexity of the search space for optimal compound operations is a significant challenge, which the beam search algorithm aims to address but doesn't eliminate.\n        *   The choice of the total number of stages for compounding operations is a user-selected hyper-parameter.\n        *   The effectiveness of ensemble methods relies on the diversity and quality of the individual variants.\n    *   **Scope of Applicability:** Primarily focused on distance-based KGE models for link prediction in general KGs. The framework is designed to be flexible enough to match rich underlying characteristics of various KG datasets.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** \\cite{ge2023} significantly advances the state-of-the-art in geometric transformation-based KGE by:\n        *   Expanding the modeling power through a richer set of 3D affine transformations.\n        *   Providing a systematic and efficient method (beam search) to explore and construct complex KGE models.\n        *   Demonstrating the effectiveness of ensemble learning for KGE, which has been under-explored, to boost link prediction performance.\n    *   **Potential Impact on Future Research:** This work opens avenues for:\n        *   Further exploration of other advanced geometric or algebraic structures for KGE.\n        *   Developing more sophisticated search algorithms for optimal KGE model architectures.\n        *   Encouraging the wider adoption and development of ensemble methods in KGE and other graph-based machine learning tasks.\n        *   Inspiring deeper analysis into which specific geometric transformations are best suited for different types of relations in KGs.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "missing link prediction",
          "3D compound geometric transformations",
          "CompoundE3D",
          "3D affine operations",
          "expanded transformation space",
          "adapted beam search algorithm",
          "model ensemble strategies",
          "Weighted-Distances-Sum (WDS)",
          "Rank Fusion",
          "non-commutative relations",
          "homogeneous coordinates",
          "state-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596.pdf"
    },
    {
      "success": true,
      "doc_id": "bced66f0149b6686a7eaef30dd702981",
      "summary": "Here's a focused summary of the paper `\\cite{sadeghian2021}` for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenging problem of temporal link prediction in Temporal Knowledge Graphs (TKGs), which involves inferring missing facts (quadruples `(h, r, t, )`) that include a temporal dimension.\n    *   **Importance and Challenge**: This problem is crucial because real-world facts and relations evolve over time, making static Knowledge Graph (KG) reasoning insufficient. It is challenging due to:\n        *   Data non-stationarity and heterogeneity.\n        *   Complex temporal dependencies between facts.\n        *   The inherent incompleteness of KGs.\n        *   Limitations of existing TKG models, which often suffer from a large number of parameters, making them difficult to train, or rely on inadequate, time-sparse datasets.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Builds upon the success of static KG embedding models, particularly rotation-based models like RotatE `\\cite{sun2019rotate}`.\n        *   Extends KG embedding techniques to incorporate time, learning representations for entities, relations, and timestamps.\n    *   **Limitations of Previous Solutions**:\n        *   Most prior research focused on static KGs, failing to capture temporal dynamics.\n        *   Early temporal approaches either ignored timestamps, aggregated static embeddings, or used sequence models (e.g., RNNs) that sometimes only learned dynamic embeddings for relations, not entities.\n        *   Many existing temporal link prediction models utilize a large number of parameters, hindering training efficiency.\n        *   Some models were evaluated on datasets sparse in the time domain, limiting their generalizability.\n        *   Static rotation models like RotatE use Euclidean distance for scoring, which can be problematic in high-dimensional spaces due to the \"curse of dimensionality.\"\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **ChronoR (Chronological Rotation embedding)**, a novel model for learning representations in TKGs based on k-dimensional rotation transformations.\n    *   **Rotation-based Transformation**: ChronoR learns a `k`-dimensional rotation transformation `Qr,` (parametrized by both relation `r` and time ``) such that when applied to a head entity `h`, it maps `h` close to its corresponding tail entity `t` (i.e., `Qr,(h)  t`). This high-dimensional rotation captures rich interactions between temporal and multi-relational characteristics.\n    *   **Novel Scoring Function**: Unlike RotatE, which uses Euclidean distance, ChronoR defines its scoring function `g(h,r,t,) := <Qr,(h), t>` based on the **inner product** (cosine similarity) between the transformed head entity and the tail entity. This is motivated by observations that Euclidean norms can be less effective in high dimensions.\n    *   **Theoretical Generalization**: The paper demonstrates a significant theoretical insight: the proposed inner product scoring function is a generalization of commonly used scoring functions in complex-domain models like ComplEx `\\cite{trouillon2016complex}` (specifically, `Re(h * r - t)`) when `k=2` (complex numbers).\n    *   **Parameterization of `Q`**: The linear operator `Q` is parameterized by concatenating relation and time embeddings (`[r|]`). An additional static rotation component `r2` is included to better represent facts that are static or less time-dependent.\n    *   **Optimization and Regularization**:\n        *   Minimizes the negative log-likelihood of correct predictions, avoiding the need for negative sampling.\n        *   Introduces a novel **tensor nuclear norm-inspired regularization** (`4()`) that treats the TKG directly as an order 4 tensor.\n        *   Incorporates a **temporal smoothness objective** (``) using the 4-norm to encourage similar transformations for chronologically closer timestamps, reflecting the smooth evolution of entities over time.\n\n4.  **Key Technical Contributions**\n    *   **Novel Model**: ChronoR, a state-of-the-art rotation-based embedding model specifically designed for temporal knowledge graphs.\n    *   **Unified Rotation and Temporal Modeling**: Effectively integrates k-dimensional rotation transformations with temporal information, allowing for complex interactions between relations and time.\n    *   **Inner Product Scoring Function**: Proposes and validates an inner product-based scoring function that is more robust in high dimensions and generalizes existing complex-domain scoring methods.\n    *   **Theoretical Link**: Provides a theoretical foundation by proving that common complex-domain scoring functions are a special case of ChronoR's approach.\n    *   **Advanced Regularization**: Introduces a novel tensor nuclear norm-inspired regularization for TKGs and a 4-norm based temporal smoothness regularization, enhancing model generalizability.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: ChronoR was evaluated on the temporal link prediction task.\n    *   **Datasets**: Performance was assessed on three widely used benchmark datasets: ICEWS14, ICEWS05-15, and YAGO15K.\n    *   **Performance Metrics**: Standard metrics for link prediction were used: Mean Reciprocal Rank (MRR), Hit@1, Hit@3, and Hit@10.\n    *   **Comparison Results**: ChronoR consistently outperformed numerous state-of-the-art baselines, including TransE, DistMult, ComplEx, SimpIE, ConT, TTransE, HyTE, TA-DistMult, DE-SimpIE, TIMEPLEX, TNTComplEx, TeRo, and TeMP-SA. For example, ChronoR (k=2) achieved an MRR of 62.53 on ICEWS14, surpassing TNTComplEx (60.72) and TeMP-SA (60.7). On ICEWS05-15, ChronoR (k=3) achieved an MRR of 68.41, outperforming TNTComplEx (66.64) and TeMP-SA (68.0).\n\n6.  **Limitations & Scope**\n    *   **Scope of Applicability**: The current work focuses on predicting temporal facts *within the observed time range* (`T`) rather than forecasting future events.\n    *   **Assumptions**: Assumes time is often discretized in TKGs.\n    *   **Computational Feasibility**: While the negative log-likelihood loss is preferred for its theoretical benefits, its computational feasibility (avoiding negative sampling) is noted to be suitable for the experimental scale, but could be a consideration for extremely large KGs.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: ChronoR significantly pushes the technical state-of-the-art in temporal knowledge graph link prediction, demonstrating superior performance across multiple benchmarks.\n    *   **Novel Modeling Paradigm**: Introduces a robust and theoretically grounded rotation-based embedding approach that effectively captures the complex interplay of entities, relations, and time in dynamic KGs.\n    *   **Unified Perspective**: Provides a unifying theoretical framework by demonstrating the generalization capability of its inner-product scoring function over existing complex-domain methods.\n    *   **Impact on Future Research**: The success of ChronoR's rotation-based approach, novel scoring function, and regularization techniques opens new avenues for exploring more sophisticated temporal transformations, extending to forecasting tasks, and applying similar principles to other dynamic graph problems.",
      "intriguing_abstract": "Real-world knowledge is inherently dynamic, yet capturing its temporal evolution for accurate prediction remains a formidable challenge in Temporal Knowledge Graphs (TKGs). Existing temporal link prediction models often struggle with parameter explosion, data non-stationarity, and inadequate representation of complex temporal dependencies. We introduce **ChronoR (Chronological Rotation embedding)**, a novel, theoretically grounded model that revolutionizes temporal link prediction by leveraging k-dimensional rotation transformations. Unlike prior approaches, ChronoR employs a robust inner product-based scoring function, which we theoretically prove generalizes widely used complex-domain scoring methods. Our innovation extends to a unique tensor nuclear norm-inspired regularization and a 4-norm based temporal smoothness objective, meticulously designed to capture the nuanced evolution of facts over time. ChronoR consistently outperforms state-of-the-art baselines across multiple benchmark datasets (ICEWS14, ICEWS05-15, YAGO15K), achieving significant gains in MRR, Hit@1, Hit@3, and Hit@10. This work not only advances the technical state-of-the-art but also offers a unified, efficient, and highly effective paradigm for modeling dynamic knowledge, paving the way for more intelligent temporal reasoning systems.",
      "keywords": [
        "Temporal Knowledge Graphs (TKGs)",
        "temporal link prediction",
        "ChronoR",
        "k-dimensional rotation embedding",
        "inner product scoring function",
        "tensor nuclear norm regularization",
        "temporal smoothness objective",
        "dynamic Knowledge Graphs",
        "theoretical generalization",
        "state-of-the-art performance",
        "data non-stationarity",
        "complex temporal dependencies",
        "rotation-based models",
        "negative log-likelihood optimization"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/4e52607397a96fb2104a99c570c9cec29c9ca519.pdf",
      "citation_key": "sadeghian2021",
      "metadata": {
        "title": "ChronoR: Rotation Based Temporal Knowledge Graph Embedding",
        "authors": [
          "A. Sadeghian",
          "Mohammadreza Armandpour",
          "Anthony Colas",
          "D. Wang"
        ],
        "published_date": "2021",
        "abstract": "Despite the importance and abundance of temporal knowledge graphs, most of the current research has been focused on reasoning on static graphs. In this paper, we study the challenging problem of inference over temporal knowledge graphs. In particular, the task of temporal link prediction. In general, this is a difficult task due to data non-stationarity, data heterogeneity, and its complex temporal dependencies. \nWe propose Chronological Rotation embedding (ChronoR), a novel model for learning representations for entities, relations, and time. Learning dense representations is frequently used as an efficient and versatile method to perform reasoning on knowledge graphs. The proposed model learns a k-dimensional rotation transformation parametrized by relation and time, such that after each fact's head entity is transformed using the rotation, it falls near its corresponding tail entity. By using high dimensional rotation as its transformation operator, ChronoR captures rich interaction between the temporal and multi-relational characteristics of a Temporal Knowledge Graph. Experimentally, we show that ChronoR is able to outperform many of the state-of-the-art methods on the benchmark datasets for temporal knowledge graph link prediction.",
        "file_path": "paper_data/knowledge_graph_embedding/4e52607397a96fb2104a99c570c9cec29c9ca519.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper `\\cite{sadeghian2021}` for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenging problem of temporal link prediction in Temporal Knowledge Graphs (TKGs), which involves inferring missing facts (quadruples `(h, r, t, )`) that include a temporal dimension.\n    *   **Importance and Challenge**: This problem is crucial because real-world facts and relations evolve over time, making static Knowledge Graph (KG) reasoning insufficient. It is challenging due to:\n        *   Data non-stationarity and heterogeneity.\n        *   Complex temporal dependencies between facts.\n        *   The inherent incompleteness of KGs.\n        *   Limitations of existing TKG models, which often suffer from a large number of parameters, making them difficult to train, or rely on inadequate, time-sparse datasets.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Builds upon the success of static KG embedding models, particularly rotation-based models like RotatE `\\cite{sun2019rotate}`.\n        *   Extends KG embedding techniques to incorporate time, learning representations for entities, relations, and timestamps.\n    *   **Limitations of Previous Solutions**:\n        *   Most prior research focused on static KGs, failing to capture temporal dynamics.\n        *   Early temporal approaches either ignored timestamps, aggregated static embeddings, or used sequence models (e.g., RNNs) that sometimes only learned dynamic embeddings for relations, not entities.\n        *   Many existing temporal link prediction models utilize a large number of parameters, hindering training efficiency.\n        *   Some models were evaluated on datasets sparse in the time domain, limiting their generalizability.\n        *   Static rotation models like RotatE use Euclidean distance for scoring, which can be problematic in high-dimensional spaces due to the \"curse of dimensionality.\"\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **ChronoR (Chronological Rotation embedding)**, a novel model for learning representations in TKGs based on k-dimensional rotation transformations.\n    *   **Rotation-based Transformation**: ChronoR learns a `k`-dimensional rotation transformation `Qr,` (parametrized by both relation `r` and time ``) such that when applied to a head entity `h`, it maps `h` close to its corresponding tail entity `t` (i.e., `Qr,(h)  t`). This high-dimensional rotation captures rich interactions between temporal and multi-relational characteristics.\n    *   **Novel Scoring Function**: Unlike RotatE, which uses Euclidean distance, ChronoR defines its scoring function `g(h,r,t,) := <Qr,(h), t>` based on the **inner product** (cosine similarity) between the transformed head entity and the tail entity. This is motivated by observations that Euclidean norms can be less effective in high dimensions.\n    *   **Theoretical Generalization**: The paper demonstrates a significant theoretical insight: the proposed inner product scoring function is a generalization of commonly used scoring functions in complex-domain models like ComplEx `\\cite{trouillon2016complex}` (specifically, `Re(h * r - t)`) when `k=2` (complex numbers).\n    *   **Parameterization of `Q`**: The linear operator `Q` is parameterized by concatenating relation and time embeddings (`[r|]`). An additional static rotation component `r2` is included to better represent facts that are static or less time-dependent.\n    *   **Optimization and Regularization**:\n        *   Minimizes the negative log-likelihood of correct predictions, avoiding the need for negative sampling.\n        *   Introduces a novel **tensor nuclear norm-inspired regularization** (`4()`) that treats the TKG directly as an order 4 tensor.\n        *   Incorporates a **temporal smoothness objective** (``) using the 4-norm to encourage similar transformations for chronologically closer timestamps, reflecting the smooth evolution of entities over time.\n\n4.  **Key Technical Contributions**\n    *   **Novel Model**: ChronoR, a state-of-the-art rotation-based embedding model specifically designed for temporal knowledge graphs.\n    *   **Unified Rotation and Temporal Modeling**: Effectively integrates k-dimensional rotation transformations with temporal information, allowing for complex interactions between relations and time.\n    *   **Inner Product Scoring Function**: Proposes and validates an inner product-based scoring function that is more robust in high dimensions and generalizes existing complex-domain scoring methods.\n    *   **Theoretical Link**: Provides a theoretical foundation by proving that common complex-domain scoring functions are a special case of ChronoR's approach.\n    *   **Advanced Regularization**: Introduces a novel tensor nuclear norm-inspired regularization for TKGs and a 4-norm based temporal smoothness regularization, enhancing model generalizability.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: ChronoR was evaluated on the temporal link prediction task.\n    *   **Datasets**: Performance was assessed on three widely used benchmark datasets: ICEWS14, ICEWS05-15, and YAGO15K.\n    *   **Performance Metrics**: Standard metrics for link prediction were used: Mean Reciprocal Rank (MRR), Hit@1, Hit@3, and Hit@10.\n    *   **Comparison Results**: ChronoR consistently outperformed numerous state-of-the-art baselines, including TransE, DistMult, ComplEx, SimpIE, ConT, TTransE, HyTE, TA-DistMult, DE-SimpIE, TIMEPLEX, TNTComplEx, TeRo, and TeMP-SA. For example, ChronoR (k=2) achieved an MRR of 62.53 on ICEWS14, surpassing TNTComplEx (60.72) and TeMP-SA (60.7). On ICEWS05-15, ChronoR (k=3) achieved an MRR of 68.41, outperforming TNTComplEx (66.64) and TeMP-SA (68.0).\n\n6.  **Limitations & Scope**\n    *   **Scope of Applicability**: The current work focuses on predicting temporal facts *within the observed time range* (`T`) rather than forecasting future events.\n    *   **Assumptions**: Assumes time is often discretized in TKGs.\n    *   **Computational Feasibility**: While the negative log-likelihood loss is preferred for its theoretical benefits, its computational feasibility (avoiding negative sampling) is noted to be suitable for the experimental scale, but could be a consideration for extremely large KGs.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: ChronoR significantly pushes the technical state-of-the-art in temporal knowledge graph link prediction, demonstrating superior performance across multiple benchmarks.\n    *   **Novel Modeling Paradigm**: Introduces a robust and theoretically grounded rotation-based embedding approach that effectively captures the complex interplay of entities, relations, and time in dynamic KGs.\n    *   **Unified Perspective**: Provides a unifying theoretical framework by demonstrating the generalization capability of its inner-product scoring function over existing complex-domain methods.\n    *   **Impact on Future Research**: The success of ChronoR's rotation-based approach, novel scoring function, and regularization techniques opens new avenues for exploring more sophisticated temporal transformations, extending to forecasting tasks, and applying similar principles to other dynamic graph problems.",
        "keywords": [
          "Temporal Knowledge Graphs (TKGs)",
          "temporal link prediction",
          "ChronoR",
          "k-dimensional rotation embedding",
          "inner product scoring function",
          "tensor nuclear norm regularization",
          "temporal smoothness objective",
          "dynamic Knowledge Graphs",
          "theoretical generalization",
          "state-of-the-art performance",
          "data non-stationarity",
          "complex temporal dependencies",
          "rotation-based models",
          "negative log-likelihood optimization"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "4e52607397a96fb2104a99c570c9cec29c9ca519.pdf"
    },
    {
      "success": true,
      "doc_id": "868fa0c35038d55b66ac95b5230dff52",
      "summary": "Here's a focused summary of the paper \"Fast and Continual Knowledge Graph Embedding via Incremental LoRA\" by Liu et al. for a literature review:\n\n---\n\n### Analysis of \"Fast and Continual Knowledge Graph Embedding via Incremental LoRA\" \\cite{liu2024}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of Continual Knowledge Graph Embedding (CKGE), which involves efficiently learning new knowledge in evolving Knowledge Graphs (KGs) while simultaneously preserving previously learned old knowledge.\n    *   **Importance and Challenge**: Real-world KGs are continuously growing (e.g., Wikidata), making traditional KGE methods that require retraining the entire KG prohibitively expensive. Existing CKGE approaches primarily focus on mitigating catastrophic forgetting of old knowledge but often neglect the efficiency of learning new knowledge, leading to significant training costs, especially with large-scale KGs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Full-parameter fine-tuning methods**: These approaches (e.g., replay-based like GEM, EMR, DiCGRL, or regularization-based like SI, EWC, LKGE) effectively mitigate catastrophic forgetting but incur high training costs due to updating all parameters or replaying old data.\n        *   **Incremental-parameter fine-tuning methods**: These methods (e.g., PNN, CWR) adapt architectural properties to accommodate new information with fewer parameters but can still lead to unacceptable increases in parameters and training time due to straightforward alignment of new and old parameter dimensions.\n        *   **Low-Rank Adapters (LoRA) in LLMs**: `\\cite{liu2024}` is inspired by LoRA's success in efficiently fine-tuning Large Language Models (LLMs) by injecting trainable low-rank decomposition matrices.\n    *   **Limitations of Previous Solutions**: Prior CKGE methods largely overlook training efficiency when KGs evolve. While LoRA has been used in LLMs and for general continual learning to alleviate forgetting, its application to the specific challenges of CKGE (especially efficient learning of new KG knowledge) is novel.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{liu2024}` proposes **FastKGE**, a fast CKGE framework that incorporates an **Incremental Low-Rank Adapter (IncLoRA)** mechanism. The framework operates in three stages:\n        1.  **Graph Layering**: New entities and relations are divided into distinct layers based on their importance, determined by their distance from the old KG (using BFS) and degree centrality within the new triples. This isolates new knowledge to specific layers.\n        2.  **IncLoRA Learning**: Embeddings for entities and relations in each layer are represented by incremental low-rank adapters (Ak, Bk). This significantly reduces the number of trainable parameters.\n        3.  **Link Predicting**: All learned LoRA groups from current and previous snapshots are concatenated with original embeddings for inference, with no additional time consumption during prediction.\n    *   **Novelty/Differentiation**:\n        *   **First to introduce LoRA to CKGE**: `\\cite{liu2024}` innovatively adapts low-rank adapters to store new KG knowledge, reducing training costs and preserving old knowledge.\n        *   **Fine-grained knowledge isolation**: New knowledge is isolated and allocated to specific layers based on the fine-grained influence between old and new KGs (distance from old graph, degree centrality).\n        *   **Adaptive Rank Allocation**: IncLoRA introduces an adaptive rank allocation strategy. Instead of a fixed rank, more important entities (those with higher degree centrality) are assigned higher ranks in their respective LoRAs, allowing for more information preservation.\n        *   **Focus on efficiency for *new* knowledge**: While mitigating forgetting, the primary innovation lies in accelerating the acquisition of new knowledge in growing KGs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of the **IncLoRA mechanism** for Continual Knowledge Graph Embedding, enabling efficient learning and storage of new knowledge.\n        *   **Graph Layering strategy** that sorts and divides new entities into layers based on distance from the old graph and degree centrality.\n        *   **Adaptive Rank Allocation** within IncLoRA, which dynamically adjusts the rank scale of adapters based on the importance (degree centrality) of entities.\n    *   **System Design/Architectural Innovations**: The **FastKGE framework** integrates graph layering, incremental low-rank decomposition, and adaptive rank allocation into a cohesive system for efficient CKGE.\n    *   **New Datasets**: Construction and release of two new, larger-scale CKGE datasets, **FB-CKGE** and **WN-CKGE**, addressing the deficiency of small initial KG sizes in existing benchmarks.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed on four traditional CKGE datasets (ENTITY, RELATION, FACT, HYBRID) and two newly constructed datasets (FB-CKGE, WN-CKGE) with larger initial KGs. The task evaluated was link prediction.\n    *   **Key Performance Metrics**: Mean Reciprocal Rank (MRR), Hits@1, Hits@3, and Hits@10 were used to measure link prediction performance. Total training time across all snapshots was measured for efficiency.\n    *   **Comparison Results**:\n        *   On **four public datasets**: FastKGE reduced training time by **34%-49%** while achieving competitive link prediction performance against state-of-the-art models (average MRR score of 21.0% for FastKGE vs. 21.1% for SOTAs).\n        *   On **two newly constructed datasets (FB-CKGE, WN-CKGE)**: FastKGE saved **51%-68%** training time and *improved* link prediction performance by **1.5%** in MRR on average.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The layering strategy primarily focuses on entities, with all new relations placed in a single layer, assuming entity growth is more significant.\n        *   The base KGE model used for experiments is TransE, and the generalizability to other KGE models is implied but not explicitly demonstrated for all.\n        *   The hyper-parameter `N` for the number of entity layers needs to be tuned.\n    *   **Scope of Applicability**: FastKGE is designed for dynamic KGs where new knowledge continuously emerges. Its primary benefit is in scenarios requiring efficient updates to KGE models without full retraining, especially for KGs with a substantial foundational graph.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{liu2024}` significantly advances the technical state-of-the-art in CKGE by introducing a novel, parameter-efficient fine-tuning paradigm based on low-rank adapters. It effectively addresses the often-neglected aspect of efficient learning for new knowledge while maintaining competitive performance in mitigating catastrophic forgetting.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research in efficient continual learning for structured data like KGs, potentially inspiring similar low-rank adaptation techniques for other dynamic graph-based tasks. The release of larger-scale CKGE datasets also provides a more realistic benchmark for future research in this domain.",
      "intriguing_abstract": "Real-world Knowledge Graphs (KGs) are dynamic, constantly evolving with new information. This poses a critical challenge for Continual Knowledge Graph Embedding (CKGE): how to efficiently integrate novel knowledge while preserving previously learned representations, without incurring prohibitive retraining costs or overlooking the efficiency of *new knowledge acquisition*.\n\nWe introduce **FastKGE**, a novel framework that pioneers the application of **Low-Rank Adapters (LoRA)** to CKGE through an **Incremental Low-Rank Adapter (IncLoRA)** mechanism. FastKGE efficiently learns new knowledge by isolating it via a **graph layering strategy** and adaptively allocating ranks to IncLoRA modules based on entity importance. This **parameter-efficient fine-tuning** approach drastically reduces trainable parameters, offering a scalable solution for dynamic KGs. Our experiments demonstrate FastKGE slashes training time by 34-68% across various benchmarks, including two new large-scale datasets (FB-CKGE, WN-CKGE) we release, while maintaining or *improving* **link prediction** performance and effectively mitigating **catastrophic forgetting**. FastKGE sets a new standard for efficient continual learning in evolving KGs, enabling practical, real-time knowledge updates.",
      "keywords": [
        "Continual Knowledge Graph Embedding (CKGE)",
        "Evolving Knowledge Graphs",
        "Low-Rank Adapters (LoRA)",
        "FastKGE framework",
        "Incremental LoRA (IncLoRA)",
        "Graph Layering strategy",
        "Adaptive Rank Allocation",
        "Catastrophic forgetting mitigation",
        "Efficient knowledge acquisition",
        "Link prediction",
        "Parameter-efficient fine-tuning",
        "New CKGE datasets"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf",
      "citation_key": "liu2024",
      "metadata": {
        "title": "Fast and Continual Knowledge Graph Embedding via Incremental LoRA",
        "authors": [
          "Jiajun Liu",
          "Wenjun Ke",
          "Peng Wang",
          "Jiahao Wang",
          "Jinhua Gao",
          "Ziyu Shang",
          "Guozheng Li",
          "Zijie Xu",
          "Ke Ji",
          "Yining Li"
        ],
        "published_date": "2024",
        "abstract": "Continual Knowledge Graph Embedding (CKGE) aims to efficiently learn new knowledge and simultaneously preserve old knowledge. Dominant approaches primarily focus on alleviating catastrophic forgetting of old knowledge but neglect efficient learning for the emergence of new knowledge. However, in real-world scenarios, knowledge graphs (KGs) are continuously growing, which brings a significant challenge to fine-tuning KGE models efficiently. To address this issue, we propose a fast CKGE framework (FastKGE), incorporating an incremental low-rank adapter (IncLoRA) mechanism to efficiently acquire new knowledge while preserving old knowledge. Specifically, to mitigate catastrophic forgetting, FastKGE isolates and allocates new knowledge to specific layers based on the fine-grained influence between old and new KGs. Subsequently, to accelerate fine-tuning, FastKGE devises an efficient IncLoRA mechanism, which embeds the specific layers into incremental low-rank adapters with fewer training parameters. Moreover, IncLoRA introduces adaptive rank allocation, which makes the LoRA aware of the importance of entities and adjusts its rank scale adaptively. We conduct experiments on four public datasets and two new datasets with a larger initial scale. Experimental results demonstrate that FastKGE can reduce training time by 34%-49% while still achieving competitive link prediction performance against state-of-the-art models on four public datasets (average MRR score of 21.0% vs. 21.1%). Meanwhile, on two newly constructed datasets, FastKGE saves 51%-68% training time and improves link prediction performance by 1.5%.",
        "file_path": "paper_data/knowledge_graph_embedding/eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Fast and Continual Knowledge Graph Embedding via Incremental LoRA\" by Liu et al. for a literature review:\n\n---\n\n### Analysis of \"Fast and Continual Knowledge Graph Embedding via Incremental LoRA\" \\cite{liu2024}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of Continual Knowledge Graph Embedding (CKGE), which involves efficiently learning new knowledge in evolving Knowledge Graphs (KGs) while simultaneously preserving previously learned old knowledge.\n    *   **Importance and Challenge**: Real-world KGs are continuously growing (e.g., Wikidata), making traditional KGE methods that require retraining the entire KG prohibitively expensive. Existing CKGE approaches primarily focus on mitigating catastrophic forgetting of old knowledge but often neglect the efficiency of learning new knowledge, leading to significant training costs, especially with large-scale KGs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Full-parameter fine-tuning methods**: These approaches (e.g., replay-based like GEM, EMR, DiCGRL, or regularization-based like SI, EWC, LKGE) effectively mitigate catastrophic forgetting but incur high training costs due to updating all parameters or replaying old data.\n        *   **Incremental-parameter fine-tuning methods**: These methods (e.g., PNN, CWR) adapt architectural properties to accommodate new information with fewer parameters but can still lead to unacceptable increases in parameters and training time due to straightforward alignment of new and old parameter dimensions.\n        *   **Low-Rank Adapters (LoRA) in LLMs**: `\\cite{liu2024}` is inspired by LoRA's success in efficiently fine-tuning Large Language Models (LLMs) by injecting trainable low-rank decomposition matrices.\n    *   **Limitations of Previous Solutions**: Prior CKGE methods largely overlook training efficiency when KGs evolve. While LoRA has been used in LLMs and for general continual learning to alleviate forgetting, its application to the specific challenges of CKGE (especially efficient learning of new KG knowledge) is novel.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{liu2024}` proposes **FastKGE**, a fast CKGE framework that incorporates an **Incremental Low-Rank Adapter (IncLoRA)** mechanism. The framework operates in three stages:\n        1.  **Graph Layering**: New entities and relations are divided into distinct layers based on their importance, determined by their distance from the old KG (using BFS) and degree centrality within the new triples. This isolates new knowledge to specific layers.\n        2.  **IncLoRA Learning**: Embeddings for entities and relations in each layer are represented by incremental low-rank adapters (Ak, Bk). This significantly reduces the number of trainable parameters.\n        3.  **Link Predicting**: All learned LoRA groups from current and previous snapshots are concatenated with original embeddings for inference, with no additional time consumption during prediction.\n    *   **Novelty/Differentiation**:\n        *   **First to introduce LoRA to CKGE**: `\\cite{liu2024}` innovatively adapts low-rank adapters to store new KG knowledge, reducing training costs and preserving old knowledge.\n        *   **Fine-grained knowledge isolation**: New knowledge is isolated and allocated to specific layers based on the fine-grained influence between old and new KGs (distance from old graph, degree centrality).\n        *   **Adaptive Rank Allocation**: IncLoRA introduces an adaptive rank allocation strategy. Instead of a fixed rank, more important entities (those with higher degree centrality) are assigned higher ranks in their respective LoRAs, allowing for more information preservation.\n        *   **Focus on efficiency for *new* knowledge**: While mitigating forgetting, the primary innovation lies in accelerating the acquisition of new knowledge in growing KGs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of the **IncLoRA mechanism** for Continual Knowledge Graph Embedding, enabling efficient learning and storage of new knowledge.\n        *   **Graph Layering strategy** that sorts and divides new entities into layers based on distance from the old graph and degree centrality.\n        *   **Adaptive Rank Allocation** within IncLoRA, which dynamically adjusts the rank scale of adapters based on the importance (degree centrality) of entities.\n    *   **System Design/Architectural Innovations**: The **FastKGE framework** integrates graph layering, incremental low-rank decomposition, and adaptive rank allocation into a cohesive system for efficient CKGE.\n    *   **New Datasets**: Construction and release of two new, larger-scale CKGE datasets, **FB-CKGE** and **WN-CKGE**, addressing the deficiency of small initial KG sizes in existing benchmarks.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed on four traditional CKGE datasets (ENTITY, RELATION, FACT, HYBRID) and two newly constructed datasets (FB-CKGE, WN-CKGE) with larger initial KGs. The task evaluated was link prediction.\n    *   **Key Performance Metrics**: Mean Reciprocal Rank (MRR), Hits@1, Hits@3, and Hits@10 were used to measure link prediction performance. Total training time across all snapshots was measured for efficiency.\n    *   **Comparison Results**:\n        *   On **four public datasets**: FastKGE reduced training time by **34%-49%** while achieving competitive link prediction performance against state-of-the-art models (average MRR score of 21.0% for FastKGE vs. 21.1% for SOTAs).\n        *   On **two newly constructed datasets (FB-CKGE, WN-CKGE)**: FastKGE saved **51%-68%** training time and *improved* link prediction performance by **1.5%** in MRR on average.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The layering strategy primarily focuses on entities, with all new relations placed in a single layer, assuming entity growth is more significant.\n        *   The base KGE model used for experiments is TransE, and the generalizability to other KGE models is implied but not explicitly demonstrated for all.\n        *   The hyper-parameter `N` for the number of entity layers needs to be tuned.\n    *   **Scope of Applicability**: FastKGE is designed for dynamic KGs where new knowledge continuously emerges. Its primary benefit is in scenarios requiring efficient updates to KGE models without full retraining, especially for KGs with a substantial foundational graph.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{liu2024}` significantly advances the technical state-of-the-art in CKGE by introducing a novel, parameter-efficient fine-tuning paradigm based on low-rank adapters. It effectively addresses the often-neglected aspect of efficient learning for new knowledge while maintaining competitive performance in mitigating catastrophic forgetting.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research in efficient continual learning for structured data like KGs, potentially inspiring similar low-rank adaptation techniques for other dynamic graph-based tasks. The release of larger-scale CKGE datasets also provides a more realistic benchmark for future research in this domain.",
        "keywords": [
          "Continual Knowledge Graph Embedding (CKGE)",
          "Evolving Knowledge Graphs",
          "Low-Rank Adapters (LoRA)",
          "FastKGE framework",
          "Incremental LoRA (IncLoRA)",
          "Graph Layering strategy",
          "Adaptive Rank Allocation",
          "Catastrophic forgetting mitigation",
          "Efficient knowledge acquisition",
          "Link prediction",
          "Parameter-efficient fine-tuning",
          "New CKGE datasets"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf"
    },
    {
      "success": true,
      "doc_id": "0c9e2d437f72f1644a083a6b917546d0",
      "summary": "Here's a focused summary of the paper \"TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction\" for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing knowledge graph embedding (KGE) methods, particularly PairRE, suffer from suboptimal performance in knowledge graph completion (link prediction) due to strict restrictions on entity distribution. PairRE forces entities onto hyper-ellipsoid surfaces, limiting optimization flexibility and potentially entangling close entity embeddings, especially for complex relations (1-to-N, N-to-1, N-to-N).\n    *   **Importance and Challenge**: Knowledge graphs are often incomplete, making link prediction a crucial task. Effectively modeling complex relations and ensuring flexible entity representation are challenging, as rigid constraints can hinder the learning of true semantic connections and lead to suboptimal embeddings.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `TranSHER` \\cite{li2022} builds upon distance-based KGE models (e.g., TransE, RotatE) and directly addresses the limitations of the recently proposed PairRE. It also implicitly competes with semantic matching models (e.g., DistMult, ComplEx, SEEK).\n    *   **Limitations of Previous Solutions**:\n        *   **PairRE**: While effective for complex relations, it strictly restricts entity embeddings to hyper-ellipsoidal surfaces, limiting their optimization path to \"arc paths\" and potentially entangling close entities, leading to suboptimal performance \\cite{li2022}.\n        *   **TransE and its extensions (TransH, TransR, ManifoldE)**: Claimed to be weak in modeling certain relation patterns (as argued by RotatE) \\cite{li2022}.\n        *   **Semantic Matching Models (e.g., RESCAL, DistMult, ComplEx, SEEK)**: Often struggle to distinguish similar entities and lack the ability to simultaneously model multiple relation patterns \\cite{li2022}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: `TranSHER` \\cite{li2022} proposes a novel translational distance-based score function that leverages relation-specific translations to relax the hyper-ellipsoidal restriction imposed by methods like PairRE.\n    *   **Novelty/Difference**:\n        *   It first maps entity vectors to hyper-ellipsoids using relation-specific mapping functions (`GH_r(eh)` and `GT_r(et)`), similar to PairRE, which helps maintain training stability by fixing L2 norms.\n        *   Crucially, it then introduces an *additional relation-specific translation term* (`Br`) between the mapped head and tail entities. This `Br` provides an extra degree of freedom, allowing more flexible optimization by relaxing the rigid \"arc path\" constraint of hyper-ellipsoidal surfaces.\n        *   The score function is defined as `fr(eh, et) = ||GH_r(eh) + Br - GT_r(et)||1` \\cite{li2022}.\n        *   It employs a component-independent initialization search strategy for relation embeddings (R), entity embeddings (E), and translations (Br) to find optimal initial distributions, which is shown to improve performance.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A novel score function for KGE, `TranSHER`, that combines hyper-ellipsoidal entity restriction with relation-specific translational components to enhance flexibility and optimization \\cite{li2022}.\n        *   The introduction of a relation-specific translation vector (`Br`) that acts as an \"extra degree of freedom\" to ease the strict hyper-ellipsoidal constraint, enabling better distribution learning for entities involved in complex relations \\cite{li2022}.\n        *   A component-independent initialization strategy for embeddings (R, E, Br) that empirically leads to better results by allowing tailored initial distributions for each component \\cite{li2022}.\n    *   **Theoretical Insights**: `TranSHER` \\cite{li2022} is proven to maintain the ability to model important relation patterns (symmetry/antisymmetry, inversion, and composition) under specific constraints, demonstrating its theoretical soundness while introducing flexibility.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive link prediction experiments were performed on five diverse datasets under both full ranking and partial ranking settings. Analytical experiments were also conducted to study the behavior of translations and case studies demonstrated superiority \\cite{li2022}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Datasets**: FB15k-237, DB100K, YAGO37 (full ranking); ogbl-wikikg2, ogbl-biokg (partial ranking) \\cite{li2022}.\n        *   **Metrics**: Mean Reciprocal Rank (MRR) as the primary metric, and Hits@N (HIT@1, HIT@3, HIT@10) as auxiliary metrics \\cite{li2022}.\n        *   **Baselines**: DistMult, ComplEx, SEEK (semantic matching); TransE, RotatE, PairRE (distance-based), with PairRE being the main baseline \\cite{li2022}.\n        *   **Results**: `TranSHER` \\cite{li2022} achieved significant performance improvements in MRR across all five datasets compared to strong baselines, including PairRE and SEEK. Notably, it showed an MRR increase of up to 4.6% on YAGO37 and 3.2% on ogbl-wikikg2. Case studies highlighted its ability to better model semantic characteristics and improve entity retrieval for complex relations (e.g., distinguishing film producers from companies).\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on `TranSHER`'s strengths in overcoming previous limitations rather than explicitly stating its own. However, its ability to model relation patterns is \"under certain constraints\" \\cite{li2022}, implying these patterns are not universally captured without specific parameter conditions. The reliance on an initialization search strategy might add complexity to hyperparameter tuning.\n    *   **Scope of Applicability**: `TranSHER` \\cite{li2022} is designed for knowledge graph completion (link prediction) tasks and demonstrates generalization across datasets from different domains and scales, including general knowledge (FB15k-237, YAGO37, DB100K, ogbl-wikikg2) and biomedical facts (ogbl-biokg).\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: `TranSHER` \\cite{li2022} significantly advances the technical state-of-the-art in KGE by providing a more flexible and effective score function. It successfully addresses the rigid entity distribution constraints of previous leading models like PairRE, leading to substantial performance gains, especially for complex relations.\n    *   **Potential Impact on Future Research**: The core idea of combining fixed-norm restrictions (for stability) with translational freedom (for flexibility) offers a promising direction for designing future KGE models. Its enhanced ability to capture semantic characteristics and model diverse relation patterns could inspire research into more robust, interpretable, and generalizable knowledge graph representations.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) is fundamental for tasks like link prediction, yet existing models often struggle to balance expressive power with representational flexibility, especially for complex relations. Methods like PairRE, while innovative, impose strict hyper-ellipsoidal restrictions on entity distributions, limiting optimization and potentially entangling close embeddings.\n\nWe introduce **TranSHER**, a novel translational distance-based KGE model that fundamentally redefines entity representation. TranSHER maintains the stability of hyper-ellipsoidal entity mapping but critically integrates a *relation-specific translation vector* between mapped head and tail entities. This innovative \"extra degree of freedom\" relaxes the rigid arc-path constraint, enabling far more flexible optimization and superior learning of true semantic connections. Coupled with a component-independent initialization strategy, TranSHER offers unprecedented expressive power.\n\nExtensive link prediction experiments on five diverse datasets, including FB15k-237 and ogbl-wikikg2, demonstrate TranSHER's significant advancements. It achieves substantial improvements in Mean Reciprocal Rank (MRR) over state-of-the-art baselines, including PairRE and SEEK, with gains up to 4.6%. TranSHER not only models complex relation patterns more effectively but also sets a new benchmark for flexible and robust knowledge graph completion, paving the way for more adaptable KGE architectures.",
      "keywords": [
        "TranSHER",
        "Knowledge Graph Embedding (KGE)",
        "Link Prediction",
        "Hyper-ellipsoidal Restriction",
        "Translational Distance-based Score Function",
        "Relation-specific Translation Vector",
        "Complex Relations Modeling",
        "Entity Distribution Flexibility",
        "Component-independent Initialization",
        "Performance Improvement",
        "Knowledge Graph Completion",
        "Semantic Characteristics"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/7e5f318bf5b9c986ca82d2d97e11f50d58ee6680.pdf",
      "citation_key": "li2022",
      "metadata": {
        "title": "TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction",
        "authors": [
          "Yizhi Li",
          "Wei Fan",
          "Chaochun Liu",
          "Chenghua Lin",
          "Jiang Qian"
        ],
        "published_date": "2022",
        "abstract": "Knowledge graph embedding methods are important for the knowledge graph completion (or link prediction) task.One state-of-the-art method, PairRE, leverages two separate vectors to model complex relations (i.e., 1-to-N, N-to-1, and N-to-N) in knowledge graphs. However, such a method strictly restricts entities on the hyper-ellipsoid surfaces which limits the optimization of entity distribution, leading to suboptimal performance of knowledge graph completion. To address this issue, we propose a novel score function TranSHER, which leverages relation-specific translations between head and tail entities to relax the constraint of hyper-ellipsoid restrictions. By introducing an intuitive and simple relation-specific translation, TranSHER can provide more direct guidance on optimization and capture more semantic characteristics of entities with complex relations. Experimental results show that TranSHER achieves state-of-the-art performance on link prediction and generalizes well to datasets in different domains and scales. Our codes are public available athttps://github.com/yizhilll/TranSHER.",
        "file_path": "paper_data/knowledge_graph_embedding/7e5f318bf5b9c986ca82d2d97e11f50d58ee6680.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction\" for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing knowledge graph embedding (KGE) methods, particularly PairRE, suffer from suboptimal performance in knowledge graph completion (link prediction) due to strict restrictions on entity distribution. PairRE forces entities onto hyper-ellipsoid surfaces, limiting optimization flexibility and potentially entangling close entity embeddings, especially for complex relations (1-to-N, N-to-1, N-to-N).\n    *   **Importance and Challenge**: Knowledge graphs are often incomplete, making link prediction a crucial task. Effectively modeling complex relations and ensuring flexible entity representation are challenging, as rigid constraints can hinder the learning of true semantic connections and lead to suboptimal embeddings.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `TranSHER` \\cite{li2022} builds upon distance-based KGE models (e.g., TransE, RotatE) and directly addresses the limitations of the recently proposed PairRE. It also implicitly competes with semantic matching models (e.g., DistMult, ComplEx, SEEK).\n    *   **Limitations of Previous Solutions**:\n        *   **PairRE**: While effective for complex relations, it strictly restricts entity embeddings to hyper-ellipsoidal surfaces, limiting their optimization path to \"arc paths\" and potentially entangling close entities, leading to suboptimal performance \\cite{li2022}.\n        *   **TransE and its extensions (TransH, TransR, ManifoldE)**: Claimed to be weak in modeling certain relation patterns (as argued by RotatE) \\cite{li2022}.\n        *   **Semantic Matching Models (e.g., RESCAL, DistMult, ComplEx, SEEK)**: Often struggle to distinguish similar entities and lack the ability to simultaneously model multiple relation patterns \\cite{li2022}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: `TranSHER` \\cite{li2022} proposes a novel translational distance-based score function that leverages relation-specific translations to relax the hyper-ellipsoidal restriction imposed by methods like PairRE.\n    *   **Novelty/Difference**:\n        *   It first maps entity vectors to hyper-ellipsoids using relation-specific mapping functions (`GH_r(eh)` and `GT_r(et)`), similar to PairRE, which helps maintain training stability by fixing L2 norms.\n        *   Crucially, it then introduces an *additional relation-specific translation term* (`Br`) between the mapped head and tail entities. This `Br` provides an extra degree of freedom, allowing more flexible optimization by relaxing the rigid \"arc path\" constraint of hyper-ellipsoidal surfaces.\n        *   The score function is defined as `fr(eh, et) = ||GH_r(eh) + Br - GT_r(et)||1` \\cite{li2022}.\n        *   It employs a component-independent initialization search strategy for relation embeddings (R), entity embeddings (E), and translations (Br) to find optimal initial distributions, which is shown to improve performance.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A novel score function for KGE, `TranSHER`, that combines hyper-ellipsoidal entity restriction with relation-specific translational components to enhance flexibility and optimization \\cite{li2022}.\n        *   The introduction of a relation-specific translation vector (`Br`) that acts as an \"extra degree of freedom\" to ease the strict hyper-ellipsoidal constraint, enabling better distribution learning for entities involved in complex relations \\cite{li2022}.\n        *   A component-independent initialization strategy for embeddings (R, E, Br) that empirically leads to better results by allowing tailored initial distributions for each component \\cite{li2022}.\n    *   **Theoretical Insights**: `TranSHER` \\cite{li2022} is proven to maintain the ability to model important relation patterns (symmetry/antisymmetry, inversion, and composition) under specific constraints, demonstrating its theoretical soundness while introducing flexibility.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive link prediction experiments were performed on five diverse datasets under both full ranking and partial ranking settings. Analytical experiments were also conducted to study the behavior of translations and case studies demonstrated superiority \\cite{li2022}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Datasets**: FB15k-237, DB100K, YAGO37 (full ranking); ogbl-wikikg2, ogbl-biokg (partial ranking) \\cite{li2022}.\n        *   **Metrics**: Mean Reciprocal Rank (MRR) as the primary metric, and Hits@N (HIT@1, HIT@3, HIT@10) as auxiliary metrics \\cite{li2022}.\n        *   **Baselines**: DistMult, ComplEx, SEEK (semantic matching); TransE, RotatE, PairRE (distance-based), with PairRE being the main baseline \\cite{li2022}.\n        *   **Results**: `TranSHER` \\cite{li2022} achieved significant performance improvements in MRR across all five datasets compared to strong baselines, including PairRE and SEEK. Notably, it showed an MRR increase of up to 4.6% on YAGO37 and 3.2% on ogbl-wikikg2. Case studies highlighted its ability to better model semantic characteristics and improve entity retrieval for complex relations (e.g., distinguishing film producers from companies).\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on `TranSHER`'s strengths in overcoming previous limitations rather than explicitly stating its own. However, its ability to model relation patterns is \"under certain constraints\" \\cite{li2022}, implying these patterns are not universally captured without specific parameter conditions. The reliance on an initialization search strategy might add complexity to hyperparameter tuning.\n    *   **Scope of Applicability**: `TranSHER` \\cite{li2022} is designed for knowledge graph completion (link prediction) tasks and demonstrates generalization across datasets from different domains and scales, including general knowledge (FB15k-237, YAGO37, DB100K, ogbl-wikikg2) and biomedical facts (ogbl-biokg).\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: `TranSHER` \\cite{li2022} significantly advances the technical state-of-the-art in KGE by providing a more flexible and effective score function. It successfully addresses the rigid entity distribution constraints of previous leading models like PairRE, leading to substantial performance gains, especially for complex relations.\n    *   **Potential Impact on Future Research**: The core idea of combining fixed-norm restrictions (for stability) with translational freedom (for flexibility) offers a promising direction for designing future KGE models. Its enhanced ability to capture semantic characteristics and model diverse relation patterns could inspire research into more robust, interpretable, and generalizable knowledge graph representations.",
        "keywords": [
          "TranSHER",
          "Knowledge Graph Embedding (KGE)",
          "Link Prediction",
          "Hyper-ellipsoidal Restriction",
          "Translational Distance-based Score Function",
          "Relation-specific Translation Vector",
          "Complex Relations Modeling",
          "Entity Distribution Flexibility",
          "Component-independent Initialization",
          "Performance Improvement",
          "Knowledge Graph Completion",
          "Semantic Characteristics"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "7e5f318bf5b9c986ca82d2d97e11f50d58ee6680.pdf"
    },
    {
      "success": true,
      "doc_id": "e59e93b4f30f4cd9e7579304a04c1ceb",
      "summary": "Here is a focused summary of the survey paper \"Knowledge Graph Embedding for Link Prediction: A Comparative Analysis\" by Rossi et al. for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey \\cite{rossi2020} provides a comprehensive comparative analysis of Knowledge Graph Embedding (KGE) methods for Link Prediction (LP), a task crucial for addressing Knowledge Graph (KG) incompleteness.\n    *   Its primary objectives are to evaluate the effectiveness and efficiency of state-of-the-art KGE models, extend analysis dimensions beyond common practices, and understand the impact of various design choices on model performance.\n\n2.  **Literature Coverage**\n    *   The paper \\cite{rossi2020} reviews 16 state-of-the-art KGE models, alongside a rule-based baseline, focusing on recent advancements in the field.\n    *   Models were selected for their state-of-the-art performance, diverse architectural approaches, and public availability, and were experimentally compared on five widely used benchmark datasets.\n\n3.  **Classification Framework**\n    *   The survey \\cite{rossi2020} introduces a novel, educational taxonomy to organize LP models based on latent features.\n    *   It categorizes models into three main families: Tensor Decomposition Models, Geometric Models, and Deep Learning Models.\n    *   These families are further subdivided, for instance, Tensor Decomposition Models include Bilinear and Non-bilinear approaches.\n\n4.  **Key Findings & Insights**\n    *   The analysis \\cite{rossi2020} provides detailed quantitative results on the effectiveness and efficiency of 16 diverse models across multiple datasets.\n    *   It reveals how specific structural features within the training data significantly influence the predictive performance of each model on individual test facts.\n    *   The paper implicitly highlights that current standard evaluation practices, which often over-represent certain entities, can obscure a model's true generalization capabilities.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey \\cite{rossi2020} identifies significant gaps in understanding the impact of various design choices in KGE methods and the specific circumstances that lead to better model performance.\n    *   It emphasizes that the strengths, weaknesses, and limitations of current techniques, particularly what makes certain facts easier or harder to predict, remain largely unknown, guiding future research towards more granular analysis.\n\n6.  **Survey Contribution**\n    *   This survey \\cite{rossi2020} offers a comprehensive and authoritative meta-analysis of KGE-based LP methods, distinguishing itself by proposing new and informative evaluation practices.\n    *   It provides a unique educational taxonomy and detailed experimental comparison of 16 state-of-the-art models, offering valuable insights into their performance characteristics and underlying structural influences.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graphs (KGs) hinges on effectively addressing their inherent incompleteness, a task critically dependent on robust Knowledge Graph Embedding (KGE) for Link Prediction (LP). This seminal survey offers an unprecedented, comprehensive comparative analysis of 16 state-of-the-art KGE models, meticulously organized by a novel, educational taxonomy spanning Tensor Decomposition, Geometric, and Deep Learning approaches. Beyond detailed quantitative performance metrics across five benchmark datasets, we reveal how specific structural features within training data profoundly dictate predictive success on individual facts. Crucially, our analysis exposes how prevailing standard evaluation practices often obscure a model's true generalization capabilities, over-representing certain entities and masking critical limitations. This authoritative meta-analysis not only provides a definitive guide to the current KGE landscape but also identifies significant gaps, emphasizing the urgent need for more granular understanding of design choices and what makes certain links inherently harder or easier to predict. Our work redefines KGE evaluation, paving the way for more robust, generalizable, and impactful Knowledge Graph completion strategies.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Link Prediction (LP)",
        "Knowledge Graph incompleteness",
        "comparative analysis",
        "state-of-the-art KGE models",
        "novel educational taxonomy",
        "Tensor Decomposition Models",
        "Geometric Models",
        "Deep Learning Models",
        "model performance evaluation",
        "impact of design choices",
        "structural features influence",
        "generalization capabilities",
        "new evaluation practices",
        "research gaps"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/8c93f3cecf79bd9f8d021f589d095305e281dd2f.pdf",
      "citation_key": "rossi2020",
      "metadata": {
        "title": "Knowledge Graph Embedding for Link Prediction",
        "authors": [
          "Andrea Rossi",
          "D. Firmani",
          "Antonio Matinata",
          "P. Merialdo",
          "Denilson Barbosa"
        ],
        "published_date": "2020",
        "abstract": "Knowledge Graphs (KGs) have found many applications in industrial and in academic settings, which in turn, have motivated considerable research efforts towards large-scale information extraction from a variety of sources. Despite such efforts, it is well known that even the largest KGs suffer from incompleteness; Link Prediction (LP) techniques address this issue by identifying missing facts among entities already in the KG. Among the recent LP techniques, those based on KG embeddings have achieved very promising performance in some benchmarks. Despite the fast-growing literature on the subject, insufficient attention has been paid to the effect of the design choices in those methods. Moreover, the standard practice in this area is to report accuracy by aggregating over a large number of test facts in which some entities are vastly more represented than others; this allows LP methods to exhibit good results by just attending to structural properties that include such entities, while ignoring the remaining majority of the KG. This analysis provides a comprehensive comparison of embedding-based LP methods, extending the dimensions of analysis beyond what is commonly available in the literature. We experimentally compare the effectiveness and efficiency of 18 state-of-the-art methods, consider a rule-based baseline, and report detailed analysis over the most popular benchmarks in the literature.",
        "file_path": "paper_data/knowledge_graph_embedding/8c93f3cecf79bd9f8d021f589d095305e281dd2f.pdf",
        "venue": "ACM Transactions on Knowledge Discovery from Data",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper \"Knowledge Graph Embedding for Link Prediction: A Comparative Analysis\" by Rossi et al. for literature review:\n\n1.  **Review Scope & Objectives**\n    *   This survey \\cite{rossi2020} provides a comprehensive comparative analysis of Knowledge Graph Embedding (KGE) methods for Link Prediction (LP), a task crucial for addressing Knowledge Graph (KG) incompleteness.\n    *   Its primary objectives are to evaluate the effectiveness and efficiency of state-of-the-art KGE models, extend analysis dimensions beyond common practices, and understand the impact of various design choices on model performance.\n\n2.  **Literature Coverage**\n    *   The paper \\cite{rossi2020} reviews 16 state-of-the-art KGE models, alongside a rule-based baseline, focusing on recent advancements in the field.\n    *   Models were selected for their state-of-the-art performance, diverse architectural approaches, and public availability, and were experimentally compared on five widely used benchmark datasets.\n\n3.  **Classification Framework**\n    *   The survey \\cite{rossi2020} introduces a novel, educational taxonomy to organize LP models based on latent features.\n    *   It categorizes models into three main families: Tensor Decomposition Models, Geometric Models, and Deep Learning Models.\n    *   These families are further subdivided, for instance, Tensor Decomposition Models include Bilinear and Non-bilinear approaches.\n\n4.  **Key Findings & Insights**\n    *   The analysis \\cite{rossi2020} provides detailed quantitative results on the effectiveness and efficiency of 16 diverse models across multiple datasets.\n    *   It reveals how specific structural features within the training data significantly influence the predictive performance of each model on individual test facts.\n    *   The paper implicitly highlights that current standard evaluation practices, which often over-represent certain entities, can obscure a model's true generalization capabilities.\n\n5.  **Research Gaps & Future Directions**\n    *   The survey \\cite{rossi2020} identifies significant gaps in understanding the impact of various design choices in KGE methods and the specific circumstances that lead to better model performance.\n    *   It emphasizes that the strengths, weaknesses, and limitations of current techniques, particularly what makes certain facts easier or harder to predict, remain largely unknown, guiding future research towards more granular analysis.\n\n6.  **Survey Contribution**\n    *   This survey \\cite{rossi2020} offers a comprehensive and authoritative meta-analysis of KGE-based LP methods, distinguishing itself by proposing new and informative evaluation practices.\n    *   It provides a unique educational taxonomy and detailed experimental comparison of 16 state-of-the-art models, offering valuable insights into their performance characteristics and underlying structural influences.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Link Prediction (LP)",
          "Knowledge Graph incompleteness",
          "comparative analysis",
          "state-of-the-art KGE models",
          "novel educational taxonomy",
          "Tensor Decomposition Models",
          "Geometric Models",
          "Deep Learning Models",
          "model performance evaluation",
          "impact of design choices",
          "structural features influence",
          "generalization capabilities",
          "new evaluation practices",
          "research gaps"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "8c93f3cecf79bd9f8d021f589d095305e281dd2f.pdf"
    },
    {
      "success": true,
      "doc_id": "3200fa5458de48f2b0ea2ff359a24347",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Temporal Knowledge Graph Embedding (TKGE) models struggle with effectively fusing temporal information, often leading to entity information evolution and suboptimal link prediction performance. They also lack the ability to simultaneously model important relation patterns and provide interpretability.\n    *   **Importance & Challenge**: TKGE models are crucial for inferring missing facts and facilitating reasoning in temporal knowledge graph systems. The challenge lies in developing models that can accurately capture temporal dynamics without distorting entity representations, while also providing insights into relation patterns and offering interpretability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the foundation of TKGE models.\n    *   **Limitations of Previous Solutions**: Previous TKGE methods primarily fuse temporal information directly into entities, which can cause entity representations to evolve undesirably. They also generally lack mechanisms to model diverse relation patterns effectively and provide interpretability, hindering their overall effectiveness and application scope.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **TeAST (Temporal knowledge graph embeddings via Archimedean Spiral Timeline)**. This model maps relations onto a unique Archimedean spiral timeline and transforms the quadruple completion problem into a 3rd-order tensor completion problem.\n    *   **Novelty/Difference**:\n        *   **Archimedean Spiral Timeline**: This novel concept ensures that relations occurring simultaneously are placed on the same timeline, and all relations evolve over time in a structured manner, avoiding direct entity evolution.\n        *   **3rd-order Tensor Completion**: Reconceptualizes the problem, potentially offering a more robust way to handle temporal relations.\n        *   **Temporal Spiral Regularizer**: Introduces a specific regularizer to maintain the orderly structure of the spiral timeline.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of TeAST, a new TKGE model that leverages an Archimedean spiral timeline for relation embedding.\n    *   **System Design/Architectural Innovations**: The core innovation lies in the design of the Archimedean spiral timeline for relation mapping, which inherently handles temporal evolution and simultaneity.\n    *   **Novel Techniques**: A novel temporal spiral regularizer is proposed to ensure the temporal orderliness of the spiral timeline.\n    *   **Theoretical Insights/Analysis**: Mathematical proofs are provided to demonstrate TeAST's capability to encode various relation patterns, addressing a key limitation of prior work.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper states that experimental results were obtained.\n    *   **Key Performance Metrics & Comparison Results**: The proposed TeAST model \"significantly outperforms existing TKGE methods,\" indicating superior performance on relevant metrics (likely link prediction accuracy, e.g., MRR, Hits@N).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper does not explicitly state limitations of TeAST itself within the provided text. However, its design assumes that mapping relations onto an Archimedean spiral timeline and using a 3rd-order tensor completion approach is an effective strategy for temporal knowledge graph embedding.\n    *   **Scope of Applicability**: TeAST is specifically designed for Temporal Knowledge Graph Embedding, focusing on link prediction, relation pattern modeling, and interpretability in temporal knowledge graph based systems.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: TeAST advances the state-of-the-art in TKGE by offering a novel approach to integrate temporal information without causing entity evolution, thereby improving link prediction performance.\n    *   **Potential Impact on Future Research**: The introduction of the Archimedean spiral timeline and the temporal spiral regularizer provides a new paradigm for modeling temporal dynamics in knowledge graphs. Its proven ability to encode various relation patterns and potential for interpretability could inspire future research into more sophisticated temporal modeling techniques and explainable AI in knowledge graphs.",
      "intriguing_abstract": "Temporal Knowledge Graph Embedding (TKGE) models are crucial for inferring missing facts, yet current approaches struggle with effectively integrating temporal information, often leading to undesirable entity evolution and suboptimal link prediction. We introduce **TeAST (Temporal knowledge graph embeddings via Archimedean Spiral Timeline)**, a novel model that fundamentally rethinks temporal dynamics. Instead of evolving entities, TeAST uniquely maps *relations* onto an Archimedean spiral timeline, ensuring simultaneous events are co-located and temporal order is preserved without distorting entity representations. This innovative approach transforms the quadruple completion problem into a robust **3rd-order tensor completion** task, further enhanced by a novel **temporal spiral regularizer**. TeAST is mathematically proven to encode diverse and complex **relation patterns**, a significant advancement over prior work, and offers unprecedented **interpretability**. Our experiments demonstrate TeAST significantly outperforms existing TKGE methods, setting a new state-of-the-art in **link prediction**. This paradigm-shifting model offers a powerful, interpretable framework for understanding and predicting evolving knowledge, paving the way for more accurate and explainable temporal reasoning systems.",
      "keywords": [
        "Temporal Knowledge Graph Embedding (TKGE)",
        "TeAST model",
        "Archimedean spiral timeline",
        "3rd-order tensor completion",
        "temporal spiral regularizer",
        "relation patterns modeling",
        "link prediction performance",
        "entity information evolution",
        "interpretability",
        "temporal dynamics",
        "novel temporal modeling",
        "state-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/cab5194d13c1ce89a96322adaac754b2cb630d87.pdf",
      "citation_key": "li2023",
      "metadata": {
        "title": "TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline",
        "authors": [
          "Jiang Li",
          "Xiangdong Su",
          "Guanglai Gao"
        ],
        "published_date": "2023",
        "abstract": "Temporal knowledge graph embedding (TKGE) models are commonly utilized to infer the missing facts and facilitate reasoning and decision-making in temporal knowledge graph based systems. However, existing methods fuse temporal information into entities, potentially leading to the evolution of entity information and limiting the link prediction performance of TKG. Meanwhile, current TKGE models often lack the ability to simultaneously model important relation patterns and provide interpretability, which hinders their effectiveness and potential applications. To address these limitations, we propose a novel TKGE model which encodes Temporal knowledge graph embeddings via Archimedean Spiral Timeline (TeAST), which maps relations onto the corresponding Archimedean spiral timeline and transforms the quadruples completion to 3th-order tensor completion problem. Specifically, the Archimedean spiral timeline ensures that relations that occur simultaneously are placed on the same timeline, and all relations evolve over time. Meanwhile, we present a novel temporal spiral regularizer to make the spiral timeline orderly. In addition, we provide mathematical proofs to demonstrate the ability of TeAST to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing TKGE methods. Our code is available at https://github.com/IMU-MachineLearningSXD/TeAST.",
        "file_path": "paper_data/knowledge_graph_embedding/cab5194d13c1ce89a96322adaac754b2cb630d87.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Temporal Knowledge Graph Embedding (TKGE) models struggle with effectively fusing temporal information, often leading to entity information evolution and suboptimal link prediction performance. They also lack the ability to simultaneously model important relation patterns and provide interpretability.\n    *   **Importance & Challenge**: TKGE models are crucial for inferring missing facts and facilitating reasoning in temporal knowledge graph systems. The challenge lies in developing models that can accurately capture temporal dynamics without distorting entity representations, while also providing insights into relation patterns and offering interpretability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the foundation of TKGE models.\n    *   **Limitations of Previous Solutions**: Previous TKGE methods primarily fuse temporal information directly into entities, which can cause entity representations to evolve undesirably. They also generally lack mechanisms to model diverse relation patterns effectively and provide interpretability, hindering their overall effectiveness and application scope.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **TeAST (Temporal knowledge graph embeddings via Archimedean Spiral Timeline)**. This model maps relations onto a unique Archimedean spiral timeline and transforms the quadruple completion problem into a 3rd-order tensor completion problem.\n    *   **Novelty/Difference**:\n        *   **Archimedean Spiral Timeline**: This novel concept ensures that relations occurring simultaneously are placed on the same timeline, and all relations evolve over time in a structured manner, avoiding direct entity evolution.\n        *   **3rd-order Tensor Completion**: Reconceptualizes the problem, potentially offering a more robust way to handle temporal relations.\n        *   **Temporal Spiral Regularizer**: Introduces a specific regularizer to maintain the orderly structure of the spiral timeline.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of TeAST, a new TKGE model that leverages an Archimedean spiral timeline for relation embedding.\n    *   **System Design/Architectural Innovations**: The core innovation lies in the design of the Archimedean spiral timeline for relation mapping, which inherently handles temporal evolution and simultaneity.\n    *   **Novel Techniques**: A novel temporal spiral regularizer is proposed to ensure the temporal orderliness of the spiral timeline.\n    *   **Theoretical Insights/Analysis**: Mathematical proofs are provided to demonstrate TeAST's capability to encode various relation patterns, addressing a key limitation of prior work.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper states that experimental results were obtained.\n    *   **Key Performance Metrics & Comparison Results**: The proposed TeAST model \"significantly outperforms existing TKGE methods,\" indicating superior performance on relevant metrics (likely link prediction accuracy, e.g., MRR, Hits@N).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper does not explicitly state limitations of TeAST itself within the provided text. However, its design assumes that mapping relations onto an Archimedean spiral timeline and using a 3rd-order tensor completion approach is an effective strategy for temporal knowledge graph embedding.\n    *   **Scope of Applicability**: TeAST is specifically designed for Temporal Knowledge Graph Embedding, focusing on link prediction, relation pattern modeling, and interpretability in temporal knowledge graph based systems.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: TeAST advances the state-of-the-art in TKGE by offering a novel approach to integrate temporal information without causing entity evolution, thereby improving link prediction performance.\n    *   **Potential Impact on Future Research**: The introduction of the Archimedean spiral timeline and the temporal spiral regularizer provides a new paradigm for modeling temporal dynamics in knowledge graphs. Its proven ability to encode various relation patterns and potential for interpretability could inspire future research into more sophisticated temporal modeling techniques and explainable AI in knowledge graphs.",
        "keywords": [
          "Temporal Knowledge Graph Embedding (TKGE)",
          "TeAST model",
          "Archimedean spiral timeline",
          "3rd-order tensor completion",
          "temporal spiral regularizer",
          "relation patterns modeling",
          "link prediction performance",
          "entity information evolution",
          "interpretability",
          "temporal dynamics",
          "novel temporal modeling",
          "state-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "cab5194d13c1ce89a96322adaac754b2cb630d87.pdf"
    },
    {
      "success": true,
      "doc_id": "91454705099bc1834e657be1e8d30b4b",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction \\cite{peng2020}\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: The task of link prediction in knowledge graphs (KGs) aims to infer missing relationships between entities. KGs often suffer from incompleteness.\n    *   **Motivation**: Traditional symbolic representation algorithms for KGs have high computational complexity and lack scalability. Knowledge graph embedding (KGE) models address this by representing entities and relations as low-dimensional vectors. The effectiveness of KGE models is significantly enhanced if they can comprehensively capture diverse connectivity patterns (e.g., symmetry, antisymmetry, inversion, composition) and mapping properties (e.g., one-to-one, one-to-many, many-to-one, many-to-many) of relations, which many existing models struggle with.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: KGE models are broadly categorized into translational distance models (e.g., TransE, TransH, TransR, TransD, RotatE) and semantic matching models (e.g., DistMult, ComplEx, ConvE).\n    *   **Limitations of Previous Solutions**:\n        *   **Translational Models**: TransE struggles with symmetric relations and complex mapping properties (1-to-N, N-to-1, N-to-N). Its variants (TransH, TransR, TransD) improve on complex mapping but often fail to model inversion and composition patterns. RotatE can model all connectivity patterns but does not address complex mapping properties.\n        *   **Semantic Matching Models**: DistMult can only handle symmetric relations. ComplEx addresses antisymmetry but not composition, and increases complexity with complex-valued embeddings. RESCAL is prone to overfitting and not scalable. Neural network-based models like ConvE offer good performance but can be more complex.\n    *   **Positioning**: \\cite{peng2020} proposes LineaRE as a simple, scalable model that uniquely combines the ability to model *all* four connectivity patterns and *all* four mapping properties, outperforming existing models that typically specialize in one aspect while sacrificing others.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{peng2020} proposes LineaRE (Linear Regression Embedding), which interprets knowledge graph embedding as a simple linear regression task. For a given triplet `(h, r, t)`, the model expects the equation `w1_r * h + br = w2_r * t` to hold, where `h` and `t` are low-dimensional real-valued entity vectors, and `w1_r`, `w2_r`, and `br` are relation-specific real-valued weight and bias vectors. The `*` denotes the Hadamard (element-wise) product.\n    *   **Scoring Function**: The plausibility of a triplet `(h, r, t)` is measured by `f_r(h,t) = ||w1_r * h + br - w2_r * t||_1`, where a lower score indicates higher plausibility.\n    *   **Innovation**:\n        *   **Comprehensive Modeling**: The core innovation lies in demonstrating that this simple linear regression framework can mathematically model all four connectivity patterns (symmetry, antisymmetry, inversion, composition) and all four complex mapping properties (1-to-1, 1-to-N, N-to-1, N-to-N).\n        *   **Simplicity and Scalability**: By defining all vectors in a real number space and using a linear scoring function, LineaRE is inherently simple and scalable to large knowledge graphs, contrasting with more complex models involving matrices, projections, or complex numbers.\n        *   **Generalization of TransE**: \\cite{peng2020} formally proves that TransE is a special case of LineaRE where `w1_r = w2_r`.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of LineaRE, a novel knowledge graph embedding model based on a linear regression interpretation of relations.\n    *   **Theoretical Insights**: Formal mathematical proofs demonstrating LineaRE's capability to model all four connectivity patterns (symmetry, antisymmetry, inversion, composition) and all four mapping properties (1-to-1, 1-to-N, N-to-1, N-to-N).\n    *   **Architectural Simplicity**: A simple and scalable model architecture using real-valued vectors and element-wise products, avoiding complex operations or higher-dimensional spaces.\n    *   **Unified Framework**: Provides a unified framework that encompasses and generalizes simpler models like TransE, while addressing their limitations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{peng2020} conducted extensive link prediction experiments.\n    *   **Datasets**: Evaluated on four widely used benchmark datasets: FB15k, WN18, FB15k-237, and WN18RR.\n    *   **Baselines**: Compared against state-of-the-art models including TransE, TransH, TransR, TransD, DistMult, ComplEx, ConvE, and RotatE.\n    *   **Performance Metrics**: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@k (for k=1, 3, 10) in the filtered setting.\n    *   **Key Results**:\n        *   LineaRE consistently and significantly outperformed all baseline models across all datasets and metrics.\n        *   The improvements were particularly notable on more challenging datasets like FB15k-237 and WN18RR.\n        *   **Ablation studies** confirmed the critical importance of all components of the relation representation (`w1_r`, `w2_r`, and `br`), showing significant performance drops when these components were constrained (e.g., `w1_r = w2_r` or `br = 0`).\n        *   **Case studies** provided empirical evidence of LineaRE's ability to correctly handle complex mapping properties (1-to-N, N-to-1, N-to-N) in real-world scenarios.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper does not explicitly state technical limitations. The model assumes that relations can be effectively approximated by linear functions in the embedding space. While powerful, this linearity might have theoretical limits in capturing extremely complex, non-linear semantic interactions if they exist beyond what the model can approximate.\n    *   **Scope of Applicability**: LineaRE is designed for link prediction in knowledge graphs. Its simplicity and scalability make it suitable for large-scale KGs. The model's effectiveness is demonstrated on general-purpose KGs (Freebase, WordNet subsets).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{peng2020} significantly advances the state-of-the-art in knowledge graph embedding for link prediction by introducing a model that is both simple and comprehensively capable of modeling diverse relation patterns and mapping properties.\n    *   **Challenging Complexity**: It demonstrates that high predictive performance and comprehensive modeling capabilities do not necessarily require complex neural architectures or complex-valued embeddings, offering a powerful and efficient alternative.\n    *   **Practical Impact**: The simplicity and scalability of LineaRE make it highly practical for real-world applications involving large and incomplete knowledge graphs, where computational efficiency is crucial.\n    *   **Future Research**: The work opens avenues for further exploration into the expressive power of linear models and their variants for knowledge graph reasoning and completion tasks.",
      "intriguing_abstract": "Knowledge graphs (KGs) are indispensable for AI, yet their inherent incompleteness poses a significant challenge for link prediction. Existing knowledge graph embedding (KGE) models often struggle to comprehensively capture the diverse connectivity patterns (e.g., symmetry, inversion, composition) and complex mapping properties (e.g., one-to-many, many-to-many) crucial for robust inference, frequently sacrificing simplicity for expressiveness.\n\nWe introduce LineaRE, a novel KGE model that surprisingly interprets relation modeling as a simple linear regression task. LineaRE uniquely and mathematically demonstrates the ability to simultaneously model *all four connectivity patterns* and *all four complex mapping properties* using only real-valued vectors and element-wise products, offering unparalleled simplicity and scalability. Our extensive experiments on FB15k, WN18, FB15k-237, and WN18RR show LineaRE consistently and significantly outperforms state-of-the-art translational and semantic matching models across all link prediction metrics. This work challenges the notion that complexity is requisite for performance, providing an efficient, powerful, and theoretically grounded solution for large-scale knowledge graph completion.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Link Prediction",
        "LineaRE",
        "Linear Regression Embedding",
        "Diverse Connectivity Patterns",
        "Complex Mapping Properties",
        "Comprehensive Modeling",
        "Simplicity and Scalability",
        "Generalization of TransE",
        "State-of-the-Art Performance",
        "Experimental Validation",
        "Ablation Studies"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/95c3d25b40f963eb248136555bd9b9e35817cc09.pdf",
      "citation_key": "peng2020",
      "metadata": {
        "title": "LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction",
        "authors": [
          "Yanhui Peng",
          "Jing Zhang"
        ],
        "published_date": "2020",
        "abstract": "The task of link prediction for knowledge graphs is to predict missing relationships between entities. Knowledge graph embedding, which aims to represent entities and relations of a knowledge graph as low dimensional vectors in a continuous vector space, has achieved promising predictive performance. If an embedding model can cover different types of connectivity patterns and mapping properties of relations as many as possible, it will potentially bring more benefits for link prediction tasks. In this paper, we propose a novel embedding model, namely LineaRE, which is capable of modeling four connectivity patterns (i.e., symmetry, antisymmetry, inversion, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many) of relations. Specifically, we regard knowledge graph embedding as a simple linear regression task, where a relation is modeled as a linear function of two low-dimensional vector-presented entities with two weight vectors and a bias vector. Since the vectors are defined in a real number space and the scoring function of the model is linear, our model is simple and scalable to large knowledge graphs. Experimental results on multiple widely used real-world datasets show that the proposed LineaRE model significantly outperforms existing state-of-the-art models for link prediction tasks.",
        "file_path": "paper_data/knowledge_graph_embedding/95c3d25b40f963eb248136555bd9b9e35817cc09.pdf",
        "venue": "Industrial Conference on Data Mining",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction \\cite{peng2020}\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: The task of link prediction in knowledge graphs (KGs) aims to infer missing relationships between entities. KGs often suffer from incompleteness.\n    *   **Motivation**: Traditional symbolic representation algorithms for KGs have high computational complexity and lack scalability. Knowledge graph embedding (KGE) models address this by representing entities and relations as low-dimensional vectors. The effectiveness of KGE models is significantly enhanced if they can comprehensively capture diverse connectivity patterns (e.g., symmetry, antisymmetry, inversion, composition) and mapping properties (e.g., one-to-one, one-to-many, many-to-one, many-to-many) of relations, which many existing models struggle with.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: KGE models are broadly categorized into translational distance models (e.g., TransE, TransH, TransR, TransD, RotatE) and semantic matching models (e.g., DistMult, ComplEx, ConvE).\n    *   **Limitations of Previous Solutions**:\n        *   **Translational Models**: TransE struggles with symmetric relations and complex mapping properties (1-to-N, N-to-1, N-to-N). Its variants (TransH, TransR, TransD) improve on complex mapping but often fail to model inversion and composition patterns. RotatE can model all connectivity patterns but does not address complex mapping properties.\n        *   **Semantic Matching Models**: DistMult can only handle symmetric relations. ComplEx addresses antisymmetry but not composition, and increases complexity with complex-valued embeddings. RESCAL is prone to overfitting and not scalable. Neural network-based models like ConvE offer good performance but can be more complex.\n    *   **Positioning**: \\cite{peng2020} proposes LineaRE as a simple, scalable model that uniquely combines the ability to model *all* four connectivity patterns and *all* four mapping properties, outperforming existing models that typically specialize in one aspect while sacrificing others.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{peng2020} proposes LineaRE (Linear Regression Embedding), which interprets knowledge graph embedding as a simple linear regression task. For a given triplet `(h, r, t)`, the model expects the equation `w1_r * h + br = w2_r * t` to hold, where `h` and `t` are low-dimensional real-valued entity vectors, and `w1_r`, `w2_r`, and `br` are relation-specific real-valued weight and bias vectors. The `*` denotes the Hadamard (element-wise) product.\n    *   **Scoring Function**: The plausibility of a triplet `(h, r, t)` is measured by `f_r(h,t) = ||w1_r * h + br - w2_r * t||_1`, where a lower score indicates higher plausibility.\n    *   **Innovation**:\n        *   **Comprehensive Modeling**: The core innovation lies in demonstrating that this simple linear regression framework can mathematically model all four connectivity patterns (symmetry, antisymmetry, inversion, composition) and all four complex mapping properties (1-to-1, 1-to-N, N-to-1, N-to-N).\n        *   **Simplicity and Scalability**: By defining all vectors in a real number space and using a linear scoring function, LineaRE is inherently simple and scalable to large knowledge graphs, contrasting with more complex models involving matrices, projections, or complex numbers.\n        *   **Generalization of TransE**: \\cite{peng2020} formally proves that TransE is a special case of LineaRE where `w1_r = w2_r`.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of LineaRE, a novel knowledge graph embedding model based on a linear regression interpretation of relations.\n    *   **Theoretical Insights**: Formal mathematical proofs demonstrating LineaRE's capability to model all four connectivity patterns (symmetry, antisymmetry, inversion, composition) and all four mapping properties (1-to-1, 1-to-N, N-to-1, N-to-N).\n    *   **Architectural Simplicity**: A simple and scalable model architecture using real-valued vectors and element-wise products, avoiding complex operations or higher-dimensional spaces.\n    *   **Unified Framework**: Provides a unified framework that encompasses and generalizes simpler models like TransE, while addressing their limitations.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{peng2020} conducted extensive link prediction experiments.\n    *   **Datasets**: Evaluated on four widely used benchmark datasets: FB15k, WN18, FB15k-237, and WN18RR.\n    *   **Baselines**: Compared against state-of-the-art models including TransE, TransH, TransR, TransD, DistMult, ComplEx, ConvE, and RotatE.\n    *   **Performance Metrics**: Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits@k (for k=1, 3, 10) in the filtered setting.\n    *   **Key Results**:\n        *   LineaRE consistently and significantly outperformed all baseline models across all datasets and metrics.\n        *   The improvements were particularly notable on more challenging datasets like FB15k-237 and WN18RR.\n        *   **Ablation studies** confirmed the critical importance of all components of the relation representation (`w1_r`, `w2_r`, and `br`), showing significant performance drops when these components were constrained (e.g., `w1_r = w2_r` or `br = 0`).\n        *   **Case studies** provided empirical evidence of LineaRE's ability to correctly handle complex mapping properties (1-to-N, N-to-1, N-to-N) in real-world scenarios.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper does not explicitly state technical limitations. The model assumes that relations can be effectively approximated by linear functions in the embedding space. While powerful, this linearity might have theoretical limits in capturing extremely complex, non-linear semantic interactions if they exist beyond what the model can approximate.\n    *   **Scope of Applicability**: LineaRE is designed for link prediction in knowledge graphs. Its simplicity and scalability make it suitable for large-scale KGs. The model's effectiveness is demonstrated on general-purpose KGs (Freebase, WordNet subsets).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{peng2020} significantly advances the state-of-the-art in knowledge graph embedding for link prediction by introducing a model that is both simple and comprehensively capable of modeling diverse relation patterns and mapping properties.\n    *   **Challenging Complexity**: It demonstrates that high predictive performance and comprehensive modeling capabilities do not necessarily require complex neural architectures or complex-valued embeddings, offering a powerful and efficient alternative.\n    *   **Practical Impact**: The simplicity and scalability of LineaRE make it highly practical for real-world applications involving large and incomplete knowledge graphs, where computational efficiency is crucial.\n    *   **Future Research**: The work opens avenues for further exploration into the expressive power of linear models and their variants for knowledge graph reasoning and completion tasks.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Link Prediction",
          "LineaRE",
          "Linear Regression Embedding",
          "Diverse Connectivity Patterns",
          "Complex Mapping Properties",
          "Comprehensive Modeling",
          "Simplicity and Scalability",
          "Generalization of TransE",
          "State-of-the-Art Performance",
          "Experimental Validation",
          "Ablation Studies"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "95c3d25b40f963eb248136555bd9b9e35817cc09.pdf"
    },
    {
      "success": true,
      "doc_id": "80ed3e94a9e0f861c12f5d8fa81ec43a",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   The paper addresses the challenge of performing multihop query modeling on incomplete fuzzy spatiotemporal knowledge graphs (KGs) \\cite{ji2024}.\n    *   This problem is critical due to the increasing demand for fuzzy spatiotemporal knowledge modeling from the proliferation of uncertain spatiotemporal data.\n    *   Existing embedding-based multihop KG querying approaches often neglect KG uncertainty and spatiotemporal sensitivity, leading to the oversight of crucial fuzzy spatiotemporal information during path reasoning \\cite{ji2024}.\n\n2.  **Related Work & Positioning**\n    *   This work builds upon and aims to improve existing embedding-based multihop KG querying approaches \\cite{ji2024}.\n    *   Previous solutions are limited by their inability to adequately account for KG uncertainty and spatiotemporal sensitivity, which results in the neglect of fuzzy spatiotemporal information during multihop path reasoning \\cite{ji2024}.\n\n3.  **Technical Approach & Innovation**\n    *   The core technical method is an embedding-based multihop query model specifically designed for fuzzy spatiotemporal KGs \\cite{ji2024}.\n    *   **Novelty**:\n        *   It utilizes quaternions to jointly embed spatiotemporal entities, representing relations as rotations from spatiotemporal subjects to objects \\cite{ji2024}.\n        *   Uncertainty is incorporated directly into the model through a bias factor within the scoring function, enabling a relaxation embedding approach \\cite{ji2024}.\n        *   The approach exploits the inherent noncommutative compositional pattern of quaternions to construct more accurate multihop paths within fuzzy spatiotemporal KGs \\cite{ji2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of a quaternion-based embedding model for fuzzy spatiotemporal KGs that jointly embeds entities and models relations as rotations \\cite{ji2024}.\n    *   **Uncertainty Handling**: A novel mechanism to incorporate uncertainty via a bias factor in the scoring function, facilitating relaxation embedding and richer representation learning \\cite{ji2024}.\n    *   **Path Reasoning Enhancement**: Leveraging the noncommutative compositional properties of quaternions to improve the accuracy of multihop path construction and reasoning \\cite{ji2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The model's effectiveness was evaluated through experiments on link prediction and path query answering tasks \\cite{ji2024}.\n    *   **Datasets**: Validation was performed on two distinct fuzzy spatiotemporal KG datasets \\cite{ji2024}.\n    *   **Key Results**: The proposed method significantly outperforms several state-of-the-art baselines across various performance metrics \\cite{ji2024}.\n\n6.  **Limitations & Scope**\n    *   The paper focuses on multihop query modeling within fuzzy spatiotemporal KGs, specifically addressing link prediction and path query answering \\cite{ji2024}.\n    *   The provided abstract does not explicitly detail specific technical limitations or assumptions of the proposed method itself, beyond addressing the limitations of prior work.\n\n7.  **Technical Significance**\n    *   This work advances the technical state-of-the-art by providing a robust embedding-based framework that effectively handles uncertainty and spatiotemporal sensitivity in multihop KG querying \\cite{ji2024}.\n    *   By learning richer representations and constructing more accurate paths through quaternion-based modeling, it significantly improves path reasoning performance on fuzzy spatiotemporal KGs \\cite{ji2024}.\n    *   The approach has the potential to impact future research in uncertain and dynamic knowledge graph reasoning, offering a novel way to integrate complex spatiotemporal and fuzzy information into KG embeddings \\cite{ji2024}.",
      "intriguing_abstract": "The proliferation of uncertain spatiotemporal data demands robust methods for multihop query modeling on incomplete fuzzy spatiotemporal knowledge graphs (KGs). Current embedding-based approaches often neglect KG uncertainty and spatiotemporal sensitivity, leading to critical information loss during path reasoning. We present a novel embedding-based multihop query model that fundamentally redefines reasoning on these complex KGs. Our innovation lies in utilizing *quaternions* to jointly embed spatiotemporal entities, representing relations as dynamic rotations. Crucially, we incorporate uncertainty directly into the model through a bias factor in the scoring function, enabling a powerful *relaxation embedding* approach. By exploiting the inherent *noncommutative compositional pattern* of quaternions, our model constructs significantly more accurate multihop paths. Evaluated on two fuzzy spatiotemporal KG datasets, our method consistently outperforms state-of-the-art baselines in link prediction and path query answering. This work offers a robust, technically advanced framework for integrating complex spatiotemporal and fuzzy information, opening new avenues for uncertain and dynamic KG reasoning.",
      "keywords": [
        "fuzzy spatiotemporal knowledge graphs",
        "multihop query modeling",
        "KG uncertainty",
        "spatiotemporal sensitivity",
        "embedding-based",
        "quaternions",
        "quaternion-based embedding model",
        "uncertainty incorporation",
        "relaxation embedding",
        "noncommutative compositional pattern",
        "path reasoning enhancement",
        "link prediction",
        "path query answering",
        "state-of-the-art performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/12cc4b65644a84a16ef7dfe7bdd70172cd38cffd.pdf",
      "citation_key": "ji2024",
      "metadata": {
        "title": "Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding",
        "authors": [
          "Hao Ji",
          "Li Yan",
          "Z. Ma"
        ],
        "published_date": "2024",
        "abstract": "The proliferation of uncertain spatiotemporal data has led to an increasing demand for fuzzy spatiotemporal knowledge modeling in various applications. However, performing multihop query modeling on incomplete fuzzy spatiotemporal knowledge graphs (KGs) poses significant challenges. Recently, embedding-based multihop KG querying approaches have gained attention. Yet, these approaches often overlook KG uncertainty and spatiotemporal sensitivity, resulting in the neglect of fuzzy spatiotemporal information during multihop path reasoning. To address these challenges, we propose an embedding-based multihop query model for fuzzy spatiotemporal KG. We use quaternion to jointly embed spatiotemporal entities, and relations are represented as rotations from spatiotemporal subject to object. We incorporate uncertainty by the scoring function's bias factor, allowing for relaxation embedding. This approach facilitates the learning of a richer representation of fuzzy spatiotemporal KGs in vector space. By exploiting the inherent noncommutative compositional pattern of quaternions, we construct more accurate multihop paths within fuzzy spatiotemporal KGs, thus improving path reasoning performance. To evaluate the effectiveness of our model, we conduct experiments on two fuzzy spatiotemporal KG datasets, focusing on link prediction and path query answering. Results show that our proposed method significantly outperforms several state-of-the-art baselines in terms of performance metrics.",
        "file_path": "paper_data/knowledge_graph_embedding/12cc4b65644a84a16ef7dfe7bdd70172cd38cffd.pdf",
        "venue": "IEEE transactions on fuzzy systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   The paper addresses the challenge of performing multihop query modeling on incomplete fuzzy spatiotemporal knowledge graphs (KGs) \\cite{ji2024}.\n    *   This problem is critical due to the increasing demand for fuzzy spatiotemporal knowledge modeling from the proliferation of uncertain spatiotemporal data.\n    *   Existing embedding-based multihop KG querying approaches often neglect KG uncertainty and spatiotemporal sensitivity, leading to the oversight of crucial fuzzy spatiotemporal information during path reasoning \\cite{ji2024}.\n\n2.  **Related Work & Positioning**\n    *   This work builds upon and aims to improve existing embedding-based multihop KG querying approaches \\cite{ji2024}.\n    *   Previous solutions are limited by their inability to adequately account for KG uncertainty and spatiotemporal sensitivity, which results in the neglect of fuzzy spatiotemporal information during multihop path reasoning \\cite{ji2024}.\n\n3.  **Technical Approach & Innovation**\n    *   The core technical method is an embedding-based multihop query model specifically designed for fuzzy spatiotemporal KGs \\cite{ji2024}.\n    *   **Novelty**:\n        *   It utilizes quaternions to jointly embed spatiotemporal entities, representing relations as rotations from spatiotemporal subjects to objects \\cite{ji2024}.\n        *   Uncertainty is incorporated directly into the model through a bias factor within the scoring function, enabling a relaxation embedding approach \\cite{ji2024}.\n        *   The approach exploits the inherent noncommutative compositional pattern of quaternions to construct more accurate multihop paths within fuzzy spatiotemporal KGs \\cite{ji2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of a quaternion-based embedding model for fuzzy spatiotemporal KGs that jointly embeds entities and models relations as rotations \\cite{ji2024}.\n    *   **Uncertainty Handling**: A novel mechanism to incorporate uncertainty via a bias factor in the scoring function, facilitating relaxation embedding and richer representation learning \\cite{ji2024}.\n    *   **Path Reasoning Enhancement**: Leveraging the noncommutative compositional properties of quaternions to improve the accuracy of multihop path construction and reasoning \\cite{ji2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The model's effectiveness was evaluated through experiments on link prediction and path query answering tasks \\cite{ji2024}.\n    *   **Datasets**: Validation was performed on two distinct fuzzy spatiotemporal KG datasets \\cite{ji2024}.\n    *   **Key Results**: The proposed method significantly outperforms several state-of-the-art baselines across various performance metrics \\cite{ji2024}.\n\n6.  **Limitations & Scope**\n    *   The paper focuses on multihop query modeling within fuzzy spatiotemporal KGs, specifically addressing link prediction and path query answering \\cite{ji2024}.\n    *   The provided abstract does not explicitly detail specific technical limitations or assumptions of the proposed method itself, beyond addressing the limitations of prior work.\n\n7.  **Technical Significance**\n    *   This work advances the technical state-of-the-art by providing a robust embedding-based framework that effectively handles uncertainty and spatiotemporal sensitivity in multihop KG querying \\cite{ji2024}.\n    *   By learning richer representations and constructing more accurate paths through quaternion-based modeling, it significantly improves path reasoning performance on fuzzy spatiotemporal KGs \\cite{ji2024}.\n    *   The approach has the potential to impact future research in uncertain and dynamic knowledge graph reasoning, offering a novel way to integrate complex spatiotemporal and fuzzy information into KG embeddings \\cite{ji2024}.",
        "keywords": [
          "fuzzy spatiotemporal knowledge graphs",
          "multihop query modeling",
          "KG uncertainty",
          "spatiotemporal sensitivity",
          "embedding-based",
          "quaternions",
          "quaternion-based embedding model",
          "uncertainty incorporation",
          "relaxation embedding",
          "noncommutative compositional pattern",
          "path reasoning enhancement",
          "link prediction",
          "path query answering",
          "state-of-the-art performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "12cc4b65644a84a16ef7dfe7bdd70172cd38cffd.pdf"
    },
    {
      "success": true,
      "doc_id": "b85a2bcf862cb25450abb668f37b1a8b",
      "summary": "Here's a focused summary of the provided technical paper for literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addressing the issue of erroneous triples inevitably injected during Knowledge Graph (KG) construction, which most existing KG embedding algorithms assume to be correct.\n    *   **Importance & Challenge**: Errors in KGs lead to significant performance degradation in downstream applications, making the development of effective error-aware KG embedding urgent and challenging.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself against \"most KG embedding algorithms\" that operate under the assumption of perfect data quality.\n    *   **Limitations of Previous Solutions**: Previous solutions fail to account for errors, leading to performance degradation when KGs contain inaccuracies.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Attributed Error-aware Knowledge Embedding (AEKE), a novel framework that leverages entity attributes to guide KG embedding model learning.\n    *   **Novelty**: AEKE integrates attribute semantics to mitigate the impact of erroneous triples. It designs two triple-level hypergraphs (for KG topology and attributes) and calculates a joint confidence score for each triple. This score is based on self-contradiction, local-global structure consistency, and structure-attribute homogeneity. These confidence scores then adaptively weight aggregation in a multi-view graph learning framework and modify the margin loss in KG embedding, effectively reducing the contribution of potential errors.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework**: Introduction of AEKE, a comprehensive framework for error-aware KG embedding \\cite{zhang2024}.\n    *   **Hypergraph Design**: Design of two distinct triple-level hypergraphs to model both KG topological structures and their associated attribute structures \\cite{zhang2024}.\n    *   **Adaptive Confidence Scoring**: A novel method for jointly calculating triple confidence scores based on internal consistency, structural consistency, and attribute homogeneity \\cite{zhang2024}.\n    *   **Error-Aware Learning Mechanism**: Integration of these confidence scores to adaptively weight aggregation in multi-view graph learning and modify the margin loss, ensuring erroneous triples contribute minimally to KG learning \\cite{zhang2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed to evaluate AEKE's performance against existing methods.\n    *   **Key Performance Metrics & Results**: AEKE demonstrated superior performance, outperforming state-of-the-art KG embedding and error detection algorithms on three real-world KGs \\cite{zhang2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The approach heavily relies on the availability and quality of entity attributes to guide error detection and embedding. Its effectiveness might be reduced in KGs with sparse or low-quality attribute information.\n    *   **Scope of Applicability**: Primarily applicable to KGs where entity attributes are available and can provide meaningful semantic context for error detection.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: AEKE significantly advances the technical state-of-the-art by providing an effective, attribute-aware mechanism to learn robust KG embeddings in the presence of errors, a critical challenge for real-world KG applications \\cite{zhang2024}.\n    *   **Potential Impact**: This work opens new avenues for research in robust KG learning, error detection, and the integration of heterogeneous information (like attributes) to improve KG quality and downstream application performance. It highlights the importance of considering data quality during the embedding process.",
      "intriguing_abstract": "Knowledge Graphs (KGs) are indispensable for AI, yet their real-world utility is severely compromised by pervasive erroneous triples, a critical challenge most KG embedding algorithms fundamentally overlook. We introduce Attributed Error-aware Knowledge Embedding (AEKE), a novel framework designed to learn robust KG embeddings by intelligently mitigating the impact of these inaccuracies. AEKE innovatively leverages rich entity attributes, constructing two distinct triple-level hypergraphs to model both KG topological structures and their associated attribute semantics.\n\nAt its core, AEKE develops a sophisticated joint confidence scoring mechanism, assessing each triple based on self-contradiction, local-global structure consistency, and structure-attribute homogeneity. These dynamic confidence scores then adaptively weight aggregation within a multi-view graph learning framework and modify the margin loss, effectively minimizing the contribution of potential errors during embedding. Extensive experiments demonstrate AEKE's superior performance, significantly outperforming state-of-the-art KG embedding and error detection methods on three real-world KGs. AEKE marks a pivotal advancement, paving the way for more reliable and robust KG applications in the face of inherent data imperfections.",
      "keywords": [
        "Knowledge Graph (KG) embedding",
        "erroneous triples",
        "Attributed Error-aware Knowledge Embedding (AEKE)",
        "entity attributes",
        "triple-level hypergraphs",
        "joint confidence score",
        "multi-view graph learning",
        "error-aware learning mechanism",
        "adaptive weighting",
        "robust KG learning",
        "data quality",
        "state-of-the-art performance",
        "error detection"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/40479fd70115e545d21c01853aad56e6922280ac.pdf",
      "citation_key": "zhang2024",
      "metadata": {
        "title": "Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding",
        "authors": [
          "Qinggang Zhang",
          "Junnan Dong",
          "Qiaoyu Tan",
          "Xiao Huang"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graphs (KGs) can structurally organize large-scale information in the form of triples and significantly support many real-world applications. While most KG embedding algorithms hold the assumption that all triples are correct, considerable errors were inevitably injected during the construction process. It is urgent to develop effective error-aware KG embedding, since errors in KGs would lead to significant performance degradation in downstream applications. To this end, we propose a novel framework named Attributed Error-aware Knowledge Embedding (AEKE). It leverages the semantics contained in entity attributes to guide the KG embedding model learning against the impact of erroneous triples. We design two triple-level hypergraphs to model the topological structures of the KG and its attributes, respectively. The confidence score of each triple is jointly calculated based on self-contradictory within the triple, consistency between local and global structures, and homogeneity between structures and attributes. We leverage confidence scores to adaptively update the weighted aggregation in the multi-view graph learning framework and margin loss in KG embedding, such that potential errors will contribute little to KG learning. Experiments on three real-world KGs demonstrate that AEKE outperforms state-of-the-art KG embedding and error detection algorithms.",
        "file_path": "paper_data/knowledge_graph_embedding/40479fd70115e545d21c01853aad56e6922280ac.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the provided technical paper for literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addressing the issue of erroneous triples inevitably injected during Knowledge Graph (KG) construction, which most existing KG embedding algorithms assume to be correct.\n    *   **Importance & Challenge**: Errors in KGs lead to significant performance degradation in downstream applications, making the development of effective error-aware KG embedding urgent and challenging.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself against \"most KG embedding algorithms\" that operate under the assumption of perfect data quality.\n    *   **Limitations of Previous Solutions**: Previous solutions fail to account for errors, leading to performance degradation when KGs contain inaccuracies.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Attributed Error-aware Knowledge Embedding (AEKE), a novel framework that leverages entity attributes to guide KG embedding model learning.\n    *   **Novelty**: AEKE integrates attribute semantics to mitigate the impact of erroneous triples. It designs two triple-level hypergraphs (for KG topology and attributes) and calculates a joint confidence score for each triple. This score is based on self-contradiction, local-global structure consistency, and structure-attribute homogeneity. These confidence scores then adaptively weight aggregation in a multi-view graph learning framework and modify the margin loss in KG embedding, effectively reducing the contribution of potential errors.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework**: Introduction of AEKE, a comprehensive framework for error-aware KG embedding \\cite{zhang2024}.\n    *   **Hypergraph Design**: Design of two distinct triple-level hypergraphs to model both KG topological structures and their associated attribute structures \\cite{zhang2024}.\n    *   **Adaptive Confidence Scoring**: A novel method for jointly calculating triple confidence scores based on internal consistency, structural consistency, and attribute homogeneity \\cite{zhang2024}.\n    *   **Error-Aware Learning Mechanism**: Integration of these confidence scores to adaptively weight aggregation in multi-view graph learning and modify the margin loss, ensuring erroneous triples contribute minimally to KG learning \\cite{zhang2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed to evaluate AEKE's performance against existing methods.\n    *   **Key Performance Metrics & Results**: AEKE demonstrated superior performance, outperforming state-of-the-art KG embedding and error detection algorithms on three real-world KGs \\cite{zhang2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The approach heavily relies on the availability and quality of entity attributes to guide error detection and embedding. Its effectiveness might be reduced in KGs with sparse or low-quality attribute information.\n    *   **Scope of Applicability**: Primarily applicable to KGs where entity attributes are available and can provide meaningful semantic context for error detection.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: AEKE significantly advances the technical state-of-the-art by providing an effective, attribute-aware mechanism to learn robust KG embeddings in the presence of errors, a critical challenge for real-world KG applications \\cite{zhang2024}.\n    *   **Potential Impact**: This work opens new avenues for research in robust KG learning, error detection, and the integration of heterogeneous information (like attributes) to improve KG quality and downstream application performance. It highlights the importance of considering data quality during the embedding process.",
        "keywords": [
          "Knowledge Graph (KG) embedding",
          "erroneous triples",
          "Attributed Error-aware Knowledge Embedding (AEKE)",
          "entity attributes",
          "triple-level hypergraphs",
          "joint confidence score",
          "multi-view graph learning",
          "error-aware learning mechanism",
          "adaptive weighting",
          "robust KG learning",
          "data quality",
          "state-of-the-art performance",
          "error detection"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "40479fd70115e545d21c01853aad56e6922280ac.pdf"
    },
    {
      "success": true,
      "doc_id": "af90027fe08478d9f2b8a2ceb66d3f6f",
      "summary": "Here is a focused summary of the empirical study for literature review:\n\n1.  **Research Questions & Hypotheses**\n    This empirical study investigates the benefits and drawbacks of various parallelization techniques for training Knowledge Graph Embedding (KGE) models on large-scale knowledge graphs \\cite{kochsiek2021}. It implicitly tests hypotheses regarding the impact of these techniques on embedding quality and training efficiency.\n\n2.  **Study Design & Methodology**\n    The study employs an experimental design, re-implementing and investigating existing parallelization techniques within a common computational framework to ensure comparability \\cite{kochsiek2021}. It also proposes and evaluates improvements, including a variation of the stratification technique and the use of basic random partitioning with suitable sampling.\n\n3.  **Data & Participants**\n    The research focuses on the training of Knowledge Graph Embedding models for large-scale Knowledge Graphs (KGs) \\cite{kochsiek2021}. Specific datasets, sample sizes, or demographic characteristics are not detailed in the provided abstract.\n\n4.  **Key Empirical Findings**\n    *   Prior evaluation methodologies for parallel KGE training are often not comparable and can be misleading \\cite{kochsiek2021}.\n    *   Most currently implemented parallel training methods tend to have a negative impact on embedding quality \\cite{kochsiek2021}.\n    *   A proposed simple but effective variation of the stratification technique (used by PyTorch BigGraph) successfully mitigates negative impacts on embedding quality \\cite{kochsiek2021}.\n    *   Basic random partitioning, when combined with suitable sampling techniques, can be an effective or even best-performing choice for parallel KGE training \\cite{kochsiek2021}.\n\n5.  **Statistical Analysis**\n    While the study reports on an experimental investigation of \"embedding quality\" and \"performance,\" the specific statistical methods applied, significance levels, or confidence intervals are not detailed in the provided abstract \\cite{kochsiek2021}. The findings are based on comparative analysis of re-implemented and improved techniques.\n\n6.  **Validity & Limitations**\n    The study enhances internal validity by re-implementing techniques in a common framework to address comparability issues found in prior work \\cite{kochsiek2021}. A limitation is that efficient and effective parallel training still requires a careful choice of techniques, indicating inherent complexity.\n\n7.  **Empirical Contribution**\n    The study empirically demonstrates that many existing parallel KGE training methods negatively impact embedding quality and highlights issues with prior evaluation methodologies \\cite{kochsiek2021}. It contributes new, effective techniques (stratification variation, random partitioning with sampling) that enable efficient and effective large-scale KGE model training, offering practical guidance for the field.",
      "intriguing_abstract": "Scaling Knowledge Graph Embedding (KGE) models to massive knowledge graphs is a paramount challenge, with parallelization often seen as the panacea. However, our empirical study uncovers a critical, often overlooked, reality: many currently implemented parallel KGE training methods significantly degrade embedding quality, and prior evaluation methodologies are frequently incomparable and misleading.\n\nWe rigorously re-implemented and investigated diverse parallelization techniques within a unified computational framework, exposing the inherent trade-offs and limitations. Crucially, we introduce novel contributions that redefine efficient and effective large-scale KGE training. Our proposed variation of the stratification technique successfully mitigates negative impacts on embedding quality, while, surprisingly, basic random partitioning, when combined with suitable sampling, emerges as an exceptionally effective, often best-performing, choice. This work provides essential practical guidance, demonstrating that scalable KGE training *without* compromising quality is achievable through careful technique selection, paving the way for robust, high-quality embeddings in real-world applications. Researchers grappling with large-scale knowledge graphs and distributed KGE training will find these insights indispensable.",
      "keywords": [
        "Knowledge Graph Embedding (KGE) models",
        "parallelization techniques",
        "large-scale Knowledge Graphs",
        "embedding quality",
        "training efficiency",
        "experimental design",
        "stratification technique variation",
        "random partitioning with sampling",
        "negative impact on embedding quality",
        "misleading evaluation methodologies",
        "efficient and effective KGE training",
        "common computational framework",
        "empirical study"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/5515fd5d14ac7b19806294119560a8c74f7fa4b2.pdf",
      "citation_key": "kochsiek2021",
      "metadata": {
        "title": "Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques",
        "authors": [
          "Adrian Kochsiek"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding (KGE) models represent the entities and relations of a knowledge graph (KG) using dense continuous representations called embeddings. KGE methods have recently gained traction for tasks such as knowledge graph completion and reasoning as well as to provide suitable entity representations for downstream learning tasks. While a large part of the available literature focuses on small KGs, a number of frameworks that are able to train KGE models for large-scale KGs by parallelization across multiple GPUs or machines have recently been proposed. So far, the benefits and drawbacks of the various parallelization techniques have not been studied comprehensively. In this paper, we report on an experimental study in which we presented, re-implemented in a common computational framework, investigated, and improved the available techniques. We found that the evaluation methodologies used in prior work are often not comparable and can be misleading, and that most of currently implemented training methods tend to have a negative impact on embedding quality. We propose a simple but effective variation of the stratification technique used by PyTorch BigGraph for mitigation. Moreover, basic random partitioning can be an effective or even the best-performing choice when combined with suitable sampling techniques. Ultimately, we found that efficient and effective parallel training of large-scale KGE models is indeed achievable but requires a careful choice of techniques.",
        "file_path": "paper_data/knowledge_graph_embedding/5515fd5d14ac7b19806294119560a8c74f7fa4b2.pdf",
        "venue": "Proceedings of the VLDB Endowment",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the empirical study for literature review:\n\n1.  **Research Questions & Hypotheses**\n    This empirical study investigates the benefits and drawbacks of various parallelization techniques for training Knowledge Graph Embedding (KGE) models on large-scale knowledge graphs \\cite{kochsiek2021}. It implicitly tests hypotheses regarding the impact of these techniques on embedding quality and training efficiency.\n\n2.  **Study Design & Methodology**\n    The study employs an experimental design, re-implementing and investigating existing parallelization techniques within a common computational framework to ensure comparability \\cite{kochsiek2021}. It also proposes and evaluates improvements, including a variation of the stratification technique and the use of basic random partitioning with suitable sampling.\n\n3.  **Data & Participants**\n    The research focuses on the training of Knowledge Graph Embedding models for large-scale Knowledge Graphs (KGs) \\cite{kochsiek2021}. Specific datasets, sample sizes, or demographic characteristics are not detailed in the provided abstract.\n\n4.  **Key Empirical Findings**\n    *   Prior evaluation methodologies for parallel KGE training are often not comparable and can be misleading \\cite{kochsiek2021}.\n    *   Most currently implemented parallel training methods tend to have a negative impact on embedding quality \\cite{kochsiek2021}.\n    *   A proposed simple but effective variation of the stratification technique (used by PyTorch BigGraph) successfully mitigates negative impacts on embedding quality \\cite{kochsiek2021}.\n    *   Basic random partitioning, when combined with suitable sampling techniques, can be an effective or even best-performing choice for parallel KGE training \\cite{kochsiek2021}.\n\n5.  **Statistical Analysis**\n    While the study reports on an experimental investigation of \"embedding quality\" and \"performance,\" the specific statistical methods applied, significance levels, or confidence intervals are not detailed in the provided abstract \\cite{kochsiek2021}. The findings are based on comparative analysis of re-implemented and improved techniques.\n\n6.  **Validity & Limitations**\n    The study enhances internal validity by re-implementing techniques in a common framework to address comparability issues found in prior work \\cite{kochsiek2021}. A limitation is that efficient and effective parallel training still requires a careful choice of techniques, indicating inherent complexity.\n\n7.  **Empirical Contribution**\n    The study empirically demonstrates that many existing parallel KGE training methods negatively impact embedding quality and highlights issues with prior evaluation methodologies \\cite{kochsiek2021}. It contributes new, effective techniques (stratification variation, random partitioning with sampling) that enable efficient and effective large-scale KGE model training, offering practical guidance for the field.",
        "keywords": [
          "Knowledge Graph Embedding (KGE) models",
          "parallelization techniques",
          "large-scale Knowledge Graphs",
          "embedding quality",
          "training efficiency",
          "experimental design",
          "stratification technique variation",
          "random partitioning with sampling",
          "negative impact on embedding quality",
          "misleading evaluation methodologies",
          "efficient and effective KGE training",
          "common computational framework",
          "empirical study"
        ],
        "is_new_direction": "0",
        "paper_type": "empirical"
      },
      "file_name": "5515fd5d14ac7b19806294119560a8c74f7fa4b2.pdf"
    },
    {
      "success": true,
      "doc_id": "b9bad6b0fe7c7fa0428a35d64826db89",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   Existing knowledge graph (KG) embedding methods, while aiming to encode entities and relations into low-dimensional vector spaces, fail to adequately consider the influence of the embedding space itself \\cite{yang2021}.\n    *   This oversight leads to unsatisfactory performance in practical applications, highlighting the need for improved expressiveness in KG embeddings \\cite{yang2021}.\n\n2.  **Related Work & Positioning**\n    *   The work positions itself by identifying a critical limitation in existing KG embedding approaches: their inability to properly account for the characteristics and influence of the embedding space \\cite{yang2021}.\n    *   Specifically, it points out the implications and limitations of the widely used Minkowski metric in current KG embedding models \\cite{yang2021}.\n\n3.  **Technical Approach & Innovation**\n    *   The core technical method involves improving the expressiveness of the embedding space by introducing a novel metric \\cite{yang2021}.\n    *   The paper first quantitatively analyzes the implications of the Minkowski metric \\cite{yang2021}.\n    *   It then proposes a new metric, named **Cycle metric**, which is based on the oscillation property of periodic functions \\cite{yang2021}.\n    *   The approach also investigates the significant influence of the function period on embedding space expressiveness, finding that a smaller period leads to better expressive ability for a trained model \\cite{yang2021}.\n    *   Finally, a new model, **CyclE**, is proposed, which integrates the Cycle Metric with popular existing KG embedding models \\cite{yang2021}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Metric**: Introduction of the \"Cycle metric,\" a new distance metric for KG embeddings derived from the oscillation property of periodic functions \\cite{yang2021}.\n    *   **Theoretical Insight**: Quantitative analysis of the Minkowski metric's implications and the discovery that the function period significantly impacts embedding space expressiveness, with smaller periods yielding better results \\cite{yang2021}.\n    *   **System Design**: Proposal of the \"CyclE\" model, which effectively combines the novel Cycle Metric with established KG embedding architectures \\cite{yang2021}.\n\n5.  **Experimental Validation**\n    *   Comprehensive experimental results were conducted to validate the findings \\cite{yang2021}.\n    *   The key performance metric and comparison result indicate that the Cycle metric is \"more appropriate than Minkowski for KG embedding\" \\cite{yang2021}.\n\n6.  **Limitations & Scope**\n    *   The paper focuses on improving embedding space expressiveness through metric design, specifically addressing the limitations of the Minkowski metric \\cite{yang2021}.\n    *   The scope of applicability is within knowledge graph embedding tasks, particularly those benefiting from enhanced metric properties \\cite{yang2021}.\n\n7.  **Technical Significance**\n    *   This work advances the technical state-of-the-art by demonstrating that the choice of metric significantly influences the expressiveness of KG embedding spaces \\cite{yang2021}.\n    *   It provides a novel metric (Cycle metric) and a corresponding model (CyclE) that outperform traditional approaches relying on Minkowski metrics \\cite{yang2021}.\n    *   The findings regarding the influence of function period on expressiveness could inspire future research into designing more sophisticated and context-aware embedding spaces for knowledge graphs \\cite{yang2021}.",
      "intriguing_abstract": "The inherent expressiveness of knowledge graph (KG) embeddings is profoundly shaped by their underlying geometric space, a critical factor often overlooked by current methodologies. We expose how the pervasive reliance on the Minkowski metric fundamentally limits model performance, necessitating a paradigm shift in metric design. This paper introduces the novel **Cycle metric**, a groundbreaking distance metric inspired by the oscillation properties of periodic functions, specifically engineered to unlock superior KG embedding expressiveness. Our quantitative analysis reveals the significant influence of function period on embedding space capabilities, demonstrating that smaller periods dramatically enhance model performance. Integrating this innovation, we propose **CyclE**, a new model that seamlessly combines the Cycle metric with established KG embedding architectures. Extensive experiments rigorously validate that the Cycle metric is demonstrably more appropriate and effective than Minkowski for diverse KG embedding tasks, yielding superior results. This work not only advances the state-of-the-art by offering a theoretically robust and empirically validated alternative but also paves the way for designing highly expressive and context-aware embedding spaces.",
      "keywords": [
        "Knowledge graph embedding",
        "embedding space expressiveness",
        "Minkowski metric limitations",
        "Cycle metric (novel distance metric)",
        "periodic functions",
        "function period influence",
        "CyclE model",
        "metric design",
        "quantitative analysis",
        "technical state-of-the-art advancement"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/e5c851867af5587466f7cd9c22f8b2c84f8c6b63.pdf",
      "citation_key": "yang2021",
      "metadata": {
        "title": "Cycle or Minkowski: Which is More Appropriate for Knowledge Graph Embedding?",
        "authors": [
          "Han Yang",
          "Leilei Zhang",
          "Bingning Wang",
          "Ting Yao",
          "Junfei Liu"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph (KG) embedding aims to encode entities and relations into low-dimensional vector spaces, in turn, can support various machine learning models on KG related tasks with good performance. However, existing methods for knowledge graph embedding fail to consider the influence of the embedding space, which makes them still unsatisfactory in practical applications. In this study, we try to improve the expressiveness of the embedding space from the perspective of the metric. Specifically, we first point out the implications of Minkowski metric used in KG embedding and then make a quantitative analysis. To solve the limitations, we introduce a new metric, named Cycle metric, based on the oscillation property of the periodic function. Furthermore, we find that the function period has a significant influence on the expressiveness of the embedding space. Given a fully trained model, the smaller the period, the better the expressive ability. Finally, to validate the findings, we propose a new model, named CyclE by combining Cycle Metric and the popular KG embeddings models. Comprehensive experimental results show that Cycle is more appropriate than Minkowski for KG embedding.",
        "file_path": "paper_data/knowledge_graph_embedding/e5c851867af5587466f7cd9c22f8b2c84f8c6b63.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   Existing knowledge graph (KG) embedding methods, while aiming to encode entities and relations into low-dimensional vector spaces, fail to adequately consider the influence of the embedding space itself \\cite{yang2021}.\n    *   This oversight leads to unsatisfactory performance in practical applications, highlighting the need for improved expressiveness in KG embeddings \\cite{yang2021}.\n\n2.  **Related Work & Positioning**\n    *   The work positions itself by identifying a critical limitation in existing KG embedding approaches: their inability to properly account for the characteristics and influence of the embedding space \\cite{yang2021}.\n    *   Specifically, it points out the implications and limitations of the widely used Minkowski metric in current KG embedding models \\cite{yang2021}.\n\n3.  **Technical Approach & Innovation**\n    *   The core technical method involves improving the expressiveness of the embedding space by introducing a novel metric \\cite{yang2021}.\n    *   The paper first quantitatively analyzes the implications of the Minkowski metric \\cite{yang2021}.\n    *   It then proposes a new metric, named **Cycle metric**, which is based on the oscillation property of periodic functions \\cite{yang2021}.\n    *   The approach also investigates the significant influence of the function period on embedding space expressiveness, finding that a smaller period leads to better expressive ability for a trained model \\cite{yang2021}.\n    *   Finally, a new model, **CyclE**, is proposed, which integrates the Cycle Metric with popular existing KG embedding models \\cite{yang2021}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Metric**: Introduction of the \"Cycle metric,\" a new distance metric for KG embeddings derived from the oscillation property of periodic functions \\cite{yang2021}.\n    *   **Theoretical Insight**: Quantitative analysis of the Minkowski metric's implications and the discovery that the function period significantly impacts embedding space expressiveness, with smaller periods yielding better results \\cite{yang2021}.\n    *   **System Design**: Proposal of the \"CyclE\" model, which effectively combines the novel Cycle Metric with established KG embedding architectures \\cite{yang2021}.\n\n5.  **Experimental Validation**\n    *   Comprehensive experimental results were conducted to validate the findings \\cite{yang2021}.\n    *   The key performance metric and comparison result indicate that the Cycle metric is \"more appropriate than Minkowski for KG embedding\" \\cite{yang2021}.\n\n6.  **Limitations & Scope**\n    *   The paper focuses on improving embedding space expressiveness through metric design, specifically addressing the limitations of the Minkowski metric \\cite{yang2021}.\n    *   The scope of applicability is within knowledge graph embedding tasks, particularly those benefiting from enhanced metric properties \\cite{yang2021}.\n\n7.  **Technical Significance**\n    *   This work advances the technical state-of-the-art by demonstrating that the choice of metric significantly influences the expressiveness of KG embedding spaces \\cite{yang2021}.\n    *   It provides a novel metric (Cycle metric) and a corresponding model (CyclE) that outperform traditional approaches relying on Minkowski metrics \\cite{yang2021}.\n    *   The findings regarding the influence of function period on expressiveness could inspire future research into designing more sophisticated and context-aware embedding spaces for knowledge graphs \\cite{yang2021}.",
        "keywords": [
          "Knowledge graph embedding",
          "embedding space expressiveness",
          "Minkowski metric limitations",
          "Cycle metric (novel distance metric)",
          "periodic functions",
          "function period influence",
          "CyclE model",
          "metric design",
          "quantitative analysis",
          "technical state-of-the-art advancement"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "e5c851867af5587466f7cd9c22f8b2c84f8c6b63.pdf"
    },
    {
      "success": true,
      "doc_id": "2063bb5a0a4e4b263763f26ab760ab15",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion \\cite{shang2024}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Knowledge Graph Completion (KGC) suffers from the incompleteness of Knowledge Graphs (KGs), requiring effective embedding representations to predict missing facts \\cite{shang2024}.\n    *   **Importance and Challenge**:\n        *   Existing GNN-based KGC models (GCNs and GATs) exhibit **data dependence**, meaning their performance is sensitive to the local structure of entity neighbors. The optimal GNN type (GCN vs. GAT) varies, and \"pre-validating\" each entity's neighbors to select the best GNN is prohibitively expensive \\cite{shang2024}.\n        *   **Message limitation**: Current message functions in GNNs primarily operate in Euclidean space, which cannot fully capture the rich, intrinsic structural information of KGs, leading to insufficient neighbor message aggregation and affecting embedding quality \\cite{shang2024}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Builds upon Knowledge Graph Embedding (KGE) methods (e.g., TransE, RotatE) and GNN-based KGC models (e.g., R-GCN, CompGCN, MR-GAT) \\cite{shang2024}.\n        *   Extends non-Euclidean KGC models (e.g., ManifoldE, MuRP, RotH) by integrating multiple geometric spaces, rather than relying on a single one \\cite{shang2024}.\n    *   **Limitations of Previous Solutions**:\n        *   **Single GNN Type**: Most GNN-based KGC models use a single type of GNN (either GCN or GAT), which degrades embedding quality due to their inherent limitations and data sensitivity \\cite{shang2024}.\n        *   **Euclidean-only Message Functions**: Existing message functions are designed solely in Euclidean space, failing to capture complex structural information present in KGs, leading to \"insufficient neighbor message\" \\cite{shang2024}.\n        *   **Expensive Pre-validation**: The ideal approach of pre-validating local KG structures to select the appropriate GNN type is computationally prohibitive \\cite{shang2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes MGTCA (Mixed Geometry Message and Trainable Convolutional Attention Network) \\cite{shang2024}.\n        *   **Mixed Geometry Message Function (MGMF)**: Generates rich neighbor messages by integrating spatial information from hyperbolic (negative curvature), hypersphere (positive curvature), and Euclidean (zero curvature) spaces jointly. It uses geometric mapping and linear transformation to combine these messages into a Euclidean output \\cite{shang2024}.\n        *   **Trainable Convolutional Attention Network (TCAN)**: Comprises three types of GNNs (GCN, GAT, and a novel KGCAT which applies convolution to attention) within a single trainable formulation. This allows for autonomous switching between GNN types and learns the required attention for each local structure, eliminating the need for pre-validation \\cite{shang2024}.\n        *   **Mixed Geometry Scoring Function**: Calculates triple scores using novel prediction and similarity functions based on the three integrated geometric spaces \\cite{shang2024}.\n    *   **Novelty/Difference**:\n        *   First to explore generating mixed geometric messages in GNN-based KGC methods \\cite{shang2024}.\n        *   First to explore autonomous switching of GNN types in KGC tasks, addressing the data dependence problem without expensive pre-validation \\cite{shang2024}.\n        *   Introduces a novel KGCAT that applies convolutional operations before the attention mechanism to balance structural information and avoid redundancy \\cite{shang2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Mixed Geometry Message Function (MGMF)**: Integrates hyperbolic, hypersphere, and Euclidean spaces to generate richer neighbor messages, improving embedding representation quality \\cite{shang2024}.\n        *   **Trainable Convolutional Attention Network (TCAN)**: A unified, trainable formulation that adaptively combines GCNs, GATs, and a new KGCAT, enabling autonomous GNN type switching and learning attention weights for local structures \\cite{shang2024}.\n        *   **Mixed Geometry Scoring Function**: A novel scoring mechanism that leverages prediction and similarity functions across multiple geometric spaces for improved link prediction \\cite{shang2024}.\n    *   **System Design/Architectural Innovations**: The overall MGTCA framework integrates these components into a multi-layer architecture for learning entity and relation embeddings \\cite{shang2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on three standard benchmark datasets \\cite{shang2024}.\n    *   **Key Performance Metrics and Comparison Results**: The paper states that MGTCA significantly improves performance compared to state-of-the-art approaches, confirming the effectiveness of its innovations \\cite{shang2024}. (Specific metrics like MRR, Hits@k are implied for KGC but not detailed in the abstract/introduction provided).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the limitations of *previous* GNN-based KGC models. It does not explicitly detail specific technical limitations or assumptions of the proposed MGTCA model itself. However, the integration of multiple geometric spaces and complex trainable attention mechanisms might imply increased computational complexity or a larger hyperparameter search space compared to simpler models.\n    *   **Scope of Applicability**: MGTCA is designed for Knowledge Graph Completion tasks, specifically link prediction, by learning improved entity and relation embeddings. Its applicability extends to KGs with diverse structural properties, as it aims to adaptively handle different local graph structures \\cite{shang2024}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: MGTCA significantly advances the technical state-of-the-art in KGC by achieving superior performance on benchmark datasets \\cite{shang2024}.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for exploring multi-geometric space modeling in GNNs, suggesting that combining different curvatures can capture richer structural information \\cite{shang2024}.\n        *   Introduces a novel paradigm for adaptive GNN architectures that can autonomously switch between different aggregation mechanisms, potentially inspiring more flexible and robust GNN designs for various graph-based tasks \\cite{shang2024}.\n        *   Addresses fundamental limitations of existing GNNs in KGC, paving the way for more effective and less data-dependent models \\cite{shang2024}.",
      "intriguing_abstract": "Knowledge Graph Completion (KGC) is fundamentally challenged by the inherent incompleteness of Knowledge Graphs (KGs) and the limitations of current Graph Neural Networks (GNNs). Existing GNN-based methods suffer from acute data dependence, where the optimal GNN type (GCN vs. GAT) varies drastically across local graph structures, and rely solely on Euclidean message functions, failing to capture the rich, intrinsic geometry of KGs.\n\nWe introduce MGTCA (Mixed Geometry Message and Trainable Convolutional Attention Network), a novel framework that revolutionizes KGC. MGTCA features a **Mixed Geometry Message Function (MGMF)**, which for the first time integrates hyperbolic, hypersphere, and Euclidean spaces to generate profoundly richer neighbor messages. Complementing this is our **Trainable Convolutional Attention Network (TCAN)**, a unified architecture that autonomously switches between GCN, GAT, and a novel KGCAT (Convolutional GAT) for each local structure, eliminating costly pre-validation. This adaptive GNN paradigm, combined with a mixed geometry scoring function, significantly enhances embedding quality and link prediction. MGTCA achieves state-of-the-art performance on benchmark datasets, pioneering new directions for multi-geometric GNNs and adaptive graph learning architectures.",
      "keywords": [
        "Knowledge Graph Completion",
        "Mixed Geometry Message Function",
        "Trainable Convolutional Attention Network",
        "multi-geometric spaces",
        "autonomous GNN type switching",
        "Knowledge Graph Embeddings",
        "convolutional attention",
        "adaptive GNN architectures",
        "Mixed Geometry Scoring Function",
        "data dependence problem",
        "richer neighbor messages"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/eb14b24b329a6cc80747644616e15491ef49596f.pdf",
      "citation_key": "shang2024",
      "metadata": {
        "title": "Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion",
        "authors": [
          "Bin Shang",
          "Yinliang Zhao",
          "Jun Liu",
          "Di Wang"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graph completion (KGC) aims to study the embedding representation to solve the incompleteness of knowledge graphs (KGs). Recently, graph convolutional networks (GCNs) and graph attention networks (GATs) have been widely used in KGC tasks by capturing neighbor information of entities. However, Both GCNs and GATs based KGC models have their limitations, and the best method is to analyze the neighbors of each entity (pre-validating), while this process is prohibitively expensive. Furthermore, the representation quality of the embeddings can affect the aggregation of neighbor information (message passing). To address the above limitations, we propose a novel knowledge graph completion model with mixed geometry message and trainable convolutional attention network named MGTCA. Concretely, the mixed geometry message function generates rich neighbor message by integrating spatially information in the hyperbolic space, hypersphere space and Euclidean space jointly. To complete the autonomous switching of graph neural networks (GNNs) and eliminate the necessity of pre-validating the local structure of KGs, a trainable convolutional attention network is proposed by comprising three types of GNNs in one trainable formulation. Furthermore, a mixed geometry scoring function is proposed, which calculates scores of triples by novel prediction function and similarity function based on different geometric spaces. Extensive experiments on three standard datasets confirm the effectiveness of our innovations, and the performance of MGTCA is significantly improved compared to the state-of-the-art approaches.",
        "file_path": "paper_data/knowledge_graph_embedding/eb14b24b329a6cc80747644616e15491ef49596f.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion \\cite{shang2024}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Knowledge Graph Completion (KGC) suffers from the incompleteness of Knowledge Graphs (KGs), requiring effective embedding representations to predict missing facts \\cite{shang2024}.\n    *   **Importance and Challenge**:\n        *   Existing GNN-based KGC models (GCNs and GATs) exhibit **data dependence**, meaning their performance is sensitive to the local structure of entity neighbors. The optimal GNN type (GCN vs. GAT) varies, and \"pre-validating\" each entity's neighbors to select the best GNN is prohibitively expensive \\cite{shang2024}.\n        *   **Message limitation**: Current message functions in GNNs primarily operate in Euclidean space, which cannot fully capture the rich, intrinsic structural information of KGs, leading to insufficient neighbor message aggregation and affecting embedding quality \\cite{shang2024}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Builds upon Knowledge Graph Embedding (KGE) methods (e.g., TransE, RotatE) and GNN-based KGC models (e.g., R-GCN, CompGCN, MR-GAT) \\cite{shang2024}.\n        *   Extends non-Euclidean KGC models (e.g., ManifoldE, MuRP, RotH) by integrating multiple geometric spaces, rather than relying on a single one \\cite{shang2024}.\n    *   **Limitations of Previous Solutions**:\n        *   **Single GNN Type**: Most GNN-based KGC models use a single type of GNN (either GCN or GAT), which degrades embedding quality due to their inherent limitations and data sensitivity \\cite{shang2024}.\n        *   **Euclidean-only Message Functions**: Existing message functions are designed solely in Euclidean space, failing to capture complex structural information present in KGs, leading to \"insufficient neighbor message\" \\cite{shang2024}.\n        *   **Expensive Pre-validation**: The ideal approach of pre-validating local KG structures to select the appropriate GNN type is computationally prohibitive \\cite{shang2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes MGTCA (Mixed Geometry Message and Trainable Convolutional Attention Network) \\cite{shang2024}.\n        *   **Mixed Geometry Message Function (MGMF)**: Generates rich neighbor messages by integrating spatial information from hyperbolic (negative curvature), hypersphere (positive curvature), and Euclidean (zero curvature) spaces jointly. It uses geometric mapping and linear transformation to combine these messages into a Euclidean output \\cite{shang2024}.\n        *   **Trainable Convolutional Attention Network (TCAN)**: Comprises three types of GNNs (GCN, GAT, and a novel KGCAT which applies convolution to attention) within a single trainable formulation. This allows for autonomous switching between GNN types and learns the required attention for each local structure, eliminating the need for pre-validation \\cite{shang2024}.\n        *   **Mixed Geometry Scoring Function**: Calculates triple scores using novel prediction and similarity functions based on the three integrated geometric spaces \\cite{shang2024}.\n    *   **Novelty/Difference**:\n        *   First to explore generating mixed geometric messages in GNN-based KGC methods \\cite{shang2024}.\n        *   First to explore autonomous switching of GNN types in KGC tasks, addressing the data dependence problem without expensive pre-validation \\cite{shang2024}.\n        *   Introduces a novel KGCAT that applies convolutional operations before the attention mechanism to balance structural information and avoid redundancy \\cite{shang2024}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Mixed Geometry Message Function (MGMF)**: Integrates hyperbolic, hypersphere, and Euclidean spaces to generate richer neighbor messages, improving embedding representation quality \\cite{shang2024}.\n        *   **Trainable Convolutional Attention Network (TCAN)**: A unified, trainable formulation that adaptively combines GCNs, GATs, and a new KGCAT, enabling autonomous GNN type switching and learning attention weights for local structures \\cite{shang2024}.\n        *   **Mixed Geometry Scoring Function**: A novel scoring mechanism that leverages prediction and similarity functions across multiple geometric spaces for improved link prediction \\cite{shang2024}.\n    *   **System Design/Architectural Innovations**: The overall MGTCA framework integrates these components into a multi-layer architecture for learning entity and relation embeddings \\cite{shang2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on three standard benchmark datasets \\cite{shang2024}.\n    *   **Key Performance Metrics and Comparison Results**: The paper states that MGTCA significantly improves performance compared to state-of-the-art approaches, confirming the effectiveness of its innovations \\cite{shang2024}. (Specific metrics like MRR, Hits@k are implied for KGC but not detailed in the abstract/introduction provided).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the limitations of *previous* GNN-based KGC models. It does not explicitly detail specific technical limitations or assumptions of the proposed MGTCA model itself. However, the integration of multiple geometric spaces and complex trainable attention mechanisms might imply increased computational complexity or a larger hyperparameter search space compared to simpler models.\n    *   **Scope of Applicability**: MGTCA is designed for Knowledge Graph Completion tasks, specifically link prediction, by learning improved entity and relation embeddings. Its applicability extends to KGs with diverse structural properties, as it aims to adaptively handle different local graph structures \\cite{shang2024}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: MGTCA significantly advances the technical state-of-the-art in KGC by achieving superior performance on benchmark datasets \\cite{shang2024}.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for exploring multi-geometric space modeling in GNNs, suggesting that combining different curvatures can capture richer structural information \\cite{shang2024}.\n        *   Introduces a novel paradigm for adaptive GNN architectures that can autonomously switch between different aggregation mechanisms, potentially inspiring more flexible and robust GNN designs for various graph-based tasks \\cite{shang2024}.\n        *   Addresses fundamental limitations of existing GNNs in KGC, paving the way for more effective and less data-dependent models \\cite{shang2024}.",
        "keywords": [
          "Knowledge Graph Completion",
          "Mixed Geometry Message Function",
          "Trainable Convolutional Attention Network",
          "multi-geometric spaces",
          "autonomous GNN type switching",
          "Knowledge Graph Embeddings",
          "convolutional attention",
          "adaptive GNN architectures",
          "Mixed Geometry Scoring Function",
          "data dependence problem",
          "richer neighbor messages"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "eb14b24b329a6cc80747644616e15491ef49596f.pdf"
    },
    {
      "success": true,
      "doc_id": "5e71577e4ccbf6649b9c12137aa2c20c",
      "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey \\cite{asmara2023} covers the domain of knowledge graph embedding (KGE), focusing specifically on the TransE, TransH, and TransR methods. Its main objectives are to provide an overview and critical analysis of these well-known and widely applied KGE techniques, comparing their features to serve as a comprehensive resource.\n\n2.  **Literature Coverage**\n    The paper reviews existing research on the specified KGE methods, examining experiments conducted by various researchers. While a specific time period or detailed selection criteria are not explicitly stated, the focus is on \"well-known and most applied\" methods, implying a selection based on their prominence and impact in the field.\n\n3.  **Classification Framework**\n    *   The survey primarily organizes the literature by critically analyzing three specific knowledge graph embedding methods: TransE.\n    *   It then extends this analysis to TransH.\n    *   Finally, it covers the TransR method, comparing the key concepts, methodologies, strengths, and limitations of each.\n\n4.  **Key Findings & Insights**\n    *   The survey identifies and critically analyzes the core concepts and methodologies underpinning TransE, TransH, and TransR.\n    *   It details the specific strengths and limitations inherent to each of these prominent KGE approaches.\n    *   The paper examines various applications and experimental results associated with TransE, TransH, and TransR, as reported by existing researchers.\n    *   A comparative analysis of the features of these three methods is provided, highlighting their distinct characteristics and performance aspects.\n\n5.  **Research Gaps & Future Directions**\n    The provided text for this survey paper does not explicitly identify specific research gaps or recommend future research directions. Its primary focus is on reviewing and comparing existing, well-established methods.\n\n6.  **Survey Contribution**\n    This survey \\cite{asmara2023} provides unique value by offering a focused overview and critical comparative analysis of three foundational knowledge graph embedding methods: TransE, TransH, and TransR. It aims to deliver a comprehensive resource for researchers and practitioners interested in these specific techniques.",
      "intriguing_abstract": "Unlocking the full potential of Knowledge Graphs hinges on robust embedding techniques that capture intricate relational semantics. This survey delves into the foundational landscape of **Knowledge Graph Embedding (KGE)**, offering a critical and comparative analysis of three cornerstone translational models: **TransE, TransH, and TransR**. Far beyond a mere overview, we meticulously dissect the core concepts, methodologies, and underlying principles that define each approach, revealing their distinct strengths and inherent limitations in representing complex relational data.\n\nBy synthesizing a wealth of existing research, including diverse applications and experimental outcomes, this paper provides an unparalleled feature-by-feature comparison. Researchers gain profound insights into how these models navigate challenges like multi-relational data and hierarchical structures, guiding informed selection for specific tasks. This comprehensive resource serves as an indispensable guide for understanding the evolution and practical implications of these prominent **KGE** methods, empowering future advancements in **relational learning** and **semantic representation**.",
      "keywords": [
        "knowledge graph embedding (KGE)",
        "TransE",
        "TransH",
        "TransR",
        "survey paper",
        "critical analysis",
        "comparative analysis",
        "foundational KGE methods",
        "core concepts and methodologies",
        "strengths and limitations",
        "experimental results",
        "focused overview",
        "comprehensive resource"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf",
      "citation_key": "asmara2023",
      "metadata": {
        "title": "A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links",
        "authors": [
          "S. M. Asmara",
          "N. A. Sahabudin",
          "Nor Syahidatul Nadiah Ismail",
          "I. A. Sabri"
        ],
        "published_date": "2023",
        "abstract": "Knowledge representation and reasoning require knowledge graph embedding as it is crucial in the area. It involves mapping entities and relationships from a knowledge graph into vectors of lower dimensions that are continuous in nature. This encoding enables machine learning algorithms to effectively reason and make predictions on graph-structured data. This review article offers an overview and critical analysis specifically about the methods of knowledge graph embedding which are TransE, TransH, and TransR. The key concepts, methodologies, strengths, and limitations of these methods, along with examining their applications and experiments conducted by existing researchers have been studied. The motivation to conduct this study is to review the well-known and most applied knowledge embedding methods and compare the features of those methods so that a comprehensive resource for researchers and practitioners interested in delving into knowledge graph embedding techniques is delivered.",
        "file_path": "paper_data/knowledge_graph_embedding/9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf",
        "venue": "International Conference on Software Engineering and Computer Systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the survey paper for literature review:\n\n1.  **Review Scope & Objectives**\n    This survey \\cite{asmara2023} covers the domain of knowledge graph embedding (KGE), focusing specifically on the TransE, TransH, and TransR methods. Its main objectives are to provide an overview and critical analysis of these well-known and widely applied KGE techniques, comparing their features to serve as a comprehensive resource.\n\n2.  **Literature Coverage**\n    The paper reviews existing research on the specified KGE methods, examining experiments conducted by various researchers. While a specific time period or detailed selection criteria are not explicitly stated, the focus is on \"well-known and most applied\" methods, implying a selection based on their prominence and impact in the field.\n\n3.  **Classification Framework**\n    *   The survey primarily organizes the literature by critically analyzing three specific knowledge graph embedding methods: TransE.\n    *   It then extends this analysis to TransH.\n    *   Finally, it covers the TransR method, comparing the key concepts, methodologies, strengths, and limitations of each.\n\n4.  **Key Findings & Insights**\n    *   The survey identifies and critically analyzes the core concepts and methodologies underpinning TransE, TransH, and TransR.\n    *   It details the specific strengths and limitations inherent to each of these prominent KGE approaches.\n    *   The paper examines various applications and experimental results associated with TransE, TransH, and TransR, as reported by existing researchers.\n    *   A comparative analysis of the features of these three methods is provided, highlighting their distinct characteristics and performance aspects.\n\n5.  **Research Gaps & Future Directions**\n    The provided text for this survey paper does not explicitly identify specific research gaps or recommend future research directions. Its primary focus is on reviewing and comparing existing, well-established methods.\n\n6.  **Survey Contribution**\n    This survey \\cite{asmara2023} provides unique value by offering a focused overview and critical comparative analysis of three foundational knowledge graph embedding methods: TransE, TransH, and TransR. It aims to deliver a comprehensive resource for researchers and practitioners interested in these specific techniques.",
        "keywords": [
          "knowledge graph embedding (KGE)",
          "TransE",
          "TransH",
          "TransR",
          "survey paper",
          "critical analysis",
          "comparative analysis",
          "foundational KGE methods",
          "core concepts and methodologies",
          "strengths and limitations",
          "experimental results",
          "focused overview",
          "comprehensive resource"
        ],
        "is_new_direction": "0",
        "paper_type": "survey"
      },
      "file_name": "9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf"
    },
    {
      "success": true,
      "doc_id": "3ab4121a5e5b55f2133935344666a2d0",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Knowledge Graph Embedding (KGE) models struggle to learn and express the full spectrum of relational (e.g., symmetry, antisymmetry, inversion, composition) and structural (e.g., hierarchies) patterns present in knowledge graphs. No single KGE model performs equally well across all pattern types \\cite{gregucci2023}.\n    *   **Importance & Challenge:** Knowledge graphs are inherently incomplete, making link prediction a fundamental task. The challenge lies in developing a unified approach that can leverage the diverse strengths of different KGE models to capture a broader range of patterns, thereby improving the accuracy of predicting missing links \\cite{gregucci2023}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself against individual KGE models (e.g., TransE, RotatE, ComplEx, DistMult, AttH/AttE) by aiming to combine their strengths rather than proposing a new standalone model \\cite{gregucci2023}.\n    *   **Limitations of Previous Solutions:**\n        *   Prior KGE ensemble methods either combine multiple runs of the *same* model (still limited in pattern coverage) or combine *different* models at the score level (e.g., score concatenation, weighted sums, relation-level ensembles) \\cite{gregucci2023}. These methods lack a fine-grained mechanism to dynamically select the most suitable model's representation for a specific query.\n        *   Approaches like MulDE, while combining models, cannot steer decisions towards the specific strengths of individual models but rely on majority guidance \\cite{gregucci2023}.\n        *   Other research combines different *geometric spaces* (e.g., Hyperbolic, Spherical, Euclidean) but does not focus on combining *query representations* from different KGE models \\cite{gregucci2023}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a general framework that integrates query representations from multiple existing KGE models (M) into a unified representation. This combination is achieved through a spherical geometric framework \\cite{gregucci2023}.\n    *   **Novelty/Difference:**\n        *   **Attention Mechanism for Query Combination:** A key innovation is the use of an attention mechanism to dynamically select the \"most suitable model to answer each query\" based on the characteristics of the underlying relation \\cite{gregucci2023}. This allows the framework to adapt to different relational patterns.\n        *   **Multi-Geometric Space Integration:** The model combines query representations in Euclidean space and then projects them onto a non-Euclidean manifold, specifically the Poincar ball, to effectively capture structural patterns like hierarchies, in addition to relational patterns \\cite{gregucci2023}.\n        *   **Spherical Query Embedding:** Each query is represented as a hypersphere, where the center is the combined query embedding and the radius is linked to ranking metrics (Hits@k) and optimized via a loss function \\cite{gregucci2023}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework:** A spherical geometric framework for integrating diverse KGE models by combining their query representations, leveraging their distinct geometric transformations \\cite{gregucci2023}.\n    *   **Adaptive Attention Mechanism:** Introduction of a Riemannian attention-based mechanism that learns to weigh the contributions of different KGE models' query representations based on the specific query's relation, enabling robust handling of heterogeneous relational patterns \\cite{gregucci2023}.\n    *   **Hybrid Geometry for Pattern Learning:** The integration of Euclidean space for query combination with projection onto the Poincar ball (hyperbolic geometry) to simultaneously capture both relational and structural (hierarchical) patterns \\cite{gregucci2023}.\n    *   **Theoretical Insights:** Provides theoretical analyses demonstrating that the combined model offers higher expressiveness and inference power than individual models, and that the combined query embedding lies within the convex hull of individual model queries, ensuring it benefits from their collective strengths \\cite{gregucci2023}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experimental analysis was conducted using various link prediction benchmarks \\cite{gregucci2023}.\n    *   **Key Performance Metrics & Results:** The combined model consistently \"outperforms individual models, including state-of-the-art approaches\" on these benchmarks \\cite{gregucci2023}. While specific datasets and detailed metrics are not in the provided abstract, the connection between the spherical radius and the Hits@k metric is highlighted, implying its use in evaluation \\cite{gregucci2023}.\n\n*   **Limitations & Scope**\n    *   **Technical Assumptions:** The approach assumes that query representations from different models can be mapped to a common space (Euclidean for combination, then projected to hyperbolic) \\cite{gregucci2023}.\n    *   **Scope of Applicability:** The method is specifically designed for link prediction in knowledge graphs, focusing on combining query representations (h,r,?) to predict tail entities.\n    *   **Inherent Trade-off:** Theoretically, for a specific *k*, the combined model's score is bounded by the individual models, meaning it might not always achieve the *absolute best* score of a single, perfectly suited model, but it consistently outperforms the worst and provides a robust average \\cite{gregucci2023}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** The proposed model significantly advances the technical state-of-the-art in link prediction by outperforming individual KGE models and existing ensemble approaches \\cite{gregucci2023}.\n    *   **Enhanced Pattern Learning Capability:** It provides a more comprehensive and adaptive framework for learning diverse relational and structural patterns in knowledge graphs, addressing a critical limitation of prior KGE models \\cite{gregucci2023}.\n    *   **Potential Impact on Future Research:** The attention-based query combination and multi-geometric space integration offer a novel paradigm for combining heterogeneous models, potentially inspiring future research in adaptive model integration for various AI tasks beyond link prediction \\cite{gregucci2023}.",
      "intriguing_abstract": "The inherent complexity of knowledge graphs, rich in diverse relational and structural patterns, poses a significant challenge for Knowledge Graph Embedding (KGE) models, as no single approach excels universally in link prediction. We present a groundbreaking framework that overcomes this limitation by adaptively integrating the strengths of multiple KGE models. Our novel approach combines query representations within a unified spherical geometric framework. A key innovation is a Riemannian attention mechanism that dynamically weighs model contributions for each specific query, selecting the most suitable representation based on the underlying relation. Furthermore, we leverage a hybrid geometric strategy, combining Euclidean space for initial query integration with projection onto the Poincar ball (hyperbolic geometry) to simultaneously capture both intricate relational and hierarchical structural patterns. This results in significantly enhanced expressiveness and inference power. Extensive experiments demonstrate that our framework consistently outperforms state-of-the-art individual KGE models and existing ensemble methods on challenging link prediction benchmarks. This work establishes a new paradigm for robust, adaptive model integration, offering a powerful solution for learning comprehensive knowledge graph representations.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "link prediction",
        "relational and structural patterns",
        "spherical geometric framework",
        "query representations",
        "adaptive attention mechanism",
        "multi-geometric space integration",
        "Poincar ball",
        "hyperbolic geometry",
        "dynamic model selection",
        "enhanced pattern learning",
        "state-of-the-art performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/d9802a67b326fe89bbd761c261937ee1e4d4d674.pdf",
      "citation_key": "gregucci2023",
      "metadata": {
        "title": "Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models",
        "authors": [
          "Cosimo Gregucci",
          "M. Nayyeri",
          "D. Hern'andez",
          "Steffen Staab"
        ],
        "published_date": "2023",
        "abstract": "Predicting missing links between entities in a knowledge graph is a fundamental task to deal with the incompleteness of data on the Web. Knowledge graph embeddings map nodes into a vector space to predict new links, scoring them according to geometric criteria. Relations in the graph may follow patterns that can be learned, e.g., some relations might be symmetric and others might be hierarchical. However, the learning capability of different embedding models varies for each pattern and, so far, no single model can learn all patterns equally well. In this paper, we combine the query representations from several models in a unified one to incorporate patterns that are independently captured by each model. Our combination uses attention to select the most suitable model to answer each query. The models are also mapped onto a non-Euclidean manifold, the Poincar ball, to capture structural patterns, such as hierarchies, besides relational patterns, such as symmetry. We prove that our combination provides a higher expressiveness and inference power than each model on its own. As a result, the combined model can learn relational and structural patterns. We conduct extensive experimental analysis with various link prediction benchmarks showing that the combined model outperforms individual models, including state-of-the-art approaches.",
        "file_path": "paper_data/knowledge_graph_embedding/d9802a67b326fe89bbd761c261937ee1e4d4d674.pdf",
        "venue": "The Web Conference",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing Knowledge Graph Embedding (KGE) models struggle to learn and express the full spectrum of relational (e.g., symmetry, antisymmetry, inversion, composition) and structural (e.g., hierarchies) patterns present in knowledge graphs. No single KGE model performs equally well across all pattern types \\cite{gregucci2023}.\n    *   **Importance & Challenge:** Knowledge graphs are inherently incomplete, making link prediction a fundamental task. The challenge lies in developing a unified approach that can leverage the diverse strengths of different KGE models to capture a broader range of patterns, thereby improving the accuracy of predicting missing links \\cite{gregucci2023}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself against individual KGE models (e.g., TransE, RotatE, ComplEx, DistMult, AttH/AttE) by aiming to combine their strengths rather than proposing a new standalone model \\cite{gregucci2023}.\n    *   **Limitations of Previous Solutions:**\n        *   Prior KGE ensemble methods either combine multiple runs of the *same* model (still limited in pattern coverage) or combine *different* models at the score level (e.g., score concatenation, weighted sums, relation-level ensembles) \\cite{gregucci2023}. These methods lack a fine-grained mechanism to dynamically select the most suitable model's representation for a specific query.\n        *   Approaches like MulDE, while combining models, cannot steer decisions towards the specific strengths of individual models but rely on majority guidance \\cite{gregucci2023}.\n        *   Other research combines different *geometric spaces* (e.g., Hyperbolic, Spherical, Euclidean) but does not focus on combining *query representations* from different KGE models \\cite{gregucci2023}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a general framework that integrates query representations from multiple existing KGE models (M) into a unified representation. This combination is achieved through a spherical geometric framework \\cite{gregucci2023}.\n    *   **Novelty/Difference:**\n        *   **Attention Mechanism for Query Combination:** A key innovation is the use of an attention mechanism to dynamically select the \"most suitable model to answer each query\" based on the characteristics of the underlying relation \\cite{gregucci2023}. This allows the framework to adapt to different relational patterns.\n        *   **Multi-Geometric Space Integration:** The model combines query representations in Euclidean space and then projects them onto a non-Euclidean manifold, specifically the Poincar ball, to effectively capture structural patterns like hierarchies, in addition to relational patterns \\cite{gregucci2023}.\n        *   **Spherical Query Embedding:** Each query is represented as a hypersphere, where the center is the combined query embedding and the radius is linked to ranking metrics (Hits@k) and optimized via a loss function \\cite{gregucci2023}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework:** A spherical geometric framework for integrating diverse KGE models by combining their query representations, leveraging their distinct geometric transformations \\cite{gregucci2023}.\n    *   **Adaptive Attention Mechanism:** Introduction of a Riemannian attention-based mechanism that learns to weigh the contributions of different KGE models' query representations based on the specific query's relation, enabling robust handling of heterogeneous relational patterns \\cite{gregucci2023}.\n    *   **Hybrid Geometry for Pattern Learning:** The integration of Euclidean space for query combination with projection onto the Poincar ball (hyperbolic geometry) to simultaneously capture both relational and structural (hierarchical) patterns \\cite{gregucci2023}.\n    *   **Theoretical Insights:** Provides theoretical analyses demonstrating that the combined model offers higher expressiveness and inference power than individual models, and that the combined query embedding lies within the convex hull of individual model queries, ensuring it benefits from their collective strengths \\cite{gregucci2023}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experimental analysis was conducted using various link prediction benchmarks \\cite{gregucci2023}.\n    *   **Key Performance Metrics & Results:** The combined model consistently \"outperforms individual models, including state-of-the-art approaches\" on these benchmarks \\cite{gregucci2023}. While specific datasets and detailed metrics are not in the provided abstract, the connection between the spherical radius and the Hits@k metric is highlighted, implying its use in evaluation \\cite{gregucci2023}.\n\n*   **Limitations & Scope**\n    *   **Technical Assumptions:** The approach assumes that query representations from different models can be mapped to a common space (Euclidean for combination, then projected to hyperbolic) \\cite{gregucci2023}.\n    *   **Scope of Applicability:** The method is specifically designed for link prediction in knowledge graphs, focusing on combining query representations (h,r,?) to predict tail entities.\n    *   **Inherent Trade-off:** Theoretically, for a specific *k*, the combined model's score is bounded by the individual models, meaning it might not always achieve the *absolute best* score of a single, perfectly suited model, but it consistently outperforms the worst and provides a robust average \\cite{gregucci2023}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** The proposed model significantly advances the technical state-of-the-art in link prediction by outperforming individual KGE models and existing ensemble approaches \\cite{gregucci2023}.\n    *   **Enhanced Pattern Learning Capability:** It provides a more comprehensive and adaptive framework for learning diverse relational and structural patterns in knowledge graphs, addressing a critical limitation of prior KGE models \\cite{gregucci2023}.\n    *   **Potential Impact on Future Research:** The attention-based query combination and multi-geometric space integration offer a novel paradigm for combining heterogeneous models, potentially inspiring future research in adaptive model integration for various AI tasks beyond link prediction \\cite{gregucci2023}.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "link prediction",
          "relational and structural patterns",
          "spherical geometric framework",
          "query representations",
          "adaptive attention mechanism",
          "multi-geometric space integration",
          "Poincar ball",
          "hyperbolic geometry",
          "dynamic model selection",
          "enhanced pattern learning",
          "state-of-the-art performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "d9802a67b326fe89bbd761c261937ee1e4d4d674.pdf"
    },
    {
      "success": true,
      "doc_id": "42def5c43397b09976c5716ed5520796",
      "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) methods, primarily based on Euclidean space, struggle to effectively model and represent the inherent hierarchical structures present in knowledge graphs. While hyperbolic embeddings show promise for hierarchical data, they often fail to adequately capture the logical patterns within knowledge graphs \\cite{pan2021}.\n    *   **Importance and Challenge**: Accurately representing hierarchical structures and logical patterns is crucial for robust knowledge graph completion and reasoning. The challenge lies in developing an embedding space that can simultaneously capture both the complex hierarchical relationships and the logical dependencies without sacrificing representation fidelity or conciseness.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the advancements in KGE, specifically addressing the limitations of both Euclidean and existing hyperbolic embedding methods \\cite{pan2021}.\n    *   **Limitations of Previous Solutions**:\n        *   **Euclidean Space Methods**: Inefficient at handling hierarchical structures, leading to suboptimal representations for such data.\n        *   **Hyperbolic Embedding Methods**: While good for hierarchies, they often do not adequately consider or capture the logical patterns inherent in knowledge graphs \\cite{pan2021}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel KGE model that leverages an **extended Poincar Ball** and a **polar coordinate system** within hyperbolic space \\cite{pan2021}.\n    *   **Novelty/Differentiation**:\n        *   **Extended Poincar Ball**: Introduces an extension to the standard Poincar Ball model to better accommodate knowledge graph structures.\n        *   **Polar Coordinate System**: Utilizes a polar coordinate system for optimization and representation within the extended Poincar Ball.\n        *   **Boundary Condition Handling**: Addresses the boundary conditions of the Poincar Ball by stretching and zooming the boundary through expanding the modulus length \\cite{pan2021}.\n        *   **Initialization and Mapping**: Employs tangent space and exponential transformation for initializing and mapping vectors to the Poincar Ball.\n\n*   **Key Technical Contributions**\n    *   **Novel Model**: A new KGE model specifically designed with an extended Poincar Ball and polar coordinate system to capture both hierarchical structures and logical patterns \\cite{pan2021}.\n    *   **Initialization and Mapping Strategy**: Utilizes tangent space and exponential transformation for robust vector initialization and mapping into hyperbolic space.\n    *   **Boundary Condition Solution**: Introduces a method to handle Poincar Ball boundary conditions by expanding the modulus length, effectively stretching and zooming the boundary.\n    *   **Optimization Strategy**: Employs polar coordinates and \"changing operators\" for optimizing the model within the extended Poincar Ball \\cite{pan2021}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The paper states that experiments were conducted, likely on standard KGE tasks such as link prediction.\n    *   **Key Performance Metrics and Comparison Results**: The method achieved \"new state-of-the-art results on part of link prediction tasks,\" demonstrating its effectiveness \\cite{pan2021}.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The method's state-of-the-art performance is noted for \"part of link prediction tasks,\" suggesting its primary strength and current validation scope lie within this specific area \\cite{pan2021}. No explicit technical limitations or assumptions are detailed in the provided abstract.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The proposed model advances the technical state-of-the-art in KGE by providing a more effective way to embed knowledge graphs that contain complex hierarchical structures and logical patterns, outperforming previous methods on certain link prediction tasks \\cite{pan2021}.\n    *   **Potential Impact**: This work opens avenues for future research in designing more sophisticated hyperbolic embedding spaces and optimization techniques that can simultaneously capture diverse structural and logical properties of knowledge graphs, potentially leading to more accurate and robust knowledge graph reasoning and completion systems.",
      "intriguing_abstract": "Accurately representing the complex interplay of hierarchical structures and logical patterns remains a formidable challenge in Knowledge Graph Embedding (KGE). While Euclidean methods falter with hierarchies and existing hyperbolic embeddings often overlook crucial logical dependencies, a fundamental gap persists. We introduce a novel KGE model that bridges this divide by leveraging an **extended Poincar Ball** and a **polar coordinate system** within hyperbolic space.\n\nOur approach innovatively addresses the inherent limitations of hyperbolic embeddings. We propose a unique method to handle Poincar Ball boundary conditions by expanding the modulus length, effectively stretching and zooming the boundary to accommodate diverse knowledge graph structures. Furthermore, we employ tangent space and exponential transformation for robust vector initialization and mapping into this specialized hyperbolic space. This powerful combination allows our model to simultaneously capture both intricate hierarchical relationships and complex logical patterns with unprecedented fidelity. Experimental validation demonstrates that our method achieves new state-of-the-art results on part of link prediction tasks, significantly advancing the field. This work paves the way for designing more sophisticated hyperbolic embedding spaces, promising more accurate and robust knowledge graph reasoning and completion systems.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Hierarchical structures",
        "Logical patterns",
        "Hyperbolic embeddings",
        "Extended Poincar Ball",
        "Polar coordinate system",
        "Boundary condition handling",
        "Tangent space initialization",
        "Exponential transformation mapping",
        "Link prediction",
        "State-of-the-art results",
        "Knowledge graph completion"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/b307e96f59fde63567cd0beb30c9e36d968fad8e.pdf",
      "citation_key": "pan2021",
      "metadata": {
        "title": "Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction",
        "authors": [
          "Zhe Pan",
          "Peng Wang"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding (KGE) using low-dimensional representations to predict missing information is widely applied in knowledge completion. Existing embedding methods are mostly built on Euclidean space, which are difficult to handle hierarchical structures. Hyperbolic embedding methods have shown the promise of high fidelity and concise representation for hierarchical data. However, the logical patterns in knowledge graphs are not considered well in these methods. To address this problem, we propose a novel KGE model with extended Poincar Ball and polar coordinate system to capture hierarchical structures. We use the tangent space and exponential transformation to initialize and map the corresponding vectors to the Poincar Ball in hyperbolic space. To solve the boundary conditions, the boundary is stretched and zoomed by expanding the modulus length in the Poincar Ball. We optimize our model using polar coordinate and changing operators in the extended Poincar Ball. Experiments achieve new state-of-the-art results on part of link prediction tasks, which demonstrates the effectiveness of our method.",
        "file_path": "paper_data/knowledge_graph_embedding/b307e96f59fde63567cd0beb30c9e36d968fad8e.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Knowledge Graph Embedding (KGE) methods, primarily based on Euclidean space, struggle to effectively model and represent the inherent hierarchical structures present in knowledge graphs. While hyperbolic embeddings show promise for hierarchical data, they often fail to adequately capture the logical patterns within knowledge graphs \\cite{pan2021}.\n    *   **Importance and Challenge**: Accurately representing hierarchical structures and logical patterns is crucial for robust knowledge graph completion and reasoning. The challenge lies in developing an embedding space that can simultaneously capture both the complex hierarchical relationships and the logical dependencies without sacrificing representation fidelity or conciseness.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon the advancements in KGE, specifically addressing the limitations of both Euclidean and existing hyperbolic embedding methods \\cite{pan2021}.\n    *   **Limitations of Previous Solutions**:\n        *   **Euclidean Space Methods**: Inefficient at handling hierarchical structures, leading to suboptimal representations for such data.\n        *   **Hyperbolic Embedding Methods**: While good for hierarchies, they often do not adequately consider or capture the logical patterns inherent in knowledge graphs \\cite{pan2021}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel KGE model that leverages an **extended Poincar Ball** and a **polar coordinate system** within hyperbolic space \\cite{pan2021}.\n    *   **Novelty/Differentiation**:\n        *   **Extended Poincar Ball**: Introduces an extension to the standard Poincar Ball model to better accommodate knowledge graph structures.\n        *   **Polar Coordinate System**: Utilizes a polar coordinate system for optimization and representation within the extended Poincar Ball.\n        *   **Boundary Condition Handling**: Addresses the boundary conditions of the Poincar Ball by stretching and zooming the boundary through expanding the modulus length \\cite{pan2021}.\n        *   **Initialization and Mapping**: Employs tangent space and exponential transformation for initializing and mapping vectors to the Poincar Ball.\n\n*   **Key Technical Contributions**\n    *   **Novel Model**: A new KGE model specifically designed with an extended Poincar Ball and polar coordinate system to capture both hierarchical structures and logical patterns \\cite{pan2021}.\n    *   **Initialization and Mapping Strategy**: Utilizes tangent space and exponential transformation for robust vector initialization and mapping into hyperbolic space.\n    *   **Boundary Condition Solution**: Introduces a method to handle Poincar Ball boundary conditions by expanding the modulus length, effectively stretching and zooming the boundary.\n    *   **Optimization Strategy**: Employs polar coordinates and \"changing operators\" for optimizing the model within the extended Poincar Ball \\cite{pan2021}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The paper states that experiments were conducted, likely on standard KGE tasks such as link prediction.\n    *   **Key Performance Metrics and Comparison Results**: The method achieved \"new state-of-the-art results on part of link prediction tasks,\" demonstrating its effectiveness \\cite{pan2021}.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The method's state-of-the-art performance is noted for \"part of link prediction tasks,\" suggesting its primary strength and current validation scope lie within this specific area \\cite{pan2021}. No explicit technical limitations or assumptions are detailed in the provided abstract.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The proposed model advances the technical state-of-the-art in KGE by providing a more effective way to embed knowledge graphs that contain complex hierarchical structures and logical patterns, outperforming previous methods on certain link prediction tasks \\cite{pan2021}.\n    *   **Potential Impact**: This work opens avenues for future research in designing more sophisticated hyperbolic embedding spaces and optimization techniques that can simultaneously capture diverse structural and logical properties of knowledge graphs, potentially leading to more accurate and robust knowledge graph reasoning and completion systems.",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Hierarchical structures",
          "Logical patterns",
          "Hyperbolic embeddings",
          "Extended Poincar Ball",
          "Polar coordinate system",
          "Boundary condition handling",
          "Tangent space initialization",
          "Exponential transformation mapping",
          "Link prediction",
          "State-of-the-art results",
          "Knowledge graph completion"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "b307e96f59fde63567cd0beb30c9e36d968fad8e.pdf"
    },
    {
      "success": true,
      "doc_id": "5ef2297627c846523c3a10ae20fe5559",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Existing translation-based knowledge graph embeddings (KGEs) struggle to precisely represent logical properties of relations, such as transitivity and symmetricity \\cite{yoon2016}.\n    *   This imprecision arises because these models typically ignore the distinct roles entities play within a triple (i.e., as a head or tail entity), leading to a less accurate embedding space \\cite{yoon2016}.\n\n*   **Related Work & Positioning**\n    *   The work builds upon and extends established translation-based KGE models like TransE, TransR, and TransD \\cite{yoon2016}.\n    *   The primary limitation of these previous solutions is their inability to accurately capture and preserve logical properties of relations due to their uniform treatment of entities regardless of their position in a triple \\cite{yoon2016}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the introduction of a **role-specific projection** \\cite{yoon2016}.\n    *   This approach maps an entity to distinct vector representations based on its role in a triple: a head entity is projected by a head projection operator, and a tail entity by a tail projection operator \\cite{yoon2016}.\n    *   This innovation allows entities to have context-dependent embeddings, which is crucial for preserving logical properties.\n\n*   **Key Technical Contributions**\n    *   Novel algorithms: The paper proposes **lppTransE, lppTransR, and lppTransD**, which are enhanced versions of TransE, TransR, and TransD, respectively, incorporating the logical property preserving (lpp) mechanism \\cite{yoon2016}.\n    *   A new conceptual framework for KGEs that emphasizes the importance of entity roles in relation to logical property preservation.\n\n*   **Experimental Validation**\n    *   Experiments were conducted on standard knowledge graph tasks: link prediction and triple classification \\cite{yoon2016}.\n    *   The proposed logical property preserving embeddings (lppTransE, lppTransR, lppTransD) demonstrated state-of-the-art performance on both tasks \\cite{yoon2016}.\n\n*   **Limitations & Scope**\n    *   The proposed method is specifically applied to and validated within the family of translation-based knowledge graph embeddings (TransE, TransR, TransD) \\cite{yoon2016}.\n    *   While effective for these models, its direct applicability or necessary adaptations for other KGE paradigms (e.g., neural network-based, factorization-based) are not explicitly discussed.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by demonstrating that explicitly preserving logical properties of relations is critical for effective knowledge graph embedding \\cite{yoon2016}.\n    *   The proposed role-specific projection method provides an effective mechanism to achieve this, offering a new direction for improving the expressiveness and accuracy of KGE models, particularly for relations with complex logical structures \\cite{yoon2016}.",
      "intriguing_abstract": "Existing translation-based knowledge graph embeddings (KGEs) frequently struggle to precisely capture the intricate logical properties of relations, such as transitivity and symmetricity. This critical limitation arises from their uniform treatment of entities, irrespective of their distinct roles as head or tail within a triple. We introduce a novel conceptual framework and a groundbreaking technical approach: **role-specific projection**. This innovation maps entities to context-dependent vector representations, utilizing distinct head and tail projection operators to accurately reflect their positional significance. Building upon this, we propose **lppTransE, lppTransR, and lppTransD**, enhanced algorithms that explicitly embed logical property preservation. Our extensive experiments on standard knowledge graph tasks, including link prediction and triple classification, demonstrate that these logical property preserving embeddings achieve state-of-the-art performance. This work fundamentally advances the expressiveness and accuracy of KGE models, offering a crucial new direction for understanding and embedding relations with complex logical structures, thereby making knowledge graphs more intelligent and reliable.",
      "keywords": [
        "Knowledge graph embeddings (KGEs)",
        "Translation-based KGEs",
        "Logical properties of relations (transitivity",
        "symmetricity)",
        "Entity roles",
        "Role-specific projection",
        "Context-dependent embeddings",
        "Logical property preserving (lpp) mechanism",
        "lppTransE",
        "lppTransR",
        "lppTransD",
        "Link prediction",
        "Triple classification",
        "State-of-the-art performance",
        "KGE expressiveness and accuracy"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d.pdf",
      "citation_key": "yoon2016",
      "metadata": {
        "title": "A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations",
        "authors": [
          "Hee-Geun Yoon",
          "Hyun-Je Song",
          "Seong-Bae Park",
          "Se-Young Park"
        ],
        "published_date": "2016",
        "abstract": "This paper proposes a novel translation-based knowledge graph embedding that preserves the logical properties of relations such as transitivity and symmetricity. The embedding space generated by existing translation-based embeddings do not represent transitive and symmetric relations precisely, because they ignore the role of entities in triples. Thus, we introduce a role-specific projection which maps an entity to distinct vectors according to its role in a triple. That is, a head entity is projected onto an embedding space by a head projection operator, and a tail entity is projected by a tail projection operator. This idea is applied to TransE, TransR, and TransD to produce lppTransE, lppTransR, and lppTransD, respectively. According to the experimental results on link prediction and triple classification, the proposed logical property preserving embeddings show the state-of-the-art performance at both tasks. These results prove that it is critical to preserve logical properties of relations while embedding knowledge graphs, and the proposed method does it effectively.",
        "file_path": "paper_data/knowledge_graph_embedding/e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Existing translation-based knowledge graph embeddings (KGEs) struggle to precisely represent logical properties of relations, such as transitivity and symmetricity \\cite{yoon2016}.\n    *   This imprecision arises because these models typically ignore the distinct roles entities play within a triple (i.e., as a head or tail entity), leading to a less accurate embedding space \\cite{yoon2016}.\n\n*   **Related Work & Positioning**\n    *   The work builds upon and extends established translation-based KGE models like TransE, TransR, and TransD \\cite{yoon2016}.\n    *   The primary limitation of these previous solutions is their inability to accurately capture and preserve logical properties of relations due to their uniform treatment of entities regardless of their position in a triple \\cite{yoon2016}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the introduction of a **role-specific projection** \\cite{yoon2016}.\n    *   This approach maps an entity to distinct vector representations based on its role in a triple: a head entity is projected by a head projection operator, and a tail entity by a tail projection operator \\cite{yoon2016}.\n    *   This innovation allows entities to have context-dependent embeddings, which is crucial for preserving logical properties.\n\n*   **Key Technical Contributions**\n    *   Novel algorithms: The paper proposes **lppTransE, lppTransR, and lppTransD**, which are enhanced versions of TransE, TransR, and TransD, respectively, incorporating the logical property preserving (lpp) mechanism \\cite{yoon2016}.\n    *   A new conceptual framework for KGEs that emphasizes the importance of entity roles in relation to logical property preservation.\n\n*   **Experimental Validation**\n    *   Experiments were conducted on standard knowledge graph tasks: link prediction and triple classification \\cite{yoon2016}.\n    *   The proposed logical property preserving embeddings (lppTransE, lppTransR, lppTransD) demonstrated state-of-the-art performance on both tasks \\cite{yoon2016}.\n\n*   **Limitations & Scope**\n    *   The proposed method is specifically applied to and validated within the family of translation-based knowledge graph embeddings (TransE, TransR, TransD) \\cite{yoon2016}.\n    *   While effective for these models, its direct applicability or necessary adaptations for other KGE paradigms (e.g., neural network-based, factorization-based) are not explicitly discussed.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by demonstrating that explicitly preserving logical properties of relations is critical for effective knowledge graph embedding \\cite{yoon2016}.\n    *   The proposed role-specific projection method provides an effective mechanism to achieve this, offering a new direction for improving the expressiveness and accuracy of KGE models, particularly for relations with complex logical structures \\cite{yoon2016}.",
        "keywords": [
          "Knowledge graph embeddings (KGEs)",
          "Translation-based KGEs",
          "Logical properties of relations (transitivity",
          "symmetricity)",
          "Entity roles",
          "Role-specific projection",
          "Context-dependent embeddings",
          "Logical property preserving (lpp) mechanism",
          "lppTransE",
          "lppTransR",
          "lppTransD",
          "Link prediction",
          "Triple classification",
          "State-of-the-art performance",
          "KGE expressiveness and accuracy"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d.pdf"
    },
    {
      "success": true,
      "doc_id": "a40bf4ea85610e6a7728380ff0d97e25",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n---\n\n### Analysis of \"Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization\" \\cite{li2024}\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Existing Knowledge Graph Embedding (KGE) models, which rely on orthogonal relation transformations, are limited by \"rigid relational orthogonalization with restricted dimension and homogeneous geometry\" \\cite{li2024}.\n        *   \"Dimension Branch\" models extend Euclidean orthogonal transformations to higher dimensions but are confined to Euclidean geometry.\n        *   \"Geometry Branch\" models leverage hyperbolic isometries for hierarchical structures but are restricted to low-dimensional spaces due to computational complexity.\n        *   All current approaches are designed for *homogeneous* geometry, failing to adequately capture the *topological heterogeneity* of real-world KGs (which can be cyclical in some regions and hierarchical in others) \\cite{li2024}.\n    *   **Motivation:** To develop a KGE framework that generalizes existing approaches in *both dimension and geometry* for orthogonal relation transformations, thereby enabling superior modeling capacity for crucial logical patterns and inherent topological heterogeneity of knowledge graphs \\cite{li2024}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Euclidean (Dimension Branch):** Models like RotatE, Rotate3D, DualE, QuatE, and HousE extend Euclidean orthogonal transformations to higher dimensions (e.g., 2D, 3D, 4D, kD) to capture logical patterns (symmetry, antisymmetry, inversion, composition) and cyclical structures. However, they are confined to Euclidean geometry and do not model hierarchy effectively \\cite{li2024}.\n        *   **Hyperbolic (Geometry Branch):** Models such as RefH, RotH, and AttH leverage hyperbolic isometries, typically in 2D, to preserve hierarchical structures. They also capture logical patterns but are restricted in dimension and less effective for cyclicity \\cite{li2024}.\n    *   **Limitations of Previous Solutions:** None of the existing approaches can simultaneously overcome the restrictions of both dimension and geometry. Dimensional extensions are limited to Euclidean space, while hyperbolic methods are constrained to low dimensions. Crucially, all prior models assume homogeneous geometry, which is insufficient for KGs exhibiting diverse topological characteristics \\cite{li2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces **GoldE** (Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization), a framework featuring a universal orthogonal parameterization based on a generalized form of Householder reflection \\cite{li2024}.\n        *   **Generalized Householder Reflection:** A novel derivation of the Householder matrix in a generalized space equipped with a quadratic inner product `x,yw = xdiag(w)y`, where `w` is a weighting vector. This allows for a unified representation of reflections across Euclidean, elliptic, and hyperbolic geometries \\cite{li2024}.\n        *   **Universal Orthogonal Mapping `Orth(U, w)`:** By treating the generalized Householder reflection as an elementary operator, a mapping `Orth` is designed to parameterize generalized orthogonal transformations. Theorem 3.1 proves that this mapping can completely cover all `k x k` generalized orthogonal matrices, ensuring universality across dimensions and geometries \\cite{li2024}.\n        *   **Elliptic Orthogonal Parameterization:** Relations are modeled as `k`-dimensional elliptic orthogonal transformations using `Orth(Ur, pr)`, where `pr` is a learnable, relation-specific weighting vector. This generalizes Euclidean models by introducing an additional relation-specific scaling operation (Claim 3.2), enabling adaptive adjustment \\cite{li2024}.\n        *   **Hyperbolic Orthogonal Parameterization:** Relations are modeled as `k`-dimensional hyperbolic orthogonal transformations. To ensure transformations remain on the positive sheet of the hyperboloid, the orthogonal matrix is restricted to the positive subgroup `O+q(k)`. This is achieved via a mapping `OrthQ(U, b)` based on polar decomposition (Proposition 3.4), rigorously preserving the hyperbolic space and breaking dimensional restrictions of prior hyperbolic models \\cite{li2024}.\n        *   **Mixed Orthogonal Parameterization:** To address topological heterogeneity, elliptic and hyperbolic parameterizations are integrated within a product manifold. This allows GoldE to simultaneously capture both cyclical and hierarchical structures by leveraging different geometries in different parts of the embedding space \\cite{li2024}.\n    *   **Innovation:** GoldE is the first framework to generalize KGE approaches in *both dimension and geometry* of orthogonal relation transformations \\cite{li2024}. Its universal orthogonal parameterization, based on generalized Householder reflections, provides a theoretically guaranteed method to achieve dimensional extension and geometric unification, offering superior capacity for modeling complex logical patterns and heterogeneous topologies.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Derivation of a generalized Householder reflection based on a quadratic inner product, unifying reflections across different geometries \\cite{li2024}.\n        *   Development of a universal orthogonal mapping `Orth(U, w)` capable of parameterizing orthogonal matrices in Euclidean, elliptic, and hyperbolic spaces of arbitrary dimension `k` \\cite{li2024}.\n        *   Introduction of elliptic orthogonal parameterization for KGE, which generalizes Euclidean models by incorporating relation-specific scaling transformations \\cite{li2024}.\n        *   Introduction of hyperbolic orthogonal parameterization for KGE, which rigorously handles transformations on hyperboloids and breaks dimensional restrictions of prior hyperbolic models \\cite{li2024}.\n        *   Integration of elliptic and hyperbolic parameterizations into a mixed orthogonal parameterization using a product manifold to effectively capture topological heterogeneity \\cite{li2024}.\n    *   **Theoretical Insights/Analysis:**\n        *   **Theorem 3.1:** Proves that the `Orth` mapping completely covers the generalized orthogonal group `Ow(k)`, establishing its universality \\cite{li2024}.\n        *   **Claim 3.2:** Reformulates elliptic parameterization as Euclidean parameterization with element-wise scaling, explaining its adaptive capabilities \\cite{li2024}.\n        *   **Proposition 3.3:** Classifies hyperbolic orthogonal matrices into positive and negative subsets, crucial for ensuring transformations remain on the correct hyperboloid sheet \\cite{li2024}.\n        *   **Proposition 3.4:** Provides a method to express any positive hyperbolic orthogonal matrix using a mapping `OrthQ` based on polar decomposition, enabling robust hyperbolic parameterization \\cite{li2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were conducted on three standard benchmark datasets \\cite{li2024}.\n    *   **Key Performance Metrics:** While not explicitly listed in the provided text, KGE benchmarks typically evaluate link prediction performance using metrics such as Mean Reciprocal Rank (MRR), Hits@N (e.g., Hits@1, Hits@3, Hits@10), and Mean Rank (MR).\n    *   **Comparison Results:** GoldE consistently achieved state-of-the-art performance, outperforming current baselines across all evaluated datasets \\cite{li2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The provided text primarily focuses on the strengths and innovations of GoldE, not its inherent limitations. It assumes that real-world KGs exhibit topological heterogeneity, requiring a mixed-geometry approach.\n    *   **Scope of Applicability:** GoldE is designed for knowledge graph embedding tasks, specifically for link prediction, where it aims to learn expressive representations of entities and relations. It is particularly well-suited for KGs with complex logical patterns and diverse, heterogeneous topological structures \\cite{li2024}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:** GoldE significantly advances the technical state-of-the-art by being the first framework to generalize KGE approaches in *both dimension and geometry* of orthogonal relation transformations \\cite{li2024}. Its empirical results demonstrate superior performance on standard benchmarks.\n    *   **Potential Impact on Future Research:**\n        *   Provides a powerful and flexible foundation for future KGE research, encouraging the exploration of higher-dimensional and mixed-geometry embedding spaces.\n        *   The universal orthogonal parameterization based on generalized Householder reflection could serve as a fundamental tool for other machine learning tasks requiring robust orthogonal transformations in various geometric settings.\n        *   Improved KGE models, like GoldE, can lead to enhanced performance in downstream applications that rely on rich knowledge representations, such as question answering, recommender systems, and semantic search.\n\n---",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) models are fundamentally constrained by rigid relational orthogonal transformations, confined to either restricted dimensions in Euclidean spaces or low-dimensional hyperbolic geometries. Crucially, they fail to capture the inherent *topological heterogeneity* of real-world knowledge graphs, which often exhibit both cyclical and hierarchical structures simultaneously. We introduce **GoldE**, a novel KGE framework that shatters these limitations by presenting the first *universal orthogonal parameterization* capable of generalizing relation transformations in *both dimension and geometry*.\n\nAt its core, GoldE leverages a groundbreaking derivation of the *generalized Householder reflection* based on a quadratic inner product, enabling a unified representation of orthogonal transformations across Euclidean, elliptic, and hyperbolic spaces of arbitrary dimension. This theoretically guaranteed *universal orthogonal mapping* allows for robust *k*-dimensional *elliptic* and *hyperbolic orthogonal parameterizations*. To address complex topological heterogeneity, GoldE integrates these into a *mixed orthogonal parameterization* via a *product manifold*, simultaneously capturing diverse logical patterns. Extensive experiments demonstrate GoldE's state-of-the-art performance on benchmark datasets, offering a powerful foundation for future KGE research and applications requiring expressive, geometrically adaptive knowledge representations.",
      "keywords": [
        "Knowledge Graph Embedding (KGE)",
        "Orthogonal Relation Transformations",
        "Topological Heterogeneity",
        "GoldE Framework",
        "Universal Orthogonal Parameterization",
        "Generalized Householder Reflection",
        "Euclidean Geometry",
        "Hyperbolic Geometry",
        "Elliptic Geometry",
        "Mixed Orthogonal Parameterization",
        "Dimensional Extension",
        "Geometric Unification",
        "Quadratic Inner Product",
        "Product Manifold",
        "State-of-the-art performance"
      ],
      "file_path": "paper_data/knowledge_graph_embedding/c075a84356b529464df2e06a02bf9b524a815152.pdf",
      "citation_key": "li2024",
      "metadata": {
        "title": "Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization",
        "authors": [
          "Rui Li",
          "Chaozhuo Li",
          "Yanming Shen",
          "Zeyu Zhang",
          "Xu Chen"
        ],
        "published_date": "2024",
        "abstract": "Recent advances in knowledge graph embedding (KGE) rely on Euclidean/hyperbolic orthogonal relation transformations to model intrinsic logical patterns and topological structures. However, existing approaches are confined to rigid relational orthogonalization with restricted dimension and homogeneous geometry, leading to deficient modeling capability. In this work, we move beyond these approaches in terms of both dimension and geometry by introducing a powerful framework named GoldE, which features a universal orthogonal parameterization based on a generalized form of Householder reflection. Such parameterization can naturally achieve dimensional extension and geometric unification with theoretical guarantees, enabling our framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity of knowledge graphs. Empirically, GoldE achieves state-of-the-art performance on three standard benchmarks. Codes are available at https://github.com/xxrep/GoldE.",
        "file_path": "paper_data/knowledge_graph_embedding/c075a84356b529464df2e06a02bf9b524a815152.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n---\n\n### Analysis of \"Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization\" \\cite{li2024}\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Existing Knowledge Graph Embedding (KGE) models, which rely on orthogonal relation transformations, are limited by \"rigid relational orthogonalization with restricted dimension and homogeneous geometry\" \\cite{li2024}.\n        *   \"Dimension Branch\" models extend Euclidean orthogonal transformations to higher dimensions but are confined to Euclidean geometry.\n        *   \"Geometry Branch\" models leverage hyperbolic isometries for hierarchical structures but are restricted to low-dimensional spaces due to computational complexity.\n        *   All current approaches are designed for *homogeneous* geometry, failing to adequately capture the *topological heterogeneity* of real-world KGs (which can be cyclical in some regions and hierarchical in others) \\cite{li2024}.\n    *   **Motivation:** To develop a KGE framework that generalizes existing approaches in *both dimension and geometry* for orthogonal relation transformations, thereby enabling superior modeling capacity for crucial logical patterns and inherent topological heterogeneity of knowledge graphs \\cite{li2024}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   **Euclidean (Dimension Branch):** Models like RotatE, Rotate3D, DualE, QuatE, and HousE extend Euclidean orthogonal transformations to higher dimensions (e.g., 2D, 3D, 4D, kD) to capture logical patterns (symmetry, antisymmetry, inversion, composition) and cyclical structures. However, they are confined to Euclidean geometry and do not model hierarchy effectively \\cite{li2024}.\n        *   **Hyperbolic (Geometry Branch):** Models such as RefH, RotH, and AttH leverage hyperbolic isometries, typically in 2D, to preserve hierarchical structures. They also capture logical patterns but are restricted in dimension and less effective for cyclicity \\cite{li2024}.\n    *   **Limitations of Previous Solutions:** None of the existing approaches can simultaneously overcome the restrictions of both dimension and geometry. Dimensional extensions are limited to Euclidean space, while hyperbolic methods are constrained to low dimensions. Crucially, all prior models assume homogeneous geometry, which is insufficient for KGs exhibiting diverse topological characteristics \\cite{li2024}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces **GoldE** (Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization), a framework featuring a universal orthogonal parameterization based on a generalized form of Householder reflection \\cite{li2024}.\n        *   **Generalized Householder Reflection:** A novel derivation of the Householder matrix in a generalized space equipped with a quadratic inner product `x,yw = xdiag(w)y`, where `w` is a weighting vector. This allows for a unified representation of reflections across Euclidean, elliptic, and hyperbolic geometries \\cite{li2024}.\n        *   **Universal Orthogonal Mapping `Orth(U, w)`:** By treating the generalized Householder reflection as an elementary operator, a mapping `Orth` is designed to parameterize generalized orthogonal transformations. Theorem 3.1 proves that this mapping can completely cover all `k x k` generalized orthogonal matrices, ensuring universality across dimensions and geometries \\cite{li2024}.\n        *   **Elliptic Orthogonal Parameterization:** Relations are modeled as `k`-dimensional elliptic orthogonal transformations using `Orth(Ur, pr)`, where `pr` is a learnable, relation-specific weighting vector. This generalizes Euclidean models by introducing an additional relation-specific scaling operation (Claim 3.2), enabling adaptive adjustment \\cite{li2024}.\n        *   **Hyperbolic Orthogonal Parameterization:** Relations are modeled as `k`-dimensional hyperbolic orthogonal transformations. To ensure transformations remain on the positive sheet of the hyperboloid, the orthogonal matrix is restricted to the positive subgroup `O+q(k)`. This is achieved via a mapping `OrthQ(U, b)` based on polar decomposition (Proposition 3.4), rigorously preserving the hyperbolic space and breaking dimensional restrictions of prior hyperbolic models \\cite{li2024}.\n        *   **Mixed Orthogonal Parameterization:** To address topological heterogeneity, elliptic and hyperbolic parameterizations are integrated within a product manifold. This allows GoldE to simultaneously capture both cyclical and hierarchical structures by leveraging different geometries in different parts of the embedding space \\cite{li2024}.\n    *   **Innovation:** GoldE is the first framework to generalize KGE approaches in *both dimension and geometry* of orthogonal relation transformations \\cite{li2024}. Its universal orthogonal parameterization, based on generalized Householder reflections, provides a theoretically guaranteed method to achieve dimensional extension and geometric unification, offering superior capacity for modeling complex logical patterns and heterogeneous topologies.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Derivation of a generalized Householder reflection based on a quadratic inner product, unifying reflections across different geometries \\cite{li2024}.\n        *   Development of a universal orthogonal mapping `Orth(U, w)` capable of parameterizing orthogonal matrices in Euclidean, elliptic, and hyperbolic spaces of arbitrary dimension `k` \\cite{li2024}.\n        *   Introduction of elliptic orthogonal parameterization for KGE, which generalizes Euclidean models by incorporating relation-specific scaling transformations \\cite{li2024}.\n        *   Introduction of hyperbolic orthogonal parameterization for KGE, which rigorously handles transformations on hyperboloids and breaks dimensional restrictions of prior hyperbolic models \\cite{li2024}.\n        *   Integration of elliptic and hyperbolic parameterizations into a mixed orthogonal parameterization using a product manifold to effectively capture topological heterogeneity \\cite{li2024}.\n    *   **Theoretical Insights/Analysis:**\n        *   **Theorem 3.1:** Proves that the `Orth` mapping completely covers the generalized orthogonal group `Ow(k)`, establishing its universality \\cite{li2024}.\n        *   **Claim 3.2:** Reformulates elliptic parameterization as Euclidean parameterization with element-wise scaling, explaining its adaptive capabilities \\cite{li2024}.\n        *   **Proposition 3.3:** Classifies hyperbolic orthogonal matrices into positive and negative subsets, crucial for ensuring transformations remain on the correct hyperboloid sheet \\cite{li2024}.\n        *   **Proposition 3.4:** Provides a method to express any positive hyperbolic orthogonal matrix using a mapping `OrthQ` based on polar decomposition, enabling robust hyperbolic parameterization \\cite{li2024}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were conducted on three standard benchmark datasets \\cite{li2024}.\n    *   **Key Performance Metrics:** While not explicitly listed in the provided text, KGE benchmarks typically evaluate link prediction performance using metrics such as Mean Reciprocal Rank (MRR), Hits@N (e.g., Hits@1, Hits@3, Hits@10), and Mean Rank (MR).\n    *   **Comparison Results:** GoldE consistently achieved state-of-the-art performance, outperforming current baselines across all evaluated datasets \\cite{li2024}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The provided text primarily focuses on the strengths and innovations of GoldE, not its inherent limitations. It assumes that real-world KGs exhibit topological heterogeneity, requiring a mixed-geometry approach.\n    *   **Scope of Applicability:** GoldE is designed for knowledge graph embedding tasks, specifically for link prediction, where it aims to learn expressive representations of entities and relations. It is particularly well-suited for KGs with complex logical patterns and diverse, heterogeneous topological structures \\cite{li2024}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:** GoldE significantly advances the technical state-of-the-art by being the first framework to generalize KGE approaches in *both dimension and geometry* of orthogonal relation transformations \\cite{li2024}. Its empirical results demonstrate superior performance on standard benchmarks.\n    *   **Potential Impact on Future Research:**\n        *   Provides a powerful and flexible foundation for future KGE research, encouraging the exploration of higher-dimensional and mixed-geometry embedding spaces.\n        *   The universal orthogonal parameterization based on generalized Householder reflection could serve as a fundamental tool for other machine learning tasks requiring robust orthogonal transformations in various geometric settings.\n        *   Improved KGE models, like GoldE, can lead to enhanced performance in downstream applications that rely on rich knowledge representations, such as question answering, recommender systems, and semantic search.\n\n---",
        "keywords": [
          "Knowledge Graph Embedding (KGE)",
          "Orthogonal Relation Transformations",
          "Topological Heterogeneity",
          "GoldE Framework",
          "Universal Orthogonal Parameterization",
          "Generalized Householder Reflection",
          "Euclidean Geometry",
          "Hyperbolic Geometry",
          "Elliptic Geometry",
          "Mixed Orthogonal Parameterization",
          "Dimensional Extension",
          "Geometric Unification",
          "Quadratic Inner Product",
          "Product Manifold",
          "State-of-the-art performance"
        ],
        "is_new_direction": "1",
        "paper_type": "technical"
      },
      "file_name": "c075a84356b529464df2e06a02bf9b524a815152.pdf"
    },
    {
      "success": true,
      "doc_id": "a27a8c352a65644838c38b0825648c0d",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b30481dd5467a187b7e1a5a2dd326d97cafd95ac.pdf",
      "citation_key": "xiong2017zqu",
      "metadata": {
        "title": "Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding",
        "authors": [
          "Chenyan Xiong",
          "Russell Power",
          "Jamie Callan"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/b30481dd5467a187b7e1a5a2dd326d97cafd95ac.pdf",
        "venue": "The Web Conference",
        "citationCount": 437,
        "score": 54.625,
        "summary": "",
        "keywords": []
      },
      "file_name": "b30481dd5467a187b7e1a5a2dd326d97cafd95ac.pdf"
    },
    {
      "success": true,
      "doc_id": "221abc429e73ad5a09d62782c3e72621",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2930168f3be575781939a57f4bb92e6b29c33b08.pdf",
      "citation_key": "gong2020b2k",
      "metadata": {
        "title": "SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation",
        "authors": [
          "Fan Gong",
          "Meng Wang",
          "Haofen Wang",
          "Sen Wang",
          "Mengyue Liu"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/2930168f3be575781939a57f4bb92e6b29c33b08.pdf",
        "venue": "Big Data Research",
        "citationCount": 166,
        "score": 33.2,
        "summary": "",
        "keywords": []
      },
      "file_name": "2930168f3be575781939a57f4bb92e6b29c33b08.pdf"
    },
    {
      "success": true,
      "doc_id": "e2a9e29b560983b3be9aadf748b5bd27",
      "summary": "The time series data in the manufacturing process reflects the sequential state of the manufacturing system, and the fusion of temporal features into the industrial knowledge graph will undoubtedly significantly improve the knowledge process efficiency of the manufacturing system. This paper proposes a semantic-aware event link reasoning over an industrial knowledge graph embedding time series data. Its knowledge graph skeleton is constructed through a specific manufacturing process. NLTK is used to transform technical documents into a structured industrial knowledge graph. We employ deep learning (DL)-based models to obtain semantic information related to product quality prediction using time series data collected from IoT devices. Then the prediction information is attached to the specified node in the knowledge graph. Thus, the knowledge graph will describe the dynamic semantic information of manufacturing contexts. Meanwhile, a dynamic event link reasoning model that uses graph embedding to aggregate manufacturing processes information is proposed. The implicit information with industrial temporal knowledge can be further mined and inferred. The case study has shown that the proposed knowledge graph link reasoning reflects dynamic temporal characteristics. Compared to the classical knowledge graph prediction models, our model is superior to the baseline methods.",
      "intriguing_abstract": "The time series data in the manufacturing process reflects the sequential state of the manufacturing system, and the fusion of temporal features into the industrial knowledge graph will undoubtedly significantly improve the knowledge process efficiency of the manufacturing system. This paper proposes a semantic-aware event link reasoning over an industrial knowledge graph embedding time series data. Its knowledge graph skeleton is constructed through a specific manufacturing process. NLTK is used to transform technical documents into a structured industrial knowledge graph. We employ deep learning (DL)-based models to obtain semantic information related to product quality prediction using time series data collected from IoT devices. Then the prediction information is attached to the specified node in the knowledge graph. Thus, the knowledge graph will describe the dynamic semantic information of manufacturing contexts. Meanwhile, a dynamic event link reasoning model that uses graph embedding to aggregate manufacturing processes information is proposed. The implicit information with industrial temporal knowledge can be further mined and inferred. The case study has shown that the proposed knowledge graph link reasoning reflects dynamic temporal characteristics. Compared to the classical knowledge graph prediction models, our model is superior to the baseline methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/da60d33d007681743d939861ae24f4cdac15667e.pdf",
      "citation_key": "zhou2022ehi",
      "metadata": {
        "title": "Semantic-aware event link reasoning over industrial knowledge graph embedding time series data",
        "authors": [
          "Bin Zhou",
          "Xingwang Shen",
          "Yuqian Lu",
          "Xinyu Li",
          "B. Hua",
          "Tianyuan Liu",
          "Jinsong Bao"
        ],
        "published_date": "2022",
        "abstract": "The time series data in the manufacturing process reflects the sequential state of the manufacturing system, and the fusion of temporal features into the industrial knowledge graph will undoubtedly significantly improve the knowledge process efficiency of the manufacturing system. This paper proposes a semantic-aware event link reasoning over an industrial knowledge graph embedding time series data. Its knowledge graph skeleton is constructed through a specific manufacturing process. NLTK is used to transform technical documents into a structured industrial knowledge graph. We employ deep learning (DL)-based models to obtain semantic information related to product quality prediction using time series data collected from IoT devices. Then the prediction information is attached to the specified node in the knowledge graph. Thus, the knowledge graph will describe the dynamic semantic information of manufacturing contexts. Meanwhile, a dynamic event link reasoning model that uses graph embedding to aggregate manufacturing processes information is proposed. The implicit information with industrial temporal knowledge can be further mined and inferred. The case study has shown that the proposed knowledge graph link reasoning reflects dynamic temporal characteristics. Compared to the classical knowledge graph prediction models, our model is superior to the baseline methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/da60d33d007681743d939861ae24f4cdac15667e.pdf",
        "venue": "International Journal of Production Research",
        "citationCount": 78,
        "score": 26.0,
        "summary": "The time series data in the manufacturing process reflects the sequential state of the manufacturing system, and the fusion of temporal features into the industrial knowledge graph will undoubtedly significantly improve the knowledge process efficiency of the manufacturing system. This paper proposes a semantic-aware event link reasoning over an industrial knowledge graph embedding time series data. Its knowledge graph skeleton is constructed through a specific manufacturing process. NLTK is used to transform technical documents into a structured industrial knowledge graph. We employ deep learning (DL)-based models to obtain semantic information related to product quality prediction using time series data collected from IoT devices. Then the prediction information is attached to the specified node in the knowledge graph. Thus, the knowledge graph will describe the dynamic semantic information of manufacturing contexts. Meanwhile, a dynamic event link reasoning model that uses graph embedding to aggregate manufacturing processes information is proposed. The implicit information with industrial temporal knowledge can be further mined and inferred. The case study has shown that the proposed knowledge graph link reasoning reflects dynamic temporal characteristics. Compared to the classical knowledge graph prediction models, our model is superior to the baseline methods.",
        "keywords": []
      },
      "file_name": "da60d33d007681743d939861ae24f4cdac15667e.pdf"
    },
    {
      "success": true,
      "doc_id": "217dc911cb3d2aa053983e62db355247",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/bb65c0898647c57c87a72e80d97a53576e3034ca.pdf",
      "citation_key": "le2022ji8",
      "metadata": {
        "title": "Knowledge graph embedding by relational rotation and complex convolution for link prediction",
        "authors": [
          "Thanh-Binh Le",
          "N. Le",
          "H. Le"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/bb65c0898647c57c87a72e80d97a53576e3034ca.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 74,
        "score": 24.666666666666664,
        "summary": "",
        "keywords": []
      },
      "file_name": "bb65c0898647c57c87a72e80d97a53576e3034ca.pdf"
    },
    {
      "success": true,
      "doc_id": "3a32fae4a6267345311ea2ff734c524b",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c03965d00865074ae66d0324c7145bf59aec73e6.pdf",
      "citation_key": "zhou2022vgb",
      "metadata": {
        "title": "JointE: Jointly utilizing 1D and 2D convolution for knowledge graph embedding",
        "authors": [
          "Zhehui Zhou",
          "Can Wang",
          "Yan Feng",
          "Defang Chen"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/c03965d00865074ae66d0324c7145bf59aec73e6.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 60,
        "score": 20.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "c03965d00865074ae66d0324c7145bf59aec73e6.pdf"
    },
    {
      "success": true,
      "doc_id": "b3c8a53b2490f614f72d57da1feacac0",
      "summary": "In this paper, we propose a new product knowledge graph (PKG) embedding approach for learning the intrinsic product relations as product knowledge for e-commerce. We define the key entities and summarize the pivotal product relations that are critical for general e-commerce applications including marketing, advertisement, search ranking and recommendation. We first provide a comprehensive comparison between PKG and ordinary knowledge graph (KG) and then illustrate why KG embedding methods are not suitable for PKG learning. We construct a self-attention-enhanced distributed representation learning model for learning PKG embeddings from raw customer activity data in an end-to-end fashion. We design an effective multi-task learning schema to fully leverage the multi-modal e-commerce data. The oincare embedding is also employed to handle complex entity structures. We use a real-world dataset from \\textslgrocery.walmart.com to evaluate the performances on knowledge completion, search ranking and recommendation. The proposed approach compares favourably to baselines in knowledge completion and downstream tasks.",
      "intriguing_abstract": "In this paper, we propose a new product knowledge graph (PKG) embedding approach for learning the intrinsic product relations as product knowledge for e-commerce. We define the key entities and summarize the pivotal product relations that are critical for general e-commerce applications including marketing, advertisement, search ranking and recommendation. We first provide a comprehensive comparison between PKG and ordinary knowledge graph (KG) and then illustrate why KG embedding methods are not suitable for PKG learning. We construct a self-attention-enhanced distributed representation learning model for learning PKG embeddings from raw customer activity data in an end-to-end fashion. We design an effective multi-task learning schema to fully leverage the multi-modal e-commerce data. The oincare embedding is also employed to handle complex entity structures. We use a real-world dataset from \\textslgrocery.walmart.com to evaluate the performances on knowledge completion, search ranking and recommendation. The proposed approach compares favourably to baselines in knowledge completion and downstream tasks.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4b0e3d0721ea9324e9950b3bb98d917da8acb222.pdf",
      "citation_key": "xu2019t6b",
      "metadata": {
        "title": "Product Knowledge Graph Embedding for E-commerce",
        "authors": [
          "Da Xu",
          "Chuanwei Ruan",
          "Evren Krpeoglu",
          "Sushant Kumar",
          "Kannan Achan"
        ],
        "published_date": "2019",
        "abstract": "In this paper, we propose a new product knowledge graph (PKG) embedding approach for learning the intrinsic product relations as product knowledge for e-commerce. We define the key entities and summarize the pivotal product relations that are critical for general e-commerce applications including marketing, advertisement, search ranking and recommendation. We first provide a comprehensive comparison between PKG and ordinary knowledge graph (KG) and then illustrate why KG embedding methods are not suitable for PKG learning. We construct a self-attention-enhanced distributed representation learning model for learning PKG embeddings from raw customer activity data in an end-to-end fashion. We design an effective multi-task learning schema to fully leverage the multi-modal e-commerce data. The oincare embedding is also employed to handle complex entity structures. We use a real-world dataset from \\textslgrocery.walmart.com to evaluate the performances on knowledge completion, search ranking and recommendation. The proposed approach compares favourably to baselines in knowledge completion and downstream tasks.",
        "file_path": "paper_data/knowledge_graph_embedding/info/4b0e3d0721ea9324e9950b3bb98d917da8acb222.pdf",
        "venue": "Web Search and Data Mining",
        "citationCount": 109,
        "score": 18.166666666666664,
        "summary": "In this paper, we propose a new product knowledge graph (PKG) embedding approach for learning the intrinsic product relations as product knowledge for e-commerce. We define the key entities and summarize the pivotal product relations that are critical for general e-commerce applications including marketing, advertisement, search ranking and recommendation. We first provide a comprehensive comparison between PKG and ordinary knowledge graph (KG) and then illustrate why KG embedding methods are not suitable for PKG learning. We construct a self-attention-enhanced distributed representation learning model for learning PKG embeddings from raw customer activity data in an end-to-end fashion. We design an effective multi-task learning schema to fully leverage the multi-modal e-commerce data. The oincare embedding is also employed to handle complex entity structures. We use a real-world dataset from \\textslgrocery.walmart.com to evaluate the performances on knowledge completion, search ranking and recommendation. The proposed approach compares favourably to baselines in knowledge completion and downstream tasks.",
        "keywords": []
      },
      "file_name": "4b0e3d0721ea9324e9950b3bb98d917da8acb222.pdf"
    },
    {
      "success": true,
      "doc_id": "5a609fd60bece08bc88298913c8c9fba",
      "summary": "Over two decades, context awareness has been incorporated into recommender systems in order to provide, not only the top-rated items to consumers but also the ones that are suitable to the user context. As a class of context-aware systems, context-aware service recommendation (CASR) aims to bind high-quality services to users, while taking into account their context requirements, including invocation time, location, social profiles, connectivity, and so on. However, current CASR approaches are not scalable with the huge amount of service data (QoS and context information, users reviews and feedbacks). In addition, they lack a rich representation of contextual information, as they adopt a simple matrix view. Moreover, current CASR approaches adopt the traditional user-service relation and they do not allow for multi-relational interactions between users and services in different contexts. To offer a scalable and context-sensitive service recommendation with great analysis and learning capabilities, we provide a rich and multi-relational representation of the CASR knowledge, based on the concept of knowledge graph. The constructed context-aware service knowledge graph (C-SKG) is, then, transformed into a low-dimensional vector space to facilitate its processing. For this purpose, we adopt Dilated Recurrent Neural Networks to propose a context-aware knowledge graph embedding, based on the principles of first-order and subgraph-aware proximity. Finally, a recommendation algorithm is defined to deliver the top-rated services according to the target user's context. Experiments have proved the accuracy and scalability of our solution, compared to state-of-the-art CASR approaches.",
      "intriguing_abstract": "Over two decades, context awareness has been incorporated into recommender systems in order to provide, not only the top-rated items to consumers but also the ones that are suitable to the user context. As a class of context-aware systems, context-aware service recommendation (CASR) aims to bind high-quality services to users, while taking into account their context requirements, including invocation time, location, social profiles, connectivity, and so on. However, current CASR approaches are not scalable with the huge amount of service data (QoS and context information, users reviews and feedbacks). In addition, they lack a rich representation of contextual information, as they adopt a simple matrix view. Moreover, current CASR approaches adopt the traditional user-service relation and they do not allow for multi-relational interactions between users and services in different contexts. To offer a scalable and context-sensitive service recommendation with great analysis and learning capabilities, we provide a rich and multi-relational representation of the CASR knowledge, based on the concept of knowledge graph. The constructed context-aware service knowledge graph (C-SKG) is, then, transformed into a low-dimensional vector space to facilitate its processing. For this purpose, we adopt Dilated Recurrent Neural Networks to propose a context-aware knowledge graph embedding, based on the principles of first-order and subgraph-aware proximity. Finally, a recommendation algorithm is defined to deliver the top-rated services according to the target user's context. Experiments have proved the accuracy and scalability of our solution, compared to state-of-the-art CASR approaches.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/8df10fa4eca07dbb5fe2fe2ecc1e546cb8a8c947.pdf",
      "citation_key": "mezni20218ml",
      "metadata": {
        "title": "Context-Aware Service Recommendation Based on Knowledge Graph Embedding",
        "authors": [
          "Haithem Mezni",
          "D. Benslimane",
          "Ladjel Bellatreche"
        ],
        "published_date": "2021",
        "abstract": "Over two decades, context awareness has been incorporated into recommender systems in order to provide, not only the top-rated items to consumers but also the ones that are suitable to the user context. As a class of context-aware systems, context-aware service recommendation (CASR) aims to bind high-quality services to users, while taking into account their context requirements, including invocation time, location, social profiles, connectivity, and so on. However, current CASR approaches are not scalable with the huge amount of service data (QoS and context information, users reviews and feedbacks). In addition, they lack a rich representation of contextual information, as they adopt a simple matrix view. Moreover, current CASR approaches adopt the traditional user-service relation and they do not allow for multi-relational interactions between users and services in different contexts. To offer a scalable and context-sensitive service recommendation with great analysis and learning capabilities, we provide a rich and multi-relational representation of the CASR knowledge, based on the concept of knowledge graph. The constructed context-aware service knowledge graph (C-SKG) is, then, transformed into a low-dimensional vector space to facilitate its processing. For this purpose, we adopt Dilated Recurrent Neural Networks to propose a context-aware knowledge graph embedding, based on the principles of first-order and subgraph-aware proximity. Finally, a recommendation algorithm is defined to deliver the top-rated services according to the target user's context. Experiments have proved the accuracy and scalability of our solution, compared to state-of-the-art CASR approaches.",
        "file_path": "paper_data/knowledge_graph_embedding/info/8df10fa4eca07dbb5fe2fe2ecc1e546cb8a8c947.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 71,
        "score": 17.75,
        "summary": "Over two decades, context awareness has been incorporated into recommender systems in order to provide, not only the top-rated items to consumers but also the ones that are suitable to the user context. As a class of context-aware systems, context-aware service recommendation (CASR) aims to bind high-quality services to users, while taking into account their context requirements, including invocation time, location, social profiles, connectivity, and so on. However, current CASR approaches are not scalable with the huge amount of service data (QoS and context information, users reviews and feedbacks). In addition, they lack a rich representation of contextual information, as they adopt a simple matrix view. Moreover, current CASR approaches adopt the traditional user-service relation and they do not allow for multi-relational interactions between users and services in different contexts. To offer a scalable and context-sensitive service recommendation with great analysis and learning capabilities, we provide a rich and multi-relational representation of the CASR knowledge, based on the concept of knowledge graph. The constructed context-aware service knowledge graph (C-SKG) is, then, transformed into a low-dimensional vector space to facilitate its processing. For this purpose, we adopt Dilated Recurrent Neural Networks to propose a context-aware knowledge graph embedding, based on the principles of first-order and subgraph-aware proximity. Finally, a recommendation algorithm is defined to deliver the top-rated services according to the target user's context. Experiments have proved the accuracy and scalability of our solution, compared to state-of-the-art CASR approaches.",
        "keywords": []
      },
      "file_name": "8df10fa4eca07dbb5fe2fe2ecc1e546cb8a8c947.pdf"
    },
    {
      "success": true,
      "doc_id": "4ee42754f0039d36dd8148df1c49f794",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/d6cc2a58df29d3e3fe4c55902880908dde32ee60.pdf",
      "citation_key": "do2021mw0",
      "metadata": {
        "title": "Developing a BERT based triple classification model using knowledge graph embedding for question answering system",
        "authors": [
          "P. Do",
          "Truong H. V. Phan"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/d6cc2a58df29d3e3fe4c55902880908dde32ee60.pdf",
        "venue": "Applied intelligence (Boston)",
        "citationCount": 68,
        "score": 17.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "d6cc2a58df29d3e3fe4c55902880908dde32ee60.pdf"
    },
    {
      "success": true,
      "doc_id": "0b0ac5fe642ba5a8e03ab57a7c171d5a",
      "summary": "Learning knowledge graph (KG) embeddings is an emerging technique for a variety of downstream tasks such as summarization, link prediction, information retrieval, and question answering. However, most existing KG embedding models neglect space and, therefore, do not perform well when applied to (geo)spatial data and tasks. Most models that do consider space primarily rely on some notions of distance. These models suffer from higher computational complexity during training while still losing information beyond the relative distance between entities. In this work, we propose a locationaware KG embedding model called SEKGE. It directly encodes spatial information such as point coordinates or bounding boxes of geographic entities into the KG embedding space. The resulting model is capable of handling different types of spatial reasoning. We also construct a geographic knowledge graph as well as a set of geographic queryanswer pairs called DBGeo to evaluate the performance of SEKGE in comparison to multiple baselines. Evaluation results show that SEKGE outperforms these baselines on the DBGeo data set for the geographic logic query answering task. This demonstrates the effectiveness of our spatiallyexplicit model and the importance of considering the scale of different geographic entities. Finally, we introduce a novel downstream task called spatial semantic lifting which links an arbitrary location in the study area to entities in the KG via some relations. Evaluation on DBGeo shows that our model outperforms the baseline by a substantial margin.",
      "intriguing_abstract": "Learning knowledge graph (KG) embeddings is an emerging technique for a variety of downstream tasks such as summarization, link prediction, information retrieval, and question answering. However, most existing KG embedding models neglect space and, therefore, do not perform well when applied to (geo)spatial data and tasks. Most models that do consider space primarily rely on some notions of distance. These models suffer from higher computational complexity during training while still losing information beyond the relative distance between entities. In this work, we propose a locationaware KG embedding model called SEKGE. It directly encodes spatial information such as point coordinates or bounding boxes of geographic entities into the KG embedding space. The resulting model is capable of handling different types of spatial reasoning. We also construct a geographic knowledge graph as well as a set of geographic queryanswer pairs called DBGeo to evaluate the performance of SEKGE in comparison to multiple baselines. Evaluation results show that SEKGE outperforms these baselines on the DBGeo data set for the geographic logic query answering task. This demonstrates the effectiveness of our spatiallyexplicit model and the importance of considering the scale of different geographic entities. Finally, we introduce a novel downstream task called spatial semantic lifting which links an arbitrary location in the study area to entities in the KG via some relations. Evaluation on DBGeo shows that our model outperforms the baseline by a substantial margin.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a57af41c3845a6d15ffbe5bd278e971ca9b8124a.pdf",
      "citation_key": "mai2020ei3",
      "metadata": {
        "title": "SEKGE: A locationaware Knowledge Graph Embedding model for Geographic Question Answering and Spatial Semantic Lifting",
        "authors": [
          "Gengchen Mai",
          "K. Janowicz",
          "Ling Cai",
          "Rui Zhu",
          "Blake Regalia",
          "Bo Yan",
          "Meilin Shi",
          "N. Lao"
        ],
        "published_date": "2020",
        "abstract": "Learning knowledge graph (KG) embeddings is an emerging technique for a variety of downstream tasks such as summarization, link prediction, information retrieval, and question answering. However, most existing KG embedding models neglect space and, therefore, do not perform well when applied to (geo)spatial data and tasks. Most models that do consider space primarily rely on some notions of distance. These models suffer from higher computational complexity during training while still losing information beyond the relative distance between entities. In this work, we propose a locationaware KG embedding model called SEKGE. It directly encodes spatial information such as point coordinates or bounding boxes of geographic entities into the KG embedding space. The resulting model is capable of handling different types of spatial reasoning. We also construct a geographic knowledge graph as well as a set of geographic queryanswer pairs called DBGeo to evaluate the performance of SEKGE in comparison to multiple baselines. Evaluation results show that SEKGE outperforms these baselines on the DBGeo data set for the geographic logic query answering task. This demonstrates the effectiveness of our spatiallyexplicit model and the importance of considering the scale of different geographic entities. Finally, we introduce a novel downstream task called spatial semantic lifting which links an arbitrary location in the study area to entities in the KG via some relations. Evaluation on DBGeo shows that our model outperforms the baseline by a substantial margin.",
        "file_path": "paper_data/knowledge_graph_embedding/info/a57af41c3845a6d15ffbe5bd278e971ca9b8124a.pdf",
        "venue": "Trans. GIS",
        "citationCount": 84,
        "score": 16.8,
        "summary": "Learning knowledge graph (KG) embeddings is an emerging technique for a variety of downstream tasks such as summarization, link prediction, information retrieval, and question answering. However, most existing KG embedding models neglect space and, therefore, do not perform well when applied to (geo)spatial data and tasks. Most models that do consider space primarily rely on some notions of distance. These models suffer from higher computational complexity during training while still losing information beyond the relative distance between entities. In this work, we propose a locationaware KG embedding model called SEKGE. It directly encodes spatial information such as point coordinates or bounding boxes of geographic entities into the KG embedding space. The resulting model is capable of handling different types of spatial reasoning. We also construct a geographic knowledge graph as well as a set of geographic queryanswer pairs called DBGeo to evaluate the performance of SEKGE in comparison to multiple baselines. Evaluation results show that SEKGE outperforms these baselines on the DBGeo data set for the geographic logic query answering task. This demonstrates the effectiveness of our spatiallyexplicit model and the importance of considering the scale of different geographic entities. Finally, we introduce a novel downstream task called spatial semantic lifting which links an arbitrary location in the study area to entities in the KG via some relations. Evaluation on DBGeo shows that our model outperforms the baseline by a substantial margin.",
        "keywords": []
      },
      "file_name": "a57af41c3845a6d15ffbe5bd278e971ca9b8124a.pdf"
    },
    {
      "success": true,
      "doc_id": "81b2f63e9b95fdb7155723df11ef6855",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf",
      "citation_key": "zhang2022eab",
      "metadata": {
        "title": "Knowledge graph embedding by logical-default attention graph convolution neural network for link prediction",
        "authors": [
          "Jiarui Zhang",
          "Jian Huang",
          "Jialong Gao",
          "Runhai Han",
          "Cong Zhou"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf",
        "venue": "Information Sciences",
        "citationCount": 50,
        "score": 16.666666666666664,
        "summary": "",
        "keywords": []
      },
      "file_name": "8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf"
    },
    {
      "success": true,
      "doc_id": "83a079b4ddb3c562295dc7ec436a07fc",
      "summary": "One in ten people are affected by rare diseases, and three out of ten children with rare diseases will not live past age five. However, the small market size of individual rare diseases, combined with the time and capital requirements of pharmaceutical R&D, have hindered the development of new drugs for these cases. A promising alternative is drug repurposing, whereby existing FDA-approved drugs might be used to treat diseases different from their original indications. In order to generate drug repurposing hypotheses in a systematic and comprehensive fashion, it is essential to integrate information from across the literature of pharmacology, genetics, and pathology. To this end, we leverage a newly developed knowledge graph, the Global Network of Biomedical Relationships (GNBR). GNBR is a large, heterogeneous knowledge graph comprising drug, disease, and gene (or protein) entities linked by a small set of semantic themes derived from the abstracts of biomedical literature. We apply a knowledge graph embedding method that explicitly models the uncertainty associated with literature-derived relationships and uses link prediction to generate drug repurposing hypotheses. This approach achieves high performance on a gold-standard test set of known drug indications (AUROC = 0.89) and is capable of generating novel repurposing hypotheses, which we independently validate using external literature sources and protein interaction networks. Finally, we demonstrate the ability of our model to produce explanations of its predictions.",
      "intriguing_abstract": "One in ten people are affected by rare diseases, and three out of ten children with rare diseases will not live past age five. However, the small market size of individual rare diseases, combined with the time and capital requirements of pharmaceutical R&D, have hindered the development of new drugs for these cases. A promising alternative is drug repurposing, whereby existing FDA-approved drugs might be used to treat diseases different from their original indications. In order to generate drug repurposing hypotheses in a systematic and comprehensive fashion, it is essential to integrate information from across the literature of pharmacology, genetics, and pathology. To this end, we leverage a newly developed knowledge graph, the Global Network of Biomedical Relationships (GNBR). GNBR is a large, heterogeneous knowledge graph comprising drug, disease, and gene (or protein) entities linked by a small set of semantic themes derived from the abstracts of biomedical literature. We apply a knowledge graph embedding method that explicitly models the uncertainty associated with literature-derived relationships and uses link prediction to generate drug repurposing hypotheses. This approach achieves high performance on a gold-standard test set of known drug indications (AUROC = 0.89) and is capable of generating novel repurposing hypotheses, which we independently validate using external literature sources and protein interaction networks. Finally, we demonstrate the ability of our model to produce explanations of its predictions.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/23ae48cdb8b7985e5a32fc79b6aae0de3230fe4f.pdf",
      "citation_key": "sosa2019ih0",
      "metadata": {
        "title": "A Literature-Based Knowledge Graph Embedding Method for Identifying Drug Repurposing Opportunities in Rare Diseases",
        "authors": [
          "Daniel N. Sosa",
          "Alexander Derry",
          "Margaret Guo",
          "Eric Wei",
          "Connor Brinton",
          "R. Altman"
        ],
        "published_date": "2019",
        "abstract": "One in ten people are affected by rare diseases, and three out of ten children with rare diseases will not live past age five. However, the small market size of individual rare diseases, combined with the time and capital requirements of pharmaceutical R&D, have hindered the development of new drugs for these cases. A promising alternative is drug repurposing, whereby existing FDA-approved drugs might be used to treat diseases different from their original indications. In order to generate drug repurposing hypotheses in a systematic and comprehensive fashion, it is essential to integrate information from across the literature of pharmacology, genetics, and pathology. To this end, we leverage a newly developed knowledge graph, the Global Network of Biomedical Relationships (GNBR). GNBR is a large, heterogeneous knowledge graph comprising drug, disease, and gene (or protein) entities linked by a small set of semantic themes derived from the abstracts of biomedical literature. We apply a knowledge graph embedding method that explicitly models the uncertainty associated with literature-derived relationships and uses link prediction to generate drug repurposing hypotheses. This approach achieves high performance on a gold-standard test set of known drug indications (AUROC = 0.89) and is capable of generating novel repurposing hypotheses, which we independently validate using external literature sources and protein interaction networks. Finally, we demonstrate the ability of our model to produce explanations of its predictions.",
        "file_path": "paper_data/knowledge_graph_embedding/info/23ae48cdb8b7985e5a32fc79b6aae0de3230fe4f.pdf",
        "venue": "bioRxiv",
        "citationCount": 97,
        "score": 16.166666666666664,
        "summary": "One in ten people are affected by rare diseases, and three out of ten children with rare diseases will not live past age five. However, the small market size of individual rare diseases, combined with the time and capital requirements of pharmaceutical R&D, have hindered the development of new drugs for these cases. A promising alternative is drug repurposing, whereby existing FDA-approved drugs might be used to treat diseases different from their original indications. In order to generate drug repurposing hypotheses in a systematic and comprehensive fashion, it is essential to integrate information from across the literature of pharmacology, genetics, and pathology. To this end, we leverage a newly developed knowledge graph, the Global Network of Biomedical Relationships (GNBR). GNBR is a large, heterogeneous knowledge graph comprising drug, disease, and gene (or protein) entities linked by a small set of semantic themes derived from the abstracts of biomedical literature. We apply a knowledge graph embedding method that explicitly models the uncertainty associated with literature-derived relationships and uses link prediction to generate drug repurposing hypotheses. This approach achieves high performance on a gold-standard test set of known drug indications (AUROC = 0.89) and is capable of generating novel repurposing hypotheses, which we independently validate using external literature sources and protein interaction networks. Finally, we demonstrate the ability of our model to produce explanations of its predictions.",
        "keywords": []
      },
      "file_name": "23ae48cdb8b7985e5a32fc79b6aae0de3230fe4f.pdf"
    },
    {
      "success": true,
      "doc_id": "64a5e89215eec28fbaf47c066e215e1d",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/87ccb0d6c3e9f6367cd753538f4e906838cea8c2.pdf",
      "citation_key": "guan2019pr4",
      "metadata": {
        "title": "Knowledge graph embedding with concepts",
        "authors": [
          "Niannian Guan",
          "Dandan Song",
          "L. Liao"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/87ccb0d6c3e9f6367cd753538f4e906838cea8c2.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 97,
        "score": 16.166666666666664,
        "summary": "",
        "keywords": []
      },
      "file_name": "87ccb0d6c3e9f6367cd753538f4e906838cea8c2.pdf"
    },
    {
      "success": true,
      "doc_id": "8bd83cb9f58230332edebc3fa0e6d02c",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/0dddf37145689e5f2899f8081d9971882e6ff1e9.pdf",
      "citation_key": "fan2014g7s",
      "metadata": {
        "title": "Transition-based Knowledge Graph Embedding with Relational Mapping Properties",
        "authors": [
          "M. Fan",
          "Qiang Zhou",
          "E. Chang",
          "T. Zheng"
        ],
        "published_date": "2014",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/0dddf37145689e5f2899f8081d9971882e6ff1e9.pdf",
        "venue": "Pacific Asia Conference on Language, Information and Computation",
        "citationCount": 172,
        "score": 15.636363636363637,
        "summary": "",
        "keywords": []
      },
      "file_name": "0dddf37145689e5f2899f8081d9971882e6ff1e9.pdf"
    },
    {
      "success": true,
      "doc_id": "be28379ba2c3af7f8d93bbcb7037fd1a",
      "summary": "Knowledge graph embedding (KGE) is a technique for learning continuous embeddings for entities and relations in the knowledge graph. Due to its benefit to a variety of downstream tasks such as knowledge graph completion, question answering and recommendation, KGE has gained significant attention recently. Despite its effectiveness in a benign environment, KGE's robustness to adversarial attacks is not well-studied. Existing attack methods on graph data cannot be directly applied to attack the embeddings of knowledge graph due to its heterogeneity. To fill this gap, we propose a collection of data poisoning attack strategies, which can effectively manipulate the plausibility of arbitrary targeted facts in a knowledge graph by adding or deleting facts on the graph. The effectiveness and efficiency of the proposed attack strategies are verified by extensive evaluations on two widely-used benchmarks.",
      "intriguing_abstract": "Knowledge graph embedding (KGE) is a technique for learning continuous embeddings for entities and relations in the knowledge graph. Due to its benefit to a variety of downstream tasks such as knowledge graph completion, question answering and recommendation, KGE has gained significant attention recently. Despite its effectiveness in a benign environment, KGE's robustness to adversarial attacks is not well-studied. Existing attack methods on graph data cannot be directly applied to attack the embeddings of knowledge graph due to its heterogeneity. To fill this gap, we propose a collection of data poisoning attack strategies, which can effectively manipulate the plausibility of arbitrary targeted facts in a knowledge graph by adding or deleting facts on the graph. The effectiveness and efficiency of the proposed attack strategies are verified by extensive evaluations on two widely-used benchmarks.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4be29e1cd866ab31f83f03723e2f307cdc1faab0.pdf",
      "citation_key": "zhang20190zu",
      "metadata": {
        "title": "Data Poisoning Attack against Knowledge Graph Embedding",
        "authors": [
          "Hengtong Zhang",
          "T. Zheng",
          "Jing Gao",
          "Chenglin Miao",
          "Lu Su",
          "Yaliang Li",
          "K. Ren"
        ],
        "published_date": "2019",
        "abstract": "Knowledge graph embedding (KGE) is a technique for learning continuous embeddings for entities and relations in the knowledge graph. Due to its benefit to a variety of downstream tasks such as knowledge graph completion, question answering and recommendation, KGE has gained significant attention recently. Despite its effectiveness in a benign environment, KGE's robustness to adversarial attacks is not well-studied. Existing attack methods on graph data cannot be directly applied to attack the embeddings of knowledge graph due to its heterogeneity. To fill this gap, we propose a collection of data poisoning attack strategies, which can effectively manipulate the plausibility of arbitrary targeted facts in a knowledge graph by adding or deleting facts on the graph. The effectiveness and efficiency of the proposed attack strategies are verified by extensive evaluations on two widely-used benchmarks.",
        "file_path": "paper_data/knowledge_graph_embedding/info/4be29e1cd866ab31f83f03723e2f307cdc1faab0.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 88,
        "score": 14.666666666666666,
        "summary": "Knowledge graph embedding (KGE) is a technique for learning continuous embeddings for entities and relations in the knowledge graph. Due to its benefit to a variety of downstream tasks such as knowledge graph completion, question answering and recommendation, KGE has gained significant attention recently. Despite its effectiveness in a benign environment, KGE's robustness to adversarial attacks is not well-studied. Existing attack methods on graph data cannot be directly applied to attack the embeddings of knowledge graph due to its heterogeneity. To fill this gap, we propose a collection of data poisoning attack strategies, which can effectively manipulate the plausibility of arbitrary targeted facts in a knowledge graph by adding or deleting facts on the graph. The effectiveness and efficiency of the proposed attack strategies are verified by extensive evaluations on two widely-used benchmarks.",
        "keywords": []
      },
      "file_name": "4be29e1cd866ab31f83f03723e2f307cdc1faab0.pdf"
    },
    {
      "success": true,
      "doc_id": "29acba8979e5d4e7599386d548aee65d",
      "summary": "The idea of citizen sensing and human as sensors is crucial for social Internet of Things, an integral part of cyberphysicalsocial systems (CPSSs). Social media data, which can be easily collected from the social world, has become a valuable resource for research in many different disciplines, e.g., crisis/disaster assessment, social event detection, or the recent COVID-19 analysis. Useful information, or knowledge derived from social data, could better serve the public if it could be processed and analyzed in more efficient and reliable ways. Advances in deep neural networks have significantly improved the performance of many social media analysis tasks. However, deep learning models typically require a large amount of labeled data for model training, while most CPSS data is not labeled, making it impractical to build effective learning models using traditional approaches. In addition, the current state-of-the-art, pretrained natural language processing (NLP) models do not make use of existing knowledge graphs, thus often leading to unsatisfactory performance in real-world applications. To address the issues, we propose a new zero-shot learning method which makes effective use of existing knowledge graphs for the classification of very large amounts of social text data. Experiments were performed on a large, real-world tweet data set related to COVID-19, the evaluation results show that the proposed method significantly outperforms six baseline models implemented with state-of-the-art deep learning models for NLP.",
      "intriguing_abstract": "The idea of citizen sensing and human as sensors is crucial for social Internet of Things, an integral part of cyberphysicalsocial systems (CPSSs). Social media data, which can be easily collected from the social world, has become a valuable resource for research in many different disciplines, e.g., crisis/disaster assessment, social event detection, or the recent COVID-19 analysis. Useful information, or knowledge derived from social data, could better serve the public if it could be processed and analyzed in more efficient and reliable ways. Advances in deep neural networks have significantly improved the performance of many social media analysis tasks. However, deep learning models typically require a large amount of labeled data for model training, while most CPSS data is not labeled, making it impractical to build effective learning models using traditional approaches. In addition, the current state-of-the-art, pretrained natural language processing (NLP) models do not make use of existing knowledge graphs, thus often leading to unsatisfactory performance in real-world applications. To address the issues, we propose a new zero-shot learning method which makes effective use of existing knowledge graphs for the classification of very large amounts of social text data. Experiments were performed on a large, real-world tweet data set related to COVID-19, the evaluation results show that the proposed method significantly outperforms six baseline models implemented with state-of-the-art deep learning models for NLP.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2a81032e5bb4b29f6e1423b6083b9a04bb54b605.pdf",
      "citation_key": "chen2022mxn",
      "metadata": {
        "title": "Zero-Shot Text Classification via Knowledge Graph Embedding for Social Media Data",
        "authors": [
          "Qi Chen",
          "Wei Wang",
          "Kaizhu Huang",
          "Frans Coenen"
        ],
        "published_date": "2022",
        "abstract": "The idea of citizen sensing and human as sensors is crucial for social Internet of Things, an integral part of cyberphysicalsocial systems (CPSSs). Social media data, which can be easily collected from the social world, has become a valuable resource for research in many different disciplines, e.g., crisis/disaster assessment, social event detection, or the recent COVID-19 analysis. Useful information, or knowledge derived from social data, could better serve the public if it could be processed and analyzed in more efficient and reliable ways. Advances in deep neural networks have significantly improved the performance of many social media analysis tasks. However, deep learning models typically require a large amount of labeled data for model training, while most CPSS data is not labeled, making it impractical to build effective learning models using traditional approaches. In addition, the current state-of-the-art, pretrained natural language processing (NLP) models do not make use of existing knowledge graphs, thus often leading to unsatisfactory performance in real-world applications. To address the issues, we propose a new zero-shot learning method which makes effective use of existing knowledge graphs for the classification of very large amounts of social text data. Experiments were performed on a large, real-world tweet data set related to COVID-19, the evaluation results show that the proposed method significantly outperforms six baseline models implemented with state-of-the-art deep learning models for NLP.",
        "file_path": "paper_data/knowledge_graph_embedding/info/2a81032e5bb4b29f6e1423b6083b9a04bb54b605.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 42,
        "score": 14.0,
        "summary": "The idea of citizen sensing and human as sensors is crucial for social Internet of Things, an integral part of cyberphysicalsocial systems (CPSSs). Social media data, which can be easily collected from the social world, has become a valuable resource for research in many different disciplines, e.g., crisis/disaster assessment, social event detection, or the recent COVID-19 analysis. Useful information, or knowledge derived from social data, could better serve the public if it could be processed and analyzed in more efficient and reliable ways. Advances in deep neural networks have significantly improved the performance of many social media analysis tasks. However, deep learning models typically require a large amount of labeled data for model training, while most CPSS data is not labeled, making it impractical to build effective learning models using traditional approaches. In addition, the current state-of-the-art, pretrained natural language processing (NLP) models do not make use of existing knowledge graphs, thus often leading to unsatisfactory performance in real-world applications. To address the issues, we propose a new zero-shot learning method which makes effective use of existing knowledge graphs for the classification of very large amounts of social text data. Experiments were performed on a large, real-world tweet data set related to COVID-19, the evaluation results show that the proposed method significantly outperforms six baseline models implemented with state-of-the-art deep learning models for NLP.",
        "keywords": []
      },
      "file_name": "2a81032e5bb4b29f6e1423b6083b9a04bb54b605.pdf"
    },
    {
      "success": true,
      "doc_id": "dd2efe093fdbe783dc8522e71b33b633",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf",
      "citation_key": "wang2022hwx",
      "metadata": {
        "title": "Temporal knowledge graph embedding via sparse transfer matrix",
        "authors": [
          "Xin Wang",
          "Shengfei Lyu",
          "Xiangyu Wang",
          "Xingyu Wu",
          "Huanhuan Chen"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf",
        "venue": "Information Sciences",
        "citationCount": 41,
        "score": 13.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf"
    },
    {
      "success": true,
      "doc_id": "0298e1d53c63061c8d96f5ed6886f93c",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/d97ec8a07cea1a18edf0a20981aad7e3dfe351e6.pdf",
      "citation_key": "chen20226e4",
      "metadata": {
        "title": "Federated knowledge graph completion via embedding-contrastive learning",
        "authors": [
          "Mingyang Chen",
          "Wen Zhang",
          "Zonggang Yuan",
          "Yantao Jia",
          "Hua-zeng Chen"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/d97ec8a07cea1a18edf0a20981aad7e3dfe351e6.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 40,
        "score": 13.333333333333332,
        "summary": "",
        "keywords": []
      },
      "file_name": "d97ec8a07cea1a18edf0a20981aad7e3dfe351e6.pdf"
    },
    {
      "success": true,
      "doc_id": "a82c5f89bc6c95d8db2edd5c359b753c",
      "summary": "Knowledge Graphs (KGs) have gained considerable attention recently from both academia and industry. In fact, incorporating graph technology and the copious of various graph datasets have led the research community to build sophisticated graph analytics tools, which has extended the application of KGs to tackle a plethora of real-life problems in dissimilar domains. Despite the abundance of the currently proliferated generic KGs, there is a vital need to construct domain-specific KGs. Further, quality and credibility should be assimilated in the process of constructing and augmenting KGs, particularly those propagated from mixed-quality resources such as social media data. For example, the amount of the political discourses in social media is overwhelming yet can be hijacked and misused by spammers to spread misinformation and false news. This paper presents a novel credibility domain-based KG Embedding framework. This framework involves capturing a fusion of data related to politics domain and obtained from heterogeneous resources into a formal KG representation depicted by a politics domain ontology. The proposed approach makes use of various knowledge-based repositories to enrich the semantics of the textual contents, thereby facilitating the interoperability of information. The proposed framework also embodies a domain-based social credibility module to ensure data quality and trustworthiness. The utility of the proposed framework is verified by means of experiments conducted on two constructed KGs. The KGs are then embedded in low-dimensional semantically-continuous space using several embedding techniques. The effectiveness of embedding techniques and social credibility module is further demonstrated and substantiated on link prediction, clustering, and visualisation tasks.",
      "intriguing_abstract": "Knowledge Graphs (KGs) have gained considerable attention recently from both academia and industry. In fact, incorporating graph technology and the copious of various graph datasets have led the research community to build sophisticated graph analytics tools, which has extended the application of KGs to tackle a plethora of real-life problems in dissimilar domains. Despite the abundance of the currently proliferated generic KGs, there is a vital need to construct domain-specific KGs. Further, quality and credibility should be assimilated in the process of constructing and augmenting KGs, particularly those propagated from mixed-quality resources such as social media data. For example, the amount of the political discourses in social media is overwhelming yet can be hijacked and misused by spammers to spread misinformation and false news. This paper presents a novel credibility domain-based KG Embedding framework. This framework involves capturing a fusion of data related to politics domain and obtained from heterogeneous resources into a formal KG representation depicted by a politics domain ontology. The proposed approach makes use of various knowledge-based repositories to enrich the semantics of the textual contents, thereby facilitating the interoperability of information. The proposed framework also embodies a domain-based social credibility module to ensure data quality and trustworthiness. The utility of the proposed framework is verified by means of experiments conducted on two constructed KGs. The KGs are then embedded in low-dimensional semantically-continuous space using several embedding techniques. The effectiveness of embedding techniques and social credibility module is further demonstrated and substantiated on link prediction, clustering, and visualisation tasks.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/389935511c395526817cf4ae62dae8913845ebdf.pdf",
      "citation_key": "abusalih2020gdu",
      "metadata": {
        "title": "Relational Learning Analysis of Social Politics using Knowledge Graph Embedding",
        "authors": [
          "Bilal Abu-Salih",
          "Marwan Al-Tawil",
          "Ibrahim Aljarah",
          "Hossam Faris",
          "P. Wongthongtham",
          "Kit Yan Chan",
          "A. Beheshti"
        ],
        "published_date": "2020",
        "abstract": "Knowledge Graphs (KGs) have gained considerable attention recently from both academia and industry. In fact, incorporating graph technology and the copious of various graph datasets have led the research community to build sophisticated graph analytics tools, which has extended the application of KGs to tackle a plethora of real-life problems in dissimilar domains. Despite the abundance of the currently proliferated generic KGs, there is a vital need to construct domain-specific KGs. Further, quality and credibility should be assimilated in the process of constructing and augmenting KGs, particularly those propagated from mixed-quality resources such as social media data. For example, the amount of the political discourses in social media is overwhelming yet can be hijacked and misused by spammers to spread misinformation and false news. This paper presents a novel credibility domain-based KG Embedding framework. This framework involves capturing a fusion of data related to politics domain and obtained from heterogeneous resources into a formal KG representation depicted by a politics domain ontology. The proposed approach makes use of various knowledge-based repositories to enrich the semantics of the textual contents, thereby facilitating the interoperability of information. The proposed framework also embodies a domain-based social credibility module to ensure data quality and trustworthiness. The utility of the proposed framework is verified by means of experiments conducted on two constructed KGs. The KGs are then embedded in low-dimensional semantically-continuous space using several embedding techniques. The effectiveness of embedding techniques and social credibility module is further demonstrated and substantiated on link prediction, clustering, and visualisation tasks.",
        "file_path": "paper_data/knowledge_graph_embedding/info/389935511c395526817cf4ae62dae8913845ebdf.pdf",
        "venue": "Data mining and knowledge discovery",
        "citationCount": 64,
        "score": 12.8,
        "summary": "Knowledge Graphs (KGs) have gained considerable attention recently from both academia and industry. In fact, incorporating graph technology and the copious of various graph datasets have led the research community to build sophisticated graph analytics tools, which has extended the application of KGs to tackle a plethora of real-life problems in dissimilar domains. Despite the abundance of the currently proliferated generic KGs, there is a vital need to construct domain-specific KGs. Further, quality and credibility should be assimilated in the process of constructing and augmenting KGs, particularly those propagated from mixed-quality resources such as social media data. For example, the amount of the political discourses in social media is overwhelming yet can be hijacked and misused by spammers to spread misinformation and false news. This paper presents a novel credibility domain-based KG Embedding framework. This framework involves capturing a fusion of data related to politics domain and obtained from heterogeneous resources into a formal KG representation depicted by a politics domain ontology. The proposed approach makes use of various knowledge-based repositories to enrich the semantics of the textual contents, thereby facilitating the interoperability of information. The proposed framework also embodies a domain-based social credibility module to ensure data quality and trustworthiness. The utility of the proposed framework is verified by means of experiments conducted on two constructed KGs. The KGs are then embedded in low-dimensional semantically-continuous space using several embedding techniques. The effectiveness of embedding techniques and social credibility module is further demonstrated and substantiated on link prediction, clustering, and visualisation tasks.",
        "keywords": []
      },
      "file_name": "389935511c395526817cf4ae62dae8913845ebdf.pdf"
    },
    {
      "success": true,
      "doc_id": "c33db47de45075d5f68359feb35a93f0",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/ba524aa0ae24971b56eef6e92491de07d097a233.pdf",
      "citation_key": "fang2022wp6",
      "metadata": {
        "title": "Learning knowledge graph embedding with a dual-attention embedding network",
        "authors": [
          "Haichuan Fang",
          "Youwei Wang",
          "Zhen Tian",
          "Yangdong Ye"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/ba524aa0ae24971b56eef6e92491de07d097a233.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 38,
        "score": 12.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "ba524aa0ae24971b56eef6e92491de07d097a233.pdf"
    },
    {
      "success": true,
      "doc_id": "0e85c0a7de26901ddb5b7e5f5a4c7ed6",
      "summary": "Current approaches to identifying drug-drug interactions (DDIs), include safety studies during drug development and post-marketing surveillance after approval, offer important opportunities to identify potential safety issues, but are unable to provide complete set of all possible DDIs. Thus, the drug discovery researchers and healthcare professionals might not be fully aware of potentially dangerous DDIs. Predicting potential drug-drug interaction helps reduce unanticipated drug interactions and drug development costs and optimizes the drug design process. Methods for prediction of DDIs have the tendency to report high accuracy but still have little impact on translational research due to systematic biases induced by networked/paired data. In this work, we aimed to present realistic evaluation settings to predict DDIs using knowledge graph embeddings. We propose a simple disjoint cross-validation scheme to evaluate drug-drug interaction predictions for the scenarios where the drugs have no known DDIs. We designed different evaluation settings to accurately assess the performance for predicting DDIs. The settings for disjoint cross-validation produced lower performance scores, as expected, but still were good at predicting the drug interactions. We have applied Logistic Regression, Naive Bayes and Random Forest on DrugBank knowledge graph with the 10-fold traditional cross validation using RDF2Vec, TransE and TransD. RDF2Vec with Skip-Gram generally surpasses other embedding methods. We also tested RDF2Vec on various drug knowledge graphs such as DrugBank, PharmGKB and KEGG to predict unknown drug-drug interactions. The performance was not enhanced significantly when an integrated knowledge graph including these three datasets was used. We showed that the knowledge embeddings are powerful predictors and comparable to current state-of-the-art methods for inferring new DDIs. We addressed the evaluation biases by introducing drug-wise and pairwise disjoint test classes. Although the performance scores for drug-wise and pairwise disjoint seem to be low, the results can be considered to be realistic in predicting the interactions for drugs with limited interaction information.",
      "intriguing_abstract": "Current approaches to identifying drug-drug interactions (DDIs), include safety studies during drug development and post-marketing surveillance after approval, offer important opportunities to identify potential safety issues, but are unable to provide complete set of all possible DDIs. Thus, the drug discovery researchers and healthcare professionals might not be fully aware of potentially dangerous DDIs. Predicting potential drug-drug interaction helps reduce unanticipated drug interactions and drug development costs and optimizes the drug design process. Methods for prediction of DDIs have the tendency to report high accuracy but still have little impact on translational research due to systematic biases induced by networked/paired data. In this work, we aimed to present realistic evaluation settings to predict DDIs using knowledge graph embeddings. We propose a simple disjoint cross-validation scheme to evaluate drug-drug interaction predictions for the scenarios where the drugs have no known DDIs. We designed different evaluation settings to accurately assess the performance for predicting DDIs. The settings for disjoint cross-validation produced lower performance scores, as expected, but still were good at predicting the drug interactions. We have applied Logistic Regression, Naive Bayes and Random Forest on DrugBank knowledge graph with the 10-fold traditional cross validation using RDF2Vec, TransE and TransD. RDF2Vec with Skip-Gram generally surpasses other embedding methods. We also tested RDF2Vec on various drug knowledge graphs such as DrugBank, PharmGKB and KEGG to predict unknown drug-drug interactions. The performance was not enhanced significantly when an integrated knowledge graph including these three datasets was used. We showed that the knowledge embeddings are powerful predictors and comparable to current state-of-the-art methods for inferring new DDIs. We addressed the evaluation biases by introducing drug-wise and pairwise disjoint test classes. Although the performance scores for drug-wise and pairwise disjoint seem to be low, the results can be considered to be realistic in predicting the interactions for drugs with limited interaction information.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a264af122f9f2ea5df46c030beb8ec0c25d6e907.pdf",
      "citation_key": "elebi2019bzc",
      "metadata": {
        "title": "Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction in realistic settings",
        "authors": [
          "R. elebi",
          "Hseyin Uyar",
          "Erkan Yasar",
          "Ozgur Gumus",
          "Oguz Dikenelli",
          "M. Dumontier"
        ],
        "published_date": "2019",
        "abstract": "Current approaches to identifying drug-drug interactions (DDIs), include safety studies during drug development and post-marketing surveillance after approval, offer important opportunities to identify potential safety issues, but are unable to provide complete set of all possible DDIs. Thus, the drug discovery researchers and healthcare professionals might not be fully aware of potentially dangerous DDIs. Predicting potential drug-drug interaction helps reduce unanticipated drug interactions and drug development costs and optimizes the drug design process. Methods for prediction of DDIs have the tendency to report high accuracy but still have little impact on translational research due to systematic biases induced by networked/paired data. In this work, we aimed to present realistic evaluation settings to predict DDIs using knowledge graph embeddings. We propose a simple disjoint cross-validation scheme to evaluate drug-drug interaction predictions for the scenarios where the drugs have no known DDIs. We designed different evaluation settings to accurately assess the performance for predicting DDIs. The settings for disjoint cross-validation produced lower performance scores, as expected, but still were good at predicting the drug interactions. We have applied Logistic Regression, Naive Bayes and Random Forest on DrugBank knowledge graph with the 10-fold traditional cross validation using RDF2Vec, TransE and TransD. RDF2Vec with Skip-Gram generally surpasses other embedding methods. We also tested RDF2Vec on various drug knowledge graphs such as DrugBank, PharmGKB and KEGG to predict unknown drug-drug interactions. The performance was not enhanced significantly when an integrated knowledge graph including these three datasets was used. We showed that the knowledge embeddings are powerful predictors and comparable to current state-of-the-art methods for inferring new DDIs. We addressed the evaluation biases by introducing drug-wise and pairwise disjoint test classes. Although the performance scores for drug-wise and pairwise disjoint seem to be low, the results can be considered to be realistic in predicting the interactions for drugs with limited interaction information.",
        "file_path": "paper_data/knowledge_graph_embedding/info/a264af122f9f2ea5df46c030beb8ec0c25d6e907.pdf",
        "venue": "BMC Bioinformatics",
        "citationCount": 75,
        "score": 12.5,
        "summary": "Current approaches to identifying drug-drug interactions (DDIs), include safety studies during drug development and post-marketing surveillance after approval, offer important opportunities to identify potential safety issues, but are unable to provide complete set of all possible DDIs. Thus, the drug discovery researchers and healthcare professionals might not be fully aware of potentially dangerous DDIs. Predicting potential drug-drug interaction helps reduce unanticipated drug interactions and drug development costs and optimizes the drug design process. Methods for prediction of DDIs have the tendency to report high accuracy but still have little impact on translational research due to systematic biases induced by networked/paired data. In this work, we aimed to present realistic evaluation settings to predict DDIs using knowledge graph embeddings. We propose a simple disjoint cross-validation scheme to evaluate drug-drug interaction predictions for the scenarios where the drugs have no known DDIs. We designed different evaluation settings to accurately assess the performance for predicting DDIs. The settings for disjoint cross-validation produced lower performance scores, as expected, but still were good at predicting the drug interactions. We have applied Logistic Regression, Naive Bayes and Random Forest on DrugBank knowledge graph with the 10-fold traditional cross validation using RDF2Vec, TransE and TransD. RDF2Vec with Skip-Gram generally surpasses other embedding methods. We also tested RDF2Vec on various drug knowledge graphs such as DrugBank, PharmGKB and KEGG to predict unknown drug-drug interactions. The performance was not enhanced significantly when an integrated knowledge graph including these three datasets was used. We showed that the knowledge embeddings are powerful predictors and comparable to current state-of-the-art methods for inferring new DDIs. We addressed the evaluation biases by introducing drug-wise and pairwise disjoint test classes. Although the performance scores for drug-wise and pairwise disjoint seem to be low, the results can be considered to be realistic in predicting the interactions for drugs with limited interaction information.",
        "keywords": []
      },
      "file_name": "a264af122f9f2ea5df46c030beb8ec0c25d6e907.pdf"
    },
    {
      "success": true,
      "doc_id": "65d881d0bbce7f3dde10a9aee3ae9ab4",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/90450fe686c0fa645a1954950adffc5b2401e4b7.pdf",
      "citation_key": "sha2019i3a",
      "metadata": {
        "title": "Hierarchical attentive knowledge graph embedding for personalized recommendation",
        "authors": [
          "Xiao Sha",
          "Zhu Sun",
          "Jie Zhang"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/90450fe686c0fa645a1954950adffc5b2401e4b7.pdf",
        "venue": "Electronic Commerce Research and Applications",
        "citationCount": 71,
        "score": 11.833333333333332,
        "summary": "",
        "keywords": []
      },
      "file_name": "90450fe686c0fa645a1954950adffc5b2401e4b7.pdf"
    },
    {
      "success": true,
      "doc_id": "446c98ce4708f6b096c81f3169899b8a",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2257eb642e9ecae24f455a58dc807ee2a843081f.pdf",
      "citation_key": "li2021ro5",
      "metadata": {
        "title": "Recalibration convolutional networks for learning interaction knowledge graph embedding",
        "authors": [
          "Zhifei Li",
          "Hai Liu",
          "Zhaoli Zhang",
          "Tingting Liu",
          "Jiangbo Shu"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/2257eb642e9ecae24f455a58dc807ee2a843081f.pdf",
        "venue": "Neurocomputing",
        "citationCount": 47,
        "score": 11.75,
        "summary": "",
        "keywords": []
      },
      "file_name": "2257eb642e9ecae24f455a58dc807ee2a843081f.pdf"
    },
    {
      "success": true,
      "doc_id": "7a38caee3949fcb12a331d9942353587",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/d77de3a4ddfa62f8105c0591fd41e549edcfd95f.pdf",
      "citation_key": "xiao20151fj",
      "metadata": {
        "title": "TransG : A Generative Mixture Model for Knowledge Graph Embedding",
        "authors": [
          "Han Xiao",
          "Minlie Huang",
          "Yu Hao",
          "Xiaoyan Zhu"
        ],
        "published_date": "2015",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/d77de3a4ddfa62f8105c0591fd41e549edcfd95f.pdf",
        "venue": "arXiv.org",
        "citationCount": 117,
        "score": 11.700000000000001,
        "summary": "",
        "keywords": []
      },
      "file_name": "d77de3a4ddfa62f8105c0591fd41e549edcfd95f.pdf"
    },
    {
      "success": true,
      "doc_id": "be828e3ebb2c578572427e1c7e8480ca",
      "summary": "Background Adverse drug reactions (ADRs) are an important concern in the medication process and can pose a substantial economic burden for patients and hospitals. Because of the limitations of clinical trials, it is difficult to identify all possible ADRs of a drug before it is marketed. We developed a new model based on data mining technology to predict potential ADRs based on available drug data. Method Based on the Word2Vec model in Nature Language Processing, we propose a new knowledge graph embedding method that embeds drugs and ADRs into their respective vectors and builds a logistic regression classification model to predict whether a given drug will have ADRs. Result First, a new knowledge graph embedding method was proposed, and comparison with similar studies showed that our model not only had high prediction accuracy but also was simpler in model structure. In our experiments, the AUC of the classification model reached a maximum of 0.87, and the mean AUC was 0.863. Conclusion In this paper, we introduce a new method to embed knowledge graph to vectorize drugs and ADRs, then use a logistic regression classification model to predict whether there is a causal relationship between them. The experiment showed that the use of knowledge graph embedding can effectively encode drugs and ADRs. And the proposed ADRs prediction system is also very effective.",
      "intriguing_abstract": "Background Adverse drug reactions (ADRs) are an important concern in the medication process and can pose a substantial economic burden for patients and hospitals. Because of the limitations of clinical trials, it is difficult to identify all possible ADRs of a drug before it is marketed. We developed a new model based on data mining technology to predict potential ADRs based on available drug data. Method Based on the Word2Vec model in Nature Language Processing, we propose a new knowledge graph embedding method that embeds drugs and ADRs into their respective vectors and builds a logistic regression classification model to predict whether a given drug will have ADRs. Result First, a new knowledge graph embedding method was proposed, and comparison with similar studies showed that our model not only had high prediction accuracy but also was simpler in model structure. In our experiments, the AUC of the classification model reached a maximum of 0.87, and the mean AUC was 0.863. Conclusion In this paper, we introduce a new method to embed knowledge graph to vectorize drugs and ADRs, then use a logistic regression classification model to predict whether there is a causal relationship between them. The experiment showed that the use of knowledge graph embedding can effectively encode drugs and ADRs. And the proposed ADRs prediction system is also very effective.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/52457f574780c53c68ad645fcdc86e2492b5074a.pdf",
      "citation_key": "zhang2021wg7",
      "metadata": {
        "title": "Prediction of adverse drug reactions based on knowledge graph embedding",
        "authors": [
          "Fei Zhang",
          "Bo Sun",
          "Xiaolin Diao",
          "Wei Zhao",
          "Ting Shu"
        ],
        "published_date": "2021",
        "abstract": "Background Adverse drug reactions (ADRs) are an important concern in the medication process and can pose a substantial economic burden for patients and hospitals. Because of the limitations of clinical trials, it is difficult to identify all possible ADRs of a drug before it is marketed. We developed a new model based on data mining technology to predict potential ADRs based on available drug data. Method Based on the Word2Vec model in Nature Language Processing, we propose a new knowledge graph embedding method that embeds drugs and ADRs into their respective vectors and builds a logistic regression classification model to predict whether a given drug will have ADRs. Result First, a new knowledge graph embedding method was proposed, and comparison with similar studies showed that our model not only had high prediction accuracy but also was simpler in model structure. In our experiments, the AUC of the classification model reached a maximum of 0.87, and the mean AUC was 0.863. Conclusion In this paper, we introduce a new method to embed knowledge graph to vectorize drugs and ADRs, then use a logistic regression classification model to predict whether there is a causal relationship between them. The experiment showed that the use of knowledge graph embedding can effectively encode drugs and ADRs. And the proposed ADRs prediction system is also very effective.",
        "file_path": "paper_data/knowledge_graph_embedding/info/52457f574780c53c68ad645fcdc86e2492b5074a.pdf",
        "venue": "BMC Medical Informatics and Decision Making",
        "citationCount": 46,
        "score": 11.5,
        "summary": "Background Adverse drug reactions (ADRs) are an important concern in the medication process and can pose a substantial economic burden for patients and hospitals. Because of the limitations of clinical trials, it is difficult to identify all possible ADRs of a drug before it is marketed. We developed a new model based on data mining technology to predict potential ADRs based on available drug data. Method Based on the Word2Vec model in Nature Language Processing, we propose a new knowledge graph embedding method that embeds drugs and ADRs into their respective vectors and builds a logistic regression classification model to predict whether a given drug will have ADRs. Result First, a new knowledge graph embedding method was proposed, and comparison with similar studies showed that our model not only had high prediction accuracy but also was simpler in model structure. In our experiments, the AUC of the classification model reached a maximum of 0.87, and the mean AUC was 0.863. Conclusion In this paper, we introduce a new method to embed knowledge graph to vectorize drugs and ADRs, then use a logistic regression classification model to predict whether there is a causal relationship between them. The experiment showed that the use of knowledge graph embedding can effectively encode drugs and ADRs. And the proposed ADRs prediction system is also very effective.",
        "keywords": []
      },
      "file_name": "52457f574780c53c68ad645fcdc86e2492b5074a.pdf"
    },
    {
      "success": true,
      "doc_id": "f0ff2370af25858e64014be03967bcd4",
      "summary": "Distant supervision is an effective method to generate large scale labeled data for relation extraction, which assumes that if a pair of entities appears in some relation of a Knowledge Graph (KG), all sentences containing those entities in a large unlabeled corpus are then labeled with that relation to train a relation classifier. However, when the pair of entities has multiple relationships in the KG, this assumption may produce noisy relation labels. This paper proposes a label-free distant supervision method, which makes no use of the relation labels under this inadequate assumption, but only uses the prior knowledge derived from the KG to supervise the learning of the classifier directly and softly. Specifically, we make use of the type information and the translation law derived from typical KG embedding model to learn embeddings for certain sentence patterns. As the supervision signal is only determined by the two aligned entities, neither hard relation labels nor extra noise-reduction model for the bag of sentences is needed in this way. The experiments show that the approach performs well in current distant supervision dataset.",
      "intriguing_abstract": "Distant supervision is an effective method to generate large scale labeled data for relation extraction, which assumes that if a pair of entities appears in some relation of a Knowledge Graph (KG), all sentences containing those entities in a large unlabeled corpus are then labeled with that relation to train a relation classifier. However, when the pair of entities has multiple relationships in the KG, this assumption may produce noisy relation labels. This paper proposes a label-free distant supervision method, which makes no use of the relation labels under this inadequate assumption, but only uses the prior knowledge derived from the KG to supervise the learning of the classifier directly and softly. Specifically, we make use of the type information and the translation law derived from typical KG embedding model to learn embeddings for certain sentence patterns. As the supervision signal is only determined by the two aligned entities, neither hard relation labels nor extra noise-reduction model for the bag of sentences is needed in this way. The experiments show that the approach performs well in current distant supervision dataset.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/ac79b551ca16f98c1c3a5592c22d8093a492c4f3.pdf",
      "citation_key": "wang20186zs",
      "metadata": {
        "title": "Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding",
        "authors": [
          "Guanying Wang",
          "Wen Zhang",
          "Ruoxu Wang",
          "Yalin Zhou",
          "Xi Chen",
          "Wei Zhang",
          "Hai Zhu",
          "Huajun Chen"
        ],
        "published_date": "2018",
        "abstract": "Distant supervision is an effective method to generate large scale labeled data for relation extraction, which assumes that if a pair of entities appears in some relation of a Knowledge Graph (KG), all sentences containing those entities in a large unlabeled corpus are then labeled with that relation to train a relation classifier. However, when the pair of entities has multiple relationships in the KG, this assumption may produce noisy relation labels. This paper proposes a label-free distant supervision method, which makes no use of the relation labels under this inadequate assumption, but only uses the prior knowledge derived from the KG to supervise the learning of the classifier directly and softly. Specifically, we make use of the type information and the translation law derived from typical KG embedding model to learn embeddings for certain sentence patterns. As the supervision signal is only determined by the two aligned entities, neither hard relation labels nor extra noise-reduction model for the bag of sentences is needed in this way. The experiments show that the approach performs well in current distant supervision dataset.",
        "file_path": "paper_data/knowledge_graph_embedding/info/ac79b551ca16f98c1c3a5592c22d8093a492c4f3.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 80,
        "score": 11.428571428571427,
        "summary": "Distant supervision is an effective method to generate large scale labeled data for relation extraction, which assumes that if a pair of entities appears in some relation of a Knowledge Graph (KG), all sentences containing those entities in a large unlabeled corpus are then labeled with that relation to train a relation classifier. However, when the pair of entities has multiple relationships in the KG, this assumption may produce noisy relation labels. This paper proposes a label-free distant supervision method, which makes no use of the relation labels under this inadequate assumption, but only uses the prior knowledge derived from the KG to supervise the learning of the classifier directly and softly. Specifically, we make use of the type information and the translation law derived from typical KG embedding model to learn embeddings for certain sentence patterns. As the supervision signal is only determined by the two aligned entities, neither hard relation labels nor extra noise-reduction model for the bag of sentences is needed in this way. The experiments show that the approach performs well in current distant supervision dataset.",
        "keywords": []
      },
      "file_name": "ac79b551ca16f98c1c3a5592c22d8093a492c4f3.pdf"
    },
    {
      "success": true,
      "doc_id": "b99915cae4427113579355a52443f70c",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/0abee37fe165b86753b306ffcc59a77e89de0599.pdf",
      "citation_key": "li2021x10",
      "metadata": {
        "title": "Achieving cognitive mass personalization via the self-X cognitive manufacturing network: An industrial-knowledge-graph- and graph-embedding-enabled pathway",
        "authors": [
          "Xinyu Li",
          "P. Zheng",
          "Jinsong Bao",
          "Liang Gao",
          "Xun Xu"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/0abee37fe165b86753b306ffcc59a77e89de0599.pdf",
        "venue": "Engineering",
        "citationCount": 45,
        "score": 11.25,
        "summary": "",
        "keywords": []
      },
      "file_name": "0abee37fe165b86753b306ffcc59a77e89de0599.pdf"
    },
    {
      "success": true,
      "doc_id": "76c6cb0934370007e6e759dfeecc41ca",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/512177d6b1e643b49b1d5ab1ad389666750144a9.pdf",
      "citation_key": "wang202110w",
      "metadata": {
        "title": "A novel knowledge graph embedding based API recommendation method for Mashup development",
        "authors": [
          "Xin Wang",
          "Xiao Liu",
          "Jin Liu",
          "Xiaomei Chen",
          "Hao Wu"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/512177d6b1e643b49b1d5ab1ad389666750144a9.pdf",
        "venue": "World wide web (Bussum)",
        "citationCount": 42,
        "score": 10.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "512177d6b1e643b49b1d5ab1ad389666750144a9.pdf"
    },
    {
      "success": true,
      "doc_id": "2b97ed8e4302c7c1dddc12c2b25ff507",
      "summary": "Recent years have witnessed the successful application of low-dimensional vector space representations of knowledge graphs to predict missing facts or find erroneous ones. However, it is not yet well-understood to what extent ontological knowledge, e.g. given as a set of (existential) rules, can be embedded in a principled way. To address this shortcoming, in this paper we introduce a general framework based on a view of relations as regions, which allows us to study the compatibility between ontological knowledge and different types of vector space embeddings. Our technical contribution is two-fold. First, we show that some of the most popular existing embedding methods are not capable of modelling even very simple types of rules, which in particular also means that they are not able to learn the type of dependencies captured by such rules. Second, we study a model in which relations are modelled as convex regions. We show particular that ontologies which are expressed using so-called quasi-chained existential rules can be exactly represented using convex regions, such that any set of facts which is induced using that vector space embedding is logically consistent and deductively closed with respect to the input ontology.",
      "intriguing_abstract": "Recent years have witnessed the successful application of low-dimensional vector space representations of knowledge graphs to predict missing facts or find erroneous ones. However, it is not yet well-understood to what extent ontological knowledge, e.g. given as a set of (existential) rules, can be embedded in a principled way. To address this shortcoming, in this paper we introduce a general framework based on a view of relations as regions, which allows us to study the compatibility between ontological knowledge and different types of vector space embeddings. Our technical contribution is two-fold. First, we show that some of the most popular existing embedding methods are not capable of modelling even very simple types of rules, which in particular also means that they are not able to learn the type of dependencies captured by such rules. Second, we study a model in which relations are modelled as convex regions. We show particular that ontologies which are expressed using so-called quasi-chained existential rules can be exactly represented using convex regions, such that any set of facts which is induced using that vector space embedding is logically consistent and deductively closed with respect to the input ontology.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/60347869db7d1940958ee465b3010b3a612bf791.pdf",
      "citation_key": "gutirrezbasulto2018oi0",
      "metadata": {
        "title": "From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules",
        "authors": [
          "Vctor Gutirrez-Basulto",
          "S. Schockaert"
        ],
        "published_date": "2018",
        "abstract": "Recent years have witnessed the successful application of low-dimensional vector space representations of knowledge graphs to predict missing facts or find erroneous ones. However, it is not yet well-understood to what extent ontological knowledge, e.g. given as a set of (existential) rules, can be embedded in a principled way. To address this shortcoming, in this paper we introduce a general framework based on a view of relations as regions, which allows us to study the compatibility between ontological knowledge and different types of vector space embeddings. Our technical contribution is two-fold. First, we show that some of the most popular existing embedding methods are not capable of modelling even very simple types of rules, which in particular also means that they are not able to learn the type of dependencies captured by such rules. Second, we study a model in which relations are modelled as convex regions. We show particular that ontologies which are expressed using so-called quasi-chained existential rules can be exactly represented using convex regions, such that any set of facts which is induced using that vector space embedding is logically consistent and deductively closed with respect to the input ontology.",
        "file_path": "paper_data/knowledge_graph_embedding/info/60347869db7d1940958ee465b3010b3a612bf791.pdf",
        "venue": "International Conference on Principles of Knowledge Representation and Reasoning",
        "citationCount": 72,
        "score": 10.285714285714285,
        "summary": "Recent years have witnessed the successful application of low-dimensional vector space representations of knowledge graphs to predict missing facts or find erroneous ones. However, it is not yet well-understood to what extent ontological knowledge, e.g. given as a set of (existential) rules, can be embedded in a principled way. To address this shortcoming, in this paper we introduce a general framework based on a view of relations as regions, which allows us to study the compatibility between ontological knowledge and different types of vector space embeddings. Our technical contribution is two-fold. First, we show that some of the most popular existing embedding methods are not capable of modelling even very simple types of rules, which in particular also means that they are not able to learn the type of dependencies captured by such rules. Second, we study a model in which relations are modelled as convex regions. We show particular that ontologies which are expressed using so-called quasi-chained existential rules can be exactly represented using convex regions, such that any set of facts which is induced using that vector space embedding is logically consistent and deductively closed with respect to the input ontology.",
        "keywords": []
      },
      "file_name": "60347869db7d1940958ee465b3010b3a612bf791.pdf"
    },
    {
      "success": true,
      "doc_id": "1a8f5279a81f6ff03cdf3715947c31f7",
      "summary": "Knowledge Graph Embeddings, i.e., projections of entities and relations to lower dimensional spaces, have been proposed for two purposes: (1)providing an encoding for data mining tasks, and (2)predicting links in a knowledge graph. Both lines of research have been pursued rather in isolation from each other so far, each with their own benchmarks and evaluation methodologies. In this paper, we argue that both tasks are actually related, and we show that the first family of approaches can also be used for the second task and vice versa. In two series of experiments, we provide a comparison of both families of approaches on both tasks, which, to the best of our knowledge, has not been done so far. Furthermore, we discuss the differences in the similarity functions evoked by the different embedding approaches.",
      "intriguing_abstract": "Knowledge Graph Embeddings, i.e., projections of entities and relations to lower dimensional spaces, have been proposed for two purposes: (1)providing an encoding for data mining tasks, and (2)predicting links in a knowledge graph. Both lines of research have been pursued rather in isolation from each other so far, each with their own benchmarks and evaluation methodologies. In this paper, we argue that both tasks are actually related, and we show that the first family of approaches can also be used for the second task and vice versa. In two series of experiments, we provide a comparison of both families of approaches on both tasks, which, to the best of our knowledge, has not been done so far. Furthermore, we discuss the differences in the similarity functions evoked by the different embedding approaches.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9f7731d72e2aa251d2994eb1729c22aa78d0f718.pdf",
      "citation_key": "portisch20221rd",
      "metadata": {
        "title": "Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction - two sides of the same coin?",
        "authors": [
          "Jan Portisch",
          "Nicolas Heist",
          "Heiko Paulheim"
        ],
        "published_date": "2022",
        "abstract": "Knowledge Graph Embeddings, i.e., projections of entities and relations to lower dimensional spaces, have been proposed for two purposes: (1)providing an encoding for data mining tasks, and (2)predicting links in a knowledge graph. Both lines of research have been pursued rather in isolation from each other so far, each with their own benchmarks and evaluation methodologies. In this paper, we argue that both tasks are actually related, and we show that the first family of approaches can also be used for the second task and vice versa. In two series of experiments, we provide a comparison of both families of approaches on both tasks, which, to the best of our knowledge, has not been done so far. Furthermore, we discuss the differences in the similarity functions evoked by the different embedding approaches.",
        "file_path": "paper_data/knowledge_graph_embedding/info/9f7731d72e2aa251d2994eb1729c22aa78d0f718.pdf",
        "venue": "Semantic Web",
        "citationCount": 29,
        "score": 9.666666666666666,
        "summary": "Knowledge Graph Embeddings, i.e., projections of entities and relations to lower dimensional spaces, have been proposed for two purposes: (1)providing an encoding for data mining tasks, and (2)predicting links in a knowledge graph. Both lines of research have been pursued rather in isolation from each other so far, each with their own benchmarks and evaluation methodologies. In this paper, we argue that both tasks are actually related, and we show that the first family of approaches can also be used for the second task and vice versa. In two series of experiments, we provide a comparison of both families of approaches on both tasks, which, to the best of our knowledge, has not been done so far. Furthermore, we discuss the differences in the similarity functions evoked by the different embedding approaches.",
        "keywords": []
      },
      "file_name": "9f7731d72e2aa251d2994eb1729c22aa78d0f718.pdf"
    },
    {
      "success": true,
      "doc_id": "a8284332ccf6b09dcf5e48d640fdada8",
      "summary": "Recent years have witnessed remarkable progress on knowledge graph embedding (KGE) methods to learn the representations of entities and relations in static knowledge graphs (SKGs). However, knowledge changes over time. In order to represent the facts happening in a specific time, temporal knowledge graph (TKG) embedding approaches are put forward. While most existing models ignore the independence of semantic and temporal information. We empirically find that current models have difficulty distinguishing representations of the same entity or relation at different timestamps. In this regard, we propose a TimeLine-Traced Knowledge Graph Embedding method (TLT-KGE) for temporal knowledge graph completion. TLT-KGE aims to embed the entities and relations with timestamps as a complex vector or a quaternion vector. Specifically, TLT-KGE models semantic information and temporal information as different axes of complex number space or quaternion space. Meanwhile, two specific components carving the relationship between semantic and temporal information are devised to buoy the modeling. In this way, the proposed method can not only distinguish the independence of the semantic and temporal information, but also establish a connection between them. Experimental results on the link prediction task demonstrate that TLT-KGE achieves substantial improvements over state-of-the-art competitors. The source code will be available on https://github.com/zhangfw123/TLT-KGE.",
      "intriguing_abstract": "Recent years have witnessed remarkable progress on knowledge graph embedding (KGE) methods to learn the representations of entities and relations in static knowledge graphs (SKGs). However, knowledge changes over time. In order to represent the facts happening in a specific time, temporal knowledge graph (TKG) embedding approaches are put forward. While most existing models ignore the independence of semantic and temporal information. We empirically find that current models have difficulty distinguishing representations of the same entity or relation at different timestamps. In this regard, we propose a TimeLine-Traced Knowledge Graph Embedding method (TLT-KGE) for temporal knowledge graph completion. TLT-KGE aims to embed the entities and relations with timestamps as a complex vector or a quaternion vector. Specifically, TLT-KGE models semantic information and temporal information as different axes of complex number space or quaternion space. Meanwhile, two specific components carving the relationship between semantic and temporal information are devised to buoy the modeling. In this way, the proposed method can not only distinguish the independence of the semantic and temporal information, but also establish a connection between them. Experimental results on the link prediction task demonstrate that TLT-KGE achieves substantial improvements over state-of-the-art competitors. The source code will be available on https://github.com/zhangfw123/TLT-KGE.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a.pdf",
      "citation_key": "zhang2022muu",
      "metadata": {
        "title": "Along the Time: Timeline-traced Embedding for Temporal Knowledge Graph Completion",
        "authors": [
          "Fuwei Zhang",
          "Zhao Zhang",
          "Xiang Ao",
          "Fuzhen Zhuang",
          "Yongjun Xu",
          "Qing He"
        ],
        "published_date": "2022",
        "abstract": "Recent years have witnessed remarkable progress on knowledge graph embedding (KGE) methods to learn the representations of entities and relations in static knowledge graphs (SKGs). However, knowledge changes over time. In order to represent the facts happening in a specific time, temporal knowledge graph (TKG) embedding approaches are put forward. While most existing models ignore the independence of semantic and temporal information. We empirically find that current models have difficulty distinguishing representations of the same entity or relation at different timestamps. In this regard, we propose a TimeLine-Traced Knowledge Graph Embedding method (TLT-KGE) for temporal knowledge graph completion. TLT-KGE aims to embed the entities and relations with timestamps as a complex vector or a quaternion vector. Specifically, TLT-KGE models semantic information and temporal information as different axes of complex number space or quaternion space. Meanwhile, two specific components carving the relationship between semantic and temporal information are devised to buoy the modeling. In this way, the proposed method can not only distinguish the independence of the semantic and temporal information, but also establish a connection between them. Experimental results on the link prediction task demonstrate that TLT-KGE achieves substantial improvements over state-of-the-art competitors. The source code will be available on https://github.com/zhangfw123/TLT-KGE.",
        "file_path": "paper_data/knowledge_graph_embedding/info/c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 28,
        "score": 9.333333333333332,
        "summary": "Recent years have witnessed remarkable progress on knowledge graph embedding (KGE) methods to learn the representations of entities and relations in static knowledge graphs (SKGs). However, knowledge changes over time. In order to represent the facts happening in a specific time, temporal knowledge graph (TKG) embedding approaches are put forward. While most existing models ignore the independence of semantic and temporal information. We empirically find that current models have difficulty distinguishing representations of the same entity or relation at different timestamps. In this regard, we propose a TimeLine-Traced Knowledge Graph Embedding method (TLT-KGE) for temporal knowledge graph completion. TLT-KGE aims to embed the entities and relations with timestamps as a complex vector or a quaternion vector. Specifically, TLT-KGE models semantic information and temporal information as different axes of complex number space or quaternion space. Meanwhile, two specific components carving the relationship between semantic and temporal information are devised to buoy the modeling. In this way, the proposed method can not only distinguish the independence of the semantic and temporal information, but also establish a connection between them. Experimental results on the link prediction task demonstrate that TLT-KGE achieves substantial improvements over state-of-the-art competitors. The source code will be available on https://github.com/zhangfw123/TLT-KGE.",
        "keywords": []
      },
      "file_name": "c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a.pdf"
    },
    {
      "success": true,
      "doc_id": "509ba00771dba9a1abfb2d7626bc8e43",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4ac5f7ad786fbee89b04023383a4fbe095ccc779.pdf",
      "citation_key": "feng2016dp7",
      "metadata": {
        "title": "Knowledge Graph Embedding by Flexible Translation",
        "authors": [
          "Jun Feng",
          "Minlie Huang",
          "Mingdong Wang",
          "Mantong Zhou",
          "Yu Hao",
          "Xiaoyan Zhu"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/4ac5f7ad786fbee89b04023383a4fbe095ccc779.pdf",
        "venue": "International Conference on Principles of Knowledge Representation and Reasoning",
        "citationCount": 80,
        "score": 8.88888888888889,
        "summary": "",
        "keywords": []
      },
      "file_name": "4ac5f7ad786fbee89b04023383a4fbe095ccc779.pdf"
    },
    {
      "success": true,
      "doc_id": "1291287253a0936d87685ef18d1e370d",
      "summary": "Urban flow analysis is an essential research for smart city construction, in which urban flow pattern analysis focuses on the continuous state of urban flow. How to mine, store and reuse traffic patterns from urban multi-source heterogeneous big data is challenging. Therefore, this paper proposes a knowledge mining network for regional flow pattern to mine and store the urban flow pattern. The proposed model consists of two modules. In the first module, the features of the region and its flow pattern are extracted as the entity and relation, respectively. In the second module, POI features are modeled to enhance the embedding representation of relation and entity. Based on the translation distance method, the knowledge triplets of regional flow patterns are mined. Finally, the proposed model is compared with some benchmark methods using Chengdu Didi order and POI datasets. Experimental results show that the proposed model is effective. In addition, the knowledge triplets are visualized and some application examples are introduced.",
      "intriguing_abstract": "Urban flow analysis is an essential research for smart city construction, in which urban flow pattern analysis focuses on the continuous state of urban flow. How to mine, store and reuse traffic patterns from urban multi-source heterogeneous big data is challenging. Therefore, this paper proposes a knowledge mining network for regional flow pattern to mine and store the urban flow pattern. The proposed model consists of two modules. In the first module, the features of the region and its flow pattern are extracted as the entity and relation, respectively. In the second module, POI features are modeled to enhance the embedding representation of relation and entity. Based on the translation distance method, the knowledge triplets of regional flow patterns are mined. Finally, the proposed model is compared with some benchmark methods using Chengdu Didi order and POI datasets. Experimental results show that the proposed model is effective. In addition, the knowledge triplets are visualized and some application examples are introduced.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9fc2fd3d53a04d082edc80bafa470a66acdebb14.pdf",
      "citation_key": "liu2021wqa",
      "metadata": {
        "title": "Urban Flow Pattern Mining Based on Multi-Source Heterogeneous Data Fusion and Knowledge Graph Embedding",
        "authors": [
          "Jia Liu",
          "Tianrui Li",
          "Shenggong Ji",
          "Peng Xie",
          "Shengdong Du",
          "Fei Teng",
          "Junbo Zhang"
        ],
        "published_date": "2021",
        "abstract": "Urban flow analysis is an essential research for smart city construction, in which urban flow pattern analysis focuses on the continuous state of urban flow. How to mine, store and reuse traffic patterns from urban multi-source heterogeneous big data is challenging. Therefore, this paper proposes a knowledge mining network for regional flow pattern to mine and store the urban flow pattern. The proposed model consists of two modules. In the first module, the features of the region and its flow pattern are extracted as the entity and relation, respectively. In the second module, POI features are modeled to enhance the embedding representation of relation and entity. Based on the translation distance method, the knowledge triplets of regional flow patterns are mined. Finally, the proposed model is compared with some benchmark methods using Chengdu Didi order and POI datasets. Experimental results show that the proposed model is effective. In addition, the knowledge triplets are visualized and some application examples are introduced.",
        "file_path": "paper_data/knowledge_graph_embedding/info/9fc2fd3d53a04d082edc80bafa470a66acdebb14.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 35,
        "score": 8.75,
        "summary": "Urban flow analysis is an essential research for smart city construction, in which urban flow pattern analysis focuses on the continuous state of urban flow. How to mine, store and reuse traffic patterns from urban multi-source heterogeneous big data is challenging. Therefore, this paper proposes a knowledge mining network for regional flow pattern to mine and store the urban flow pattern. The proposed model consists of two modules. In the first module, the features of the region and its flow pattern are extracted as the entity and relation, respectively. In the second module, POI features are modeled to enhance the embedding representation of relation and entity. Based on the translation distance method, the knowledge triplets of regional flow patterns are mined. Finally, the proposed model is compared with some benchmark methods using Chengdu Didi order and POI datasets. Experimental results show that the proposed model is effective. In addition, the knowledge triplets are visualized and some application examples are introduced.",
        "keywords": []
      },
      "file_name": "9fc2fd3d53a04d082edc80bafa470a66acdebb14.pdf"
    },
    {
      "success": true,
      "doc_id": "2e6eb30bde4ef3be25c359743d1a17b3",
      "summary": "Drug discovery is the process by which new candidate medications are discovered. Developing a new drug is a lengthy, complex, and expensive process. Here, in this paper, we propose a biomedical knowledge graph embedding-based recurrent neural network method called GrEDeL, which discovers potential drugs for diseases by mining published biomedical literature. GrEDeL first builds a biomedical knowledge graph by exploiting the relations extracted from biomedical abstracts. Then, the graph data are converted into a low dimensional space by leveraging the knowledge graph embedding methods. After that, a recurrent neural network model is trained by the known drug therapies which are represented by graph embeddings. Finally, it uses the learned model to discover candidate drugs for diseases of interest from biomedical literature. The experimental results show that our method could not only effectively discover new drugs by mining literature, but also could provide the corresponding mechanism of actions for the candidate drugs. It could be a supplementary method for the current traditional drug discovery methods.",
      "intriguing_abstract": "Drug discovery is the process by which new candidate medications are discovered. Developing a new drug is a lengthy, complex, and expensive process. Here, in this paper, we propose a biomedical knowledge graph embedding-based recurrent neural network method called GrEDeL, which discovers potential drugs for diseases by mining published biomedical literature. GrEDeL first builds a biomedical knowledge graph by exploiting the relations extracted from biomedical abstracts. Then, the graph data are converted into a low dimensional space by leveraging the knowledge graph embedding methods. After that, a recurrent neural network model is trained by the known drug therapies which are represented by graph embeddings. Finally, it uses the learned model to discover candidate drugs for diseases of interest from biomedical literature. The experimental results show that our method could not only effectively discover new drugs by mining literature, but also could provide the corresponding mechanism of actions for the candidate drugs. It could be a supplementary method for the current traditional drug discovery methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/747dff7b9cd0d6feb16c340b684b1923034e8777.pdf",
      "citation_key": "sang2019gjl",
      "metadata": {
        "title": "GrEDeL: A Knowledge Graph Embedding Based Method for Drug Discovery From Biomedical Literatures",
        "authors": [
          "Shengtian Sang",
          "Zhihao Yang",
          "Xiaoxia Liu",
          "Lei Wang",
          "Hongfei Lin",
          "Jian Wang",
          "M. Dumontier"
        ],
        "published_date": "2019",
        "abstract": "Drug discovery is the process by which new candidate medications are discovered. Developing a new drug is a lengthy, complex, and expensive process. Here, in this paper, we propose a biomedical knowledge graph embedding-based recurrent neural network method called GrEDeL, which discovers potential drugs for diseases by mining published biomedical literature. GrEDeL first builds a biomedical knowledge graph by exploiting the relations extracted from biomedical abstracts. Then, the graph data are converted into a low dimensional space by leveraging the knowledge graph embedding methods. After that, a recurrent neural network model is trained by the known drug therapies which are represented by graph embeddings. Finally, it uses the learned model to discover candidate drugs for diseases of interest from biomedical literature. The experimental results show that our method could not only effectively discover new drugs by mining literature, but also could provide the corresponding mechanism of actions for the candidate drugs. It could be a supplementary method for the current traditional drug discovery methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/747dff7b9cd0d6feb16c340b684b1923034e8777.pdf",
        "venue": "IEEE Access",
        "citationCount": 52,
        "score": 8.666666666666666,
        "summary": "Drug discovery is the process by which new candidate medications are discovered. Developing a new drug is a lengthy, complex, and expensive process. Here, in this paper, we propose a biomedical knowledge graph embedding-based recurrent neural network method called GrEDeL, which discovers potential drugs for diseases by mining published biomedical literature. GrEDeL first builds a biomedical knowledge graph by exploiting the relations extracted from biomedical abstracts. Then, the graph data are converted into a low dimensional space by leveraging the knowledge graph embedding methods. After that, a recurrent neural network model is trained by the known drug therapies which are represented by graph embeddings. Finally, it uses the learned model to discover candidate drugs for diseases of interest from biomedical literature. The experimental results show that our method could not only effectively discover new drugs by mining literature, but also could provide the corresponding mechanism of actions for the candidate drugs. It could be a supplementary method for the current traditional drug discovery methods.",
        "keywords": []
      },
      "file_name": "747dff7b9cd0d6feb16c340b684b1923034e8777.pdf"
    },
    {
      "success": true,
      "doc_id": "7e8943760b5453c242a7e4531df1d3d4",
      "summary": "Most of the existing medicine recommendation systems that are mainly based on electronic medical records (EMRs) are significantly assisting doctors to make better clinical decisions benefiting both patients and caregivers. Even though the growth of EMRs is at a lighting fast speed in the era of big data, content limitations in EMRs restrain the existed recommendation systems to reflect relevant medical facts, such as drug-drug interactions. Many medical knowledge graphs that contain drug-related information, such as DrugBank, may give hope for the recommendation systems. However, the direct use of these knowledge graphs in the systems suffers from robustness caused by the incompleteness of the graphs. To address these challenges, we stand on recent advances in graph embedding learning techniques and propose a novel framework, called Safe Medicine Recommendation (SMR), in this paper. Specifically, SMR first constructs a high-quality heterogeneous graph by bridging EMRs (MIMIC-III) and medical knowledge graphs (ICD-9 ontology and DrugBank). Then, SMR jointly embeds diseases, medicines, patients, and their corresponding relations into a shared lower dimensional space. Finally, SMR uses the embeddings to decompose the medicine recommendation into a link prediction process while considering the patient's diagnoses and adverse drug reactions. To our best knowledge, SMR is the first to learn embeddings of a patient-disease-medicine graph for medicine recommendation in the world. Extensive experiments on real datasets are conducted to evaluate the effectiveness of proposed framework.",
      "intriguing_abstract": "Most of the existing medicine recommendation systems that are mainly based on electronic medical records (EMRs) are significantly assisting doctors to make better clinical decisions benefiting both patients and caregivers. Even though the growth of EMRs is at a lighting fast speed in the era of big data, content limitations in EMRs restrain the existed recommendation systems to reflect relevant medical facts, such as drug-drug interactions. Many medical knowledge graphs that contain drug-related information, such as DrugBank, may give hope for the recommendation systems. However, the direct use of these knowledge graphs in the systems suffers from robustness caused by the incompleteness of the graphs. To address these challenges, we stand on recent advances in graph embedding learning techniques and propose a novel framework, called Safe Medicine Recommendation (SMR), in this paper. Specifically, SMR first constructs a high-quality heterogeneous graph by bridging EMRs (MIMIC-III) and medical knowledge graphs (ICD-9 ontology and DrugBank). Then, SMR jointly embeds diseases, medicines, patients, and their corresponding relations into a shared lower dimensional space. Finally, SMR uses the embeddings to decompose the medicine recommendation into a link prediction process while considering the patient's diagnoses and adverse drug reactions. To our best knowledge, SMR is the first to learn embeddings of a patient-disease-medicine graph for medicine recommendation in the world. Extensive experiments on real datasets are conducted to evaluate the effectiveness of proposed framework.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/3e76e90180fc8300ecdeb5b543015cc68e0fd249.pdf",
      "citation_key": "wang2017yjq",
      "metadata": {
        "title": "Safe Medicine Recommendation via Medical Knowledge Graph Embedding",
        "authors": [
          "M. Wang",
          "Mengyue Liu",
          "Jun Liu",
          "Sen Wang",
          "Guodong Long",
          "B. Qian"
        ],
        "published_date": "2017",
        "abstract": "Most of the existing medicine recommendation systems that are mainly based on electronic medical records (EMRs) are significantly assisting doctors to make better clinical decisions benefiting both patients and caregivers. Even though the growth of EMRs is at a lighting fast speed in the era of big data, content limitations in EMRs restrain the existed recommendation systems to reflect relevant medical facts, such as drug-drug interactions. Many medical knowledge graphs that contain drug-related information, such as DrugBank, may give hope for the recommendation systems. However, the direct use of these knowledge graphs in the systems suffers from robustness caused by the incompleteness of the graphs. To address these challenges, we stand on recent advances in graph embedding learning techniques and propose a novel framework, called Safe Medicine Recommendation (SMR), in this paper. Specifically, SMR first constructs a high-quality heterogeneous graph by bridging EMRs (MIMIC-III) and medical knowledge graphs (ICD-9 ontology and DrugBank). Then, SMR jointly embeds diseases, medicines, patients, and their corresponding relations into a shared lower dimensional space. Finally, SMR uses the embeddings to decompose the medicine recommendation into a link prediction process while considering the patient's diagnoses and adverse drug reactions. To our best knowledge, SMR is the first to learn embeddings of a patient-disease-medicine graph for medicine recommendation in the world. Extensive experiments on real datasets are conducted to evaluate the effectiveness of proposed framework.",
        "file_path": "paper_data/knowledge_graph_embedding/info/3e76e90180fc8300ecdeb5b543015cc68e0fd249.pdf",
        "venue": "arXiv.org",
        "citationCount": 69,
        "score": 8.625,
        "summary": "Most of the existing medicine recommendation systems that are mainly based on electronic medical records (EMRs) are significantly assisting doctors to make better clinical decisions benefiting both patients and caregivers. Even though the growth of EMRs is at a lighting fast speed in the era of big data, content limitations in EMRs restrain the existed recommendation systems to reflect relevant medical facts, such as drug-drug interactions. Many medical knowledge graphs that contain drug-related information, such as DrugBank, may give hope for the recommendation systems. However, the direct use of these knowledge graphs in the systems suffers from robustness caused by the incompleteness of the graphs. To address these challenges, we stand on recent advances in graph embedding learning techniques and propose a novel framework, called Safe Medicine Recommendation (SMR), in this paper. Specifically, SMR first constructs a high-quality heterogeneous graph by bridging EMRs (MIMIC-III) and medical knowledge graphs (ICD-9 ontology and DrugBank). Then, SMR jointly embeds diseases, medicines, patients, and their corresponding relations into a shared lower dimensional space. Finally, SMR uses the embeddings to decompose the medicine recommendation into a link prediction process while considering the patient's diagnoses and adverse drug reactions. To our best knowledge, SMR is the first to learn embeddings of a patient-disease-medicine graph for medicine recommendation in the world. Extensive experiments on real datasets are conducted to evaluate the effectiveness of proposed framework.",
        "keywords": []
      },
      "file_name": "3e76e90180fc8300ecdeb5b543015cc68e0fd249.pdf"
    },
    {
      "success": true,
      "doc_id": "9436748d91a587c37dd5c513cd68346c",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/547dfe2a9d6a1bb1023f2208fb31f3a0671bf9ca.pdf",
      "citation_key": "jiang20219xl",
      "metadata": {
        "title": "Kernel multi-attention neural network for knowledge graph embedding",
        "authors": [
          "Dan Jiang",
          "Ronggui Wang",
          "Juan Yang",
          "Lixia Xue"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/547dfe2a9d6a1bb1023f2208fb31f3a0671bf9ca.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 34,
        "score": 8.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "547dfe2a9d6a1bb1023f2208fb31f3a0671bf9ca.pdf"
    },
    {
      "success": true,
      "doc_id": "36f2680ca4f3598ce5595c4e9c9bf4c8",
      "summary": "Knowledge graph (KG) embedding seeks to learn vector representations for entities and relations. Conventional models reason over graph structures, but they suffer from the issues of graph incompleteness and long-tail entities. Recent studies have used pre-trained language models to learn embeddings based on the textual information of entities and relations, but they cannot take advantage of graph structures. In the paper, we show empirically that these two kinds of features are complementary for KG embedding. To this end, we propose CoLE, a Co-distillation Learning method for KG Embedding that exploits the complementarity of graph structures and text information. Its graph embedding model employs Transformer to reconstruct the representation of an entity from its neighborhood subgraph. Its text embedding model uses a pre-trained language model to generate entity representations from the soft prompts of their names, descriptions and relational neighbors. To let the two models promote each other, we propose co-distillation learning that allows them to distill selective knowledge from each other's prediction logits. In our co-distillation learning, each model serves as both a teacher and a student. Experiments on benchmark datasets demonstrate that the two models outperform their related baselines, and the ensemble method CoLE with co-distillation learning advances the state-of-the-art of KG embedding.",
      "intriguing_abstract": "Knowledge graph (KG) embedding seeks to learn vector representations for entities and relations. Conventional models reason over graph structures, but they suffer from the issues of graph incompleteness and long-tail entities. Recent studies have used pre-trained language models to learn embeddings based on the textual information of entities and relations, but they cannot take advantage of graph structures. In the paper, we show empirically that these two kinds of features are complementary for KG embedding. To this end, we propose CoLE, a Co-distillation Learning method for KG Embedding that exploits the complementarity of graph structures and text information. Its graph embedding model employs Transformer to reconstruct the representation of an entity from its neighborhood subgraph. Its text embedding model uses a pre-trained language model to generate entity representations from the soft prompts of their names, descriptions and relational neighbors. To let the two models promote each other, we propose co-distillation learning that allows them to distill selective knowledge from each other's prediction logits. In our co-distillation learning, each model serves as both a teacher and a student. Experiments on benchmark datasets demonstrate that the two models outperform their related baselines, and the ensemble method CoLE with co-distillation learning advances the state-of-the-art of KG embedding.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/39eb51ae87c168ad4339214de6b91e2e2fdcfaa1.pdf",
      "citation_key": "liu2022fu5",
      "metadata": {
        "title": "I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning",
        "authors": [
          "Yang Liu",
          "Zequn Sun",
          "Guang-pu Li",
          "Wei Hu"
        ],
        "published_date": "2022",
        "abstract": "Knowledge graph (KG) embedding seeks to learn vector representations for entities and relations. Conventional models reason over graph structures, but they suffer from the issues of graph incompleteness and long-tail entities. Recent studies have used pre-trained language models to learn embeddings based on the textual information of entities and relations, but they cannot take advantage of graph structures. In the paper, we show empirically that these two kinds of features are complementary for KG embedding. To this end, we propose CoLE, a Co-distillation Learning method for KG Embedding that exploits the complementarity of graph structures and text information. Its graph embedding model employs Transformer to reconstruct the representation of an entity from its neighborhood subgraph. Its text embedding model uses a pre-trained language model to generate entity representations from the soft prompts of their names, descriptions and relational neighbors. To let the two models promote each other, we propose co-distillation learning that allows them to distill selective knowledge from each other's prediction logits. In our co-distillation learning, each model serves as both a teacher and a student. Experiments on benchmark datasets demonstrate that the two models outperform their related baselines, and the ensemble method CoLE with co-distillation learning advances the state-of-the-art of KG embedding.",
        "file_path": "paper_data/knowledge_graph_embedding/info/39eb51ae87c168ad4339214de6b91e2e2fdcfaa1.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 25,
        "score": 8.333333333333332,
        "summary": "Knowledge graph (KG) embedding seeks to learn vector representations for entities and relations. Conventional models reason over graph structures, but they suffer from the issues of graph incompleteness and long-tail entities. Recent studies have used pre-trained language models to learn embeddings based on the textual information of entities and relations, but they cannot take advantage of graph structures. In the paper, we show empirically that these two kinds of features are complementary for KG embedding. To this end, we propose CoLE, a Co-distillation Learning method for KG Embedding that exploits the complementarity of graph structures and text information. Its graph embedding model employs Transformer to reconstruct the representation of an entity from its neighborhood subgraph. Its text embedding model uses a pre-trained language model to generate entity representations from the soft prompts of their names, descriptions and relational neighbors. To let the two models promote each other, we propose co-distillation learning that allows them to distill selective knowledge from each other's prediction logits. In our co-distillation learning, each model serves as both a teacher and a student. Experiments on benchmark datasets demonstrate that the two models outperform their related baselines, and the ensemble method CoLE with co-distillation learning advances the state-of-the-art of KG embedding.",
        "keywords": []
      },
      "file_name": "39eb51ae87c168ad4339214de6b91e2e2fdcfaa1.pdf"
    },
    {
      "success": true,
      "doc_id": "0dd6acea3d9c311c388a99a4f3c641cd",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/fee5ac3604ccdefee2b65275fed47503234099e2.pdf",
      "citation_key": "khan202236g",
      "metadata": {
        "title": "Similarity attributed knowledge graph embedding enhancement for item recommendation",
        "authors": [
          "Nasrullah Khan",
          "Zongmin Ma",
          "Aman Ullah",
          "Kemal Polat"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/fee5ac3604ccdefee2b65275fed47503234099e2.pdf",
        "venue": "Information Sciences",
        "citationCount": 24,
        "score": 8.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "fee5ac3604ccdefee2b65275fed47503234099e2.pdf"
    },
    {
      "success": true,
      "doc_id": "fab2f911877b4f021d5c4b8af8bb50ad",
      "summary": "Over the last decade, service selection and recommendation had been two strongly related service filtering steps. While service selection aims to filter the best available services according to QoS and contextual criteria, service recommendation refines the selection results by taking into account additional criteria, such as users feedbacks and ratings, similarities between users tastes, etc. However, the ever changing services environment, users tastes, as well as the perception and popularity of available services, rise a question regarding the appropriate means to capture and analyze such changes over time. Most service recommendation solutions are static and do not offer a multi-relational modeling of user-service interactions over time. Time is a contextual dimension that has, recently, received a lot of attention, leading to a new class of recommender systems, called time-aware recommender systems. In this work, we propose a service recommendation method that takes advantage of temporal knowledge graphs. As a de facto standard to model multiple and complex interactions between heterogeneous entities, knowledge graphs will serve as a historical knowledge base for our TASR system. We, first, model the user-service interactions over time, by constructing a temporal service knowledge graph (TSKG) that will be later enriched through a completion step. Second, to explore the TSKG and extract top-rated services, we use Convolutional Neural Networks (CNN) to embed the TSKG into a low-dimensional vector space, facilitating then its mining. Experimental studies have proven the effectiveness and accuracy of our approach, compared to traditional TASR methods and time-unaware KG-based recommendation.",
      "intriguing_abstract": "Over the last decade, service selection and recommendation had been two strongly related service filtering steps. While service selection aims to filter the best available services according to QoS and contextual criteria, service recommendation refines the selection results by taking into account additional criteria, such as users feedbacks and ratings, similarities between users tastes, etc. However, the ever changing services environment, users tastes, as well as the perception and popularity of available services, rise a question regarding the appropriate means to capture and analyze such changes over time. Most service recommendation solutions are static and do not offer a multi-relational modeling of user-service interactions over time. Time is a contextual dimension that has, recently, received a lot of attention, leading to a new class of recommender systems, called time-aware recommender systems. In this work, we propose a service recommendation method that takes advantage of temporal knowledge graphs. As a de facto standard to model multiple and complex interactions between heterogeneous entities, knowledge graphs will serve as a historical knowledge base for our TASR system. We, first, model the user-service interactions over time, by constructing a temporal service knowledge graph (TSKG) that will be later enriched through a completion step. Second, to explore the TSKG and extract top-rated services, we use Convolutional Neural Networks (CNN) to embed the TSKG into a low-dimensional vector space, facilitating then its mining. Experimental studies have proven the effectiveness and accuracy of our approach, compared to traditional TASR methods and time-unaware KG-based recommendation.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/154fac5040865b4d74cf5a2cad39381c134a8b7d.pdf",
      "citation_key": "mezni2021ezn",
      "metadata": {
        "title": "Temporal Knowledge Graph Embedding for Effective Service Recommendation",
        "authors": [
          "Haithem Mezni"
        ],
        "published_date": "2021",
        "abstract": "Over the last decade, service selection and recommendation had been two strongly related service filtering steps. While service selection aims to filter the best available services according to QoS and contextual criteria, service recommendation refines the selection results by taking into account additional criteria, such as users feedbacks and ratings, similarities between users tastes, etc. However, the ever changing services environment, users tastes, as well as the perception and popularity of available services, rise a question regarding the appropriate means to capture and analyze such changes over time. Most service recommendation solutions are static and do not offer a multi-relational modeling of user-service interactions over time. Time is a contextual dimension that has, recently, received a lot of attention, leading to a new class of recommender systems, called time-aware recommender systems. In this work, we propose a service recommendation method that takes advantage of temporal knowledge graphs. As a de facto standard to model multiple and complex interactions between heterogeneous entities, knowledge graphs will serve as a historical knowledge base for our TASR system. We, first, model the user-service interactions over time, by constructing a temporal service knowledge graph (TSKG) that will be later enriched through a completion step. Second, to explore the TSKG and extract top-rated services, we use Convolutional Neural Networks (CNN) to embed the TSKG into a low-dimensional vector space, facilitating then its mining. Experimental studies have proven the effectiveness and accuracy of our approach, compared to traditional TASR methods and time-unaware KG-based recommendation.",
        "file_path": "paper_data/knowledge_graph_embedding/info/154fac5040865b4d74cf5a2cad39381c134a8b7d.pdf",
        "venue": "IEEE Transactions on Services Computing",
        "citationCount": 32,
        "score": 8.0,
        "summary": "Over the last decade, service selection and recommendation had been two strongly related service filtering steps. While service selection aims to filter the best available services according to QoS and contextual criteria, service recommendation refines the selection results by taking into account additional criteria, such as users feedbacks and ratings, similarities between users tastes, etc. However, the ever changing services environment, users tastes, as well as the perception and popularity of available services, rise a question regarding the appropriate means to capture and analyze such changes over time. Most service recommendation solutions are static and do not offer a multi-relational modeling of user-service interactions over time. Time is a contextual dimension that has, recently, received a lot of attention, leading to a new class of recommender systems, called time-aware recommender systems. In this work, we propose a service recommendation method that takes advantage of temporal knowledge graphs. As a de facto standard to model multiple and complex interactions between heterogeneous entities, knowledge graphs will serve as a historical knowledge base for our TASR system. We, first, model the user-service interactions over time, by constructing a temporal service knowledge graph (TSKG) that will be later enriched through a completion step. Second, to explore the TSKG and extract top-rated services, we use Convolutional Neural Networks (CNN) to embed the TSKG into a low-dimensional vector space, facilitating then its mining. Experimental studies have proven the effectiveness and accuracy of our approach, compared to traditional TASR methods and time-unaware KG-based recommendation.",
        "keywords": []
      },
      "file_name": "154fac5040865b4d74cf5a2cad39381c134a8b7d.pdf"
    },
    {
      "success": true,
      "doc_id": "394eb502321778b843c0d2bad9918328",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/543497b1e551ad6473ddb9aa46697db28bccd3f5.pdf",
      "citation_key": "zhang2021wix",
      "metadata": {
        "title": "Structural context-based knowledge graph embedding for link prediction",
        "authors": [
          "Qianjin Zhang",
          "Ronggui Wang",
          "Juan Yang",
          "Lixia Xue"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/543497b1e551ad6473ddb9aa46697db28bccd3f5.pdf",
        "venue": "Neurocomputing",
        "citationCount": 32,
        "score": 8.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "543497b1e551ad6473ddb9aa46697db28bccd3f5.pdf"
    },
    {
      "success": true,
      "doc_id": "e830802fdbefa21079377e439aa3abd7",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6cc55dec26f5c078c6872d612c1561b1646d459a.pdf",
      "citation_key": "huang2021u42",
      "metadata": {
        "title": "Knowledge graph embedding by relational and entity rotation",
        "authors": [
          "Xuqian Huang",
          "Jiuyang Tang",
          "Zhen Tan",
          "Weixin Zeng",
          "Ji Wang",
          "Xiang Zhao"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/6cc55dec26f5c078c6872d612c1561b1646d459a.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 30,
        "score": 7.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "6cc55dec26f5c078c6872d612c1561b1646d459a.pdf"
    },
    {
      "success": true,
      "doc_id": "6ef374afaec10ada5a6b628ab08b395f",
      "summary": "Knowledge graphs are inherently incomplete. Therefore substantial research has been directed toward knowledge graph completion (KGC), i.e., predicting missing triples from the information represented in the knowledge graph (KG). KG embedding models (KGEs) have yielded promising results for KGC, yet any current KGE is incapable of: (1) fully capturing vital inference patterns (e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy and composition), and (3) providing an intuitive interpretation of captured patterns. In this work, we propose ExpressivE, a fully expressive spatio-functional KGE that solves all these challenges simultaneously. ExpressivE embeds pairs of entities as points and relations as hyper-parallelograms in the virtual triple space $\\mathbb{R}^{2d}$. This model design allows ExpressivE not only to capture a rich set of inference patterns jointly but additionally to display any supported inference pattern through the spatial relation of hyper-parallelograms, offering an intuitive and consistent geometric interpretation of ExpressivE embeddings and their captured patterns. Experimental results on standard KGC benchmarks reveal that ExpressivE is competitive with state-of-the-art KGEs and even significantly outperforms them on WN18RR.",
      "intriguing_abstract": "Knowledge graphs are inherently incomplete. Therefore substantial research has been directed toward knowledge graph completion (KGC), i.e., predicting missing triples from the information represented in the knowledge graph (KG). KG embedding models (KGEs) have yielded promising results for KGC, yet any current KGE is incapable of: (1) fully capturing vital inference patterns (e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy and composition), and (3) providing an intuitive interpretation of captured patterns. In this work, we propose ExpressivE, a fully expressive spatio-functional KGE that solves all these challenges simultaneously. ExpressivE embeds pairs of entities as points and relations as hyper-parallelograms in the virtual triple space $\\mathbb{R}^{2d}$. This model design allows ExpressivE not only to capture a rich set of inference patterns jointly but additionally to display any supported inference pattern through the spatial relation of hyper-parallelograms, offering an intuitive and consistent geometric interpretation of ExpressivE embeddings and their captured patterns. Experimental results on standard KGC benchmarks reveal that ExpressivE is competitive with state-of-the-art KGEs and even significantly outperforms them on WN18RR.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/ee5ceab9fa5f3bad231469923a03ad16184b51b9.pdf",
      "citation_key": "pavlovic2022qte",
      "metadata": {
        "title": "ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion",
        "authors": [
          "Aleksandar Pavlovic",
          "Emanuel Sallinger"
        ],
        "published_date": "2022",
        "abstract": "Knowledge graphs are inherently incomplete. Therefore substantial research has been directed toward knowledge graph completion (KGC), i.e., predicting missing triples from the information represented in the knowledge graph (KG). KG embedding models (KGEs) have yielded promising results for KGC, yet any current KGE is incapable of: (1) fully capturing vital inference patterns (e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy and composition), and (3) providing an intuitive interpretation of captured patterns. In this work, we propose ExpressivE, a fully expressive spatio-functional KGE that solves all these challenges simultaneously. ExpressivE embeds pairs of entities as points and relations as hyper-parallelograms in the virtual triple space $\\mathbb{R}^{2d}$. This model design allows ExpressivE not only to capture a rich set of inference patterns jointly but additionally to display any supported inference pattern through the spatial relation of hyper-parallelograms, offering an intuitive and consistent geometric interpretation of ExpressivE embeddings and their captured patterns. Experimental results on standard KGC benchmarks reveal that ExpressivE is competitive with state-of-the-art KGEs and even significantly outperforms them on WN18RR.",
        "file_path": "paper_data/knowledge_graph_embedding/info/ee5ceab9fa5f3bad231469923a03ad16184b51b9.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "Knowledge graphs are inherently incomplete. Therefore substantial research has been directed toward knowledge graph completion (KGC), i.e., predicting missing triples from the information represented in the knowledge graph (KG). KG embedding models (KGEs) have yielded promising results for KGC, yet any current KGE is incapable of: (1) fully capturing vital inference patterns (e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy and composition), and (3) providing an intuitive interpretation of captured patterns. In this work, we propose ExpressivE, a fully expressive spatio-functional KGE that solves all these challenges simultaneously. ExpressivE embeds pairs of entities as points and relations as hyper-parallelograms in the virtual triple space $\\mathbb{R}^{2d}$. This model design allows ExpressivE not only to capture a rich set of inference patterns jointly but additionally to display any supported inference pattern through the spatial relation of hyper-parallelograms, offering an intuitive and consistent geometric interpretation of ExpressivE embeddings and their captured patterns. Experimental results on standard KGC benchmarks reveal that ExpressivE is competitive with state-of-the-art KGEs and even significantly outperforms them on WN18RR.",
        "keywords": []
      },
      "file_name": "ee5ceab9fa5f3bad231469923a03ad16184b51b9.pdf"
    },
    {
      "success": true,
      "doc_id": "286867efe69aa6b496028b2869adced9",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/3705cfe0d7dab8881518cb932f2465ca432d3f24.pdf",
      "citation_key": "wang20213kg",
      "metadata": {
        "title": "Hierarchical-aware relation rotational knowledge graph embedding for link prediction",
        "authors": [
          "Shensi Wang",
          "Kun Fu",
          "Xian Sun",
          "Zequn Zhang",
          "Shuchao Li",
          "Li Jin"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/3705cfe0d7dab8881518cb932f2465ca432d3f24.pdf",
        "venue": "Neurocomputing",
        "citationCount": 29,
        "score": 7.25,
        "summary": "",
        "keywords": []
      },
      "file_name": "3705cfe0d7dab8881518cb932f2465ca432d3f24.pdf"
    },
    {
      "success": true,
      "doc_id": "2ccc6cad3795657bab5ca6c70e31a325",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/882d6fe22a093ff95a8106a215bca37603ada710.pdf",
      "citation_key": "zhang2019rlm",
      "metadata": {
        "title": "Quaternion Knowledge Graph Embedding",
        "authors": [
          "Shuai Zhang",
          "Yi Tay",
          "Lina Yao",
          "Qi Liu"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/882d6fe22a093ff95a8106a215bca37603ada710.pdf",
        "venue": "arXiv.org",
        "citationCount": 43,
        "score": 7.166666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "882d6fe22a093ff95a8106a215bca37603ada710.pdf"
    },
    {
      "success": true,
      "doc_id": "44e5205425205403eab4759ced7e4793",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/92ef8ff6715733697ca915c65cb18b160a764da6.pdf",
      "citation_key": "mai20195rp",
      "metadata": {
        "title": "Relaxing Unanswerable Geographic Questions Using A Spatially Explicit Knowledge Graph Embedding Model",
        "authors": [
          "Gengchen Mai",
          "Bo Yan",
          "K. Janowicz",
          "Rui Zhu"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/92ef8ff6715733697ca915c65cb18b160a764da6.pdf",
        "venue": "Agile Conference",
        "citationCount": 42,
        "score": 7.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "92ef8ff6715733697ca915c65cb18b160a764da6.pdf"
    },
    {
      "success": true,
      "doc_id": "a4d6b396b44f975508c687cff83305dc",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a0ca7d39296d8d31dbbf300f58e7e375fb879492.pdf",
      "citation_key": "han2018tzc",
      "metadata": {
        "title": "DeepWeak: Reasoning common software weaknesses via knowledge graph embedding",
        "authors": [
          "Zhuobing Han",
          "Xiaohong Li",
          "Hongtao Liu",
          "Zhenchang Xing",
          "Zhiyong Feng"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/a0ca7d39296d8d31dbbf300f58e7e375fb879492.pdf",
        "venue": "IEEE International Conference on Software Analysis, Evolution, and Reengineering",
        "citationCount": 47,
        "score": 6.7142857142857135,
        "summary": "",
        "keywords": []
      },
      "file_name": "a0ca7d39296d8d31dbbf300f58e7e375fb879492.pdf"
    },
    {
      "success": true,
      "doc_id": "9a814cbdfdc209aa03dad88d2cde356a",
      "summary": "Knowledge graph embedding aims to learn representations of entities and relations in low-dimensional space. Recently, extensive studies combine the characteristics of knowledge graphs with different geometric spaces, including Euclidean space, complex space, hyperbolic space and others, which achieves significant progress in representation learning. However, existing methods are subject to at least one of the following limitations: 1) ignoring the uncertainty, 2) incapability of complex relation patterns. To address the above issues simultaneously, we propose a novel model named DiriE, which embeds entities as Dirichlet distributions and relations as multinomial distributions. DiriE employs Bayesian inference to measure the relations between entities and learns binary embeddings of knowledge graphs for modeling complex relation patterns. Additionally, we propose a two-step negative triple generation method that generates negative triples of both entities and relations. We conduct a solid theoretical analysis to demonstrate the effectiveness and robustness of our method, including the expressiveness of complex relation patterns and the ability to model uncertainty. Furthermore, extensive experiments show that our method outperforms state-of-the-art methods in link prediction on benchmark datasets.",
      "intriguing_abstract": "Knowledge graph embedding aims to learn representations of entities and relations in low-dimensional space. Recently, extensive studies combine the characteristics of knowledge graphs with different geometric spaces, including Euclidean space, complex space, hyperbolic space and others, which achieves significant progress in representation learning. However, existing methods are subject to at least one of the following limitations: 1) ignoring the uncertainty, 2) incapability of complex relation patterns. To address the above issues simultaneously, we propose a novel model named DiriE, which embeds entities as Dirichlet distributions and relations as multinomial distributions. DiriE employs Bayesian inference to measure the relations between entities and learns binary embeddings of knowledge graphs for modeling complex relation patterns. Additionally, we propose a two-step negative triple generation method that generates negative triples of both entities and relations. We conduct a solid theoretical analysis to demonstrate the effectiveness and robustness of our method, including the expressiveness of complex relation patterns and the ability to model uncertainty. Furthermore, extensive experiments show that our method outperforms state-of-the-art methods in link prediction on benchmark datasets.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9155e1340e9263cf042d144681acccfc0c9d194b.pdf",
      "citation_key": "wang2022fvx",
      "metadata": {
        "title": "DiriE: Knowledge Graph Embedding with Dirichlet Distribution",
        "authors": [
          "Feiyang Wang",
          "Zhongbao Zhang",
          "Li Sun",
          "Junda Ye",
          "Yang Yan"
        ],
        "published_date": "2022",
        "abstract": "Knowledge graph embedding aims to learn representations of entities and relations in low-dimensional space. Recently, extensive studies combine the characteristics of knowledge graphs with different geometric spaces, including Euclidean space, complex space, hyperbolic space and others, which achieves significant progress in representation learning. However, existing methods are subject to at least one of the following limitations: 1) ignoring the uncertainty, 2) incapability of complex relation patterns. To address the above issues simultaneously, we propose a novel model named DiriE, which embeds entities as Dirichlet distributions and relations as multinomial distributions. DiriE employs Bayesian inference to measure the relations between entities and learns binary embeddings of knowledge graphs for modeling complex relation patterns. Additionally, we propose a two-step negative triple generation method that generates negative triples of both entities and relations. We conduct a solid theoretical analysis to demonstrate the effectiveness and robustness of our method, including the expressiveness of complex relation patterns and the ability to model uncertainty. Furthermore, extensive experiments show that our method outperforms state-of-the-art methods in link prediction on benchmark datasets.",
        "file_path": "paper_data/knowledge_graph_embedding/info/9155e1340e9263cf042d144681acccfc0c9d194b.pdf",
        "venue": "The Web Conference",
        "citationCount": 20,
        "score": 6.666666666666666,
        "summary": "Knowledge graph embedding aims to learn representations of entities and relations in low-dimensional space. Recently, extensive studies combine the characteristics of knowledge graphs with different geometric spaces, including Euclidean space, complex space, hyperbolic space and others, which achieves significant progress in representation learning. However, existing methods are subject to at least one of the following limitations: 1) ignoring the uncertainty, 2) incapability of complex relation patterns. To address the above issues simultaneously, we propose a novel model named DiriE, which embeds entities as Dirichlet distributions and relations as multinomial distributions. DiriE employs Bayesian inference to measure the relations between entities and learns binary embeddings of knowledge graphs for modeling complex relation patterns. Additionally, we propose a two-step negative triple generation method that generates negative triples of both entities and relations. We conduct a solid theoretical analysis to demonstrate the effectiveness and robustness of our method, including the expressiveness of complex relation patterns and the ability to model uncertainty. Furthermore, extensive experiments show that our method outperforms state-of-the-art methods in link prediction on benchmark datasets.",
        "keywords": []
      },
      "file_name": "9155e1340e9263cf042d144681acccfc0c9d194b.pdf"
    },
    {
      "success": true,
      "doc_id": "fccd532c76095ce8d27830d676eed4b5",
      "summary": "In knowledge graph representation learning, link prediction is among the most popular and influential tasks. Its surge in popularity has resulted in a panoply of orthogonal embedding-based methods projecting entities and relations into low-dimensional continuous vectors. To further enrich the research space, the community witnessed a prolific development of evaluation benchmarks with a variety of structures and domains. Therefore, researchers and practitioners face an unprecedented challenge in effectively identifying the best solution to their needs. To this end, we propose the most comprehensive and up-to-date study to systematically assess the effectiveness and efficiency of embedding models for knowledge graph completion. We compare 13 models on six datasets with different sizes, domains, and relational properties, covering translational, semantic matching, and neural network-based encoders. A fine-grained evaluation is conducted to compare each technique head-to-head in terms of standard metrics, training and evaluation times, memory consumption, carbon footprint, and space geometry. Our results demonstrate the high dependence between performance and graph types, identifying the best options for each scenario. Among all the encoding strategies, the new generation of translational models emerges as the most promising, bringing out the best and most consistent results across all the datasets and evaluation criteria.",
      "intriguing_abstract": "In knowledge graph representation learning, link prediction is among the most popular and influential tasks. Its surge in popularity has resulted in a panoply of orthogonal embedding-based methods projecting entities and relations into low-dimensional continuous vectors. To further enrich the research space, the community witnessed a prolific development of evaluation benchmarks with a variety of structures and domains. Therefore, researchers and practitioners face an unprecedented challenge in effectively identifying the best solution to their needs. To this end, we propose the most comprehensive and up-to-date study to systematically assess the effectiveness and efficiency of embedding models for knowledge graph completion. We compare 13 models on six datasets with different sizes, domains, and relational properties, covering translational, semantic matching, and neural network-based encoders. A fine-grained evaluation is conducted to compare each technique head-to-head in terms of standard metrics, training and evaluation times, memory consumption, carbon footprint, and space geometry. Our results demonstrate the high dependence between performance and graph types, identifying the best options for each scenario. Among all the encoding strategies, the new generation of translational models emerges as the most promising, bringing out the best and most consistent results across all the datasets and evaluation criteria.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b5167990eda7d48f1a70a1fcb900ed5d46c40985.pdf",
      "citation_key": "ferrari2022r82",
      "metadata": {
        "title": "Comprehensive Analysis of Knowledge Graph Embedding Techniques Benchmarked on Link Prediction",
        "authors": [
          "Ilaria Ferrari",
          "Giacomo Frisoni",
          "Paolo Italiani",
          "G. Moro",
          "Claudio Sartori"
        ],
        "published_date": "2022",
        "abstract": "In knowledge graph representation learning, link prediction is among the most popular and influential tasks. Its surge in popularity has resulted in a panoply of orthogonal embedding-based methods projecting entities and relations into low-dimensional continuous vectors. To further enrich the research space, the community witnessed a prolific development of evaluation benchmarks with a variety of structures and domains. Therefore, researchers and practitioners face an unprecedented challenge in effectively identifying the best solution to their needs. To this end, we propose the most comprehensive and up-to-date study to systematically assess the effectiveness and efficiency of embedding models for knowledge graph completion. We compare 13 models on six datasets with different sizes, domains, and relational properties, covering translational, semantic matching, and neural network-based encoders. A fine-grained evaluation is conducted to compare each technique head-to-head in terms of standard metrics, training and evaluation times, memory consumption, carbon footprint, and space geometry. Our results demonstrate the high dependence between performance and graph types, identifying the best options for each scenario. Among all the encoding strategies, the new generation of translational models emerges as the most promising, bringing out the best and most consistent results across all the datasets and evaluation criteria.",
        "file_path": "paper_data/knowledge_graph_embedding/info/b5167990eda7d48f1a70a1fcb900ed5d46c40985.pdf",
        "venue": "Electronics",
        "citationCount": 20,
        "score": 6.666666666666666,
        "summary": "In knowledge graph representation learning, link prediction is among the most popular and influential tasks. Its surge in popularity has resulted in a panoply of orthogonal embedding-based methods projecting entities and relations into low-dimensional continuous vectors. To further enrich the research space, the community witnessed a prolific development of evaluation benchmarks with a variety of structures and domains. Therefore, researchers and practitioners face an unprecedented challenge in effectively identifying the best solution to their needs. To this end, we propose the most comprehensive and up-to-date study to systematically assess the effectiveness and efficiency of embedding models for knowledge graph completion. We compare 13 models on six datasets with different sizes, domains, and relational properties, covering translational, semantic matching, and neural network-based encoders. A fine-grained evaluation is conducted to compare each technique head-to-head in terms of standard metrics, training and evaluation times, memory consumption, carbon footprint, and space geometry. Our results demonstrate the high dependence between performance and graph types, identifying the best options for each scenario. Among all the encoding strategies, the new generation of translational models emerges as the most promising, bringing out the best and most consistent results across all the datasets and evaluation criteria.",
        "keywords": []
      },
      "file_name": "b5167990eda7d48f1a70a1fcb900ed5d46c40985.pdf"
    },
    {
      "success": true,
      "doc_id": "9720776ed695a8da672ed58e3fc777c0",
      "summary": "Temporal knowledge graphs store the dynamics of entities and relations during a time period. However, typical temporal knowledge graphs often suffer from incomplete dynamics with missing facts in real-world scenarios. Hence, modeling temporal knowledge graphs to complete the missing facts is important. In this paper, we tackle the temporal knowledge graph completion task by proposing TempCaps, which is a Capsule network-based embedding model for Temporal knowledge graph completion. TempCaps models temporal knowledge graphs by introducing a novel dynamic routing aggregator inspired by Capsule Networks. Specifically, TempCaps builds entity embeddings by dynamically routing retrieved temporal relation and neighbor information. Experimental results demonstrate that TempCaps reaches state-of-the-art performance for temporal knowledge graph completion. Additional analysis also shows that TempCaps is efficient.",
      "intriguing_abstract": "Temporal knowledge graphs store the dynamics of entities and relations during a time period. However, typical temporal knowledge graphs often suffer from incomplete dynamics with missing facts in real-world scenarios. Hence, modeling temporal knowledge graphs to complete the missing facts is important. In this paper, we tackle the temporal knowledge graph completion task by proposing TempCaps, which is a Capsule network-based embedding model for Temporal knowledge graph completion. TempCaps models temporal knowledge graphs by introducing a novel dynamic routing aggregator inspired by Capsule Networks. Specifically, TempCaps builds entity embeddings by dynamically routing retrieved temporal relation and neighbor information. Experimental results demonstrate that TempCaps reaches state-of-the-art performance for temporal knowledge graph completion. Additional analysis also shows that TempCaps is efficient.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/0a8faa6c0e6dc9f743e96f276239d02d8839aca2.pdf",
      "citation_key": "fu2022df2",
      "metadata": {
        "title": "TempCaps: A Capsule Network-based Embedding Model for Temporal Knowledge Graph Completion",
        "authors": [
          "Guirong Fu",
          "Zhao Meng",
          "Zhen Han",
          "Zifeng Ding",
          "Yunpu Ma",
          "Matthias Schubert",
          "Volker Tresp",
          "Roger Wattenhofer"
        ],
        "published_date": "2022",
        "abstract": "Temporal knowledge graphs store the dynamics of entities and relations during a time period. However, typical temporal knowledge graphs often suffer from incomplete dynamics with missing facts in real-world scenarios. Hence, modeling temporal knowledge graphs to complete the missing facts is important. In this paper, we tackle the temporal knowledge graph completion task by proposing TempCaps, which is a Capsule network-based embedding model for Temporal knowledge graph completion. TempCaps models temporal knowledge graphs by introducing a novel dynamic routing aggregator inspired by Capsule Networks. Specifically, TempCaps builds entity embeddings by dynamically routing retrieved temporal relation and neighbor information. Experimental results demonstrate that TempCaps reaches state-of-the-art performance for temporal knowledge graph completion. Additional analysis also shows that TempCaps is efficient.",
        "file_path": "paper_data/knowledge_graph_embedding/info/0a8faa6c0e6dc9f743e96f276239d02d8839aca2.pdf",
        "venue": "SPNLP",
        "citationCount": 20,
        "score": 6.666666666666666,
        "summary": "Temporal knowledge graphs store the dynamics of entities and relations during a time period. However, typical temporal knowledge graphs often suffer from incomplete dynamics with missing facts in real-world scenarios. Hence, modeling temporal knowledge graphs to complete the missing facts is important. In this paper, we tackle the temporal knowledge graph completion task by proposing TempCaps, which is a Capsule network-based embedding model for Temporal knowledge graph completion. TempCaps models temporal knowledge graphs by introducing a novel dynamic routing aggregator inspired by Capsule Networks. Specifically, TempCaps builds entity embeddings by dynamically routing retrieved temporal relation and neighbor information. Experimental results demonstrate that TempCaps reaches state-of-the-art performance for temporal knowledge graph completion. Additional analysis also shows that TempCaps is efficient.",
        "keywords": []
      },
      "file_name": "0a8faa6c0e6dc9f743e96f276239d02d8839aca2.pdf"
    },
    {
      "success": true,
      "doc_id": "ee88808d31a096e6d7bc1ee2a581e793",
      "summary": "Knowledge Graph (KG) embedding projects entities and relations into low dimensional vector space, which has been successfully applied in KG completion task. The previous embedding approaches only model entities and their relations, ignoring a large number of entities numeric attributes in KGs. In this paper, we propose a new KG embedding model which jointly model entity relations and numeric attributes. Our approach combines an attribute embedding model with a translation-based structure embedding model, which learns the embeddings of entities, relations, and attributes simultaneously. Experiments of link prediction on YAGO and Freebase show that the performance is effectively improved by adding entities numeric attributes in the embedding model.",
      "intriguing_abstract": "Knowledge Graph (KG) embedding projects entities and relations into low dimensional vector space, which has been successfully applied in KG completion task. The previous embedding approaches only model entities and their relations, ignoring a large number of entities numeric attributes in KGs. In this paper, we propose a new KG embedding model which jointly model entity relations and numeric attributes. Our approach combines an attribute embedding model with a translation-based structure embedding model, which learns the embeddings of entities, relations, and attributes simultaneously. Experiments of link prediction on YAGO and Freebase show that the performance is effectively improved by adding entities numeric attributes in the embedding model.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/71245f9d9ba0317f78151698dc1ddba7583a3afd.pdf",
      "citation_key": "wu2018c4b",
      "metadata": {
        "title": "Knowledge Graph Embedding with Numeric Attributes of Entities",
        "authors": [
          "Yanrong Wu",
          "Zhichun Wang"
        ],
        "published_date": "2018",
        "abstract": "Knowledge Graph (KG) embedding projects entities and relations into low dimensional vector space, which has been successfully applied in KG completion task. The previous embedding approaches only model entities and their relations, ignoring a large number of entities numeric attributes in KGs. In this paper, we propose a new KG embedding model which jointly model entity relations and numeric attributes. Our approach combines an attribute embedding model with a translation-based structure embedding model, which learns the embeddings of entities, relations, and attributes simultaneously. Experiments of link prediction on YAGO and Freebase show that the performance is effectively improved by adding entities numeric attributes in the embedding model.",
        "file_path": "paper_data/knowledge_graph_embedding/info/71245f9d9ba0317f78151698dc1ddba7583a3afd.pdf",
        "venue": "Rep4NLP@ACL",
        "citationCount": 46,
        "score": 6.571428571428571,
        "summary": "Knowledge Graph (KG) embedding projects entities and relations into low dimensional vector space, which has been successfully applied in KG completion task. The previous embedding approaches only model entities and their relations, ignoring a large number of entities numeric attributes in KGs. In this paper, we propose a new KG embedding model which jointly model entity relations and numeric attributes. Our approach combines an attribute embedding model with a translation-based structure embedding model, which learns the embeddings of entities, relations, and attributes simultaneously. Experiments of link prediction on YAGO and Freebase show that the performance is effectively improved by adding entities numeric attributes in the embedding model.",
        "keywords": []
      },
      "file_name": "71245f9d9ba0317f78151698dc1ddba7583a3afd.pdf"
    },
    {
      "success": true,
      "doc_id": "6835e4af6e1d9b744fd9fc89c87d1c04",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f0499c2123e17106039e8e772878aad073ccf916.pdf",
      "citation_key": "zhang202121t",
      "metadata": {
        "title": "Knowledge graph embedding by reflection transformation",
        "authors": [
          "Qianjin Zhang",
          "Ronggui Wang",
          "Juan Yang",
          "Lixia Xue"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/f0499c2123e17106039e8e772878aad073ccf916.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 26,
        "score": 6.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "f0499c2123e17106039e8e772878aad073ccf916.pdf"
    },
    {
      "success": true,
      "doc_id": "2f2f2bc18099fd8502ddb079f0ab155a",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2bdb9985208a7c7805676029300e3ba648125bd1.pdf",
      "citation_key": "mohamed2019meq",
      "metadata": {
        "title": "Loss Functions in Knowledge Graph Embedding Models",
        "authors": [
          "Sameh K. Mohamed",
          "V. Novek",
          "P. Vandenbussche",
          "Emir Muoz"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/2bdb9985208a7c7805676029300e3ba648125bd1.pdf",
        "venue": "DL4KG@ESWC",
        "citationCount": 38,
        "score": 6.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "2bdb9985208a7c7805676029300e3ba648125bd1.pdf"
    },
    {
      "success": true,
      "doc_id": "6a106a3adea20606a0f7e5301808b132",
      "summary": "Entity alignment is a crucial task in knowledge graph fusion. However, most entity alignment approaches have the scalability problem. Recent methods address this issue by dividing large KGs into small blocks for embedding and alignment learning in each. However, such a partitioning and learning process results in an excessive loss of structure and alignment. Therefore, in this work, we propose a scalable GNN-based entity alignment approach to reduce the structure and alignment loss from three perspectives. First, we propose a centrality-based subgraph generation algorithm to recall some landmark entities serving as the bridges between different subgraphs. Second, we introduce self-supervised entity reconstruction to recover entity representations from incomplete neighborhood subgraphs, and design cross-subgraph negative sampling to incorporate entities from other subgraphs in alignment learning. Third, during the inference process, we merge the embeddings of subgraphs to make a single space for alignment search. Experimental results on the benchmark OpenEA dataset and the proposed large DBpedia1M dataset verify the effectiveness of our approach.",
      "intriguing_abstract": "Entity alignment is a crucial task in knowledge graph fusion. However, most entity alignment approaches have the scalability problem. Recent methods address this issue by dividing large KGs into small blocks for embedding and alignment learning in each. However, such a partitioning and learning process results in an excessive loss of structure and alignment. Therefore, in this work, we propose a scalable GNN-based entity alignment approach to reduce the structure and alignment loss from three perspectives. First, we propose a centrality-based subgraph generation algorithm to recall some landmark entities serving as the bridges between different subgraphs. Second, we introduce self-supervised entity reconstruction to recover entity representations from incomplete neighborhood subgraphs, and design cross-subgraph negative sampling to incorporate entities from other subgraphs in alignment learning. Third, during the inference process, we merge the embeddings of subgraphs to make a single space for alignment search. Experimental results on the benchmark OpenEA dataset and the proposed large DBpedia1M dataset verify the effectiveness of our approach.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7ccb05062f9ea7179532fd3355cf984b0102cfc5.pdf",
      "citation_key": "xin2022dam",
      "metadata": {
        "title": "Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning and Embedding",
        "authors": [
          "Kexuan Xin",
          "Zequn Sun",
          "Wen Hua",
          "Wei Hu",
          "Jianfeng Qu",
          "Xiaofang Zhou"
        ],
        "published_date": "2022",
        "abstract": "Entity alignment is a crucial task in knowledge graph fusion. However, most entity alignment approaches have the scalability problem. Recent methods address this issue by dividing large KGs into small blocks for embedding and alignment learning in each. However, such a partitioning and learning process results in an excessive loss of structure and alignment. Therefore, in this work, we propose a scalable GNN-based entity alignment approach to reduce the structure and alignment loss from three perspectives. First, we propose a centrality-based subgraph generation algorithm to recall some landmark entities serving as the bridges between different subgraphs. Second, we introduce self-supervised entity reconstruction to recover entity representations from incomplete neighborhood subgraphs, and design cross-subgraph negative sampling to incorporate entities from other subgraphs in alignment learning. Third, during the inference process, we merge the embeddings of subgraphs to make a single space for alignment search. Experimental results on the benchmark OpenEA dataset and the proposed large DBpedia1M dataset verify the effectiveness of our approach.",
        "file_path": "paper_data/knowledge_graph_embedding/info/7ccb05062f9ea7179532fd3355cf984b0102cfc5.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "Entity alignment is a crucial task in knowledge graph fusion. However, most entity alignment approaches have the scalability problem. Recent methods address this issue by dividing large KGs into small blocks for embedding and alignment learning in each. However, such a partitioning and learning process results in an excessive loss of structure and alignment. Therefore, in this work, we propose a scalable GNN-based entity alignment approach to reduce the structure and alignment loss from three perspectives. First, we propose a centrality-based subgraph generation algorithm to recall some landmark entities serving as the bridges between different subgraphs. Second, we introduce self-supervised entity reconstruction to recover entity representations from incomplete neighborhood subgraphs, and design cross-subgraph negative sampling to incorporate entities from other subgraphs in alignment learning. Third, during the inference process, we merge the embeddings of subgraphs to make a single space for alignment search. Experimental results on the benchmark OpenEA dataset and the proposed large DBpedia1M dataset verify the effectiveness of our approach.",
        "keywords": []
      },
      "file_name": "7ccb05062f9ea7179532fd3355cf984b0102cfc5.pdf"
    },
    {
      "success": true,
      "doc_id": "06bc6f3ba081f2fb61246b291299e8e7",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c8214cac9c841f7b295a78c5bf71b6ed37c40eec.pdf",
      "citation_key": "nie20195gc",
      "metadata": {
        "title": "Knowledge graph embedding via reasoning over entities, relations, and text",
        "authors": [
          "Binling Nie",
          "Shouqian Sun"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/c8214cac9c841f7b295a78c5bf71b6ed37c40eec.pdf",
        "venue": "Future generations computer systems",
        "citationCount": 37,
        "score": 6.166666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "c8214cac9c841f7b295a78c5bf71b6ed37c40eec.pdf"
    },
    {
      "success": true,
      "doc_id": "88c776e3f0300daa19905bb5a19f1496",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/dab87bce4ac8c6033f5836f575b57c4a665b4f49.pdf",
      "citation_key": "liu2018kvd",
      "metadata": {
        "title": "Stock Price Movement Prediction from Financial News with Deep Learning and Knowledge Graph Embedding",
        "authors": [
          "Yang Liu",
          "Qingguo Zeng",
          "Huanrui Yang",
          "Adrian Carrio"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/dab87bce4ac8c6033f5836f575b57c4a665b4f49.pdf",
        "venue": "Pacific Rim Knowledge Acquisition Workshop",
        "citationCount": 43,
        "score": 6.142857142857142,
        "summary": "",
        "keywords": []
      },
      "file_name": "dab87bce4ac8c6033f5836f575b57c4a665b4f49.pdf"
    },
    {
      "success": true,
      "doc_id": "180cc237ac975a55b963286b47361642",
      "summary": "In this paper, we describe an embedding-based entity recommendation framework for Wikipedia that organizes Wikipedia into a collection of graphs layered on top of each others, learns complementary entity representations from their topology and content, and combines them with a lightweight learning-to-rank approach to recommend related entities on Wikipedia. Through offline and online evaluations, we show that the resulting embeddings and recommendations perform well in terms of quality and user engagement. Balancing simplicity and quality, this framework provides default entity recommendations for English and other languages in the Yahoo! Knowledge Graph, which Wikipedia is a core subset of.",
      "intriguing_abstract": "In this paper, we describe an embedding-based entity recommendation framework for Wikipedia that organizes Wikipedia into a collection of graphs layered on top of each others, learns complementary entity representations from their topology and content, and combines them with a lightweight learning-to-rank approach to recommend related entities on Wikipedia. Through offline and online evaluations, we show that the resulting embeddings and recommendations perform well in terms of quality and user engagement. Balancing simplicity and quality, this framework provides default entity recommendations for English and other languages in the Yahoo! Knowledge Graph, which Wikipedia is a core subset of.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7ae22798887ff4e19033a8028007e1780b53ba8c.pdf",
      "citation_key": "ni2020ruj",
      "metadata": {
        "title": "Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph",
        "authors": [
          "Chien-Chun Ni",
          "Kin Sum Liu",
          "Nicolas Torzec"
        ],
        "published_date": "2020",
        "abstract": "In this paper, we describe an embedding-based entity recommendation framework for Wikipedia that organizes Wikipedia into a collection of graphs layered on top of each others, learns complementary entity representations from their topology and content, and combines them with a lightweight learning-to-rank approach to recommend related entities on Wikipedia. Through offline and online evaluations, we show that the resulting embeddings and recommendations perform well in terms of quality and user engagement. Balancing simplicity and quality, this framework provides default entity recommendations for English and other languages in the Yahoo! Knowledge Graph, which Wikipedia is a core subset of.",
        "file_path": "paper_data/knowledge_graph_embedding/info/7ae22798887ff4e19033a8028007e1780b53ba8c.pdf",
        "venue": "The Web Conference",
        "citationCount": 30,
        "score": 6.0,
        "summary": "In this paper, we describe an embedding-based entity recommendation framework for Wikipedia that organizes Wikipedia into a collection of graphs layered on top of each others, learns complementary entity representations from their topology and content, and combines them with a lightweight learning-to-rank approach to recommend related entities on Wikipedia. Through offline and online evaluations, we show that the resulting embeddings and recommendations perform well in terms of quality and user engagement. Balancing simplicity and quality, this framework provides default entity recommendations for English and other languages in the Yahoo! Knowledge Graph, which Wikipedia is a core subset of.",
        "keywords": []
      },
      "file_name": "7ae22798887ff4e19033a8028007e1780b53ba8c.pdf"
    },
    {
      "success": true,
      "doc_id": "a71cff047ebf844068237920cc014ca3",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/01c1e7830031b25410ed70965d239ac439a6fb68.pdf",
      "citation_key": "li20215pu",
      "metadata": {
        "title": "Learning graph attention-aware knowledge graph embedding",
        "authors": [
          "Chen Li",
          "Xutan Peng",
          "Yuhang Niu",
          "Shanghang Zhang",
          "Hao Peng",
          "Chuan Zhou",
          "Jianxin Li"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/01c1e7830031b25410ed70965d239ac439a6fb68.pdf",
        "venue": "Neurocomputing",
        "citationCount": 24,
        "score": 6.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "01c1e7830031b25410ed70965d239ac439a6fb68.pdf"
    },
    {
      "success": true,
      "doc_id": "47a005824d96568346ec6ee5e1d2b21d",
      "summary": "Pykg2vec is an open-source Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 16 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of TensorFlow and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, mean rank evaluation, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at this https URL.",
      "intriguing_abstract": "Pykg2vec is an open-source Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 16 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of TensorFlow and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, mean rank evaluation, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at this https URL.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/021cbcd59c0438ac8a50c511be7634b0c00a1b89.pdf",
      "citation_key": "yu2019qgs",
      "metadata": {
        "title": "Pykg2vec: A Python Library for Knowledge Graph Embedding",
        "authors": [
          "S. Yu",
          "Sujit Rokka Chhetri",
          "A. Canedo",
          "Palash Goyal",
          "M. A. Faruque"
        ],
        "published_date": "2019",
        "abstract": "Pykg2vec is an open-source Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 16 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of TensorFlow and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, mean rank evaluation, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at this https URL.",
        "file_path": "paper_data/knowledge_graph_embedding/info/021cbcd59c0438ac8a50c511be7634b0c00a1b89.pdf",
        "venue": "Journal of machine learning research",
        "citationCount": 36,
        "score": 6.0,
        "summary": "Pykg2vec is an open-source Python library for learning the representations of the entities and relations in knowledge graphs. Pykg2vec's flexible and modular software architecture currently implements 16 state-of-the-art knowledge graph embedding algorithms, and is designed to easily incorporate new algorithms. The goal of pykg2vec is to provide a practical and educational platform to accelerate research in knowledge graph representation learning. Pykg2vec is built on top of TensorFlow and Python's multiprocessing framework and provides modules for batch generation, Bayesian hyperparameter optimization, mean rank evaluation, embedding, and result visualization. Pykg2vec is released under the MIT License and is also available in the Python Package Index (PyPI). The source code of pykg2vec is available at this https URL.",
        "keywords": []
      },
      "file_name": "021cbcd59c0438ac8a50c511be7634b0c00a1b89.pdf"
    },
    {
      "success": true,
      "doc_id": "a86a102edddf00f92e8356bcaff4ecfc",
      "summary": "Knowledge graphs are used to represent relational information in terms of triples. To enable learning about domains, embedding models, such as tensor factorization models, can be used to make predictions of new triples. Often there is background taxonomic information (in terms of subclasses and subproperties) that should also be taken into account. We show that existing fully expressive (a.k.a. universal) models cannot provably respect subclass and subproperty information. We show that minimal modifications to an existing knowledge graph completion method enables injection of taxonomic information. Moreover, we prove that our model is fully expressive, assuming a lower-bound on the size of the embeddings. Experimental results on public knowledge graphs show that despite its simplicity our approach is surprisingly effective.",
      "intriguing_abstract": "Knowledge graphs are used to represent relational information in terms of triples. To enable learning about domains, embedding models, such as tensor factorization models, can be used to make predictions of new triples. Often there is background taxonomic information (in terms of subclasses and subproperties) that should also be taken into account. We show that existing fully expressive (a.k.a. universal) models cannot provably respect subclass and subproperty information. We show that minimal modifications to an existing knowledge graph completion method enables injection of taxonomic information. Moreover, we prove that our model is fully expressive, assuming a lower-bound on the size of the embeddings. Experimental results on public knowledge graphs show that despite its simplicity our approach is surprisingly effective.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f211a2123e28d60cd8cdc05449c3cb7da2610b0a.pdf",
      "citation_key": "fatemi2018e6v",
      "metadata": {
        "title": "Improved Knowledge Graph Embedding using Background Taxonomic Information",
        "authors": [
          "Bahare Fatemi",
          "Siamak Ravanbakhsh",
          "D. Poole"
        ],
        "published_date": "2018",
        "abstract": "Knowledge graphs are used to represent relational information in terms of triples. To enable learning about domains, embedding models, such as tensor factorization models, can be used to make predictions of new triples. Often there is background taxonomic information (in terms of subclasses and subproperties) that should also be taken into account. We show that existing fully expressive (a.k.a. universal) models cannot provably respect subclass and subproperty information. We show that minimal modifications to an existing knowledge graph completion method enables injection of taxonomic information. Moreover, we prove that our model is fully expressive, assuming a lower-bound on the size of the embeddings. Experimental results on public knowledge graphs show that despite its simplicity our approach is surprisingly effective.",
        "file_path": "paper_data/knowledge_graph_embedding/info/f211a2123e28d60cd8cdc05449c3cb7da2610b0a.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 41,
        "score": 5.857142857142857,
        "summary": "Knowledge graphs are used to represent relational information in terms of triples. To enable learning about domains, embedding models, such as tensor factorization models, can be used to make predictions of new triples. Often there is background taxonomic information (in terms of subclasses and subproperties) that should also be taken into account. We show that existing fully expressive (a.k.a. universal) models cannot provably respect subclass and subproperty information. We show that minimal modifications to an existing knowledge graph completion method enables injection of taxonomic information. Moreover, we prove that our model is fully expressive, assuming a lower-bound on the size of the embeddings. Experimental results on public knowledge graphs show that despite its simplicity our approach is surprisingly effective.",
        "keywords": []
      },
      "file_name": "f211a2123e28d60cd8cdc05449c3cb7da2610b0a.pdf"
    },
    {
      "success": true,
      "doc_id": "ba67b0680ac784a6c612bfdfd0b535e2",
      "summary": "In this paper, we study the problem of embedding uncertain knowledge graphs, where each relation between entities is associated with a confidence score. Observing the existing embedding methods may discard the uncertainty information, only incorporate a specific type of score function, or cause many false-negative samples in the training, we propose the PASSLEAF framework to solve the above issues. PASSLEAF consists of two parts, one is a model that can incorporate different types of scoring functions to predict the relation confidence scores and the other is the semi-supervised learning model by exploiting both positive and negative samples associated with the estimated confidence scores. Furthermore, PASSLEAF leverages a sample pool as a relay of generated samples to further augment the semi-supervised learning. Experiment results show that our proposed framework can learn better embedding in terms of having higher accuracy in both the confidence score prediction and tail entity prediction.",
      "intriguing_abstract": "In this paper, we study the problem of embedding uncertain knowledge graphs, where each relation between entities is associated with a confidence score. Observing the existing embedding methods may discard the uncertainty information, only incorporate a specific type of score function, or cause many false-negative samples in the training, we propose the PASSLEAF framework to solve the above issues. PASSLEAF consists of two parts, one is a model that can incorporate different types of scoring functions to predict the relation confidence scores and the other is the semi-supervised learning model by exploiting both positive and negative samples associated with the estimated confidence scores. Furthermore, PASSLEAF leverages a sample pool as a relay of generated samples to further augment the semi-supervised learning. Experiment results show that our proposed framework can learn better embedding in terms of having higher accuracy in both the confidence score prediction and tail entity prediction.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/3646e2947827c0a9314443e5cbb15575fafaf4ba.pdf",
      "citation_key": "chen2021i5t",
      "metadata": {
        "title": "PASSLEAF: A Pool-bAsed Semi-Supervised LEArning Framework for Uncertain Knowledge Graph Embedding",
        "authors": [
          "Zhuo Chen",
          "Mi-Yen Yeh",
          "Tei-Wei Kuo"
        ],
        "published_date": "2021",
        "abstract": "In this paper, we study the problem of embedding uncertain knowledge graphs, where each relation between entities is associated with a confidence score. Observing the existing embedding methods may discard the uncertainty information, only incorporate a specific type of score function, or cause many false-negative samples in the training, we propose the PASSLEAF framework to solve the above issues. PASSLEAF consists of two parts, one is a model that can incorporate different types of scoring functions to predict the relation confidence scores and the other is the semi-supervised learning model by exploiting both positive and negative samples associated with the estimated confidence scores. Furthermore, PASSLEAF leverages a sample pool as a relay of generated samples to further augment the semi-supervised learning. Experiment results show that our proposed framework can learn better embedding in terms of having higher accuracy in both the confidence score prediction and tail entity prediction.",
        "file_path": "paper_data/knowledge_graph_embedding/info/3646e2947827c0a9314443e5cbb15575fafaf4ba.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 23,
        "score": 5.75,
        "summary": "In this paper, we study the problem of embedding uncertain knowledge graphs, where each relation between entities is associated with a confidence score. Observing the existing embedding methods may discard the uncertainty information, only incorporate a specific type of score function, or cause many false-negative samples in the training, we propose the PASSLEAF framework to solve the above issues. PASSLEAF consists of two parts, one is a model that can incorporate different types of scoring functions to predict the relation confidence scores and the other is the semi-supervised learning model by exploiting both positive and negative samples associated with the estimated confidence scores. Furthermore, PASSLEAF leverages a sample pool as a relay of generated samples to further augment the semi-supervised learning. Experiment results show that our proposed framework can learn better embedding in terms of having higher accuracy in both the confidence score prediction and tail entity prediction.",
        "keywords": []
      },
      "file_name": "3646e2947827c0a9314443e5cbb15575fafaf4ba.pdf"
    },
    {
      "success": true,
      "doc_id": "bd4afdcd2eae083ca3a5d37b73822cd5",
      "summary": "With the popularization and application of Artificial Intelligence technology, knowledge graph embedding methods are widely used for a variety of machine learning tasks. However, most of the current knowledge graph embedding models are trained with a large number of parameters and high computational time complexity. This becomes a main obstacle to apply these existing models to large-scale knowledge graphs. To address this challenge, we propose HET-KG, a distributed system for training knowledge graph embedding efficiently. HET-KG can reduce the communication overheads by introducing a cache embedding table structure to maintain hot-embeddings at each worker. To improve the effectiveness of the cache mechanism, we design a prefetching algorithm and a filtering algorithm for adaptively selecting hot-embeddings, and provide two kinds of hot-embedding table construction strategies. To address the issue of inconsistency between the local cached hot-embeddings and the global embeddings, we also develop a hot-embedding synchronization algorithm for dynamically updating the cache embedding table, which can guarantee the inconsistency bounded within a given threshold. Finally, extensive experiments are conducted on three knowledge graph datasets FB15k, WN18, and Freebase-86m. The experimental results show that HET-KG achieves 3.7x and 1.1x speedup over the state-of-the-art systems PyTorch-BigGraph and DGL-KE, respectively.",
      "intriguing_abstract": "With the popularization and application of Artificial Intelligence technology, knowledge graph embedding methods are widely used for a variety of machine learning tasks. However, most of the current knowledge graph embedding models are trained with a large number of parameters and high computational time complexity. This becomes a main obstacle to apply these existing models to large-scale knowledge graphs. To address this challenge, we propose HET-KG, a distributed system for training knowledge graph embedding efficiently. HET-KG can reduce the communication overheads by introducing a cache embedding table structure to maintain hot-embeddings at each worker. To improve the effectiveness of the cache mechanism, we design a prefetching algorithm and a filtering algorithm for adaptively selecting hot-embeddings, and provide two kinds of hot-embedding table construction strategies. To address the issue of inconsistency between the local cached hot-embeddings and the global embeddings, we also develop a hot-embedding synchronization algorithm for dynamically updating the cache embedding table, which can guarantee the inconsistency bounded within a given threshold. Finally, extensive experiments are conducted on three knowledge graph datasets FB15k, WN18, and Freebase-86m. The experimental results show that HET-KG achieves 3.7x and 1.1x speedup over the state-of-the-art systems PyTorch-BigGraph and DGL-KE, respectively.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/67c03d7a477059dc20faa02e3b45ca7055433615.pdf",
      "citation_key": "dong2022c6z",
      "metadata": {
        "title": "HET-KG: Communication-Efficient Knowledge Graph Embedding Training via Hotness-Aware Cache",
        "authors": [
          "Sicong Dong",
          "Xupeng Miao",
          "Peng Liu",
          "Xin Wang",
          "Bin Cui",
          "Jianxin Li"
        ],
        "published_date": "2022",
        "abstract": "With the popularization and application of Artificial Intelligence technology, knowledge graph embedding methods are widely used for a variety of machine learning tasks. However, most of the current knowledge graph embedding models are trained with a large number of parameters and high computational time complexity. This becomes a main obstacle to apply these existing models to large-scale knowledge graphs. To address this challenge, we propose HET-KG, a distributed system for training knowledge graph embedding efficiently. HET-KG can reduce the communication overheads by introducing a cache embedding table structure to maintain hot-embeddings at each worker. To improve the effectiveness of the cache mechanism, we design a prefetching algorithm and a filtering algorithm for adaptively selecting hot-embeddings, and provide two kinds of hot-embedding table construction strategies. To address the issue of inconsistency between the local cached hot-embeddings and the global embeddings, we also develop a hot-embedding synchronization algorithm for dynamically updating the cache embedding table, which can guarantee the inconsistency bounded within a given threshold. Finally, extensive experiments are conducted on three knowledge graph datasets FB15k, WN18, and Freebase-86m. The experimental results show that HET-KG achieves 3.7x and 1.1x speedup over the state-of-the-art systems PyTorch-BigGraph and DGL-KE, respectively.",
        "file_path": "paper_data/knowledge_graph_embedding/info/67c03d7a477059dc20faa02e3b45ca7055433615.pdf",
        "venue": "IEEE International Conference on Data Engineering",
        "citationCount": 17,
        "score": 5.666666666666666,
        "summary": "With the popularization and application of Artificial Intelligence technology, knowledge graph embedding methods are widely used for a variety of machine learning tasks. However, most of the current knowledge graph embedding models are trained with a large number of parameters and high computational time complexity. This becomes a main obstacle to apply these existing models to large-scale knowledge graphs. To address this challenge, we propose HET-KG, a distributed system for training knowledge graph embedding efficiently. HET-KG can reduce the communication overheads by introducing a cache embedding table structure to maintain hot-embeddings at each worker. To improve the effectiveness of the cache mechanism, we design a prefetching algorithm and a filtering algorithm for adaptively selecting hot-embeddings, and provide two kinds of hot-embedding table construction strategies. To address the issue of inconsistency between the local cached hot-embeddings and the global embeddings, we also develop a hot-embedding synchronization algorithm for dynamically updating the cache embedding table, which can guarantee the inconsistency bounded within a given threshold. Finally, extensive experiments are conducted on three knowledge graph datasets FB15k, WN18, and Freebase-86m. The experimental results show that HET-KG achieves 3.7x and 1.1x speedup over the state-of-the-art systems PyTorch-BigGraph and DGL-KE, respectively.",
        "keywords": []
      },
      "file_name": "67c03d7a477059dc20faa02e3b45ca7055433615.pdf"
    },
    {
      "success": true,
      "doc_id": "cf4d13b02edb2a0738840fbe42d4e47c",
      "summary": "Techniques that map the entities and relations of the knowledge graph (KG) into a low-dimensional continuous space are called KG embedding or knowledge representation learning. However, most existing techniques learn the embeddings based on the facts in KG alone, suffering from the issues of imperfection and spareness of KG. Recently, the research on textual information in KG embedding has attracted much attention due to the rich semantic information supplied by the texts. Thus, in this paper, a survey of techniques for textual information based KG embedding is proposed. Firstly, we introduce the techniques for encoding the textual information to represent the entities and relations from perspectives of encoding models and scoring functions, respectively. Secondly, methods for incorporating the textual information in the existing embedding techniques are summarized. Thirdly, we discuss the training procedure of textual information based KG embedding techniques. Finally, applications of KG embedding with textual information in the specific tasks such as KG completion in zero-shot scenario, multilingual entity alignment, relation extraction and recommender system are explored. We hope that this survey will give insights to researchers into textual information based KG embedding.",
      "intriguing_abstract": "Techniques that map the entities and relations of the knowledge graph (KG) into a low-dimensional continuous space are called KG embedding or knowledge representation learning. However, most existing techniques learn the embeddings based on the facts in KG alone, suffering from the issues of imperfection and spareness of KG. Recently, the research on textual information in KG embedding has attracted much attention due to the rich semantic information supplied by the texts. Thus, in this paper, a survey of techniques for textual information based KG embedding is proposed. Firstly, we introduce the techniques for encoding the textual information to represent the entities and relations from perspectives of encoding models and scoring functions, respectively. Secondly, methods for incorporating the textual information in the existing embedding techniques are summarized. Thirdly, we discuss the training procedure of textual information based KG embedding techniques. Finally, applications of KG embedding with textual information in the specific tasks such as KG completion in zero-shot scenario, multilingual entity alignment, relation extraction and recommender system are explored. We hope that this survey will give insights to researchers into textual information based KG embedding.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/91d8e1339eddee3217a6897cebdeb526b4bb1f72.pdf",
      "citation_key": "lu20206x1",
      "metadata": {
        "title": "Utilizing Textual Information in Knowledge Graph Embedding: A Survey of Methods and Applications",
        "authors": [
          "Fengyuan Lu",
          "Peijin Cong",
          "Xinli Huang"
        ],
        "published_date": "2020",
        "abstract": "Techniques that map the entities and relations of the knowledge graph (KG) into a low-dimensional continuous space are called KG embedding or knowledge representation learning. However, most existing techniques learn the embeddings based on the facts in KG alone, suffering from the issues of imperfection and spareness of KG. Recently, the research on textual information in KG embedding has attracted much attention due to the rich semantic information supplied by the texts. Thus, in this paper, a survey of techniques for textual information based KG embedding is proposed. Firstly, we introduce the techniques for encoding the textual information to represent the entities and relations from perspectives of encoding models and scoring functions, respectively. Secondly, methods for incorporating the textual information in the existing embedding techniques are summarized. Thirdly, we discuss the training procedure of textual information based KG embedding techniques. Finally, applications of KG embedding with textual information in the specific tasks such as KG completion in zero-shot scenario, multilingual entity alignment, relation extraction and recommender system are explored. We hope that this survey will give insights to researchers into textual information based KG embedding.",
        "file_path": "paper_data/knowledge_graph_embedding/info/91d8e1339eddee3217a6897cebdeb526b4bb1f72.pdf",
        "venue": "IEEE Access",
        "citationCount": 28,
        "score": 5.6000000000000005,
        "summary": "Techniques that map the entities and relations of the knowledge graph (KG) into a low-dimensional continuous space are called KG embedding or knowledge representation learning. However, most existing techniques learn the embeddings based on the facts in KG alone, suffering from the issues of imperfection and spareness of KG. Recently, the research on textual information in KG embedding has attracted much attention due to the rich semantic information supplied by the texts. Thus, in this paper, a survey of techniques for textual information based KG embedding is proposed. Firstly, we introduce the techniques for encoding the textual information to represent the entities and relations from perspectives of encoding models and scoring functions, respectively. Secondly, methods for incorporating the textual information in the existing embedding techniques are summarized. Thirdly, we discuss the training procedure of textual information based KG embedding techniques. Finally, applications of KG embedding with textual information in the specific tasks such as KG completion in zero-shot scenario, multilingual entity alignment, relation extraction and recommender system are explored. We hope that this survey will give insights to researchers into textual information based KG embedding.",
        "keywords": []
      },
      "file_name": "91d8e1339eddee3217a6897cebdeb526b4bb1f72.pdf"
    },
    {
      "success": true,
      "doc_id": "c54798813b87f36c2ac2d99fd0a0760e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b1464e3f0c82e21e23dfd9bc28e423856754b3d6.pdf",
      "citation_key": "li2022nr8",
      "metadata": {
        "title": "Improving knowledge graph completion via increasing embedding interactions",
        "authors": [
          "Weidong Li",
          "Rong Peng",
          "Zhi Li"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/b1464e3f0c82e21e23dfd9bc28e423856754b3d6.pdf",
        "venue": "Applied intelligence (Boston)",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "b1464e3f0c82e21e23dfd9bc28e423856754b3d6.pdf"
    },
    {
      "success": true,
      "doc_id": "8588d98d9601383e26b91a4c98eef617",
      "summary": "We consider the problem of embedding knowledge graphs (KGs) into continuous vector spaces. Existing methods can only deal with explicit relationships within each triple, i.e., local connectivity patterns, but cannot handle implicit relationships across different triples, i.e., contextual connectivity patterns. This paper proposes context-dependent KG embedding, a twostage scheme that takes into account both types of connectivity patterns and obtains more accurate embeddings. We evaluate our approach on the tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods.",
      "intriguing_abstract": "We consider the problem of embedding knowledge graphs (KGs) into continuous vector spaces. Existing methods can only deal with explicit relationships within each triple, i.e., local connectivity patterns, but cannot handle implicit relationships across different triples, i.e., contextual connectivity patterns. This paper proposes context-dependent KG embedding, a twostage scheme that takes into account both types of connectivity patterns and obtains more accurate embeddings. We evaluate our approach on the tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/57a7804d4e4e57de9a5c096ce7ea3e50d2c86f0f.pdf",
      "citation_key": "luo2015df2",
      "metadata": {
        "title": "Context-Dependent Knowledge Graph Embedding",
        "authors": [
          "Yuanfei Luo",
          "Quan Wang",
          "Bin Wang",
          "Li Guo"
        ],
        "published_date": "2015",
        "abstract": "We consider the problem of embedding knowledge graphs (KGs) into continuous vector spaces. Existing methods can only deal with explicit relationships within each triple, i.e., local connectivity patterns, but cannot handle implicit relationships across different triples, i.e., contextual connectivity patterns. This paper proposes context-dependent KG embedding, a twostage scheme that takes into account both types of connectivity patterns and obtains more accurate embeddings. We evaluate our approach on the tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/57a7804d4e4e57de9a5c096ce7ea3e50d2c86f0f.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 53,
        "score": 5.300000000000001,
        "summary": "We consider the problem of embedding knowledge graphs (KGs) into continuous vector spaces. Existing methods can only deal with explicit relationships within each triple, i.e., local connectivity patterns, but cannot handle implicit relationships across different triples, i.e., contextual connectivity patterns. This paper proposes context-dependent KG embedding, a twostage scheme that takes into account both types of connectivity patterns and obtains more accurate embeddings. We evaluate our approach on the tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "57a7804d4e4e57de9a5c096ce7ea3e50d2c86f0f.pdf"
    },
    {
      "success": true,
      "doc_id": "a7eaaeb4e59c6ccf2ba91af2ac230d77",
      "summary": "We study the problem of learning knowledge representations of entities and relations in knowledge graphs to predict missing links. The key to precisely accomplish a such task is modeling and inferring the diverse patterns of the relations. In this paper, we present a new rotation-based knowledge representation learning model named Path-RotatE, which considers additional paths to model rich inference patterns between entities. In addition, this paper considers the correlation between the path and the direct relation. In this way, we improve reliability of the path, making it more suitable to train. Finally, this paper conducts entity prediction experiments on datasets such as FB15k, FB15-237, WN18 and WN18RR. The results show that the Path-RotatE model has a certain improvement in MR, MRR and Hits@N compared to RotatE, PTransE and other baseline models.",
      "intriguing_abstract": "We study the problem of learning knowledge representations of entities and relations in knowledge graphs to predict missing links. The key to precisely accomplish a such task is modeling and inferring the diverse patterns of the relations. In this paper, we present a new rotation-based knowledge representation learning model named Path-RotatE, which considers additional paths to model rich inference patterns between entities. In addition, this paper considers the correlation between the path and the direct relation. In this way, we improve reliability of the path, making it more suitable to train. Finally, this paper conducts entity prediction experiments on datasets such as FB15k, FB15-237, WN18 and WN18RR. The results show that the Path-RotatE model has a certain improvement in MR, MRR and Hits@N compared to RotatE, PTransE and other baseline models.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/678dacdf029becac1116f345520f8e4afff5a873.pdf",
      "citation_key": "zhou20216m0",
      "metadata": {
        "title": "Path-RotatE: Knowledge Graph Embedding by Relational Rotation of Path in Complex Space",
        "authors": [
          "Xiaohan Zhou",
          "Yunhui Yi",
          "Geng Jia"
        ],
        "published_date": "2021",
        "abstract": "We study the problem of learning knowledge representations of entities and relations in knowledge graphs to predict missing links. The key to precisely accomplish a such task is modeling and inferring the diverse patterns of the relations. In this paper, we present a new rotation-based knowledge representation learning model named Path-RotatE, which considers additional paths to model rich inference patterns between entities. In addition, this paper considers the correlation between the path and the direct relation. In this way, we improve reliability of the path, making it more suitable to train. Finally, this paper conducts entity prediction experiments on datasets such as FB15k, FB15-237, WN18 and WN18RR. The results show that the Path-RotatE model has a certain improvement in MR, MRR and Hits@N compared to RotatE, PTransE and other baseline models.",
        "file_path": "paper_data/knowledge_graph_embedding/info/678dacdf029becac1116f345520f8e4afff5a873.pdf",
        "venue": "International Conference on Innovative Computing and Cloud Computing",
        "citationCount": 20,
        "score": 5.0,
        "summary": "We study the problem of learning knowledge representations of entities and relations in knowledge graphs to predict missing links. The key to precisely accomplish a such task is modeling and inferring the diverse patterns of the relations. In this paper, we present a new rotation-based knowledge representation learning model named Path-RotatE, which considers additional paths to model rich inference patterns between entities. In addition, this paper considers the correlation between the path and the direct relation. In this way, we improve reliability of the path, making it more suitable to train. Finally, this paper conducts entity prediction experiments on datasets such as FB15k, FB15-237, WN18 and WN18RR. The results show that the Path-RotatE model has a certain improvement in MR, MRR and Hits@N compared to RotatE, PTransE and other baseline models.",
        "keywords": []
      },
      "file_name": "678dacdf029becac1116f345520f8e4afff5a873.pdf"
    },
    {
      "success": true,
      "doc_id": "5229dd9ec08a9aa7de22458b26370ed0",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/1a25c8afacb6d36d4d8635eb9e3f8b8cf2e2122c.pdf",
      "citation_key": "zhao202095o",
      "metadata": {
        "title": "Structure-augmented knowledge graph embedding for sparse data with rule learning",
        "authors": [
          "Feng Zhao",
          "Haoran Sun",
          "Langjunqing Jin",
          "Hai Jin"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/1a25c8afacb6d36d4d8635eb9e3f8b8cf2e2122c.pdf",
        "venue": "Computer Communications",
        "citationCount": 25,
        "score": 5.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "1a25c8afacb6d36d4d8635eb9e3f8b8cf2e2122c.pdf"
    },
    {
      "success": true,
      "doc_id": "9f494bfb5ce1ba9c15c7506274329dcf",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/60ad3ce0492a004020ff55653a51d6bfc457f12d.pdf",
      "citation_key": "jia201870f",
      "metadata": {
        "title": "Path-specific knowledge graph embedding",
        "authors": [
          "Yantao Jia",
          "Yuanzhuo Wang",
          "Xiaolong Jin",
          "Xueqi Cheng"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/60ad3ce0492a004020ff55653a51d6bfc457f12d.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 34,
        "score": 4.857142857142857,
        "summary": "",
        "keywords": []
      },
      "file_name": "60ad3ce0492a004020ff55653a51d6bfc457f12d.pdf"
    },
    {
      "success": true,
      "doc_id": "4815c1cff4a8067a5b94c3d5626e629b",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/434b32d34b5d21071fc78a081741757f263c14ae.pdf",
      "citation_key": "mai2018u0h",
      "metadata": {
        "title": "Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines",
        "authors": [
          "Gengchen Mai",
          "K. Janowicz",
          "Bo Yan"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/434b32d34b5d21071fc78a081741757f263c14ae.pdf",
        "venue": "Semdeep/NLIWoD@ISWC",
        "citationCount": 34,
        "score": 4.857142857142857,
        "summary": "",
        "keywords": []
      },
      "file_name": "434b32d34b5d21071fc78a081741757f263c14ae.pdf"
    },
    {
      "success": true,
      "doc_id": "eba75f0ba9b549f29d581b92a739940c",
      "summary": "Leveraging domain knowledge is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by topic models. In this paper, we develop topic modeling with knowledge graph embedding (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of topic modeling, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.",
      "intriguing_abstract": "Leveraging domain knowledge is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by topic models. In this paper, we develop topic modeling with knowledge graph embedding (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of topic modeling, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4a96636d1fc92221f2232d2d74be6e303cd0642a.pdf",
      "citation_key": "li201949n",
      "metadata": {
        "title": "Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process",
        "authors": [
          "Dingcheng Li",
          "Siamak Zamani",
          "Jingyuan Zhang",
          "Ping Li"
        ],
        "published_date": "2019",
        "abstract": "Leveraging domain knowledge is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by topic models. In this paper, we develop topic modeling with knowledge graph embedding (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of topic modeling, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/4a96636d1fc92221f2232d2d74be6e303cd0642a.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 29,
        "score": 4.833333333333333,
        "summary": "Leveraging domain knowledge is an effective strategy for enhancing the quality of inferred low-dimensional representations of documents by topic models. In this paper, we develop topic modeling with knowledge graph embedding (TMKGE), a Bayesian nonparametric model to employ knowledge graph (KG) embedding in the context of topic modeling, for extracting more coherent topics. Specifically, we build a hierarchical Dirichlet process (HDP) based model to flexibly borrow information from KG to improve the interpretability of topics. An efficient online variational inference method based on a stick-breaking construction of HDP is developed for TMKGE, making TMKGE suitable for large document corpora and KGs. Experiments on three public datasets illustrate the superior performance of TMKGE in terms of topic coherence and document classification accuracy, compared to state-of-the-art topic modeling methods.",
        "keywords": []
      },
      "file_name": "4a96636d1fc92221f2232d2d74be6e303cd0642a.pdf"
    },
    {
      "success": true,
      "doc_id": "70f4ab3b9ae029cd3a41864d66c47862",
      "summary": "Recently, Knowledge Graph Embedding (KGE) has attracted considerable research efforts, since it simplifies the manipulation while preserving the inherent structure of the KG. However to some extent, most existing KGE approaches ignore the historical changes of structural information involved in dynamic knowledge graphs (DKGs). To deal with this problem, this paper presents a Timespan-aware Dynamic knowledge Graph Embedding Evolution (TDG2E) method that considers temporal evolving process of DKGs. The major innovations of our paper are two-fold. Firstly, a Gated Recurrent Units (GRU) based model is utilized in TDG2E to deal with the dependency among sub-KGs that is inevitably involved in the learning process of the dynamic knowledge graph embedding. Furthermore, we incorporate an auxiliary loss to supervise the learning process of the next sub-KG by utilizing previous structural information (i.e., the hidden state of GRU). In contrast with existing approaches in the literature (e.g., HyTE and t-TransE), TDG2E preserves structural information of current sub-KG and the temporal evolving process of the DKG simultaneously. Secondly, to further deal with the time unbalance issue underlying the DKGs, a Timespan Gate is designed in GRU. It makes TDG2E possible to model the temporal evolving process of DKGs more effectively by incorporating the timespan between adjacent sub-KGs. Extensive experiments on two large temporal datasets (i.e., YAGO11k and Wikidata12k) extracted from real-world KGs validate that the proposed TDG2E significantly outperforms traditional KGE methods in terms of Mean Rank and Hit Rate.",
      "intriguing_abstract": "Recently, Knowledge Graph Embedding (KGE) has attracted considerable research efforts, since it simplifies the manipulation while preserving the inherent structure of the KG. However to some extent, most existing KGE approaches ignore the historical changes of structural information involved in dynamic knowledge graphs (DKGs). To deal with this problem, this paper presents a Timespan-aware Dynamic knowledge Graph Embedding Evolution (TDG2E) method that considers temporal evolving process of DKGs. The major innovations of our paper are two-fold. Firstly, a Gated Recurrent Units (GRU) based model is utilized in TDG2E to deal with the dependency among sub-KGs that is inevitably involved in the learning process of the dynamic knowledge graph embedding. Furthermore, we incorporate an auxiliary loss to supervise the learning process of the next sub-KG by utilizing previous structural information (i.e., the hidden state of GRU). In contrast with existing approaches in the literature (e.g., HyTE and t-TransE), TDG2E preserves structural information of current sub-KG and the temporal evolving process of the DKG simultaneously. Secondly, to further deal with the time unbalance issue underlying the DKGs, a Timespan Gate is designed in GRU. It makes TDG2E possible to model the temporal evolving process of DKGs more effectively by incorporating the timespan between adjacent sub-KGs. Extensive experiments on two large temporal datasets (i.e., YAGO11k and Wikidata12k) extracted from real-world KGs validate that the proposed TDG2E significantly outperforms traditional KGE methods in terms of Mean Rank and Hit Rate.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9c17d3f1837ae9f10f57c0b07c8288137d84026b.pdf",
      "citation_key": "tang2020ufr",
      "metadata": {
        "title": "Timespan-Aware Dynamic Knowledge Graph Embedding by Incorporating Temporal Evolution",
        "authors": [
          "Xiaoli Tang",
          "Rui Yuan",
          "Qianyu Li",
          "Tengyun Wang",
          "Haizhi Yang",
          "Yundong Cai",
          "Hengjie Song"
        ],
        "published_date": "2020",
        "abstract": "Recently, Knowledge Graph Embedding (KGE) has attracted considerable research efforts, since it simplifies the manipulation while preserving the inherent structure of the KG. However to some extent, most existing KGE approaches ignore the historical changes of structural information involved in dynamic knowledge graphs (DKGs). To deal with this problem, this paper presents a Timespan-aware Dynamic knowledge Graph Embedding Evolution (TDG2E) method that considers temporal evolving process of DKGs. The major innovations of our paper are two-fold. Firstly, a Gated Recurrent Units (GRU) based model is utilized in TDG2E to deal with the dependency among sub-KGs that is inevitably involved in the learning process of the dynamic knowledge graph embedding. Furthermore, we incorporate an auxiliary loss to supervise the learning process of the next sub-KG by utilizing previous structural information (i.e., the hidden state of GRU). In contrast with existing approaches in the literature (e.g., HyTE and t-TransE), TDG2E preserves structural information of current sub-KG and the temporal evolving process of the DKG simultaneously. Secondly, to further deal with the time unbalance issue underlying the DKGs, a Timespan Gate is designed in GRU. It makes TDG2E possible to model the temporal evolving process of DKGs more effectively by incorporating the timespan between adjacent sub-KGs. Extensive experiments on two large temporal datasets (i.e., YAGO11k and Wikidata12k) extracted from real-world KGs validate that the proposed TDG2E significantly outperforms traditional KGE methods in terms of Mean Rank and Hit Rate.",
        "file_path": "paper_data/knowledge_graph_embedding/info/9c17d3f1837ae9f10f57c0b07c8288137d84026b.pdf",
        "venue": "IEEE Access",
        "citationCount": 24,
        "score": 4.800000000000001,
        "summary": "Recently, Knowledge Graph Embedding (KGE) has attracted considerable research efforts, since it simplifies the manipulation while preserving the inherent structure of the KG. However to some extent, most existing KGE approaches ignore the historical changes of structural information involved in dynamic knowledge graphs (DKGs). To deal with this problem, this paper presents a Timespan-aware Dynamic knowledge Graph Embedding Evolution (TDG2E) method that considers temporal evolving process of DKGs. The major innovations of our paper are two-fold. Firstly, a Gated Recurrent Units (GRU) based model is utilized in TDG2E to deal with the dependency among sub-KGs that is inevitably involved in the learning process of the dynamic knowledge graph embedding. Furthermore, we incorporate an auxiliary loss to supervise the learning process of the next sub-KG by utilizing previous structural information (i.e., the hidden state of GRU). In contrast with existing approaches in the literature (e.g., HyTE and t-TransE), TDG2E preserves structural information of current sub-KG and the temporal evolving process of the DKG simultaneously. Secondly, to further deal with the time unbalance issue underlying the DKGs, a Timespan Gate is designed in GRU. It makes TDG2E possible to model the temporal evolving process of DKGs more effectively by incorporating the timespan between adjacent sub-KGs. Extensive experiments on two large temporal datasets (i.e., YAGO11k and Wikidata12k) extracted from real-world KGs validate that the proposed TDG2E significantly outperforms traditional KGE methods in terms of Mean Rank and Hit Rate.",
        "keywords": []
      },
      "file_name": "9c17d3f1837ae9f10f57c0b07c8288137d84026b.pdf"
    },
    {
      "success": true,
      "doc_id": "58009cbe11354f5ba5eb3d74e8b388a2",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/e740a9aa753fcc926857ef4b90c1f91dd086e08d.pdf",
      "citation_key": "guo2022qtv",
      "metadata": {
        "title": "Understanding and Improving Knowledge Graph Embedding for Entity Alignment",
        "authors": [
          "Lingbing Guo",
          "Qiang Zhang",
          "Zequn Sun",
          "Mingyang Chen",
          "Wei Hu",
          "Huajun Chen"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/e740a9aa753fcc926857ef4b90c1f91dd086e08d.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "e740a9aa753fcc926857ef4b90c1f91dd086e08d.pdf"
    },
    {
      "success": true,
      "doc_id": "ba3d7824c6c9c8c9842847f29641f792",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/315b239040f73063076014fdfabcc621b2719d83.pdf",
      "citation_key": "jiang202235y",
      "metadata": {
        "title": "Multiview feature augmented neural network for knowledge graph embedding",
        "authors": [
          "Dan Jiang",
          "Ronggui Wang",
          "Lixia Xue",
          "Juan Yang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/315b239040f73063076014fdfabcc621b2719d83.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "315b239040f73063076014fdfabcc621b2719d83.pdf"
    },
    {
      "success": true,
      "doc_id": "0286affaa0078009ab42a39e7d6fd917",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/96b1f6fb6e904a674aef5cd32efee3edfa1c8ee2.pdf",
      "citation_key": "liu201918i",
      "metadata": {
        "title": "Context-Aware Temporal Knowledge Graph Embedding",
        "authors": [
          "Yu Liu",
          "Wen Hua",
          "Kexuan Xin",
          "Xiaofang Zhou"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/96b1f6fb6e904a674aef5cd32efee3edfa1c8ee2.pdf",
        "venue": "WISE",
        "citationCount": 28,
        "score": 4.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "96b1f6fb6e904a674aef5cd32efee3edfa1c8ee2.pdf"
    },
    {
      "success": true,
      "doc_id": "a61e1c4012bf713465d3baf8fe03ecc0",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/5d6b4c5e48ec0585facea96a746bcbf7225d424c.pdf",
      "citation_key": "zhang2020s4x",
      "metadata": {
        "title": "Knowledge graph embedding by translating in time domain space for link prediction",
        "authors": [
          "Qianjin Zhang",
          "Ronggui Wang",
          "Juan Yang",
          "Lixia Xue"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/5d6b4c5e48ec0585facea96a746bcbf7225d424c.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 23,
        "score": 4.6000000000000005,
        "summary": "",
        "keywords": []
      },
      "file_name": "5d6b4c5e48ec0585facea96a746bcbf7225d424c.pdf"
    },
    {
      "success": true,
      "doc_id": "f2803385a4d169ab0deabdc187296223",
      "summary": "Knowledge graph embedding aims at representing entities and relations in a knowledge graph as dense, low-dimensional and real-valued vectors. It can efficiently measure semantic correlations of entities and relations in knowledge graphs, and improve the performance of knowledge acquisition, fusion and inference. Among various embedding models appeared in recent years, the translation-based models such as TransE, TransH, TransR and TranSparse achieve state-of-the-art performance. However, the translation principle applied in these models is too strict and can not deal with complex entities and relations very well. In this paper, by introducing parameter vectors into the translation principle which treats each relation as a translation from the head entity to the tail entity, we propose a novel dynamic translation principle which supports flexible translation between the embeddings of entities and relations. We use this principle to improve the TransE, TransR and TranSparse models respectively and build new models named TransE-DT, TransR-DT and TranSparse-DT correspondingly. Experimental results show that our dynamic translation principle achieves great improvement in both the link prediction task and the triple classification task.",
      "intriguing_abstract": "Knowledge graph embedding aims at representing entities and relations in a knowledge graph as dense, low-dimensional and real-valued vectors. It can efficiently measure semantic correlations of entities and relations in knowledge graphs, and improve the performance of knowledge acquisition, fusion and inference. Among various embedding models appeared in recent years, the translation-based models such as TransE, TransH, TransR and TranSparse achieve state-of-the-art performance. However, the translation principle applied in these models is too strict and can not deal with complex entities and relations very well. In this paper, by introducing parameter vectors into the translation principle which treats each relation as a translation from the head entity to the tail entity, we propose a novel dynamic translation principle which supports flexible translation between the embeddings of entities and relations. We use this principle to improve the TransE, TransR and TranSparse models respectively and build new models named TransE-DT, TransR-DT and TranSparse-DT correspondingly. Experimental results show that our dynamic translation principle achieves great improvement in both the link prediction task and the triple classification task.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/441f124d48662d6bd4f8e3190633371aa1b034eb.pdf",
      "citation_key": "chang20179yf",
      "metadata": {
        "title": "Knowledge Graph Embedding by Dynamic Translation",
        "authors": [
          "Liang Chang",
          "Manli Zhu",
          "T. Gu",
          "Chenzhong Bin",
          "Junyan Qian",
          "Ji Zhang"
        ],
        "published_date": "2017",
        "abstract": "Knowledge graph embedding aims at representing entities and relations in a knowledge graph as dense, low-dimensional and real-valued vectors. It can efficiently measure semantic correlations of entities and relations in knowledge graphs, and improve the performance of knowledge acquisition, fusion and inference. Among various embedding models appeared in recent years, the translation-based models such as TransE, TransH, TransR and TranSparse achieve state-of-the-art performance. However, the translation principle applied in these models is too strict and can not deal with complex entities and relations very well. In this paper, by introducing parameter vectors into the translation principle which treats each relation as a translation from the head entity to the tail entity, we propose a novel dynamic translation principle which supports flexible translation between the embeddings of entities and relations. We use this principle to improve the TransE, TransR and TranSparse models respectively and build new models named TransE-DT, TransR-DT and TranSparse-DT correspondingly. Experimental results show that our dynamic translation principle achieves great improvement in both the link prediction task and the triple classification task.",
        "file_path": "paper_data/knowledge_graph_embedding/info/441f124d48662d6bd4f8e3190633371aa1b034eb.pdf",
        "venue": "IEEE Access",
        "citationCount": 36,
        "score": 4.5,
        "summary": "Knowledge graph embedding aims at representing entities and relations in a knowledge graph as dense, low-dimensional and real-valued vectors. It can efficiently measure semantic correlations of entities and relations in knowledge graphs, and improve the performance of knowledge acquisition, fusion and inference. Among various embedding models appeared in recent years, the translation-based models such as TransE, TransH, TransR and TranSparse achieve state-of-the-art performance. However, the translation principle applied in these models is too strict and can not deal with complex entities and relations very well. In this paper, by introducing parameter vectors into the translation principle which treats each relation as a translation from the head entity to the tail entity, we propose a novel dynamic translation principle which supports flexible translation between the embeddings of entities and relations. We use this principle to improve the TransE, TransR and TranSparse models respectively and build new models named TransE-DT, TransR-DT and TranSparse-DT correspondingly. Experimental results show that our dynamic translation principle achieves great improvement in both the link prediction task and the triple classification task.",
        "keywords": []
      },
      "file_name": "441f124d48662d6bd4f8e3190633371aa1b034eb.pdf"
    },
    {
      "success": true,
      "doc_id": "34c296d089588704200b279b9c0e76e1",
      "summary": "The goal of temporal knowledge graph embedding (TKGE) is to represent the entities and relations in a given temporal knowledge graph (TKG) as low-dimensional vectors (i.e., embeddings), which preserve both semantic information and temporal dynamics of the factual information. In this paper, we posit that the intrinsic difficulty of existing TKGE methods lies in the lack of information in KG snapshots with timestamps, each of which contains the facts that co-occur at a specific timestamp. To address this challenge, we propose a novel self-supervised TKGE approach, THOR (Three-tower grapH cOnvolution netwoRks (GCNs)), which extracts latent knowledge from TKGs by jointly leveraging both temporal and atemporal dependencies between entities and the structural dependency between relations. THOR learns the embeddings of entities and relations Our experiments on three real-world datasets demonstrate that THOR significantly outperforms 13 competitors in terms of TKG completion tasks. The codebase of THOR is available at https://github.com/EJHyun/THOR.",
      "intriguing_abstract": "The goal of temporal knowledge graph embedding (TKGE) is to represent the entities and relations in a given temporal knowledge graph (TKG) as low-dimensional vectors (i.e., embeddings), which preserve both semantic information and temporal dynamics of the factual information. In this paper, we posit that the intrinsic difficulty of existing TKGE methods lies in the lack of information in KG snapshots with timestamps, each of which contains the facts that co-occur at a specific timestamp. To address this challenge, we propose a novel self-supervised TKGE approach, THOR (Three-tower grapH cOnvolution netwoRks (GCNs)), which extracts latent knowledge from TKGs by jointly leveraging both temporal and atemporal dependencies between entities and the structural dependency between relations. THOR learns the embeddings of entities and relations Our experiments on three real-world datasets demonstrate that THOR significantly outperforms 13 competitors in terms of TKG completion tasks. The codebase of THOR is available at https://github.com/EJHyun/THOR.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/5f9ea28be0d3bb9a73d62512190a772b10e92db0.pdf",
      "citation_key": "lee2022hr9",
      "metadata": {
        "title": "THOR: Self-Supervised Temporal Knowledge Graph Embedding via Three-Tower Graph Convolutional Networks",
        "authors": [
          "Yeon-Chang Lee",
          "Sang-Wook Kim"
        ],
        "published_date": "2022",
        "abstract": "The goal of temporal knowledge graph embedding (TKGE) is to represent the entities and relations in a given temporal knowledge graph (TKG) as low-dimensional vectors (i.e., embeddings), which preserve both semantic information and temporal dynamics of the factual information. In this paper, we posit that the intrinsic difficulty of existing TKGE methods lies in the lack of information in KG snapshots with timestamps, each of which contains the facts that co-occur at a specific timestamp. To address this challenge, we propose a novel self-supervised TKGE approach, THOR (Three-tower grapH cOnvolution netwoRks (GCNs)), which extracts latent knowledge from TKGs by jointly leveraging both temporal and atemporal dependencies between entities and the structural dependency between relations. THOR learns the embeddings of entities and relations Our experiments on three real-world datasets demonstrate that THOR significantly outperforms 13 competitors in terms of TKG completion tasks. The codebase of THOR is available at https://github.com/EJHyun/THOR.",
        "file_path": "paper_data/knowledge_graph_embedding/info/5f9ea28be0d3bb9a73d62512190a772b10e92db0.pdf",
        "venue": "Industrial Conference on Data Mining",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "The goal of temporal knowledge graph embedding (TKGE) is to represent the entities and relations in a given temporal knowledge graph (TKG) as low-dimensional vectors (i.e., embeddings), which preserve both semantic information and temporal dynamics of the factual information. In this paper, we posit that the intrinsic difficulty of existing TKGE methods lies in the lack of information in KG snapshots with timestamps, each of which contains the facts that co-occur at a specific timestamp. To address this challenge, we propose a novel self-supervised TKGE approach, THOR (Three-tower grapH cOnvolution netwoRks (GCNs)), which extracts latent knowledge from TKGs by jointly leveraging both temporal and atemporal dependencies between entities and the structural dependency between relations. THOR learns the embeddings of entities and relations Our experiments on three real-world datasets demonstrate that THOR significantly outperforms 13 competitors in terms of TKG completion tasks. The codebase of THOR is available at https://github.com/EJHyun/THOR.",
        "keywords": []
      },
      "file_name": "5f9ea28be0d3bb9a73d62512190a772b10e92db0.pdf"
    },
    {
      "success": true,
      "doc_id": "e1298fe65f14867c4b24a6d716d589ba",
      "summary": "While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently. To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph. Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage. Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget, which achieves 9.1% average relative improvement for four embedding models on the large-scale KGs in open graph benchmark. Our code is released in https://github. com/AutoML-Research/KGTuner.",
      "intriguing_abstract": "While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently. To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph. Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage. Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget, which achieves 9.1% average relative improvement for four embedding models on the large-scale KGs in open graph benchmark. Our code is released in https://github. com/AutoML-Research/KGTuner.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/836d1d1c94f0fd0713c77b86ce136fffd059dbc0.pdf",
      "citation_key": "zhang2022fpm",
      "metadata": {
        "title": "Efficient Hyper-parameter Search for Knowledge Graph Embedding",
        "authors": [
          "Yongqi Zhang",
          "Zhanke Zhou",
          "Quanming Yao",
          "Yong Li"
        ],
        "published_date": "2022",
        "abstract": "While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently. To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph. Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage. Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget, which achieves 9.1% average relative improvement for four embedding models on the large-scale KGs in open graph benchmark. Our code is released in https://github. com/AutoML-Research/KGTuner.",
        "file_path": "paper_data/knowledge_graph_embedding/info/836d1d1c94f0fd0713c77b86ce136fffd059dbc0.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "While hyper-parameters (HPs) are important for knowledge graph (KG) learning, existing methods fail to search them efficiently. To solve this problem, we first analyze the properties of different HPs and measure the transfer ability from small subgraph to the full graph. Based on the analysis, we propose an efficient two-stage search algorithm KGTuner, which efficiently explores HP configurations on small subgraph at the first stage and transfers the top-performed configurations for fine-tuning on the large full graph at the second stage. Experiments show that our method can consistently find better HPs than the baseline algorithms within the same time budget, which achieves 9.1% average relative improvement for four embedding models on the large-scale KGs in open graph benchmark. Our code is released in https://github. com/AutoML-Research/KGTuner.",
        "keywords": []
      },
      "file_name": "836d1d1c94f0fd0713c77b86ce136fffd059dbc0.pdf"
    },
    {
      "success": true,
      "doc_id": "6d6cf2ffdb5643064a0e97e9fb0596e1",
      "summary": "Recommender system is able to realize personalized information filtering, which is a key way for knowledge discovering in information-rich environment. Knowledge graphs contain rich semantic associations between entities, which can be utilized to strengthen relationships between recommended items and bring interpretability for recommendation. With the establishment of knowledge graphs in various fields, the increasing numbers of researchers have carried out studies on recommendation algorithm based on knowledge graphs. Among them, recommendation algorithm based on knowledge graph embedding is a kind of simple and effective way to introduce knowledge entities and their relation into traditional recommender system. In this paper, the study about this kind of studies is summarized by analysis of existing literatures. We discuss and compare several studies, and the key elements of these algorithms are statistically analyzed.",
      "intriguing_abstract": "Recommender system is able to realize personalized information filtering, which is a key way for knowledge discovering in information-rich environment. Knowledge graphs contain rich semantic associations between entities, which can be utilized to strengthen relationships between recommended items and bring interpretability for recommendation. With the establishment of knowledge graphs in various fields, the increasing numbers of researchers have carried out studies on recommendation algorithm based on knowledge graphs. Among them, recommendation algorithm based on knowledge graph embedding is a kind of simple and effective way to introduce knowledge entities and their relation into traditional recommender system. In this paper, the study about this kind of studies is summarized by analysis of existing literatures. We discuss and compare several studies, and the key elements of these algorithms are statistically analyzed.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf",
      "citation_key": "liu2019e1u",
      "metadata": {
        "title": "A Survey of Recommendation Algorithms Based on Knowledge Graph Embedding",
        "authors": [
          "Chang Liu",
          "Lun Li",
          "Xiaolu Yao",
          "Lin Tang"
        ],
        "published_date": "2019",
        "abstract": "Recommender system is able to realize personalized information filtering, which is a key way for knowledge discovering in information-rich environment. Knowledge graphs contain rich semantic associations between entities, which can be utilized to strengthen relationships between recommended items and bring interpretability for recommendation. With the establishment of knowledge graphs in various fields, the increasing numbers of researchers have carried out studies on recommendation algorithm based on knowledge graphs. Among them, recommendation algorithm based on knowledge graph embedding is a kind of simple and effective way to introduce knowledge entities and their relation into traditional recommender system. In this paper, the study about this kind of studies is summarized by analysis of existing literatures. We discuss and compare several studies, and the key elements of these algorithms are statistically analyzed.",
        "file_path": "paper_data/knowledge_graph_embedding/info/0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf",
        "venue": "2019 IEEE International Conference on Computer Science and Educational Informatization (CSEI)",
        "citationCount": 26,
        "score": 4.333333333333333,
        "summary": "Recommender system is able to realize personalized information filtering, which is a key way for knowledge discovering in information-rich environment. Knowledge graphs contain rich semantic associations between entities, which can be utilized to strengthen relationships between recommended items and bring interpretability for recommendation. With the establishment of knowledge graphs in various fields, the increasing numbers of researchers have carried out studies on recommendation algorithm based on knowledge graphs. Among them, recommendation algorithm based on knowledge graph embedding is a kind of simple and effective way to introduce knowledge entities and their relation into traditional recommender system. In this paper, the study about this kind of studies is summarized by analysis of existing literatures. We discuss and compare several studies, and the key elements of these algorithms are statistically analyzed.",
        "keywords": []
      },
      "file_name": "0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf"
    },
    {
      "success": true,
      "doc_id": "a689841df0133f40af37c4f3a53b49ab",
      "summary": "Metaphor is a figure of speech that describes one thing (a target) by mentioning another thing (a source) in a way that is not literally true. Metaphor understanding is an interesting but challenging problem in natural language processing. This paper presents a novel method for metaphor processing based on knowledge graph (KG) embedding. Conceptually, we abstract the structure of a metaphor as an attribute-dependent relation between the target and the source. Each specific metaphor can be represented as a metaphor triple $(target, attribute, source)$. Therefore, we can model metaphor triples just like modeling fact triples in a KG and exploit KG embedding techniques to learn better representations of concepts, attributes and concept relations. In this way, metaphor interpretation and generation could be seen as KG completion, while metaphor detection could be viewed as a representation learning enhanced concept pair classification problem. Technically, we build a Chinese metaphor KG in the form of metaphor triples based on simile recognition, and also extract concept-attribute collocations to help describe concepts and measure concept relations. We extend the translation-based and the rotation-based KG embedding models to jointly optimize metaphor KG embedding and concept-attribute collocation embedding. Experimental results demonstrate the effectiveness of our method. Simile recognition is feasible for building the metaphor triple resource. The proposed models improve the performance on metaphor interpretation and generation, and the learned representations also benefit nominal metaphor detection compared with strong baselines.",
      "intriguing_abstract": "Metaphor is a figure of speech that describes one thing (a target) by mentioning another thing (a source) in a way that is not literally true. Metaphor understanding is an interesting but challenging problem in natural language processing. This paper presents a novel method for metaphor processing based on knowledge graph (KG) embedding. Conceptually, we abstract the structure of a metaphor as an attribute-dependent relation between the target and the source. Each specific metaphor can be represented as a metaphor triple $(target, attribute, source)$. Therefore, we can model metaphor triples just like modeling fact triples in a KG and exploit KG embedding techniques to learn better representations of concepts, attributes and concept relations. In this way, metaphor interpretation and generation could be seen as KG completion, while metaphor detection could be viewed as a representation learning enhanced concept pair classification problem. Technically, we build a Chinese metaphor KG in the form of metaphor triples based on simile recognition, and also extract concept-attribute collocations to help describe concepts and measure concept relations. We extend the translation-based and the rotation-based KG embedding models to jointly optimize metaphor KG embedding and concept-attribute collocation embedding. Experimental results demonstrate the effectiveness of our method. Simile recognition is feasible for building the metaphor triple resource. The proposed models improve the performance on metaphor interpretation and generation, and the learned representations also benefit nominal metaphor detection compared with strong baselines.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/00529345e4a604674477f8a1dc1333114883b8d9.pdf",
      "citation_key": "song2021fnl",
      "metadata": {
        "title": "A Knowledge Graph Embedding Approach for Metaphor Processing",
        "authors": [
          "Wei Song",
          "Jingjin Guo",
          "Ruiji Fu",
          "Ting Liu",
          "Lizhen Liu"
        ],
        "published_date": "2021",
        "abstract": "Metaphor is a figure of speech that describes one thing (a target) by mentioning another thing (a source) in a way that is not literally true. Metaphor understanding is an interesting but challenging problem in natural language processing. This paper presents a novel method for metaphor processing based on knowledge graph (KG) embedding. Conceptually, we abstract the structure of a metaphor as an attribute-dependent relation between the target and the source. Each specific metaphor can be represented as a metaphor triple $(target, attribute, source)$. Therefore, we can model metaphor triples just like modeling fact triples in a KG and exploit KG embedding techniques to learn better representations of concepts, attributes and concept relations. In this way, metaphor interpretation and generation could be seen as KG completion, while metaphor detection could be viewed as a representation learning enhanced concept pair classification problem. Technically, we build a Chinese metaphor KG in the form of metaphor triples based on simile recognition, and also extract concept-attribute collocations to help describe concepts and measure concept relations. We extend the translation-based and the rotation-based KG embedding models to jointly optimize metaphor KG embedding and concept-attribute collocation embedding. Experimental results demonstrate the effectiveness of our method. Simile recognition is feasible for building the metaphor triple resource. The proposed models improve the performance on metaphor interpretation and generation, and the learned representations also benefit nominal metaphor detection compared with strong baselines.",
        "file_path": "paper_data/knowledge_graph_embedding/info/00529345e4a604674477f8a1dc1333114883b8d9.pdf",
        "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "citationCount": 17,
        "score": 4.25,
        "summary": "Metaphor is a figure of speech that describes one thing (a target) by mentioning another thing (a source) in a way that is not literally true. Metaphor understanding is an interesting but challenging problem in natural language processing. This paper presents a novel method for metaphor processing based on knowledge graph (KG) embedding. Conceptually, we abstract the structure of a metaphor as an attribute-dependent relation between the target and the source. Each specific metaphor can be represented as a metaphor triple $(target, attribute, source)$. Therefore, we can model metaphor triples just like modeling fact triples in a KG and exploit KG embedding techniques to learn better representations of concepts, attributes and concept relations. In this way, metaphor interpretation and generation could be seen as KG completion, while metaphor detection could be viewed as a representation learning enhanced concept pair classification problem. Technically, we build a Chinese metaphor KG in the form of metaphor triples based on simile recognition, and also extract concept-attribute collocations to help describe concepts and measure concept relations. We extend the translation-based and the rotation-based KG embedding models to jointly optimize metaphor KG embedding and concept-attribute collocation embedding. Experimental results demonstrate the effectiveness of our method. Simile recognition is feasible for building the metaphor triple resource. The proposed models improve the performance on metaphor interpretation and generation, and the learned representations also benefit nominal metaphor detection compared with strong baselines.",
        "keywords": []
      },
      "file_name": "00529345e4a604674477f8a1dc1333114883b8d9.pdf"
    },
    {
      "success": true,
      "doc_id": "da6a2388605bef03af8f114bb575f77e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f0d5351c76448e28626177ece5ce97715087a0f9.pdf",
      "citation_key": "gradgyenge2017xdy",
      "metadata": {
        "title": "Graph Embedding Based Recommendation Techniques on the Knowledge Graph",
        "authors": [
          "Lszl Grad-Gyenge",
          "A. Kiss",
          "P. Filzmoser"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/f0d5351c76448e28626177ece5ce97715087a0f9.pdf",
        "venue": "User Modeling, Adaptation, and Personalization",
        "citationCount": 33,
        "score": 4.125,
        "summary": "",
        "keywords": []
      },
      "file_name": "f0d5351c76448e28626177ece5ce97715087a0f9.pdf"
    },
    {
      "success": true,
      "doc_id": "dde49e436899390bcc350cd574e4e2e5",
      "summary": "Knowledge graph embedding is an effective way to represent knowledge graph, which greatly enhance the performances on knowledge graph completion tasks, e.g., entity or relation prediction. For knowledge graph embedding models, designing a powerful loss framework is crucial to the discrimination between correct and incorrect triplets. Margin-based ranking loss is a commonly used negative sampling framework to make a suitable margin between the scores of positive and negative triples. However, this loss can not ensure ideal low scores for the positive triplets and high scores for the negative triplets, which is not beneficial for knowledge completion tasks. In this paper, we present a double limit scoring loss to separately set upper bound for correct triplets and lower bound for incorrect triplets, which provides more effective and flexible optimization for knowledge graph embedding. Upon the presented loss framework, we present several knowledge graph embedding models including TransE-SS, TransH-SS, TransD-SS, ProjE-SS and ComplEx-SS. The experimental results on link prediction and triplet classification show that our proposed models have the significant improvement compared to state-of-the-art baselines.",
      "intriguing_abstract": "Knowledge graph embedding is an effective way to represent knowledge graph, which greatly enhance the performances on knowledge graph completion tasks, e.g., entity or relation prediction. For knowledge graph embedding models, designing a powerful loss framework is crucial to the discrimination between correct and incorrect triplets. Margin-based ranking loss is a commonly used negative sampling framework to make a suitable margin between the scores of positive and negative triples. However, this loss can not ensure ideal low scores for the positive triplets and high scores for the negative triplets, which is not beneficial for knowledge completion tasks. In this paper, we present a double limit scoring loss to separately set upper bound for correct triplets and lower bound for incorrect triplets, which provides more effective and flexible optimization for knowledge graph embedding. Upon the presented loss framework, we present several knowledge graph embedding models including TransE-SS, TransH-SS, TransD-SS, ProjE-SS and ComplEx-SS. The experimental results on link prediction and triplet classification show that our proposed models have the significant improvement compared to state-of-the-art baselines.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9866a21c0ada20b62b28b3722c975595be819e24.pdf",
      "citation_key": "zhou20218bt",
      "metadata": {
        "title": "Knowledge Graph Embedding by Double Limit Scoring Loss",
        "authors": [
          "Xiaofei Zhou",
          "Lingfeng Niu",
          "Qiannan Zhu",
          "Xingquan Zhu",
          "Ping Liu",
          "Jianlong Tan",
          "Li Guo"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding is an effective way to represent knowledge graph, which greatly enhance the performances on knowledge graph completion tasks, e.g., entity or relation prediction. For knowledge graph embedding models, designing a powerful loss framework is crucial to the discrimination between correct and incorrect triplets. Margin-based ranking loss is a commonly used negative sampling framework to make a suitable margin between the scores of positive and negative triples. However, this loss can not ensure ideal low scores for the positive triplets and high scores for the negative triplets, which is not beneficial for knowledge completion tasks. In this paper, we present a double limit scoring loss to separately set upper bound for correct triplets and lower bound for incorrect triplets, which provides more effective and flexible optimization for knowledge graph embedding. Upon the presented loss framework, we present several knowledge graph embedding models including TransE-SS, TransH-SS, TransD-SS, ProjE-SS and ComplEx-SS. The experimental results on link prediction and triplet classification show that our proposed models have the significant improvement compared to state-of-the-art baselines.",
        "file_path": "paper_data/knowledge_graph_embedding/info/9866a21c0ada20b62b28b3722c975595be819e24.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 16,
        "score": 4.0,
        "summary": "Knowledge graph embedding is an effective way to represent knowledge graph, which greatly enhance the performances on knowledge graph completion tasks, e.g., entity or relation prediction. For knowledge graph embedding models, designing a powerful loss framework is crucial to the discrimination between correct and incorrect triplets. Margin-based ranking loss is a commonly used negative sampling framework to make a suitable margin between the scores of positive and negative triples. However, this loss can not ensure ideal low scores for the positive triplets and high scores for the negative triplets, which is not beneficial for knowledge completion tasks. In this paper, we present a double limit scoring loss to separately set upper bound for correct triplets and lower bound for incorrect triplets, which provides more effective and flexible optimization for knowledge graph embedding. Upon the presented loss framework, we present several knowledge graph embedding models including TransE-SS, TransH-SS, TransD-SS, ProjE-SS and ComplEx-SS. The experimental results on link prediction and triplet classification show that our proposed models have the significant improvement compared to state-of-the-art baselines.",
        "keywords": []
      },
      "file_name": "9866a21c0ada20b62b28b3722c975595be819e24.pdf"
    },
    {
      "success": true,
      "doc_id": "1eedd472f3c242ab8fb4285f2a48a1f6",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/50e7017c7768b7b2f5215a35539db1490ddc37ab.pdf",
      "citation_key": "chen20210ah",
      "metadata": {
        "title": "MbiusE: Knowledge Graph Embedding on Mbius Ring",
        "authors": [
          "Yao Chen",
          "Jiangang Liu",
          "Zhe Zhang",
          "Shiping Wen",
          "Wenjun Xiong"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/50e7017c7768b7b2f5215a35539db1490ddc37ab.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 16,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "50e7017c7768b7b2f5215a35539db1490ddc37ab.pdf"
    },
    {
      "success": true,
      "doc_id": "50d4e6158de853a97f7150af9d3f50d8",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/95a501bfe4b09323e6e178edd64dc24a6935c23f.pdf",
      "citation_key": "zhang2020i7j",
      "metadata": {
        "title": "Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding",
        "authors": [
          "Yongqi Zhang",
          "Quanming Yao",
          "Lei Chen"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/95a501bfe4b09323e6e178edd64dc24a6935c23f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 20,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "95a501bfe4b09323e6e178edd64dc24a6935c23f.pdf"
    },
    {
      "success": true,
      "doc_id": "54ca9fd8645a6a2ff538078bd401b826",
      "summary": "TorchKGE is a Python module for knowledge graph (KG) embedding relying solely on PyTorch. This package provides researchers and engineers with a clean and efficient API to design and test new models. It features a KG data structure, simple model interfaces and modules for negative sampling and model evaluation. Its main strength is a very fast evaluation module for the link prediction task, a central application of KG embedding. Various KG embedding models are also already implemented. Special attention has been paid to code efficiency and simplicity, documentation and API consistency. It is distributed using PyPI under BSD license. Source code and pointers to documentation and deployment can be found at this https URL.",
      "intriguing_abstract": "TorchKGE is a Python module for knowledge graph (KG) embedding relying solely on PyTorch. This package provides researchers and engineers with a clean and efficient API to design and test new models. It features a KG data structure, simple model interfaces and modules for negative sampling and model evaluation. Its main strength is a very fast evaluation module for the link prediction task, a central application of KG embedding. Various KG embedding models are also already implemented. Special attention has been paid to code efficiency and simplicity, documentation and API consistency. It is distributed using PyPI under BSD license. Source code and pointers to documentation and deployment can be found at this https URL.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/46b5198a535dfcaf1cc7d57d471ad9ec050e46cf.pdf",
      "citation_key": "boschin2020ki4",
      "metadata": {
        "title": "TorchKGE: Knowledge Graph Embedding in Python and PyTorch",
        "authors": [
          "Armand Boschin"
        ],
        "published_date": "2020",
        "abstract": "TorchKGE is a Python module for knowledge graph (KG) embedding relying solely on PyTorch. This package provides researchers and engineers with a clean and efficient API to design and test new models. It features a KG data structure, simple model interfaces and modules for negative sampling and model evaluation. Its main strength is a very fast evaluation module for the link prediction task, a central application of KG embedding. Various KG embedding models are also already implemented. Special attention has been paid to code efficiency and simplicity, documentation and API consistency. It is distributed using PyPI under BSD license. Source code and pointers to documentation and deployment can be found at this https URL.",
        "file_path": "paper_data/knowledge_graph_embedding/info/46b5198a535dfcaf1cc7d57d471ad9ec050e46cf.pdf",
        "venue": "arXiv.org",
        "citationCount": 20,
        "score": 4.0,
        "summary": "TorchKGE is a Python module for knowledge graph (KG) embedding relying solely on PyTorch. This package provides researchers and engineers with a clean and efficient API to design and test new models. It features a KG data structure, simple model interfaces and modules for negative sampling and model evaluation. Its main strength is a very fast evaluation module for the link prediction task, a central application of KG embedding. Various KG embedding models are also already implemented. Special attention has been paid to code efficiency and simplicity, documentation and API consistency. It is distributed using PyPI under BSD license. Source code and pointers to documentation and deployment can be found at this https URL.",
        "keywords": []
      },
      "file_name": "46b5198a535dfcaf1cc7d57d471ad9ec050e46cf.pdf"
    },
    {
      "success": true,
      "doc_id": "f2b4a57120ec3e3e8b412b9e4484a9b6",
      "summary": "Large scale knowledge graph embedding has attracted much attention from both academia and industry in the field of Artificial Intelligence. However, most existing methods concentrate solely on fact triples contained in the given knowledge graph. Inspired by the fact that logic rules can provide a flexible and declarative language for expressing rich background knowledge, it is natural to integrate logic rules into knowledge graph embedding, to transfer human knowledge to entity and relation embedding, and strengthen the learning process. In this paper, we propose a novel logic rule-enhanced method which can be easily integrated with any translation based knowledge graph embedding model, such as TransE . We first introduce a method to automatically mine the logic rules and corresponding confidences from the triples. And then, to put both triples and mined logic rules within the same semantic space, all triples in the knowledge graph are represented as first-order logic. Finally, we define several operations on the first-order logic and minimize a global loss over both of the mined logic rules and the transformed first-order logics. We conduct extensive experiments for link prediction and triple classification on three datasets: WN18, FB166, and FB15K. Experiments show that the rule-enhanced method can significantly improve the performance of several baselines. The highlight of our model is that the filtered Hits@1, which is a pivotal evaluation in the knowledge inference task, has a significant improvement (up to 700% improvement).",
      "intriguing_abstract": "Large scale knowledge graph embedding has attracted much attention from both academia and industry in the field of Artificial Intelligence. However, most existing methods concentrate solely on fact triples contained in the given knowledge graph. Inspired by the fact that logic rules can provide a flexible and declarative language for expressing rich background knowledge, it is natural to integrate logic rules into knowledge graph embedding, to transfer human knowledge to entity and relation embedding, and strengthen the learning process. In this paper, we propose a novel logic rule-enhanced method which can be easily integrated with any translation based knowledge graph embedding model, such as TransE . We first introduce a method to automatically mine the logic rules and corresponding confidences from the triples. And then, to put both triples and mined logic rules within the same semantic space, all triples in the knowledge graph are represented as first-order logic. Finally, we define several operations on the first-order logic and minimize a global loss over both of the mined logic rules and the transformed first-order logics. We conduct extensive experiments for link prediction and triple classification on three datasets: WN18, FB166, and FB15K. Experiments show that the rule-enhanced method can significantly improve the performance of several baselines. The highlight of our model is that the filtered Hits@1, which is a pivotal evaluation in the knowledge inference task, has a significant improvement (up to 700% improvement).",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/cda7a1bdce2bfa77c2d463b631ff84b69ce3c9ed.pdf",
      "citation_key": "wang20199fe",
      "metadata": {
        "title": "Logic Rules Powered Knowledge Graph Embedding",
        "authors": [
          "P. Wang",
          "D. Dou",
          "Fangzhao Wu",
          "Nisansa de Silva",
          "Lianwen Jin"
        ],
        "published_date": "2019",
        "abstract": "Large scale knowledge graph embedding has attracted much attention from both academia and industry in the field of Artificial Intelligence. However, most existing methods concentrate solely on fact triples contained in the given knowledge graph. Inspired by the fact that logic rules can provide a flexible and declarative language for expressing rich background knowledge, it is natural to integrate logic rules into knowledge graph embedding, to transfer human knowledge to entity and relation embedding, and strengthen the learning process. In this paper, we propose a novel logic rule-enhanced method which can be easily integrated with any translation based knowledge graph embedding model, such as TransE . We first introduce a method to automatically mine the logic rules and corresponding confidences from the triples. And then, to put both triples and mined logic rules within the same semantic space, all triples in the knowledge graph are represented as first-order logic. Finally, we define several operations on the first-order logic and minimize a global loss over both of the mined logic rules and the transformed first-order logics. We conduct extensive experiments for link prediction and triple classification on three datasets: WN18, FB166, and FB15K. Experiments show that the rule-enhanced method can significantly improve the performance of several baselines. The highlight of our model is that the filtered Hits@1, which is a pivotal evaluation in the knowledge inference task, has a significant improvement (up to 700% improvement).",
        "file_path": "paper_data/knowledge_graph_embedding/info/cda7a1bdce2bfa77c2d463b631ff84b69ce3c9ed.pdf",
        "venue": "arXiv.org",
        "citationCount": 23,
        "score": 3.833333333333333,
        "summary": "Large scale knowledge graph embedding has attracted much attention from both academia and industry in the field of Artificial Intelligence. However, most existing methods concentrate solely on fact triples contained in the given knowledge graph. Inspired by the fact that logic rules can provide a flexible and declarative language for expressing rich background knowledge, it is natural to integrate logic rules into knowledge graph embedding, to transfer human knowledge to entity and relation embedding, and strengthen the learning process. In this paper, we propose a novel logic rule-enhanced method which can be easily integrated with any translation based knowledge graph embedding model, such as TransE . We first introduce a method to automatically mine the logic rules and corresponding confidences from the triples. And then, to put both triples and mined logic rules within the same semantic space, all triples in the knowledge graph are represented as first-order logic. Finally, we define several operations on the first-order logic and minimize a global loss over both of the mined logic rules and the transformed first-order logics. We conduct extensive experiments for link prediction and triple classification on three datasets: WN18, FB166, and FB15K. Experiments show that the rule-enhanced method can significantly improve the performance of several baselines. The highlight of our model is that the filtered Hits@1, which is a pivotal evaluation in the knowledge inference task, has a significant improvement (up to 700% improvement).",
        "keywords": []
      },
      "file_name": "cda7a1bdce2bfa77c2d463b631ff84b69ce3c9ed.pdf"
    },
    {
      "success": true,
      "doc_id": "ce78acd5cf28321303b55b6fe17bf4a9",
      "summary": "Exploring the effects a chemical compound has on a species takes a considerable experimental effort. Appropriate methods for estimating and suggesting new effects can dramatically reduce the work needed to be done by a laboratory. In this paper we explore the suitability of using a knowledge graph embedding approach for ecotoxicological effect prediction. A knowledge graph has been constructed from publicly available data sets, including a species taxonomy and chemical classification and similarity. The publicly available effect data is integrated to the knowledge graph using ontology alignment techniques. Our experimental results show that the knowledge graph based approach improves the selected baselines.",
      "intriguing_abstract": "Exploring the effects a chemical compound has on a species takes a considerable experimental effort. Appropriate methods for estimating and suggesting new effects can dramatically reduce the work needed to be done by a laboratory. In this paper we explore the suitability of using a knowledge graph embedding approach for ecotoxicological effect prediction. A knowledge graph has been constructed from publicly available data sets, including a species taxonomy and chemical classification and similarity. The publicly available effect data is integrated to the knowledge graph using ontology alignment techniques. Our experimental results show that the knowledge graph based approach improves the selected baselines.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f76a6e8f059820667af53edbd42d33fc4bca85fd.pdf",
      "citation_key": "myklebust201941l",
      "metadata": {
        "title": "Knowledge Graph Embedding for Ecotoxicological Effect Prediction",
        "authors": [
          "E. B. Myklebust",
          "Ernesto Jimnez-Ruiz",
          "Jiaoyan Chen",
          "Raoul Wolf",
          "K. Tollefsen"
        ],
        "published_date": "2019",
        "abstract": "Exploring the effects a chemical compound has on a species takes a considerable experimental effort. Appropriate methods for estimating and suggesting new effects can dramatically reduce the work needed to be done by a laboratory. In this paper we explore the suitability of using a knowledge graph embedding approach for ecotoxicological effect prediction. A knowledge graph has been constructed from publicly available data sets, including a species taxonomy and chemical classification and similarity. The publicly available effect data is integrated to the knowledge graph using ontology alignment techniques. Our experimental results show that the knowledge graph based approach improves the selected baselines.",
        "file_path": "paper_data/knowledge_graph_embedding/info/f76a6e8f059820667af53edbd42d33fc4bca85fd.pdf",
        "venue": "International Workshop on the Semantic Web",
        "citationCount": 23,
        "score": 3.833333333333333,
        "summary": "Exploring the effects a chemical compound has on a species takes a considerable experimental effort. Appropriate methods for estimating and suggesting new effects can dramatically reduce the work needed to be done by a laboratory. In this paper we explore the suitability of using a knowledge graph embedding approach for ecotoxicological effect prediction. A knowledge graph has been constructed from publicly available data sets, including a species taxonomy and chemical classification and similarity. The publicly available effect data is integrated to the knowledge graph using ontology alignment techniques. Our experimental results show that the knowledge graph based approach improves the selected baselines.",
        "keywords": []
      },
      "file_name": "f76a6e8f059820667af53edbd42d33fc4bca85fd.pdf"
    },
    {
      "success": true,
      "doc_id": "8467f21d8b9abc97fe54951ec5b1339b",
      "summary": "Recommendation systems are information filtering mechanisms used in E-commerce, media and entertainment industry. It essentially facilitate the customers for a better user experience by processing the content user-specific. This is known as personalization. However, though leveraged by machine learning algorithms existing recommendation systems, still suffers from the problem of cold-start and sparcity. These problems could be resolved by using knowledge graphs since it gives a semantic explanation of recommendations. Also, graph learning method overcomes the problems of manual feature extraction and is effective for feature learning in predicting tasks. In this research, we develop a semantic based recommender through link prediction in a knowledge graph. We apply graph embedding techniques for extracting the semantics of explicable recommendations. The proposed method is validated by building a knowledge graph using the MovieLens dataset. We observed that factorization based scoring functions such as HolE and DistMult provides better semantic recommendations.",
      "intriguing_abstract": "Recommendation systems are information filtering mechanisms used in E-commerce, media and entertainment industry. It essentially facilitate the customers for a better user experience by processing the content user-specific. This is known as personalization. However, though leveraged by machine learning algorithms existing recommendation systems, still suffers from the problem of cold-start and sparcity. These problems could be resolved by using knowledge graphs since it gives a semantic explanation of recommendations. Also, graph learning method overcomes the problems of manual feature extraction and is effective for feature learning in predicting tasks. In this research, we develop a semantic based recommender through link prediction in a knowledge graph. We apply graph embedding techniques for extracting the semantics of explicable recommendations. The proposed method is validated by building a knowledge graph using the MovieLens dataset. We observed that factorization based scoring functions such as HolE and DistMult provides better semantic recommendations.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/40667a731593a44d4e2f9391f1d14f368321b751.pdf",
      "citation_key": "kartheek2021aj7",
      "metadata": {
        "title": "Building Semantic Based Recommender System Using Knowledge Graph Embedding",
        "authors": [
          "Miriyala Kartheek",
          "G. Sajeev"
        ],
        "published_date": "2021",
        "abstract": "Recommendation systems are information filtering mechanisms used in E-commerce, media and entertainment industry. It essentially facilitate the customers for a better user experience by processing the content user-specific. This is known as personalization. However, though leveraged by machine learning algorithms existing recommendation systems, still suffers from the problem of cold-start and sparcity. These problems could be resolved by using knowledge graphs since it gives a semantic explanation of recommendations. Also, graph learning method overcomes the problems of manual feature extraction and is effective for feature learning in predicting tasks. In this research, we develop a semantic based recommender through link prediction in a knowledge graph. We apply graph embedding techniques for extracting the semantics of explicable recommendations. The proposed method is validated by building a knowledge graph using the MovieLens dataset. We observed that factorization based scoring functions such as HolE and DistMult provides better semantic recommendations.",
        "file_path": "paper_data/knowledge_graph_embedding/info/40667a731593a44d4e2f9391f1d14f368321b751.pdf",
        "venue": "International Conference on Intelligent Information Processing",
        "citationCount": 15,
        "score": 3.75,
        "summary": "Recommendation systems are information filtering mechanisms used in E-commerce, media and entertainment industry. It essentially facilitate the customers for a better user experience by processing the content user-specific. This is known as personalization. However, though leveraged by machine learning algorithms existing recommendation systems, still suffers from the problem of cold-start and sparcity. These problems could be resolved by using knowledge graphs since it gives a semantic explanation of recommendations. Also, graph learning method overcomes the problems of manual feature extraction and is effective for feature learning in predicting tasks. In this research, we develop a semantic based recommender through link prediction in a knowledge graph. We apply graph embedding techniques for extracting the semantics of explicable recommendations. The proposed method is validated by building a knowledge graph using the MovieLens dataset. We observed that factorization based scoring functions such as HolE and DistMult provides better semantic recommendations.",
        "keywords": []
      },
      "file_name": "40667a731593a44d4e2f9391f1d14f368321b751.pdf"
    },
    {
      "success": true,
      "doc_id": "943f00019f37d9c986a9c8bfe9077a2b",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6bf53a97f5a3f5b0375f4702cbec28d8e9ab61c0.pdf",
      "citation_key": "sha2019plw",
      "metadata": {
        "title": "Attentive Knowledge Graph Embedding for Personalized Recommendation",
        "authors": [
          "Xiao Sha",
          "Zhu Sun",
          "Jie Zhang"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/6bf53a97f5a3f5b0375f4702cbec28d8e9ab61c0.pdf",
        "venue": "arXiv.org",
        "citationCount": 22,
        "score": 3.6666666666666665,
        "summary": "",
        "keywords": []
      },
      "file_name": "6bf53a97f5a3f5b0375f4702cbec28d8e9ab61c0.pdf"
    },
    {
      "success": true,
      "doc_id": "19dadbe601ef25115ccf5feeb2ae22e2",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4ae2631fb5e99cb64ff7d6e7ed3a1e6b0bedd269.pdf",
      "citation_key": "lu2020x6y",
      "metadata": {
        "title": "DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding",
        "authors": [
          "Haonan Lu",
          "Hailin Hu"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/4ae2631fb5e99cb64ff7d6e7ed3a1e6b0bedd269.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 3.6,
        "summary": "",
        "keywords": []
      },
      "file_name": "4ae2631fb5e99cb64ff7d6e7ed3a1e6b0bedd269.pdf"
    },
    {
      "success": true,
      "doc_id": "3810d682b5a3ac89abe06676785dccb7",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/d76b3bf29366b4f0902ea145a3f7c020a35f084f.pdf",
      "citation_key": "zhang2020c15",
      "metadata": {
        "title": "Improve the translational distance models for knowledge graph embedding",
        "authors": [
          "Siheng Zhang",
          "Zhengya Sun",
          "Wensheng Zhang"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/d76b3bf29366b4f0902ea145a3f7c020a35f084f.pdf",
        "venue": "Journal of Intelligence and Information Systems",
        "citationCount": 18,
        "score": 3.6,
        "summary": "",
        "keywords": []
      },
      "file_name": "d76b3bf29366b4f0902ea145a3f7c020a35f084f.pdf"
    },
    {
      "success": true,
      "doc_id": "feba2e454cb8fa9a09c540724a5b3f4c",
      "summary": "Knowledge graph embedding is studied to embed entities and relations of a knowledge graph into continuous vector spaces, which benefits a variety of real-world applications. Among existing solutions, translation-based models, which employ geometric translation to design score function, have drawn much attention. However, these models primarily concentrate on evidence from observing whether the triplets are plausible, and ignore the fact that the relation also implies certain semantic constraints on its subject or object entity. In this paper, we present a general framework for enhancing knowledge graph embedding with relational constraints (KRC). Specifically, we elaborately design the score function by encoding regularities between a relation and its arguments into the translation-based embedding space. Additionally, we propose a soft margin-based ranking loss for effectively training the KRC model, which characterizes different semantic distances between negative and positive triplets. Furthermore, we combine regularities with distributional representations to predict the missing triplets, which possesses certain robust guarantee. We evaluate our method on the task of knowledge graph completion. Extensive experiments show that KRC achieves substantial improvements against baselines.",
      "intriguing_abstract": "Knowledge graph embedding is studied to embed entities and relations of a knowledge graph into continuous vector spaces, which benefits a variety of real-world applications. Among existing solutions, translation-based models, which employ geometric translation to design score function, have drawn much attention. However, these models primarily concentrate on evidence from observing whether the triplets are plausible, and ignore the fact that the relation also implies certain semantic constraints on its subject or object entity. In this paper, we present a general framework for enhancing knowledge graph embedding with relational constraints (KRC). Specifically, we elaborately design the score function by encoding regularities between a relation and its arguments into the translation-based embedding space. Additionally, we propose a soft margin-based ranking loss for effectively training the KRC model, which characterizes different semantic distances between negative and positive triplets. Furthermore, we combine regularities with distributional representations to predict the missing triplets, which possesses certain robust guarantee. We evaluate our method on the task of knowledge graph completion. Extensive experiments show that KRC achieves substantial improvements against baselines.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/151c9bb547306d66ba252be7c20e35f711e9f330.pdf",
      "citation_key": "li2020ek4",
      "metadata": {
        "title": "Enhancing Knowledge Graph Embedding with Relational Constraints",
        "authors": [
          "Mingda Li",
          "Zhengya Sun",
          "Siheng Zhang",
          "Wensheng Zhang"
        ],
        "published_date": "2020",
        "abstract": "Knowledge graph embedding is studied to embed entities and relations of a knowledge graph into continuous vector spaces, which benefits a variety of real-world applications. Among existing solutions, translation-based models, which employ geometric translation to design score function, have drawn much attention. However, these models primarily concentrate on evidence from observing whether the triplets are plausible, and ignore the fact that the relation also implies certain semantic constraints on its subject or object entity. In this paper, we present a general framework for enhancing knowledge graph embedding with relational constraints (KRC). Specifically, we elaborately design the score function by encoding regularities between a relation and its arguments into the translation-based embedding space. Additionally, we propose a soft margin-based ranking loss for effectively training the KRC model, which characterizes different semantic distances between negative and positive triplets. Furthermore, we combine regularities with distributional representations to predict the missing triplets, which possesses certain robust guarantee. We evaluate our method on the task of knowledge graph completion. Extensive experiments show that KRC achieves substantial improvements against baselines.",
        "file_path": "paper_data/knowledge_graph_embedding/info/151c9bb547306d66ba252be7c20e35f711e9f330.pdf",
        "venue": "2020 IEEE International Conference on Knowledge Graph (ICKG)",
        "citationCount": 18,
        "score": 3.6,
        "summary": "Knowledge graph embedding is studied to embed entities and relations of a knowledge graph into continuous vector spaces, which benefits a variety of real-world applications. Among existing solutions, translation-based models, which employ geometric translation to design score function, have drawn much attention. However, these models primarily concentrate on evidence from observing whether the triplets are plausible, and ignore the fact that the relation also implies certain semantic constraints on its subject or object entity. In this paper, we present a general framework for enhancing knowledge graph embedding with relational constraints (KRC). Specifically, we elaborately design the score function by encoding regularities between a relation and its arguments into the translation-based embedding space. Additionally, we propose a soft margin-based ranking loss for effectively training the KRC model, which characterizes different semantic distances between negative and positive triplets. Furthermore, we combine regularities with distributional representations to predict the missing triplets, which possesses certain robust guarantee. We evaluate our method on the task of knowledge graph completion. Extensive experiments show that KRC achieves substantial improvements against baselines.",
        "keywords": []
      },
      "file_name": "151c9bb547306d66ba252be7c20e35f711e9f330.pdf"
    },
    {
      "success": true,
      "doc_id": "cc8102d39cde56c74c3f21e891e943ee",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c0827be29366be4b8cfa0dfbef4ead3f7b08f562.pdf",
      "citation_key": "li2020he5",
      "metadata": {
        "title": "Deep Hybrid Knowledge Graph Embedding for Top-N Recommendation",
        "authors": [
          "Jian Li",
          "Zhuoming Xu",
          "Yan Tang",
          "Bo Zhao",
          "Haimei Tian"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/c0827be29366be4b8cfa0dfbef4ead3f7b08f562.pdf",
        "venue": "Web Information System and Application Conference",
        "citationCount": 18,
        "score": 3.6,
        "summary": "",
        "keywords": []
      },
      "file_name": "c0827be29366be4b8cfa0dfbef4ead3f7b08f562.pdf"
    },
    {
      "success": true,
      "doc_id": "1d961c44de3a919130ec08e92e74a342",
      "summary": "In an age overflowing with information, the task of converting unstructured data into structured data are a vital task of great need. Currently, most relation extraction modules are more focused on the extraction of local mention-level relationsusually from short volumes of text. However, in most cases, the most vital and important relations are those that are described in length and detail. In this research, we propose GREG: A Global level Relation Extractor model using knowledge graph embeddings for document-level inputs. The model uses vector representations of mention-level local relations to construct knowledge graphs that can represent the input document. The knowledge graph is then used to predict global level relations from documents or large bodies of text. The proposed model is largely divided into two modules which are synchronized during their training. Thus, each of the models modules is designed to deal with local relations and global relations separately. This allows the model to avoid the problem of struggling against loss of information due to too much information crunched into smaller sized representations when attempting global level relation extraction. Through evaluation, we have shown that the proposed model yields high performances in both predicting global level relations and local level relations consistently.",
      "intriguing_abstract": "In an age overflowing with information, the task of converting unstructured data into structured data are a vital task of great need. Currently, most relation extraction modules are more focused on the extraction of local mention-level relationsusually from short volumes of text. However, in most cases, the most vital and important relations are those that are described in length and detail. In this research, we propose GREG: A Global level Relation Extractor model using knowledge graph embeddings for document-level inputs. The model uses vector representations of mention-level local relations to construct knowledge graphs that can represent the input document. The knowledge graph is then used to predict global level relations from documents or large bodies of text. The proposed model is largely divided into two modules which are synchronized during their training. Thus, each of the models modules is designed to deal with local relations and global relations separately. This allows the model to avoid the problem of struggling against loss of information due to too much information crunched into smaller sized representations when attempting global level relation extraction. Through evaluation, we have shown that the proposed model yields high performances in both predicting global level relations and local level relations consistently.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2d38cdaf2e232b5d1cb1dce388aa0fe75babcf29.pdf",
      "citation_key": "kim2020zu3",
      "metadata": {
        "title": "GREG: A Global Level Relation Extraction with Knowledge Graph Embedding",
        "authors": [
          "Kuekyeng Kim",
          "Yuna Hur",
          "Gyeongmin Kim",
          "Heuiseok Lim"
        ],
        "published_date": "2020",
        "abstract": "In an age overflowing with information, the task of converting unstructured data into structured data are a vital task of great need. Currently, most relation extraction modules are more focused on the extraction of local mention-level relationsusually from short volumes of text. However, in most cases, the most vital and important relations are those that are described in length and detail. In this research, we propose GREG: A Global level Relation Extractor model using knowledge graph embeddings for document-level inputs. The model uses vector representations of mention-level local relations to construct knowledge graphs that can represent the input document. The knowledge graph is then used to predict global level relations from documents or large bodies of text. The proposed model is largely divided into two modules which are synchronized during their training. Thus, each of the models modules is designed to deal with local relations and global relations separately. This allows the model to avoid the problem of struggling against loss of information due to too much information crunched into smaller sized representations when attempting global level relation extraction. Through evaluation, we have shown that the proposed model yields high performances in both predicting global level relations and local level relations consistently.",
        "file_path": "paper_data/knowledge_graph_embedding/info/2d38cdaf2e232b5d1cb1dce388aa0fe75babcf29.pdf",
        "venue": "Applied Sciences",
        "citationCount": 18,
        "score": 3.6,
        "summary": "In an age overflowing with information, the task of converting unstructured data into structured data are a vital task of great need. Currently, most relation extraction modules are more focused on the extraction of local mention-level relationsusually from short volumes of text. However, in most cases, the most vital and important relations are those that are described in length and detail. In this research, we propose GREG: A Global level Relation Extractor model using knowledge graph embeddings for document-level inputs. The model uses vector representations of mention-level local relations to construct knowledge graphs that can represent the input document. The knowledge graph is then used to predict global level relations from documents or large bodies of text. The proposed model is largely divided into two modules which are synchronized during their training. Thus, each of the models modules is designed to deal with local relations and global relations separately. This allows the model to avoid the problem of struggling against loss of information due to too much information crunched into smaller sized representations when attempting global level relation extraction. Through evaluation, we have shown that the proposed model yields high performances in both predicting global level relations and local level relations consistently.",
        "keywords": []
      },
      "file_name": "2d38cdaf2e232b5d1cb1dce388aa0fe75babcf29.pdf"
    },
    {
      "success": true,
      "doc_id": "01ed37e25ebb7e8cb532a7611692c5dc",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/d6508e8825a6a1281ad415de47a2f108d98df87d.pdf",
      "citation_key": "zhu2018l0u",
      "metadata": {
        "title": "Modeling the Correlations of Relations for Knowledge Graph Embedding",
        "authors": [
          "Jizhao Zhu",
          "Yantao Jia",
          "Jun Xu",
          "Jianzhong Qiao",
          "Xueqi Cheng"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/d6508e8825a6a1281ad415de47a2f108d98df87d.pdf",
        "venue": "Journal of Computational Science and Technology",
        "citationCount": 25,
        "score": 3.571428571428571,
        "summary": "",
        "keywords": []
      },
      "file_name": "d6508e8825a6a1281ad415de47a2f108d98df87d.pdf"
    },
    {
      "success": true,
      "doc_id": "8cc9e2adaeae649ffed2c9a801647f83",
      "summary": "Knowledge graphs contain rich relational structures of the world, and thus complement data-driven knowledge discovery from heterogeneous data. Relational inference between distant entities in large-scale knowledge graphs demands fast relation-specific algebraic manipulations. One of the most effective methods is to embed symbolic relations and entities into continuous spaces, where relations are approximately linear translation between projected images of entities in the relation space. However, state-of-art relation projection methods such as TransR, TransD or TransSparse do not model the correlation between relations, and thus are not scalable to complex knowledge graphs with thousands of relations, both in term of computational demand and statistical robustness. To this end we introduce TransF, a novel translation-based method which mitigates the burden of relation projection by explicitly modeling the basis subspaces of projection matrices. As a result, TransF is far more light weight than the existing projection methods, and is robust when facing a high number of relations. Experimental results on canonical link prediction and triples classification tasks show that our proposed model outperforms competing rivals by a large margin and achieves state-of-the-art performance. Especially, TransF improves by 9% (5%) on the head/tail entity prediction task with N-to-l (l-to-N) over the best performing translation-based method.",
      "intriguing_abstract": "Knowledge graphs contain rich relational structures of the world, and thus complement data-driven knowledge discovery from heterogeneous data. Relational inference between distant entities in large-scale knowledge graphs demands fast relation-specific algebraic manipulations. One of the most effective methods is to embed symbolic relations and entities into continuous spaces, where relations are approximately linear translation between projected images of entities in the relation space. However, state-of-art relation projection methods such as TransR, TransD or TransSparse do not model the correlation between relations, and thus are not scalable to complex knowledge graphs with thousands of relations, both in term of computational demand and statistical robustness. To this end we introduce TransF, a novel translation-based method which mitigates the burden of relation projection by explicitly modeling the basis subspaces of projection matrices. As a result, TransF is far more light weight than the existing projection methods, and is robust when facing a high number of relations. Experimental results on canonical link prediction and triples classification tasks show that our proposed model outperforms competing rivals by a large margin and achieves state-of-the-art performance. Especially, TransF improves by 9% (5%) on the head/tail entity prediction task with N-to-l (l-to-N) over the best performing translation-based method.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/18101998fb57704b79eb4c4c37891144ede8f8b9.pdf",
      "citation_key": "do20184o2",
      "metadata": {
        "title": "Knowledge Graph Embedding with Multiple Relation Projections",
        "authors": [
          "Kien Do",
          "T. Tran",
          "S. Venkatesh"
        ],
        "published_date": "2018",
        "abstract": "Knowledge graphs contain rich relational structures of the world, and thus complement data-driven knowledge discovery from heterogeneous data. Relational inference between distant entities in large-scale knowledge graphs demands fast relation-specific algebraic manipulations. One of the most effective methods is to embed symbolic relations and entities into continuous spaces, where relations are approximately linear translation between projected images of entities in the relation space. However, state-of-art relation projection methods such as TransR, TransD or TransSparse do not model the correlation between relations, and thus are not scalable to complex knowledge graphs with thousands of relations, both in term of computational demand and statistical robustness. To this end we introduce TransF, a novel translation-based method which mitigates the burden of relation projection by explicitly modeling the basis subspaces of projection matrices. As a result, TransF is far more light weight than the existing projection methods, and is robust when facing a high number of relations. Experimental results on canonical link prediction and triples classification tasks show that our proposed model outperforms competing rivals by a large margin and achieves state-of-the-art performance. Especially, TransF improves by 9% (5%) on the head/tail entity prediction task with N-to-l (l-to-N) over the best performing translation-based method.",
        "file_path": "paper_data/knowledge_graph_embedding/info/18101998fb57704b79eb4c4c37891144ede8f8b9.pdf",
        "venue": "International Conference on Pattern Recognition",
        "citationCount": 25,
        "score": 3.571428571428571,
        "summary": "Knowledge graphs contain rich relational structures of the world, and thus complement data-driven knowledge discovery from heterogeneous data. Relational inference between distant entities in large-scale knowledge graphs demands fast relation-specific algebraic manipulations. One of the most effective methods is to embed symbolic relations and entities into continuous spaces, where relations are approximately linear translation between projected images of entities in the relation space. However, state-of-art relation projection methods such as TransR, TransD or TransSparse do not model the correlation between relations, and thus are not scalable to complex knowledge graphs with thousands of relations, both in term of computational demand and statistical robustness. To this end we introduce TransF, a novel translation-based method which mitigates the burden of relation projection by explicitly modeling the basis subspaces of projection matrices. As a result, TransF is far more light weight than the existing projection methods, and is robust when facing a high number of relations. Experimental results on canonical link prediction and triples classification tasks show that our proposed model outperforms competing rivals by a large margin and achieves state-of-the-art performance. Especially, TransF improves by 9% (5%) on the head/tail entity prediction task with N-to-l (l-to-N) over the best performing translation-based method.",
        "keywords": []
      },
      "file_name": "18101998fb57704b79eb4c4c37891144ede8f8b9.pdf"
    },
    {
      "success": true,
      "doc_id": "a49199f86d052b031f422c4e1d567f6c",
      "summary": "In this work, the first quantum Anstze for the statistical relational learning on knowledge graphs using parametric quantum circuits are proposed. Two types of variational quantum circuits for knowledge graph embedding are introduced. Inspired by the classical representation learning, latent features for entities are first considered as coefficients of quantum states, while predicates are characterized by parametric gates acting on the quantum states. For the first model, the quantum advantages disappear when it comes to the optimization of this model. Therefore, a second quantum circuit model is introduced where embeddings of entities are generated from parameterized quantum gates acting on the pure quantum state. The benefit of the second method is that the quantum embeddings can be trained efficiently meanwhile preserving the quantum advantages. It is shown that the proposed methods can achieve comparable results to the stateoftheart classical models, for example, RESCAL, DistMult. Furthermore, after optimizing the models, the complexity of inductive inference on the knowledge graphs might be reduced with respect to the number of entities.",
      "intriguing_abstract": "In this work, the first quantum Anstze for the statistical relational learning on knowledge graphs using parametric quantum circuits are proposed. Two types of variational quantum circuits for knowledge graph embedding are introduced. Inspired by the classical representation learning, latent features for entities are first considered as coefficients of quantum states, while predicates are characterized by parametric gates acting on the quantum states. For the first model, the quantum advantages disappear when it comes to the optimization of this model. Therefore, a second quantum circuit model is introduced where embeddings of entities are generated from parameterized quantum gates acting on the pure quantum state. The benefit of the second method is that the quantum embeddings can be trained efficiently meanwhile preserving the quantum advantages. It is shown that the proposed methods can achieve comparable results to the stateoftheart classical models, for example, RESCAL, DistMult. Furthermore, after optimizing the models, the complexity of inductive inference on the knowledge graphs might be reduced with respect to the number of entities.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/23830bb104b25103162ec9f9f463624d9a434194.pdf",
      "citation_key": "ma20194ua",
      "metadata": {
        "title": "Variational Quantum Circuit Model for Knowledge Graph Embedding",
        "authors": [
          "Yunpu Ma",
          "Volker Tresp",
          "Liming Zhao",
          "Yuyi Wang"
        ],
        "published_date": "2019",
        "abstract": "In this work, the first quantum Anstze for the statistical relational learning on knowledge graphs using parametric quantum circuits are proposed. Two types of variational quantum circuits for knowledge graph embedding are introduced. Inspired by the classical representation learning, latent features for entities are first considered as coefficients of quantum states, while predicates are characterized by parametric gates acting on the quantum states. For the first model, the quantum advantages disappear when it comes to the optimization of this model. Therefore, a second quantum circuit model is introduced where embeddings of entities are generated from parameterized quantum gates acting on the pure quantum state. The benefit of the second method is that the quantum embeddings can be trained efficiently meanwhile preserving the quantum advantages. It is shown that the proposed methods can achieve comparable results to the stateoftheart classical models, for example, RESCAL, DistMult. Furthermore, after optimizing the models, the complexity of inductive inference on the knowledge graphs might be reduced with respect to the number of entities.",
        "file_path": "paper_data/knowledge_graph_embedding/info/23830bb104b25103162ec9f9f463624d9a434194.pdf",
        "venue": "Advanced Quantum Technologies",
        "citationCount": 21,
        "score": 3.5,
        "summary": "In this work, the first quantum Anstze for the statistical relational learning on knowledge graphs using parametric quantum circuits are proposed. Two types of variational quantum circuits for knowledge graph embedding are introduced. Inspired by the classical representation learning, latent features for entities are first considered as coefficients of quantum states, while predicates are characterized by parametric gates acting on the quantum states. For the first model, the quantum advantages disappear when it comes to the optimization of this model. Therefore, a second quantum circuit model is introduced where embeddings of entities are generated from parameterized quantum gates acting on the pure quantum state. The benefit of the second method is that the quantum embeddings can be trained efficiently meanwhile preserving the quantum advantages. It is shown that the proposed methods can achieve comparable results to the stateoftheart classical models, for example, RESCAL, DistMult. Furthermore, after optimizing the models, the complexity of inductive inference on the knowledge graphs might be reduced with respect to the number of entities.",
        "keywords": []
      },
      "file_name": "23830bb104b25103162ec9f9f463624d9a434194.pdf"
    },
    {
      "success": true,
      "doc_id": "9daac0e36d447669b0346dd9d68d2f83",
      "summary": "Along with the rapidly increasing massive online data, recommender systems have been used as an effective approach for filtering useful information, which have been widely adopted in many web applications. In recent years, new techniques such as neural network and deep learning have demonstrated its effectiveness in studies of recommender systems. In this paper, we propose a novel approach for collaborative filtering with implicit feedback based on knowledge graph embedding. The basic ideal is to model the interactions between users and items as an interaction knowledge graph with a single relation, whose vector representation is learned through knowledge graph embedding. Base on the learned representation, the collaborative filtering problem is converted into link prediction in the interaction knowledge graph. KGECF, a neural network for knowledge graph embedding based collaborative filtering, is proposed based on this ideal and the RotatE knowledge graph embedding model. Experimental results on five datasets with different characteristics show that the KGECF model achieves the state-of-the-art (SOTA) performance on all datasets. And unlike other SOTA models that have obvious performance drops on AMusic and AToy datasets, our models performance is very stable across all datasets.",
      "intriguing_abstract": "Along with the rapidly increasing massive online data, recommender systems have been used as an effective approach for filtering useful information, which have been widely adopted in many web applications. In recent years, new techniques such as neural network and deep learning have demonstrated its effectiveness in studies of recommender systems. In this paper, we propose a novel approach for collaborative filtering with implicit feedback based on knowledge graph embedding. The basic ideal is to model the interactions between users and items as an interaction knowledge graph with a single relation, whose vector representation is learned through knowledge graph embedding. Base on the learned representation, the collaborative filtering problem is converted into link prediction in the interaction knowledge graph. KGECF, a neural network for knowledge graph embedding based collaborative filtering, is proposed based on this ideal and the RotatE knowledge graph embedding model. Experimental results on five datasets with different characteristics show that the KGECF model achieves the state-of-the-art (SOTA) performance on all datasets. And unlike other SOTA models that have obvious performance drops on AMusic and AToy datasets, our models performance is very stable across all datasets.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/77e23cd2437c6afb16082793badbb02842442e13.pdf",
      "citation_key": "zhang2020wou",
      "metadata": {
        "title": "Knowledge Graph Embedding Based Collaborative Filtering",
        "authors": [
          "Yuhang Zhang",
          "Jun Wang",
          "Jie Luo"
        ],
        "published_date": "2020",
        "abstract": "Along with the rapidly increasing massive online data, recommender systems have been used as an effective approach for filtering useful information, which have been widely adopted in many web applications. In recent years, new techniques such as neural network and deep learning have demonstrated its effectiveness in studies of recommender systems. In this paper, we propose a novel approach for collaborative filtering with implicit feedback based on knowledge graph embedding. The basic ideal is to model the interactions between users and items as an interaction knowledge graph with a single relation, whose vector representation is learned through knowledge graph embedding. Base on the learned representation, the collaborative filtering problem is converted into link prediction in the interaction knowledge graph. KGECF, a neural network for knowledge graph embedding based collaborative filtering, is proposed based on this ideal and the RotatE knowledge graph embedding model. Experimental results on five datasets with different characteristics show that the KGECF model achieves the state-of-the-art (SOTA) performance on all datasets. And unlike other SOTA models that have obvious performance drops on AMusic and AToy datasets, our models performance is very stable across all datasets.",
        "file_path": "paper_data/knowledge_graph_embedding/info/77e23cd2437c6afb16082793badbb02842442e13.pdf",
        "venue": "IEEE Access",
        "citationCount": 17,
        "score": 3.4000000000000004,
        "summary": "Along with the rapidly increasing massive online data, recommender systems have been used as an effective approach for filtering useful information, which have been widely adopted in many web applications. In recent years, new techniques such as neural network and deep learning have demonstrated its effectiveness in studies of recommender systems. In this paper, we propose a novel approach for collaborative filtering with implicit feedback based on knowledge graph embedding. The basic ideal is to model the interactions between users and items as an interaction knowledge graph with a single relation, whose vector representation is learned through knowledge graph embedding. Base on the learned representation, the collaborative filtering problem is converted into link prediction in the interaction knowledge graph. KGECF, a neural network for knowledge graph embedding based collaborative filtering, is proposed based on this ideal and the RotatE knowledge graph embedding model. Experimental results on five datasets with different characteristics show that the KGECF model achieves the state-of-the-art (SOTA) performance on all datasets. And unlike other SOTA models that have obvious performance drops on AMusic and AToy datasets, our models performance is very stable across all datasets.",
        "keywords": []
      },
      "file_name": "77e23cd2437c6afb16082793badbb02842442e13.pdf"
    },
    {
      "success": true,
      "doc_id": "fee79df5db888418651617d6a5944e48",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/92351a799555df8d49465c2d4959118030339cc0.pdf",
      "citation_key": "zhang2019hs5",
      "metadata": {
        "title": "XTransE: Explainable Knowledge Graph Embedding for Link Prediction with Lifestyles in e-Commerce",
        "authors": [
          "Wen Zhang",
          "Shumin Deng",
          "Han Wang",
          "Qiang Chen",
          "Wei Zhang",
          "Huajun Chen"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/92351a799555df8d49465c2d4959118030339cc0.pdf",
        "venue": "Joint International Conference of Semantic Technology",
        "citationCount": 20,
        "score": 3.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "92351a799555df8d49465c2d4959118030339cc0.pdf"
    },
    {
      "success": true,
      "doc_id": "59a637f4618335a3d0e065dc39c9a7b9",
      "summary": "Knowledge graph embedding has become a promising method for knowledge graph completion. In this work, we propose Hybrid-TE, a hybrid translation-based temporal knowledge graph embedding, which combines two translation models TransD and HyTE for modeling both temporal and multirelational facts. Benefiting from two underlying models, Hybrid-TE first builds entity and relation embeddings in separate vector space for modelling multi-relational facts, and then explicitly learns time information by translational embedding on timespecific hyperplanes. We observe that a simple combination of two models does not lead to a satisfactory predictive precision. We therefore propose to project a triplet to all time-specific hyperplanes on which it is temporally valid. Besides, we also explore extra negative relation samplings that differ from positive samplings in relations. We conduct extensive experiments with real datasets on link prediction, relation prediction and temporal scope prediction. Experiments show significant improvements over previous time-insensitive or time-aware models.",
      "intriguing_abstract": "Knowledge graph embedding has become a promising method for knowledge graph completion. In this work, we propose Hybrid-TE, a hybrid translation-based temporal knowledge graph embedding, which combines two translation models TransD and HyTE for modeling both temporal and multirelational facts. Benefiting from two underlying models, Hybrid-TE first builds entity and relation embeddings in separate vector space for modelling multi-relational facts, and then explicitly learns time information by translational embedding on timespecific hyperplanes. We observe that a simple combination of two models does not lead to a satisfactory predictive precision. We therefore propose to project a triplet to all time-specific hyperplanes on which it is temporally valid. Besides, we also explore extra negative relation samplings that differ from positive samplings in relations. We conduct extensive experiments with real datasets on link prediction, relation prediction and temporal scope prediction. Experiments show significant improvements over previous time-insensitive or time-aware models.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6de535eb1b0024887227f7987e6eb22478af2a95.pdf",
      "citation_key": "wang20198d2",
      "metadata": {
        "title": "Hybrid-TE: Hybrid Translation-Based Temporal Knowledge Graph Embedding",
        "authors": [
          "Zhihao Wang",
          "Xin Li"
        ],
        "published_date": "2019",
        "abstract": "Knowledge graph embedding has become a promising method for knowledge graph completion. In this work, we propose Hybrid-TE, a hybrid translation-based temporal knowledge graph embedding, which combines two translation models TransD and HyTE for modeling both temporal and multirelational facts. Benefiting from two underlying models, Hybrid-TE first builds entity and relation embeddings in separate vector space for modelling multi-relational facts, and then explicitly learns time information by translational embedding on timespecific hyperplanes. We observe that a simple combination of two models does not lead to a satisfactory predictive precision. We therefore propose to project a triplet to all time-specific hyperplanes on which it is temporally valid. Besides, we also explore extra negative relation samplings that differ from positive samplings in relations. We conduct extensive experiments with real datasets on link prediction, relation prediction and temporal scope prediction. Experiments show significant improvements over previous time-insensitive or time-aware models.",
        "file_path": "paper_data/knowledge_graph_embedding/info/6de535eb1b0024887227f7987e6eb22478af2a95.pdf",
        "venue": "IEEE International Conference on Tools with Artificial Intelligence",
        "citationCount": 20,
        "score": 3.333333333333333,
        "summary": "Knowledge graph embedding has become a promising method for knowledge graph completion. In this work, we propose Hybrid-TE, a hybrid translation-based temporal knowledge graph embedding, which combines two translation models TransD and HyTE for modeling both temporal and multirelational facts. Benefiting from two underlying models, Hybrid-TE first builds entity and relation embeddings in separate vector space for modelling multi-relational facts, and then explicitly learns time information by translational embedding on timespecific hyperplanes. We observe that a simple combination of two models does not lead to a satisfactory predictive precision. We therefore propose to project a triplet to all time-specific hyperplanes on which it is temporally valid. Besides, we also explore extra negative relation samplings that differ from positive samplings in relations. We conduct extensive experiments with real datasets on link prediction, relation prediction and temporal scope prediction. Experiments show significant improvements over previous time-insensitive or time-aware models.",
        "keywords": []
      },
      "file_name": "6de535eb1b0024887227f7987e6eb22478af2a95.pdf"
    },
    {
      "success": true,
      "doc_id": "f10a59b0618e57a38a29bd90952b4483",
      "summary": "Knowledge graph is a popular format for representing knowledge, with many applications to semantic search engines, question-answering systems, and recommender systems. Real-world knowledge graphs are usually incomplete, so knowledge graph embedding methods, such as Canonical decomposition/Parallel factorization (CP), DistMult, and ComplEx, have been proposed to address this issue. These methods represent entities and relations as embedding vectors in semantic space and predict the links between them. The embedding vectors themselves contain rich semantic information and can be used in other applications such as data analysis. However, mechanisms in these models and the embedding vectors themselves vary greatly, making it difficult to understand and compare them. Given this lack of understanding, we risk using them ineffectively or incorrectly, particularly for complicated models, such as CP, with two role-based embedding vectors, or the state-of-the-art ComplEx model, with complex-valued embedding vectors. In this paper, we propose a multi-embedding interaction mechanism as a new approach to uniting and generalizing these models. We derive them theoretically via this mechanism and provide empirical analyses and comparisons between them. We also propose a new multi-embedding model based on quaternion algebra and show that it achieves promising results using popular benchmarks. Source code is available on github at this https URL",
      "intriguing_abstract": "Knowledge graph is a popular format for representing knowledge, with many applications to semantic search engines, question-answering systems, and recommender systems. Real-world knowledge graphs are usually incomplete, so knowledge graph embedding methods, such as Canonical decomposition/Parallel factorization (CP), DistMult, and ComplEx, have been proposed to address this issue. These methods represent entities and relations as embedding vectors in semantic space and predict the links between them. The embedding vectors themselves contain rich semantic information and can be used in other applications such as data analysis. However, mechanisms in these models and the embedding vectors themselves vary greatly, making it difficult to understand and compare them. Given this lack of understanding, we risk using them ineffectively or incorrectly, particularly for complicated models, such as CP, with two role-based embedding vectors, or the state-of-the-art ComplEx model, with complex-valued embedding vectors. In this paper, we propose a multi-embedding interaction mechanism as a new approach to uniting and generalizing these models. We derive them theoretically via this mechanism and provide empirical analyses and comparisons between them. We also propose a new multi-embedding model based on quaternion algebra and show that it achieves promising results using popular benchmarks. Source code is available on github at this https URL",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/be7b102315ce70a7e01eb87c1140dd6850148e8b.pdf",
      "citation_key": "tran20195x3",
      "metadata": {
        "title": "Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective",
        "authors": [
          "Hung Nghiep Tran",
          "A. Takasu"
        ],
        "published_date": "2019",
        "abstract": "Knowledge graph is a popular format for representing knowledge, with many applications to semantic search engines, question-answering systems, and recommender systems. Real-world knowledge graphs are usually incomplete, so knowledge graph embedding methods, such as Canonical decomposition/Parallel factorization (CP), DistMult, and ComplEx, have been proposed to address this issue. These methods represent entities and relations as embedding vectors in semantic space and predict the links between them. The embedding vectors themselves contain rich semantic information and can be used in other applications such as data analysis. However, mechanisms in these models and the embedding vectors themselves vary greatly, making it difficult to understand and compare them. Given this lack of understanding, we risk using them ineffectively or incorrectly, particularly for complicated models, such as CP, with two role-based embedding vectors, or the state-of-the-art ComplEx model, with complex-valued embedding vectors. In this paper, we propose a multi-embedding interaction mechanism as a new approach to uniting and generalizing these models. We derive them theoretically via this mechanism and provide empirical analyses and comparisons between them. We also propose a new multi-embedding model based on quaternion algebra and show that it achieves promising results using popular benchmarks. Source code is available on github at this https URL",
        "file_path": "paper_data/knowledge_graph_embedding/info/be7b102315ce70a7e01eb87c1140dd6850148e8b.pdf",
        "venue": "EDBT/ICDT Workshops",
        "citationCount": 20,
        "score": 3.333333333333333,
        "summary": "Knowledge graph is a popular format for representing knowledge, with many applications to semantic search engines, question-answering systems, and recommender systems. Real-world knowledge graphs are usually incomplete, so knowledge graph embedding methods, such as Canonical decomposition/Parallel factorization (CP), DistMult, and ComplEx, have been proposed to address this issue. These methods represent entities and relations as embedding vectors in semantic space and predict the links between them. The embedding vectors themselves contain rich semantic information and can be used in other applications such as data analysis. However, mechanisms in these models and the embedding vectors themselves vary greatly, making it difficult to understand and compare them. Given this lack of understanding, we risk using them ineffectively or incorrectly, particularly for complicated models, such as CP, with two role-based embedding vectors, or the state-of-the-art ComplEx model, with complex-valued embedding vectors. In this paper, we propose a multi-embedding interaction mechanism as a new approach to uniting and generalizing these models. We derive them theoretically via this mechanism and provide empirical analyses and comparisons between them. We also propose a new multi-embedding model based on quaternion algebra and show that it achieves promising results using popular benchmarks. Source code is available on github at this https URL",
        "keywords": []
      },
      "file_name": "be7b102315ce70a7e01eb87c1140dd6850148e8b.pdf"
    },
    {
      "success": true,
      "doc_id": "f91e2ed869f80148fbe495c86836a7ef",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/5b6a24ea3ffdccb14ce0267a815845c62ef026c9.pdf",
      "citation_key": "xiong2018fof",
      "metadata": {
        "title": "Knowledge Graph Embedding via Relation Paths and Dynamic Mapping Matrix",
        "authors": [
          "Shengwu Xiong",
          "Weitao Huang",
          "P. Duan"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/5b6a24ea3ffdccb14ce0267a815845c62ef026c9.pdf",
        "venue": "ER Workshops",
        "citationCount": 23,
        "score": 3.2857142857142856,
        "summary": "",
        "keywords": []
      },
      "file_name": "5b6a24ea3ffdccb14ce0267a815845c62ef026c9.pdf"
    },
    {
      "success": true,
      "doc_id": "67c64135deb1d8999f8c977ed8917166",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/75f7e3459e53fa0775c941cb703f049797851ef0.pdf",
      "citation_key": "radstok2021yup",
      "metadata": {
        "title": "Are Knowledge Graph Embedding Models Biased, or Is it the Data That They Are Trained on?",
        "authors": [
          "Wessel Radstok",
          "M. Chekol",
          "M. Schfer"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/75f7e3459e53fa0775c941cb703f049797851ef0.pdf",
        "venue": "Wikidata@ISWC",
        "citationCount": 13,
        "score": 3.25,
        "summary": "",
        "keywords": []
      },
      "file_name": "75f7e3459e53fa0775c941cb703f049797851ef0.pdf"
    },
    {
      "success": true,
      "doc_id": "783cbcbdd0777b8305ef510b3b60c72f",
      "summary": "Multi-source spatio-temporal data analysis is an important task in the development of smart cities. However, traditional data analysis methods cannot adapt to the growth rate of massive multi-source spatio-temporal data and explain the practical significance of results. To explore the network structure and semantic relationships, we propose a general framework for multi-source spatio-temporal data analysis via knowledge graph embedding. The framework extracts low-dimensional feature representation from multi-source spatio-temporal data in a high-dimensional space, and recognizes the network structure and semantic relationships about multi-source spatio-temporal data. Experiment results show that the framework can not only effectively utilize multi-source spatio-temporal data, but also explore the network structure and semantic relationship. Taking real Shanghai datasets as an example, we confirm the validity of the multi-source spatio-temporal data analytical framework based on knowledge graph embedding.",
      "intriguing_abstract": "Multi-source spatio-temporal data analysis is an important task in the development of smart cities. However, traditional data analysis methods cannot adapt to the growth rate of massive multi-source spatio-temporal data and explain the practical significance of results. To explore the network structure and semantic relationships, we propose a general framework for multi-source spatio-temporal data analysis via knowledge graph embedding. The framework extracts low-dimensional feature representation from multi-source spatio-temporal data in a high-dimensional space, and recognizes the network structure and semantic relationships about multi-source spatio-temporal data. Experiment results show that the framework can not only effectively utilize multi-source spatio-temporal data, but also explore the network structure and semantic relationship. Taking real Shanghai datasets as an example, we confirm the validity of the multi-source spatio-temporal data analytical framework based on knowledge graph embedding.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/3ea066e35fdd45162a7fa94e995abe0beeceb532.pdf",
      "citation_key": "zhao2020o6z",
      "metadata": {
        "title": "Urban Multi-Source Spatio-Temporal Data Analysis Aware Knowledge Graph Embedding",
        "authors": [
          "Ling Zhao",
          "Hanhan Deng",
          "L. Qiu",
          "Sumin Li",
          "Zhixiang Hou",
          "Hai-bin Sun",
          "Yun Chen"
        ],
        "published_date": "2020",
        "abstract": "Multi-source spatio-temporal data analysis is an important task in the development of smart cities. However, traditional data analysis methods cannot adapt to the growth rate of massive multi-source spatio-temporal data and explain the practical significance of results. To explore the network structure and semantic relationships, we propose a general framework for multi-source spatio-temporal data analysis via knowledge graph embedding. The framework extracts low-dimensional feature representation from multi-source spatio-temporal data in a high-dimensional space, and recognizes the network structure and semantic relationships about multi-source spatio-temporal data. Experiment results show that the framework can not only effectively utilize multi-source spatio-temporal data, but also explore the network structure and semantic relationship. Taking real Shanghai datasets as an example, we confirm the validity of the multi-source spatio-temporal data analytical framework based on knowledge graph embedding.",
        "file_path": "paper_data/knowledge_graph_embedding/info/3ea066e35fdd45162a7fa94e995abe0beeceb532.pdf",
        "venue": "Symmetry",
        "citationCount": 16,
        "score": 3.2,
        "summary": "Multi-source spatio-temporal data analysis is an important task in the development of smart cities. However, traditional data analysis methods cannot adapt to the growth rate of massive multi-source spatio-temporal data and explain the practical significance of results. To explore the network structure and semantic relationships, we propose a general framework for multi-source spatio-temporal data analysis via knowledge graph embedding. The framework extracts low-dimensional feature representation from multi-source spatio-temporal data in a high-dimensional space, and recognizes the network structure and semantic relationships about multi-source spatio-temporal data. Experiment results show that the framework can not only effectively utilize multi-source spatio-temporal data, but also explore the network structure and semantic relationship. Taking real Shanghai datasets as an example, we confirm the validity of the multi-source spatio-temporal data analytical framework based on knowledge graph embedding.",
        "keywords": []
      },
      "file_name": "3ea066e35fdd45162a7fa94e995abe0beeceb532.pdf"
    },
    {
      "success": true,
      "doc_id": "fce2cd09c0c13481d7f522d31d4ea4fd",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c7a630751e45e3a74691bd0fc0880b4bf87be101.pdf",
      "citation_key": "zhang20182ey",
      "metadata": {
        "title": "Discriminative Path-Based Knowledge Graph Embedding for Precise Link Prediction",
        "authors": [
          "Maoyuan Zhang",
          "Qi Wang",
          "Wukui Xu",
          "Wei Li",
          "Shuyuan Sun"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/c7a630751e45e3a74691bd0fc0880b4bf87be101.pdf",
        "venue": "European Conference on Information Retrieval",
        "citationCount": 22,
        "score": 3.142857142857143,
        "summary": "",
        "keywords": []
      },
      "file_name": "c7a630751e45e3a74691bd0fc0880b4bf87be101.pdf"
    },
    {
      "success": true,
      "doc_id": "921ab105c9a63403ca5960c887d9b8a4",
      "summary": "Knowledge graphs incompleteness has motivated many researchers to propose methods to automatically infer missing facts in knowledge graphs. Knowledge graph embedding has been an active research area for knowledge graph completion, with great improvement from the early TransE to the current state-of-the-art ConvKB. ConvKB considers a knowledge graph as a set of triples, and employs a convolutional neural network to capture global relationships and transitional characteristics between entities and relations in the knowledge graph. However, it only utilizes the triple information, and ignores the rich information contained in relation paths. In fact, a path of one relation describes the relation from some aspect in a fine-grained way. Therefore, it is beneficial to take relation paths into consideration for knowledge graph embedding. In this paper, we present a novel convolutional neural network-based embedding model PConvKB, which improves knowledge graph embedding by incorporating relation paths locally and globally. Specifically, we introduce attention mechanism to measure the local importance of relation paths. Moreover, we propose a simple yet effective measure DIPF to compute the global importance of relation paths. Experimental results show that our model achieves substantial improvements against state-of-the-art methods.",
      "intriguing_abstract": "Knowledge graphs incompleteness has motivated many researchers to propose methods to automatically infer missing facts in knowledge graphs. Knowledge graph embedding has been an active research area for knowledge graph completion, with great improvement from the early TransE to the current state-of-the-art ConvKB. ConvKB considers a knowledge graph as a set of triples, and employs a convolutional neural network to capture global relationships and transitional characteristics between entities and relations in the knowledge graph. However, it only utilizes the triple information, and ignores the rich information contained in relation paths. In fact, a path of one relation describes the relation from some aspect in a fine-grained way. Therefore, it is beneficial to take relation paths into consideration for knowledge graph embedding. In this paper, we present a novel convolutional neural network-based embedding model PConvKB, which improves knowledge graph embedding by incorporating relation paths locally and globally. Specifically, we introduce attention mechanism to measure the local importance of relation paths. Moreover, we propose a simple yet effective measure DIPF to compute the global importance of relation paths. Experimental results show that our model achieves substantial improvements against state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a2a7f85d2ba28750725c4956eb14d53f6a90f003.pdf",
      "citation_key": "jia20207dd",
      "metadata": {
        "title": "Improving Knowledge Graph Embedding Using Locally and Globally Attentive Relation Paths",
        "authors": [
          "Ningning Jia",
          "Xiang Cheng",
          "Sen Su"
        ],
        "published_date": "2020",
        "abstract": "Knowledge graphs incompleteness has motivated many researchers to propose methods to automatically infer missing facts in knowledge graphs. Knowledge graph embedding has been an active research area for knowledge graph completion, with great improvement from the early TransE to the current state-of-the-art ConvKB. ConvKB considers a knowledge graph as a set of triples, and employs a convolutional neural network to capture global relationships and transitional characteristics between entities and relations in the knowledge graph. However, it only utilizes the triple information, and ignores the rich information contained in relation paths. In fact, a path of one relation describes the relation from some aspect in a fine-grained way. Therefore, it is beneficial to take relation paths into consideration for knowledge graph embedding. In this paper, we present a novel convolutional neural network-based embedding model PConvKB, which improves knowledge graph embedding by incorporating relation paths locally and globally. Specifically, we introduce attention mechanism to measure the local importance of relation paths. Moreover, we propose a simple yet effective measure DIPF to compute the global importance of relation paths. Experimental results show that our model achieves substantial improvements against state-of-the-art methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/a2a7f85d2ba28750725c4956eb14d53f6a90f003.pdf",
        "venue": "European Conference on Information Retrieval",
        "citationCount": 15,
        "score": 3.0,
        "summary": "Knowledge graphs incompleteness has motivated many researchers to propose methods to automatically infer missing facts in knowledge graphs. Knowledge graph embedding has been an active research area for knowledge graph completion, with great improvement from the early TransE to the current state-of-the-art ConvKB. ConvKB considers a knowledge graph as a set of triples, and employs a convolutional neural network to capture global relationships and transitional characteristics between entities and relations in the knowledge graph. However, it only utilizes the triple information, and ignores the rich information contained in relation paths. In fact, a path of one relation describes the relation from some aspect in a fine-grained way. Therefore, it is beneficial to take relation paths into consideration for knowledge graph embedding. In this paper, we present a novel convolutional neural network-based embedding model PConvKB, which improves knowledge graph embedding by incorporating relation paths locally and globally. Specifically, we introduce attention mechanism to measure the local importance of relation paths. Moreover, we propose a simple yet effective measure DIPF to compute the global importance of relation paths. Experimental results show that our model achieves substantial improvements against state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "a2a7f85d2ba28750725c4956eb14d53f6a90f003.pdf"
    },
    {
      "success": true,
      "doc_id": "2447f898f5b447b24be1aaa1c73c3f51",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/bb0613ea0d39e35901aa0018de40deaf35cbbd5d.pdf",
      "citation_key": "zhu2019ir6",
      "metadata": {
        "title": "A neural translating general hyperplane for knowledge graph embedding",
        "authors": [
          "Qiannan Zhu",
          "Xiaofei Zhou",
          "P. Zhang",
          "Yong Shi"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/bb0613ea0d39e35901aa0018de40deaf35cbbd5d.pdf",
        "venue": "Journal of Computer Science",
        "citationCount": 17,
        "score": 2.833333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "bb0613ea0d39e35901aa0018de40deaf35cbbd5d.pdf"
    },
    {
      "success": true,
      "doc_id": "e16d7726c994beeff871360992c846af",
      "summary": "Knowledge graph enhanced information retrieval systems have attracted considerable attention due to their ability to improve performance and provide additional explainability. As the knowledge graphs usually include fruitful facts, they are also good sources of side information. However, recent studies have shown that the usefulness of knowledge graphs depends highly on their representation, e.g., the embeddings of entities and relations. Embedding entities and relations in low-dimensional space is a successful knowledge graph representation solution. Most of the works lie in modeling symmetry/asymmetry/composition/inversion relations but pay less attention to the hierarchical relations. Recent studies have observed the fact that there exist rich semantic hierarchical relations in knowledge graphs such as Freebase (entities are connected in a taxonomic hierarchy) and WordNet (entities are synsets linked together in a hierarchy).To address the above problems, we propose Hierarchical Hyperbolic Neural Graph Embedding (H2E), a new knowledge graph representation approach, which is able to better preserve hierarchical relations. Specifically, the entities/relations representations are learned in a hyperbolic polar embedding space. In a hyperbolic polar embedding space, the entity and relation are modeled as a dual-embedding with modulus embedding part and phase embedding part, enabling the explicitly modeling of two types of hierarchies: inter-level hierarchy and intra-level hierarchy. As the polar embedding is defined i n hyperbolic space, the ability of modeling and inferring hierarchical relations are mutual enhanced. In addition, by noticing the existence of the rich relational context, we propose an attentional neural context aggregation to adaptively integrate the relational context for further enhancing the ability to preserve the hierarchical relations. The empirical study on three benchmark datasets for the link prediction task demonstrates significant performance gains compared to some existing state-of-the-art methods and verifies the effectiveness of the proposed method on hierarchical relations.",
      "intriguing_abstract": "Knowledge graph enhanced information retrieval systems have attracted considerable attention due to their ability to improve performance and provide additional explainability. As the knowledge graphs usually include fruitful facts, they are also good sources of side information. However, recent studies have shown that the usefulness of knowledge graphs depends highly on their representation, e.g., the embeddings of entities and relations. Embedding entities and relations in low-dimensional space is a successful knowledge graph representation solution. Most of the works lie in modeling symmetry/asymmetry/composition/inversion relations but pay less attention to the hierarchical relations. Recent studies have observed the fact that there exist rich semantic hierarchical relations in knowledge graphs such as Freebase (entities are connected in a taxonomic hierarchy) and WordNet (entities are synsets linked together in a hierarchy).To address the above problems, we propose Hierarchical Hyperbolic Neural Graph Embedding (H2E), a new knowledge graph representation approach, which is able to better preserve hierarchical relations. Specifically, the entities/relations representations are learned in a hyperbolic polar embedding space. In a hyperbolic polar embedding space, the entity and relation are modeled as a dual-embedding with modulus embedding part and phase embedding part, enabling the explicitly modeling of two types of hierarchies: inter-level hierarchy and intra-level hierarchy. As the polar embedding is defined i n hyperbolic space, the ability of modeling and inferring hierarchical relations are mutual enhanced. In addition, by noticing the existence of the rich relational context, we propose an attentional neural context aggregation to adaptively integrate the relational context for further enhancing the ability to preserve the hierarchical relations. The empirical study on three benchmark datasets for the link prediction task demonstrates significant performance gains compared to some existing state-of-the-art methods and verifies the effectiveness of the proposed method on hierarchical relations.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/509fa029989e89a4b82dd01ab75734aed937d684.pdf",
      "citation_key": "wang2021dgy",
      "metadata": {
        "title": "Knowledge Graph Representation via Hierarchical Hyperbolic Neural Graph Embedding",
        "authors": [
          "Shen Wang",
          "Xiaokai Wei",
          "C. D. Santos",
          "Zhiguo Wang",
          "Ramesh Nallapati",
          "Andrew O. Arnold",
          "Philip S. Yu"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph enhanced information retrieval systems have attracted considerable attention due to their ability to improve performance and provide additional explainability. As the knowledge graphs usually include fruitful facts, they are also good sources of side information. However, recent studies have shown that the usefulness of knowledge graphs depends highly on their representation, e.g., the embeddings of entities and relations. Embedding entities and relations in low-dimensional space is a successful knowledge graph representation solution. Most of the works lie in modeling symmetry/asymmetry/composition/inversion relations but pay less attention to the hierarchical relations. Recent studies have observed the fact that there exist rich semantic hierarchical relations in knowledge graphs such as Freebase (entities are connected in a taxonomic hierarchy) and WordNet (entities are synsets linked together in a hierarchy).To address the above problems, we propose Hierarchical Hyperbolic Neural Graph Embedding (H2E), a new knowledge graph representation approach, which is able to better preserve hierarchical relations. Specifically, the entities/relations representations are learned in a hyperbolic polar embedding space. In a hyperbolic polar embedding space, the entity and relation are modeled as a dual-embedding with modulus embedding part and phase embedding part, enabling the explicitly modeling of two types of hierarchies: inter-level hierarchy and intra-level hierarchy. As the polar embedding is defined i n hyperbolic space, the ability of modeling and inferring hierarchical relations are mutual enhanced. In addition, by noticing the existence of the rich relational context, we propose an attentional neural context aggregation to adaptively integrate the relational context for further enhancing the ability to preserve the hierarchical relations. The empirical study on three benchmark datasets for the link prediction task demonstrates significant performance gains compared to some existing state-of-the-art methods and verifies the effectiveness of the proposed method on hierarchical relations.",
        "file_path": "paper_data/knowledge_graph_embedding/info/509fa029989e89a4b82dd01ab75734aed937d684.pdf",
        "venue": "2021 IEEE International Conference on Big Data (Big Data)",
        "citationCount": 11,
        "score": 2.75,
        "summary": "Knowledge graph enhanced information retrieval systems have attracted considerable attention due to their ability to improve performance and provide additional explainability. As the knowledge graphs usually include fruitful facts, they are also good sources of side information. However, recent studies have shown that the usefulness of knowledge graphs depends highly on their representation, e.g., the embeddings of entities and relations. Embedding entities and relations in low-dimensional space is a successful knowledge graph representation solution. Most of the works lie in modeling symmetry/asymmetry/composition/inversion relations but pay less attention to the hierarchical relations. Recent studies have observed the fact that there exist rich semantic hierarchical relations in knowledge graphs such as Freebase (entities are connected in a taxonomic hierarchy) and WordNet (entities are synsets linked together in a hierarchy).To address the above problems, we propose Hierarchical Hyperbolic Neural Graph Embedding (H2E), a new knowledge graph representation approach, which is able to better preserve hierarchical relations. Specifically, the entities/relations representations are learned in a hyperbolic polar embedding space. In a hyperbolic polar embedding space, the entity and relation are modeled as a dual-embedding with modulus embedding part and phase embedding part, enabling the explicitly modeling of two types of hierarchies: inter-level hierarchy and intra-level hierarchy. As the polar embedding is defined i n hyperbolic space, the ability of modeling and inferring hierarchical relations are mutual enhanced. In addition, by noticing the existence of the rich relational context, we propose an attentional neural context aggregation to adaptively integrate the relational context for further enhancing the ability to preserve the hierarchical relations. The empirical study on three benchmark datasets for the link prediction task demonstrates significant performance gains compared to some existing state-of-the-art methods and verifies the effectiveness of the proposed method on hierarchical relations.",
        "keywords": []
      },
      "file_name": "509fa029989e89a4b82dd01ab75734aed937d684.pdf"
    },
    {
      "success": true,
      "doc_id": "4f00fd32b54ddc7298eefbe6c75253a4",
      "summary": "Knowledge graph embedding (KGE) models learn to project symbolic entities and relations into a continuous vector space based on the observed triplets. However, existing KGE models cannot make a proper trade-off between the graph context and the model complexity, which makes them still far from satisfactory. In this paper, we propose a lightweight framework named LightCAKE for context-aware KGE. LightCAKE explicitly models the graph context without introducing redundant trainable parameters, and uses an iterative aggregation strategy to integrate the context information into the entity/relation embeddings. As a generic framework, it can be used with many simple KGE models to achieve excellent results. Finally, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.",
      "intriguing_abstract": "Knowledge graph embedding (KGE) models learn to project symbolic entities and relations into a continuous vector space based on the observed triplets. However, existing KGE models cannot make a proper trade-off between the graph context and the model complexity, which makes them still far from satisfactory. In this paper, we propose a lightweight framework named LightCAKE for context-aware KGE. LightCAKE explicitly models the graph context without introducing redundant trainable parameters, and uses an iterative aggregation strategy to integrate the context information into the entity/relation embeddings. As a generic framework, it can be used with many simple KGE models to achieve excellent results. Finally, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4f2cc26b689cdac36ceb2037338eac65e7e5a193.pdf",
      "citation_key": "ning20219et",
      "metadata": {
        "title": "LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding",
        "authors": [
          "Zhiyuan Ning",
          "Ziyue Qiao",
          "Hao Dong",
          "Yi Du",
          "Yuanchun Zhou"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding (KGE) models learn to project symbolic entities and relations into a continuous vector space based on the observed triplets. However, existing KGE models cannot make a proper trade-off between the graph context and the model complexity, which makes them still far from satisfactory. In this paper, we propose a lightweight framework named LightCAKE for context-aware KGE. LightCAKE explicitly models the graph context without introducing redundant trainable parameters, and uses an iterative aggregation strategy to integrate the context information into the entity/relation embeddings. As a generic framework, it can be used with many simple KGE models to achieve excellent results. Finally, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.",
        "file_path": "paper_data/knowledge_graph_embedding/info/4f2cc26b689cdac36ceb2037338eac65e7e5a193.pdf",
        "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining",
        "citationCount": 11,
        "score": 2.75,
        "summary": "Knowledge graph embedding (KGE) models learn to project symbolic entities and relations into a continuous vector space based on the observed triplets. However, existing KGE models cannot make a proper trade-off between the graph context and the model complexity, which makes them still far from satisfactory. In this paper, we propose a lightweight framework named LightCAKE for context-aware KGE. LightCAKE explicitly models the graph context without introducing redundant trainable parameters, and uses an iterative aggregation strategy to integrate the context information into the entity/relation embeddings. As a generic framework, it can be used with many simple KGE models to achieve excellent results. Finally, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.",
        "keywords": []
      },
      "file_name": "4f2cc26b689cdac36ceb2037338eac65e7e5a193.pdf"
    },
    {
      "success": true,
      "doc_id": "dc1acba7468f247244628db819fc1603",
      "summary": "Knowledge graph embedding methods learn embeddings of entities and relations in a low dimensional space which can be used for various downstream machine learning tasks such as link prediction and entity matching. Various graph convolutional network methods have been proposed which use different types of information to learn the features of entities and relations. However, these methods assign the same weight (importance) to the neighbors when aggregating the information, ignoring the role of different relations with the neighboring entities. To this end, we propose a relation-aware graph attention model that leverages relation information to compute different weights to the neighboring nodes for learning embeddings of entities and relations. We evaluate our proposed approach on link prediction and entity matching tasks. Our experimental results on link prediction on three datasets (one proprietary and two public) and results on unsupervised entity matching on one proprietary dataset demonstrate the effectiveness of the relation-aware attention.",
      "intriguing_abstract": "Knowledge graph embedding methods learn embeddings of entities and relations in a low dimensional space which can be used for various downstream machine learning tasks such as link prediction and entity matching. Various graph convolutional network methods have been proposed which use different types of information to learn the features of entities and relations. However, these methods assign the same weight (importance) to the neighbors when aggregating the information, ignoring the role of different relations with the neighboring entities. To this end, we propose a relation-aware graph attention model that leverages relation information to compute different weights to the neighboring nodes for learning embeddings of entities and relations. We evaluate our proposed approach on link prediction and entity matching tasks. Our experimental results on link prediction on three datasets (one proprietary and two public) and results on unsupervised entity matching on one proprietary dataset demonstrate the effectiveness of the relation-aware attention.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7bb4cd36de648ca44cc390fe886ee70a4b2ad1ac.pdf",
      "citation_key": "sheikh20213qq",
      "metadata": {
        "title": "Knowledge Graph Embedding using Graph Convolutional Networks with Relation-Aware Attention",
        "authors": [
          "Nasrullah Sheikh",
          "Xiao Qin",
          "B. Reinwald",
          "Christoph Miksovic",
          "P. Scotton"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding methods learn embeddings of entities and relations in a low dimensional space which can be used for various downstream machine learning tasks such as link prediction and entity matching. Various graph convolutional network methods have been proposed which use different types of information to learn the features of entities and relations. However, these methods assign the same weight (importance) to the neighbors when aggregating the information, ignoring the role of different relations with the neighboring entities. To this end, we propose a relation-aware graph attention model that leverages relation information to compute different weights to the neighboring nodes for learning embeddings of entities and relations. We evaluate our proposed approach on link prediction and entity matching tasks. Our experimental results on link prediction on three datasets (one proprietary and two public) and results on unsupervised entity matching on one proprietary dataset demonstrate the effectiveness of the relation-aware attention.",
        "file_path": "paper_data/knowledge_graph_embedding/info/7bb4cd36de648ca44cc390fe886ee70a4b2ad1ac.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 2.75,
        "summary": "Knowledge graph embedding methods learn embeddings of entities and relations in a low dimensional space which can be used for various downstream machine learning tasks such as link prediction and entity matching. Various graph convolutional network methods have been proposed which use different types of information to learn the features of entities and relations. However, these methods assign the same weight (importance) to the neighbors when aggregating the information, ignoring the role of different relations with the neighboring entities. To this end, we propose a relation-aware graph attention model that leverages relation information to compute different weights to the neighboring nodes for learning embeddings of entities and relations. We evaluate our proposed approach on link prediction and entity matching tasks. Our experimental results on link prediction on three datasets (one proprietary and two public) and results on unsupervised entity matching on one proprietary dataset demonstrate the effectiveness of the relation-aware attention.",
        "keywords": []
      },
      "file_name": "7bb4cd36de648ca44cc390fe886ee70a4b2ad1ac.pdf"
    },
    {
      "success": true,
      "doc_id": "01e88b3d8fc27a89c0ac79a377f7b034",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/93db6077c12cc83ea165a0d8851efb69b0055f3a.pdf",
      "citation_key": "rim2021s9a",
      "metadata": {
        "title": "Behavioral Testing of Knowledge Graph Embedding Models for Link Prediction",
        "authors": [
          "Wiem Ben Rim",
          "Carolin (Haas) Lawrence",
          "Kiril Gashteovski",
          "Mathias Niepert",
          "Naoaki Okazaki"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/93db6077c12cc83ea165a0d8851efb69b0055f3a.pdf",
        "venue": "Conference on Automated Knowledge Base Construction",
        "citationCount": 11,
        "score": 2.75,
        "summary": "",
        "keywords": []
      },
      "file_name": "93db6077c12cc83ea165a0d8851efb69b0055f3a.pdf"
    },
    {
      "success": true,
      "doc_id": "d7020bedbaf57a96e291f1bec632e632",
      "summary": "Knowledge graph representation has been a long standing goal of artificial intelligence. In this paper, we consider a method for knowledge graph embedding of hyper-relational data, which are commonly found in knowledge graphs. Previous models such as Trans (E, H, R) and CTransR are either insufficient for embedding hyper-relational data or focus on projecting an entity into multiple embeddings, which might not be effective for generalization nor accurately reflect real knowledge. To overcome these issues, we propose the novel model TransHR, which transforms the hyper-relations in a pair of entities into an individual vector, serving as a translation between them. We experimentally evaluate our model on two typical tasks-link prediction and triple classification. The results demonstrate that TransHR significantly outperforms Trans (E, H, R) and CTransR, especially for hyperrelational data.",
      "intriguing_abstract": "Knowledge graph representation has been a long standing goal of artificial intelligence. In this paper, we consider a method for knowledge graph embedding of hyper-relational data, which are commonly found in knowledge graphs. Previous models such as Trans (E, H, R) and CTransR are either insufficient for embedding hyper-relational data or focus on projecting an entity into multiple embeddings, which might not be effective for generalization nor accurately reflect real knowledge. To overcome these issues, we propose the novel model TransHR, which transforms the hyper-relations in a pair of entities into an individual vector, serving as a translation between them. We experimentally evaluate our model on two typical tasks-link prediction and triple classification. The results demonstrate that TransHR significantly outperforms Trans (E, H, R) and CTransR, especially for hyperrelational data.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2f700be8a387101411a84199adfe30636e331752.pdf",
      "citation_key": "zhang20179i2",
      "metadata": {
        "title": "Knowledge Graph Embedding for Hyper-Relational Data",
        "authors": [
          "Chunhong Zhang",
          "Miao Zhou",
          "Xiao Han",
          "Zheng Hu",
          "Yang Ji"
        ],
        "published_date": "2017",
        "abstract": "Knowledge graph representation has been a long standing goal of artificial intelligence. In this paper, we consider a method for knowledge graph embedding of hyper-relational data, which are commonly found in knowledge graphs. Previous models such as Trans (E, H, R) and CTransR are either insufficient for embedding hyper-relational data or focus on projecting an entity into multiple embeddings, which might not be effective for generalization nor accurately reflect real knowledge. To overcome these issues, we propose the novel model TransHR, which transforms the hyper-relations in a pair of entities into an individual vector, serving as a translation between them. We experimentally evaluate our model on two typical tasks-link prediction and triple classification. The results demonstrate that TransHR significantly outperforms Trans (E, H, R) and CTransR, especially for hyperrelational data.",
        "file_path": "paper_data/knowledge_graph_embedding/info/2f700be8a387101411a84199adfe30636e331752.pdf",
        "venue": "",
        "citationCount": 22,
        "score": 2.75,
        "summary": "Knowledge graph representation has been a long standing goal of artificial intelligence. In this paper, we consider a method for knowledge graph embedding of hyper-relational data, which are commonly found in knowledge graphs. Previous models such as Trans (E, H, R) and CTransR are either insufficient for embedding hyper-relational data or focus on projecting an entity into multiple embeddings, which might not be effective for generalization nor accurately reflect real knowledge. To overcome these issues, we propose the novel model TransHR, which transforms the hyper-relations in a pair of entities into an individual vector, serving as a translation between them. We experimentally evaluate our model on two typical tasks-link prediction and triple classification. The results demonstrate that TransHR significantly outperforms Trans (E, H, R) and CTransR, especially for hyperrelational data.",
        "keywords": []
      },
      "file_name": "2f700be8a387101411a84199adfe30636e331752.pdf"
    },
    {
      "success": true,
      "doc_id": "00851be0fc051ade4f391e5a9c9cf5fe",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2dba03d338170bde4e965909230256086bafa9f8.pdf",
      "citation_key": "elebi20182bd",
      "metadata": {
        "title": "Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction using Linked Open Data",
        "authors": [
          "R. elebi",
          "Erkan Yasar",
          "Hseyin Uyar",
          "zgr Gms",
          "Oguz Dikenelli",
          "M. Dumontier"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/2dba03d338170bde4e965909230256086bafa9f8.pdf",
        "venue": "Workshop on Semantic Web Applications and Tools for Life Sciences",
        "citationCount": 19,
        "score": 2.714285714285714,
        "summary": "",
        "keywords": []
      },
      "file_name": "2dba03d338170bde4e965909230256086bafa9f8.pdf"
    },
    {
      "success": true,
      "doc_id": "038a2a581175fbe149a7beb883fd9c6e",
      "summary": "Industry is evolving towards Industry 4.0, which holds the promise of increased flexibility in manufacturing, better quality and improved productivity. A core actor of this growth is using sensors, which must capture data that can used in unforeseen ways to achieve a performance not achievable without them. However, the complexity of this improved setting is much greater than what is currently used in practice. Hence, it is imperative that the management cannot only be performed by human labor force, but part of that will be done by automated algorithms instead. A natural way to represent the data generated by this large amount of sensors, which are not acting measuring independent variables, and the interaction of the different devices is by using a graph data model. Then, machine learning could be used to aid the Industry 4.0 system to, for example, perform predictive maintenance. However, machine learning directly on graphs, needs feature engineering and has scalability issues. In this paper we discuss methods to convert (embed) the graph in a vector space, such that it becomes feasible to use traditional machine learning methods for Industry 4.0 settings.",
      "intriguing_abstract": "Industry is evolving towards Industry 4.0, which holds the promise of increased flexibility in manufacturing, better quality and improved productivity. A core actor of this growth is using sensors, which must capture data that can used in unforeseen ways to achieve a performance not achievable without them. However, the complexity of this improved setting is much greater than what is currently used in practice. Hence, it is imperative that the management cannot only be performed by human labor force, but part of that will be done by automated algorithms instead. A natural way to represent the data generated by this large amount of sensors, which are not acting measuring independent variables, and the interaction of the different devices is by using a graph data model. Then, machine learning could be used to aid the Industry 4.0 system to, for example, perform predictive maintenance. However, machine learning directly on graphs, needs feature engineering and has scalability issues. In this paper we discuss methods to convert (embed) the graph in a vector space, such that it becomes feasible to use traditional machine learning methods for Industry 4.0 settings.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c2648a294ef2fc299e1dd959bc1f92973f9c9ebc.pdf",
      "citation_key": "garofalo20185g9",
      "metadata": {
        "title": "Leveraging Knowledge Graph Embedding Techniques for Industry 4.0 Use Cases",
        "authors": [
          "Martina Garofalo",
          "Maria Angela Pellegrino",
          "Abdulrahman Altabba",
          "Michael Cochez"
        ],
        "published_date": "2018",
        "abstract": "Industry is evolving towards Industry 4.0, which holds the promise of increased flexibility in manufacturing, better quality and improved productivity. A core actor of this growth is using sensors, which must capture data that can used in unforeseen ways to achieve a performance not achievable without them. However, the complexity of this improved setting is much greater than what is currently used in practice. Hence, it is imperative that the management cannot only be performed by human labor force, but part of that will be done by automated algorithms instead. A natural way to represent the data generated by this large amount of sensors, which are not acting measuring independent variables, and the interaction of the different devices is by using a graph data model. Then, machine learning could be used to aid the Industry 4.0 system to, for example, perform predictive maintenance. However, machine learning directly on graphs, needs feature engineering and has scalability issues. In this paper we discuss methods to convert (embed) the graph in a vector space, such that it becomes feasible to use traditional machine learning methods for Industry 4.0 settings.",
        "file_path": "paper_data/knowledge_graph_embedding/info/c2648a294ef2fc299e1dd959bc1f92973f9c9ebc.pdf",
        "venue": "arXiv.org",
        "citationCount": 19,
        "score": 2.714285714285714,
        "summary": "Industry is evolving towards Industry 4.0, which holds the promise of increased flexibility in manufacturing, better quality and improved productivity. A core actor of this growth is using sensors, which must capture data that can used in unforeseen ways to achieve a performance not achievable without them. However, the complexity of this improved setting is much greater than what is currently used in practice. Hence, it is imperative that the management cannot only be performed by human labor force, but part of that will be done by automated algorithms instead. A natural way to represent the data generated by this large amount of sensors, which are not acting measuring independent variables, and the interaction of the different devices is by using a graph data model. Then, machine learning could be used to aid the Industry 4.0 system to, for example, perform predictive maintenance. However, machine learning directly on graphs, needs feature engineering and has scalability issues. In this paper we discuss methods to convert (embed) the graph in a vector space, such that it becomes feasible to use traditional machine learning methods for Industry 4.0 settings.",
        "keywords": []
      },
      "file_name": "c2648a294ef2fc299e1dd959bc1f92973f9c9ebc.pdf"
    },
    {
      "success": true,
      "doc_id": "9e078e8bf30aac3d03e7ea42c0b21ba9",
      "summary": "Knowledge Graph Embedding (KGE) aims to represent entities and relations of knowledge graph in a low-dimensional continuous vector space. Recent works focus on incorporating structural knowledge with additional information, such as entity descriptions, relation paths and so on. However, common used additional information usually contains plenty of noise, which makes it hard to learn valuable representation. In this paper, we propose a new kind of additional information, called entity neighbors, which contain both semantic and topological features about given entity. We then develop a deep memory network model to encode information from neighbors. Employing a gating mechanism, representations of structure and neighbors are integrated into a joint representation. The experimental results show that our model outperforms existing KGE methods utilizing entity descriptions and achieves state-of-the-art metrics on 4 datasets.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) aims to represent entities and relations of knowledge graph in a low-dimensional continuous vector space. Recent works focus on incorporating structural knowledge with additional information, such as entity descriptions, relation paths and so on. However, common used additional information usually contains plenty of noise, which makes it hard to learn valuable representation. In this paper, we propose a new kind of additional information, called entity neighbors, which contain both semantic and topological features about given entity. We then develop a deep memory network model to encode information from neighbors. Employing a gating mechanism, representations of structure and neighbors are integrated into a joint representation. The experimental results show that our model outperforms existing KGE methods utilizing entity descriptions and achieves state-of-the-art metrics on 4 datasets.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/62c50e300ee87b185401ce27323bbb3f5262fdff.pdf",
      "citation_key": "wang201825m",
      "metadata": {
        "title": "Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network",
        "authors": [
          "Kai Wang",
          "Yu Liu",
          "Xiujuan Xu",
          "Dan Lin"
        ],
        "published_date": "2018",
        "abstract": "Knowledge Graph Embedding (KGE) aims to represent entities and relations of knowledge graph in a low-dimensional continuous vector space. Recent works focus on incorporating structural knowledge with additional information, such as entity descriptions, relation paths and so on. However, common used additional information usually contains plenty of noise, which makes it hard to learn valuable representation. In this paper, we propose a new kind of additional information, called entity neighbors, which contain both semantic and topological features about given entity. We then develop a deep memory network model to encode information from neighbors. Employing a gating mechanism, representations of structure and neighbors are integrated into a joint representation. The experimental results show that our model outperforms existing KGE methods utilizing entity descriptions and achieves state-of-the-art metrics on 4 datasets.",
        "file_path": "paper_data/knowledge_graph_embedding/info/62c50e300ee87b185401ce27323bbb3f5262fdff.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 2.571428571428571,
        "summary": "Knowledge Graph Embedding (KGE) aims to represent entities and relations of knowledge graph in a low-dimensional continuous vector space. Recent works focus on incorporating structural knowledge with additional information, such as entity descriptions, relation paths and so on. However, common used additional information usually contains plenty of noise, which makes it hard to learn valuable representation. In this paper, we propose a new kind of additional information, called entity neighbors, which contain both semantic and topological features about given entity. We then develop a deep memory network model to encode information from neighbors. Employing a gating mechanism, representations of structure and neighbors are integrated into a joint representation. The experimental results show that our model outperforms existing KGE methods utilizing entity descriptions and achieves state-of-the-art metrics on 4 datasets.",
        "keywords": []
      },
      "file_name": "62c50e300ee87b185401ce27323bbb3f5262fdff.pdf"
    },
    {
      "success": true,
      "doc_id": "e4a33c6285c9307d4fd418eb753bc36e",
      "summary": "Knowledge graph embedding aims to represent entities and relations in a continuous feature space while preserving the structure of a knowledge graph. Most existing knowledge graph embedding methods either focus only on a flat structure of the given knowledge graph or exploit the predefined types of entities to explore an enriched structure. In this paper, we define the metagraph of a knowledge graph by proposing a new affinity metric that measures the structural similarity between entities, and then grouping close entities by hypergraph clustering. Without any prior information about entity types, a set of semantically close entities is successfully merged into one super-entity in our metagraph representation. We propose the metagraph-based pre-training model of knowledge graph embedding where we first learn representations in the metagraph and initialize the entities and relations in the original knowledge graph with the learned representations. Experimental results show that our method is effective in improving the accuracy of state-of-the-art knowledge graph embedding methods.",
      "intriguing_abstract": "Knowledge graph embedding aims to represent entities and relations in a continuous feature space while preserving the structure of a knowledge graph. Most existing knowledge graph embedding methods either focus only on a flat structure of the given knowledge graph or exploit the predefined types of entities to explore an enriched structure. In this paper, we define the metagraph of a knowledge graph by proposing a new affinity metric that measures the structural similarity between entities, and then grouping close entities by hypergraph clustering. Without any prior information about entity types, a set of semantically close entities is successfully merged into one super-entity in our metagraph representation. We propose the metagraph-based pre-training model of knowledge graph embedding where we first learn representations in the metagraph and initialize the entities and relations in the original knowledge graph with the learned representations. Experimental results show that our method is effective in improving the accuracy of state-of-the-art knowledge graph embedding methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/66f19b09f644578f808e69f38d3e76f8b972f813.pdf",
      "citation_key": "chung2021u2l",
      "metadata": {
        "title": "Knowledge Graph Embedding via Metagraph Learning",
        "authors": [
          "Chanyoung Chung",
          "Joyce Jiyoung Whang"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding aims to represent entities and relations in a continuous feature space while preserving the structure of a knowledge graph. Most existing knowledge graph embedding methods either focus only on a flat structure of the given knowledge graph or exploit the predefined types of entities to explore an enriched structure. In this paper, we define the metagraph of a knowledge graph by proposing a new affinity metric that measures the structural similarity between entities, and then grouping close entities by hypergraph clustering. Without any prior information about entity types, a set of semantically close entities is successfully merged into one super-entity in our metagraph representation. We propose the metagraph-based pre-training model of knowledge graph embedding where we first learn representations in the metagraph and initialize the entities and relations in the original knowledge graph with the learned representations. Experimental results show that our method is effective in improving the accuracy of state-of-the-art knowledge graph embedding methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/66f19b09f644578f808e69f38d3e76f8b972f813.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 10,
        "score": 2.5,
        "summary": "Knowledge graph embedding aims to represent entities and relations in a continuous feature space while preserving the structure of a knowledge graph. Most existing knowledge graph embedding methods either focus only on a flat structure of the given knowledge graph or exploit the predefined types of entities to explore an enriched structure. In this paper, we define the metagraph of a knowledge graph by proposing a new affinity metric that measures the structural similarity between entities, and then grouping close entities by hypergraph clustering. Without any prior information about entity types, a set of semantically close entities is successfully merged into one super-entity in our metagraph representation. We propose the metagraph-based pre-training model of knowledge graph embedding where we first learn representations in the metagraph and initialize the entities and relations in the original knowledge graph with the learned representations. Experimental results show that our method is effective in improving the accuracy of state-of-the-art knowledge graph embedding methods.",
        "keywords": []
      },
      "file_name": "66f19b09f644578f808e69f38d3e76f8b972f813.pdf"
    },
    {
      "success": true,
      "doc_id": "a1944f51c20bc2662ba9ca2ace23ac4d",
      "summary": "The trends of open science have enabled several open scholarly datasets which include millions of papers and authors. Managing, exploring, and utilizing such large and complicated datasets effectively are challenging. In recent years, the knowledge graph has emerged as a universal data format for representing knowledge about heterogeneous entities and their relationships. The knowledge graph can be modeled by knowledge graph embedding methods, which represent entities and relations as embedding vectors in semantic space, then model the interactions between these embedding vectors. However, the semantic structures in the knowledge graph embedding space are not well-studied, thus knowledge graph embedding methods are usually only used for knowledge graph completion but not data representation and analysis. In this paper, we propose to analyze these semantic structures based on the well-studied word embedding space and use them to support data exploration. We also define the semantic queries, which are algebraic operations between the embedding vectors in the knowledge graph embedding space, to solve queries such as similarity and analogy between the entities on the original datasets. We then design a general framework for data exploration by semantic queries and discuss the solution to some traditional scholarly data exploration tasks. We also propose some new interesting tasks that can be solved based on the uncanny semantic structures of the embedding space.",
      "intriguing_abstract": "The trends of open science have enabled several open scholarly datasets which include millions of papers and authors. Managing, exploring, and utilizing such large and complicated datasets effectively are challenging. In recent years, the knowledge graph has emerged as a universal data format for representing knowledge about heterogeneous entities and their relationships. The knowledge graph can be modeled by knowledge graph embedding methods, which represent entities and relations as embedding vectors in semantic space, then model the interactions between these embedding vectors. However, the semantic structures in the knowledge graph embedding space are not well-studied, thus knowledge graph embedding methods are usually only used for knowledge graph completion but not data representation and analysis. In this paper, we propose to analyze these semantic structures based on the well-studied word embedding space and use them to support data exploration. We also define the semantic queries, which are algebraic operations between the embedding vectors in the knowledge graph embedding space, to solve queries such as similarity and analogy between the entities on the original datasets. We then design a general framework for data exploration by semantic queries and discuss the solution to some traditional scholarly data exploration tasks. We also propose some new interesting tasks that can be solved based on the uncanny semantic structures of the embedding space.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9b68475f787be0999e7d09456003e664d37c2155.pdf",
      "citation_key": "tran2019j42",
      "metadata": {
        "title": "Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space",
        "authors": [
          "Hung Nghiep Tran",
          "A. Takasu"
        ],
        "published_date": "2019",
        "abstract": "The trends of open science have enabled several open scholarly datasets which include millions of papers and authors. Managing, exploring, and utilizing such large and complicated datasets effectively are challenging. In recent years, the knowledge graph has emerged as a universal data format for representing knowledge about heterogeneous entities and their relationships. The knowledge graph can be modeled by knowledge graph embedding methods, which represent entities and relations as embedding vectors in semantic space, then model the interactions between these embedding vectors. However, the semantic structures in the knowledge graph embedding space are not well-studied, thus knowledge graph embedding methods are usually only used for knowledge graph completion but not data representation and analysis. In this paper, we propose to analyze these semantic structures based on the well-studied word embedding space and use them to support data exploration. We also define the semantic queries, which are algebraic operations between the embedding vectors in the knowledge graph embedding space, to solve queries such as similarity and analogy between the entities on the original datasets. We then design a general framework for data exploration by semantic queries and discuss the solution to some traditional scholarly data exploration tasks. We also propose some new interesting tasks that can be solved based on the uncanny semantic structures of the embedding space.",
        "file_path": "paper_data/knowledge_graph_embedding/info/9b68475f787be0999e7d09456003e664d37c2155.pdf",
        "venue": "International Conference on Theory and Practice of Digital Libraries",
        "citationCount": 15,
        "score": 2.5,
        "summary": "The trends of open science have enabled several open scholarly datasets which include millions of papers and authors. Managing, exploring, and utilizing such large and complicated datasets effectively are challenging. In recent years, the knowledge graph has emerged as a universal data format for representing knowledge about heterogeneous entities and their relationships. The knowledge graph can be modeled by knowledge graph embedding methods, which represent entities and relations as embedding vectors in semantic space, then model the interactions between these embedding vectors. However, the semantic structures in the knowledge graph embedding space are not well-studied, thus knowledge graph embedding methods are usually only used for knowledge graph completion but not data representation and analysis. In this paper, we propose to analyze these semantic structures based on the well-studied word embedding space and use them to support data exploration. We also define the semantic queries, which are algebraic operations between the embedding vectors in the knowledge graph embedding space, to solve queries such as similarity and analogy between the entities on the original datasets. We then design a general framework for data exploration by semantic queries and discuss the solution to some traditional scholarly data exploration tasks. We also propose some new interesting tasks that can be solved based on the uncanny semantic structures of the embedding space.",
        "keywords": []
      },
      "file_name": "9b68475f787be0999e7d09456003e664d37c2155.pdf"
    },
    {
      "success": true,
      "doc_id": "4e672a4e82dc4ab83e582bcbe0c1ea56",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f0ac0c2f82886700dc7e7a178d597d33deebfc88.pdf",
      "citation_key": "shi2017m2h",
      "metadata": {
        "title": "Knowledge Graph Embedding with Triple Context",
        "authors": [
          "Jun Shi",
          "Huan Gao",
          "G. Qi",
          "Zhangquan Zhou"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/f0ac0c2f82886700dc7e7a178d597d33deebfc88.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 19,
        "score": 2.375,
        "summary": "",
        "keywords": []
      },
      "file_name": "f0ac0c2f82886700dc7e7a178d597d33deebfc88.pdf"
    },
    {
      "success": true,
      "doc_id": "5ee24a61ee2a40e9582de4fa52485c8a",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a5aeca7ef265b27ff6d9ea08873c9499632b6439.pdf",
      "citation_key": "zhang2017ixt",
      "metadata": {
        "title": "Knowledge Graph Embedding with Diversity of Structures",
        "authors": [
          "Wen Zhang"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/a5aeca7ef265b27ff6d9ea08873c9499632b6439.pdf",
        "venue": "The Web Conference",
        "citationCount": 19,
        "score": 2.375,
        "summary": "",
        "keywords": []
      },
      "file_name": "a5aeca7ef265b27ff6d9ea08873c9499632b6439.pdf"
    },
    {
      "success": true,
      "doc_id": "d2dc1a7419c259e48ff52660f07fa06e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/8412cc4dd7c8d309d7a84573637d4daaad8d33b5.pdf",
      "citation_key": "zhu20196p1",
      "metadata": {
        "title": "Top-N Collaborative Filtering Recommendation Algorithm Based on Knowledge Graph Embedding",
        "authors": [
          "Ming-Yi Zhu",
          "De-sheng Zhen",
          "Ran Tao",
          "Youqun Shi",
          "Xiangyang Feng",
          "Qian Wang"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/8412cc4dd7c8d309d7a84573637d4daaad8d33b5.pdf",
        "venue": "International Conference on Knowledge Management in Organizations",
        "citationCount": 14,
        "score": 2.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "8412cc4dd7c8d309d7a84573637d4daaad8d33b5.pdf"
    },
    {
      "success": true,
      "doc_id": "4e6a6a4507d23662f1978757a1e3e3f8",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/8be21591c29d68d99e89a71fc7755f09f5eed3a1.pdf",
      "citation_key": "kertkeidkachorn2019dkn",
      "metadata": {
        "title": "GTransE: Generalizing Translation-Based Model on Uncertain Knowledge Graph Embedding",
        "authors": [
          "Natthawut Kertkeidkachorn",
          "Xin Liu",
          "R. Ichise"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/8be21591c29d68d99e89a71fc7755f09f5eed3a1.pdf",
        "venue": "JSAI",
        "citationCount": 14,
        "score": 2.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "8be21591c29d68d99e89a71fc7755f09f5eed3a1.pdf"
    },
    {
      "success": true,
      "doc_id": "dc513d7ba777d8a63acee5e0911fae42",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6493e6d563282fcb65029162a71cd2cb8168765b.pdf",
      "citation_key": "zhu2019zqy",
      "metadata": {
        "title": "A semi-supervised model for knowledge graph embedding",
        "authors": [
          "Jia Zhu",
          "Zetao Zheng",
          "Min Yang",
          "G. Fung",
          "Yong Tang"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/6493e6d563282fcb65029162a71cd2cb8168765b.pdf",
        "venue": "Data mining and knowledge discovery",
        "citationCount": 13,
        "score": 2.1666666666666665,
        "summary": "",
        "keywords": []
      },
      "file_name": "6493e6d563282fcb65029162a71cd2cb8168765b.pdf"
    },
    {
      "success": true,
      "doc_id": "ef2fc12e6f09f5b641c388fb941f2905",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/d5eabc89e2346411134569a603e63a143d1d6552.pdf",
      "citation_key": "zhang20193g2",
      "metadata": {
        "title": "Towards Data Poisoning Attack against Knowledge Graph Embedding",
        "authors": [
          "Hengtong Zhang",
          "T. Zheng",
          "Jing Gao",
          "Chenglin Miao",
          "Lu Su",
          "Yaliang Li",
          "K. Ren"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/d5eabc89e2346411134569a603e63a143d1d6552.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 2.1666666666666665,
        "summary": "",
        "keywords": []
      },
      "file_name": "d5eabc89e2346411134569a603e63a143d1d6552.pdf"
    },
    {
      "success": true,
      "doc_id": "336caab6d7db49ac49e3f12a2c7bfd58",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/89cf9719b97e69f5bb7d715d5a16609676c14e86.pdf",
      "citation_key": "liu2019fcs",
      "metadata": {
        "title": "Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding",
        "authors": [
          "Wenqiang Liu",
          "Hongyun Cai",
          "Xu Cheng",
          "Sifa Xie",
          "Yipeng Yu",
          "Hanyu Zhang"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/89cf9719b97e69f5bb7d715d5a16609676c14e86.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 13,
        "score": 2.1666666666666665,
        "summary": "",
        "keywords": []
      },
      "file_name": "89cf9719b97e69f5bb7d715d5a16609676c14e86.pdf"
    },
    {
      "success": true,
      "doc_id": "9b875e586d444e3f09b8ed4a1e2ac50a",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/1c1b5fd282d3a1fe03a671f7d13092d49cb31139.pdf",
      "citation_key": "kanojia20171in",
      "metadata": {
        "title": "Enhancing Knowledge Graph Embedding with Probabilistic Negative Sampling",
        "authors": [
          "Vibhor Kanojia",
          "Hideyuki Maeda",
          "Riku Togashi",
          "Sumio Fujita"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/1c1b5fd282d3a1fe03a671f7d13092d49cb31139.pdf",
        "venue": "The Web Conference",
        "citationCount": 14,
        "score": 1.75,
        "summary": "",
        "keywords": []
      },
      "file_name": "1c1b5fd282d3a1fe03a671f7d13092d49cb31139.pdf"
    },
    {
      "success": true,
      "doc_id": "1a53b03b89f6562eb8c4ee8764702e73",
      "summary": "Knowledge graph embedding aims to represent entities and relations of a knowledge graph in continuous vector spaces. It has increasingly drawn attention for its ability to encode semantics in low dimensional vectors as well as its outstanding performance on many applications, such as question answering systems and information retrieval tasks. Existing methods often handle each triple independently, without considering context information of a triple in the knowledge graph, such an information can be useful for inference of new knowledge. Moreover, the relations and paths between an entity pair also provide information for inference. In this paper, we define a novel context-dependent knowledge graph representation model named triple-context-based knowledge embedding, which is based on the notion of triple context used for embedding entities and relations. For each triple, the triple context is composed of two kinds of graph structured information: one is a set of neighboring entities along with their outgoing relations, the other is a set of relation paths which contain a pair of target entities. Our embedding method is designed to utilize the triple context of each triple while learning embeddings of entities and relations. The method is evaluated on multiple tasks in the paper. Experimental results reveal that our method achieves significant improvements over the state-of-the-art methods.",
      "intriguing_abstract": "Knowledge graph embedding aims to represent entities and relations of a knowledge graph in continuous vector spaces. It has increasingly drawn attention for its ability to encode semantics in low dimensional vectors as well as its outstanding performance on many applications, such as question answering systems and information retrieval tasks. Existing methods often handle each triple independently, without considering context information of a triple in the knowledge graph, such an information can be useful for inference of new knowledge. Moreover, the relations and paths between an entity pair also provide information for inference. In this paper, we define a novel context-dependent knowledge graph representation model named triple-context-based knowledge embedding, which is based on the notion of triple context used for embedding entities and relations. For each triple, the triple context is composed of two kinds of graph structured information: one is a set of neighboring entities along with their outgoing relations, the other is a set of relation paths which contain a pair of target entities. Our embedding method is designed to utilize the triple context of each triple while learning embeddings of entities and relations. The method is evaluated on multiple tasks in the paper. Experimental results reveal that our method achieves significant improvements over the state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7f7137d3e1de7e0e801c27d5e8b963dfd6d94eb4.pdf",
      "citation_key": "gao2018di0",
      "metadata": {
        "title": "Triple Context-Based Knowledge Graph Embedding",
        "authors": [
          "Huan Gao",
          "Jun Shi",
          "G. Qi",
          "M. Wang"
        ],
        "published_date": "2018",
        "abstract": "Knowledge graph embedding aims to represent entities and relations of a knowledge graph in continuous vector spaces. It has increasingly drawn attention for its ability to encode semantics in low dimensional vectors as well as its outstanding performance on many applications, such as question answering systems and information retrieval tasks. Existing methods often handle each triple independently, without considering context information of a triple in the knowledge graph, such an information can be useful for inference of new knowledge. Moreover, the relations and paths between an entity pair also provide information for inference. In this paper, we define a novel context-dependent knowledge graph representation model named triple-context-based knowledge embedding, which is based on the notion of triple context used for embedding entities and relations. For each triple, the triple context is composed of two kinds of graph structured information: one is a set of neighboring entities along with their outgoing relations, the other is a set of relation paths which contain a pair of target entities. Our embedding method is designed to utilize the triple context of each triple while learning embeddings of entities and relations. The method is evaluated on multiple tasks in the paper. Experimental results reveal that our method achieves significant improvements over the state-of-the-art methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/7f7137d3e1de7e0e801c27d5e8b963dfd6d94eb4.pdf",
        "venue": "IEEE Access",
        "citationCount": 12,
        "score": 1.7142857142857142,
        "summary": "Knowledge graph embedding aims to represent entities and relations of a knowledge graph in continuous vector spaces. It has increasingly drawn attention for its ability to encode semantics in low dimensional vectors as well as its outstanding performance on many applications, such as question answering systems and information retrieval tasks. Existing methods often handle each triple independently, without considering context information of a triple in the knowledge graph, such an information can be useful for inference of new knowledge. Moreover, the relations and paths between an entity pair also provide information for inference. In this paper, we define a novel context-dependent knowledge graph representation model named triple-context-based knowledge embedding, which is based on the notion of triple context used for embedding entities and relations. For each triple, the triple context is composed of two kinds of graph structured information: one is a set of neighboring entities along with their outgoing relations, the other is a set of relation paths which contain a pair of target entities. Our embedding method is designed to utilize the triple context of each triple while learning embeddings of entities and relations. The method is evaluated on multiple tasks in the paper. Experimental results reveal that our method achieves significant improvements over the state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "7f7137d3e1de7e0e801c27d5e8b963dfd6d94eb4.pdf"
    },
    {
      "success": true,
      "doc_id": "e1f8547ab73034ccdedaf445f4db1567",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/49899fd94cd272914f7d1e81b0915058c25bb665.pdf",
      "citation_key": "mai2018egi",
      "metadata": {
        "title": "Support and Centrality: Learning Weights for Knowledge Graph Embedding Models",
        "authors": [
          "Gengchen Mai",
          "K. Janowicz",
          "Bo Yan"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/49899fd94cd272914f7d1e81b0915058c25bb665.pdf",
        "venue": "International Conference Knowledge Engineering and Knowledge Management",
        "citationCount": 11,
        "score": 1.5714285714285714,
        "summary": "",
        "keywords": []
      },
      "file_name": "49899fd94cd272914f7d1e81b0915058c25bb665.pdf"
    },
    {
      "success": true,
      "doc_id": "e97ae007a75d885de21e9fcf62677613",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/e64557514ab856d22ddbb34bc23ffb7085d5d6b0.pdf",
      "citation_key": "xiao2016bb9",
      "metadata": {
        "title": "Knowledge Semantic Representation: A Generative Model for Interpretable Knowledge Graph Embedding",
        "authors": [
          "Han Xiao",
          "Minlie Huang",
          "Xiaoyan Zhu"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/e64557514ab856d22ddbb34bc23ffb7085d5d6b0.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 1.3333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "e64557514ab856d22ddbb34bc23ffb7085d5d6b0.pdf"
    },
    {
      "success": true,
      "doc_id": "cc0c43ae619f3e4037d4f8ba2fd49806",
      "summary": "In complex assembly industry settings, fault localization involves rapidly and accurately identifying the source of a fault and obtaining a troubleshooting solution based on fault symptoms. This study proposes a knowledge-enhanced joint model that incorporates aviation assembly knowledge graph (KG) embedding into large language models (LLMs). This model utilizes graph-structured Big Data within KGs to conduct prefix-tuning of the LLMs. The KGs for prefix-tuning enable an online reconfiguration of the LLMs, which avoids a massive computational load. Through the subgraph embedding learning process, the specialized knowledge of the joint model within the aviation assembly domain, especially in fault localization, is strengthened. In the context of aviation assembly functional testing, the joint model can generate knowledge subgraphs, fuse knowledge through retrieval augmentation, and ultimately provide knowledge-based reasoning responses. In practical industrial scenario experiments, the joint enhancement model demonstrates an accuracy of 98.5% for fault diagnosis and troubleshooting schemes.",
      "intriguing_abstract": "In complex assembly industry settings, fault localization involves rapidly and accurately identifying the source of a fault and obtaining a troubleshooting solution based on fault symptoms. This study proposes a knowledge-enhanced joint model that incorporates aviation assembly knowledge graph (KG) embedding into large language models (LLMs). This model utilizes graph-structured Big Data within KGs to conduct prefix-tuning of the LLMs. The KGs for prefix-tuning enable an online reconfiguration of the LLMs, which avoids a massive computational load. Through the subgraph embedding learning process, the specialized knowledge of the joint model within the aviation assembly domain, especially in fault localization, is strengthened. In the context of aviation assembly functional testing, the joint model can generate knowledge subgraphs, fuse knowledge through retrieval augmentation, and ultimately provide knowledge-based reasoning responses. In practical industrial scenario experiments, the joint enhancement model demonstrates an accuracy of 98.5% for fault diagnosis and troubleshooting schemes.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7eece37709dceba5086f48dc43ac1a69d0427486.pdf",
      "citation_key": "liu2024q3q",
      "metadata": {
        "title": "Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly",
        "authors": [
          "Peifeng Liu",
          "Lu Qian",
          "Xingwei Zhao",
          "Bo Tao"
        ],
        "published_date": "2024",
        "abstract": "In complex assembly industry settings, fault localization involves rapidly and accurately identifying the source of a fault and obtaining a troubleshooting solution based on fault symptoms. This study proposes a knowledge-enhanced joint model that incorporates aviation assembly knowledge graph (KG) embedding into large language models (LLMs). This model utilizes graph-structured Big Data within KGs to conduct prefix-tuning of the LLMs. The KGs for prefix-tuning enable an online reconfiguration of the LLMs, which avoids a massive computational load. Through the subgraph embedding learning process, the specialized knowledge of the joint model within the aviation assembly domain, especially in fault localization, is strengthened. In the context of aviation assembly functional testing, the joint model can generate knowledge subgraphs, fuse knowledge through retrieval augmentation, and ultimately provide knowledge-based reasoning responses. In practical industrial scenario experiments, the joint enhancement model demonstrates an accuracy of 98.5% for fault diagnosis and troubleshooting schemes.",
        "file_path": "paper_data/knowledge_graph_embedding/info/7eece37709dceba5086f48dc43ac1a69d0427486.pdf",
        "venue": "IEEE Transactions on Industrial Informatics",
        "citationCount": 60,
        "score": 60.0,
        "summary": "In complex assembly industry settings, fault localization involves rapidly and accurately identifying the source of a fault and obtaining a troubleshooting solution based on fault symptoms. This study proposes a knowledge-enhanced joint model that incorporates aviation assembly knowledge graph (KG) embedding into large language models (LLMs). This model utilizes graph-structured Big Data within KGs to conduct prefix-tuning of the LLMs. The KGs for prefix-tuning enable an online reconfiguration of the LLMs, which avoids a massive computational load. Through the subgraph embedding learning process, the specialized knowledge of the joint model within the aviation assembly domain, especially in fault localization, is strengthened. In the context of aviation assembly functional testing, the joint model can generate knowledge subgraphs, fuse knowledge through retrieval augmentation, and ultimately provide knowledge-based reasoning responses. In practical industrial scenario experiments, the joint enhancement model demonstrates an accuracy of 98.5% for fault diagnosis and troubleshooting schemes.",
        "keywords": []
      },
      "file_name": "7eece37709dceba5086f48dc43ac1a69d0427486.pdf"
    },
    {
      "success": true,
      "doc_id": "7c9e5b69c3d22e05929e57356a6e6583",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/83424a4fea2e75311632059914bf358bc045435f.pdf",
      "citation_key": "zhang2024cjl",
      "metadata": {
        "title": "A review of recommender systems based on knowledge graph embedding",
        "authors": [
          "Jin-cheng Zhang",
          "A. Zain",
          "Kai Zhou",
          "Xi Chen",
          "Renmin Zhang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/83424a4fea2e75311632059914bf358bc045435f.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 45,
        "score": 45.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "83424a4fea2e75311632059914bf358bc045435f.pdf"
    },
    {
      "success": true,
      "doc_id": "a7990fb733b932dd224181c88c617df4",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/3f8b13ede9f4d3a770ec8b4771b6036b9f603bfa.pdf",
      "citation_key": "su2023v6e",
      "metadata": {
        "title": "Biomedical Knowledge Graph Embedding With Capsule Network for Multi-Label Drug-Drug Interaction Prediction",
        "authors": [
          "Xiao-Rui Su",
          "Zhuhong You",
          "Deshuang Huang",
          "Lei Wang",
          "Leon Wong",
          "Boya Ji",
          "Bowei Zhao"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/3f8b13ede9f4d3a770ec8b4771b6036b9f603bfa.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 67,
        "score": 33.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "3f8b13ede9f4d3a770ec8b4771b6036b9f603bfa.pdf"
    },
    {
      "success": true,
      "doc_id": "a8f215f86f0286555f79fea8804afc3e",
      "summary": "Federated Learning (FL) recently emerges as a paradigm to train a global machine learning model across distributed clients without sharing raw data. Knowledge Graph (KG) embedding represents KGs in a continuous vector space, serving as the backbone of many knowledge-driven applications. As a promising combination, federated KG embedding can fully take advantage of knowledge learned from different clients while preserving the privacy of local data. However, realistic problems such as data heterogeneity and knowledge forgetting still remain to be concerned. In this paper, we propose FedLU, a novel FL framework for heterogeneous KG embedding learning and unlearning. To cope with the drift between local optimization and global convergence caused by data heterogeneity, we propose mutual knowledge distillation to transfer local knowledge to global, and absorb global knowledge back. Moreover, we present an unlearning method based on cognitive neuroscience, which combines retroactive interference and passive decay to erase specific knowledge from local clients and propagate to the global model by reusing knowledge distillation. We construct new datasets for assessing realistic performance of the state-of-the-arts. Extensive experiments show that FedLU achieves superior results in both link prediction and knowledge forgetting.",
      "intriguing_abstract": "Federated Learning (FL) recently emerges as a paradigm to train a global machine learning model across distributed clients without sharing raw data. Knowledge Graph (KG) embedding represents KGs in a continuous vector space, serving as the backbone of many knowledge-driven applications. As a promising combination, federated KG embedding can fully take advantage of knowledge learned from different clients while preserving the privacy of local data. However, realistic problems such as data heterogeneity and knowledge forgetting still remain to be concerned. In this paper, we propose FedLU, a novel FL framework for heterogeneous KG embedding learning and unlearning. To cope with the drift between local optimization and global convergence caused by data heterogeneity, we propose mutual knowledge distillation to transfer local knowledge to global, and absorb global knowledge back. Moreover, we present an unlearning method based on cognitive neuroscience, which combines retroactive interference and passive decay to erase specific knowledge from local clients and propagate to the global model by reusing knowledge distillation. We construct new datasets for assessing realistic performance of the state-of-the-arts. Extensive experiments show that FedLU achieves superior results in both link prediction and knowledge forgetting.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/ac0c9afa9c19f0700d903e00a92e83e41587add3.pdf",
      "citation_key": "zhu2023bfj",
      "metadata": {
        "title": "Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning",
        "authors": [
          "Xiangrong Zhu",
          "Guang-pu Li",
          "Wei Hu"
        ],
        "published_date": "2023",
        "abstract": "Federated Learning (FL) recently emerges as a paradigm to train a global machine learning model across distributed clients without sharing raw data. Knowledge Graph (KG) embedding represents KGs in a continuous vector space, serving as the backbone of many knowledge-driven applications. As a promising combination, federated KG embedding can fully take advantage of knowledge learned from different clients while preserving the privacy of local data. However, realistic problems such as data heterogeneity and knowledge forgetting still remain to be concerned. In this paper, we propose FedLU, a novel FL framework for heterogeneous KG embedding learning and unlearning. To cope with the drift between local optimization and global convergence caused by data heterogeneity, we propose mutual knowledge distillation to transfer local knowledge to global, and absorb global knowledge back. Moreover, we present an unlearning method based on cognitive neuroscience, which combines retroactive interference and passive decay to erase specific knowledge from local clients and propagate to the global model by reusing knowledge distillation. We construct new datasets for assessing realistic performance of the state-of-the-arts. Extensive experiments show that FedLU achieves superior results in both link prediction and knowledge forgetting.",
        "file_path": "paper_data/knowledge_graph_embedding/info/ac0c9afa9c19f0700d903e00a92e83e41587add3.pdf",
        "venue": "The Web Conference",
        "citationCount": 65,
        "score": 32.5,
        "summary": "Federated Learning (FL) recently emerges as a paradigm to train a global machine learning model across distributed clients without sharing raw data. Knowledge Graph (KG) embedding represents KGs in a continuous vector space, serving as the backbone of many knowledge-driven applications. As a promising combination, federated KG embedding can fully take advantage of knowledge learned from different clients while preserving the privacy of local data. However, realistic problems such as data heterogeneity and knowledge forgetting still remain to be concerned. In this paper, we propose FedLU, a novel FL framework for heterogeneous KG embedding learning and unlearning. To cope with the drift between local optimization and global convergence caused by data heterogeneity, we propose mutual knowledge distillation to transfer local knowledge to global, and absorb global knowledge back. Moreover, we present an unlearning method based on cognitive neuroscience, which combines retroactive interference and passive decay to erase specific knowledge from local clients and propagate to the global model by reusing knowledge distillation. We construct new datasets for assessing realistic performance of the state-of-the-arts. Extensive experiments show that FedLU achieves superior results in both link prediction and knowledge forgetting.",
        "keywords": []
      },
      "file_name": "ac0c9afa9c19f0700d903e00a92e83e41587add3.pdf"
    },
    {
      "success": true,
      "doc_id": "249eb0ccc920a0059332ed4a7835cdbc",
      "summary": "Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges. To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge. However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods. On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs. On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively. In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs. First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features. Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation. Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge. Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines. Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score. More exploratory experiments validate the effectiveness of IncDE in proficiently learning new knowledge while preserving old knowledge across all time steps.",
      "intriguing_abstract": "Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges. To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge. However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods. On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs. On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively. In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs. First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features. Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation. Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge. Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines. Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score. More exploratory experiments validate the effectiveness of IncDE in proficiently learning new knowledge while preserving old knowledge across all time steps.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f42d060fb530a11daecd90695211c01a5c264f8d.pdf",
      "citation_key": "liu2024to0",
      "metadata": {
        "title": "Towards Continual Knowledge Graph Embedding via Incremental Distillation",
        "authors": [
          "Jiajun Liu",
          "Wenjun Ke",
          "Peng Wang",
          "Ziyu Shang",
          "Jinhua Gao",
          "Guozheng Li",
          "Ke Ji",
          "Yanhe Liu"
        ],
        "published_date": "2024",
        "abstract": "Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges. To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge. However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods. On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs. On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively. In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs. First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features. Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation. Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge. Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines. Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score. More exploratory experiments validate the effectiveness of IncDE in proficiently learning new knowledge while preserving old knowledge across all time steps.",
        "file_path": "paper_data/knowledge_graph_embedding/info/f42d060fb530a11daecd90695211c01a5c264f8d.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 31,
        "score": 31.0,
        "summary": "Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges. To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge. However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods. On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs. On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively. In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs. First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features. Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation. Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge. Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines. Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score. More exploratory experiments validate the effectiveness of IncDE in proficiently learning new knowledge while preserving old knowledge across all time steps.",
        "keywords": []
      },
      "file_name": "f42d060fb530a11daecd90695211c01a5c264f8d.pdf"
    },
    {
      "success": true,
      "doc_id": "c230bbc8a41dd74088e9e66f6c63df08",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7aca91d068d20d3389b28b8277ebc3d488be459f.pdf",
      "citation_key": "wang2024vgj",
      "metadata": {
        "title": "Knowledge-aware fine-grained attention networks with refined knowledge graph embedding for personalized recommendation",
        "authors": [
          "Wei Wang",
          "Xiaoxuan Shen",
          "Baolin Yi",
          "Huanyu Zhang",
          "Jianfang Liu",
          "Chao Dai"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/7aca91d068d20d3389b28b8277ebc3d488be459f.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 27,
        "score": 27.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "7aca91d068d20d3389b28b8277ebc3d488be459f.pdf"
    },
    {
      "success": true,
      "doc_id": "25e656c6c09164e004de802235cefc74",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/fa07384402f5c9d5b789edf7667bbcc555f381e3.pdf",
      "citation_key": "li2024920",
      "metadata": {
        "title": "SDFormer: A shallow-to-deep feature interaction for knowledge graph embedding",
        "authors": [
          "Duantengchuan Li",
          "Tao Xia",
          "Jing Wang",
          "Fobo Shi",
          "Qi Zhang",
          "Bing Li",
          "Yu Xiong"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/fa07384402f5c9d5b789edf7667bbcc555f381e3.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 26,
        "score": 26.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "fa07384402f5c9d5b789edf7667bbcc555f381e3.pdf"
    },
    {
      "success": true,
      "doc_id": "68645fad37247891889f7ff8b8ed140b",
      "summary": "Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outperforms 14 different state-of-the-art methods on varied inductive learning scenarios.",
      "intriguing_abstract": "Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outperforms 14 different state-of-the-art methods on varied inductive learning scenarios.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/48c2e0d87b84efca7f11462bbdac1be1177e2433.pdf",
      "citation_key": "lee202380l",
      "metadata": {
        "title": "InGram: Inductive Knowledge Graph Embedding via Relation Graphs",
        "authors": [
          "Jaejun Lee",
          "Chanyoung Chung",
          "Joyce Jiyoung Whang"
        ],
        "published_date": "2023",
        "abstract": "Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outperforms 14 different state-of-the-art methods on varied inductive learning scenarios.",
        "file_path": "paper_data/knowledge_graph_embedding/info/48c2e0d87b84efca7f11462bbdac1be1177e2433.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 50,
        "score": 25.0,
        "summary": "Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outperforms 14 different state-of-the-art methods on varied inductive learning scenarios.",
        "keywords": []
      },
      "file_name": "48c2e0d87b84efca7f11462bbdac1be1177e2433.pdf"
    },
    {
      "success": true,
      "doc_id": "05aab20c4f039fc6b2bfbc103dc18bb9",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/51c18009b2c566d7cddc934b2cf9a1bca813f58f.pdf",
      "citation_key": "shokrzadeh2023twj",
      "metadata": {
        "title": "Knowledge graph-based recommendation system enhanced by neural collaborative filtering and knowledge graph embedding",
        "authors": [
          "Zeinab Shokrzadeh",
          "M. Feizi-Derakhshi",
          "M. Balafar",
          "Jamshid Bagherzadeh Mohasefi"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/51c18009b2c566d7cddc934b2cf9a1bca813f58f.pdf",
        "venue": "Ain Shams Engineering Journal",
        "citationCount": 50,
        "score": 25.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "51c18009b2c566d7cddc934b2cf9a1bca813f58f.pdf"
    },
    {
      "success": true,
      "doc_id": "aa9bb8304e3f7b9d3f80e2524fee1740",
      "summary": "Cognitive diagnosis (CD) aims to reveal the proficiency of students on specific knowledge concepts and traits of test exercises (e.g., difficulty). It plays a critical role in intelligent education systems by supporting personalized learning guidance. However, recent developments in CD mostly concentrate on improving the accuracy of diagnostic results and often overlook the important and practical task: domain-level zero-shot cognitive diagnosis (DZCD). The primary challenge of DZCD is the deficiency of student behavior data in the target domain due to the absence of student-exercise interactions or unavailability of exercising records for training purposes. To tackle the cold-start issue, we propose a two-stage solution named TechCD (Transferable knowledgE Concept grapH embedding framework for Cognitive Diagnosis). The fundamental notion involves utilizing a pedagogical knowledge concept graph (KCG) as a mediator to connect disparate domains, allowing the transmission of student cognitive signals from established domains to the zero-shot cold-start domain. Specifically, a naive yet effective graph convolutional network (GCN) with the bottom-layer discarding operation is initially employed over the KCG to learn transferable student cognitive states and domain-specific exercise traits. Moreover, we give three implementations of the general TechCD framework following the typical cognitive diagnosis solutions. Finally, extensive experiments on real-world datasets not only prove that Tech can effectively perform zero-shot diagnosis, but also give some popular applications such as exercise recommendation.",
      "intriguing_abstract": "Cognitive diagnosis (CD) aims to reveal the proficiency of students on specific knowledge concepts and traits of test exercises (e.g., difficulty). It plays a critical role in intelligent education systems by supporting personalized learning guidance. However, recent developments in CD mostly concentrate on improving the accuracy of diagnostic results and often overlook the important and practical task: domain-level zero-shot cognitive diagnosis (DZCD). The primary challenge of DZCD is the deficiency of student behavior data in the target domain due to the absence of student-exercise interactions or unavailability of exercising records for training purposes. To tackle the cold-start issue, we propose a two-stage solution named TechCD (Transferable knowledgE Concept grapH embedding framework for Cognitive Diagnosis). The fundamental notion involves utilizing a pedagogical knowledge concept graph (KCG) as a mediator to connect disparate domains, allowing the transmission of student cognitive signals from established domains to the zero-shot cold-start domain. Specifically, a naive yet effective graph convolutional network (GCN) with the bottom-layer discarding operation is initially employed over the KCG to learn transferable student cognitive states and domain-specific exercise traits. Moreover, we give three implementations of the general TechCD framework following the typical cognitive diagnosis solutions. Finally, extensive experiments on real-world datasets not only prove that Tech can effectively perform zero-shot diagnosis, but also give some popular applications such as exercise recommendation.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/5cbf9bc26b3d0471cb37c3f4a931990b1260d82d.pdf",
      "citation_key": "gao2023086",
      "metadata": {
        "title": "Leveraging Transferable Knowledge Concept Graph Embedding for Cold-Start Cognitive Diagnosis",
        "authors": [
          "Weibo Gao",
          "Hao Wang",
          "Qi Liu",
          "Fei Wang",
          "Xin Lin",
          "Linan Yue",
          "Zheng Zhang",
          "Rui Lv",
          "Shijin Wang"
        ],
        "published_date": "2023",
        "abstract": "Cognitive diagnosis (CD) aims to reveal the proficiency of students on specific knowledge concepts and traits of test exercises (e.g., difficulty). It plays a critical role in intelligent education systems by supporting personalized learning guidance. However, recent developments in CD mostly concentrate on improving the accuracy of diagnostic results and often overlook the important and practical task: domain-level zero-shot cognitive diagnosis (DZCD). The primary challenge of DZCD is the deficiency of student behavior data in the target domain due to the absence of student-exercise interactions or unavailability of exercising records for training purposes. To tackle the cold-start issue, we propose a two-stage solution named TechCD (Transferable knowledgE Concept grapH embedding framework for Cognitive Diagnosis). The fundamental notion involves utilizing a pedagogical knowledge concept graph (KCG) as a mediator to connect disparate domains, allowing the transmission of student cognitive signals from established domains to the zero-shot cold-start domain. Specifically, a naive yet effective graph convolutional network (GCN) with the bottom-layer discarding operation is initially employed over the KCG to learn transferable student cognitive states and domain-specific exercise traits. Moreover, we give three implementations of the general TechCD framework following the typical cognitive diagnosis solutions. Finally, extensive experiments on real-world datasets not only prove that Tech can effectively perform zero-shot diagnosis, but also give some popular applications such as exercise recommendation.",
        "file_path": "paper_data/knowledge_graph_embedding/info/5cbf9bc26b3d0471cb37c3f4a931990b1260d82d.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 45,
        "score": 22.5,
        "summary": "Cognitive diagnosis (CD) aims to reveal the proficiency of students on specific knowledge concepts and traits of test exercises (e.g., difficulty). It plays a critical role in intelligent education systems by supporting personalized learning guidance. However, recent developments in CD mostly concentrate on improving the accuracy of diagnostic results and often overlook the important and practical task: domain-level zero-shot cognitive diagnosis (DZCD). The primary challenge of DZCD is the deficiency of student behavior data in the target domain due to the absence of student-exercise interactions or unavailability of exercising records for training purposes. To tackle the cold-start issue, we propose a two-stage solution named TechCD (Transferable knowledgE Concept grapH embedding framework for Cognitive Diagnosis). The fundamental notion involves utilizing a pedagogical knowledge concept graph (KCG) as a mediator to connect disparate domains, allowing the transmission of student cognitive signals from established domains to the zero-shot cold-start domain. Specifically, a naive yet effective graph convolutional network (GCN) with the bottom-layer discarding operation is initially employed over the KCG to learn transferable student cognitive states and domain-specific exercise traits. Moreover, we give three implementations of the general TechCD framework following the typical cognitive diagnosis solutions. Finally, extensive experiments on real-world datasets not only prove that Tech can effectively perform zero-shot diagnosis, but also give some popular applications such as exercise recommendation.",
        "keywords": []
      },
      "file_name": "5cbf9bc26b3d0471cb37c3f4a931990b1260d82d.pdf"
    },
    {
      "success": true,
      "doc_id": "384b3e52849c7be8169c8d2db6691e3f",
      "summary": "In clinical treatment, identifying potential adverse reactions of drugs can help assist doctors in making medication decisions. In response to the problems in previous studies that features are high-dimensional and sparse, independent prediction models need to be constructed for each adverse reaction of drugs, and the prediction accuracy is low, this paper develops an adverse drug reaction prediction model based on knowledge graph embedding and deep learning, which can predict experimental results. Unified prediction of adverse drug reactions covered. Knowledge graph embedding technology can fuse the associated information between drugs and alleviate the shortcomings of high-dimensional sparsity in feature matrices, and the efficient training capabilities of deep learning can improve the prediction accuracy of the model. This article builds an adverse drug reaction knowledge graph based on drug feature data; by analyzing the embedding effect of the knowledge graph under different embedding strategies, the best embedding strategy is selected to obtain sample vectors; and then a convolutional neural network model is constructed to predict adverse reactions. The results show that under the DistMult embedding model and 400-dimensional embedding strategy, the convolutional neural network model has the best prediction effect; the average accuracy, F1 score, recall rate and area under the curve of repeated experiments are better than the methods reported in the literature. The obtained prediction model has good prediction accuracy and stability, and can provide an effective reference for later safe medication guidance.",
      "intriguing_abstract": "In clinical treatment, identifying potential adverse reactions of drugs can help assist doctors in making medication decisions. In response to the problems in previous studies that features are high-dimensional and sparse, independent prediction models need to be constructed for each adverse reaction of drugs, and the prediction accuracy is low, this paper develops an adverse drug reaction prediction model based on knowledge graph embedding and deep learning, which can predict experimental results. Unified prediction of adverse drug reactions covered. Knowledge graph embedding technology can fuse the associated information between drugs and alleviate the shortcomings of high-dimensional sparsity in feature matrices, and the efficient training capabilities of deep learning can improve the prediction accuracy of the model. This article builds an adverse drug reaction knowledge graph based on drug feature data; by analyzing the embedding effect of the knowledge graph under different embedding strategies, the best embedding strategy is selected to obtain sample vectors; and then a convolutional neural network model is constructed to predict adverse reactions. The results show that under the DistMult embedding model and 400-dimensional embedding strategy, the convolutional neural network model has the best prediction effect; the average accuracy, F1 score, recall rate and area under the curve of repeated experiments are better than the methods reported in the literature. The obtained prediction model has good prediction accuracy and stability, and can provide an effective reference for later safe medication guidance.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4383242be5bdfb30ffa84e58cc252acfb58d4878.pdf",
      "citation_key": "li2024sgp",
      "metadata": {
        "title": "Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning",
        "authors": [
          "Yufeng Li",
          "Wenchao Zhao",
          "Bo Dang",
          "Xu Yan",
          "Weimin Wang",
          "Min Gao",
          "Mingxuan Xiao"
        ],
        "published_date": "2024",
        "abstract": "In clinical treatment, identifying potential adverse reactions of drugs can help assist doctors in making medication decisions. In response to the problems in previous studies that features are high-dimensional and sparse, independent prediction models need to be constructed for each adverse reaction of drugs, and the prediction accuracy is low, this paper develops an adverse drug reaction prediction model based on knowledge graph embedding and deep learning, which can predict experimental results. Unified prediction of adverse drug reactions covered. Knowledge graph embedding technology can fuse the associated information between drugs and alleviate the shortcomings of high-dimensional sparsity in feature matrices, and the efficient training capabilities of deep learning can improve the prediction accuracy of the model. This article builds an adverse drug reaction knowledge graph based on drug feature data; by analyzing the embedding effect of the knowledge graph under different embedding strategies, the best embedding strategy is selected to obtain sample vectors; and then a convolutional neural network model is constructed to predict adverse reactions. The results show that under the DistMult embedding model and 400-dimensional embedding strategy, the convolutional neural network model has the best prediction effect; the average accuracy, F1 score, recall rate and area under the curve of repeated experiments are better than the methods reported in the literature. The obtained prediction model has good prediction accuracy and stability, and can provide an effective reference for later safe medication guidance.",
        "file_path": "paper_data/knowledge_graph_embedding/info/4383242be5bdfb30ffa84e58cc252acfb58d4878.pdf",
        "venue": "2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)",
        "citationCount": 17,
        "score": 17.0,
        "summary": "In clinical treatment, identifying potential adverse reactions of drugs can help assist doctors in making medication decisions. In response to the problems in previous studies that features are high-dimensional and sparse, independent prediction models need to be constructed for each adverse reaction of drugs, and the prediction accuracy is low, this paper develops an adverse drug reaction prediction model based on knowledge graph embedding and deep learning, which can predict experimental results. Unified prediction of adverse drug reactions covered. Knowledge graph embedding technology can fuse the associated information between drugs and alleviate the shortcomings of high-dimensional sparsity in feature matrices, and the efficient training capabilities of deep learning can improve the prediction accuracy of the model. This article builds an adverse drug reaction knowledge graph based on drug feature data; by analyzing the embedding effect of the knowledge graph under different embedding strategies, the best embedding strategy is selected to obtain sample vectors; and then a convolutional neural network model is constructed to predict adverse reactions. The results show that under the DistMult embedding model and 400-dimensional embedding strategy, the convolutional neural network model has the best prediction effect; the average accuracy, F1 score, recall rate and area under the curve of repeated experiments are better than the methods reported in the literature. The obtained prediction model has good prediction accuracy and stability, and can provide an effective reference for later safe medication guidance.",
        "keywords": []
      },
      "file_name": "4383242be5bdfb30ffa84e58cc252acfb58d4878.pdf"
    },
    {
      "success": true,
      "doc_id": "42a5c889f18130f889832e6803b97685",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f26d45a806d1f1319f37eb41b8aa87d768a1d656.pdf",
      "citation_key": "xue2023qi7",
      "metadata": {
        "title": "Learning knowledge graph embedding with multi-granularity relational augmentation network",
        "authors": [
          "Zengcan Xue",
          "Zhao Zhang",
          "Hai Liu",
          "Shuoqiu Yang",
          "Shuyun Han"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/f26d45a806d1f1319f37eb41b8aa87d768a1d656.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 33,
        "score": 16.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "f26d45a806d1f1319f37eb41b8aa87d768a1d656.pdf"
    },
    {
      "success": true,
      "doc_id": "e754ba3ff5d951234f819fbf31c643fa",
      "summary": "Abstract Target identification is one of the crucial tasks in drug research and development, as it aids in uncovering the action mechanism of herbs/drugs and discovering new therapeutic targets. Although multiple algorithms of herb target prediction have been proposed, due to the incompleteness of clinical knowledge and the limitation of unsupervised models, accurate identification for herb targets still faces huge challenges of data and models. To address this, we proposed a deep learning-based target prediction framework termed HTINet2, which designed three key modules, namely, traditional Chinese medicine (TCM) and clinical knowledge graph embedding, residual graph representation learning, and supervised target prediction. In the first module, we constructed a large-scale knowledge graph that covers the TCM properties and clinical treatment knowledge of herbs, and designed a component of deep knowledge embedding to learn the deep knowledge embedding of herbs and targets. In the remaining two modules, we designed a residual-like graph convolution network to capture the deep interactions among herbs and targets, and a Bayesian personalized ranking loss to conduct supervised training and target prediction. Finally, we designed comprehensive experiments, of which comparison with baselines indicated the excellent performance of HTINet2 (HR@10 increased by 122.7% and NDCG@10 by 35.7%), ablation experiments illustrated the positive effect of our designed modules of HTINet2, and case study demonstrated the reliability of the predicted targets of Artemisia annua and Coptis chinensis based on the knowledge base, literature, and molecular docking.",
      "intriguing_abstract": "Abstract Target identification is one of the crucial tasks in drug research and development, as it aids in uncovering the action mechanism of herbs/drugs and discovering new therapeutic targets. Although multiple algorithms of herb target prediction have been proposed, due to the incompleteness of clinical knowledge and the limitation of unsupervised models, accurate identification for herb targets still faces huge challenges of data and models. To address this, we proposed a deep learning-based target prediction framework termed HTINet2, which designed three key modules, namely, traditional Chinese medicine (TCM) and clinical knowledge graph embedding, residual graph representation learning, and supervised target prediction. In the first module, we constructed a large-scale knowledge graph that covers the TCM properties and clinical treatment knowledge of herbs, and designed a component of deep knowledge embedding to learn the deep knowledge embedding of herbs and targets. In the remaining two modules, we designed a residual-like graph convolution network to capture the deep interactions among herbs and targets, and a Bayesian personalized ranking loss to conduct supervised training and target prediction. Finally, we designed comprehensive experiments, of which comparison with baselines indicated the excellent performance of HTINet2 (HR@10 increased by 122.7% and NDCG@10 by 35.7%), ablation experiments illustrated the positive effect of our designed modules of HTINet2, and case study demonstrated the reliability of the predicted targets of Artemisia annua and Coptis chinensis based on the knowledge base, literature, and molecular docking.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7b569aecc97f5fe57ce19ca0670a6b1bc62c7f7c.pdf",
      "citation_key": "duan2024d3f",
      "metadata": {
        "title": "HTINet2: herbtarget prediction via knowledge graph embedding and residual-like graph neural network",
        "authors": [
          "Pengbo Duan",
          "Kuo Yang",
          "Xin Su",
          "Shuyue Fan",
          "Xin Dong",
          "Fenghui Zhang",
          "Xianan Li",
          "Xiaoyan Xing",
          "Qiang Zhu",
          "Jian Yu",
          "Xuezhong Zhou"
        ],
        "published_date": "2024",
        "abstract": "Abstract Target identification is one of the crucial tasks in drug research and development, as it aids in uncovering the action mechanism of herbs/drugs and discovering new therapeutic targets. Although multiple algorithms of herb target prediction have been proposed, due to the incompleteness of clinical knowledge and the limitation of unsupervised models, accurate identification for herb targets still faces huge challenges of data and models. To address this, we proposed a deep learning-based target prediction framework termed HTINet2, which designed three key modules, namely, traditional Chinese medicine (TCM) and clinical knowledge graph embedding, residual graph representation learning, and supervised target prediction. In the first module, we constructed a large-scale knowledge graph that covers the TCM properties and clinical treatment knowledge of herbs, and designed a component of deep knowledge embedding to learn the deep knowledge embedding of herbs and targets. In the remaining two modules, we designed a residual-like graph convolution network to capture the deep interactions among herbs and targets, and a Bayesian personalized ranking loss to conduct supervised training and target prediction. Finally, we designed comprehensive experiments, of which comparison with baselines indicated the excellent performance of HTINet2 (HR@10 increased by 122.7% and NDCG@10 by 35.7%), ablation experiments illustrated the positive effect of our designed modules of HTINet2, and case study demonstrated the reliability of the predicted targets of Artemisia annua and Coptis chinensis based on the knowledge base, literature, and molecular docking.",
        "file_path": "paper_data/knowledge_graph_embedding/info/7b569aecc97f5fe57ce19ca0670a6b1bc62c7f7c.pdf",
        "venue": "Briefings Bioinform.",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Abstract Target identification is one of the crucial tasks in drug research and development, as it aids in uncovering the action mechanism of herbs/drugs and discovering new therapeutic targets. Although multiple algorithms of herb target prediction have been proposed, due to the incompleteness of clinical knowledge and the limitation of unsupervised models, accurate identification for herb targets still faces huge challenges of data and models. To address this, we proposed a deep learning-based target prediction framework termed HTINet2, which designed three key modules, namely, traditional Chinese medicine (TCM) and clinical knowledge graph embedding, residual graph representation learning, and supervised target prediction. In the first module, we constructed a large-scale knowledge graph that covers the TCM properties and clinical treatment knowledge of herbs, and designed a component of deep knowledge embedding to learn the deep knowledge embedding of herbs and targets. In the remaining two modules, we designed a residual-like graph convolution network to capture the deep interactions among herbs and targets, and a Bayesian personalized ranking loss to conduct supervised training and target prediction. Finally, we designed comprehensive experiments, of which comparison with baselines indicated the excellent performance of HTINet2 (HR@10 increased by 122.7% and NDCG@10 by 35.7%), ablation experiments illustrated the positive effect of our designed modules of HTINet2, and case study demonstrated the reliability of the predicted targets of Artemisia annua and Coptis chinensis based on the knowledge base, literature, and molecular docking.",
        "keywords": []
      },
      "file_name": "7b569aecc97f5fe57ce19ca0670a6b1bc62c7f7c.pdf"
    },
    {
      "success": true,
      "doc_id": "9c20669ae54f56e575794d403644936b",
      "summary": "Trajectory similarity computation serves as a fundamental functionality of various spatial information applications. Although existing deep learning similarity computation methods offer better efficiency and accuracy than non-learning solutions, they are still immature in trajectory embedding and suffer from poor generality and heavy preprocessing for training. Targeting these limitations, we propose a novel framework named KGTS based on knowledge graph grid embedding, prompt trajectory embedding, and unsupervised contrastive learning for improved trajectory similarity computation. Specifically, we first embed map grids with a GRot embedding method to vigorously grasp the neighbouring relations of grids. Then, a prompt trajectory embedding network incorporates the resulting grid embedding and extracts trajectory structure and point order information. It is trained by unsupervised contrastive learning, which not only alleviates the heavy preprocessing burden but also provides exceptional generality with creatively designed strategies for positive sample generation. The prompt trajectory embedding adopts a customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding. Extensive experiments on two real-world trajectory datasets demonstrate the superior performance of KGTS over state-of-the-art methods.",
      "intriguing_abstract": "Trajectory similarity computation serves as a fundamental functionality of various spatial information applications. Although existing deep learning similarity computation methods offer better efficiency and accuracy than non-learning solutions, they are still immature in trajectory embedding and suffer from poor generality and heavy preprocessing for training. Targeting these limitations, we propose a novel framework named KGTS based on knowledge graph grid embedding, prompt trajectory embedding, and unsupervised contrastive learning for improved trajectory similarity computation. Specifically, we first embed map grids with a GRot embedding method to vigorously grasp the neighbouring relations of grids. Then, a prompt trajectory embedding network incorporates the resulting grid embedding and extracts trajectory structure and point order information. It is trained by unsupervised contrastive learning, which not only alleviates the heavy preprocessing burden but also provides exceptional generality with creatively designed strategies for positive sample generation. The prompt trajectory embedding adopts a customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding. Extensive experiments on two real-world trajectory datasets demonstrate the superior performance of KGTS over state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf",
      "citation_key": "chen20246rm",
      "metadata": {
        "title": "KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding",
        "authors": [
          "Zhen Chen",
          "Dalin Zhang",
          "Shanshan Feng",
          "Kaixuan Chen",
          "Lisi Chen",
          "Peng Han",
          "Shuo Shang"
        ],
        "published_date": "2024",
        "abstract": "Trajectory similarity computation serves as a fundamental functionality of various spatial information applications. Although existing deep learning similarity computation methods offer better efficiency and accuracy than non-learning solutions, they are still immature in trajectory embedding and suffer from poor generality and heavy preprocessing for training. Targeting these limitations, we propose a novel framework named KGTS based on knowledge graph grid embedding, prompt trajectory embedding, and unsupervised contrastive learning for improved trajectory similarity computation. Specifically, we first embed map grids with a GRot embedding method to vigorously grasp the neighbouring relations of grids. Then, a prompt trajectory embedding network incorporates the resulting grid embedding and extracts trajectory structure and point order information. It is trained by unsupervised contrastive learning, which not only alleviates the heavy preprocessing burden but also provides exceptional generality with creatively designed strategies for positive sample generation. The prompt trajectory embedding adopts a customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding. Extensive experiments on two real-world trajectory datasets demonstrate the superior performance of KGTS over state-of-the-art methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Trajectory similarity computation serves as a fundamental functionality of various spatial information applications. Although existing deep learning similarity computation methods offer better efficiency and accuracy than non-learning solutions, they are still immature in trajectory embedding and suffer from poor generality and heavy preprocessing for training. Targeting these limitations, we propose a novel framework named KGTS based on knowledge graph grid embedding, prompt trajectory embedding, and unsupervised contrastive learning for improved trajectory similarity computation. Specifically, we first embed map grids with a GRot embedding method to vigorously grasp the neighbouring relations of grids. Then, a prompt trajectory embedding network incorporates the resulting grid embedding and extracts trajectory structure and point order information. It is trained by unsupervised contrastive learning, which not only alleviates the heavy preprocessing burden but also provides exceptional generality with creatively designed strategies for positive sample generation. The prompt trajectory embedding adopts a customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding. Extensive experiments on two real-world trajectory datasets demonstrate the superior performance of KGTS over state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf"
    },
    {
      "success": true,
      "doc_id": "b999d1f721d85954a5be4767b0847da2",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/e83b693a44ec32ddfb084d13138e8d7ebc85a7c3.pdf",
      "citation_key": "zhu2022o32",
      "metadata": {
        "title": "DFMKE: A dual fusion multi-modal knowledge graph embedding framework for entity alignment",
        "authors": [
          "Jia Zhu",
          "Changqin Huang",
          "P. D. Meo"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/e83b693a44ec32ddfb084d13138e8d7ebc85a7c3.pdf",
        "venue": "Information Fusion",
        "citationCount": 41,
        "score": 13.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "e83b693a44ec32ddfb084d13138e8d7ebc85a7c3.pdf"
    },
    {
      "success": true,
      "doc_id": "52e54a8e57e5cd2c8e81dce9d6bfc7f3",
      "summary": "The orchestration of cloud computing infrastructures is challenging, considering the number, heterogeneity and dynamicity of the involved resources, along with the highly distributed nature of the applications that use them for computation and storage. Evidently, the volume of relevant monitoring data can be significant, and the ability to collect, analyze, and act on this data in real time is critical for the infrastructures efficient use. In this study, we introduce a novel methodology that adeptly manages the diverse, dynamic, and voluminous nature of cloud resources and the applications that they support. We use knowledge graphs to represent computing and storage resources and illustrate the relationships between them and the applications that utilize them. We then train GraphSAGE to acquire vector-based representations of the infrastructures properties, while preserving the structural properties of the graph. These are efficiently provided as input to two unsupervised machine learning algorithms, namely CBLOF and Isolation Forest, for the detection of storage and computing overusage events, where CBLOF demonstrates better performance across all our evaluation metrics. Following the detection of such events, we have also developed appropriate re-optimization mechanisms that ensure the performance of the served applications. Evaluated in a simulated environment, our methods demonstrate a significant advancement in anomaly detection and infrastructure optimization. The results underscore the potential of this closed-loop operation in dynamically adapting to the evolving demands of cloud infrastructures. By integrating data representation and machine learning methods with proactive management strategies, this research contributes substantially to the field of cloud computing, offering a scalable, intelligent solution for modern cloud infrastructures.",
      "intriguing_abstract": "The orchestration of cloud computing infrastructures is challenging, considering the number, heterogeneity and dynamicity of the involved resources, along with the highly distributed nature of the applications that use them for computation and storage. Evidently, the volume of relevant monitoring data can be significant, and the ability to collect, analyze, and act on this data in real time is critical for the infrastructures efficient use. In this study, we introduce a novel methodology that adeptly manages the diverse, dynamic, and voluminous nature of cloud resources and the applications that they support. We use knowledge graphs to represent computing and storage resources and illustrate the relationships between them and the applications that utilize them. We then train GraphSAGE to acquire vector-based representations of the infrastructures properties, while preserving the structural properties of the graph. These are efficiently provided as input to two unsupervised machine learning algorithms, namely CBLOF and Isolation Forest, for the detection of storage and computing overusage events, where CBLOF demonstrates better performance across all our evaluation metrics. Following the detection of such events, we have also developed appropriate re-optimization mechanisms that ensure the performance of the served applications. Evaluated in a simulated environment, our methods demonstrate a significant advancement in anomaly detection and infrastructure optimization. The results underscore the potential of this closed-loop operation in dynamically adapting to the evolving demands of cloud infrastructures. By integrating data representation and machine learning methods with proactive management strategies, this research contributes substantially to the field of cloud computing, offering a scalable, intelligent solution for modern cloud infrastructures.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f284977aa917be0ff15b835b538294b827135d19.pdf",
      "citation_key": "mitropoulou20235t0",
      "metadata": {
        "title": "Anomaly Detection in Cloud Computing using Knowledge Graph Embedding and Machine Learning Mechanisms",
        "authors": [
          "Katerina Mitropoulou",
          "Panagiotis C. Kokkinos",
          "P. Soumplis",
          "Emmanouel A. Varvarigos"
        ],
        "published_date": "2023",
        "abstract": "The orchestration of cloud computing infrastructures is challenging, considering the number, heterogeneity and dynamicity of the involved resources, along with the highly distributed nature of the applications that use them for computation and storage. Evidently, the volume of relevant monitoring data can be significant, and the ability to collect, analyze, and act on this data in real time is critical for the infrastructures efficient use. In this study, we introduce a novel methodology that adeptly manages the diverse, dynamic, and voluminous nature of cloud resources and the applications that they support. We use knowledge graphs to represent computing and storage resources and illustrate the relationships between them and the applications that utilize them. We then train GraphSAGE to acquire vector-based representations of the infrastructures properties, while preserving the structural properties of the graph. These are efficiently provided as input to two unsupervised machine learning algorithms, namely CBLOF and Isolation Forest, for the detection of storage and computing overusage events, where CBLOF demonstrates better performance across all our evaluation metrics. Following the detection of such events, we have also developed appropriate re-optimization mechanisms that ensure the performance of the served applications. Evaluated in a simulated environment, our methods demonstrate a significant advancement in anomaly detection and infrastructure optimization. The results underscore the potential of this closed-loop operation in dynamically adapting to the evolving demands of cloud infrastructures. By integrating data representation and machine learning methods with proactive management strategies, this research contributes substantially to the field of cloud computing, offering a scalable, intelligent solution for modern cloud infrastructures.",
        "file_path": "paper_data/knowledge_graph_embedding/info/f284977aa917be0ff15b835b538294b827135d19.pdf",
        "venue": "Journal of Grid Computing",
        "citationCount": 27,
        "score": 13.5,
        "summary": "The orchestration of cloud computing infrastructures is challenging, considering the number, heterogeneity and dynamicity of the involved resources, along with the highly distributed nature of the applications that use them for computation and storage. Evidently, the volume of relevant monitoring data can be significant, and the ability to collect, analyze, and act on this data in real time is critical for the infrastructures efficient use. In this study, we introduce a novel methodology that adeptly manages the diverse, dynamic, and voluminous nature of cloud resources and the applications that they support. We use knowledge graphs to represent computing and storage resources and illustrate the relationships between them and the applications that utilize them. We then train GraphSAGE to acquire vector-based representations of the infrastructures properties, while preserving the structural properties of the graph. These are efficiently provided as input to two unsupervised machine learning algorithms, namely CBLOF and Isolation Forest, for the detection of storage and computing overusage events, where CBLOF demonstrates better performance across all our evaluation metrics. Following the detection of such events, we have also developed appropriate re-optimization mechanisms that ensure the performance of the served applications. Evaluated in a simulated environment, our methods demonstrate a significant advancement in anomaly detection and infrastructure optimization. The results underscore the potential of this closed-loop operation in dynamically adapting to the evolving demands of cloud infrastructures. By integrating data representation and machine learning methods with proactive management strategies, this research contributes substantially to the field of cloud computing, offering a scalable, intelligent solution for modern cloud infrastructures.",
        "keywords": []
      },
      "file_name": "f284977aa917be0ff15b835b538294b827135d19.pdf"
    },
    {
      "success": true,
      "doc_id": "f31ddd22f22761f4376fca7c5a78d031",
      "summary": "A fundamental task for knowledge graphs (KGs) is knowledge graph completion (KGC). It aims to predict unseen edges by learning representations for all the entities and relations in a KG. A common concern when learning representations on traditional graphs is degree bias. It can affect graph algorithms by learning poor representations for lower-degree nodes, often leading to low performance on such nodes. However, there has been limited research on whether there exists degree bias for embedding-based KGC and how such bias affects the performance of KGC. In this paper, we validate the existence of degree bias in embedding-based KGC and identify the key factor to degree bias. We then introduce a novel data augmentation method, KG-Mixup, to generate synthetic triples to mitigate such bias. Extensive experiments have demonstrated that our method can improve various embedding-based KGC methods and outperform other methods tackling the bias problem on multiple benchmark datasets. 1",
      "intriguing_abstract": "A fundamental task for knowledge graphs (KGs) is knowledge graph completion (KGC). It aims to predict unseen edges by learning representations for all the entities and relations in a KG. A common concern when learning representations on traditional graphs is degree bias. It can affect graph algorithms by learning poor representations for lower-degree nodes, often leading to low performance on such nodes. However, there has been limited research on whether there exists degree bias for embedding-based KGC and how such bias affects the performance of KGC. In this paper, we validate the existence of degree bias in embedding-based KGC and identify the key factor to degree bias. We then introduce a novel data augmentation method, KG-Mixup, to generate synthetic triples to mitigate such bias. Extensive experiments have demonstrated that our method can improve various embedding-based KGC methods and outperform other methods tackling the bias problem on multiple benchmark datasets. 1",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f3fa1ef467c996b30242124a298b5b9d031e9ed5.pdf",
      "citation_key": "shomer2023imo",
      "metadata": {
        "title": "Toward Degree Bias in Embedding-Based Knowledge Graph Completion",
        "authors": [
          "Harry Shomer",
          "Wei Jin",
          "Wentao Wang",
          "Jiliang Tang"
        ],
        "published_date": "2023",
        "abstract": "A fundamental task for knowledge graphs (KGs) is knowledge graph completion (KGC). It aims to predict unseen edges by learning representations for all the entities and relations in a KG. A common concern when learning representations on traditional graphs is degree bias. It can affect graph algorithms by learning poor representations for lower-degree nodes, often leading to low performance on such nodes. However, there has been limited research on whether there exists degree bias for embedding-based KGC and how such bias affects the performance of KGC. In this paper, we validate the existence of degree bias in embedding-based KGC and identify the key factor to degree bias. We then introduce a novel data augmentation method, KG-Mixup, to generate synthetic triples to mitigate such bias. Extensive experiments have demonstrated that our method can improve various embedding-based KGC methods and outperform other methods tackling the bias problem on multiple benchmark datasets. 1",
        "file_path": "paper_data/knowledge_graph_embedding/info/f3fa1ef467c996b30242124a298b5b9d031e9ed5.pdf",
        "venue": "The Web Conference",
        "citationCount": 27,
        "score": 13.5,
        "summary": "A fundamental task for knowledge graphs (KGs) is knowledge graph completion (KGC). It aims to predict unseen edges by learning representations for all the entities and relations in a KG. A common concern when learning representations on traditional graphs is degree bias. It can affect graph algorithms by learning poor representations for lower-degree nodes, often leading to low performance on such nodes. However, there has been limited research on whether there exists degree bias for embedding-based KGC and how such bias affects the performance of KGC. In this paper, we validate the existence of degree bias in embedding-based KGC and identify the key factor to degree bias. We then introduce a novel data augmentation method, KG-Mixup, to generate synthetic triples to mitigate such bias. Extensive experiments have demonstrated that our method can improve various embedding-based KGC methods and outperform other methods tackling the bias problem on multiple benchmark datasets. 1",
        "keywords": []
      },
      "file_name": "f3fa1ef467c996b30242124a298b5b9d031e9ed5.pdf"
    },
    {
      "success": true,
      "doc_id": "48dc1bd46c8b043418392e67d3966cfa",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/61ef322fba87ccfd36c004afc875542a290fe879.pdf",
      "citation_key": "wang202490m",
      "metadata": {
        "title": "TracKGE: Transformer with Relation-pattern Adaptive Contrastive Learning for Knowledge Graph Embedding",
        "authors": [
          "Mingjie Wang",
          "Zijie Li",
          "Jun Wang",
          "Wei Zou",
          "Juxiang Zhou",
          "Jianhou Gan"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/61ef322fba87ccfd36c004afc875542a290fe879.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 13,
        "score": 13.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "61ef322fba87ccfd36c004afc875542a290fe879.pdf"
    },
    {
      "success": true,
      "doc_id": "a6238121e1207a0b25410bac8579c98f",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/5bef4d28d12dd578ce8a971d88d2779ec01c7ec5.pdf",
      "citation_key": "li2024bl5",
      "metadata": {
        "title": "Decoupled semantic graph neural network for knowledge graph embedding",
        "authors": [
          "Zhifei Li",
          "Wei Huang",
          "Xuchao Gong",
          "Xiangyu Luo",
          "Kui Xiao",
          "Honglian Deng",
          "Miao Zhang",
          "Yan Zhang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/5bef4d28d12dd578ce8a971d88d2779ec01c7ec5.pdf",
        "venue": "Neurocomputing",
        "citationCount": 13,
        "score": 13.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "5bef4d28d12dd578ce8a971d88d2779ec01c7ec5.pdf"
    },
    {
      "success": true,
      "doc_id": "81cd92d836f100ca02d06756ef81c363",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c441b2833db8bd96b4ad133679a68f79d464ef59.pdf",
      "citation_key": "li2024y2a",
      "metadata": {
        "title": "KGIE: Knowledge graph convolutional network for recommender system with interactive embedding",
        "authors": [
          "Mingqi Li",
          "Wenming Ma",
          "Zihao Chu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/c441b2833db8bd96b4ad133679a68f79d464ef59.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 13,
        "score": 13.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "c441b2833db8bd96b4ad133679a68f79d464ef59.pdf"
    },
    {
      "success": true,
      "doc_id": "9fdebc933a23b31fb86b99df81456d6e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/edfbe0b62b9f628858d05b64bd830cf9b0a1ab74.pdf",
      "citation_key": "jia2023krv",
      "metadata": {
        "title": "Extrapolation over temporal knowledge graph via hyperbolic embedding",
        "authors": [
          "Yan Jia",
          "Mengqi Lin",
          "Yechen Wang",
          "Jianming Li",
          "Kai Chen",
          "Joanna Siebert",
          "Geordie Z. Zhang",
          "Qingyun Liao"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/edfbe0b62b9f628858d05b64bd830cf9b0a1ab74.pdf",
        "venue": "CAAI Transactions on Intelligence Technology",
        "citationCount": 23,
        "score": 11.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "edfbe0b62b9f628858d05b64bd830cf9b0a1ab74.pdf"
    },
    {
      "success": true,
      "doc_id": "f9ee5842e2a74b8d7809561bf26a97d9",
      "summary": "Representing the structural relations between entities, i.e., knowledge graph embedding, which is a method to learn low-dimensional representations of knowledge, has become an increasingly prevalent research orientation in cognitive and human intelligence. It is significant to study how to interrelate, fuse and embed the knowledge graph data from different domains while considering data not shared. In this paper, we propose a model of cross-domain knowledge graph embedding in federated learning (FedCKE), in which entity/relation embedding between different domains can interact securely in the case that data is not shared. In advance of client model training, we present an inter-domain encrypted entity/relation alignment method using the encrypted sample alignment method in vertical federated learning, which can obtain entity/relation intersections between different domains without revealing any triples structure and additional entities/relations in the respective datasets. On the server, we aggregate the same entity/relation embeddings by the association in conjunction with the parameter-secure aggregation method in horizontal federated learning. Experimental results on three real datasets show that the proposed FedCKE model is able to enhance the embedding of different clients (domains).",
      "intriguing_abstract": "Representing the structural relations between entities, i.e., knowledge graph embedding, which is a method to learn low-dimensional representations of knowledge, has become an increasingly prevalent research orientation in cognitive and human intelligence. It is significant to study how to interrelate, fuse and embed the knowledge graph data from different domains while considering data not shared. In this paper, we propose a model of cross-domain knowledge graph embedding in federated learning (FedCKE), in which entity/relation embedding between different domains can interact securely in the case that data is not shared. In advance of client model training, we present an inter-domain encrypted entity/relation alignment method using the encrypted sample alignment method in vertical federated learning, which can obtain entity/relation intersections between different domains without revealing any triples structure and additional entities/relations in the respective datasets. On the server, we aggregate the same entity/relation embeddings by the association in conjunction with the parameter-secure aggregation method in horizontal federated learning. Experimental results on three real datasets show that the proposed FedCKE model is able to enhance the embedding of different clients (domains).",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/88e700e9fd6c14f3aa4502176a60512ca4020e35.pdf",
      "citation_key": "huang2023grx",
      "metadata": {
        "title": "FedCKE: Cross-Domain Knowledge Graph Embedding in Federated Learning",
        "authors": [
          "Wei Huang",
          "Jia Liu",
          "Tianrui Li",
          "Shenggong Ji",
          "Dexian Wang",
          "Tianqiang Huang"
        ],
        "published_date": "2023",
        "abstract": "Representing the structural relations between entities, i.e., knowledge graph embedding, which is a method to learn low-dimensional representations of knowledge, has become an increasingly prevalent research orientation in cognitive and human intelligence. It is significant to study how to interrelate, fuse and embed the knowledge graph data from different domains while considering data not shared. In this paper, we propose a model of cross-domain knowledge graph embedding in federated learning (FedCKE), in which entity/relation embedding between different domains can interact securely in the case that data is not shared. In advance of client model training, we present an inter-domain encrypted entity/relation alignment method using the encrypted sample alignment method in vertical federated learning, which can obtain entity/relation intersections between different domains without revealing any triples structure and additional entities/relations in the respective datasets. On the server, we aggregate the same entity/relation embeddings by the association in conjunction with the parameter-secure aggregation method in horizontal federated learning. Experimental results on three real datasets show that the proposed FedCKE model is able to enhance the embedding of different clients (domains).",
        "file_path": "paper_data/knowledge_graph_embedding/info/88e700e9fd6c14f3aa4502176a60512ca4020e35.pdf",
        "venue": "IEEE Transactions on Big Data",
        "citationCount": 22,
        "score": 11.0,
        "summary": "Representing the structural relations between entities, i.e., knowledge graph embedding, which is a method to learn low-dimensional representations of knowledge, has become an increasingly prevalent research orientation in cognitive and human intelligence. It is significant to study how to interrelate, fuse and embed the knowledge graph data from different domains while considering data not shared. In this paper, we propose a model of cross-domain knowledge graph embedding in federated learning (FedCKE), in which entity/relation embedding between different domains can interact securely in the case that data is not shared. In advance of client model training, we present an inter-domain encrypted entity/relation alignment method using the encrypted sample alignment method in vertical federated learning, which can obtain entity/relation intersections between different domains without revealing any triples structure and additional entities/relations in the respective datasets. On the server, we aggregate the same entity/relation embeddings by the association in conjunction with the parameter-secure aggregation method in horizontal federated learning. Experimental results on three real datasets show that the proposed FedCKE model is able to enhance the embedding of different clients (domains).",
        "keywords": []
      },
      "file_name": "88e700e9fd6c14f3aa4502176a60512ca4020e35.pdf"
    },
    {
      "success": true,
      "doc_id": "149a00ab3ef406e6bdc8fa90cf333e15",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/942541df1b97a9d1e46059c7c2d11503adc51c4c.pdf",
      "citation_key": "wang2023s70",
      "metadata": {
        "title": "Knowledge graph embedding learning system for defect diagnosis in additive manufacturing",
        "authors": [
          "Ruoxin Wang",
          "C. F. Cheung"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/942541df1b97a9d1e46059c7c2d11503adc51c4c.pdf",
        "venue": "Computers in industry (Print)",
        "citationCount": 21,
        "score": 10.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "942541df1b97a9d1e46059c7c2d11503adc51c4c.pdf"
    },
    {
      "success": true,
      "doc_id": "1fcb8cd01239228cc318b15bda5cca81",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/abc424e17642df01e0e056427250526bc624f762.pdf",
      "citation_key": "hou20237gt",
      "metadata": {
        "title": "T-GAE: A Timespan-aware Graph Attention-based Embedding Model for Temporal Knowledge Graph Completion",
        "authors": [
          "Xiangning Hou",
          "Ruizhe Ma",
          "Li Yan",
          "Z. Ma"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/abc424e17642df01e0e056427250526bc624f762.pdf",
        "venue": "Information Sciences",
        "citationCount": 21,
        "score": 10.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "abc424e17642df01e0e056427250526bc624f762.pdf"
    },
    {
      "success": true,
      "doc_id": "6fca176f4a6e7a80fee4310954a0a0c5",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/825d7339eadadd2baf962f7d3c8fe7dc0cdc9819.pdf",
      "citation_key": "jiang2023opm",
      "metadata": {
        "title": "Multisource hierarchical neural network for knowledge graph embedding",
        "authors": [
          "Dan Jiang",
          "Ronggui Wang",
          "Lixia Xue",
          "Juan Yang"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/825d7339eadadd2baf962f7d3c8fe7dc0cdc9819.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 20,
        "score": 10.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "825d7339eadadd2baf962f7d3c8fe7dc0cdc9819.pdf"
    },
    {
      "success": true,
      "doc_id": "3d96e78d02238149fc83a939298afc20",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b6839f89a59132f0e62011a218ec229a27ffff6b.pdf",
      "citation_key": "lu2022bwo",
      "metadata": {
        "title": "DensE: An enhanced non-commutative representation for knowledge graph embedding with adaptive semantic hierarchy",
        "authors": [
          "H. Lu",
          "Hailin Hu",
          "Xiaodong Lin"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/b6839f89a59132f0e62011a218ec229a27ffff6b.pdf",
        "venue": "Neurocomputing",
        "citationCount": 30,
        "score": 10.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "b6839f89a59132f0e62011a218ec229a27ffff6b.pdf"
    },
    {
      "success": true,
      "doc_id": "5e45cae55fd5210e5bc639df1955ee18",
      "summary": "Background The pharmaceutical field faces a significant challenge in validating drug target interactions (DTIs) due to the time and cost involved, leading to only a fraction being experimentally verified. To expedite drug discovery, accurate computational methods are essential for predicting potential interactions. Recently, machine learning techniques, particularly graph-based methods, have gained prominence. These methods utilize networks of drugs and targets, employing knowledge graph embedding (KGE) to represent structured information from knowledge graphs in a continuous vector space. This phenomenon highlights the growing inclination to utilize graph topologies as a means to improve the precision of predicting DTIs, hence addressing the pressing requirement for effective computational methodologies in the field of drug discovery. Results The present study presents a novel approach called DTIOG for the prediction of DTIs. The methodology employed in this study involves the utilization of a KGE strategy, together with the incorporation of contextual information obtained from protein sequences. More specifically, the study makes use of Protein Bidirectional Encoder Representations from Transformers (ProtBERT) for this purpose. DTIOG utilizes a two-step process to compute embedding vectors using KGE techniques. Additionally, it employs ProtBERT to determine targettarget similarity. Different similarity measures, such as Cosine similarity or Euclidean distance, are utilized in the prediction procedure. In addition to the contextual embedding, the proposed unique approach incorporates local representations obtained from the Simplified Molecular Input Line Entry Specification (SMILES) of drugs and the amino acid sequences of protein targets. Conclusions The effectiveness of the proposed approach was assessed through extensive experimentation on datasets pertaining to Enzymes, Ion Channels, and G-protein-coupled Receptors. The remarkable efficacy of DTIOG was showcased through the utilization of diverse similarity measures in order to calculate the similarities between drugs and targets. The combination of these factors, along with the incorporation of various classifiers, enabled the model to outperform existing algorithms in its ability to predict DTIs. The consistent observation of this advantage across all datasets underlines the robustness and accuracy of DTIOG in the domain of DTIs. Additionally, our case study suggests that the DTIOG can serve as a valuable tool for discovering new DTIs.",
      "intriguing_abstract": "Background The pharmaceutical field faces a significant challenge in validating drug target interactions (DTIs) due to the time and cost involved, leading to only a fraction being experimentally verified. To expedite drug discovery, accurate computational methods are essential for predicting potential interactions. Recently, machine learning techniques, particularly graph-based methods, have gained prominence. These methods utilize networks of drugs and targets, employing knowledge graph embedding (KGE) to represent structured information from knowledge graphs in a continuous vector space. This phenomenon highlights the growing inclination to utilize graph topologies as a means to improve the precision of predicting DTIs, hence addressing the pressing requirement for effective computational methodologies in the field of drug discovery. Results The present study presents a novel approach called DTIOG for the prediction of DTIs. The methodology employed in this study involves the utilization of a KGE strategy, together with the incorporation of contextual information obtained from protein sequences. More specifically, the study makes use of Protein Bidirectional Encoder Representations from Transformers (ProtBERT) for this purpose. DTIOG utilizes a two-step process to compute embedding vectors using KGE techniques. Additionally, it employs ProtBERT to determine targettarget similarity. Different similarity measures, such as Cosine similarity or Euclidean distance, are utilized in the prediction procedure. In addition to the contextual embedding, the proposed unique approach incorporates local representations obtained from the Simplified Molecular Input Line Entry Specification (SMILES) of drugs and the amino acid sequences of protein targets. Conclusions The effectiveness of the proposed approach was assessed through extensive experimentation on datasets pertaining to Enzymes, Ion Channels, and G-protein-coupled Receptors. The remarkable efficacy of DTIOG was showcased through the utilization of diverse similarity measures in order to calculate the similarities between drugs and targets. The combination of these factors, along with the incorporation of various classifiers, enabled the model to outperform existing algorithms in its ability to predict DTIs. The consistent observation of this advantage across all datasets underlines the robustness and accuracy of DTIOG in the domain of DTIs. Additionally, our case study suggests that the DTIOG can serve as a valuable tool for discovering new DTIs.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/59116a07dbdb3cdeebb20085fdfde8b899de8f6a.pdf",
      "citation_key": "djeddi2023g71",
      "metadata": {
        "title": "Advancing drugtarget interaction prediction: a comprehensive graph-based approach integrating knowledge graph embedding and ProtBert pretraining",
        "authors": [
          "W. Djeddi",
          "Khalil Hermi",
          "S. Yahia",
          "Gayo Diallo"
        ],
        "published_date": "2023",
        "abstract": "Background The pharmaceutical field faces a significant challenge in validating drug target interactions (DTIs) due to the time and cost involved, leading to only a fraction being experimentally verified. To expedite drug discovery, accurate computational methods are essential for predicting potential interactions. Recently, machine learning techniques, particularly graph-based methods, have gained prominence. These methods utilize networks of drugs and targets, employing knowledge graph embedding (KGE) to represent structured information from knowledge graphs in a continuous vector space. This phenomenon highlights the growing inclination to utilize graph topologies as a means to improve the precision of predicting DTIs, hence addressing the pressing requirement for effective computational methodologies in the field of drug discovery. Results The present study presents a novel approach called DTIOG for the prediction of DTIs. The methodology employed in this study involves the utilization of a KGE strategy, together with the incorporation of contextual information obtained from protein sequences. More specifically, the study makes use of Protein Bidirectional Encoder Representations from Transformers (ProtBERT) for this purpose. DTIOG utilizes a two-step process to compute embedding vectors using KGE techniques. Additionally, it employs ProtBERT to determine targettarget similarity. Different similarity measures, such as Cosine similarity or Euclidean distance, are utilized in the prediction procedure. In addition to the contextual embedding, the proposed unique approach incorporates local representations obtained from the Simplified Molecular Input Line Entry Specification (SMILES) of drugs and the amino acid sequences of protein targets. Conclusions The effectiveness of the proposed approach was assessed through extensive experimentation on datasets pertaining to Enzymes, Ion Channels, and G-protein-coupled Receptors. The remarkable efficacy of DTIOG was showcased through the utilization of diverse similarity measures in order to calculate the similarities between drugs and targets. The combination of these factors, along with the incorporation of various classifiers, enabled the model to outperform existing algorithms in its ability to predict DTIs. The consistent observation of this advantage across all datasets underlines the robustness and accuracy of DTIOG in the domain of DTIs. Additionally, our case study suggests that the DTIOG can serve as a valuable tool for discovering new DTIs.",
        "file_path": "paper_data/knowledge_graph_embedding/info/59116a07dbdb3cdeebb20085fdfde8b899de8f6a.pdf",
        "venue": "BMC Bioinformatics",
        "citationCount": 19,
        "score": 9.5,
        "summary": "Background The pharmaceutical field faces a significant challenge in validating drug target interactions (DTIs) due to the time and cost involved, leading to only a fraction being experimentally verified. To expedite drug discovery, accurate computational methods are essential for predicting potential interactions. Recently, machine learning techniques, particularly graph-based methods, have gained prominence. These methods utilize networks of drugs and targets, employing knowledge graph embedding (KGE) to represent structured information from knowledge graphs in a continuous vector space. This phenomenon highlights the growing inclination to utilize graph topologies as a means to improve the precision of predicting DTIs, hence addressing the pressing requirement for effective computational methodologies in the field of drug discovery. Results The present study presents a novel approach called DTIOG for the prediction of DTIs. The methodology employed in this study involves the utilization of a KGE strategy, together with the incorporation of contextual information obtained from protein sequences. More specifically, the study makes use of Protein Bidirectional Encoder Representations from Transformers (ProtBERT) for this purpose. DTIOG utilizes a two-step process to compute embedding vectors using KGE techniques. Additionally, it employs ProtBERT to determine targettarget similarity. Different similarity measures, such as Cosine similarity or Euclidean distance, are utilized in the prediction procedure. In addition to the contextual embedding, the proposed unique approach incorporates local representations obtained from the Simplified Molecular Input Line Entry Specification (SMILES) of drugs and the amino acid sequences of protein targets. Conclusions The effectiveness of the proposed approach was assessed through extensive experimentation on datasets pertaining to Enzymes, Ion Channels, and G-protein-coupled Receptors. The remarkable efficacy of DTIOG was showcased through the utilization of diverse similarity measures in order to calculate the similarities between drugs and targets. The combination of these factors, along with the incorporation of various classifiers, enabled the model to outperform existing algorithms in its ability to predict DTIs. The consistent observation of this advantage across all datasets underlines the robustness and accuracy of DTIOG in the domain of DTIs. Additionally, our case study suggests that the DTIOG can serve as a valuable tool for discovering new DTIs.",
        "keywords": []
      },
      "file_name": "59116a07dbdb3cdeebb20085fdfde8b899de8f6a.pdf"
    },
    {
      "success": true,
      "doc_id": "b0bb1ae1d92ed3ac742e05eb43f93254",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/3cab78074e79122fd28cd76f37fd8805e8e4fc31.pdf",
      "citation_key": "zhang20243iw",
      "metadata": {
        "title": "A survey on temporal knowledge graph embedding: Models and applications",
        "authors": [
          "Yuchao Zhang",
          "Xiangjie Kong",
          "Zhehui Shen",
          "Jianxin Li",
          "Qiuhua Yi",
          "Guojiang Shen",
          "Bo Dong"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/3cab78074e79122fd28cd76f37fd8805e8e4fc31.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 9,
        "score": 9.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "3cab78074e79122fd28cd76f37fd8805e8e4fc31.pdf"
    },
    {
      "success": true,
      "doc_id": "952164de99986a00ee02daf79129169f",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/ed21098804490b98899bcb7195084983ce69ed6c.pdf",
      "citation_key": "le2023hjy",
      "metadata": {
        "title": "Knowledge graph embedding with the special orthogonal group in quaternion space for link prediction",
        "authors": [
          "Thanh-Binh Le",
          "Huy Tran",
          "H. Le"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/ed21098804490b98899bcb7195084983ce69ed6c.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 18,
        "score": 9.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "ed21098804490b98899bcb7195084983ce69ed6c.pdf"
    },
    {
      "success": true,
      "doc_id": "8bf00b1fe2d268495fc22ab63c89837a",
      "summary": "Knowledge graph embedding (KGE), which maps entities and relations in a knowledge graph into continuous vector spaces, has achieved great success in predicting missing links in knowledge graphs. However, knowledge graphs often contain incomplete triples that are difficult to inductively infer by KGEs. To address this challenge, we resort to analogical inference and propose a novel and general self-supervised framework AnKGE to enhance KGE models with analogical inference capability. We propose an analogical object retriever that retrieves appropriate analogical objects from entity-level, relation-level, and triple-level. And in AnKGE, we train an analogy function for each level of analogical inference with the original element embedding from a well-trained KGE model as input, which outputs the analogical object embedding. In order to combine inductive inference capability from the original KGE model and analogical inference capability enhanced by AnKGE, we interpolate the analogy score with the base model score and introduce the adaptive weights in the score function for prediction. Through extensive experiments on FB15k-237 and WN18RR datasets, we show that AnKGE achieves competitive results on link prediction task and well performs analogical inference.",
      "intriguing_abstract": "Knowledge graph embedding (KGE), which maps entities and relations in a knowledge graph into continuous vector spaces, has achieved great success in predicting missing links in knowledge graphs. However, knowledge graphs often contain incomplete triples that are difficult to inductively infer by KGEs. To address this challenge, we resort to analogical inference and propose a novel and general self-supervised framework AnKGE to enhance KGE models with analogical inference capability. We propose an analogical object retriever that retrieves appropriate analogical objects from entity-level, relation-level, and triple-level. And in AnKGE, we train an analogy function for each level of analogical inference with the original element embedding from a well-trained KGE model as input, which outputs the analogical object embedding. In order to combine inductive inference capability from the original KGE model and analogical inference capability enhanced by AnKGE, we interpolate the analogy score with the base model score and introduce the adaptive weights in the score function for prediction. Through extensive experiments on FB15k-237 and WN18RR datasets, we show that AnKGE achieves competitive results on link prediction task and well performs analogical inference.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf",
      "citation_key": "yao2023y12",
      "metadata": {
        "title": "Analogical Inference Enhanced Knowledge Graph Embedding",
        "authors": [
          "Zhen Yao",
          "Wen Zhang",
          "Mingyang Chen",
          "Yufen Huang",
          "Yezhou Yang",
          "Hua-zeng Chen"
        ],
        "published_date": "2023",
        "abstract": "Knowledge graph embedding (KGE), which maps entities and relations in a knowledge graph into continuous vector spaces, has achieved great success in predicting missing links in knowledge graphs. However, knowledge graphs often contain incomplete triples that are difficult to inductively infer by KGEs. To address this challenge, we resort to analogical inference and propose a novel and general self-supervised framework AnKGE to enhance KGE models with analogical inference capability. We propose an analogical object retriever that retrieves appropriate analogical objects from entity-level, relation-level, and triple-level. And in AnKGE, we train an analogy function for each level of analogical inference with the original element embedding from a well-trained KGE model as input, which outputs the analogical object embedding. In order to combine inductive inference capability from the original KGE model and analogical inference capability enhanced by AnKGE, we interpolate the analogy score with the base model score and introduce the adaptive weights in the score function for prediction. Through extensive experiments on FB15k-237 and WN18RR datasets, we show that AnKGE achieves competitive results on link prediction task and well performs analogical inference.",
        "file_path": "paper_data/knowledge_graph_embedding/info/354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 14,
        "score": 7.0,
        "summary": "Knowledge graph embedding (KGE), which maps entities and relations in a knowledge graph into continuous vector spaces, has achieved great success in predicting missing links in knowledge graphs. However, knowledge graphs often contain incomplete triples that are difficult to inductively infer by KGEs. To address this challenge, we resort to analogical inference and propose a novel and general self-supervised framework AnKGE to enhance KGE models with analogical inference capability. We propose an analogical object retriever that retrieves appropriate analogical objects from entity-level, relation-level, and triple-level. And in AnKGE, we train an analogy function for each level of analogical inference with the original element embedding from a well-trained KGE model as input, which outputs the analogical object embedding. In order to combine inductive inference capability from the original KGE model and analogical inference capability enhanced by AnKGE, we interpolate the analogy score with the base model score and introduce the adaptive weights in the score function for prediction. Through extensive experiments on FB15k-237 and WN18RR datasets, we show that AnKGE achieves competitive results on link prediction task and well performs analogical inference.",
        "keywords": []
      },
      "file_name": "354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf"
    },
    {
      "success": true,
      "doc_id": "58c43037ea001cf9abc6e11abe0072d2",
      "summary": "Accurate prediction of future events brings great benefits and reduces losses for society in many domains, such as civil unrest, pandemics, and crimes. Knowledge graph is a general language for describing and modeling complex systems. Different types of events continually occur, which are often related to historical and concurrent events. In this paper, we formalize the future event prediction as a temporal knowledge graph reasoning problem. Most existing studies either conduct reasoning on static knowledge graphs or assume knowledges graphs of all timestamps are available during the training process. As a result, they cannot effectively reason over temporal knowledge graphs and predict events happening in the future. To address this problem, some recent works learn to infer future events based on historical event-based temporal knowledge graphs. However, these methods do not comprehensively consider the latent patterns and influences behind historical events and concurrent events simultaneously. This paper proposes a new graph representation learning model, namely Recurrent Event Graph ATtention Network (RE-GAT), based on a novel historical and concurrent events attention-aware mechanism by modeling the event knowledge graph sequence recurrently. More specifically, our RE-GAT uses an attention-based historical events embedding module to encode past events, and employs an attentionbased concurrent events embedding module to model the associations of events at the same timestamp. A translation-based decoder module and a learning objective are developed to optimize the embeddings of entities and relations. We evaluate our proposed method on four benchmark datasets. Extensive experimental results demonstrate the superiority of our RE-GAT model comparing to various baselines, which proves that our method can more accurately predict what events are going to happen.",
      "intriguing_abstract": "Accurate prediction of future events brings great benefits and reduces losses for society in many domains, such as civil unrest, pandemics, and crimes. Knowledge graph is a general language for describing and modeling complex systems. Different types of events continually occur, which are often related to historical and concurrent events. In this paper, we formalize the future event prediction as a temporal knowledge graph reasoning problem. Most existing studies either conduct reasoning on static knowledge graphs or assume knowledges graphs of all timestamps are available during the training process. As a result, they cannot effectively reason over temporal knowledge graphs and predict events happening in the future. To address this problem, some recent works learn to infer future events based on historical event-based temporal knowledge graphs. However, these methods do not comprehensively consider the latent patterns and influences behind historical events and concurrent events simultaneously. This paper proposes a new graph representation learning model, namely Recurrent Event Graph ATtention Network (RE-GAT), based on a novel historical and concurrent events attention-aware mechanism by modeling the event knowledge graph sequence recurrently. More specifically, our RE-GAT uses an attention-based historical events embedding module to encode past events, and employs an attentionbased concurrent events embedding module to model the associations of events at the same timestamp. A translation-based decoder module and a learning objective are developed to optimize the embeddings of entities and relations. We evaluate our proposed method on four benchmark datasets. Extensive experimental results demonstrate the superiority of our RE-GAT model comparing to various baselines, which proves that our method can more accurately predict what events are going to happen.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf",
      "citation_key": "li2023y5q",
      "metadata": {
        "title": "Future Event Prediction Based on Temporal Knowledge Graph Embedding",
        "authors": [
          "Zhipeng Li",
          "Shanshan Feng",
          "Jun Shi",
          "Yang Zhou",
          "Yong Liao",
          "Yangzhao Yang",
          "Yangyang Li",
          "Nenghai Yu",
          "Xun Shao"
        ],
        "published_date": "2023",
        "abstract": "Accurate prediction of future events brings great benefits and reduces losses for society in many domains, such as civil unrest, pandemics, and crimes. Knowledge graph is a general language for describing and modeling complex systems. Different types of events continually occur, which are often related to historical and concurrent events. In this paper, we formalize the future event prediction as a temporal knowledge graph reasoning problem. Most existing studies either conduct reasoning on static knowledge graphs or assume knowledges graphs of all timestamps are available during the training process. As a result, they cannot effectively reason over temporal knowledge graphs and predict events happening in the future. To address this problem, some recent works learn to infer future events based on historical event-based temporal knowledge graphs. However, these methods do not comprehensively consider the latent patterns and influences behind historical events and concurrent events simultaneously. This paper proposes a new graph representation learning model, namely Recurrent Event Graph ATtention Network (RE-GAT), based on a novel historical and concurrent events attention-aware mechanism by modeling the event knowledge graph sequence recurrently. More specifically, our RE-GAT uses an attention-based historical events embedding module to encode past events, and employs an attentionbased concurrent events embedding module to model the associations of events at the same timestamp. A translation-based decoder module and a learning objective are developed to optimize the embeddings of entities and relations. We evaluate our proposed method on four benchmark datasets. Extensive experimental results demonstrate the superiority of our RE-GAT model comparing to various baselines, which proves that our method can more accurately predict what events are going to happen.",
        "file_path": "paper_data/knowledge_graph_embedding/info/c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf",
        "venue": "Computer systems science and engineering",
        "citationCount": 13,
        "score": 6.5,
        "summary": "Accurate prediction of future events brings great benefits and reduces losses for society in many domains, such as civil unrest, pandemics, and crimes. Knowledge graph is a general language for describing and modeling complex systems. Different types of events continually occur, which are often related to historical and concurrent events. In this paper, we formalize the future event prediction as a temporal knowledge graph reasoning problem. Most existing studies either conduct reasoning on static knowledge graphs or assume knowledges graphs of all timestamps are available during the training process. As a result, they cannot effectively reason over temporal knowledge graphs and predict events happening in the future. To address this problem, some recent works learn to infer future events based on historical event-based temporal knowledge graphs. However, these methods do not comprehensively consider the latent patterns and influences behind historical events and concurrent events simultaneously. This paper proposes a new graph representation learning model, namely Recurrent Event Graph ATtention Network (RE-GAT), based on a novel historical and concurrent events attention-aware mechanism by modeling the event knowledge graph sequence recurrently. More specifically, our RE-GAT uses an attention-based historical events embedding module to encode past events, and employs an attentionbased concurrent events embedding module to model the associations of events at the same timestamp. A translation-based decoder module and a learning objective are developed to optimize the embeddings of entities and relations. We evaluate our proposed method on four benchmark datasets. Extensive experimental results demonstrate the superiority of our RE-GAT model comparing to various baselines, which proves that our method can more accurately predict what events are going to happen.",
        "keywords": []
      },
      "file_name": "c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf"
    },
    {
      "success": true,
      "doc_id": "be2aef71c8ffb3e43a1dfcac28b49af5",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/dc949e502e35307753a1acbcdf937f0cd866e63b.pdf",
      "citation_key": "yang2022j7z",
      "metadata": {
        "title": "Approximate inferring with confidence predicting based on uncertain knowledge graph embedding",
        "authors": [
          "Shihan Yang",
          "Weiya Zhang",
          "R. Tang",
          "Mingkai Zhang",
          "Zhensheng Huang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/dc949e502e35307753a1acbcdf937f0cd866e63b.pdf",
        "venue": "Information Sciences",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "dc949e502e35307753a1acbcdf937f0cd866e63b.pdf"
    },
    {
      "success": true,
      "doc_id": "6f716ee7ad349c4aeb6030bbd1582990",
      "summary": "In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.",
      "intriguing_abstract": "In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a64167fcaa7a487575c6479510e57795afc9974e.pdf",
      "citation_key": "banerjee2023fdi",
      "metadata": {
        "title": "GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering",
        "authors": [
          "Debayan Banerjee",
          "Pranav Ajit Nair",
          "Ricardo Usbeck",
          "Chris Biemann"
        ],
        "published_date": "2023",
        "abstract": "In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.",
        "file_path": "paper_data/knowledge_graph_embedding/info/a64167fcaa7a487575c6479510e57795afc9974e.pdf",
        "venue": "Extended Semantic Web Conference",
        "citationCount": 12,
        "score": 6.0,
        "summary": "In this work, we present an end-to-end Knowledge Graph Question Answering (KGQA) system named GETT-QA. GETT-QA uses T5, a popular text-to-text pre-trained language model. The model takes a question in natural language as input and produces a simpler form of the intended SPARQL query. In the simpler form, the model does not directly produce entity and relation IDs. Instead, it produces corresponding entity and relation labels. The labels are grounded to KG entity and relation IDs in a subsequent step. To further improve the results, we instruct the model to produce a truncated version of the KG embedding for each entity. The truncated KG embedding enables a finer search for disambiguation purposes. We find that T5 is able to learn the truncated KG embeddings without any change of loss function, improving KGQA performance. As a result, we report strong results for LC-QuAD 2.0 and SimpleQuestions-Wikidata datasets on end-to-end KGQA over Wikidata.",
        "keywords": []
      },
      "file_name": "a64167fcaa7a487575c6479510e57795afc9974e.pdf"
    },
    {
      "success": true,
      "doc_id": "a89511f2f9c43141bb9b985dbe0c8779",
      "summary": "Knowledge Graph Embedding (KGE) is a fundamental technique that extracts expressive representation from knowledge graph (KG) to facilitate diverse downstream tasks. The emerging federated KGE (FKGE) collaboratively trains from distributed KGs held among clients while avoiding exchanging clients sensitive raw KGs, which can still suffer from privacy threats as evidenced in other federated model trainings (e.g., neural networks). However, quantifying and defending against such privacy threats remain unexplored for FKGE which possesses unique properties not shared by previously studied models. In this paper, we conduct the first holistic study of the privacy threat on FKGE from both attack and defense perspectives. For the attack, we quantify the privacy threat by proposing three new inference attacks, which reveal substantial privacy risk by successfully inferring the existence of the KG triple from victim clients. For the defense, we propose DP-Flames, a novel differentially private FKGE with private selection, which offers a better privacy-utility tradeoff by exploiting the entity-binding sparse gradient property of FKGE and comes with a tight privacy accountant by incorporating the state-of-the-art private selection technique. We further propose an adaptive privacy budget allocation policy to dynamically adjust defense magnitude across the training procedure. Comprehensive evaluations demonstrate that the proposed defense can successfully mitigate the privacy threat by effectively reducing the success rate of inference attacks from to on average with only a modest utility decrease.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) is a fundamental technique that extracts expressive representation from knowledge graph (KG) to facilitate diverse downstream tasks. The emerging federated KGE (FKGE) collaboratively trains from distributed KGs held among clients while avoiding exchanging clients sensitive raw KGs, which can still suffer from privacy threats as evidenced in other federated model trainings (e.g., neural networks). However, quantifying and defending against such privacy threats remain unexplored for FKGE which possesses unique properties not shared by previously studied models. In this paper, we conduct the first holistic study of the privacy threat on FKGE from both attack and defense perspectives. For the attack, we quantify the privacy threat by proposing three new inference attacks, which reveal substantial privacy risk by successfully inferring the existence of the KG triple from victim clients. For the defense, we propose DP-Flames, a novel differentially private FKGE with private selection, which offers a better privacy-utility tradeoff by exploiting the entity-binding sparse gradient property of FKGE and comes with a tight privacy accountant by incorporating the state-of-the-art private selection technique. We further propose an adaptive privacy budget allocation policy to dynamically adjust defense magnitude across the training procedure. Comprehensive evaluations demonstrate that the proposed defense can successfully mitigate the privacy threat by effectively reducing the success rate of inference attacks from to on average with only a modest utility decrease.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f9a575349133b2d4bf512cfb7754fca6d13b0a81.pdf",
      "citation_key": "hu20230kr",
      "metadata": {
        "title": "Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding",
        "authors": [
          "Yuke Hu",
          "Wei Liang",
          "Ruofan Wu",
          "Kai Y. Xiao",
          "Weiqiang Wang",
          "Xiaochen Li",
          "Jinfei Liu",
          "Zhan Qin"
        ],
        "published_date": "2023",
        "abstract": "Knowledge Graph Embedding (KGE) is a fundamental technique that extracts expressive representation from knowledge graph (KG) to facilitate diverse downstream tasks. The emerging federated KGE (FKGE) collaboratively trains from distributed KGs held among clients while avoiding exchanging clients sensitive raw KGs, which can still suffer from privacy threats as evidenced in other federated model trainings (e.g., neural networks). However, quantifying and defending against such privacy threats remain unexplored for FKGE which possesses unique properties not shared by previously studied models. In this paper, we conduct the first holistic study of the privacy threat on FKGE from both attack and defense perspectives. For the attack, we quantify the privacy threat by proposing three new inference attacks, which reveal substantial privacy risk by successfully inferring the existence of the KG triple from victim clients. For the defense, we propose DP-Flames, a novel differentially private FKGE with private selection, which offers a better privacy-utility tradeoff by exploiting the entity-binding sparse gradient property of FKGE and comes with a tight privacy accountant by incorporating the state-of-the-art private selection technique. We further propose an adaptive privacy budget allocation policy to dynamically adjust defense magnitude across the training procedure. Comprehensive evaluations demonstrate that the proposed defense can successfully mitigate the privacy threat by effectively reducing the success rate of inference attacks from to on average with only a modest utility decrease.",
        "file_path": "paper_data/knowledge_graph_embedding/info/f9a575349133b2d4bf512cfb7754fca6d13b0a81.pdf",
        "venue": "The Web Conference",
        "citationCount": 12,
        "score": 6.0,
        "summary": "Knowledge Graph Embedding (KGE) is a fundamental technique that extracts expressive representation from knowledge graph (KG) to facilitate diverse downstream tasks. The emerging federated KGE (FKGE) collaboratively trains from distributed KGs held among clients while avoiding exchanging clients sensitive raw KGs, which can still suffer from privacy threats as evidenced in other federated model trainings (e.g., neural networks). However, quantifying and defending against such privacy threats remain unexplored for FKGE which possesses unique properties not shared by previously studied models. In this paper, we conduct the first holistic study of the privacy threat on FKGE from both attack and defense perspectives. For the attack, we quantify the privacy threat by proposing three new inference attacks, which reveal substantial privacy risk by successfully inferring the existence of the KG triple from victim clients. For the defense, we propose DP-Flames, a novel differentially private FKGE with private selection, which offers a better privacy-utility tradeoff by exploiting the entity-binding sparse gradient property of FKGE and comes with a tight privacy accountant by incorporating the state-of-the-art private selection technique. We further propose an adaptive privacy budget allocation policy to dynamically adjust defense magnitude across the training procedure. Comprehensive evaluations demonstrate that the proposed defense can successfully mitigate the privacy threat by effectively reducing the success rate of inference attacks from to on average with only a modest utility decrease.",
        "keywords": []
      },
      "file_name": "f9a575349133b2d4bf512cfb7754fca6d13b0a81.pdf"
    },
    {
      "success": true,
      "doc_id": "7f4b2221a002220a20a16520cdfea38f",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/5f850f1f522f959e2d3dcad263d05b0fdbb187c3.pdf",
      "citation_key": "li2023wgg",
      "metadata": {
        "title": "EventKGE: Event knowledge graph embedding with event causal transfer",
        "authors": [
          "Daiyi Li",
          "Li Yan",
          "Xiaowen Zhang",
          "Wei Jia",
          "Z. Ma"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/5f850f1f522f959e2d3dcad263d05b0fdbb187c3.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 11,
        "score": 5.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "5f850f1f522f959e2d3dcad263d05b0fdbb187c3.pdf"
    },
    {
      "success": true,
      "doc_id": "7e6ae33aab57473e459994a7d06ae003",
      "summary": "DrugDrug interaction (DDI) prediction is essential in pharmaceutical research and clinical application. Existing computational methods mainly extract data from multiple resources and treat it as binary classification. However, this cannot unambiguously tell the boundary between positive and negative samples owing to the incompleteness and uncertainty of derived data. A granular computing method called three-way decision is proved to be effective in making uncertain decision, but it relies on supplementary information to make delay decision. Recently, biomedical knowledge graph has been regarded as an important source to obtain abundant supplementary information about drugs. This paper proposes a three-way decision-based method called 3WDDI, in combination with knowledge graph embedding as supplementary features to enhance DDI prediction. The drug pairs are divided into positive, negative and boundary regions by Convolutional Neural Network (CNN) according to drug chemical structure feature. Further, delay decision is made for objects in the boundary region by integrating knowledge graph embedding feature to promote the accuracy of decision-making. The empirical results show that 3WDDI yields up to 0.8922, 0.9614, 0.9582, 0.8930 for Accuracy, AUPR, AUC and F1-score, respectively, and outperforms several baseline models.",
      "intriguing_abstract": "DrugDrug interaction (DDI) prediction is essential in pharmaceutical research and clinical application. Existing computational methods mainly extract data from multiple resources and treat it as binary classification. However, this cannot unambiguously tell the boundary between positive and negative samples owing to the incompleteness and uncertainty of derived data. A granular computing method called three-way decision is proved to be effective in making uncertain decision, but it relies on supplementary information to make delay decision. Recently, biomedical knowledge graph has been regarded as an important source to obtain abundant supplementary information about drugs. This paper proposes a three-way decision-based method called 3WDDI, in combination with knowledge graph embedding as supplementary features to enhance DDI prediction. The drug pairs are divided into positive, negative and boundary regions by Convolutional Neural Network (CNN) according to drug chemical structure feature. Further, delay decision is made for objects in the boundary region by integrating knowledge graph embedding feature to promote the accuracy of decision-making. The empirical results show that 3WDDI yields up to 0.8922, 0.9614, 0.9582, 0.8930 for Accuracy, AUPR, AUC and F1-score, respectively, and outperforms several baseline models.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4c68ee32d3db73d4d05803c1b3f2f4b929a88b78.pdf",
      "citation_key": "hao2022cl4",
      "metadata": {
        "title": "Enhancing drugdrug interaction prediction by three-way decision and knowledge graph embedding",
        "authors": [
          "Xinkun Hao",
          "Qingfeng Chen",
          "Haiming Pan",
          "Jie Qiu",
          "Yuxiao Zhang",
          "Qian Yu",
          "Zongzhao Han",
          "Xiaojing Du"
        ],
        "published_date": "2022",
        "abstract": "DrugDrug interaction (DDI) prediction is essential in pharmaceutical research and clinical application. Existing computational methods mainly extract data from multiple resources and treat it as binary classification. However, this cannot unambiguously tell the boundary between positive and negative samples owing to the incompleteness and uncertainty of derived data. A granular computing method called three-way decision is proved to be effective in making uncertain decision, but it relies on supplementary information to make delay decision. Recently, biomedical knowledge graph has been regarded as an important source to obtain abundant supplementary information about drugs. This paper proposes a three-way decision-based method called 3WDDI, in combination with knowledge graph embedding as supplementary features to enhance DDI prediction. The drug pairs are divided into positive, negative and boundary regions by Convolutional Neural Network (CNN) according to drug chemical structure feature. Further, delay decision is made for objects in the boundary region by integrating knowledge graph embedding feature to promote the accuracy of decision-making. The empirical results show that 3WDDI yields up to 0.8922, 0.9614, 0.9582, 0.8930 for Accuracy, AUPR, AUC and F1-score, respectively, and outperforms several baseline models.",
        "file_path": "paper_data/knowledge_graph_embedding/info/4c68ee32d3db73d4d05803c1b3f2f4b929a88b78.pdf",
        "venue": "Granular Computing",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "DrugDrug interaction (DDI) prediction is essential in pharmaceutical research and clinical application. Existing computational methods mainly extract data from multiple resources and treat it as binary classification. However, this cannot unambiguously tell the boundary between positive and negative samples owing to the incompleteness and uncertainty of derived data. A granular computing method called three-way decision is proved to be effective in making uncertain decision, but it relies on supplementary information to make delay decision. Recently, biomedical knowledge graph has been regarded as an important source to obtain abundant supplementary information about drugs. This paper proposes a three-way decision-based method called 3WDDI, in combination with knowledge graph embedding as supplementary features to enhance DDI prediction. The drug pairs are divided into positive, negative and boundary regions by Convolutional Neural Network (CNN) according to drug chemical structure feature. Further, delay decision is made for objects in the boundary region by integrating knowledge graph embedding feature to promote the accuracy of decision-making. The empirical results show that 3WDDI yields up to 0.8922, 0.9614, 0.9582, 0.8930 for Accuracy, AUPR, AUC and F1-score, respectively, and outperforms several baseline models.",
        "keywords": []
      },
      "file_name": "4c68ee32d3db73d4d05803c1b3f2f4b929a88b78.pdf"
    },
    {
      "success": true,
      "doc_id": "6df0473d6f16b49cdb4d0efdfebebca9",
      "summary": "Knowledge graph embedding (KGE) is effectively exploited in providing precise and accurate recommendations from many perspectives in different application scenarios. However, such methods that utilize entire embedded Knowledge Graph (KG) without applying information-relevance regulatory constraints fail to stop the noise penetration into the underlying information. Moreover, higher computational time complexity is a CPU overhead in KG-enhanced systems and applications. The occurrence of these limitations significantly degrade the recommendation performance. Therefore, to cope with these challenges we proposed novel KGEE (Knowledge Graph Embedding Enhancement) approach of Hashing-based Semantic-relevance Attributed Graph-embedding Enhancement (H-SAGE) to model semantically-relevant higher-order entities and relations into the unique Meta-paths. For this purpose, we introduced Node Relevance-based Guided-walk (NRG) modeling technique. Further, to deal with the computational time-complexity, we converted the relevant information to the Hash-codes and proposed Deep-Probabilistic (dProb) technique to place hash-codes in the relevant hash-buckets. Again, we used dProb to generate guided function-calls to maximize the possibility of Hash-Hits in the hash-buckets. In case of Hash-Miss, we applied Locality Sensitive (LS) hashing to retrieve the required information. We performed experiments on three benchmark datasets and compared the empirical as well as the computational performance of H-SAGE with the baseline approaches. The achieved results and comparisons demonstrate that the proposed approach has outperformed the-state-of-the-art methods in the mentioned facets of evaluation.",
      "intriguing_abstract": "Knowledge graph embedding (KGE) is effectively exploited in providing precise and accurate recommendations from many perspectives in different application scenarios. However, such methods that utilize entire embedded Knowledge Graph (KG) without applying information-relevance regulatory constraints fail to stop the noise penetration into the underlying information. Moreover, higher computational time complexity is a CPU overhead in KG-enhanced systems and applications. The occurrence of these limitations significantly degrade the recommendation performance. Therefore, to cope with these challenges we proposed novel KGEE (Knowledge Graph Embedding Enhancement) approach of Hashing-based Semantic-relevance Attributed Graph-embedding Enhancement (H-SAGE) to model semantically-relevant higher-order entities and relations into the unique Meta-paths. For this purpose, we introduced Node Relevance-based Guided-walk (NRG) modeling technique. Further, to deal with the computational time-complexity, we converted the relevant information to the Hash-codes and proposed Deep-Probabilistic (dProb) technique to place hash-codes in the relevant hash-buckets. Again, we used dProb to generate guided function-calls to maximize the possibility of Hash-Hits in the hash-buckets. In case of Hash-Miss, we applied Locality Sensitive (LS) hashing to retrieve the required information. We performed experiments on three benchmark datasets and compared the empirical as well as the computational performance of H-SAGE with the baseline approaches. The achieved results and comparisons demonstrate that the proposed approach has outperformed the-state-of-the-art methods in the mentioned facets of evaluation.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2ac47be80b02a3ff1b87c46cf2b8c27e739c2873.pdf",
      "citation_key": "khan20222j1",
      "metadata": {
        "title": "Hashing-based semantic relevance attributed knowledge graph embedding enhancement for deep probabilistic recommendation",
        "authors": [
          "Nasrullah Khan",
          "Z. Ma",
          "Li Yan",
          "Aman Ullah"
        ],
        "published_date": "2022",
        "abstract": "Knowledge graph embedding (KGE) is effectively exploited in providing precise and accurate recommendations from many perspectives in different application scenarios. However, such methods that utilize entire embedded Knowledge Graph (KG) without applying information-relevance regulatory constraints fail to stop the noise penetration into the underlying information. Moreover, higher computational time complexity is a CPU overhead in KG-enhanced systems and applications. The occurrence of these limitations significantly degrade the recommendation performance. Therefore, to cope with these challenges we proposed novel KGEE (Knowledge Graph Embedding Enhancement) approach of Hashing-based Semantic-relevance Attributed Graph-embedding Enhancement (H-SAGE) to model semantically-relevant higher-order entities and relations into the unique Meta-paths. For this purpose, we introduced Node Relevance-based Guided-walk (NRG) modeling technique. Further, to deal with the computational time-complexity, we converted the relevant information to the Hash-codes and proposed Deep-Probabilistic (dProb) technique to place hash-codes in the relevant hash-buckets. Again, we used dProb to generate guided function-calls to maximize the possibility of Hash-Hits in the hash-buckets. In case of Hash-Miss, we applied Locality Sensitive (LS) hashing to retrieve the required information. We performed experiments on three benchmark datasets and compared the empirical as well as the computational performance of H-SAGE with the baseline approaches. The achieved results and comparisons demonstrate that the proposed approach has outperformed the-state-of-the-art methods in the mentioned facets of evaluation.",
        "file_path": "paper_data/knowledge_graph_embedding/info/2ac47be80b02a3ff1b87c46cf2b8c27e739c2873.pdf",
        "venue": "Applied intelligence (Boston)",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Knowledge graph embedding (KGE) is effectively exploited in providing precise and accurate recommendations from many perspectives in different application scenarios. However, such methods that utilize entire embedded Knowledge Graph (KG) without applying information-relevance regulatory constraints fail to stop the noise penetration into the underlying information. Moreover, higher computational time complexity is a CPU overhead in KG-enhanced systems and applications. The occurrence of these limitations significantly degrade the recommendation performance. Therefore, to cope with these challenges we proposed novel KGEE (Knowledge Graph Embedding Enhancement) approach of Hashing-based Semantic-relevance Attributed Graph-embedding Enhancement (H-SAGE) to model semantically-relevant higher-order entities and relations into the unique Meta-paths. For this purpose, we introduced Node Relevance-based Guided-walk (NRG) modeling technique. Further, to deal with the computational time-complexity, we converted the relevant information to the Hash-codes and proposed Deep-Probabilistic (dProb) technique to place hash-codes in the relevant hash-buckets. Again, we used dProb to generate guided function-calls to maximize the possibility of Hash-Hits in the hash-buckets. In case of Hash-Miss, we applied Locality Sensitive (LS) hashing to retrieve the required information. We performed experiments on three benchmark datasets and compared the empirical as well as the computational performance of H-SAGE with the baseline approaches. The achieved results and comparisons demonstrate that the proposed approach has outperformed the-state-of-the-art methods in the mentioned facets of evaluation.",
        "keywords": []
      },
      "file_name": "2ac47be80b02a3ff1b87c46cf2b8c27e739c2873.pdf"
    },
    {
      "success": true,
      "doc_id": "032bed227d1a7f1e03b77dae6cd73d3d",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b5aedc0464d04aa3fed7e59a141d9be7ee18c217.pdf",
      "citation_key": "le2022ybl",
      "metadata": {
        "title": "Knowledge graph embedding by projection and rotation on hyperplanes for link prediction",
        "authors": [
          "Thanh-Binh Le",
          "Ngoc Huynh",
          "Bac Le"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/b5aedc0464d04aa3fed7e59a141d9be7ee18c217.pdf",
        "venue": "Applied intelligence (Boston)",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "b5aedc0464d04aa3fed7e59a141d9be7ee18c217.pdf"
    },
    {
      "success": true,
      "doc_id": "87dccf794faedd97ed69dfc2cc6642d6",
      "summary": "The representation of semantic information pertaining to the real world has been active research for some time now. Among the available methods, knowledge graphs have emerged as a widely accepted approach. Meanwhile, graph neural networks (GNNs) have demonstrated excellent performance in embedding graph-based information. Given the natural graph structure of knowledge graphs, employing GNNs to embed them is expected to yield a more interpretable and trustworthy representation of the learned knowledge. In this paper, we propose three customized GNNs for different scenarios of knowledge graph representation, including traditional, multimodal, and uncertain knowledge graphs. In the traditional knowledge graph scenario, we present a graph self-supervised learning method, named deep relation graph infomax (DRGI), which incorporates both the complete graph structure information and semantic information. In the multimodal knowledge graph scenario, we introduce a novel network, named hyper-node relational graph attention network (HRGAT), which combines different modal information with graph structure information for a more precise representation of multimodal knowledge graphs. In the uncertain knowledge graph scenario, we define a novel message-passing paradigm with box embedding, named box graph neural network (BGNN). BGNN leverages both the graph structure information of uncertain knowledge graphs and the probabilistic semantics of box embedding. To validate the effectiveness of our proposed methods, we conduct a series of experiments and report the results. We also discuss possible future work in GNN-based knowledge graph embedding.",
      "intriguing_abstract": "The representation of semantic information pertaining to the real world has been active research for some time now. Among the available methods, knowledge graphs have emerged as a widely accepted approach. Meanwhile, graph neural networks (GNNs) have demonstrated excellent performance in embedding graph-based information. Given the natural graph structure of knowledge graphs, employing GNNs to embed them is expected to yield a more interpretable and trustworthy representation of the learned knowledge. In this paper, we propose three customized GNNs for different scenarios of knowledge graph representation, including traditional, multimodal, and uncertain knowledge graphs. In the traditional knowledge graph scenario, we present a graph self-supervised learning method, named deep relation graph infomax (DRGI), which incorporates both the complete graph structure information and semantic information. In the multimodal knowledge graph scenario, we introduce a novel network, named hyper-node relational graph attention network (HRGAT), which combines different modal information with graph structure information for a more precise representation of multimodal knowledge graphs. In the uncertain knowledge graph scenario, we define a novel message-passing paradigm with box embedding, named box graph neural network (BGNN). BGNN leverages both the graph structure information of uncertain knowledge graphs and the probabilistic semantics of box embedding. To validate the effectiveness of our proposed methods, we conduct a series of experiments and report the results. We also discuss possible future work in GNN-based knowledge graph embedding.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/463c7e28be13eb02620ad7e29b562bf6e5014ba2.pdf",
      "citation_key": "liang202338l",
      "metadata": {
        "title": "Knowledge Graph Embedding Based on Graph Neural Network",
        "authors": [
          "Shuang Liang"
        ],
        "published_date": "2023",
        "abstract": "The representation of semantic information pertaining to the real world has been active research for some time now. Among the available methods, knowledge graphs have emerged as a widely accepted approach. Meanwhile, graph neural networks (GNNs) have demonstrated excellent performance in embedding graph-based information. Given the natural graph structure of knowledge graphs, employing GNNs to embed them is expected to yield a more interpretable and trustworthy representation of the learned knowledge. In this paper, we propose three customized GNNs for different scenarios of knowledge graph representation, including traditional, multimodal, and uncertain knowledge graphs. In the traditional knowledge graph scenario, we present a graph self-supervised learning method, named deep relation graph infomax (DRGI), which incorporates both the complete graph structure information and semantic information. In the multimodal knowledge graph scenario, we introduce a novel network, named hyper-node relational graph attention network (HRGAT), which combines different modal information with graph structure information for a more precise representation of multimodal knowledge graphs. In the uncertain knowledge graph scenario, we define a novel message-passing paradigm with box embedding, named box graph neural network (BGNN). BGNN leverages both the graph structure information of uncertain knowledge graphs and the probabilistic semantics of box embedding. To validate the effectiveness of our proposed methods, we conduct a series of experiments and report the results. We also discuss possible future work in GNN-based knowledge graph embedding.",
        "file_path": "paper_data/knowledge_graph_embedding/info/463c7e28be13eb02620ad7e29b562bf6e5014ba2.pdf",
        "venue": "IEEE International Conference on Data Engineering",
        "citationCount": 10,
        "score": 5.0,
        "summary": "The representation of semantic information pertaining to the real world has been active research for some time now. Among the available methods, knowledge graphs have emerged as a widely accepted approach. Meanwhile, graph neural networks (GNNs) have demonstrated excellent performance in embedding graph-based information. Given the natural graph structure of knowledge graphs, employing GNNs to embed them is expected to yield a more interpretable and trustworthy representation of the learned knowledge. In this paper, we propose three customized GNNs for different scenarios of knowledge graph representation, including traditional, multimodal, and uncertain knowledge graphs. In the traditional knowledge graph scenario, we present a graph self-supervised learning method, named deep relation graph infomax (DRGI), which incorporates both the complete graph structure information and semantic information. In the multimodal knowledge graph scenario, we introduce a novel network, named hyper-node relational graph attention network (HRGAT), which combines different modal information with graph structure information for a more precise representation of multimodal knowledge graphs. In the uncertain knowledge graph scenario, we define a novel message-passing paradigm with box embedding, named box graph neural network (BGNN). BGNN leverages both the graph structure information of uncertain knowledge graphs and the probabilistic semantics of box embedding. To validate the effectiveness of our proposed methods, we conduct a series of experiments and report the results. We also discuss possible future work in GNN-based knowledge graph embedding.",
        "keywords": []
      },
      "file_name": "463c7e28be13eb02620ad7e29b562bf6e5014ba2.pdf"
    },
    {
      "success": true,
      "doc_id": "311562b6c33bfd6de5056faf1aa0b907",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7009fd9eb533df6882644a1c8e1019dc034b9cc5.pdf",
      "citation_key": "khan2022ipv",
      "metadata": {
        "title": "DCA-IoMT: Knowledge-Graph-Embedding-Enhanced Deep Collaborative Alert Recommendation Against COVID-19",
        "authors": [
          "Nasrullah Khan",
          "Zongmin Ma",
          "Aman Ullah",
          "K. Polat"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/7009fd9eb533df6882644a1c8e1019dc034b9cc5.pdf",
        "venue": "IEEE Transactions on Industrial Informatics",
        "citationCount": 15,
        "score": 5.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "7009fd9eb533df6882644a1c8e1019dc034b9cc5.pdf"
    },
    {
      "success": true,
      "doc_id": "b8c344b42aae671e6968cc9986a1ff5b",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/e186e5000174ea70729c90d465e60279c5f88646.pdf",
      "citation_key": "he2022e37",
      "metadata": {
        "title": "Improving temporal knowledge graph embedding using tensor factorization",
        "authors": [
          "Peng He",
          "Gang Zhou",
          "Mengli Zhang",
          "Jianghong Wei",
          "Jingxiang Chen"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/e186e5000174ea70729c90d465e60279c5f88646.pdf",
        "venue": "Applied intelligence (Boston)",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "e186e5000174ea70729c90d465e60279c5f88646.pdf"
    },
    {
      "success": true,
      "doc_id": "657d056508a3e75d3f0d3b241fee44b2",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/70dc4c1ec4cda0a7c88751fb9a6b0c648e48e11f.pdf",
      "citation_key": "shen2022d5j",
      "metadata": {
        "title": "Entity alignment with adaptive margin learning knowledge graph embedding",
        "authors": [
          "Linshan Shen",
          "Rongbo He",
          "Shaobin Huang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/70dc4c1ec4cda0a7c88751fb9a6b0c648e48e11f.pdf",
        "venue": "Data & Knowledge Engineering",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "70dc4c1ec4cda0a7c88751fb9a6b0c648e48e11f.pdf"
    },
    {
      "success": true,
      "doc_id": "351ef783c7b1e4b78f6e0e1f05aced80",
      "summary": "The scoring function, which measures the plausibility of triplets in knowledge graphs (KGs), is the key to ensure the excellent performance of KG embedding, and its design is also an important problem in the literature. Automated machine learning (AutoML) techniques have recently been introduced into KG to design task-aware scoring functions, which achieve the state-of-the-art performance in KG embedding. However, the effectiveness of searched scoring functions is still not as good as desired. In this paper, observing that existing scoring functions can exhibit distinct performance on different semantic patterns, we are motivated to explore such semantics by searching relationa-ware scoring functions. But the relation-aware search requires a much larger search space than the previous one. Hence, we propose to encode the space as a supernet and propose an efficient alternative minimization algorithm to search through the supernet in a one-shot manner. Finally, experimental results on benchmark datasets demonstrate that the proposed method can efficiently search relation-aware scoring functions, and achieve better embedding performance than state-of-the-art methods.1",
      "intriguing_abstract": "The scoring function, which measures the plausibility of triplets in knowledge graphs (KGs), is the key to ensure the excellent performance of KG embedding, and its design is also an important problem in the literature. Automated machine learning (AutoML) techniques have recently been introduced into KG to design task-aware scoring functions, which achieve the state-of-the-art performance in KG embedding. However, the effectiveness of searched scoring functions is still not as good as desired. In this paper, observing that existing scoring functions can exhibit distinct performance on different semantic patterns, we are motivated to explore such semantics by searching relationa-ware scoring functions. But the relation-aware search requires a much larger search space than the previous one. Hence, we propose to encode the space as a supernet and propose an efficient alternative minimization algorithm to search through the supernet in a one-shot manner. Finally, experimental results on benchmark datasets demonstrate that the proposed method can efficiently search relation-aware scoring functions, and achieve better embedding performance than state-of-the-art methods.1",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/5a8c6890e524b708dc262d3f456c985e8a46d7d1.pdf",
      "citation_key": "di20210ib",
      "metadata": {
        "title": "Efficient Relation-aware Scoring Function Search for Knowledge Graph Embedding",
        "authors": [
          "Shimin Di",
          "Quanming Yao",
          "Yongqi Zhang",
          "Lei Chen"
        ],
        "published_date": "2021",
        "abstract": "The scoring function, which measures the plausibility of triplets in knowledge graphs (KGs), is the key to ensure the excellent performance of KG embedding, and its design is also an important problem in the literature. Automated machine learning (AutoML) techniques have recently been introduced into KG to design task-aware scoring functions, which achieve the state-of-the-art performance in KG embedding. However, the effectiveness of searched scoring functions is still not as good as desired. In this paper, observing that existing scoring functions can exhibit distinct performance on different semantic patterns, we are motivated to explore such semantics by searching relationa-ware scoring functions. But the relation-aware search requires a much larger search space than the previous one. Hence, we propose to encode the space as a supernet and propose an efficient alternative minimization algorithm to search through the supernet in a one-shot manner. Finally, experimental results on benchmark datasets demonstrate that the proposed method can efficiently search relation-aware scoring functions, and achieve better embedding performance than state-of-the-art methods.1",
        "file_path": "paper_data/knowledge_graph_embedding/info/5a8c6890e524b708dc262d3f456c985e8a46d7d1.pdf",
        "venue": "IEEE International Conference on Data Engineering",
        "citationCount": 16,
        "score": 4.0,
        "summary": "The scoring function, which measures the plausibility of triplets in knowledge graphs (KGs), is the key to ensure the excellent performance of KG embedding, and its design is also an important problem in the literature. Automated machine learning (AutoML) techniques have recently been introduced into KG to design task-aware scoring functions, which achieve the state-of-the-art performance in KG embedding. However, the effectiveness of searched scoring functions is still not as good as desired. In this paper, observing that existing scoring functions can exhibit distinct performance on different semantic patterns, we are motivated to explore such semantics by searching relationa-ware scoring functions. But the relation-aware search requires a much larger search space than the previous one. Hence, we propose to encode the space as a supernet and propose an efficient alternative minimization algorithm to search through the supernet in a one-shot manner. Finally, experimental results on benchmark datasets demonstrate that the proposed method can efficiently search relation-aware scoring functions, and achieve better embedding performance than state-of-the-art methods.1",
        "keywords": []
      },
      "file_name": "5a8c6890e524b708dc262d3f456c985e8a46d7d1.pdf"
    },
    {
      "success": true,
      "doc_id": "96e87f30cd1c34d600746a6aef4387e5",
      "summary": "Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however totally rely on the explicit types or neglect the diverse type representations specific to various relations. Besides, none of the existing methods is capable of inferring all the relation patterns of symmetry, inversion and composition as well as the complex properties of 1-N, N-1 and N-N relations, simultaneously. To explore the type information for any KG, we develop a novel KGE framework with Automated Entity TypE Representation (AutoETER), which learns the latent type embedding of each entity by regarding each relation as a translation operation between the types of two entities with a relation-aware projection mechanism. Particularly, our designed automated type representation learning mechanism is a pluggable module which can be easily incorporated with any KGE model. Besides, our approach could model and infer all the relation patterns and complex relations. Experiments on four datasets demonstrate the superior performance of our model compared to state-of-the-art baselines on link prediction tasks, and the visualization of type clustering provides clearly the explanation of type embeddings and verifies the effectiveness of our model.",
      "intriguing_abstract": "Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however totally rely on the explicit types or neglect the diverse type representations specific to various relations. Besides, none of the existing methods is capable of inferring all the relation patterns of symmetry, inversion and composition as well as the complex properties of 1-N, N-1 and N-N relations, simultaneously. To explore the type information for any KG, we develop a novel KGE framework with Automated Entity TypE Representation (AutoETER), which learns the latent type embedding of each entity by regarding each relation as a translation operation between the types of two entities with a relation-aware projection mechanism. Particularly, our designed automated type representation learning mechanism is a pluggable module which can be easily incorporated with any KGE model. Besides, our approach could model and infer all the relation patterns and complex relations. Experiments on four datasets demonstrate the superior performance of our model compared to state-of-the-art baselines on link prediction tasks, and the visualization of type clustering provides clearly the explanation of type embeddings and verifies the effectiveness of our model.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/86631a005e1a88a66926ac0c364ed0101a02b7e7.pdf",
      "citation_key": "niu2020uyy",
      "metadata": {
        "title": "AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding",
        "authors": [
          "Guanglin Niu",
          "Bo Li",
          "Yongfei Zhang",
          "Shiliang Pu",
          "Jingyang Li"
        ],
        "published_date": "2020",
        "abstract": "Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however totally rely on the explicit types or neglect the diverse type representations specific to various relations. Besides, none of the existing methods is capable of inferring all the relation patterns of symmetry, inversion and composition as well as the complex properties of 1-N, N-1 and N-N relations, simultaneously. To explore the type information for any KG, we develop a novel KGE framework with Automated Entity TypE Representation (AutoETER), which learns the latent type embedding of each entity by regarding each relation as a translation operation between the types of two entities with a relation-aware projection mechanism. Particularly, our designed automated type representation learning mechanism is a pluggable module which can be easily incorporated with any KGE model. Besides, our approach could model and infer all the relation patterns and complex relations. Experiments on four datasets demonstrate the superior performance of our model compared to state-of-the-art baselines on link prediction tasks, and the visualization of type clustering provides clearly the explanation of type embeddings and verifies the effectiveness of our model.",
        "file_path": "paper_data/knowledge_graph_embedding/info/86631a005e1a88a66926ac0c364ed0101a02b7e7.pdf",
        "venue": "Findings",
        "citationCount": 20,
        "score": 4.0,
        "summary": "Recent advances in Knowledge Graph Embedding (KGE) allow for representing entities and relations in continuous vector spaces. Some traditional KGE models leveraging additional type information can improve the representation of entities which however totally rely on the explicit types or neglect the diverse type representations specific to various relations. Besides, none of the existing methods is capable of inferring all the relation patterns of symmetry, inversion and composition as well as the complex properties of 1-N, N-1 and N-N relations, simultaneously. To explore the type information for any KG, we develop a novel KGE framework with Automated Entity TypE Representation (AutoETER), which learns the latent type embedding of each entity by regarding each relation as a translation operation between the types of two entities with a relation-aware projection mechanism. Particularly, our designed automated type representation learning mechanism is a pluggable module which can be easily incorporated with any KGE model. Besides, our approach could model and infer all the relation patterns and complex relations. Experiments on four datasets demonstrate the superior performance of our model compared to state-of-the-art baselines on link prediction tasks, and the visualization of type clustering provides clearly the explanation of type embeddings and verifies the effectiveness of our model.",
        "keywords": []
      },
      "file_name": "86631a005e1a88a66926ac0c364ed0101a02b7e7.pdf"
    },
    {
      "success": true,
      "doc_id": "a6c606d7af0ae010e261150e2e249c9e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/92b9aeabaaac0f20f66c5a68fbb4fc268c5eaae5.pdf",
      "citation_key": "nie2023ejz",
      "metadata": {
        "title": "Correlation embedding learning with dynamic semantic enhanced sampling for knowledge graph completion",
        "authors": [
          "H. Nie",
          "Xiangguo Zhao",
          "Xin Bi",
          "Yuliang Ma",
          "George Y. Yuan"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/92b9aeabaaac0f20f66c5a68fbb4fc268c5eaae5.pdf",
        "venue": "World wide web (Bussum)",
        "citationCount": 8,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "92b9aeabaaac0f20f66c5a68fbb4fc268c5eaae5.pdf"
    },
    {
      "success": true,
      "doc_id": "89373c41909aecf4ebbb71e026973ffd",
      "summary": "The bilinear method is mainstream in Knowledge Graph Embedding (KGE), aiming to learn low-dimensional representations for entities and relations in Knowledge Graph (KG) and complete missing links. Most of the existing works are to find patterns between relationships and effectively model them to accomplish this task. Previous works have mainly discovered 6 important patterns like non-commutativity. Although some bilinear methods succeed in modeling these patterns, they neglect to handle 1-to-N, N-to-1, and N-to-N relations (or complex relations) concurrently, which hurts their expressiveness. To this end, we integrate scaling, the combination of translation and rotation that can solve complex relations and patterns, respectively, where scaling is a simplification of projection. Therefore, we propose a corresponding bilinear model Scaling Translation and Rotation (STaR) consisting of the above two parts. Besides, since translation cannot be incorporated into the bilinear model directly, we introduce translation matrix as the equivalent. Theoretical analysis proves that STaR is capable of modeling all patterns and handling complex relations simultaneously, and experiments demonstrate its effectiveness on commonly used benchmarks for link prediction.",
      "intriguing_abstract": "The bilinear method is mainstream in Knowledge Graph Embedding (KGE), aiming to learn low-dimensional representations for entities and relations in Knowledge Graph (KG) and complete missing links. Most of the existing works are to find patterns between relationships and effectively model them to accomplish this task. Previous works have mainly discovered 6 important patterns like non-commutativity. Although some bilinear methods succeed in modeling these patterns, they neglect to handle 1-to-N, N-to-1, and N-to-N relations (or complex relations) concurrently, which hurts their expressiveness. To this end, we integrate scaling, the combination of translation and rotation that can solve complex relations and patterns, respectively, where scaling is a simplification of projection. Therefore, we propose a corresponding bilinear model Scaling Translation and Rotation (STaR) consisting of the above two parts. Besides, since translation cannot be incorporated into the bilinear model directly, we introduce translation matrix as the equivalent. Theoretical analysis proves that STaR is capable of modeling all patterns and handling complex relations simultaneously, and experiments demonstrate its effectiveness on commonly used benchmarks for link prediction.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/ce494973ceefe5ac011f7e9879843530395fa9db.pdf",
      "citation_key": "li2022du0",
      "metadata": {
        "title": "STaR: Knowledge Graph Embedding by Scaling, Translation and Rotation",
        "authors": [
          "Jiayi Li",
          "Yujiu Yang"
        ],
        "published_date": "2022",
        "abstract": "The bilinear method is mainstream in Knowledge Graph Embedding (KGE), aiming to learn low-dimensional representations for entities and relations in Knowledge Graph (KG) and complete missing links. Most of the existing works are to find patterns between relationships and effectively model them to accomplish this task. Previous works have mainly discovered 6 important patterns like non-commutativity. Although some bilinear methods succeed in modeling these patterns, they neglect to handle 1-to-N, N-to-1, and N-to-N relations (or complex relations) concurrently, which hurts their expressiveness. To this end, we integrate scaling, the combination of translation and rotation that can solve complex relations and patterns, respectively, where scaling is a simplification of projection. Therefore, we propose a corresponding bilinear model Scaling Translation and Rotation (STaR) consisting of the above two parts. Besides, since translation cannot be incorporated into the bilinear model directly, we introduce translation matrix as the equivalent. Theoretical analysis proves that STaR is capable of modeling all patterns and handling complex relations simultaneously, and experiments demonstrate its effectiveness on commonly used benchmarks for link prediction.",
        "file_path": "paper_data/knowledge_graph_embedding/info/ce494973ceefe5ac011f7e9879843530395fa9db.pdf",
        "venue": "Autonomous Infrastructure, Management and Security",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "The bilinear method is mainstream in Knowledge Graph Embedding (KGE), aiming to learn low-dimensional representations for entities and relations in Knowledge Graph (KG) and complete missing links. Most of the existing works are to find patterns between relationships and effectively model them to accomplish this task. Previous works have mainly discovered 6 important patterns like non-commutativity. Although some bilinear methods succeed in modeling these patterns, they neglect to handle 1-to-N, N-to-1, and N-to-N relations (or complex relations) concurrently, which hurts their expressiveness. To this end, we integrate scaling, the combination of translation and rotation that can solve complex relations and patterns, respectively, where scaling is a simplification of projection. Therefore, we propose a corresponding bilinear model Scaling Translation and Rotation (STaR) consisting of the above two parts. Besides, since translation cannot be incorporated into the bilinear model directly, we introduce translation matrix as the equivalent. Theoretical analysis proves that STaR is capable of modeling all patterns and handling complex relations simultaneously, and experiments demonstrate its effectiveness on commonly used benchmarks for link prediction.",
        "keywords": []
      },
      "file_name": "ce494973ceefe5ac011f7e9879843530395fa9db.pdf"
    },
    {
      "success": true,
      "doc_id": "de1f0428e897b6cdc1f4f95bf25438bc",
      "summary": "Learned knowledge graph representations supporting robots contain a wealth of domain knowledge that drives robot behavior. However, there does not exist an inference reconciliation framework that expresses how a knowledge graph representation affects a robot's sequential decision making. We use a pedagogical approach to explain the inferences of a learned, black-box knowledge graph representation, a knowledge graph embedding. Our interpretable model uses a decision tree classifier to locally approximate the predictions of the black-box model and provides natural language explanations interpretable by non-experts. Results from our algorithmic evaluation affirm our model design choices, and the results of our user studies with non-experts support the need for the proposed inference reconciliation framework. Critically, results from our simulated robot evaluation indicate that our explanations enable non-experts to correct erratic robot behaviors due to nonsensical beliefs within the black-box.",
      "intriguing_abstract": "Learned knowledge graph representations supporting robots contain a wealth of domain knowledge that drives robot behavior. However, there does not exist an inference reconciliation framework that expresses how a knowledge graph representation affects a robot's sequential decision making. We use a pedagogical approach to explain the inferences of a learned, black-box knowledge graph representation, a knowledge graph embedding. Our interpretable model uses a decision tree classifier to locally approximate the predictions of the black-box model and provides natural language explanations interpretable by non-experts. Results from our algorithmic evaluation affirm our model design choices, and the results of our user studies with non-experts support the need for the proposed inference reconciliation framework. Critically, results from our simulated robot evaluation indicate that our explanations enable non-experts to correct erratic robot behaviors due to nonsensical beliefs within the black-box.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/25edfb99d3b8377a11433cf7be2bcd9f8bfbdb87.pdf",
      "citation_key": "daruna2022dmk",
      "metadata": {
        "title": "Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions",
        "authors": [
          "A. Daruna",
          "Devleena Das",
          "S. Chernova"
        ],
        "published_date": "2022",
        "abstract": "Learned knowledge graph representations supporting robots contain a wealth of domain knowledge that drives robot behavior. However, there does not exist an inference reconciliation framework that expresses how a knowledge graph representation affects a robot's sequential decision making. We use a pedagogical approach to explain the inferences of a learned, black-box knowledge graph representation, a knowledge graph embedding. Our interpretable model uses a decision tree classifier to locally approximate the predictions of the black-box model and provides natural language explanations interpretable by non-experts. Results from our algorithmic evaluation affirm our model design choices, and the results of our user studies with non-experts support the need for the proposed inference reconciliation framework. Critically, results from our simulated robot evaluation indicate that our explanations enable non-experts to correct erratic robot behaviors due to nonsensical beliefs within the black-box.",
        "file_path": "paper_data/knowledge_graph_embedding/info/25edfb99d3b8377a11433cf7be2bcd9f8bfbdb87.pdf",
        "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Learned knowledge graph representations supporting robots contain a wealth of domain knowledge that drives robot behavior. However, there does not exist an inference reconciliation framework that expresses how a knowledge graph representation affects a robot's sequential decision making. We use a pedagogical approach to explain the inferences of a learned, black-box knowledge graph representation, a knowledge graph embedding. Our interpretable model uses a decision tree classifier to locally approximate the predictions of the black-box model and provides natural language explanations interpretable by non-experts. Results from our algorithmic evaluation affirm our model design choices, and the results of our user studies with non-experts support the need for the proposed inference reconciliation framework. Critically, results from our simulated robot evaluation indicate that our explanations enable non-experts to correct erratic robot behaviors due to nonsensical beliefs within the black-box.",
        "keywords": []
      },
      "file_name": "25edfb99d3b8377a11433cf7be2bcd9f8bfbdb87.pdf"
    },
    {
      "success": true,
      "doc_id": "55642bf2a1a0ac42155cc30d0959bb5b",
      "summary": "Some unseen facts in knowledge graphs (KGs) can be complemented by multi-hop knowledge graph reasoning. The process of multi-hop reasoning can be presented as a serialized decision problem, and then be solved with reinforcement learning (RL). However, existing RL-based reasoning methods cannot deal with the hierarchical information in KGs effectively. To solve this issue, we propose a novel RL-based multi-hop KG reasoning model Path Additional Action-space Ranking (PAAR). Our model first proposes a hyperbolic knowledge graph embedding (KGE) model which can effectively capture hierarchical information in KGs. To introduce hierarchical information into RL, the hyperbolic KGE vector is subsequently added to the state space and helps to expand the action space. In order to alleviate the reward sparsity problem in RL, our model utilizes the score function of hyperbolic KGE model as a soft reward. Finally, the metric of hyperbolic space is added to the training of RL as a penalty strategy to constrain the sufficiency of multi-hop reasoning paths. Experimental results on two real-world datasets show that our proposed model not only provides effective answers but also offers sufficient paths.",
      "intriguing_abstract": "Some unseen facts in knowledge graphs (KGs) can be complemented by multi-hop knowledge graph reasoning. The process of multi-hop reasoning can be presented as a serialized decision problem, and then be solved with reinforcement learning (RL). However, existing RL-based reasoning methods cannot deal with the hierarchical information in KGs effectively. To solve this issue, we propose a novel RL-based multi-hop KG reasoning model Path Additional Action-space Ranking (PAAR). Our model first proposes a hyperbolic knowledge graph embedding (KGE) model which can effectively capture hierarchical information in KGs. To introduce hierarchical information into RL, the hyperbolic KGE vector is subsequently added to the state space and helps to expand the action space. In order to alleviate the reward sparsity problem in RL, our model utilizes the score function of hyperbolic KGE model as a soft reward. Finally, the metric of hyperbolic space is added to the training of RL as a penalty strategy to constrain the sufficiency of multi-hop reasoning paths. Experimental results on two real-world datasets show that our proposed model not only provides effective answers but also offers sufficient paths.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/709a128e752414c973613814ddc2509f2abe092f.pdf",
      "citation_key": "zhou20210ma",
      "metadata": {
        "title": "Multi-hop Knowledge Graph Reasoning Based on Hyperbolic Knowledge Graph Embedding and Reinforcement Learning",
        "authors": [
          "Xing-Chun Zhou",
          "Peng Wang",
          "Qi Luo",
          "Zhe Pan"
        ],
        "published_date": "2021",
        "abstract": "Some unseen facts in knowledge graphs (KGs) can be complemented by multi-hop knowledge graph reasoning. The process of multi-hop reasoning can be presented as a serialized decision problem, and then be solved with reinforcement learning (RL). However, existing RL-based reasoning methods cannot deal with the hierarchical information in KGs effectively. To solve this issue, we propose a novel RL-based multi-hop KG reasoning model Path Additional Action-space Ranking (PAAR). Our model first proposes a hyperbolic knowledge graph embedding (KGE) model which can effectively capture hierarchical information in KGs. To introduce hierarchical information into RL, the hyperbolic KGE vector is subsequently added to the state space and helps to expand the action space. In order to alleviate the reward sparsity problem in RL, our model utilizes the score function of hyperbolic KGE model as a soft reward. Finally, the metric of hyperbolic space is added to the training of RL as a penalty strategy to constrain the sufficiency of multi-hop reasoning paths. Experimental results on two real-world datasets show that our proposed model not only provides effective answers but also offers sufficient paths.",
        "file_path": "paper_data/knowledge_graph_embedding/info/709a128e752414c973613814ddc2509f2abe092f.pdf",
        "venue": "IJCKG",
        "citationCount": 13,
        "score": 3.25,
        "summary": "Some unseen facts in knowledge graphs (KGs) can be complemented by multi-hop knowledge graph reasoning. The process of multi-hop reasoning can be presented as a serialized decision problem, and then be solved with reinforcement learning (RL). However, existing RL-based reasoning methods cannot deal with the hierarchical information in KGs effectively. To solve this issue, we propose a novel RL-based multi-hop KG reasoning model Path Additional Action-space Ranking (PAAR). Our model first proposes a hyperbolic knowledge graph embedding (KGE) model which can effectively capture hierarchical information in KGs. To introduce hierarchical information into RL, the hyperbolic KGE vector is subsequently added to the state space and helps to expand the action space. In order to alleviate the reward sparsity problem in RL, our model utilizes the score function of hyperbolic KGE model as a soft reward. Finally, the metric of hyperbolic space is added to the training of RL as a penalty strategy to constrain the sufficiency of multi-hop reasoning paths. Experimental results on two real-world datasets show that our proposed model not only provides effective answers but also offers sufficient paths.",
        "keywords": []
      },
      "file_name": "709a128e752414c973613814ddc2509f2abe092f.pdf"
    },
    {
      "success": true,
      "doc_id": "87c148e4f6521c3d0a566b486c88e610",
      "summary": "With the further development of knowledge graphs, many weighted knowledge graphs (WKGs) have been published and greatly promote various applications. However, current deterministic knowledge graph embedding algorithms cannot encode weighted knowledge graphs well. This paper gives a promising framework WeExt that can extend deterministic knowledge graph embedding models to enable them to learn weighted knowledge graph embeddings. In addtion, we introduce weighted link prediction to evaluate the weighted knowledge graph embedding models performance on completing WKGs. Finally, we give concrete implementation of WeExt based on two translational distance models and two semantic matching models. Our experimental results show the proposed framework achieves promising performance in link prediction, weight prediction, and weighted link prediction.",
      "intriguing_abstract": "With the further development of knowledge graphs, many weighted knowledge graphs (WKGs) have been published and greatly promote various applications. However, current deterministic knowledge graph embedding algorithms cannot encode weighted knowledge graphs well. This paper gives a promising framework WeExt that can extend deterministic knowledge graph embedding models to enable them to learn weighted knowledge graph embeddings. In addtion, we introduce weighted link prediction to evaluate the weighted knowledge graph embedding models performance on completing WKGs. Finally, we give concrete implementation of WeExt based on two translational distance models and two semantic matching models. Our experimental results show the proposed framework achieves promising performance in link prediction, weight prediction, and weighted link prediction.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/18fd8982051fc1de652a9882c2c52db11bca646b.pdf",
      "citation_key": "kun202384f",
      "metadata": {
        "title": "WeExt: A Framework of Extending Deterministic Knowledge Graph Embedding Models for Embedding Weighted Knowledge Graphs",
        "authors": [
          "Kong Wei Kun",
          "Xin Liu",
          "Teeradaj Racharak",
          "Guanqun Sun",
          "Jianan Chen",
          "Qiang Ma",
          "Le-Minh Nguyen"
        ],
        "published_date": "2023",
        "abstract": "With the further development of knowledge graphs, many weighted knowledge graphs (WKGs) have been published and greatly promote various applications. However, current deterministic knowledge graph embedding algorithms cannot encode weighted knowledge graphs well. This paper gives a promising framework WeExt that can extend deterministic knowledge graph embedding models to enable them to learn weighted knowledge graph embeddings. In addtion, we introduce weighted link prediction to evaluate the weighted knowledge graph embedding models performance on completing WKGs. Finally, we give concrete implementation of WeExt based on two translational distance models and two semantic matching models. Our experimental results show the proposed framework achieves promising performance in link prediction, weight prediction, and weighted link prediction.",
        "file_path": "paper_data/knowledge_graph_embedding/info/18fd8982051fc1de652a9882c2c52db11bca646b.pdf",
        "venue": "IEEE Access",
        "citationCount": 6,
        "score": 3.0,
        "summary": "With the further development of knowledge graphs, many weighted knowledge graphs (WKGs) have been published and greatly promote various applications. However, current deterministic knowledge graph embedding algorithms cannot encode weighted knowledge graphs well. This paper gives a promising framework WeExt that can extend deterministic knowledge graph embedding models to enable them to learn weighted knowledge graph embeddings. In addtion, we introduce weighted link prediction to evaluate the weighted knowledge graph embedding models performance on completing WKGs. Finally, we give concrete implementation of WeExt based on two translational distance models and two semantic matching models. Our experimental results show the proposed framework achieves promising performance in link prediction, weight prediction, and weighted link prediction.",
        "keywords": []
      },
      "file_name": "18fd8982051fc1de652a9882c2c52db11bca646b.pdf"
    },
    {
      "success": true,
      "doc_id": "69aa99742fb0503a021a6d760d2a1f77",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a7f0b4776d3df11cf0d0e72785c3035cc744726c.pdf",
      "citation_key": "dong2022taz",
      "metadata": {
        "title": "RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space",
        "authors": [
          "Yao Dong",
          "Lei Wang",
          "Ji Xiang",
          "Xiaobo Guo",
          "Yuqiang Xie"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/a7f0b4776d3df11cf0d0e72785c3035cc744726c.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 9,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "a7f0b4776d3df11cf0d0e72785c3035cc744726c.pdf"
    },
    {
      "success": true,
      "doc_id": "16005cd1f1652f014ef77c8a43ff79a4",
      "summary": "In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.",
      "intriguing_abstract": "In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/e2783f8aa4c61443760a8754cd6d88165d50b213.pdf",
      "citation_key": "kamigaito20218jz",
      "metadata": {
        "title": "Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding",
        "authors": [
          "Hidetaka Kamigaito",
          "Katsuhiko Hayashi"
        ],
        "published_date": "2021",
        "abstract": "In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.",
        "file_path": "paper_data/knowledge_graph_embedding/info/e2783f8aa4c61443760a8754cd6d88165d50b213.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 12,
        "score": 3.0,
        "summary": "In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.",
        "keywords": []
      },
      "file_name": "e2783f8aa4c61443760a8754cd6d88165d50b213.pdf"
    },
    {
      "success": true,
      "doc_id": "791cb0d8a080a5ef30a6bf72462af304",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/77fedfa533871c6c4218285493f725d5df4e74e5.pdf",
      "citation_key": "krause2022th0",
      "metadata": {
        "title": "Dynamic Knowledge Graph Embeddings via Local Embedding Reconstructions",
        "authors": [
          "Franziska Krause"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/77fedfa533871c6c4218285493f725d5df4e74e5.pdf",
        "venue": "Extended Semantic Web Conference",
        "citationCount": 9,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "77fedfa533871c6c4218285493f725d5df4e74e5.pdf"
    },
    {
      "success": true,
      "doc_id": "5d737f7eec56aafc67a8440ae34044fc",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/695ef4cf57b4fd0c7ec17a6e10dffade51f38179.pdf",
      "citation_key": "zhang20213h6",
      "metadata": {
        "title": "Knowledge graph embedding with shared latent semantic units",
        "authors": [
          "Zhao Zhang",
          "Fuzhen Zhuang",
          "Meng Qu",
          "Zheng-Yu Niu",
          "Hui Xiong",
          "Qing He"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/695ef4cf57b4fd0c7ec17a6e10dffade51f38179.pdf",
        "venue": "Neural Networks",
        "citationCount": 11,
        "score": 2.75,
        "summary": "",
        "keywords": []
      },
      "file_name": "695ef4cf57b4fd0c7ec17a6e10dffade51f38179.pdf"
    },
    {
      "success": true,
      "doc_id": "ad6216895cffc5ab928bd137b8483c5b",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/90d5e74b18d03f733c6086418bfe9b20bb6a0a69.pdf",
      "citation_key": "li2021tm6",
      "metadata": {
        "title": "Rule-based data augmentation for knowledge graph embedding",
        "authors": [
          "Guang-pu Li",
          "Zequn Sun",
          "Lei Qian",
          "Qiang Guo",
          "Wei Hu"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/90d5e74b18d03f733c6086418bfe9b20bb6a0a69.pdf",
        "venue": "AI Open",
        "citationCount": 10,
        "score": 2.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "90d5e74b18d03f733c6086418bfe9b20bb6a0a69.pdf"
    },
    {
      "success": true,
      "doc_id": "48dd5a5148e1703e7e45c2085156c230",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c495b2780accfbb53a932181e3c9fd957d16895d.pdf",
      "citation_key": "wang2020au0",
      "metadata": {
        "title": "Enhancing knowledge graph embedding by composite neighbors for link prediction",
        "authors": [
          "Kai Wang",
          "Yu Liu",
          "Xiujuan Xu",
          "Quan Z. Sheng"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/c495b2780accfbb53a932181e3c9fd957d16895d.pdf",
        "venue": "Computing",
        "citationCount": 12,
        "score": 2.4000000000000004,
        "summary": "",
        "keywords": []
      },
      "file_name": "c495b2780accfbb53a932181e3c9fd957d16895d.pdf"
    },
    {
      "success": true,
      "doc_id": "10f627242565cfb57cab55739a222fa4",
      "summary": "Knowledge graph embedding (KGE) plays an important role in downstream tasks, such as question answering, recommendation system, and entity recognition. Most existing KGE methods focus on modeling static knowledge graphs. However, many knowledge graphs are incremental in reality. Existing KGE methods are time-consuming to update the embedding space incrementally, and have difficulty in keeping the timeliness of the knowledge graph embedding. To address this problem, we propose a novel knowledge graph embedding method by rotating on hyperplane (RotatH), which supports updating the embedding space incrementally and ensures the timeliness and accuracy of knowledge graph embedding. Specifically, our proposed method first employs relation-specific hyperplanes to update the incremental entities into the trained vector space efficiently. Meanwhile, by combining hyperplane and rotation, our method can deal with complex relations, such as many-to-many and symmetry relations, and has high performance in both incremental and static environments. Moreover, our method introduces a mean-based method to constraint the density of incremental entities. We conduct extensive link prediction experiments on two real-world incremental datasets and two benchmark datasets. The experimental results show that our model incrementally updates embedding space efficiently and outperforms static models on benchmarks.",
      "intriguing_abstract": "Knowledge graph embedding (KGE) plays an important role in downstream tasks, such as question answering, recommendation system, and entity recognition. Most existing KGE methods focus on modeling static knowledge graphs. However, many knowledge graphs are incremental in reality. Existing KGE methods are time-consuming to update the embedding space incrementally, and have difficulty in keeping the timeliness of the knowledge graph embedding. To address this problem, we propose a novel knowledge graph embedding method by rotating on hyperplane (RotatH), which supports updating the embedding space incrementally and ensures the timeliness and accuracy of knowledge graph embedding. Specifically, our proposed method first employs relation-specific hyperplanes to update the incremental entities into the trained vector space efficiently. Meanwhile, by combining hyperplane and rotation, our method can deal with complex relations, such as many-to-many and symmetry relations, and has high performance in both incremental and static environments. Moreover, our method introduces a mean-based method to constraint the density of incremental entities. We conduct extensive link prediction experiments on two real-world incremental datasets and two benchmark datasets. The experimental results show that our model incrementally updates embedding space efficiently and outperforms static models on benchmarks.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/85bfec413860c072529ab8399676ab4b072f2e34.pdf",
      "citation_key": "wei20215a7",
      "metadata": {
        "title": "Incremental Update of Knowledge Graph Embedding by Rotating on Hyperplanes",
        "authors": [
          "Yuyang Wei",
          "Wei Chen",
          "Zhixu Li",
          "Lei Zhao"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding (KGE) plays an important role in downstream tasks, such as question answering, recommendation system, and entity recognition. Most existing KGE methods focus on modeling static knowledge graphs. However, many knowledge graphs are incremental in reality. Existing KGE methods are time-consuming to update the embedding space incrementally, and have difficulty in keeping the timeliness of the knowledge graph embedding. To address this problem, we propose a novel knowledge graph embedding method by rotating on hyperplane (RotatH), which supports updating the embedding space incrementally and ensures the timeliness and accuracy of knowledge graph embedding. Specifically, our proposed method first employs relation-specific hyperplanes to update the incremental entities into the trained vector space efficiently. Meanwhile, by combining hyperplane and rotation, our method can deal with complex relations, such as many-to-many and symmetry relations, and has high performance in both incremental and static environments. Moreover, our method introduces a mean-based method to constraint the density of incremental entities. We conduct extensive link prediction experiments on two real-world incremental datasets and two benchmark datasets. The experimental results show that our model incrementally updates embedding space efficiently and outperforms static models on benchmarks.",
        "file_path": "paper_data/knowledge_graph_embedding/info/85bfec413860c072529ab8399676ab4b072f2e34.pdf",
        "venue": "2021 IEEE International Conference on Web Services (ICWS)",
        "citationCount": 9,
        "score": 2.25,
        "summary": "Knowledge graph embedding (KGE) plays an important role in downstream tasks, such as question answering, recommendation system, and entity recognition. Most existing KGE methods focus on modeling static knowledge graphs. However, many knowledge graphs are incremental in reality. Existing KGE methods are time-consuming to update the embedding space incrementally, and have difficulty in keeping the timeliness of the knowledge graph embedding. To address this problem, we propose a novel knowledge graph embedding method by rotating on hyperplane (RotatH), which supports updating the embedding space incrementally and ensures the timeliness and accuracy of knowledge graph embedding. Specifically, our proposed method first employs relation-specific hyperplanes to update the incremental entities into the trained vector space efficiently. Meanwhile, by combining hyperplane and rotation, our method can deal with complex relations, such as many-to-many and symmetry relations, and has high performance in both incremental and static environments. Moreover, our method introduces a mean-based method to constraint the density of incremental entities. We conduct extensive link prediction experiments on two real-world incremental datasets and two benchmark datasets. The experimental results show that our model incrementally updates embedding space efficiently and outperforms static models on benchmarks.",
        "keywords": []
      },
      "file_name": "85bfec413860c072529ab8399676ab4b072f2e34.pdf"
    },
    {
      "success": true,
      "doc_id": "80f8542048b5f44b6161adef31a9c0be",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a89f61021e5382912aaeb3f69a6d8a6265787af4.pdf",
      "citation_key": "zhang2021rjh",
      "metadata": {
        "title": "Simple and automated negative sampling for knowledge graph embedding",
        "authors": [
          "Yongqi Zhang",
          "Quanming Yao",
          "Lei Chen"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/a89f61021e5382912aaeb3f69a6d8a6265787af4.pdf",
        "venue": "The VLDB journal",
        "citationCount": 9,
        "score": 2.25,
        "summary": "",
        "keywords": []
      },
      "file_name": "a89f61021e5382912aaeb3f69a6d8a6265787af4.pdf"
    },
    {
      "success": true,
      "doc_id": "260956f8c345c4b7bdc0e978a0c0c270",
      "summary": "Developing scalable solutions for training Graph Neural Networks (GNNs) for link prediction tasks is challenging due to the inherent data dependencies which entail high computational costs and a huge memory footprint. We propose a new method for scaling training of knowledge graph embedding models for link prediction to address these challenges. Towards this end, we propose the following algorithmic strategies: self-sufficient partitions, constraint-based negative sampling, and edge mini-batch training. The experimental evaluation shows that our scaling solution for GNN-based knowledge graph embedding models achieves a 16x speed up on benchmark datasets while maintaining a comparable model performance to non-distributed methods on standard metrics.",
      "intriguing_abstract": "Developing scalable solutions for training Graph Neural Networks (GNNs) for link prediction tasks is challenging due to the inherent data dependencies which entail high computational costs and a huge memory footprint. We propose a new method for scaling training of knowledge graph embedding models for link prediction to address these challenges. Towards this end, we propose the following algorithmic strategies: self-sufficient partitions, constraint-based negative sampling, and edge mini-batch training. The experimental evaluation shows that our scaling solution for GNN-based knowledge graph embedding models achieves a 16x speed up on benchmark datasets while maintaining a comparable model performance to non-distributed methods on standard metrics.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b3cbbc1f34a20c22853f3dd347fd635b2e414fd5.pdf",
      "citation_key": "sheikh202245c",
      "metadata": {
        "title": "Scaling knowledge graph embedding models for link prediction",
        "authors": [
          "Nasrullah Sheikh",
          "Xiao Qin",
          "B. Reinwald",
          "Chuan Lei"
        ],
        "published_date": "2022",
        "abstract": "Developing scalable solutions for training Graph Neural Networks (GNNs) for link prediction tasks is challenging due to the inherent data dependencies which entail high computational costs and a huge memory footprint. We propose a new method for scaling training of knowledge graph embedding models for link prediction to address these challenges. Towards this end, we propose the following algorithmic strategies: self-sufficient partitions, constraint-based negative sampling, and edge mini-batch training. The experimental evaluation shows that our scaling solution for GNN-based knowledge graph embedding models achieves a 16x speed up on benchmark datasets while maintaining a comparable model performance to non-distributed methods on standard metrics.",
        "file_path": "paper_data/knowledge_graph_embedding/info/b3cbbc1f34a20c22853f3dd347fd635b2e414fd5.pdf",
        "venue": "EuroMLSys@EuroSys",
        "citationCount": 6,
        "score": 2.0,
        "summary": "Developing scalable solutions for training Graph Neural Networks (GNNs) for link prediction tasks is challenging due to the inherent data dependencies which entail high computational costs and a huge memory footprint. We propose a new method for scaling training of knowledge graph embedding models for link prediction to address these challenges. Towards this end, we propose the following algorithmic strategies: self-sufficient partitions, constraint-based negative sampling, and edge mini-batch training. The experimental evaluation shows that our scaling solution for GNN-based knowledge graph embedding models achieves a 16x speed up on benchmark datasets while maintaining a comparable model performance to non-distributed methods on standard metrics.",
        "keywords": []
      },
      "file_name": "b3cbbc1f34a20c22853f3dd347fd635b2e414fd5.pdf"
    },
    {
      "success": true,
      "doc_id": "cdb09ffdac075e0b81398f925e5f4322",
      "summary": "Conventional knowledge graph embedding methods mainly assume that all entities at reasoning stage are available in the original training graph. But in real-world application scenarios, newly emerged entities are always inevitable, which results in the severe problem of out-of-knowledge-graph entities. Existing efforts on this issue mostly either utilize additional resources, e.g., entity descriptions, or simply aggregate in-knowledge-graph neighbors to embed these new entities inductively. However, high-quality additional resources are usually hard to obtain and existing neighbors of new entities may be too sparse to provide enough information for modeling these entities. Meanwhile, they may fail to integrate the rich information of ontological concepts, which provide a general figure of instance entities and usually remain unchanged in knowledge graph. To this end, we propose a novel inductive framework namely CatE to solve the sparsity problem with the enhancement from ontological concepts. Specifically, we first adopt the transformer encoder to model the complex contextual structure of the ontological concepts. Then, we further develop a template refinement strategy for generating the target entity embedding, where the concept embedding is used to form a basic skeleton of the target entity and the individual characteristics of the entity will be enriched by its existing neighbors. Finally, extensive experiments on public datasets demonstrate the effectiveness of our proposed model compared with state-of-the-art baseline methods.",
      "intriguing_abstract": "Conventional knowledge graph embedding methods mainly assume that all entities at reasoning stage are available in the original training graph. But in real-world application scenarios, newly emerged entities are always inevitable, which results in the severe problem of out-of-knowledge-graph entities. Existing efforts on this issue mostly either utilize additional resources, e.g., entity descriptions, or simply aggregate in-knowledge-graph neighbors to embed these new entities inductively. However, high-quality additional resources are usually hard to obtain and existing neighbors of new entities may be too sparse to provide enough information for modeling these entities. Meanwhile, they may fail to integrate the rich information of ontological concepts, which provide a general figure of instance entities and usually remain unchanged in knowledge graph. To this end, we propose a novel inductive framework namely CatE to solve the sparsity problem with the enhancement from ontological concepts. Specifically, we first adopt the transformer encoder to model the complex contextual structure of the ontological concepts. Then, we further develop a template refinement strategy for generating the target entity embedding, where the concept embedding is used to form a basic skeleton of the target entity and the individual characteristics of the entity will be enriched by its existing neighbors. Finally, extensive experiments on public datasets demonstrate the effectiveness of our proposed model compared with state-of-the-art baseline methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/df7265b4652b21bc690497b3967a708d811ddd23.pdf",
      "citation_key": "ren2021muc",
      "metadata": {
        "title": "Ontological Concept Structure Aware Knowledge Transfer for Inductive Knowledge Graph Embedding",
        "authors": [
          "Chao Ren",
          "Le Zhang",
          "Lintao Fang",
          "Tong Xu",
          "Zhefeng Wang",
          "Senchao Yuan",
          "Enhong Chen"
        ],
        "published_date": "2021",
        "abstract": "Conventional knowledge graph embedding methods mainly assume that all entities at reasoning stage are available in the original training graph. But in real-world application scenarios, newly emerged entities are always inevitable, which results in the severe problem of out-of-knowledge-graph entities. Existing efforts on this issue mostly either utilize additional resources, e.g., entity descriptions, or simply aggregate in-knowledge-graph neighbors to embed these new entities inductively. However, high-quality additional resources are usually hard to obtain and existing neighbors of new entities may be too sparse to provide enough information for modeling these entities. Meanwhile, they may fail to integrate the rich information of ontological concepts, which provide a general figure of instance entities and usually remain unchanged in knowledge graph. To this end, we propose a novel inductive framework namely CatE to solve the sparsity problem with the enhancement from ontological concepts. Specifically, we first adopt the transformer encoder to model the complex contextual structure of the ontological concepts. Then, we further develop a template refinement strategy for generating the target entity embedding, where the concept embedding is used to form a basic skeleton of the target entity and the individual characteristics of the entity will be enriched by its existing neighbors. Finally, extensive experiments on public datasets demonstrate the effectiveness of our proposed model compared with state-of-the-art baseline methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/df7265b4652b21bc690497b3967a708d811ddd23.pdf",
        "venue": "IEEE International Joint Conference on Neural Network",
        "citationCount": 8,
        "score": 2.0,
        "summary": "Conventional knowledge graph embedding methods mainly assume that all entities at reasoning stage are available in the original training graph. But in real-world application scenarios, newly emerged entities are always inevitable, which results in the severe problem of out-of-knowledge-graph entities. Existing efforts on this issue mostly either utilize additional resources, e.g., entity descriptions, or simply aggregate in-knowledge-graph neighbors to embed these new entities inductively. However, high-quality additional resources are usually hard to obtain and existing neighbors of new entities may be too sparse to provide enough information for modeling these entities. Meanwhile, they may fail to integrate the rich information of ontological concepts, which provide a general figure of instance entities and usually remain unchanged in knowledge graph. To this end, we propose a novel inductive framework namely CatE to solve the sparsity problem with the enhancement from ontological concepts. Specifically, we first adopt the transformer encoder to model the complex contextual structure of the ontological concepts. Then, we further develop a template refinement strategy for generating the target entity embedding, where the concept embedding is used to form a basic skeleton of the target entity and the individual characteristics of the entity will be enriched by its existing neighbors. Finally, extensive experiments on public datasets demonstrate the effectiveness of our proposed model compared with state-of-the-art baseline methods.",
        "keywords": []
      },
      "file_name": "df7265b4652b21bc690497b3967a708d811ddd23.pdf"
    },
    {
      "success": true,
      "doc_id": "98099afab562603fd727f5f342dfdace",
      "summary": "Convolutional neural networks raised the bar for machine learning and artificial intelligence applications, mainly due to the abundance of data and computations. However, there is not always enough data for training, especially when it comes to historical collections of cultural heritage where the original artworks have been destroyed or damaged over time. Transfer Learning and domain adaptation techniques are possible solutions to tackle the issue of data scarcity. This article presents a new method for domain adaptation based on Knowledge graph embeddings. Knowledge Graph embedding forms a projection of a knowledge graph into a lower-dimensional where entities and relations are represented into continuous vector spaces. Our method incorporates these semantic vector spaces as a key ingredient to guide the domain adaptation process. We combined knowledge graph embeddings with visual embeddings from the images and trained a neural network with the combined embeddings as anchors using an extension of Fishers linear discriminant. We evaluated our approach on two cultural heritage datasets of images containing medieval and renaissance musical instruments. The experimental results showed a significant increase in the baselines and state-of-the-art performance compared with other domain adaptation methods.",
      "intriguing_abstract": "Convolutional neural networks raised the bar for machine learning and artificial intelligence applications, mainly due to the abundance of data and computations. However, there is not always enough data for training, especially when it comes to historical collections of cultural heritage where the original artworks have been destroyed or damaged over time. Transfer Learning and domain adaptation techniques are possible solutions to tackle the issue of data scarcity. This article presents a new method for domain adaptation based on Knowledge graph embeddings. Knowledge Graph embedding forms a projection of a knowledge graph into a lower-dimensional where entities and relations are represented into continuous vector spaces. Our method incorporates these semantic vector spaces as a key ingredient to guide the domain adaptation process. We combined knowledge graph embeddings with visual embeddings from the images and trained a neural network with the combined embeddings as anchors using an extension of Fishers linear discriminant. We evaluated our approach on two cultural heritage datasets of images containing medieval and renaissance musical instruments. The experimental results showed a significant increase in the baselines and state-of-the-art performance compared with other domain adaptation methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f6182d5c14c6047d197f1af842862653a13238f2.pdf",
      "citation_key": "eyharabide2021wx4",
      "metadata": {
        "title": "Knowledge Graph Embedding-Based Domain Adaptation for Musical Instrument Recognition",
        "authors": [
          "Victoria Eyharabide",
          "I. E. I. Bekkouch",
          "Nicolae Drago Constantin"
        ],
        "published_date": "2021",
        "abstract": "Convolutional neural networks raised the bar for machine learning and artificial intelligence applications, mainly due to the abundance of data and computations. However, there is not always enough data for training, especially when it comes to historical collections of cultural heritage where the original artworks have been destroyed or damaged over time. Transfer Learning and domain adaptation techniques are possible solutions to tackle the issue of data scarcity. This article presents a new method for domain adaptation based on Knowledge graph embeddings. Knowledge Graph embedding forms a projection of a knowledge graph into a lower-dimensional where entities and relations are represented into continuous vector spaces. Our method incorporates these semantic vector spaces as a key ingredient to guide the domain adaptation process. We combined knowledge graph embeddings with visual embeddings from the images and trained a neural network with the combined embeddings as anchors using an extension of Fishers linear discriminant. We evaluated our approach on two cultural heritage datasets of images containing medieval and renaissance musical instruments. The experimental results showed a significant increase in the baselines and state-of-the-art performance compared with other domain adaptation methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/f6182d5c14c6047d197f1af842862653a13238f2.pdf",
        "venue": "De Computis",
        "citationCount": 8,
        "score": 2.0,
        "summary": "Convolutional neural networks raised the bar for machine learning and artificial intelligence applications, mainly due to the abundance of data and computations. However, there is not always enough data for training, especially when it comes to historical collections of cultural heritage where the original artworks have been destroyed or damaged over time. Transfer Learning and domain adaptation techniques are possible solutions to tackle the issue of data scarcity. This article presents a new method for domain adaptation based on Knowledge graph embeddings. Knowledge Graph embedding forms a projection of a knowledge graph into a lower-dimensional where entities and relations are represented into continuous vector spaces. Our method incorporates these semantic vector spaces as a key ingredient to guide the domain adaptation process. We combined knowledge graph embeddings with visual embeddings from the images and trained a neural network with the combined embeddings as anchors using an extension of Fishers linear discriminant. We evaluated our approach on two cultural heritage datasets of images containing medieval and renaissance musical instruments. The experimental results showed a significant increase in the baselines and state-of-the-art performance compared with other domain adaptation methods.",
        "keywords": []
      },
      "file_name": "f6182d5c14c6047d197f1af842862653a13238f2.pdf"
    },
    {
      "success": true,
      "doc_id": "e59a32997462f2be1b3f48f895a7cd77",
      "summary": "Knowledge graphs (KGs) have been widely applied in many fields such as recommendation systems and knowledge reasoning. Embedding KGs into a continuous vector space has quickly gained significant attention. However, most traditional KG embedding models assume that all the facts in the existing KGs are completely correct, ignoring that KG construction usually involves automatic mechanisms. These automatic construction processes inevitably generate a lot of noises and conflicts, including low-quality errors (e.g., entity type errors). Moreover, these low-quality noises could greatly influence the quality of rule extraction, which may reduce the efficiency of Rule-Guided Embedding model (RUGE). To address this problem, an efficient method to eliminate those entity type errors in triples is proposed and applied to RUGE. Experimental results demonstrate that the filtering of low-quality noises can greatly improve the accuracy of knowledge representation learning as well as the quality of rules, further illustrating the effectiveness of our method.",
      "intriguing_abstract": "Knowledge graphs (KGs) have been widely applied in many fields such as recommendation systems and knowledge reasoning. Embedding KGs into a continuous vector space has quickly gained significant attention. However, most traditional KG embedding models assume that all the facts in the existing KGs are completely correct, ignoring that KG construction usually involves automatic mechanisms. These automatic construction processes inevitably generate a lot of noises and conflicts, including low-quality errors (e.g., entity type errors). Moreover, these low-quality noises could greatly influence the quality of rule extraction, which may reduce the efficiency of Rule-Guided Embedding model (RUGE). To address this problem, an efficient method to eliminate those entity type errors in triples is proposed and applied to RUGE. Experimental results demonstrate that the filtering of low-quality noises can greatly improve the accuracy of knowledge representation learning as well as the quality of rules, further illustrating the effectiveness of our method.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/082856e9b36fac60b9b9400abffaff0e74552fe1.pdf",
      "citation_key": "hong2020hyg",
      "metadata": {
        "title": "Rule-enhanced Noisy Knowledge Graph Embedding via Low-quality Error Detection",
        "authors": [
          "Y. Hong",
          "Chenyang Bu",
          "Tingting Jiang"
        ],
        "published_date": "2020",
        "abstract": "Knowledge graphs (KGs) have been widely applied in many fields such as recommendation systems and knowledge reasoning. Embedding KGs into a continuous vector space has quickly gained significant attention. However, most traditional KG embedding models assume that all the facts in the existing KGs are completely correct, ignoring that KG construction usually involves automatic mechanisms. These automatic construction processes inevitably generate a lot of noises and conflicts, including low-quality errors (e.g., entity type errors). Moreover, these low-quality noises could greatly influence the quality of rule extraction, which may reduce the efficiency of Rule-Guided Embedding model (RUGE). To address this problem, an efficient method to eliminate those entity type errors in triples is proposed and applied to RUGE. Experimental results demonstrate that the filtering of low-quality noises can greatly improve the accuracy of knowledge representation learning as well as the quality of rules, further illustrating the effectiveness of our method.",
        "file_path": "paper_data/knowledge_graph_embedding/info/082856e9b36fac60b9b9400abffaff0e74552fe1.pdf",
        "venue": "2020 IEEE International Conference on Knowledge Graph (ICKG)",
        "citationCount": 10,
        "score": 2.0,
        "summary": "Knowledge graphs (KGs) have been widely applied in many fields such as recommendation systems and knowledge reasoning. Embedding KGs into a continuous vector space has quickly gained significant attention. However, most traditional KG embedding models assume that all the facts in the existing KGs are completely correct, ignoring that KG construction usually involves automatic mechanisms. These automatic construction processes inevitably generate a lot of noises and conflicts, including low-quality errors (e.g., entity type errors). Moreover, these low-quality noises could greatly influence the quality of rule extraction, which may reduce the efficiency of Rule-Guided Embedding model (RUGE). To address this problem, an efficient method to eliminate those entity type errors in triples is proposed and applied to RUGE. Experimental results demonstrate that the filtering of low-quality noises can greatly improve the accuracy of knowledge representation learning as well as the quality of rules, further illustrating the effectiveness of our method.",
        "keywords": []
      },
      "file_name": "082856e9b36fac60b9b9400abffaff0e74552fe1.pdf"
    },
    {
      "success": true,
      "doc_id": "492c0244f6352cccea05c7f4fa5c4abd",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b25744d3c5d93e49b1906991dc8b5426ea2cf51d.pdf",
      "citation_key": "huang2020sqc",
      "metadata": {
        "title": "CoRelatE: Learning the correlation in multi-fold relations for knowledge graph embedding",
        "authors": [
          "Yan Huang",
          "Haili Sun",
          "Xu Ke",
          "Xu Ke",
          "Lu Songfeng",
          "Wang Tongyang",
          "Xinfang Zhang"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/b25744d3c5d93e49b1906991dc8b5426ea2cf51d.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 10,
        "score": 2.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "b25744d3c5d93e49b1906991dc8b5426ea2cf51d.pdf"
    },
    {
      "success": true,
      "doc_id": "56afc00341cef9f102c926750d29a372",
      "summary": "Knowledge reasoning using knowledge graphs has attracted much attention. However, there is difficulty in integrating various related works to realize complex reasoning with explanation using multiple knowledge graphs. To do this, I propose a reasoning framework which combines multiple knowledge graph embedding techniques with corresponding explainable AI techniques. Experiments using the third knowledge graph reasoning challenge dataset demonstrate the effectiveness of the framework.",
      "intriguing_abstract": "Knowledge reasoning using knowledge graphs has attracted much attention. However, there is difficulty in integrating various related works to realize complex reasoning with explanation using multiple knowledge graphs. To do this, I propose a reasoning framework which combines multiple knowledge graph embedding techniques with corresponding explainable AI techniques. Experiments using the third knowledge graph reasoning challenge dataset demonstrate the effectiveness of the framework.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/18bcad2521cbe8df9d84b1adff1dd57c72c68a9d.pdf",
      "citation_key": "kurokawa2021f4f",
      "metadata": {
        "title": "Explainable Knowledge Reasoning Framework Using Multiple Knowledge Graph Embedding",
        "authors": [
          "M. Kurokawa"
        ],
        "published_date": "2021",
        "abstract": "Knowledge reasoning using knowledge graphs has attracted much attention. However, there is difficulty in integrating various related works to realize complex reasoning with explanation using multiple knowledge graphs. To do this, I propose a reasoning framework which combines multiple knowledge graph embedding techniques with corresponding explainable AI techniques. Experiments using the third knowledge graph reasoning challenge dataset demonstrate the effectiveness of the framework.",
        "file_path": "paper_data/knowledge_graph_embedding/info/18bcad2521cbe8df9d84b1adff1dd57c72c68a9d.pdf",
        "venue": "IJCKG",
        "citationCount": 6,
        "score": 1.5,
        "summary": "Knowledge reasoning using knowledge graphs has attracted much attention. However, there is difficulty in integrating various related works to realize complex reasoning with explanation using multiple knowledge graphs. To do this, I propose a reasoning framework which combines multiple knowledge graph embedding techniques with corresponding explainable AI techniques. Experiments using the third knowledge graph reasoning challenge dataset demonstrate the effectiveness of the framework.",
        "keywords": []
      },
      "file_name": "18bcad2521cbe8df9d84b1adff1dd57c72c68a9d.pdf"
    },
    {
      "success": true,
      "doc_id": "e6ee7dc03d06f2f0a3486c8238418422",
      "summary": "Knowledge graph embedding (KGE) models have become popular means for making discoveries in knowledge graphs (e.g., RDF graphs) in an efficient and scalable manner. The key to success of these models is their ability to learn low-rank vector representations for knowledge graph entities and relations. Despite the rapid development of KGE models, state-of-the-art approaches have mostly focused on new ways to represent embeddings interaction functions (i.e., scoring functions). In this paper, we argue that the choice of other training components such as the loss function, hyperparameters and negative sampling strategies can also have substantial impact on the model efficiency. This area has been rather neglected by previous works so far and our contribution is towards closing this gap by a thorough analysis of possible choices of training loss functions, hyperparameters and negative sampling techniques. We finally investigate the effects of specific choices on the scalability and accuracy of knowledge graph embedding models.",
      "intriguing_abstract": "Knowledge graph embedding (KGE) models have become popular means for making discoveries in knowledge graphs (e.g., RDF graphs) in an efficient and scalable manner. The key to success of these models is their ability to learn low-rank vector representations for knowledge graph entities and relations. Despite the rapid development of KGE models, state-of-the-art approaches have mostly focused on new ways to represent embeddings interaction functions (i.e., scoring functions). In this paper, we argue that the choice of other training components such as the loss function, hyperparameters and negative sampling strategies can also have substantial impact on the model efficiency. This area has been rather neglected by previous works so far and our contribution is towards closing this gap by a thorough analysis of possible choices of training loss functions, hyperparameters and negative sampling techniques. We finally investigate the effects of specific choices on the scalability and accuracy of knowledge graph embedding models.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/bdd6c1a6695e3d201b70f4a913ffc758b74216e7.pdf",
      "citation_key": "mohamed2021dwg",
      "metadata": {
        "title": "On Training Knowledge Graph Embedding Models",
        "authors": [
          "Sameh K. Mohamed",
          "Emir Muoz",
          "V. Novek"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding (KGE) models have become popular means for making discoveries in knowledge graphs (e.g., RDF graphs) in an efficient and scalable manner. The key to success of these models is their ability to learn low-rank vector representations for knowledge graph entities and relations. Despite the rapid development of KGE models, state-of-the-art approaches have mostly focused on new ways to represent embeddings interaction functions (i.e., scoring functions). In this paper, we argue that the choice of other training components such as the loss function, hyperparameters and negative sampling strategies can also have substantial impact on the model efficiency. This area has been rather neglected by previous works so far and our contribution is towards closing this gap by a thorough analysis of possible choices of training loss functions, hyperparameters and negative sampling techniques. We finally investigate the effects of specific choices on the scalability and accuracy of knowledge graph embedding models.",
        "file_path": "paper_data/knowledge_graph_embedding/info/bdd6c1a6695e3d201b70f4a913ffc758b74216e7.pdf",
        "venue": "Inf.",
        "citationCount": 6,
        "score": 1.5,
        "summary": "Knowledge graph embedding (KGE) models have become popular means for making discoveries in knowledge graphs (e.g., RDF graphs) in an efficient and scalable manner. The key to success of these models is their ability to learn low-rank vector representations for knowledge graph entities and relations. Despite the rapid development of KGE models, state-of-the-art approaches have mostly focused on new ways to represent embeddings interaction functions (i.e., scoring functions). In this paper, we argue that the choice of other training components such as the loss function, hyperparameters and negative sampling strategies can also have substantial impact on the model efficiency. This area has been rather neglected by previous works so far and our contribution is towards closing this gap by a thorough analysis of possible choices of training loss functions, hyperparameters and negative sampling techniques. We finally investigate the effects of specific choices on the scalability and accuracy of knowledge graph embedding models.",
        "keywords": []
      },
      "file_name": "bdd6c1a6695e3d201b70f4a913ffc758b74216e7.pdf"
    },
    {
      "success": true,
      "doc_id": "9f7b6828d001d32e3fa80d59f602c269",
      "summary": "Knowledge graph embedding involves learning representations of entities -- the vertices of the graph -- and relations -- the edges of the graph -- such that the resulting representations encode the known factual information represented by the knowledge graph and can be used in the inference of new relations. We show that knowledge graph embedding is naturally expressed in the topological and categorical language of \\textit{cellular sheaves}: a knowledge graph embedding can be described as an approximate global section of an appropriate \\textit{knowledge sheaf} over the graph, with consistency constraints induced by the knowledge graph's schema. This approach provides a generalized framework for reasoning about knowledge graph embedding models and allows for the expression of a wide range of prior constraints on embeddings. Further, the resulting embeddings can be easily adapted for reasoning over composite relations without special training. We implement these ideas to highlight the benefits of the extensions inspired by this new perspective.",
      "intriguing_abstract": "Knowledge graph embedding involves learning representations of entities -- the vertices of the graph -- and relations -- the edges of the graph -- such that the resulting representations encode the known factual information represented by the knowledge graph and can be used in the inference of new relations. We show that knowledge graph embedding is naturally expressed in the topological and categorical language of \\textit{cellular sheaves}: a knowledge graph embedding can be described as an approximate global section of an appropriate \\textit{knowledge sheaf} over the graph, with consistency constraints induced by the knowledge graph's schema. This approach provides a generalized framework for reasoning about knowledge graph embedding models and allows for the expression of a wide range of prior constraints on embeddings. Further, the resulting embeddings can be easily adapted for reasoning over composite relations without special training. We implement these ideas to highlight the benefits of the extensions inspired by this new perspective.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/e93565f447a42b158df27ba75385f5e2fc30dde7.pdf",
      "citation_key": "gebhart2021gtp",
      "metadata": {
        "title": "Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding",
        "authors": [
          "Thomas Gebhart",
          "J. Hansen",
          "Paul Schrater"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph embedding involves learning representations of entities -- the vertices of the graph -- and relations -- the edges of the graph -- such that the resulting representations encode the known factual information represented by the knowledge graph and can be used in the inference of new relations. We show that knowledge graph embedding is naturally expressed in the topological and categorical language of \\textit{cellular sheaves}: a knowledge graph embedding can be described as an approximate global section of an appropriate \\textit{knowledge sheaf} over the graph, with consistency constraints induced by the knowledge graph's schema. This approach provides a generalized framework for reasoning about knowledge graph embedding models and allows for the expression of a wide range of prior constraints on embeddings. Further, the resulting embeddings can be easily adapted for reasoning over composite relations without special training. We implement these ideas to highlight the benefits of the extensions inspired by this new perspective.",
        "file_path": "paper_data/knowledge_graph_embedding/info/e93565f447a42b158df27ba75385f5e2fc30dde7.pdf",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "citationCount": 6,
        "score": 1.5,
        "summary": "Knowledge graph embedding involves learning representations of entities -- the vertices of the graph -- and relations -- the edges of the graph -- such that the resulting representations encode the known factual information represented by the knowledge graph and can be used in the inference of new relations. We show that knowledge graph embedding is naturally expressed in the topological and categorical language of \\textit{cellular sheaves}: a knowledge graph embedding can be described as an approximate global section of an appropriate \\textit{knowledge sheaf} over the graph, with consistency constraints induced by the knowledge graph's schema. This approach provides a generalized framework for reasoning about knowledge graph embedding models and allows for the expression of a wide range of prior constraints on embeddings. Further, the resulting embeddings can be easily adapted for reasoning over composite relations without special training. We implement these ideas to highlight the benefits of the extensions inspired by this new perspective.",
        "keywords": []
      },
      "file_name": "e93565f447a42b158df27ba75385f5e2fc30dde7.pdf"
    },
    {
      "success": true,
      "doc_id": "60cc14bf8dcf8a4c33946ecc0d892aef",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/cf436f34ca6aabe1971c3531d465ecaa3d480d68.pdf",
      "citation_key": "deng2024643",
      "metadata": {
        "title": "Knowledge graph embedding based on dynamic adaptive atrous convolution and attention mechanism for link prediction",
        "authors": [
          "Weibin Deng",
          "Yiteng Zhang",
          "Hong Yu",
          "Hongxing Li"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/cf436f34ca6aabe1971c3531d465ecaa3d480d68.pdf",
        "venue": "Information Processing & Management",
        "citationCount": 19,
        "score": 19.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "cf436f34ca6aabe1971c3531d465ecaa3d480d68.pdf"
    },
    {
      "success": true,
      "doc_id": "f2f0d3a0ef628c74e3053fcdfe7cc9ac",
      "summary": "The diagnosis of mild cognitive impairment (MCI), which is an early stage of Alzheimers disease (AD), has great clinical significance. Medical imaging and gene sequencing technologies have provided sufficient multimodality data for MCI diagnostic studies. However, how to effectively extract the rich representations from multimodality data remains a challenging task. To address this challenging task, we propose a new multimodality multiview graph representations and knowledge embedding (MMGK) framework to diagnose MCI. First, to obtain rich information from multimodality data, we extract multiview feature representations from magnetic resonance imaging (MRI) and genetic data. Afterward, considering the correlations between subjects, all subjects are constructed into a graph based on the different single-view feature representations, respectively. To further enrich the correlations between subjects, demographic data are utilized through knowledge embedding. Finally, to perform MCI diagnosis on multiview graphs, graph convolutional networks (GCNs) are utilized. In addition, to further improve the performance of MCI diagnosis, a two-step ensemble learning method is proposed. The proposed framework is evaluated on 188 subjects from the AD Neuroimaging Initiative (ADNI). Experimental results show that our proposed framework achieves good performance with accuracy reaching 0.888, and outperforms some state-of-the-art (SOTA) methods. In addition, the proposed framework is applied to Parkinsons disease (PD) diagnosis and achieves 0.856 accuracy. Overall, our proposed method has potential for clinical application in MCI diagnosis and other diseases via integrating MRI, genetic data, and demographic data. Our code is available at: https://github.com/miacsu/MMGK.",
      "intriguing_abstract": "The diagnosis of mild cognitive impairment (MCI), which is an early stage of Alzheimers disease (AD), has great clinical significance. Medical imaging and gene sequencing technologies have provided sufficient multimodality data for MCI diagnostic studies. However, how to effectively extract the rich representations from multimodality data remains a challenging task. To address this challenging task, we propose a new multimodality multiview graph representations and knowledge embedding (MMGK) framework to diagnose MCI. First, to obtain rich information from multimodality data, we extract multiview feature representations from magnetic resonance imaging (MRI) and genetic data. Afterward, considering the correlations between subjects, all subjects are constructed into a graph based on the different single-view feature representations, respectively. To further enrich the correlations between subjects, demographic data are utilized through knowledge embedding. Finally, to perform MCI diagnosis on multiview graphs, graph convolutional networks (GCNs) are utilized. In addition, to further improve the performance of MCI diagnosis, a two-step ensemble learning method is proposed. The proposed framework is evaluated on 188 subjects from the AD Neuroimaging Initiative (ADNI). Experimental results show that our proposed framework achieves good performance with accuracy reaching 0.888, and outperforms some state-of-the-art (SOTA) methods. In addition, the proposed framework is applied to Parkinsons disease (PD) diagnosis and achieves 0.856 accuracy. Overall, our proposed method has potential for clinical application in MCI diagnosis and other diseases via integrating MRI, genetic data, and demographic data. Our code is available at: https://github.com/miacsu/MMGK.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/76016197d7d4f2213a4ace29988c93285793e154.pdf",
      "citation_key": "liu2024zr9",
      "metadata": {
        "title": "MMGK: Multimodality Multiview Graph Representations and Knowledge Embedding for Mild Cognitive Impairment Diagnosis",
        "authors": [
          "Jin Liu",
          "Hao Du",
          "R. Guo",
          "H. Bai",
          "Hulin Kuang",
          "Jianxin Wang"
        ],
        "published_date": "2024",
        "abstract": "The diagnosis of mild cognitive impairment (MCI), which is an early stage of Alzheimers disease (AD), has great clinical significance. Medical imaging and gene sequencing technologies have provided sufficient multimodality data for MCI diagnostic studies. However, how to effectively extract the rich representations from multimodality data remains a challenging task. To address this challenging task, we propose a new multimodality multiview graph representations and knowledge embedding (MMGK) framework to diagnose MCI. First, to obtain rich information from multimodality data, we extract multiview feature representations from magnetic resonance imaging (MRI) and genetic data. Afterward, considering the correlations between subjects, all subjects are constructed into a graph based on the different single-view feature representations, respectively. To further enrich the correlations between subjects, demographic data are utilized through knowledge embedding. Finally, to perform MCI diagnosis on multiview graphs, graph convolutional networks (GCNs) are utilized. In addition, to further improve the performance of MCI diagnosis, a two-step ensemble learning method is proposed. The proposed framework is evaluated on 188 subjects from the AD Neuroimaging Initiative (ADNI). Experimental results show that our proposed framework achieves good performance with accuracy reaching 0.888, and outperforms some state-of-the-art (SOTA) methods. In addition, the proposed framework is applied to Parkinsons disease (PD) diagnosis and achieves 0.856 accuracy. Overall, our proposed method has potential for clinical application in MCI diagnosis and other diseases via integrating MRI, genetic data, and demographic data. Our code is available at: https://github.com/miacsu/MMGK.",
        "file_path": "paper_data/knowledge_graph_embedding/info/76016197d7d4f2213a4ace29988c93285793e154.pdf",
        "venue": "IEEE Transactions on Computational Social Systems",
        "citationCount": 16,
        "score": 16.0,
        "summary": "The diagnosis of mild cognitive impairment (MCI), which is an early stage of Alzheimers disease (AD), has great clinical significance. Medical imaging and gene sequencing technologies have provided sufficient multimodality data for MCI diagnostic studies. However, how to effectively extract the rich representations from multimodality data remains a challenging task. To address this challenging task, we propose a new multimodality multiview graph representations and knowledge embedding (MMGK) framework to diagnose MCI. First, to obtain rich information from multimodality data, we extract multiview feature representations from magnetic resonance imaging (MRI) and genetic data. Afterward, considering the correlations between subjects, all subjects are constructed into a graph based on the different single-view feature representations, respectively. To further enrich the correlations between subjects, demographic data are utilized through knowledge embedding. Finally, to perform MCI diagnosis on multiview graphs, graph convolutional networks (GCNs) are utilized. In addition, to further improve the performance of MCI diagnosis, a two-step ensemble learning method is proposed. The proposed framework is evaluated on 188 subjects from the AD Neuroimaging Initiative (ADNI). Experimental results show that our proposed framework achieves good performance with accuracy reaching 0.888, and outperforms some state-of-the-art (SOTA) methods. In addition, the proposed framework is applied to Parkinsons disease (PD) diagnosis and achieves 0.856 accuracy. Overall, our proposed method has potential for clinical application in MCI diagnosis and other diseases via integrating MRI, genetic data, and demographic data. Our code is available at: https://github.com/miacsu/MMGK.",
        "keywords": []
      },
      "file_name": "76016197d7d4f2213a4ace29988c93285793e154.pdf"
    },
    {
      "success": true,
      "doc_id": "5e9f832f92cf0cedeafe39b98ed3f900",
      "summary": "Abstract The prediction of molecular interactions is vital for drug discovery. Existing methods often focus on individual prediction tasks and overlook the relationships between them. Additionally, certain tasks encounter limitations due to insufficient data availability, resulting in limited performance. To overcome these limitations, we propose KGE-UNIT, a unified framework that combines knowledge graph embedding (KGE) and multi-task learning, for simultaneous prediction of drugtarget interactions (DTIs) and drugdrug interactions (DDIs) and enhancing the performance of each task, even when data availability is limited. Via KGE, we extract heterogeneous features from the drug knowledge graph to enhance the structural features of drug and protein nodes, thereby improving the quality of features. Additionally, employing multi-task learning, we introduce an innovative predictor that comprises the task-aware Convolutional Neural Network-based (CNN-based) encoder and the task-aware attention decoder which can fuse better multimodal features, capture the contextual interactions of molecular tasks and enhance task awareness, leading to improved performance. Experiments on two imbalanced datasets for DTIs and DDIs demonstrate the superiority of KGE-UNIT, achieving high area under the receiver operating characteristics curves (AUROCs) (0.942, 0.987) and area under the precision-recall curve ( AUPRs) (0.930, 0.980) for DTIs and high AUROCs (0.975, 0.989) and AUPRs (0.966, 0.988) for DDIs. Notably, on the LUO dataset where the data were more limited, KGE-UNIT exhibited a more pronounced improvement, with increases of 4.32\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUROC and 3.56\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUPR for DTIs and 6.56\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUROC and 8.17\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUPR for DDIs. The scalability of KGE-UNIT is demonstrated through its extension to proteinprotein interactions prediction, ablation studies and case studies further validate its effectiveness.",
      "intriguing_abstract": "Abstract The prediction of molecular interactions is vital for drug discovery. Existing methods often focus on individual prediction tasks and overlook the relationships between them. Additionally, certain tasks encounter limitations due to insufficient data availability, resulting in limited performance. To overcome these limitations, we propose KGE-UNIT, a unified framework that combines knowledge graph embedding (KGE) and multi-task learning, for simultaneous prediction of drugtarget interactions (DTIs) and drugdrug interactions (DDIs) and enhancing the performance of each task, even when data availability is limited. Via KGE, we extract heterogeneous features from the drug knowledge graph to enhance the structural features of drug and protein nodes, thereby improving the quality of features. Additionally, employing multi-task learning, we introduce an innovative predictor that comprises the task-aware Convolutional Neural Network-based (CNN-based) encoder and the task-aware attention decoder which can fuse better multimodal features, capture the contextual interactions of molecular tasks and enhance task awareness, leading to improved performance. Experiments on two imbalanced datasets for DTIs and DDIs demonstrate the superiority of KGE-UNIT, achieving high area under the receiver operating characteristics curves (AUROCs) (0.942, 0.987) and area under the precision-recall curve ( AUPRs) (0.930, 0.980) for DTIs and high AUROCs (0.975, 0.989) and AUPRs (0.966, 0.988) for DDIs. Notably, on the LUO dataset where the data were more limited, KGE-UNIT exhibited a more pronounced improvement, with increases of 4.32\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUROC and 3.56\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUPR for DTIs and 6.56\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUROC and 8.17\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUPR for DDIs. The scalability of KGE-UNIT is demonstrated through its extension to proteinprotein interactions prediction, ablation studies and case studies further validate its effectiveness.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9730f484b84074c1d61c154211ea06cc6ff20940.pdf",
      "citation_key": "zhang2024zmq",
      "metadata": {
        "title": "KGE-UNIT: toward the unification of molecular interactions prediction based on knowledge graph and multi-task learning on drug discovery",
        "authors": [
          "Chengcheng Zhang",
          "Tianyi Zang",
          "Tianyi Zhao"
        ],
        "published_date": "2024",
        "abstract": "Abstract The prediction of molecular interactions is vital for drug discovery. Existing methods often focus on individual prediction tasks and overlook the relationships between them. Additionally, certain tasks encounter limitations due to insufficient data availability, resulting in limited performance. To overcome these limitations, we propose KGE-UNIT, a unified framework that combines knowledge graph embedding (KGE) and multi-task learning, for simultaneous prediction of drugtarget interactions (DTIs) and drugdrug interactions (DDIs) and enhancing the performance of each task, even when data availability is limited. Via KGE, we extract heterogeneous features from the drug knowledge graph to enhance the structural features of drug and protein nodes, thereby improving the quality of features. Additionally, employing multi-task learning, we introduce an innovative predictor that comprises the task-aware Convolutional Neural Network-based (CNN-based) encoder and the task-aware attention decoder which can fuse better multimodal features, capture the contextual interactions of molecular tasks and enhance task awareness, leading to improved performance. Experiments on two imbalanced datasets for DTIs and DDIs demonstrate the superiority of KGE-UNIT, achieving high area under the receiver operating characteristics curves (AUROCs) (0.942, 0.987) and area under the precision-recall curve ( AUPRs) (0.930, 0.980) for DTIs and high AUROCs (0.975, 0.989) and AUPRs (0.966, 0.988) for DDIs. Notably, on the LUO dataset where the data were more limited, KGE-UNIT exhibited a more pronounced improvement, with increases of 4.32\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUROC and 3.56\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUPR for DTIs and 6.56\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUROC and 8.17\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUPR for DDIs. The scalability of KGE-UNIT is demonstrated through its extension to proteinprotein interactions prediction, ablation studies and case studies further validate its effectiveness.",
        "file_path": "paper_data/knowledge_graph_embedding/info/9730f484b84074c1d61c154211ea06cc6ff20940.pdf",
        "venue": "Briefings Bioinform.",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Abstract The prediction of molecular interactions is vital for drug discovery. Existing methods often focus on individual prediction tasks and overlook the relationships between them. Additionally, certain tasks encounter limitations due to insufficient data availability, resulting in limited performance. To overcome these limitations, we propose KGE-UNIT, a unified framework that combines knowledge graph embedding (KGE) and multi-task learning, for simultaneous prediction of drugtarget interactions (DTIs) and drugdrug interactions (DDIs) and enhancing the performance of each task, even when data availability is limited. Via KGE, we extract heterogeneous features from the drug knowledge graph to enhance the structural features of drug and protein nodes, thereby improving the quality of features. Additionally, employing multi-task learning, we introduce an innovative predictor that comprises the task-aware Convolutional Neural Network-based (CNN-based) encoder and the task-aware attention decoder which can fuse better multimodal features, capture the contextual interactions of molecular tasks and enhance task awareness, leading to improved performance. Experiments on two imbalanced datasets for DTIs and DDIs demonstrate the superiority of KGE-UNIT, achieving high area under the receiver operating characteristics curves (AUROCs) (0.942, 0.987) and area under the precision-recall curve ( AUPRs) (0.930, 0.980) for DTIs and high AUROCs (0.975, 0.989) and AUPRs (0.966, 0.988) for DDIs. Notably, on the LUO dataset where the data were more limited, KGE-UNIT exhibited a more pronounced improvement, with increases of 4.32\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUROC and 3.56\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUPR for DTIs and 6.56\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUROC and 8.17\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{upgreek} \\usepackage{mathrsfs} \\setlength{\\oddsidemargin}{-69pt} \\begin{document} $\\%$\\end{document} in AUPR for DDIs. The scalability of KGE-UNIT is demonstrated through its extension to proteinprotein interactions prediction, ablation studies and case studies further validate its effectiveness.",
        "keywords": []
      },
      "file_name": "9730f484b84074c1d61c154211ea06cc6ff20940.pdf"
    },
    {
      "success": true,
      "doc_id": "c29617c99ae5f9e00c41bdfcd6c4aa43",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/10c388fa25dd6f07707a414946e5b7a674e7155b.pdf",
      "citation_key": "he2024vks",
      "metadata": {
        "title": "ConvTKG: A query-aware convolutional neural network-based embedding model for temporal knowledge graph completion",
        "authors": [
          "Mingsheng He",
          "Lin Zhu",
          "Luyi Bai"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/10c388fa25dd6f07707a414946e5b7a674e7155b.pdf",
        "venue": "Neurocomputing",
        "citationCount": 15,
        "score": 15.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "10c388fa25dd6f07707a414946e5b7a674e7155b.pdf"
    },
    {
      "success": true,
      "doc_id": "fa2ee66d86ccc8cf0dd244a0238942f6",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7e6a50b70223dc00c712a17537fb7e23f8fd5ad4.pdf",
      "citation_key": "zhang2024fy0",
      "metadata": {
        "title": "SimRE: Simple contrastive learning with soft logical rule for knowledge graph embedding",
        "authors": [
          "Dong Zhang",
          "Zhe Rong",
          "Chengyuan Xue",
          "Guanyu Li"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/7e6a50b70223dc00c712a17537fb7e23f8fd5ad4.pdf",
        "venue": "Information Sciences",
        "citationCount": 14,
        "score": 14.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "7e6a50b70223dc00c712a17537fb7e23f8fd5ad4.pdf"
    },
    {
      "success": true,
      "doc_id": "2f360d8bf280ac540975d5f76cc1840c",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/ae58ebc99f67eed0de7f4ba2ca6f7ceb9ab056fb.pdf",
      "citation_key": "zhang2024ivc",
      "metadata": {
        "title": "CDRGN-SDE: Cross-Dimensional Recurrent Graph Network with neural Stochastic Differential Equation for temporal knowledge graph embedding",
        "authors": [
          "Dong Zhang",
          "Wenlong Feng",
          "Zonghang Wu",
          "Guanyu Li",
          "Bo Ning"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/ae58ebc99f67eed0de7f4ba2ca6f7ceb9ab056fb.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 13,
        "score": 13.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "ae58ebc99f67eed0de7f4ba2ca6f7ceb9ab056fb.pdf"
    },
    {
      "success": true,
      "doc_id": "15d7ab7e6a70b5b28b25c5372ae78979",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6ad02ad36e7a2c7d72d1a0b15ffc61dae2be1d7a.pdf",
      "citation_key": "jing2024nxw",
      "metadata": {
        "title": "XMKR: Explainable manufacturing knowledge recommendation for collaborative design with graph embedding learning",
        "authors": [
          "Yanzhen Jing",
          "Guanghui Zhou",
          "Chao Zhang",
          "Fengtian Chang",
          "Hairui Yan",
          "Zhongdong Xiao"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/6ad02ad36e7a2c7d72d1a0b15ffc61dae2be1d7a.pdf",
        "venue": "Advanced Engineering Informatics",
        "citationCount": 13,
        "score": 13.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "6ad02ad36e7a2c7d72d1a0b15ffc61dae2be1d7a.pdf"
    },
    {
      "success": true,
      "doc_id": "e05bf63a40c181861ada13d7d8d52414",
      "summary": "Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/75ba0b92bcf095e7cd1544425f1818fed195f83f.pdf",
      "citation_key": "jiang2024zlc",
      "metadata": {
        "title": "KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge",
        "authors": [
          "Pengcheng Jiang",
          "Lang Cao",
          "Cao Xiao",
          "Parminder Bhatia",
          "Jimeng Sun",
          "Jiawei Han"
        ],
        "published_date": "2024",
        "abstract": "Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.",
        "file_path": "paper_data/knowledge_graph_embedding/info/75ba0b92bcf095e7cd1544425f1818fed195f83f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Knowledge Graph Embedding (KGE) techniques are crucial in learning compact representations of entities and relations within a knowledge graph, facilitating efficient reasoning and knowledge discovery. While existing methods typically focus either on training KGE models solely based on graph structure or fine-tuning pre-trained language models with classification data in KG, KG-FIT leverages LLM-guided refinement to construct a semantically coherent hierarchical structure of entity clusters. By incorporating this hierarchical knowledge along with textual information during the fine-tuning process, KG-FIT effectively captures both global semantics from the LLM and local semantics from the KG. Extensive experiments on the benchmark datasets FB15K-237, YAGO3-10, and PrimeKG demonstrate the superiority of KG-FIT over state-of-the-art pre-trained language model-based methods, achieving improvements of 14.4%, 13.5%, and 11.9% in the Hits@10 metric for the link prediction task, respectively. Furthermore, KG-FIT yields substantial performance gains of 12.6%, 6.7%, and 17.7% compared to the structure-based base models upon which it is built. These results highlight the effectiveness of KG-FIT in incorporating open-world knowledge from LLMs to significantly enhance the expressiveness and informativeness of KG embeddings.",
        "keywords": []
      },
      "file_name": "75ba0b92bcf095e7cd1544425f1818fed195f83f.pdf"
    },
    {
      "success": true,
      "doc_id": "cae147edea67e93e3051da8f32fb0a4f",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/905d27e361c50da406439bdac25807dd38258fd8.pdf",
      "citation_key": "han2024u0t",
      "metadata": {
        "title": "Knowledge enhanced graph inference network based entity-relation extraction and knowledge graph construction for industrial domain",
        "authors": [
          "Zhulin Han",
          "Jian Wang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/905d27e361c50da406439bdac25807dd38258fd8.pdf",
        "venue": "Frontiers of Engineering Management",
        "citationCount": 12,
        "score": 12.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "905d27e361c50da406439bdac25807dd38258fd8.pdf"
    },
    {
      "success": true,
      "doc_id": "d496fbbed6355dc06e98d447925a6eca",
      "summary": "In the globalization trend, Chinas cultural heritage is in danger of gradually disappearing. The protection and inheritance of these precious cultural resources has become a critical task. This paper focuses on the Miao batik culture in Guizhou Province, China, and explores the application of knowledge graphs, natural language processing, and deep learning techniques in the promotion and protection of batik culture. We propose a dual-channel mechanism that integrates semantic and visual information, aiming to connect batik pattern features with cultural connotations. First, we use natural language processing techniques to automatically extract batik-related entities and relationships from the literature, and construct and visualize a structured batik pattern knowledge graph. Based on this knowledge graph, users can textually search and understand the images, meanings, taboos, and other cultural information of specific patterns. Second, for the batik pattern classification, we propose an improved ResNet34 model. By embedding average pooling and convolutional operations into the residual blocks and introducing long-range residual connections, the classification performance is enhanced. By inputting pattern images into this model, their categories can be accurately identified, and then the underlying cultural connotations can be understood. Experimental results show that our model outperforms other mainstream models in evaluation metrics such as accuracy, precision, recall, and F1-score, achieving 94.46%, 94.47%, 93.62%, and 93.8%, respectively. This research provides new ideas for the digital protection of batik culture and demonstrates the great potential of artificial intelligence technology in cultural heritage protection.",
      "intriguing_abstract": "In the globalization trend, Chinas cultural heritage is in danger of gradually disappearing. The protection and inheritance of these precious cultural resources has become a critical task. This paper focuses on the Miao batik culture in Guizhou Province, China, and explores the application of knowledge graphs, natural language processing, and deep learning techniques in the promotion and protection of batik culture. We propose a dual-channel mechanism that integrates semantic and visual information, aiming to connect batik pattern features with cultural connotations. First, we use natural language processing techniques to automatically extract batik-related entities and relationships from the literature, and construct and visualize a structured batik pattern knowledge graph. Based on this knowledge graph, users can textually search and understand the images, meanings, taboos, and other cultural information of specific patterns. Second, for the batik pattern classification, we propose an improved ResNet34 model. By embedding average pooling and convolutional operations into the residual blocks and introducing long-range residual connections, the classification performance is enhanced. By inputting pattern images into this model, their categories can be accurately identified, and then the underlying cultural connotations can be understood. Experimental results show that our model outperforms other mainstream models in evaluation metrics such as accuracy, precision, recall, and F1-score, achieving 94.46%, 94.47%, 93.62%, and 93.8%, respectively. This research provides new ideas for the digital protection of batik culture and demonstrates the great potential of artificial intelligence technology in cultural heritage protection.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b2646d9ee88c3dd6822b039a38c9604932aaaf47.pdf",
      "citation_key": "quan2024o2a",
      "metadata": {
        "title": "Protection of Guizhou Miao batik culture based on knowledge graph and deep learning",
        "authors": [
          "Huafeng Quan",
          "Yiting Li",
          "Dashuai Liu",
          "Yue Zhou"
        ],
        "published_date": "2024",
        "abstract": "In the globalization trend, Chinas cultural heritage is in danger of gradually disappearing. The protection and inheritance of these precious cultural resources has become a critical task. This paper focuses on the Miao batik culture in Guizhou Province, China, and explores the application of knowledge graphs, natural language processing, and deep learning techniques in the promotion and protection of batik culture. We propose a dual-channel mechanism that integrates semantic and visual information, aiming to connect batik pattern features with cultural connotations. First, we use natural language processing techniques to automatically extract batik-related entities and relationships from the literature, and construct and visualize a structured batik pattern knowledge graph. Based on this knowledge graph, users can textually search and understand the images, meanings, taboos, and other cultural information of specific patterns. Second, for the batik pattern classification, we propose an improved ResNet34 model. By embedding average pooling and convolutional operations into the residual blocks and introducing long-range residual connections, the classification performance is enhanced. By inputting pattern images into this model, their categories can be accurately identified, and then the underlying cultural connotations can be understood. Experimental results show that our model outperforms other mainstream models in evaluation metrics such as accuracy, precision, recall, and F1-score, achieving 94.46%, 94.47%, 93.62%, and 93.8%, respectively. This research provides new ideas for the digital protection of batik culture and demonstrates the great potential of artificial intelligence technology in cultural heritage protection.",
        "file_path": "paper_data/knowledge_graph_embedding/info/b2646d9ee88c3dd6822b039a38c9604932aaaf47.pdf",
        "venue": "Heritage Science",
        "citationCount": 12,
        "score": 12.0,
        "summary": "In the globalization trend, Chinas cultural heritage is in danger of gradually disappearing. The protection and inheritance of these precious cultural resources has become a critical task. This paper focuses on the Miao batik culture in Guizhou Province, China, and explores the application of knowledge graphs, natural language processing, and deep learning techniques in the promotion and protection of batik culture. We propose a dual-channel mechanism that integrates semantic and visual information, aiming to connect batik pattern features with cultural connotations. First, we use natural language processing techniques to automatically extract batik-related entities and relationships from the literature, and construct and visualize a structured batik pattern knowledge graph. Based on this knowledge graph, users can textually search and understand the images, meanings, taboos, and other cultural information of specific patterns. Second, for the batik pattern classification, we propose an improved ResNet34 model. By embedding average pooling and convolutional operations into the residual blocks and introducing long-range residual connections, the classification performance is enhanced. By inputting pattern images into this model, their categories can be accurately identified, and then the underlying cultural connotations can be understood. Experimental results show that our model outperforms other mainstream models in evaluation metrics such as accuracy, precision, recall, and F1-score, achieving 94.46%, 94.47%, 93.62%, and 93.8%, respectively. This research provides new ideas for the digital protection of batik culture and demonstrates the great potential of artificial intelligence technology in cultural heritage protection.",
        "keywords": []
      },
      "file_name": "b2646d9ee88c3dd6822b039a38c9604932aaaf47.pdf"
    },
    {
      "success": true,
      "doc_id": "dc835a14de59c1f9fb4b1ede9a6abe90",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c7666fbaa49da21c465dbfabcf5fdd768b8c7b9e.pdf",
      "citation_key": "liu2024tc2",
      "metadata": {
        "title": "A multi-hierarchical aggregation-based graph convolutional network for industrial knowledge graph embedding towards cognitive intelligent manufacturing",
        "authors": [
          "Bufan Liu",
          "Chun-Hsien Chen",
          "Zuoxu Wang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/c7666fbaa49da21c465dbfabcf5fdd768b8c7b9e.pdf",
        "venue": "Journal of manufacturing systems",
        "citationCount": 11,
        "score": 11.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "c7666fbaa49da21c465dbfabcf5fdd768b8c7b9e.pdf"
    },
    {
      "success": true,
      "doc_id": "f2cc05ddd34e76ae48d9900a480fdbea",
      "summary": "This paper investigates the advantages of representing and processing semantic knowledge extracted into graphs within the emerging paradigm of semantic communications. The proposed approach leverages semantic and pragmatic aspects, incorporating recent advances on large language models (LLMs) to achieve compact representations of knowledge to be processed and exchanged between intelligent agents. This is accomplished by using the cascade of LLMs and graph neural networks (GNNs) as semantic encoders, where information to be shared is selected to be meaningful at the receiver. The embedding vectors produced by the proposed semantic encoder represent information in the form of triplets: nodes (semantic concepts entities), edges(relations between concepts), nodes. Thus, semantic information is associated with the representation of relationships among elements in the space of semantic concept abstractions. In this paper, we investigate the potential of achieving high compression rates in communication by incorporating relations that link elements within graph embeddings. We propose sending semantic symbols solely equivalent to node embeddings through the wireless channel and inferring the complete knowledge graph at the receiver. Numerical simulations illustrate the effectiveness of leveraging knowledge graphs to semantically compress and transmit information.",
      "intriguing_abstract": "This paper investigates the advantages of representing and processing semantic knowledge extracted into graphs within the emerging paradigm of semantic communications. The proposed approach leverages semantic and pragmatic aspects, incorporating recent advances on large language models (LLMs) to achieve compact representations of knowledge to be processed and exchanged between intelligent agents. This is accomplished by using the cascade of LLMs and graph neural networks (GNNs) as semantic encoders, where information to be shared is selected to be meaningful at the receiver. The embedding vectors produced by the proposed semantic encoder represent information in the form of triplets: nodes (semantic concepts entities), edges(relations between concepts), nodes. Thus, semantic information is associated with the representation of relationships among elements in the space of semantic concept abstractions. In this paper, we investigate the potential of achieving high compression rates in communication by incorporating relations that link elements within graph embeddings. We propose sending semantic symbols solely equivalent to node embeddings through the wireless channel and inferring the complete knowledge graph at the receiver. Numerical simulations illustrate the effectiveness of leveraging knowledge graphs to semantically compress and transmit information.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/f1b7682df472a88fbaac3e6049f638ecec6937e7.pdf",
      "citation_key": "hello2024hgf",
      "metadata": {
        "title": "Semantic Communication Enhanced by Knowledge Graph Representation Learning",
        "authors": [
          "Nour Hello",
          "P. Lorenzo",
          "E. Strinati"
        ],
        "published_date": "2024",
        "abstract": "This paper investigates the advantages of representing and processing semantic knowledge extracted into graphs within the emerging paradigm of semantic communications. The proposed approach leverages semantic and pragmatic aspects, incorporating recent advances on large language models (LLMs) to achieve compact representations of knowledge to be processed and exchanged between intelligent agents. This is accomplished by using the cascade of LLMs and graph neural networks (GNNs) as semantic encoders, where information to be shared is selected to be meaningful at the receiver. The embedding vectors produced by the proposed semantic encoder represent information in the form of triplets: nodes (semantic concepts entities), edges(relations between concepts), nodes. Thus, semantic information is associated with the representation of relationships among elements in the space of semantic concept abstractions. In this paper, we investigate the potential of achieving high compression rates in communication by incorporating relations that link elements within graph embeddings. We propose sending semantic symbols solely equivalent to node embeddings through the wireless channel and inferring the complete knowledge graph at the receiver. Numerical simulations illustrate the effectiveness of leveraging knowledge graphs to semantically compress and transmit information.",
        "file_path": "paper_data/knowledge_graph_embedding/info/f1b7682df472a88fbaac3e6049f638ecec6937e7.pdf",
        "venue": "International Workshop on Signal Processing Advances in Wireless Communications",
        "citationCount": 11,
        "score": 11.0,
        "summary": "This paper investigates the advantages of representing and processing semantic knowledge extracted into graphs within the emerging paradigm of semantic communications. The proposed approach leverages semantic and pragmatic aspects, incorporating recent advances on large language models (LLMs) to achieve compact representations of knowledge to be processed and exchanged between intelligent agents. This is accomplished by using the cascade of LLMs and graph neural networks (GNNs) as semantic encoders, where information to be shared is selected to be meaningful at the receiver. The embedding vectors produced by the proposed semantic encoder represent information in the form of triplets: nodes (semantic concepts entities), edges(relations between concepts), nodes. Thus, semantic information is associated with the representation of relationships among elements in the space of semantic concept abstractions. In this paper, we investigate the potential of achieving high compression rates in communication by incorporating relations that link elements within graph embeddings. We propose sending semantic symbols solely equivalent to node embeddings through the wireless channel and inferring the complete knowledge graph at the receiver. Numerical simulations illustrate the effectiveness of leveraging knowledge graphs to semantically compress and transmit information.",
        "keywords": []
      },
      "file_name": "f1b7682df472a88fbaac3e6049f638ecec6937e7.pdf"
    },
    {
      "success": true,
      "doc_id": "b95ee539dfc68d5eade8689c9a7d2f9c",
      "summary": "Knowledge graph completion (KGC) aims to infer missing facts based on existing facts within a KG. Recently, research on generative models (GMs) has addressed the limitations of embedding methods in terms of generality and scalability. However, GM-based methods are sensitive to contextual facts on KG, so the contextual facts of poor quality can cause GMs to generate erroneous results. To improve the performance of GM-based methods for various KGC tasks, we propose a COntextual FactS GuIded GeneratioN (COSIGN) model. First, to enhance the inference ability of the generative model, we designed a contextual facts collector to achieve human-like retrieval behavior. Second, a contextual facts organizer is proposed to learn the organized capabilities of LLMs through knowledge distillation. Finally, the organized contextual facts as the input of the inference generator to generate missing facts. Experimental results demonstrate that COSIGN outperforms state-of-the-art baseline techniques in terms of performance.",
      "intriguing_abstract": "Knowledge graph completion (KGC) aims to infer missing facts based on existing facts within a KG. Recently, research on generative models (GMs) has addressed the limitations of embedding methods in terms of generality and scalability. However, GM-based methods are sensitive to contextual facts on KG, so the contextual facts of poor quality can cause GMs to generate erroneous results. To improve the performance of GM-based methods for various KGC tasks, we propose a COntextual FactS GuIded GeneratioN (COSIGN) model. First, to enhance the inference ability of the generative model, we designed a contextual facts collector to achieve human-like retrieval behavior. Second, a contextual facts organizer is proposed to learn the organized capabilities of LLMs through knowledge distillation. Finally, the organized contextual facts as the input of the inference generator to generate missing facts. Experimental results demonstrate that COSIGN outperforms state-of-the-art baseline techniques in terms of performance.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/d66622beef468f7b934a5bf601cb8a3fcefe78f3.pdf",
      "citation_key": "li2024z0e",
      "metadata": {
        "title": "COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion",
        "authors": [
          "Jinpeng Li",
          "Hang Yu",
          "Xiangfeng Luo",
          "Qian Liu"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graph completion (KGC) aims to infer missing facts based on existing facts within a KG. Recently, research on generative models (GMs) has addressed the limitations of embedding methods in terms of generality and scalability. However, GM-based methods are sensitive to contextual facts on KG, so the contextual facts of poor quality can cause GMs to generate erroneous results. To improve the performance of GM-based methods for various KGC tasks, we propose a COntextual FactS GuIded GeneratioN (COSIGN) model. First, to enhance the inference ability of the generative model, we designed a contextual facts collector to achieve human-like retrieval behavior. Second, a contextual facts organizer is proposed to learn the organized capabilities of LLMs through knowledge distillation. Finally, the organized contextual facts as the input of the inference generator to generate missing facts. Experimental results demonstrate that COSIGN outperforms state-of-the-art baseline techniques in terms of performance.",
        "file_path": "paper_data/knowledge_graph_embedding/info/d66622beef468f7b934a5bf601cb8a3fcefe78f3.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Knowledge graph completion (KGC) aims to infer missing facts based on existing facts within a KG. Recently, research on generative models (GMs) has addressed the limitations of embedding methods in terms of generality and scalability. However, GM-based methods are sensitive to contextual facts on KG, so the contextual facts of poor quality can cause GMs to generate erroneous results. To improve the performance of GM-based methods for various KGC tasks, we propose a COntextual FactS GuIded GeneratioN (COSIGN) model. First, to enhance the inference ability of the generative model, we designed a contextual facts collector to achieve human-like retrieval behavior. Second, a contextual facts organizer is proposed to learn the organized capabilities of LLMs through knowledge distillation. Finally, the organized contextual facts as the input of the inference generator to generate missing facts. Experimental results demonstrate that COSIGN outperforms state-of-the-art baseline techniques in terms of performance.",
        "keywords": []
      },
      "file_name": "d66622beef468f7b934a5bf601cb8a3fcefe78f3.pdf"
    },
    {
      "success": true,
      "doc_id": "4b226d279d4a60dc592bcc4e096b0e93",
      "summary": "Prospectivity mapping based on deep learning typically requires substantial amounts of geological feature information from known mineral deposits. Due to the limited spatial distribution of ore deposits, the training of predictive models is often hampered by insufficient positive samples. Meanwhile, data-driven mineral prospectivity mapping often overlooks domain knowledge and expert experience, leading to poor interpretability of predictive results. To address this problem, we employed the Gaussian mixture model (GMM) for spatial feature classification to expand the number of positive samples. The approach integrated the embedding of geological map knowledge graphs with geological exploration data to enhance the knowledge constraints of the prospecting model, which enabled the integration of knowledge with data. Considering the complex spatial structure of geological elements, a bi-branch utilizing the 1-dimensional convolutional neural network (CNN1D) and graph convolutional network (GCN) was used to extract geological spatial features for model training and prediction. To validate the effectiveness of the method, a gold mineralization prediction study was conducted in the Wulonggou area (Qinghai province, western China). The results indicate that, when the number of GMM spatial feature classifications was 17, the positive-to-negative sample ratio was optimal, and the embedding of the knowledge graph controlled the prediction area distribution effectively, which demonstrated strong consistency between the prospecting area and the known mineral deposits. Compared with the predictions by CNN1D, the fused prediction model of CNN1D and GCN yielded higher accuracy. Our model identified 11 classes of mineralization potential areas and provides geological interpretations for different prediction categories.",
      "intriguing_abstract": "Prospectivity mapping based on deep learning typically requires substantial amounts of geological feature information from known mineral deposits. Due to the limited spatial distribution of ore deposits, the training of predictive models is often hampered by insufficient positive samples. Meanwhile, data-driven mineral prospectivity mapping often overlooks domain knowledge and expert experience, leading to poor interpretability of predictive results. To address this problem, we employed the Gaussian mixture model (GMM) for spatial feature classification to expand the number of positive samples. The approach integrated the embedding of geological map knowledge graphs with geological exploration data to enhance the knowledge constraints of the prospecting model, which enabled the integration of knowledge with data. Considering the complex spatial structure of geological elements, a bi-branch utilizing the 1-dimensional convolutional neural network (CNN1D) and graph convolutional network (GCN) was used to extract geological spatial features for model training and prediction. To validate the effectiveness of the method, a gold mineralization prediction study was conducted in the Wulonggou area (Qinghai province, western China). The results indicate that, when the number of GMM spatial feature classifications was 17, the positive-to-negative sample ratio was optimal, and the embedding of the knowledge graph controlled the prediction area distribution effectively, which demonstrated strong consistency between the prospecting area and the known mineral deposits. Compared with the predictions by CNN1D, the fused prediction model of CNN1D and GCN yielded higher accuracy. Our model identified 11 classes of mineralization potential areas and provides geological interpretations for different prediction categories.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/20486c2fb358730ee99ae39b5e0a88d7b39ca720.pdf",
      "citation_key": "yan2024joa",
      "metadata": {
        "title": "Mineral Prospectivity Mapping Based on Spatial Feature Classification with Geological Map Knowledge Graph Embedding: Case Study of Gold Ore Prediction at Wulonggou, Qinghai Province (Western China)",
        "authors": [
          "Qun Yan",
          "Juan Zhao",
          "Linfu Xue",
          "Liqiong Wei",
          "Mingjia Ji",
          "Xiang-jin Ran",
          "Junhao Dai"
        ],
        "published_date": "2024",
        "abstract": "Prospectivity mapping based on deep learning typically requires substantial amounts of geological feature information from known mineral deposits. Due to the limited spatial distribution of ore deposits, the training of predictive models is often hampered by insufficient positive samples. Meanwhile, data-driven mineral prospectivity mapping often overlooks domain knowledge and expert experience, leading to poor interpretability of predictive results. To address this problem, we employed the Gaussian mixture model (GMM) for spatial feature classification to expand the number of positive samples. The approach integrated the embedding of geological map knowledge graphs with geological exploration data to enhance the knowledge constraints of the prospecting model, which enabled the integration of knowledge with data. Considering the complex spatial structure of geological elements, a bi-branch utilizing the 1-dimensional convolutional neural network (CNN1D) and graph convolutional network (GCN) was used to extract geological spatial features for model training and prediction. To validate the effectiveness of the method, a gold mineralization prediction study was conducted in the Wulonggou area (Qinghai province, western China). The results indicate that, when the number of GMM spatial feature classifications was 17, the positive-to-negative sample ratio was optimal, and the embedding of the knowledge graph controlled the prediction area distribution effectively, which demonstrated strong consistency between the prospecting area and the known mineral deposits. Compared with the predictions by CNN1D, the fused prediction model of CNN1D and GCN yielded higher accuracy. Our model identified 11 classes of mineralization potential areas and provides geological interpretations for different prediction categories.",
        "file_path": "paper_data/knowledge_graph_embedding/info/20486c2fb358730ee99ae39b5e0a88d7b39ca720.pdf",
        "venue": "Natural Resources Research",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Prospectivity mapping based on deep learning typically requires substantial amounts of geological feature information from known mineral deposits. Due to the limited spatial distribution of ore deposits, the training of predictive models is often hampered by insufficient positive samples. Meanwhile, data-driven mineral prospectivity mapping often overlooks domain knowledge and expert experience, leading to poor interpretability of predictive results. To address this problem, we employed the Gaussian mixture model (GMM) for spatial feature classification to expand the number of positive samples. The approach integrated the embedding of geological map knowledge graphs with geological exploration data to enhance the knowledge constraints of the prospecting model, which enabled the integration of knowledge with data. Considering the complex spatial structure of geological elements, a bi-branch utilizing the 1-dimensional convolutional neural network (CNN1D) and graph convolutional network (GCN) was used to extract geological spatial features for model training and prediction. To validate the effectiveness of the method, a gold mineralization prediction study was conducted in the Wulonggou area (Qinghai province, western China). The results indicate that, when the number of GMM spatial feature classifications was 17, the positive-to-negative sample ratio was optimal, and the embedding of the knowledge graph controlled the prediction area distribution effectively, which demonstrated strong consistency between the prospecting area and the known mineral deposits. Compared with the predictions by CNN1D, the fused prediction model of CNN1D and GCN yielded higher accuracy. Our model identified 11 classes of mineralization potential areas and provides geological interpretations for different prediction categories.",
        "keywords": []
      },
      "file_name": "20486c2fb358730ee99ae39b5e0a88d7b39ca720.pdf"
    },
    {
      "success": true,
      "doc_id": "b37893b38fe6e700288c5afba4e845df",
      "summary": "The growing number of publications in the field of artificial intelligence highlights the need for researchers to enhance their efficiency in searching for relevant articles. Most paper recommendation models either rely on simplistic citation relationships among papers or focus on content-based approaches, both of which overlook interactions within academic networks. To address the aforementioned problem, knowledge graph embedding (KGE) methods have been used for citation recommendations because recent research proves that graph representations can effectively improve recommendation model accuracy. However, academic networks are dynamic, leading to changes in the representations of users and items over time. The majority of KGE-based citation recommendations are primarily designed for static graphs, thus failing to capture the evolution of dynamic knowledge graph (DKG) structures. To address these challenges, we introduced the evolving knowledge graph embedding (EKGE) method. In this methodology, evolving knowledge graphs are input into time-series models to learn the patterns of structural evolution. The model has the capability to generate embeddings for each entity at various time points, thereby overcoming limitation of static models that require retraining to acquire embeddings at each specific time point. To enhance the efficiency of feature extraction, we employed a multiple attention strategy. This helped the model find recommendation lists that are closely related to a users needs, leading to improved recommendation accuracy. Various experiments conducted on a citation recommendation dataset revealed that the EKGE model exhibits a 1.13% increase in prediction accuracy compared to other KGE methods. Moreover, the models accuracy can be further increased by an additional 0.84% through the incorporation of an attention mechanism.",
      "intriguing_abstract": "The growing number of publications in the field of artificial intelligence highlights the need for researchers to enhance their efficiency in searching for relevant articles. Most paper recommendation models either rely on simplistic citation relationships among papers or focus on content-based approaches, both of which overlook interactions within academic networks. To address the aforementioned problem, knowledge graph embedding (KGE) methods have been used for citation recommendations because recent research proves that graph representations can effectively improve recommendation model accuracy. However, academic networks are dynamic, leading to changes in the representations of users and items over time. The majority of KGE-based citation recommendations are primarily designed for static graphs, thus failing to capture the evolution of dynamic knowledge graph (DKG) structures. To address these challenges, we introduced the evolving knowledge graph embedding (EKGE) method. In this methodology, evolving knowledge graphs are input into time-series models to learn the patterns of structural evolution. The model has the capability to generate embeddings for each entity at various time points, thereby overcoming limitation of static models that require retraining to acquire embeddings at each specific time point. To enhance the efficiency of feature extraction, we employed a multiple attention strategy. This helped the model find recommendation lists that are closely related to a users needs, leading to improved recommendation accuracy. Various experiments conducted on a citation recommendation dataset revealed that the EKGE model exhibits a 1.13% increase in prediction accuracy compared to other KGE methods. Moreover, the models accuracy can be further increased by an additional 0.84% through the incorporation of an attention mechanism.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b49f6029d681ac286ab929238f5aef5f352767c8.pdf",
      "citation_key": "liu2024tn0",
      "metadata": {
        "title": "Evolving Knowledge Graph Representation Learning with Multiple Attention Strategies for Citation Recommendation System",
        "authors": [
          "Jhih-Chen Liu",
          "Chiao-Ting Chen",
          "Chi Lee",
          "Szu-Hao Huang"
        ],
        "published_date": "2024",
        "abstract": "The growing number of publications in the field of artificial intelligence highlights the need for researchers to enhance their efficiency in searching for relevant articles. Most paper recommendation models either rely on simplistic citation relationships among papers or focus on content-based approaches, both of which overlook interactions within academic networks. To address the aforementioned problem, knowledge graph embedding (KGE) methods have been used for citation recommendations because recent research proves that graph representations can effectively improve recommendation model accuracy. However, academic networks are dynamic, leading to changes in the representations of users and items over time. The majority of KGE-based citation recommendations are primarily designed for static graphs, thus failing to capture the evolution of dynamic knowledge graph (DKG) structures. To address these challenges, we introduced the evolving knowledge graph embedding (EKGE) method. In this methodology, evolving knowledge graphs are input into time-series models to learn the patterns of structural evolution. The model has the capability to generate embeddings for each entity at various time points, thereby overcoming limitation of static models that require retraining to acquire embeddings at each specific time point. To enhance the efficiency of feature extraction, we employed a multiple attention strategy. This helped the model find recommendation lists that are closely related to a users needs, leading to improved recommendation accuracy. Various experiments conducted on a citation recommendation dataset revealed that the EKGE model exhibits a 1.13% increase in prediction accuracy compared to other KGE methods. Moreover, the models accuracy can be further increased by an additional 0.84% through the incorporation of an attention mechanism.",
        "file_path": "paper_data/knowledge_graph_embedding/info/b49f6029d681ac286ab929238f5aef5f352767c8.pdf",
        "venue": "ACM Transactions on Intelligent Systems and Technology",
        "citationCount": 8,
        "score": 8.0,
        "summary": "The growing number of publications in the field of artificial intelligence highlights the need for researchers to enhance their efficiency in searching for relevant articles. Most paper recommendation models either rely on simplistic citation relationships among papers or focus on content-based approaches, both of which overlook interactions within academic networks. To address the aforementioned problem, knowledge graph embedding (KGE) methods have been used for citation recommendations because recent research proves that graph representations can effectively improve recommendation model accuracy. However, academic networks are dynamic, leading to changes in the representations of users and items over time. The majority of KGE-based citation recommendations are primarily designed for static graphs, thus failing to capture the evolution of dynamic knowledge graph (DKG) structures. To address these challenges, we introduced the evolving knowledge graph embedding (EKGE) method. In this methodology, evolving knowledge graphs are input into time-series models to learn the patterns of structural evolution. The model has the capability to generate embeddings for each entity at various time points, thereby overcoming limitation of static models that require retraining to acquire embeddings at each specific time point. To enhance the efficiency of feature extraction, we employed a multiple attention strategy. This helped the model find recommendation lists that are closely related to a users needs, leading to improved recommendation accuracy. Various experiments conducted on a citation recommendation dataset revealed that the EKGE model exhibits a 1.13% increase in prediction accuracy compared to other KGE methods. Moreover, the models accuracy can be further increased by an additional 0.84% through the incorporation of an attention mechanism.",
        "keywords": []
      },
      "file_name": "b49f6029d681ac286ab929238f5aef5f352767c8.pdf"
    },
    {
      "success": true,
      "doc_id": "dfb488061beef66072d9f981956e48fc",
      "summary": "A knowledge graph (KG) is a technique for modeling entities and their interrelations. Knowledge graph embedding (KGE) translates these entities and relationships into a continuous vector space to facilitate dense and efficient representations. In the domain of chemistry, applying KG and KGE techniques integrates heterogeneous chemical information into a coherent and user-friendly framework, enhances the representation of chemical data features, and is beneficial for downstream tasks, such as chemical property prediction. This paper begins with a comprehensive review of classical and contemporary KGE methodologies, including distance-based models, semantic matching models, and neural network-based approaches. We then catalogue the primary databases employed in chemistry and biochemistry that furnish the KGs with essential chemical data. Subsequently, we explore the latest applications of KG and KGE in chemistry, focusing on risk assessment, property prediction, and drug discovery. Finally, we discuss the current challenges to KG and KGE techniques and provide a perspective on their potential future developments.",
      "intriguing_abstract": "A knowledge graph (KG) is a technique for modeling entities and their interrelations. Knowledge graph embedding (KGE) translates these entities and relationships into a continuous vector space to facilitate dense and efficient representations. In the domain of chemistry, applying KG and KGE techniques integrates heterogeneous chemical information into a coherent and user-friendly framework, enhances the representation of chemical data features, and is beneficial for downstream tasks, such as chemical property prediction. This paper begins with a comprehensive review of classical and contemporary KGE methodologies, including distance-based models, semantic matching models, and neural network-based approaches. We then catalogue the primary databases employed in chemistry and biochemistry that furnish the KGs with essential chemical data. Subsequently, we explore the latest applications of KG and KGE in chemistry, focusing on risk assessment, property prediction, and drug discovery. Finally, we discuss the current challenges to KG and KGE techniques and provide a perspective on their potential future developments.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c5a19440511a741edd1581d41d37d3e9b7088186.pdf",
      "citation_key": "wang20245dw",
      "metadata": {
        "title": "Research Progresses and Applications of Knowledge Graph Embedding Technique in Chemistry",
        "authors": [
          "Chuanghui Wang",
          "Yunqing Yang",
          "Jinshuai Song",
          "Xiaofei Nan"
        ],
        "published_date": "2024",
        "abstract": "A knowledge graph (KG) is a technique for modeling entities and their interrelations. Knowledge graph embedding (KGE) translates these entities and relationships into a continuous vector space to facilitate dense and efficient representations. In the domain of chemistry, applying KG and KGE techniques integrates heterogeneous chemical information into a coherent and user-friendly framework, enhances the representation of chemical data features, and is beneficial for downstream tasks, such as chemical property prediction. This paper begins with a comprehensive review of classical and contemporary KGE methodologies, including distance-based models, semantic matching models, and neural network-based approaches. We then catalogue the primary databases employed in chemistry and biochemistry that furnish the KGs with essential chemical data. Subsequently, we explore the latest applications of KG and KGE in chemistry, focusing on risk assessment, property prediction, and drug discovery. Finally, we discuss the current challenges to KG and KGE techniques and provide a perspective on their potential future developments.",
        "file_path": "paper_data/knowledge_graph_embedding/info/c5a19440511a741edd1581d41d37d3e9b7088186.pdf",
        "venue": "Journal of Chemical Information and Modeling",
        "citationCount": 7,
        "score": 7.0,
        "summary": "A knowledge graph (KG) is a technique for modeling entities and their interrelations. Knowledge graph embedding (KGE) translates these entities and relationships into a continuous vector space to facilitate dense and efficient representations. In the domain of chemistry, applying KG and KGE techniques integrates heterogeneous chemical information into a coherent and user-friendly framework, enhances the representation of chemical data features, and is beneficial for downstream tasks, such as chemical property prediction. This paper begins with a comprehensive review of classical and contemporary KGE methodologies, including distance-based models, semantic matching models, and neural network-based approaches. We then catalogue the primary databases employed in chemistry and biochemistry that furnish the KGs with essential chemical data. Subsequently, we explore the latest applications of KG and KGE in chemistry, focusing on risk assessment, property prediction, and drug discovery. Finally, we discuss the current challenges to KG and KGE techniques and provide a perspective on their potential future developments.",
        "keywords": []
      },
      "file_name": "c5a19440511a741edd1581d41d37d3e9b7088186.pdf"
    },
    {
      "success": true,
      "doc_id": "9c45bea8ebf32bbef5c6519cfe80e6b2",
      "summary": "Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion. However, most existing KGE methods suffer from the challenge of multiple relation semantics, which often degrades their performance. This is because most KGE methods learn fixed continuous vectors for entities (relations) and make deterministic entity predictions to complete the knowledge graph, which hardly captures multiple relation semantics. To tackle this issue, previous works try to learn complex probabilistic embeddings instead of fixed embeddings but suffer from heavy computational complexity. In contrast, this paper proposes a simple yet efficient framework namely the Knowledge Graph Diffusion Model (KGDM) to capture the multiple relation semantics in prediction. Its key idea is to cast the problem of entity prediction into conditional entity generation. Specifically, KGDM estimates the probabilistic distribution of target entities in prediction through Denoising Diffusion Probabilistic Models (DDPM). To bridge the gap between continuous diffusion models and discrete KGs, two learnable embedding functions are defined to map entities and relation to continuous vectors. To consider connectivity patterns of KGs, a Conditional Entity Denoiser model is introduced to generate target entities conditioned on given entities and relations. Extensive experiments demonstrate that KGDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.",
      "intriguing_abstract": "Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion. However, most existing KGE methods suffer from the challenge of multiple relation semantics, which often degrades their performance. This is because most KGE methods learn fixed continuous vectors for entities (relations) and make deterministic entity predictions to complete the knowledge graph, which hardly captures multiple relation semantics. To tackle this issue, previous works try to learn complex probabilistic embeddings instead of fixed embeddings but suffer from heavy computational complexity. In contrast, this paper proposes a simple yet efficient framework namely the Knowledge Graph Diffusion Model (KGDM) to capture the multiple relation semantics in prediction. Its key idea is to cast the problem of entity prediction into conditional entity generation. Specifically, KGDM estimates the probabilistic distribution of target entities in prediction through Denoising Diffusion Probabilistic Models (DDPM). To bridge the gap between continuous diffusion models and discrete KGs, two learnable embedding functions are defined to map entities and relation to continuous vectors. To consider connectivity patterns of KGs, a Conditional Entity Denoiser model is introduced to generate target entities conditioned on given entities and relations. Extensive experiments demonstrate that KGDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/822ad7c33316202a2511d300c6b8a263b758ad1a.pdf",
      "citation_key": "long2024soi",
      "metadata": {
        "title": "KGDM: A Diffusion Model to Capture Multiple Relation Semantics for Knowledge Graph Embedding",
        "authors": [
          "Xiao Long",
          "Liansheng Zhuang",
          "Aodi Li",
          "Jiuchang Wei",
          "Houqiang Li",
          "Shafei Wang"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion. However, most existing KGE methods suffer from the challenge of multiple relation semantics, which often degrades their performance. This is because most KGE methods learn fixed continuous vectors for entities (relations) and make deterministic entity predictions to complete the knowledge graph, which hardly captures multiple relation semantics. To tackle this issue, previous works try to learn complex probabilistic embeddings instead of fixed embeddings but suffer from heavy computational complexity. In contrast, this paper proposes a simple yet efficient framework namely the Knowledge Graph Diffusion Model (KGDM) to capture the multiple relation semantics in prediction. Its key idea is to cast the problem of entity prediction into conditional entity generation. Specifically, KGDM estimates the probabilistic distribution of target entities in prediction through Denoising Diffusion Probabilistic Models (DDPM). To bridge the gap between continuous diffusion models and discrete KGs, two learnable embedding functions are defined to map entities and relation to continuous vectors. To consider connectivity patterns of KGs, a Conditional Entity Denoiser model is introduced to generate target entities conditioned on given entities and relations. Extensive experiments demonstrate that KGDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.",
        "file_path": "paper_data/knowledge_graph_embedding/info/822ad7c33316202a2511d300c6b8a263b758ad1a.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion. However, most existing KGE methods suffer from the challenge of multiple relation semantics, which often degrades their performance. This is because most KGE methods learn fixed continuous vectors for entities (relations) and make deterministic entity predictions to complete the knowledge graph, which hardly captures multiple relation semantics. To tackle this issue, previous works try to learn complex probabilistic embeddings instead of fixed embeddings but suffer from heavy computational complexity. In contrast, this paper proposes a simple yet efficient framework namely the Knowledge Graph Diffusion Model (KGDM) to capture the multiple relation semantics in prediction. Its key idea is to cast the problem of entity prediction into conditional entity generation. Specifically, KGDM estimates the probabilistic distribution of target entities in prediction through Denoising Diffusion Probabilistic Models (DDPM). To bridge the gap between continuous diffusion models and discrete KGs, two learnable embedding functions are defined to map entities and relation to continuous vectors. To consider connectivity patterns of KGs, a Conditional Entity Denoiser model is introduced to generate target entities conditioned on given entities and relations. Extensive experiments demonstrate that KGDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.",
        "keywords": []
      },
      "file_name": "822ad7c33316202a2511d300c6b8a263b758ad1a.pdf"
    },
    {
      "success": true,
      "doc_id": "f82056f7a500e50a14233243e453a104",
      "summary": "Knowledge graph reasoning (KGR) answers logical queries over a knowledge graph (KG), and embedding-based KGR (EKGR) becomes popular recently, which embeds both queries and KG entities such that the vector embeddings of a query and its answer entities are similar. Compared with traditional KGR methods based on subgraph matching, EKGR produces fewer intermediate results and is more robust to missing and noisy information in the KG. However, existing systems are inefficient for serving online EKGR queries because they can only batch queries of the same type for execution (i.e., query-level batching ) and hence have limited batching opportunities due to the heterogeneity of queries. To serve EKGR queries efficiently, we propose the Atom system with operator-level batching, which decomposes queries into operators and batches operators of the same type from different queries for execution. The insight is that the types of operators are far fewer than the types of queries, and thus different queries typically share common operators, yielding more batching opportunities. To schedule the operators, Atom adopts a hybrid policy, which improves system throughput and avoids starving rare operators. For efficiency, Atom incorporates system optimizations including two-level pipeline, opportunistic submission, pre-allocated memory buffer, and tailored GPU kernels. Experiment results show that compared with existing systems, Atom can improve query throughput by over 20x and reduce query latency by over 5x. Micro experiments suggest that the designs and optimizations are effective in improving system performance.",
      "intriguing_abstract": "Knowledge graph reasoning (KGR) answers logical queries over a knowledge graph (KG), and embedding-based KGR (EKGR) becomes popular recently, which embeds both queries and KG entities such that the vector embeddings of a query and its answer entities are similar. Compared with traditional KGR methods based on subgraph matching, EKGR produces fewer intermediate results and is more robust to missing and noisy information in the KG. However, existing systems are inefficient for serving online EKGR queries because they can only batch queries of the same type for execution (i.e., query-level batching ) and hence have limited batching opportunities due to the heterogeneity of queries. To serve EKGR queries efficiently, we propose the Atom system with operator-level batching, which decomposes queries into operators and batches operators of the same type from different queries for execution. The insight is that the types of operators are far fewer than the types of queries, and thus different queries typically share common operators, yielding more batching opportunities. To schedule the operators, Atom adopts a hybrid policy, which improves system throughput and avoids starving rare operators. For efficiency, Atom incorporates system optimizations including two-level pipeline, opportunistic submission, pre-allocated memory buffer, and tailored GPU kernels. Experiment results show that compared with existing systems, Atom can improve query throughput by over 20x and reduce query latency by over 5x. Micro experiments suggest that the designs and optimizations are effective in improving system performance.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/ba61c59abb560ff47a8dd780c8ccffb0af5e14c2.pdf",
      "citation_key": "zhou2024ayq",
      "metadata": {
        "title": "Atom: An Efficient Query Serving System for Embedding-based Knowledge Graph Reasoning with Operator-level Batching",
        "authors": [
          "Qihui Zhou",
          "Peiqi Yin",
          "Xiao Yan",
          "Changji Li",
          "Guanxian Jiang",
          "James Cheng"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graph reasoning (KGR) answers logical queries over a knowledge graph (KG), and embedding-based KGR (EKGR) becomes popular recently, which embeds both queries and KG entities such that the vector embeddings of a query and its answer entities are similar. Compared with traditional KGR methods based on subgraph matching, EKGR produces fewer intermediate results and is more robust to missing and noisy information in the KG. However, existing systems are inefficient for serving online EKGR queries because they can only batch queries of the same type for execution (i.e., query-level batching ) and hence have limited batching opportunities due to the heterogeneity of queries. To serve EKGR queries efficiently, we propose the Atom system with operator-level batching, which decomposes queries into operators and batches operators of the same type from different queries for execution. The insight is that the types of operators are far fewer than the types of queries, and thus different queries typically share common operators, yielding more batching opportunities. To schedule the operators, Atom adopts a hybrid policy, which improves system throughput and avoids starving rare operators. For efficiency, Atom incorporates system optimizations including two-level pipeline, opportunistic submission, pre-allocated memory buffer, and tailored GPU kernels. Experiment results show that compared with existing systems, Atom can improve query throughput by over 20x and reduce query latency by over 5x. Micro experiments suggest that the designs and optimizations are effective in improving system performance.",
        "file_path": "paper_data/knowledge_graph_embedding/info/ba61c59abb560ff47a8dd780c8ccffb0af5e14c2.pdf",
        "venue": "Proc. ACM Manag. Data",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Knowledge graph reasoning (KGR) answers logical queries over a knowledge graph (KG), and embedding-based KGR (EKGR) becomes popular recently, which embeds both queries and KG entities such that the vector embeddings of a query and its answer entities are similar. Compared with traditional KGR methods based on subgraph matching, EKGR produces fewer intermediate results and is more robust to missing and noisy information in the KG. However, existing systems are inefficient for serving online EKGR queries because they can only batch queries of the same type for execution (i.e., query-level batching ) and hence have limited batching opportunities due to the heterogeneity of queries. To serve EKGR queries efficiently, we propose the Atom system with operator-level batching, which decomposes queries into operators and batches operators of the same type from different queries for execution. The insight is that the types of operators are far fewer than the types of queries, and thus different queries typically share common operators, yielding more batching opportunities. To schedule the operators, Atom adopts a hybrid policy, which improves system throughput and avoids starving rare operators. For efficiency, Atom incorporates system optimizations including two-level pipeline, opportunistic submission, pre-allocated memory buffer, and tailored GPU kernels. Experiment results show that compared with existing systems, Atom can improve query throughput by over 20x and reduce query latency by over 5x. Micro experiments suggest that the designs and optimizations are effective in improving system performance.",
        "keywords": []
      },
      "file_name": "ba61c59abb560ff47a8dd780c8ccffb0af5e14c2.pdf"
    },
    {
      "success": true,
      "doc_id": "8552a81fccec8301cee45117c3914548",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/b3c340aa22bcd183c41836ef7265d656f741911f.pdf",
      "citation_key": "huang2024t19",
      "metadata": {
        "title": "Incorporating environmental knowledge embedding and spatial-temporal graph attention networks for inland vessel traffic flow prediction",
        "authors": [
          "Chen Huang",
          "Deshan Chen",
          "Tengze Fan",
          "Bing Wu",
          "Xinping Yan"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/b3c340aa22bcd183c41836ef7265d656f741911f.pdf",
        "venue": "Engineering applications of artificial intelligence",
        "citationCount": 7,
        "score": 7.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "b3c340aa22bcd183c41836ef7265d656f741911f.pdf"
    },
    {
      "success": true,
      "doc_id": "d5e7255d9416038fe60f5a851fed0f2e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7c82aa0ae4b4e027a2df8afe9bbeccf88368c62f.pdf",
      "citation_key": "lu2024fsd",
      "metadata": {
        "title": "Deep hyperbolic convolutional model for knowledge graph embedding",
        "authors": [
          "Ming Lu",
          "Yancong Li",
          "Jiangxiao Zhang",
          "Haiying Ren",
          "Xiaoming Zhang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/7c82aa0ae4b4e027a2df8afe9bbeccf88368c62f.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 6,
        "score": 6.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "7c82aa0ae4b4e027a2df8afe9bbeccf88368c62f.pdf"
    },
    {
      "success": true,
      "doc_id": "de16faee40602f9adb58c7077fae3c85",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/0d9a788260e3abff4794d79f72b2b5ab2fb5abe5.pdf",
      "citation_key": "liu2024yar",
      "metadata": {
        "title": "Uncertain knowledge graph embedding: an effective method combining multi-relation and multi-path",
        "authors": [
          "Qi Liu",
          "Qinghua Zhang",
          "Fan Zhao",
          "Guoyin Wang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/0d9a788260e3abff4794d79f72b2b5ab2fb5abe5.pdf",
        "venue": "Frontiers Comput. Sci.",
        "citationCount": 6,
        "score": 6.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "0d9a788260e3abff4794d79f72b2b5ab2fb5abe5.pdf"
    },
    {
      "success": true,
      "doc_id": "0600815c249b7f9402c18f5fe0661ef6",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6cba788eea4fdb3bd0d1db4ecdd8a70040b81e62.pdf",
      "citation_key": "khan20242y2",
      "metadata": {
        "title": "Continual knowledge graph embedding enhancement for joint interaction-based next click recommendation",
        "authors": [
          "Nasrullah Khan",
          "Zongmin Ma",
          "Ruizhe Ma",
          "Kemal Polat"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/6cba788eea4fdb3bd0d1db4ecdd8a70040b81e62.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 6,
        "score": 6.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "6cba788eea4fdb3bd0d1db4ecdd8a70040b81e62.pdf"
    },
    {
      "success": true,
      "doc_id": "97a982c250117e0ec7d9aa3a6588fdd5",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6c195ec2d5a491ffca9ab893968c4d44a6d0ce7d.pdf",
      "citation_key": "xue2025ee8",
      "metadata": {
        "title": "MHRN: A multi-perspective hierarchical relation network for knowledge graph embedding",
        "authors": [
          "Zengcan Xue",
          "Zhaoli Zhang",
          "Hai Liu",
          "Zhifei Li",
          "Shuyun Han",
          "Erqi Zhang"
        ],
        "published_date": "2025",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/6c195ec2d5a491ffca9ab893968c4d44a6d0ce7d.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 6,
        "score": 6.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "6c195ec2d5a491ffca9ab893968c4d44a6d0ce7d.pdf"
    },
    {
      "success": true,
      "doc_id": "f387dd8831aaf5fa1b9dbbec9752a578",
      "summary": "Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion tasks. Existing KGE models typically map entities and relations into a unified continuous vector space and define a score function to capture the connectivity patterns among the elements (entities and relations) of facts. The score on a fact measures its plausibility in a knowledge graph (KG). However, since the connectivity patterns are very complex in a real knowledge graph, it is difficult to define an explicit and efficient score function to capture them, which also limits their performance. This paper argues that plausible facts in a knowledge graph come from a distribution in the low-dimensional fact space. Inspired by this insight, this paper proposes a novel framework called Fact Embedding through Diffusion Model (FDM) to address the knowledge graph completion task. Instead of defining a score function to measure the plausibility of facts in a knowledge graph, this framework directly learns the distribution of plausible facts from the known knowledge graph and casts the entity prediction task into the conditional fact generation task. Specifically, we concatenate the elements embedding in a fact as a whole and take it as input. Then, we introduce a Conditional Fact Denoiser to learn the reverse denoising diffusion process and generate the target fact embedding from noised data. Extensive experiments demonstrate that FDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.",
      "intriguing_abstract": "Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion tasks. Existing KGE models typically map entities and relations into a unified continuous vector space and define a score function to capture the connectivity patterns among the elements (entities and relations) of facts. The score on a fact measures its plausibility in a knowledge graph (KG). However, since the connectivity patterns are very complex in a real knowledge graph, it is difficult to define an explicit and efficient score function to capture them, which also limits their performance. This paper argues that plausible facts in a knowledge graph come from a distribution in the low-dimensional fact space. Inspired by this insight, this paper proposes a novel framework called Fact Embedding through Diffusion Model (FDM) to address the knowledge graph completion task. Instead of defining a score function to measure the plausibility of facts in a knowledge graph, this framework directly learns the distribution of plausible facts from the known knowledge graph and casts the entity prediction task into the conditional fact generation task. Specifically, we concatenate the elements embedding in a fact as a whole and take it as input. Then, we introduce a Conditional Fact Denoiser to learn the reverse denoising diffusion process and generate the target fact embedding from noised data. Extensive experiments demonstrate that FDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/37b274eb6fa68dede9f4aaad6dec1e2ea56095ce.pdf",
      "citation_key": "long20248vt",
      "metadata": {
        "title": "Fact Embedding through Diffusion Model for Knowledge Graph Completion",
        "authors": [
          "Xiao Long",
          "Liansheng Zhuang",
          "Aodi Li",
          "Houqiang Li",
          "Shafei Wang"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion tasks. Existing KGE models typically map entities and relations into a unified continuous vector space and define a score function to capture the connectivity patterns among the elements (entities and relations) of facts. The score on a fact measures its plausibility in a knowledge graph (KG). However, since the connectivity patterns are very complex in a real knowledge graph, it is difficult to define an explicit and efficient score function to capture them, which also limits their performance. This paper argues that plausible facts in a knowledge graph come from a distribution in the low-dimensional fact space. Inspired by this insight, this paper proposes a novel framework called Fact Embedding through Diffusion Model (FDM) to address the knowledge graph completion task. Instead of defining a score function to measure the plausibility of facts in a knowledge graph, this framework directly learns the distribution of plausible facts from the known knowledge graph and casts the entity prediction task into the conditional fact generation task. Specifically, we concatenate the elements embedding in a fact as a whole and take it as input. Then, we introduce a Conditional Fact Denoiser to learn the reverse denoising diffusion process and generate the target fact embedding from noised data. Extensive experiments demonstrate that FDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.",
        "file_path": "paper_data/knowledge_graph_embedding/info/37b274eb6fa68dede9f4aaad6dec1e2ea56095ce.pdf",
        "venue": "The Web Conference",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Knowledge graph embedding (KGE) is an efficient and scalable method for knowledge graph completion tasks. Existing KGE models typically map entities and relations into a unified continuous vector space and define a score function to capture the connectivity patterns among the elements (entities and relations) of facts. The score on a fact measures its plausibility in a knowledge graph (KG). However, since the connectivity patterns are very complex in a real knowledge graph, it is difficult to define an explicit and efficient score function to capture them, which also limits their performance. This paper argues that plausible facts in a knowledge graph come from a distribution in the low-dimensional fact space. Inspired by this insight, this paper proposes a novel framework called Fact Embedding through Diffusion Model (FDM) to address the knowledge graph completion task. Instead of defining a score function to measure the plausibility of facts in a knowledge graph, this framework directly learns the distribution of plausible facts from the known knowledge graph and casts the entity prediction task into the conditional fact generation task. Specifically, we concatenate the elements embedding in a fact as a whole and take it as input. Then, we introduce a Conditional Fact Denoiser to learn the reverse denoising diffusion process and generate the target fact embedding from noised data. Extensive experiments demonstrate that FDM significantly outperforms existing state-of-the-art methods in three benchmark datasets.",
        "keywords": []
      },
      "file_name": "37b274eb6fa68dede9f4aaad6dec1e2ea56095ce.pdf"
    },
    {
      "success": true,
      "doc_id": "1d9d51e7a5b4e34dd340276984dd42fc",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9be88067bd7351b36bb0c698f5559ced3918a1d5.pdf",
      "citation_key": "huang20240su",
      "metadata": {
        "title": "Knowledge graph confidence-aware embedding for recommendation",
        "authors": [
          "Chen Huang",
          "Fei Yu",
          "Zhiguo Wan",
          "Fengying Li",
          "Hui Ji",
          "Yuandi Li"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/9be88067bd7351b36bb0c698f5559ced3918a1d5.pdf",
        "venue": "Neural Networks",
        "citationCount": 6,
        "score": 6.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "9be88067bd7351b36bb0c698f5559ced3918a1d5.pdf"
    },
    {
      "success": true,
      "doc_id": "0f8ed7ae66b656acf873cf1b583b070e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/e0d17f8b2fffff6c5eaf3f13bc45126196ddd128.pdf",
      "citation_key": "wang2024nej",
      "metadata": {
        "title": "GFedKG: GNN-based federated embedding model for knowledge graph completion",
        "authors": [
          "Yuzhuo Wang",
          "Hongzhi Wang",
          "Xianglong Liu",
          "Yu Yan"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/e0d17f8b2fffff6c5eaf3f13bc45126196ddd128.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 6,
        "score": 6.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "e0d17f8b2fffff6c5eaf3f13bc45126196ddd128.pdf"
    },
    {
      "success": true,
      "doc_id": "0d3f61d16bebbe1ab9a82b9d48eb3078",
      "summary": "Abstract The accurate identification of disease-associated genes is crucial for understanding the molecular mechanisms underlying various diseases. Most current methods focus on constructing biological networks and utilizing machine learning, particularly deep learning, to identify disease genes. However, these methods overlook complex relations among entities in biological knowledge graphs. Such information has been successfully applied in other areas of life science research, demonstrating their effectiveness. Knowledge graph embedding methods can learn the semantic information of different relations within the knowledge graphs. Nonetheless, the performance of existing representation learning techniques, when applied to domain-specific biological data, remains suboptimal. To solve these problems, we construct a biological knowledge graph centered on diseases and genes, and develop an end-to-end knowledge graph completion framework for disease gene prediction using interactional tensor decomposition named KDGene. KDGene incorporates an interaction module that bridges entity and relation embeddings within tensor decomposition, aiming to improve the representation of semantically similar concepts in specific domains and enhance the ability to accurately predict disease genes. Experimental results show that KDGene significantly outperforms state-of-the-art algorithms, whether existing disease gene prediction methods or knowledge graph embedding methods for general domains. Moreover, the comprehensive biological analysis of the predicted results further validates KDGenes capability to accurately identify new candidate genes. This work proposes a scalable knowledge graph completion framework to identify disease candidate genes, from which the results are promising to provide valuable references for further wet experiments. Data and source codes are available at https://github.com/2020MEAI/KDGene.",
      "intriguing_abstract": "Abstract The accurate identification of disease-associated genes is crucial for understanding the molecular mechanisms underlying various diseases. Most current methods focus on constructing biological networks and utilizing machine learning, particularly deep learning, to identify disease genes. However, these methods overlook complex relations among entities in biological knowledge graphs. Such information has been successfully applied in other areas of life science research, demonstrating their effectiveness. Knowledge graph embedding methods can learn the semantic information of different relations within the knowledge graphs. Nonetheless, the performance of existing representation learning techniques, when applied to domain-specific biological data, remains suboptimal. To solve these problems, we construct a biological knowledge graph centered on diseases and genes, and develop an end-to-end knowledge graph completion framework for disease gene prediction using interactional tensor decomposition named KDGene. KDGene incorporates an interaction module that bridges entity and relation embeddings within tensor decomposition, aiming to improve the representation of semantically similar concepts in specific domains and enhance the ability to accurately predict disease genes. Experimental results show that KDGene significantly outperforms state-of-the-art algorithms, whether existing disease gene prediction methods or knowledge graph embedding methods for general domains. Moreover, the comprehensive biological analysis of the predicted results further validates KDGenes capability to accurately identify new candidate genes. This work proposes a scalable knowledge graph completion framework to identify disease candidate genes, from which the results are promising to provide valuable references for further wet experiments. Data and source codes are available at https://github.com/2020MEAI/KDGene.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a4b6e13efa80bedf8e588ac69f91fdaecc8e5077.pdf",
      "citation_key": "wang2024c8z",
      "metadata": {
        "title": "KDGene: knowledge graph completion for disease gene prediction using interactional tensor decomposition",
        "authors": [
          "Xinyan Wang",
          "Kuo Yang",
          "Ting Jia",
          "Fanghui Gu",
          "Chongyu Wang",
          "Kuan Xu",
          "Zixin Shu",
          "Jianan Xia",
          "Qiang Zhu",
          "Xuezhong Zhou"
        ],
        "published_date": "2024",
        "abstract": "Abstract The accurate identification of disease-associated genes is crucial for understanding the molecular mechanisms underlying various diseases. Most current methods focus on constructing biological networks and utilizing machine learning, particularly deep learning, to identify disease genes. However, these methods overlook complex relations among entities in biological knowledge graphs. Such information has been successfully applied in other areas of life science research, demonstrating their effectiveness. Knowledge graph embedding methods can learn the semantic information of different relations within the knowledge graphs. Nonetheless, the performance of existing representation learning techniques, when applied to domain-specific biological data, remains suboptimal. To solve these problems, we construct a biological knowledge graph centered on diseases and genes, and develop an end-to-end knowledge graph completion framework for disease gene prediction using interactional tensor decomposition named KDGene. KDGene incorporates an interaction module that bridges entity and relation embeddings within tensor decomposition, aiming to improve the representation of semantically similar concepts in specific domains and enhance the ability to accurately predict disease genes. Experimental results show that KDGene significantly outperforms state-of-the-art algorithms, whether existing disease gene prediction methods or knowledge graph embedding methods for general domains. Moreover, the comprehensive biological analysis of the predicted results further validates KDGenes capability to accurately identify new candidate genes. This work proposes a scalable knowledge graph completion framework to identify disease candidate genes, from which the results are promising to provide valuable references for further wet experiments. Data and source codes are available at https://github.com/2020MEAI/KDGene.",
        "file_path": "paper_data/knowledge_graph_embedding/info/a4b6e13efa80bedf8e588ac69f91fdaecc8e5077.pdf",
        "venue": "Briefings Bioinform.",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Abstract The accurate identification of disease-associated genes is crucial for understanding the molecular mechanisms underlying various diseases. Most current methods focus on constructing biological networks and utilizing machine learning, particularly deep learning, to identify disease genes. However, these methods overlook complex relations among entities in biological knowledge graphs. Such information has been successfully applied in other areas of life science research, demonstrating their effectiveness. Knowledge graph embedding methods can learn the semantic information of different relations within the knowledge graphs. Nonetheless, the performance of existing representation learning techniques, when applied to domain-specific biological data, remains suboptimal. To solve these problems, we construct a biological knowledge graph centered on diseases and genes, and develop an end-to-end knowledge graph completion framework for disease gene prediction using interactional tensor decomposition named KDGene. KDGene incorporates an interaction module that bridges entity and relation embeddings within tensor decomposition, aiming to improve the representation of semantically similar concepts in specific domains and enhance the ability to accurately predict disease genes. Experimental results show that KDGene significantly outperforms state-of-the-art algorithms, whether existing disease gene prediction methods or knowledge graph embedding methods for general domains. Moreover, the comprehensive biological analysis of the predicted results further validates KDGenes capability to accurately identify new candidate genes. This work proposes a scalable knowledge graph completion framework to identify disease candidate genes, from which the results are promising to provide valuable references for further wet experiments. Data and source codes are available at https://github.com/2020MEAI/KDGene.",
        "keywords": []
      },
      "file_name": "a4b6e13efa80bedf8e588ac69f91fdaecc8e5077.pdf"
    },
    {
      "success": true,
      "doc_id": "bdf567935d0176b261534dfa8c4764a9",
      "summary": "Knowledge Graph Embedding (KGE) is a critical field aiming to transform the elements of knowledge graphs (KGs) into continuous spaces, offering great potential for structured data representation. In contemporary KGE research, the utilization of either hyperbolic or Euclidean space for knowledge graph Embedding is a common practice. However, knowledge graphs encompass diverse geometric data structures, including chains and hierarchies, whose hybrid nature exceeds the capacity of a single embedding space to capture effectively. This paper introduces a novel and highly effective approach called Unified Geometry Knowledge Graph Embedding (UniGE) to address the challenge of representing diverse geometric data in KGs. UniGE stands out as a novel KGE method that seamlessly integrates KGE in both Euclidean and hyperbolic geometric spaces. We introduce an embedding alignment method and fusion strategy, which harnesses optimal transport techniques and the Wasserstein barycenter method. Furthermore, we offer a comprehensive theoretical analysis to substantiate the superiority of our approach, as evident from a more robust error bound. To substantiate the strength of UniGE, we conducted comprehensive experiments on three benchmark datasets. The results consistently demonstrate that UniGE outperforms state-of-the-art methods, aligning with the conclusions drawn from our theoretical analysis.",
      "intriguing_abstract": "Knowledge Graph Embedding (KGE) is a critical field aiming to transform the elements of knowledge graphs (KGs) into continuous spaces, offering great potential for structured data representation. In contemporary KGE research, the utilization of either hyperbolic or Euclidean space for knowledge graph Embedding is a common practice. However, knowledge graphs encompass diverse geometric data structures, including chains and hierarchies, whose hybrid nature exceeds the capacity of a single embedding space to capture effectively. This paper introduces a novel and highly effective approach called Unified Geometry Knowledge Graph Embedding (UniGE) to address the challenge of representing diverse geometric data in KGs. UniGE stands out as a novel KGE method that seamlessly integrates KGE in both Euclidean and hyperbolic geometric spaces. We introduce an embedding alignment method and fusion strategy, which harnesses optimal transport techniques and the Wasserstein barycenter method. Furthermore, we offer a comprehensive theoretical analysis to substantiate the superiority of our approach, as evident from a more robust error bound. To substantiate the strength of UniGE, we conducted comprehensive experiments on three benchmark datasets. The results consistently demonstrate that UniGE outperforms state-of-the-art methods, aligning with the conclusions drawn from our theoretical analysis.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/ccb6674576de48f8cfd99374c3b737a94dc3cb98.pdf",
      "citation_key": "liu2024x0k",
      "metadata": {
        "title": "Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport",
        "authors": [
          "Yuhan Liu",
          "Zelin Cao",
          "Xing Gao",
          "Ji Zhang",
          "Rui Yan"
        ],
        "published_date": "2024",
        "abstract": "Knowledge Graph Embedding (KGE) is a critical field aiming to transform the elements of knowledge graphs (KGs) into continuous spaces, offering great potential for structured data representation. In contemporary KGE research, the utilization of either hyperbolic or Euclidean space for knowledge graph Embedding is a common practice. However, knowledge graphs encompass diverse geometric data structures, including chains and hierarchies, whose hybrid nature exceeds the capacity of a single embedding space to capture effectively. This paper introduces a novel and highly effective approach called Unified Geometry Knowledge Graph Embedding (UniGE) to address the challenge of representing diverse geometric data in KGs. UniGE stands out as a novel KGE method that seamlessly integrates KGE in both Euclidean and hyperbolic geometric spaces. We introduce an embedding alignment method and fusion strategy, which harnesses optimal transport techniques and the Wasserstein barycenter method. Furthermore, we offer a comprehensive theoretical analysis to substantiate the superiority of our approach, as evident from a more robust error bound. To substantiate the strength of UniGE, we conducted comprehensive experiments on three benchmark datasets. The results consistently demonstrate that UniGE outperforms state-of-the-art methods, aligning with the conclusions drawn from our theoretical analysis.",
        "file_path": "paper_data/knowledge_graph_embedding/info/ccb6674576de48f8cfd99374c3b737a94dc3cb98.pdf",
        "venue": "The Web Conference",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Knowledge Graph Embedding (KGE) is a critical field aiming to transform the elements of knowledge graphs (KGs) into continuous spaces, offering great potential for structured data representation. In contemporary KGE research, the utilization of either hyperbolic or Euclidean space for knowledge graph Embedding is a common practice. However, knowledge graphs encompass diverse geometric data structures, including chains and hierarchies, whose hybrid nature exceeds the capacity of a single embedding space to capture effectively. This paper introduces a novel and highly effective approach called Unified Geometry Knowledge Graph Embedding (UniGE) to address the challenge of representing diverse geometric data in KGs. UniGE stands out as a novel KGE method that seamlessly integrates KGE in both Euclidean and hyperbolic geometric spaces. We introduce an embedding alignment method and fusion strategy, which harnesses optimal transport techniques and the Wasserstein barycenter method. Furthermore, we offer a comprehensive theoretical analysis to substantiate the superiority of our approach, as evident from a more robust error bound. To substantiate the strength of UniGE, we conducted comprehensive experiments on three benchmark datasets. The results consistently demonstrate that UniGE outperforms state-of-the-art methods, aligning with the conclusions drawn from our theoretical analysis.",
        "keywords": []
      },
      "file_name": "ccb6674576de48f8cfd99374c3b737a94dc3cb98.pdf"
    },
    {
      "success": true,
      "doc_id": "63ac22a84ad12ed0e594d925d7fe538d",
      "summary": "The purpose of representation learning is to encode the entities and relations in a knowledge graph as low-dimensional and real-valued vectors through machine learning technology. Traditional representation learning methods like TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of a graphs entities, are effective for learning the embeddings of knowledge bases, but struggle to effectively model complex relations like one-to-many, many-to-one, and many-to-many. To overcome the above issues, we introduce a new method for knowledge representation, reasoning, and completion based on multi-translation principles and TransE (TransE-MTP). By defining multiple translation principles (MTPs) for different relation types, such as one-to-one and complex relations like one-to-many, many-to-one, and many-to-many, and combining MTPs with a typical translating-based model for modeling multi-relational data (TransE), the proposed method, TransE-MTP, ensures that multiple optimization objectives can be targeted and optimized during training on complex relations, thereby providing superior prediction performance. We implement a prototype of TransE-MTP to demonstrate its effectiveness at link prediction and triplet classification on two prominent knowledge graph datasets: Freebase and Wordnet. Our experimental results show that the proposed method enhanced the performance of both TransE and knowledge graph embedding by translating on hyperplanes (TransH), which confirms its effectiveness and competitiveness.",
      "intriguing_abstract": "The purpose of representation learning is to encode the entities and relations in a knowledge graph as low-dimensional and real-valued vectors through machine learning technology. Traditional representation learning methods like TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of a graphs entities, are effective for learning the embeddings of knowledge bases, but struggle to effectively model complex relations like one-to-many, many-to-one, and many-to-many. To overcome the above issues, we introduce a new method for knowledge representation, reasoning, and completion based on multi-translation principles and TransE (TransE-MTP). By defining multiple translation principles (MTPs) for different relation types, such as one-to-one and complex relations like one-to-many, many-to-one, and many-to-many, and combining MTPs with a typical translating-based model for modeling multi-relational data (TransE), the proposed method, TransE-MTP, ensures that multiple optimization objectives can be targeted and optimized during training on complex relations, thereby providing superior prediction performance. We implement a prototype of TransE-MTP to demonstrate its effectiveness at link prediction and triplet classification on two prominent knowledge graph datasets: Freebase and Wordnet. Our experimental results show that the proposed method enhanced the performance of both TransE and knowledge graph embedding by translating on hyperplanes (TransH), which confirms its effectiveness and competitiveness.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf",
      "citation_key": "li2024uio",
      "metadata": {
        "title": "TransE-MTP: A New Representation Learning Method for Knowledge Graph Embedding with Multi-Translation Principles and TransE",
        "authors": [
          "Yongfang Li",
          "Chunhua Zhu"
        ],
        "published_date": "2024",
        "abstract": "The purpose of representation learning is to encode the entities and relations in a knowledge graph as low-dimensional and real-valued vectors through machine learning technology. Traditional representation learning methods like TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of a graphs entities, are effective for learning the embeddings of knowledge bases, but struggle to effectively model complex relations like one-to-many, many-to-one, and many-to-many. To overcome the above issues, we introduce a new method for knowledge representation, reasoning, and completion based on multi-translation principles and TransE (TransE-MTP). By defining multiple translation principles (MTPs) for different relation types, such as one-to-one and complex relations like one-to-many, many-to-one, and many-to-many, and combining MTPs with a typical translating-based model for modeling multi-relational data (TransE), the proposed method, TransE-MTP, ensures that multiple optimization objectives can be targeted and optimized during training on complex relations, thereby providing superior prediction performance. We implement a prototype of TransE-MTP to demonstrate its effectiveness at link prediction and triplet classification on two prominent knowledge graph datasets: Freebase and Wordnet. Our experimental results show that the proposed method enhanced the performance of both TransE and knowledge graph embedding by translating on hyperplanes (TransH), which confirms its effectiveness and competitiveness.",
        "file_path": "paper_data/knowledge_graph_embedding/info/75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf",
        "venue": "Electronics",
        "citationCount": 5,
        "score": 5.0,
        "summary": "The purpose of representation learning is to encode the entities and relations in a knowledge graph as low-dimensional and real-valued vectors through machine learning technology. Traditional representation learning methods like TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of a graphs entities, are effective for learning the embeddings of knowledge bases, but struggle to effectively model complex relations like one-to-many, many-to-one, and many-to-many. To overcome the above issues, we introduce a new method for knowledge representation, reasoning, and completion based on multi-translation principles and TransE (TransE-MTP). By defining multiple translation principles (MTPs) for different relation types, such as one-to-one and complex relations like one-to-many, many-to-one, and many-to-many, and combining MTPs with a typical translating-based model for modeling multi-relational data (TransE), the proposed method, TransE-MTP, ensures that multiple optimization objectives can be targeted and optimized during training on complex relations, thereby providing superior prediction performance. We implement a prototype of TransE-MTP to demonstrate its effectiveness at link prediction and triplet classification on two prominent knowledge graph datasets: Freebase and Wordnet. Our experimental results show that the proposed method enhanced the performance of both TransE and knowledge graph embedding by translating on hyperplanes (TransH), which confirms its effectiveness and competitiveness.",
        "keywords": []
      },
      "file_name": "75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf"
    },
    {
      "success": true,
      "doc_id": "1e7ecf68ffbed4633c96edde54c03f25",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/8ff387296878f23632a588076823b160673866ab.pdf",
      "citation_key": "zhang2024z78",
      "metadata": {
        "title": "Knowledge graph embedding with inverse function representation for link prediction",
        "authors": [
          "Qianjin Zhang",
          "Yandan Xu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/8ff387296878f23632a588076823b160673866ab.pdf",
        "venue": "Engineering applications of artificial intelligence",
        "citationCount": 5,
        "score": 5.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "8ff387296878f23632a588076823b160673866ab.pdf"
    },
    {
      "success": true,
      "doc_id": "62f79b7ce4d45b80f9a1b4aa5f057d0b",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6a66b459955959c4b8a67bd298ed291506923b7a.pdf",
      "citation_key": "wang2024534",
      "metadata": {
        "title": "A collaborative learning framework for knowledge graph embedding and reasoning",
        "authors": [
          "Hao Wang",
          "Dandan Song",
          "Zhijing Wu",
          "Jia Li",
          "Yanru Zhou",
          "Jing Xu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/6a66b459955959c4b8a67bd298ed291506923b7a.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 5,
        "score": 5.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "6a66b459955959c4b8a67bd298ed291506923b7a.pdf"
    },
    {
      "success": true,
      "doc_id": "4e2ffeed00b44294f262ad3575f0213d",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6b69c8848a1cc50ed8775beb483c71cfc314c66b.pdf",
      "citation_key": "ni202438q",
      "metadata": {
        "title": "Identifying compound-protein interactions with knowledge graph embedding of perturbation transcriptomics",
        "authors": [
          "Shengkun Ni",
          "Xiangtai Kong",
          "Yingying Zhang",
          "Zhengyang Chen",
          "Zhaokun Wang",
          "Zunyun Fu",
          "Ruifeng Huo",
          "X. Tong",
          "Ning Qu",
          "Xiaolong Wu",
          "Kun Wang",
          "Wei Zhang",
          "Runze Zhang",
          "Zimei Zhang",
          "Jiangshan Shi",
          "Yitian Wang",
          "Ruirui Yang",
          "Xutong Li",
          "Sulin Zhang",
          "Mingyue Zheng"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/6b69c8848a1cc50ed8775beb483c71cfc314c66b.pdf",
        "venue": "Cell Genomics",
        "citationCount": 5,
        "score": 5.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "6b69c8848a1cc50ed8775beb483c71cfc314c66b.pdf"
    },
    {
      "success": true,
      "doc_id": "081ebc64e108312330c8653e91cadbd7",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/d57e01d80c7f0f86b5e3f096b193ab9210e9095f.pdf",
      "citation_key": "nie202499i",
      "metadata": {
        "title": "Knowledge Graph Efficient Construction: Embedding Chain-of-Thought into LLMs",
        "authors": [
          "Jixuan Nie",
          "Xia Hou",
          "Wenfeng Song",
          "Xuan Wang",
          "Xingliang Jin",
          "Xinyu Zhang",
          "ShuoZhe Zhang",
          "Jiaqi Shi"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/d57e01d80c7f0f86b5e3f096b193ab9210e9095f.pdf",
        "venue": "VLDB Workshops",
        "citationCount": 5,
        "score": 5.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "d57e01d80c7f0f86b5e3f096b193ab9210e9095f.pdf"
    },
    {
      "success": true,
      "doc_id": "7d5cf37f496891b646453961b32bd550",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a9bfb9ab236553768782f2b90a69c5625f033186.pdf",
      "citation_key": "wang2024d52",
      "metadata": {
        "title": "ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion",
        "authors": [
          "Jingchao Wang",
          "Weimin Li",
          "Fangfang Liu",
          "ZhenHai Wang",
          "A. M. Luvembe",
          "Qun Jin",
          "Quan-ke Pan",
          "Fangyu Liu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/a9bfb9ab236553768782f2b90a69c5625f033186.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 5,
        "score": 5.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "a9bfb9ab236553768782f2b90a69c5625f033186.pdf"
    },
    {
      "success": true,
      "doc_id": "8caa159f109765c9c020ba029eae1826",
      "summary": "Graphs in real-world applications usually evolve constantly presenting dynamic behaviors such as social networks and transportation networks. Hence, dynamic graph embedding has gained much attention recently. In dynamic graphs, both the topology and node attributes could change over time, which pose great challenges for developing effective embedding models. Typically, the evolution process of a dynamic graph can be recorded as a series of snapshots. We observe that the evolution process inherently provides both prior information (previous snapshots) and validation information (the next snapshot). The prior information can be used to fit the evolution process, while the validation information can be used to improve the generalization ability of a graph embedding model. However, existing dynamic graph embedding models only utilize the prior information, but overlook the validation information. To tackle this issue, this paper proposes a novel dynamic graph embedding method via Model-Agnostic Meta-Learning, which utilizes both kinds of information to obtain better graph representation. The extensive experiments on eight real-world datasets demonstrate the superiority of our proposed method over state-of-the-art methods on various graph analysis tasks.",
      "intriguing_abstract": "Graphs in real-world applications usually evolve constantly presenting dynamic behaviors such as social networks and transportation networks. Hence, dynamic graph embedding has gained much attention recently. In dynamic graphs, both the topology and node attributes could change over time, which pose great challenges for developing effective embedding models. Typically, the evolution process of a dynamic graph can be recorded as a series of snapshots. We observe that the evolution process inherently provides both prior information (previous snapshots) and validation information (the next snapshot). The prior information can be used to fit the evolution process, while the validation information can be used to improve the generalization ability of a graph embedding model. However, existing dynamic graph embedding models only utilize the prior information, but overlook the validation information. To tackle this issue, this paper proposes a novel dynamic graph embedding method via Model-Agnostic Meta-Learning, which utilizes both kinds of information to obtain better graph representation. The extensive experiments on eight real-world datasets demonstrate the superiority of our proposed method over state-of-the-art methods on various graph analysis tasks.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6903aea3553a449257388580028e0bddf119d021.pdf",
      "citation_key": "mao2024v2s",
      "metadata": {
        "title": "Dynamic Graph Embedding via Meta-Learning",
        "authors": [
          "Yuren Mao",
          "Yu Hao",
          "Xin Cao",
          "Yixiang Fang",
          "Xuemin Lin",
          "Hua Mao",
          "Zhiqiang Xu"
        ],
        "published_date": "2024",
        "abstract": "Graphs in real-world applications usually evolve constantly presenting dynamic behaviors such as social networks and transportation networks. Hence, dynamic graph embedding has gained much attention recently. In dynamic graphs, both the topology and node attributes could change over time, which pose great challenges for developing effective embedding models. Typically, the evolution process of a dynamic graph can be recorded as a series of snapshots. We observe that the evolution process inherently provides both prior information (previous snapshots) and validation information (the next snapshot). The prior information can be used to fit the evolution process, while the validation information can be used to improve the generalization ability of a graph embedding model. However, existing dynamic graph embedding models only utilize the prior information, but overlook the validation information. To tackle this issue, this paper proposes a novel dynamic graph embedding method via Model-Agnostic Meta-Learning, which utilizes both kinds of information to obtain better graph representation. The extensive experiments on eight real-world datasets demonstrate the superiority of our proposed method over state-of-the-art methods on various graph analysis tasks.",
        "file_path": "paper_data/knowledge_graph_embedding/info/6903aea3553a449257388580028e0bddf119d021.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Graphs in real-world applications usually evolve constantly presenting dynamic behaviors such as social networks and transportation networks. Hence, dynamic graph embedding has gained much attention recently. In dynamic graphs, both the topology and node attributes could change over time, which pose great challenges for developing effective embedding models. Typically, the evolution process of a dynamic graph can be recorded as a series of snapshots. We observe that the evolution process inherently provides both prior information (previous snapshots) and validation information (the next snapshot). The prior information can be used to fit the evolution process, while the validation information can be used to improve the generalization ability of a graph embedding model. However, existing dynamic graph embedding models only utilize the prior information, but overlook the validation information. To tackle this issue, this paper proposes a novel dynamic graph embedding method via Model-Agnostic Meta-Learning, which utilizes both kinds of information to obtain better graph representation. The extensive experiments on eight real-world datasets demonstrate the superiority of our proposed method over state-of-the-art methods on various graph analysis tasks.",
        "keywords": []
      },
      "file_name": "6903aea3553a449257388580028e0bddf119d021.pdf"
    },
    {
      "success": true,
      "doc_id": "dfc00761a468c939252b74b8fce4f780",
      "summary": "Factoid entity questions (FEQ), which seek answers in the form of a single entity from knowledge sources, such as DBpedia and Wikidata, constitute a substantial portion of user queries in search engines. This article introduces the knowledge graph embedding model for FEQ (KGE-FEQ) answering. Leveraging a textual knowledge graph derived from extensive text collections, KGE-FEQ encodes textual relationships between entities. The model employs a two-step process: (1) Triple Retrieval, where relevant triples are retrieved from the textual knowledge graph based on semantic similarities to the question, and (2) Answer Selection, where a knowledge graph embedding approach is utilized for answering the question. This involves positioning the embedding for the answer entity close to the embedding of the question entity, incorporating a vector representing the question and textual relations between entities. Extensive experiments evaluate the performance of the proposed approach, comparing KGE-FEQ to state-of-the-art baselines in FEQ answering and the most advanced open-domain question answering techniques applied to FEQs. The results show that KGE-FEQ outperforms existing methods across different datasets. Ablation studies highlights the effectiveness of KGE-FEQ when both the question and textual relations between entities are considered for answering questions.",
      "intriguing_abstract": "Factoid entity questions (FEQ), which seek answers in the form of a single entity from knowledge sources, such as DBpedia and Wikidata, constitute a substantial portion of user queries in search engines. This article introduces the knowledge graph embedding model for FEQ (KGE-FEQ) answering. Leveraging a textual knowledge graph derived from extensive text collections, KGE-FEQ encodes textual relationships between entities. The model employs a two-step process: (1) Triple Retrieval, where relevant triples are retrieved from the textual knowledge graph based on semantic similarities to the question, and (2) Answer Selection, where a knowledge graph embedding approach is utilized for answering the question. This involves positioning the embedding for the answer entity close to the embedding of the question entity, incorporating a vector representing the question and textual relations between entities. Extensive experiments evaluate the performance of the proposed approach, comparing KGE-FEQ to state-of-the-art baselines in FEQ answering and the most advanced open-domain question answering techniques applied to FEQs. The results show that KGE-FEQ outperforms existing methods across different datasets. Ablation studies highlights the effectiveness of KGE-FEQ when both the question and textual relations between entities are considered for answering questions.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/767d56fe80f7681b97943a8bff39f0b580e4acd8.pdf",
      "citation_key": "jafarzadeh202468v",
      "metadata": {
        "title": "A Knowledge Graph Embedding Model for Answering Factoid Entity Questions",
        "authors": [
          "Parastoo Jafarzadeh",
          "F. Ensan",
          "Mahdiyar Ali Akbar Alavi",
          "Fattane Zarrinkalam"
        ],
        "published_date": "2024",
        "abstract": "Factoid entity questions (FEQ), which seek answers in the form of a single entity from knowledge sources, such as DBpedia and Wikidata, constitute a substantial portion of user queries in search engines. This article introduces the knowledge graph embedding model for FEQ (KGE-FEQ) answering. Leveraging a textual knowledge graph derived from extensive text collections, KGE-FEQ encodes textual relationships between entities. The model employs a two-step process: (1) Triple Retrieval, where relevant triples are retrieved from the textual knowledge graph based on semantic similarities to the question, and (2) Answer Selection, where a knowledge graph embedding approach is utilized for answering the question. This involves positioning the embedding for the answer entity close to the embedding of the question entity, incorporating a vector representing the question and textual relations between entities. Extensive experiments evaluate the performance of the proposed approach, comparing KGE-FEQ to state-of-the-art baselines in FEQ answering and the most advanced open-domain question answering techniques applied to FEQs. The results show that KGE-FEQ outperforms existing methods across different datasets. Ablation studies highlights the effectiveness of KGE-FEQ when both the question and textual relations between entities are considered for answering questions.",
        "file_path": "paper_data/knowledge_graph_embedding/info/767d56fe80f7681b97943a8bff39f0b580e4acd8.pdf",
        "venue": "ACM Trans. Inf. Syst.",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Factoid entity questions (FEQ), which seek answers in the form of a single entity from knowledge sources, such as DBpedia and Wikidata, constitute a substantial portion of user queries in search engines. This article introduces the knowledge graph embedding model for FEQ (KGE-FEQ) answering. Leveraging a textual knowledge graph derived from extensive text collections, KGE-FEQ encodes textual relationships between entities. The model employs a two-step process: (1) Triple Retrieval, where relevant triples are retrieved from the textual knowledge graph based on semantic similarities to the question, and (2) Answer Selection, where a knowledge graph embedding approach is utilized for answering the question. This involves positioning the embedding for the answer entity close to the embedding of the question entity, incorporating a vector representing the question and textual relations between entities. Extensive experiments evaluate the performance of the proposed approach, comparing KGE-FEQ to state-of-the-art baselines in FEQ answering and the most advanced open-domain question answering techniques applied to FEQs. The results show that KGE-FEQ outperforms existing methods across different datasets. Ablation studies highlights the effectiveness of KGE-FEQ when both the question and textual relations between entities are considered for answering questions.",
        "keywords": []
      },
      "file_name": "767d56fe80f7681b97943a8bff39f0b580e4acd8.pdf"
    },
    {
      "success": true,
      "doc_id": "b30e47c0a5a54f7441ddbe8ed4e043ba",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9e7799ef313143aa9c0669a7d1918fcfd5d21359.pdf",
      "citation_key": "wang2024dea",
      "metadata": {
        "title": "Enhancing knowledge graph embedding with structure and semantic features",
        "authors": [
          "Yalin Wang",
          "Yubin Peng",
          "Jingyu Guo"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/9e7799ef313143aa9c0669a7d1918fcfd5d21359.pdf",
        "venue": "Applied intelligence (Boston)",
        "citationCount": 4,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "9e7799ef313143aa9c0669a7d1918fcfd5d21359.pdf"
    },
    {
      "success": true,
      "doc_id": "2f69dbe7e1be77154ace79191ad50b34",
      "summary": ",",
      "intriguing_abstract": ",",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/563b3d57927b688e59322dbbfc973e5f1b269584.pdf",
      "citation_key": "lu202436n",
      "metadata": {
        "title": "HyperCL: A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology",
        "authors": [
          "Yuhuan Lu",
          "Weijian Yu",
          "Xin Jing",
          "Dingqi Yang"
        ],
        "published_date": "2024",
        "abstract": ",",
        "file_path": "paper_data/knowledge_graph_embedding/info/563b3d57927b688e59322dbbfc973e5f1b269584.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 4,
        "score": 4.0,
        "summary": ",",
        "keywords": []
      },
      "file_name": "563b3d57927b688e59322dbbfc973e5f1b269584.pdf"
    },
    {
      "success": true,
      "doc_id": "7d2ddda01ad743bcb7d7fe407751d0cd",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/984c18fa61b10b6d1c34affc98f27ca8344d4224.pdf",
      "citation_key": "han2024gaq",
      "metadata": {
        "title": "A Temporal Knowledge Graph Embedding Model Based on Variable Translation",
        "authors": [
          "Yadan Han",
          "Guangquan Lu",
          "Shichao Zhang",
          "Liang Zhang",
          "Cuifang Zou",
          "Guoqiu Wen"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/984c18fa61b10b6d1c34affc98f27ca8344d4224.pdf",
        "venue": "Tsinghua Science and Technology",
        "citationCount": 4,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "984c18fa61b10b6d1c34affc98f27ca8344d4224.pdf"
    },
    {
      "success": true,
      "doc_id": "cf5de002726820f6b87b309d1771c6a5",
      "summary": "In many fields, such as social networks and recommendation systems with high time requirements, fake news and false information are often released in real time, impacting on peoples daily life. Entity alignment (EA) in temporal knowledge graph (TKG) can fuse the information contained in entities by finding equivalent entities, thus helping to determine the regular pattern of disinformation under time change. The existing methods either ignore the use of temporal attributes information and structural information or the modeling of that is insufficient, which has become a major obstacle to the further and wider application of TKG EA. In this article, we put forward a new idea of training for the processing of time attributes and relational structure information, to further enhance the ability in the EA process of TKGs. By forming box embedding matrix and name embedding matrix, and adaptively fusing the above information, we propose a new TKG EA solution. We carry out comparative experiments on standard news media and social media datasets collected from the real world, which validates the effectiveness of our proposal.",
      "intriguing_abstract": "In many fields, such as social networks and recommendation systems with high time requirements, fake news and false information are often released in real time, impacting on peoples daily life. Entity alignment (EA) in temporal knowledge graph (TKG) can fuse the information contained in entities by finding equivalent entities, thus helping to determine the regular pattern of disinformation under time change. The existing methods either ignore the use of temporal attributes information and structural information or the modeling of that is insufficient, which has become a major obstacle to the further and wider application of TKG EA. In this article, we put forward a new idea of training for the processing of time attributes and relational structure information, to further enhance the ability in the EA process of TKGs. By forming box embedding matrix and name embedding matrix, and adaptively fusing the above information, we propose a new TKG EA solution. We carry out comparative experiments on standard news media and social media datasets collected from the real world, which validates the effectiveness of our proposal.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4a0048f1942a68e7c39adac43588d1604af26fc7.pdf",
      "citation_key": "liu2024jz8",
      "metadata": {
        "title": "Enhancing Temporal Knowledge Graph Alignment in News Domain With Box Embedding",
        "authors": [
          "Bingchen Liu",
          "Shifu Hou",
          "Weiyi Zhong",
          "Xiaoran Zhao",
          "Yuwen Liu",
          "Yihong Yang",
          "Shijun Liu",
          "Li Pan"
        ],
        "published_date": "2024",
        "abstract": "In many fields, such as social networks and recommendation systems with high time requirements, fake news and false information are often released in real time, impacting on peoples daily life. Entity alignment (EA) in temporal knowledge graph (TKG) can fuse the information contained in entities by finding equivalent entities, thus helping to determine the regular pattern of disinformation under time change. The existing methods either ignore the use of temporal attributes information and structural information or the modeling of that is insufficient, which has become a major obstacle to the further and wider application of TKG EA. In this article, we put forward a new idea of training for the processing of time attributes and relational structure information, to further enhance the ability in the EA process of TKGs. By forming box embedding matrix and name embedding matrix, and adaptively fusing the above information, we propose a new TKG EA solution. We carry out comparative experiments on standard news media and social media datasets collected from the real world, which validates the effectiveness of our proposal.",
        "file_path": "paper_data/knowledge_graph_embedding/info/4a0048f1942a68e7c39adac43588d1604af26fc7.pdf",
        "venue": "IEEE Transactions on Computational Social Systems",
        "citationCount": 4,
        "score": 4.0,
        "summary": "In many fields, such as social networks and recommendation systems with high time requirements, fake news and false information are often released in real time, impacting on peoples daily life. Entity alignment (EA) in temporal knowledge graph (TKG) can fuse the information contained in entities by finding equivalent entities, thus helping to determine the regular pattern of disinformation under time change. The existing methods either ignore the use of temporal attributes information and structural information or the modeling of that is insufficient, which has become a major obstacle to the further and wider application of TKG EA. In this article, we put forward a new idea of training for the processing of time attributes and relational structure information, to further enhance the ability in the EA process of TKGs. By forming box embedding matrix and name embedding matrix, and adaptively fusing the above information, we propose a new TKG EA solution. We carry out comparative experiments on standard news media and social media datasets collected from the real world, which validates the effectiveness of our proposal.",
        "keywords": []
      },
      "file_name": "4a0048f1942a68e7c39adac43588d1604af26fc7.pdf"
    },
    {
      "success": true,
      "doc_id": "9985386ebdb983b460e62cd117991207",
      "summary": "Query embedding approaches answer complex logical queries over incomplete knowledge graphs (KGs) by computing and operating on low-dimensional vector representations of entities, relations, and queries. However, current query embedding models heavily rely on excessively parameterized neural networks and cannot explain the knowledge learned from the graph. We propose a novel query embedding method, AConE, which explains the knowledge learned from the graph in the form of $SROI^-$ description logic axioms while being more parameter-efficient than most existing approaches. AConE associates queries to a $SROI^-$ description logic concept. Every $SROI^-$ concept is embedded as a cone in complex vector space, and each $SROI^-$ relation is embedded as a transformation that rotates and scales cones. We show theoretically that AConE can learn $SROI^-$ axioms, and defines an algebra whose operations correspond one to one to $SROI^-$ description logic concept constructs. Our empirical study on multiple query datasets shows that AConE achieves superior results over previous baselines with fewer parameters. Notably on the WN18RR dataset, AConE achieves significant improvement over baseline models. We provide comprehensive analyses showing that the capability to represent axioms positively impacts the results of query answering.",
      "intriguing_abstract": "Query embedding approaches answer complex logical queries over incomplete knowledge graphs (KGs) by computing and operating on low-dimensional vector representations of entities, relations, and queries. However, current query embedding models heavily rely on excessively parameterized neural networks and cannot explain the knowledge learned from the graph. We propose a novel query embedding method, AConE, which explains the knowledge learned from the graph in the form of $SROI^-$ description logic axioms while being more parameter-efficient than most existing approaches. AConE associates queries to a $SROI^-$ description logic concept. Every $SROI^-$ concept is embedded as a cone in complex vector space, and each $SROI^-$ relation is embedded as a transformation that rotates and scales cones. We show theoretically that AConE can learn $SROI^-$ axioms, and defines an algebra whose operations correspond one to one to $SROI^-$ description logic concept constructs. Our empirical study on multiple query datasets shows that AConE achieves superior results over previous baselines with fewer parameters. Notably on the WN18RR dataset, AConE achieves significant improvement over baseline models. We provide comprehensive analyses showing that the capability to represent axioms positively impacts the results of query answering.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/49dfd47177fa3aeab8a6bea82a77ec8bdb93bf1e.pdf",
      "citation_key": "he2024y6o",
      "metadata": {
        "title": "Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning",
        "authors": [
          "Yunjie He",
          "Daniel Hernndez",
          "M. Nayyeri",
          "Bo Xiong",
          "Yuqicheng Zhu",
          "Evgeny Kharlamov",
          "Steffen Staab"
        ],
        "published_date": "2024",
        "abstract": "Query embedding approaches answer complex logical queries over incomplete knowledge graphs (KGs) by computing and operating on low-dimensional vector representations of entities, relations, and queries. However, current query embedding models heavily rely on excessively parameterized neural networks and cannot explain the knowledge learned from the graph. We propose a novel query embedding method, AConE, which explains the knowledge learned from the graph in the form of $SROI^-$ description logic axioms while being more parameter-efficient than most existing approaches. AConE associates queries to a $SROI^-$ description logic concept. Every $SROI^-$ concept is embedded as a cone in complex vector space, and each $SROI^-$ relation is embedded as a transformation that rotates and scales cones. We show theoretically that AConE can learn $SROI^-$ axioms, and defines an algebra whose operations correspond one to one to $SROI^-$ description logic concept constructs. Our empirical study on multiple query datasets shows that AConE achieves superior results over previous baselines with fewer parameters. Notably on the WN18RR dataset, AConE achieves significant improvement over baseline models. We provide comprehensive analyses showing that the capability to represent axioms positively impacts the results of query answering.",
        "file_path": "paper_data/knowledge_graph_embedding/info/49dfd47177fa3aeab8a6bea82a77ec8bdb93bf1e.pdf",
        "venue": "",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Query embedding approaches answer complex logical queries over incomplete knowledge graphs (KGs) by computing and operating on low-dimensional vector representations of entities, relations, and queries. However, current query embedding models heavily rely on excessively parameterized neural networks and cannot explain the knowledge learned from the graph. We propose a novel query embedding method, AConE, which explains the knowledge learned from the graph in the form of $SROI^-$ description logic axioms while being more parameter-efficient than most existing approaches. AConE associates queries to a $SROI^-$ description logic concept. Every $SROI^-$ concept is embedded as a cone in complex vector space, and each $SROI^-$ relation is embedded as a transformation that rotates and scales cones. We show theoretically that AConE can learn $SROI^-$ axioms, and defines an algebra whose operations correspond one to one to $SROI^-$ description logic concept constructs. Our empirical study on multiple query datasets shows that AConE achieves superior results over previous baselines with fewer parameters. Notably on the WN18RR dataset, AConE achieves significant improvement over baseline models. We provide comprehensive analyses showing that the capability to represent axioms positively impacts the results of query answering.",
        "keywords": []
      },
      "file_name": "49dfd47177fa3aeab8a6bea82a77ec8bdb93bf1e.pdf"
    },
    {
      "success": true,
      "doc_id": "8683287535de893d274f960bda68b8fd",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2a5c888b2df4fd8c49aef46ee065422b00b178c0.pdf",
      "citation_key": "fang20243a4",
      "metadata": {
        "title": "Knowledge graph completion with low-dimensional gated hierarchical hyperbolic embedding",
        "authors": [
          "Yan Fang",
          "Xiaodong Liu",
          "Wei Lu",
          "W. Pedrycz",
          "Qi Lang",
          "Jianhua Yang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/2a5c888b2df4fd8c49aef46ee065422b00b178c0.pdf",
        "venue": "Knowledge-Based Systems",
        "citationCount": 4,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "2a5c888b2df4fd8c49aef46ee065422b00b178c0.pdf"
    },
    {
      "success": true,
      "doc_id": "a043011a349e58800a89c2a44087eba3",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/48c07506022634f332b410fb59dca9f61f89b032.pdf",
      "citation_key": "zhang2024h9k",
      "metadata": {
        "title": "Knowledge graph accuracy evaluation: an LLM-enhanced embedding approach",
        "authors": [
          "Mingtao Zhang",
          "Guoli Yang",
          "Yi Liu",
          "Jing Shi",
          "Xiaoying Bai"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/48c07506022634f332b410fb59dca9f61f89b032.pdf",
        "venue": "International Journal of Data Science and Analysis",
        "citationCount": 4,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "48c07506022634f332b410fb59dca9f61f89b032.pdf"
    },
    {
      "success": true,
      "doc_id": "6e709b73a2a5d101d430847ec0191843",
      "summary": "Recent studies successfully learned static graph embeddings that are structurally fair by preventing the effectiveness disparity of high- and low-degree vertex groups in downstream graph mining tasks. However, achieving structure fairness in dynamic graph embedding remains an open problem. Neglecting degree changes in dynamic graphs will significantly impair embedding effectiveness without notably improving structure fairness. This is because the embedding performance of high-degree and low-to-high-degree vertices will significantly drop close to the generally poorer embedding performance of most slightly changed vertices in the long-tail part of the power-law distribution. We first identify biased structural evolutions in a dynamic graph based on the evolving trend of vertex degree and then propose FairDGE, the first structurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biased structural evolutions by jointly embedding the connection changes among vertices and the long-short-term evolutionary trend of vertex degrees. Furthermore, a novel dual debiasing approach is devised to encode fair embeddings contrastively, customizing debiasing strategies for different biased structural evolutions. This innovative debiasing strategy breaks the effectiveness bottleneck of embeddings without notable fairness loss. Extensive experiments demonstrate that FairDGE achieves simultaneous improvement in the effectiveness and fairness of embeddings.",
      "intriguing_abstract": "Recent studies successfully learned static graph embeddings that are structurally fair by preventing the effectiveness disparity of high- and low-degree vertex groups in downstream graph mining tasks. However, achieving structure fairness in dynamic graph embedding remains an open problem. Neglecting degree changes in dynamic graphs will significantly impair embedding effectiveness without notably improving structure fairness. This is because the embedding performance of high-degree and low-to-high-degree vertices will significantly drop close to the generally poorer embedding performance of most slightly changed vertices in the long-tail part of the power-law distribution. We first identify biased structural evolutions in a dynamic graph based on the evolving trend of vertex degree and then propose FairDGE, the first structurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biased structural evolutions by jointly embedding the connection changes among vertices and the long-short-term evolutionary trend of vertex degrees. Furthermore, a novel dual debiasing approach is devised to encode fair embeddings contrastively, customizing debiasing strategies for different biased structural evolutions. This innovative debiasing strategy breaks the effectiveness bottleneck of embeddings without notable fairness loss. Extensive experiments demonstrate that FairDGE achieves simultaneous improvement in the effectiveness and fairness of embeddings.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/575af1587dea578d48eb27f45f008203565d9170.pdf",
      "citation_key": "li2024wyh",
      "metadata": {
        "title": "Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach",
        "authors": [
          "Yicong Li",
          "Yu Yang",
          "Jiannong Cao",
          "Shuaiqi Liu",
          "Haoran Tang",
          "Guandong Xu"
        ],
        "published_date": "2024",
        "abstract": "Recent studies successfully learned static graph embeddings that are structurally fair by preventing the effectiveness disparity of high- and low-degree vertex groups in downstream graph mining tasks. However, achieving structure fairness in dynamic graph embedding remains an open problem. Neglecting degree changes in dynamic graphs will significantly impair embedding effectiveness without notably improving structure fairness. This is because the embedding performance of high-degree and low-to-high-degree vertices will significantly drop close to the generally poorer embedding performance of most slightly changed vertices in the long-tail part of the power-law distribution. We first identify biased structural evolutions in a dynamic graph based on the evolving trend of vertex degree and then propose FairDGE, the first structurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biased structural evolutions by jointly embedding the connection changes among vertices and the long-short-term evolutionary trend of vertex degrees. Furthermore, a novel dual debiasing approach is devised to encode fair embeddings contrastively, customizing debiasing strategies for different biased structural evolutions. This innovative debiasing strategy breaks the effectiveness bottleneck of embeddings without notable fairness loss. Extensive experiments demonstrate that FairDGE achieves simultaneous improvement in the effectiveness and fairness of embeddings.",
        "file_path": "paper_data/knowledge_graph_embedding/info/575af1587dea578d48eb27f45f008203565d9170.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Recent studies successfully learned static graph embeddings that are structurally fair by preventing the effectiveness disparity of high- and low-degree vertex groups in downstream graph mining tasks. However, achieving structure fairness in dynamic graph embedding remains an open problem. Neglecting degree changes in dynamic graphs will significantly impair embedding effectiveness without notably improving structure fairness. This is because the embedding performance of high-degree and low-to-high-degree vertices will significantly drop close to the generally poorer embedding performance of most slightly changed vertices in the long-tail part of the power-law distribution. We first identify biased structural evolutions in a dynamic graph based on the evolving trend of vertex degree and then propose FairDGE, the first structurally Fair Dynamic Graph Embedding algorithm. FairDGE learns biased structural evolutions by jointly embedding the connection changes among vertices and the long-short-term evolutionary trend of vertex degrees. Furthermore, a novel dual debiasing approach is devised to encode fair embeddings contrastively, customizing debiasing strategies for different biased structural evolutions. This innovative debiasing strategy breaks the effectiveness bottleneck of embeddings without notable fairness loss. Extensive experiments demonstrate that FairDGE achieves simultaneous improvement in the effectiveness and fairness of embeddings.",
        "keywords": []
      },
      "file_name": "575af1587dea578d48eb27f45f008203565d9170.pdf"
    },
    {
      "success": true,
      "doc_id": "5f40a2c1ebef3bad86c5e591c6498962",
      "summary": "Predicting wind speed over the ocean is difficult due to the unequal distribution of buoy stations and the occasional fluctuations in the wind field. This study proposes a dynamic graph embedding-based graph neural networklong short-term memory joint framework (DGE-GAT-LSTM) to estimate wind speed at numerous stations by considering their spatio-temporal information properties. To begin, the buoys that are pertinent to the target station are chosen based on their geographic position. Then, the local graph structures connecting the stations are represented using cosine similarity at each time interval. Subsequently, the graph neural network captures intricate spatial characteristics, while the LSTM module acquires knowledge of temporal interdependence. The graph neural network and LSTM module are sequentially interconnected to collectively capture spatio-temporal correlations. Ultimately, the multi-step prediction outcomes are produced in a sequential way, where each step relies on the previous predictions. The empirical data are derived from direct measurements made by NDBC buoys. The results indicate that the suggested method achieves a mean absolute error reduction ranging from 1% to 36% when compared to other benchmark methods. This improvement in accuracy is statistically significant. This approach effectively addresses the challenges of inadequate information integration and the complexity of modeling temporal correlations in the forecast of ocean wind speed. It offers valuable insights for optimizing the selection of offshore wind farm locations and enhancing operational and management capabilities.",
      "intriguing_abstract": "Predicting wind speed over the ocean is difficult due to the unequal distribution of buoy stations and the occasional fluctuations in the wind field. This study proposes a dynamic graph embedding-based graph neural networklong short-term memory joint framework (DGE-GAT-LSTM) to estimate wind speed at numerous stations by considering their spatio-temporal information properties. To begin, the buoys that are pertinent to the target station are chosen based on their geographic position. Then, the local graph structures connecting the stations are represented using cosine similarity at each time interval. Subsequently, the graph neural network captures intricate spatial characteristics, while the LSTM module acquires knowledge of temporal interdependence. The graph neural network and LSTM module are sequentially interconnected to collectively capture spatio-temporal correlations. Ultimately, the multi-step prediction outcomes are produced in a sequential way, where each step relies on the previous predictions. The empirical data are derived from direct measurements made by NDBC buoys. The results indicate that the suggested method achieves a mean absolute error reduction ranging from 1% to 36% when compared to other benchmark methods. This improvement in accuracy is statistically significant. This approach effectively addresses the challenges of inadequate information integration and the complexity of modeling temporal correlations in the forecast of ocean wind speed. It offers valuable insights for optimizing the selection of offshore wind farm locations and enhancing operational and management capabilities.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/7bd50842503e23e6479447b98912ac482ef43adc.pdf",
      "citation_key": "dong2024ijo",
      "metadata": {
        "title": "Short-Term Marine Wind Speed Forecasting Based on Dynamic Graph Embedding and Spatiotemporal Information",
        "authors": [
          "Dibo Dong",
          "Shangwei Wang",
          "Qiaoying Guo",
          "Yiting Ding",
          "Xing Li",
          "Zicheng You"
        ],
        "published_date": "2024",
        "abstract": "Predicting wind speed over the ocean is difficult due to the unequal distribution of buoy stations and the occasional fluctuations in the wind field. This study proposes a dynamic graph embedding-based graph neural networklong short-term memory joint framework (DGE-GAT-LSTM) to estimate wind speed at numerous stations by considering their spatio-temporal information properties. To begin, the buoys that are pertinent to the target station are chosen based on their geographic position. Then, the local graph structures connecting the stations are represented using cosine similarity at each time interval. Subsequently, the graph neural network captures intricate spatial characteristics, while the LSTM module acquires knowledge of temporal interdependence. The graph neural network and LSTM module are sequentially interconnected to collectively capture spatio-temporal correlations. Ultimately, the multi-step prediction outcomes are produced in a sequential way, where each step relies on the previous predictions. The empirical data are derived from direct measurements made by NDBC buoys. The results indicate that the suggested method achieves a mean absolute error reduction ranging from 1% to 36% when compared to other benchmark methods. This improvement in accuracy is statistically significant. This approach effectively addresses the challenges of inadequate information integration and the complexity of modeling temporal correlations in the forecast of ocean wind speed. It offers valuable insights for optimizing the selection of offshore wind farm locations and enhancing operational and management capabilities.",
        "file_path": "paper_data/knowledge_graph_embedding/info/7bd50842503e23e6479447b98912ac482ef43adc.pdf",
        "venue": "Journal of Marine Science and Engineering",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Predicting wind speed over the ocean is difficult due to the unequal distribution of buoy stations and the occasional fluctuations in the wind field. This study proposes a dynamic graph embedding-based graph neural networklong short-term memory joint framework (DGE-GAT-LSTM) to estimate wind speed at numerous stations by considering their spatio-temporal information properties. To begin, the buoys that are pertinent to the target station are chosen based on their geographic position. Then, the local graph structures connecting the stations are represented using cosine similarity at each time interval. Subsequently, the graph neural network captures intricate spatial characteristics, while the LSTM module acquires knowledge of temporal interdependence. The graph neural network and LSTM module are sequentially interconnected to collectively capture spatio-temporal correlations. Ultimately, the multi-step prediction outcomes are produced in a sequential way, where each step relies on the previous predictions. The empirical data are derived from direct measurements made by NDBC buoys. The results indicate that the suggested method achieves a mean absolute error reduction ranging from 1% to 36% when compared to other benchmark methods. This improvement in accuracy is statistically significant. This approach effectively addresses the challenges of inadequate information integration and the complexity of modeling temporal correlations in the forecast of ocean wind speed. It offers valuable insights for optimizing the selection of offshore wind farm locations and enhancing operational and management capabilities.",
        "keywords": []
      },
      "file_name": "7bd50842503e23e6479447b98912ac482ef43adc.pdf"
    },
    {
      "success": true,
      "doc_id": "93b3a65d52d7ee32d8d56fa4de9ca446",
      "summary": "In knowledge graph embedding, multidimensional representations of entities and relations are learned in vector space. Although distance-based graph embedding methods have shown promise in link prediction, they neglect context information among the triplet components, i.e., the head_entity, relation, and tail_entity, limiting their ability to describe multivariate relation patterns and mapping properties. Such context information denotes the entity structural association inside the same triplet and implies the correlation between entities that are not directly connected. In this work, we propose a novel knowledge graph embedding model that explicitly considers context information in graph embedding via triplet component interactions (TCIE). To build connections between components and incorporate contextual information, entities and relations are represented as vectors comprised of two specialized parts, enabling comprehensive interaction. By simultaneously interacting with one-hop related head and tail entities, TCIE strengthens the connections between distant entities and enables contextual information to be transmitted across the knowledge graph. Mathematical proofs and experiments are performed to analyse the modelling ability of TCIE in knowledge graph embedding. TCIE shows a strong capacity for modelling four relation patterns (i.e., symmetry, antisymmetry, inverse, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many). The experimental evaluation of ogbl-wikikg2, ogbl-biokg, FB15k, and FB15k-237 shows that TCIE achieves state-of-the-art results in link prediction.",
      "intriguing_abstract": "In knowledge graph embedding, multidimensional representations of entities and relations are learned in vector space. Although distance-based graph embedding methods have shown promise in link prediction, they neglect context information among the triplet components, i.e., the head_entity, relation, and tail_entity, limiting their ability to describe multivariate relation patterns and mapping properties. Such context information denotes the entity structural association inside the same triplet and implies the correlation between entities that are not directly connected. In this work, we propose a novel knowledge graph embedding model that explicitly considers context information in graph embedding via triplet component interactions (TCIE). To build connections between components and incorporate contextual information, entities and relations are represented as vectors comprised of two specialized parts, enabling comprehensive interaction. By simultaneously interacting with one-hop related head and tail entities, TCIE strengthens the connections between distant entities and enables contextual information to be transmitted across the knowledge graph. Mathematical proofs and experiments are performed to analyse the modelling ability of TCIE in knowledge graph embedding. TCIE shows a strong capacity for modelling four relation patterns (i.e., symmetry, antisymmetry, inverse, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many). The experimental evaluation of ogbl-wikikg2, ogbl-biokg, FB15k, and FB15k-237 shows that TCIE achieves state-of-the-art results in link prediction.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf",
      "citation_key": "wang20246c7",
      "metadata": {
        "title": "Knowledge Graph Embedding via Triplet Component Interactions",
        "authors": [
          "Tao Wang",
          "Bo Shen",
          "Jinglin Zhang",
          "Yu Zhong"
        ],
        "published_date": "2024",
        "abstract": "In knowledge graph embedding, multidimensional representations of entities and relations are learned in vector space. Although distance-based graph embedding methods have shown promise in link prediction, they neglect context information among the triplet components, i.e., the head_entity, relation, and tail_entity, limiting their ability to describe multivariate relation patterns and mapping properties. Such context information denotes the entity structural association inside the same triplet and implies the correlation between entities that are not directly connected. In this work, we propose a novel knowledge graph embedding model that explicitly considers context information in graph embedding via triplet component interactions (TCIE). To build connections between components and incorporate contextual information, entities and relations are represented as vectors comprised of two specialized parts, enabling comprehensive interaction. By simultaneously interacting with one-hop related head and tail entities, TCIE strengthens the connections between distant entities and enables contextual information to be transmitted across the knowledge graph. Mathematical proofs and experiments are performed to analyse the modelling ability of TCIE in knowledge graph embedding. TCIE shows a strong capacity for modelling four relation patterns (i.e., symmetry, antisymmetry, inverse, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many). The experimental evaluation of ogbl-wikikg2, ogbl-biokg, FB15k, and FB15k-237 shows that TCIE achieves state-of-the-art results in link prediction.",
        "file_path": "paper_data/knowledge_graph_embedding/info/4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf",
        "venue": "Neural Processing Letters",
        "citationCount": 3,
        "score": 3.0,
        "summary": "In knowledge graph embedding, multidimensional representations of entities and relations are learned in vector space. Although distance-based graph embedding methods have shown promise in link prediction, they neglect context information among the triplet components, i.e., the head_entity, relation, and tail_entity, limiting their ability to describe multivariate relation patterns and mapping properties. Such context information denotes the entity structural association inside the same triplet and implies the correlation between entities that are not directly connected. In this work, we propose a novel knowledge graph embedding model that explicitly considers context information in graph embedding via triplet component interactions (TCIE). To build connections between components and incorporate contextual information, entities and relations are represented as vectors comprised of two specialized parts, enabling comprehensive interaction. By simultaneously interacting with one-hop related head and tail entities, TCIE strengthens the connections between distant entities and enables contextual information to be transmitted across the knowledge graph. Mathematical proofs and experiments are performed to analyse the modelling ability of TCIE in knowledge graph embedding. TCIE shows a strong capacity for modelling four relation patterns (i.e., symmetry, antisymmetry, inverse, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many). The experimental evaluation of ogbl-wikikg2, ogbl-biokg, FB15k, and FB15k-237 shows that TCIE achieves state-of-the-art results in link prediction.",
        "keywords": []
      },
      "file_name": "4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf"
    },
    {
      "success": true,
      "doc_id": "40d9fc0d2d21bc25b25ea4bfe3a0d963",
      "summary": "Knowledge graph embedding represents entities and relations as low-dimensional continuous vectors. Recently, researchers have attempted to leverage the potential semantic connections between entities with hierarchical relationships in the knowledge graph. However, existing knowledge embedding methods that model entity hierarchical structures face the following challenges: (1) Setting the same embedding dimension for entities at different hierarchical levels can lead to overfitting issues and the waste of storage and computational resources; (2) Manually searching the embedding dimension in discrete space makes it easy to miss the optimal parameters and increases training costs. Therefore, we propose a knowledge embedding method, HEAES, that automatically learns the embedding dimensions for entities based on their hierarchical structure. Specifically, we first propose a new modeling method to capture the hierarchical relationships of entities, and we then train the model on a triple classification task. Then, we adaptively learn the pruning threshold to trim the embedding vectors, automatically learning different embedding dimensions for entities at different hierarchical levels. Experiments on the YAGO26K and DB111K datasets verify that introducing entity hierarchical information helps boost the model performance.",
      "intriguing_abstract": "Knowledge graph embedding represents entities and relations as low-dimensional continuous vectors. Recently, researchers have attempted to leverage the potential semantic connections between entities with hierarchical relationships in the knowledge graph. However, existing knowledge embedding methods that model entity hierarchical structures face the following challenges: (1) Setting the same embedding dimension for entities at different hierarchical levels can lead to overfitting issues and the waste of storage and computational resources; (2) Manually searching the embedding dimension in discrete space makes it easy to miss the optimal parameters and increases training costs. Therefore, we propose a knowledge embedding method, HEAES, that automatically learns the embedding dimensions for entities based on their hierarchical structure. Specifically, we first propose a new modeling method to capture the hierarchical relationships of entities, and we then train the model on a triple classification task. Then, we adaptively learn the pruning threshold to trim the embedding vectors, automatically learning different embedding dimensions for entities at different hierarchical levels. Experiments on the YAGO26K and DB111K datasets verify that introducing entity hierarchical information helps boost the model performance.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/c3861a930a65e8d9ee7ab9f0a6ee71e0e59df7ed.pdf",
      "citation_key": "zhang2024yjo",
      "metadata": {
        "title": "Knowledge Graph Embedding for Hierarchical Entities Based on Auto-Embedding Size",
        "authors": [
          "Pengfei Zhang",
          "Xiaoxue Zhang",
          "Yang Fang",
          "Jinzhi Liao",
          "Wubin Ma",
          "Zhen Tan",
          "Weidong Xiao"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graph embedding represents entities and relations as low-dimensional continuous vectors. Recently, researchers have attempted to leverage the potential semantic connections between entities with hierarchical relationships in the knowledge graph. However, existing knowledge embedding methods that model entity hierarchical structures face the following challenges: (1) Setting the same embedding dimension for entities at different hierarchical levels can lead to overfitting issues and the waste of storage and computational resources; (2) Manually searching the embedding dimension in discrete space makes it easy to miss the optimal parameters and increases training costs. Therefore, we propose a knowledge embedding method, HEAES, that automatically learns the embedding dimensions for entities based on their hierarchical structure. Specifically, we first propose a new modeling method to capture the hierarchical relationships of entities, and we then train the model on a triple classification task. Then, we adaptively learn the pruning threshold to trim the embedding vectors, automatically learning different embedding dimensions for entities at different hierarchical levels. Experiments on the YAGO26K and DB111K datasets verify that introducing entity hierarchical information helps boost the model performance.",
        "file_path": "paper_data/knowledge_graph_embedding/info/c3861a930a65e8d9ee7ab9f0a6ee71e0e59df7ed.pdf",
        "venue": "Mathematics",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Knowledge graph embedding represents entities and relations as low-dimensional continuous vectors. Recently, researchers have attempted to leverage the potential semantic connections between entities with hierarchical relationships in the knowledge graph. However, existing knowledge embedding methods that model entity hierarchical structures face the following challenges: (1) Setting the same embedding dimension for entities at different hierarchical levels can lead to overfitting issues and the waste of storage and computational resources; (2) Manually searching the embedding dimension in discrete space makes it easy to miss the optimal parameters and increases training costs. Therefore, we propose a knowledge embedding method, HEAES, that automatically learns the embedding dimensions for entities based on their hierarchical structure. Specifically, we first propose a new modeling method to capture the hierarchical relationships of entities, and we then train the model on a triple classification task. Then, we adaptively learn the pruning threshold to trim the embedding vectors, automatically learning different embedding dimensions for entities at different hierarchical levels. Experiments on the YAGO26K and DB111K datasets verify that introducing entity hierarchical information helps boost the model performance.",
        "keywords": []
      },
      "file_name": "c3861a930a65e8d9ee7ab9f0a6ee71e0e59df7ed.pdf"
    },
    {
      "success": true,
      "doc_id": "059ed0d040ff8cfb740e1763cddb0612",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/217a4712feae7d7590d813d23e88f5fbb4f2c37f.pdf",
      "citation_key": "liang20247wv",
      "metadata": {
        "title": "Clustering then Propagation: Select Better Anchors for Knowledge Graph Embedding",
        "authors": [
          "K. Liang",
          "Yue Liu",
          "Hao Li",
          "Lingyuan Meng",
          "Suyuan Liu",
          "Siwei Wang",
          "Sihang Zhou",
          "Xinwang Liu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/217a4712feae7d7590d813d23e88f5fbb4f2c37f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "217a4712feae7d7590d813d23e88f5fbb4f2c37f.pdf"
    },
    {
      "success": true,
      "doc_id": "94bc69f73bf4e4564bdd0632701b3680",
      "summary": "Fake news is a prevalent issue in modern society, leading to misinformation, and societal harm. News credibility assessment is a crucial approach for evaluating the accuracy and authenticity of news. It plays a significant role in enhancing public awareness and understanding of news, while also effectively mitigating the dissemination of fake news. However, news credibility assessment meets challenges when processing large-scale and constantly growing data, due to insufficient and unreliable labels and standards, and diversity and semantic ambiguity of news contents. Recently, machine learning models have been well developed to address these issues, but suffer from limited effectiveness. A unified framework is also required for them to represent various entities and relationships involved in news stories. This article proposes an entity ontology-based knowledge graph network (EKNet) to leverage knowledge graphs and entity frameworks for news credibility assessment. The model utilizes the information from knowledge graphs by combining entities and relationships from news and knowledge graphs. Experimental results show that the EKNet has advantages in evaluating news credibility over existing methods. Specifically, compared to several strong baselines, the model demonstrates a significant performance improvement in scores across various tasks. Which indicates that using the EKNet to address the challenges in news credibility assessment is highly effective and can conduct better performance for the problem of fake news in the social media environment.",
      "intriguing_abstract": "Fake news is a prevalent issue in modern society, leading to misinformation, and societal harm. News credibility assessment is a crucial approach for evaluating the accuracy and authenticity of news. It plays a significant role in enhancing public awareness and understanding of news, while also effectively mitigating the dissemination of fake news. However, news credibility assessment meets challenges when processing large-scale and constantly growing data, due to insufficient and unreliable labels and standards, and diversity and semantic ambiguity of news contents. Recently, machine learning models have been well developed to address these issues, but suffer from limited effectiveness. A unified framework is also required for them to represent various entities and relationships involved in news stories. This article proposes an entity ontology-based knowledge graph network (EKNet) to leverage knowledge graphs and entity frameworks for news credibility assessment. The model utilizes the information from knowledge graphs by combining entities and relationships from news and knowledge graphs. Experimental results show that the EKNet has advantages in evaluating news credibility over existing methods. Specifically, compared to several strong baselines, the model demonstrates a significant performance improvement in scores across various tasks. Which indicates that using the EKNet to address the challenges in news credibility assessment is highly effective and can conduct better performance for the problem of fake news in the social media environment.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/cf696a919b8476a4d74b8b726e919812a2f05779.pdf",
      "citation_key": "liu2024t05",
      "metadata": {
        "title": "An Entity Ontology-Based Knowledge Graph Embedding Approach to News Credibility Assessment",
        "authors": [
          "Qi Liu",
          "Yuanyuan Jin",
          "Xuefei Cao",
          "Xiaodong Liu",
          "Xiaokang Zhou",
          "Yonghong Zhang",
          "Xiaolong Xu",
          "Lianyong Qi"
        ],
        "published_date": "2024",
        "abstract": "Fake news is a prevalent issue in modern society, leading to misinformation, and societal harm. News credibility assessment is a crucial approach for evaluating the accuracy and authenticity of news. It plays a significant role in enhancing public awareness and understanding of news, while also effectively mitigating the dissemination of fake news. However, news credibility assessment meets challenges when processing large-scale and constantly growing data, due to insufficient and unreliable labels and standards, and diversity and semantic ambiguity of news contents. Recently, machine learning models have been well developed to address these issues, but suffer from limited effectiveness. A unified framework is also required for them to represent various entities and relationships involved in news stories. This article proposes an entity ontology-based knowledge graph network (EKNet) to leverage knowledge graphs and entity frameworks for news credibility assessment. The model utilizes the information from knowledge graphs by combining entities and relationships from news and knowledge graphs. Experimental results show that the EKNet has advantages in evaluating news credibility over existing methods. Specifically, compared to several strong baselines, the model demonstrates a significant performance improvement in scores across various tasks. Which indicates that using the EKNet to address the challenges in news credibility assessment is highly effective and can conduct better performance for the problem of fake news in the social media environment.",
        "file_path": "paper_data/knowledge_graph_embedding/info/cf696a919b8476a4d74b8b726e919812a2f05779.pdf",
        "venue": "IEEE Transactions on Computational Social Systems",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Fake news is a prevalent issue in modern society, leading to misinformation, and societal harm. News credibility assessment is a crucial approach for evaluating the accuracy and authenticity of news. It plays a significant role in enhancing public awareness and understanding of news, while also effectively mitigating the dissemination of fake news. However, news credibility assessment meets challenges when processing large-scale and constantly growing data, due to insufficient and unreliable labels and standards, and diversity and semantic ambiguity of news contents. Recently, machine learning models have been well developed to address these issues, but suffer from limited effectiveness. A unified framework is also required for them to represent various entities and relationships involved in news stories. This article proposes an entity ontology-based knowledge graph network (EKNet) to leverage knowledge graphs and entity frameworks for news credibility assessment. The model utilizes the information from knowledge graphs by combining entities and relationships from news and knowledge graphs. Experimental results show that the EKNet has advantages in evaluating news credibility over existing methods. Specifically, compared to several strong baselines, the model demonstrates a significant performance improvement in scores across various tasks. Which indicates that using the EKNet to address the challenges in news credibility assessment is highly effective and can conduct better performance for the problem of fake news in the social media environment.",
        "keywords": []
      },
      "file_name": "cf696a919b8476a4d74b8b726e919812a2f05779.pdf"
    },
    {
      "success": true,
      "doc_id": "00b4f7e0aa9e37adc384e264a4cd8f8b",
      "summary": "ABSTRACT Context-aware recommender systems have been employed in heterogeneous domains and systems and have been implemented using a diverse range of approaches designed to realize personalization with targeted service provision in systems which are context aware. The diverse range of approaches used include knowledge graphs, which have gained traction for recommender systems driven by large complex datasets with user profiles, social network data, and logfile data are stored in directed heterogeneous knowledge graph(s) where nodes represent entities and edges correspond to relationships. However, the representation of entities generally fails to model the relationships that exist between entities. In this paper, we present a novel process to improve the efficacy of recommender systems employing knowledge graph embeddings with a convolutional network. In our proposed model, all user and item-based relationships are considered to enable the detection of the relationships that exist between them. To evaluate our proposed model, experimental testing has been implemented using real-world public datasets including a comparative analysis between the state of the art baseline approaches and the proposed method. Our reported experimental results indicate that our proposed model outperforms the alternative methods in user recommendations with enhanced targeted service provision and personalization.",
      "intriguing_abstract": "ABSTRACT Context-aware recommender systems have been employed in heterogeneous domains and systems and have been implemented using a diverse range of approaches designed to realize personalization with targeted service provision in systems which are context aware. The diverse range of approaches used include knowledge graphs, which have gained traction for recommender systems driven by large complex datasets with user profiles, social network data, and logfile data are stored in directed heterogeneous knowledge graph(s) where nodes represent entities and edges correspond to relationships. However, the representation of entities generally fails to model the relationships that exist between entities. In this paper, we present a novel process to improve the efficacy of recommender systems employing knowledge graph embeddings with a convolutional network. In our proposed model, all user and item-based relationships are considered to enable the detection of the relationships that exist between them. To evaluate our proposed model, experimental testing has been implemented using real-world public datasets including a comparative analysis between the state of the art baseline approaches and the proposed method. Our reported experimental results indicate that our proposed model outperforms the alternative methods in user recommendations with enhanced targeted service provision and personalization.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/91d5aa3d43237ec60266563ec6e8079f86532cfa.pdf",
      "citation_key": "pham20243mh",
      "metadata": {
        "title": "IDGCN: A Proposed Knowledge Graph Embedding With Graph Convolution Network For Context-Aware Recommendation Systems",
        "authors": [
          "H. V. Pham",
          "Trung Tuan Nguyen",
          "Luu Minh Tuan",
          "Philip Moore"
        ],
        "published_date": "2024",
        "abstract": "ABSTRACT Context-aware recommender systems have been employed in heterogeneous domains and systems and have been implemented using a diverse range of approaches designed to realize personalization with targeted service provision in systems which are context aware. The diverse range of approaches used include knowledge graphs, which have gained traction for recommender systems driven by large complex datasets with user profiles, social network data, and logfile data are stored in directed heterogeneous knowledge graph(s) where nodes represent entities and edges correspond to relationships. However, the representation of entities generally fails to model the relationships that exist between entities. In this paper, we present a novel process to improve the efficacy of recommender systems employing knowledge graph embeddings with a convolutional network. In our proposed model, all user and item-based relationships are considered to enable the detection of the relationships that exist between them. To evaluate our proposed model, experimental testing has been implemented using real-world public datasets including a comparative analysis between the state of the art baseline approaches and the proposed method. Our reported experimental results indicate that our proposed model outperforms the alternative methods in user recommendations with enhanced targeted service provision and personalization.",
        "file_path": "paper_data/knowledge_graph_embedding/info/91d5aa3d43237ec60266563ec6e8079f86532cfa.pdf",
        "venue": "Journal of Organizational Computing and Electronic Commerce",
        "citationCount": 3,
        "score": 3.0,
        "summary": "ABSTRACT Context-aware recommender systems have been employed in heterogeneous domains and systems and have been implemented using a diverse range of approaches designed to realize personalization with targeted service provision in systems which are context aware. The diverse range of approaches used include knowledge graphs, which have gained traction for recommender systems driven by large complex datasets with user profiles, social network data, and logfile data are stored in directed heterogeneous knowledge graph(s) where nodes represent entities and edges correspond to relationships. However, the representation of entities generally fails to model the relationships that exist between entities. In this paper, we present a novel process to improve the efficacy of recommender systems employing knowledge graph embeddings with a convolutional network. In our proposed model, all user and item-based relationships are considered to enable the detection of the relationships that exist between them. To evaluate our proposed model, experimental testing has been implemented using real-world public datasets including a comparative analysis between the state of the art baseline approaches and the proposed method. Our reported experimental results indicate that our proposed model outperforms the alternative methods in user recommendations with enhanced targeted service provision and personalization.",
        "keywords": []
      },
      "file_name": "91d5aa3d43237ec60266563ec6e8079f86532cfa.pdf"
    },
    {
      "success": true,
      "doc_id": "c2c265b7741a4e83395791819d442017",
      "summary": "Drug combinations can reduce drug resistance and side effects and enable the improvement of disease treatment efficacy. Therefore, how to effectively identify drug-drug interactions (DDIs) is a challenging problem. Currently, there exist several approaches that leverage advanced representation learning and graph-based techniques for DDIs prediction. While these methods have demonstrated promising results, a limited number of approaches effectively utilize the potential of knowledge graphs (KGs), which provide information on drug attributes and multirelation among entities. In this work, we introduce a novel attention-based KGs representation learning framework. To encode drug SMILES sequence, a pretrained model is used, while molecular structure information is mapped as the initialization of nodes within the KG using a message-passing neural network. Additionally, the knowledge-aware graph attention network is employed to capture the drug and its topological neighbor representation in the KG representation module. To prevent the oversmoothing problem, the residual layer is used in the DDI prediction module. Comprehensive experiments on several datasets have demonstrated that the proposed method outperforms the state-of-the-art algorithms on the DDI prediction task across a range of evaluation metrics. It achieves an accuracy of 0.924 and an AUC of 0.9705 on the KEGG dataset and attains an ACC of 0.9777 and an AUC of 0.9959 on the OGB-biokg dataset. These experimental findings affirm that our approach is a dependable model for predicting the association of drugs.",
      "intriguing_abstract": "Drug combinations can reduce drug resistance and side effects and enable the improvement of disease treatment efficacy. Therefore, how to effectively identify drug-drug interactions (DDIs) is a challenging problem. Currently, there exist several approaches that leverage advanced representation learning and graph-based techniques for DDIs prediction. While these methods have demonstrated promising results, a limited number of approaches effectively utilize the potential of knowledge graphs (KGs), which provide information on drug attributes and multirelation among entities. In this work, we introduce a novel attention-based KGs representation learning framework. To encode drug SMILES sequence, a pretrained model is used, while molecular structure information is mapped as the initialization of nodes within the KG using a message-passing neural network. Additionally, the knowledge-aware graph attention network is employed to capture the drug and its topological neighbor representation in the KG representation module. To prevent the oversmoothing problem, the residual layer is used in the DDI prediction module. Comprehensive experiments on several datasets have demonstrated that the proposed method outperforms the state-of-the-art algorithms on the DDI prediction task across a range of evaluation metrics. It achieves an accuracy of 0.924 and an AUC of 0.9705 on the KEGG dataset and attains an ACC of 0.9777 and an AUC of 0.9959 on the OGB-biokg dataset. These experimental findings affirm that our approach is a dependable model for predicting the association of drugs.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/58480444670ff933fe644563f7e2948a79503442.pdf",
      "citation_key": "li2024gar",
      "metadata": {
        "title": "Attention-Based Learning for Predicting Drug-Drug Interactions in Knowledge Graph Embedding Based on Multisource Fusion Information",
        "authors": [
          "Yu Li",
          "Zhu-Hong You",
          "Shu-Min Wang",
          "Cheng-Gang Mi",
          "Mei-Neng Wang",
          "Yu-An Huang",
          "Haicheng Yi"
        ],
        "published_date": "2024",
        "abstract": "Drug combinations can reduce drug resistance and side effects and enable the improvement of disease treatment efficacy. Therefore, how to effectively identify drug-drug interactions (DDIs) is a challenging problem. Currently, there exist several approaches that leverage advanced representation learning and graph-based techniques for DDIs prediction. While these methods have demonstrated promising results, a limited number of approaches effectively utilize the potential of knowledge graphs (KGs), which provide information on drug attributes and multirelation among entities. In this work, we introduce a novel attention-based KGs representation learning framework. To encode drug SMILES sequence, a pretrained model is used, while molecular structure information is mapped as the initialization of nodes within the KG using a message-passing neural network. Additionally, the knowledge-aware graph attention network is employed to capture the drug and its topological neighbor representation in the KG representation module. To prevent the oversmoothing problem, the residual layer is used in the DDI prediction module. Comprehensive experiments on several datasets have demonstrated that the proposed method outperforms the state-of-the-art algorithms on the DDI prediction task across a range of evaluation metrics. It achieves an accuracy of 0.924 and an AUC of 0.9705 on the KEGG dataset and attains an ACC of 0.9777 and an AUC of 0.9959 on the OGB-biokg dataset. These experimental findings affirm that our approach is a dependable model for predicting the association of drugs.",
        "file_path": "paper_data/knowledge_graph_embedding/info/58480444670ff933fe644563f7e2948a79503442.pdf",
        "venue": "International Journal of Intelligent Systems",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Drug combinations can reduce drug resistance and side effects and enable the improvement of disease treatment efficacy. Therefore, how to effectively identify drug-drug interactions (DDIs) is a challenging problem. Currently, there exist several approaches that leverage advanced representation learning and graph-based techniques for DDIs prediction. While these methods have demonstrated promising results, a limited number of approaches effectively utilize the potential of knowledge graphs (KGs), which provide information on drug attributes and multirelation among entities. In this work, we introduce a novel attention-based KGs representation learning framework. To encode drug SMILES sequence, a pretrained model is used, while molecular structure information is mapped as the initialization of nodes within the KG using a message-passing neural network. Additionally, the knowledge-aware graph attention network is employed to capture the drug and its topological neighbor representation in the KG representation module. To prevent the oversmoothing problem, the residual layer is used in the DDI prediction module. Comprehensive experiments on several datasets have demonstrated that the proposed method outperforms the state-of-the-art algorithms on the DDI prediction task across a range of evaluation metrics. It achieves an accuracy of 0.924 and an AUC of 0.9705 on the KEGG dataset and attains an ACC of 0.9777 and an AUC of 0.9959 on the OGB-biokg dataset. These experimental findings affirm that our approach is a dependable model for predicting the association of drugs.",
        "keywords": []
      },
      "file_name": "58480444670ff933fe644563f7e2948a79503442.pdf"
    },
    {
      "success": true,
      "doc_id": "2bac8622b83627301da57911a21ab375",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9b836b4764d4f6947ac684fd4ba3e8c3597d95bd.pdf",
      "citation_key": "li2024nje",
      "metadata": {
        "title": "Drugtarget interaction prediction using knowledge graph embedding",
        "authors": [
          "Nan Li",
          "Zhihao Yang",
          "Jian Wang",
          "Hongfei Lin"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/9b836b4764d4f6947ac684fd4ba3e8c3597d95bd.pdf",
        "venue": "iScience",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "9b836b4764d4f6947ac684fd4ba3e8c3597d95bd.pdf"
    },
    {
      "success": true,
      "doc_id": "f90fab2029e063ebeaa68d6cfa9ea531",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/bd0e8d6db97111686d02b51134f87439f8f1acfa.pdf",
      "citation_key": "bao20249xp",
      "metadata": {
        "title": "HGCGE: hyperbolic graph convolutional networks-based knowledge graph embedding for link prediction",
        "authors": [
          "Liming Bao",
          "Yan Wang",
          "Xiaoyu Song",
          "Tao Sun"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/bd0e8d6db97111686d02b51134f87439f8f1acfa.pdf",
        "venue": "Knowledge and Information Systems",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "bd0e8d6db97111686d02b51134f87439f8f1acfa.pdf"
    },
    {
      "success": true,
      "doc_id": "607ea12b663c8cb9bda2b043b1439e64",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/bea79d59ab3d203d06c88ebf67ac47cb34adeaa7.pdf",
      "citation_key": "xu2024fto",
      "metadata": {
        "title": "Entity-relation aggregation mechanism graph neural network for knowledge graph embedding",
        "authors": [
          "Guoshun Xu",
          "Guozheng Rao",
          "Li Zhang",
          "Qing Cong"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/bea79d59ab3d203d06c88ebf67ac47cb34adeaa7.pdf",
        "venue": "Applied intelligence (Boston)",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "bea79d59ab3d203d06c88ebf67ac47cb34adeaa7.pdf"
    },
    {
      "success": true,
      "doc_id": "2d338df3311b111e4c6b29cbf0851f34",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/241904795d94dcb1946ad46c9184c59899783af1.pdf",
      "citation_key": "liang2024z0q",
      "metadata": {
        "title": "Effective Knowledge Graph Embedding with Quaternion Convolutional Networks",
        "authors": [
          "Qiuyu Liang",
          "Weihua Wang",
          "Jie Yu",
          "F. Bao"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/241904795d94dcb1946ad46c9184c59899783af1.pdf",
        "venue": "Natural Language Processing and Chinese Computing",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "241904795d94dcb1946ad46c9184c59899783af1.pdf"
    },
    {
      "success": true,
      "doc_id": "fade688d77142104f45c73fcf18f4412",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/55dab161c25d1dd04fbeecdeca085274bfe8463f.pdf",
      "citation_key": "liu2024ixy",
      "metadata": {
        "title": "Multi-Filter soft shrinkage network for knowledge graph embedding",
        "authors": [
          "Jie Liu",
          "Lizheng Zu",
          "Yunbin Yan",
          "Jiye Zuo",
          "Benliang Sang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/55dab161c25d1dd04fbeecdeca085274bfe8463f.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "55dab161c25d1dd04fbeecdeca085274bfe8463f.pdf"
    },
    {
      "success": true,
      "doc_id": "783412c7003e8efa39c897a8cd35601b",
      "summary": "The stability and reliability of modern industrial processes are key factors in ensuring production safety and achieving high quality and efficiency. Complex industrial processes are characterized by dynamics, nonlinearity, and multivariable coupling, which present challenges for fault diagnosis. To address the limitations of traditional fault diagnosis methods in handling complex industrial processes, a fault diagnosis framework of knowledge graph embedded graph convolutional network and bidirectional gated recurrent unit (KG-GCBiGCN) is proposed in this article. First, a domain knowledge graph of the industrial process is constructed in a top-down manner, leveraging the expertise of domain specialists. Next, node correlation analysis is performed using the maximum information coefficient (MIC) alongside the knowledge graph, resulting in the formation of an adjacency matrix. Then, the knowledge graph is embedded within the graph convolutional network (GCN), and a bidirectional gated recurrent unit (BiGRU) is introduced to capture the dynamic correlations present in time-series data, thereby enhancing the performance of the fault diagnosis model. Finally, fault diagnosis is achieved based on the predicted residuals. By integrating the deep information from the knowledge graph, the framework facilitates inference of the root cause of faults and identifies the propagation paths of the faults. The effectiveness of the proposed method is validated using actual data from the hot strip mill process (HSMP), achieving a fault detection accuracy of 99.23%.",
      "intriguing_abstract": "The stability and reliability of modern industrial processes are key factors in ensuring production safety and achieving high quality and efficiency. Complex industrial processes are characterized by dynamics, nonlinearity, and multivariable coupling, which present challenges for fault diagnosis. To address the limitations of traditional fault diagnosis methods in handling complex industrial processes, a fault diagnosis framework of knowledge graph embedded graph convolutional network and bidirectional gated recurrent unit (KG-GCBiGCN) is proposed in this article. First, a domain knowledge graph of the industrial process is constructed in a top-down manner, leveraging the expertise of domain specialists. Next, node correlation analysis is performed using the maximum information coefficient (MIC) alongside the knowledge graph, resulting in the formation of an adjacency matrix. Then, the knowledge graph is embedded within the graph convolutional network (GCN), and a bidirectional gated recurrent unit (BiGRU) is introduced to capture the dynamic correlations present in time-series data, thereby enhancing the performance of the fault diagnosis model. Finally, fault diagnosis is achieved based on the predicted residuals. By integrating the deep information from the knowledge graph, the framework facilitates inference of the root cause of faults and identifies the propagation paths of the faults. The effectiveness of the proposed method is validated using actual data from the hot strip mill process (HSMP), achieving a fault detection accuracy of 99.23%.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/3ff6b617cd839c9d85cb7b58aa6ad56e95b6cf69.pdf",
      "citation_key": "dong2025l9k",
      "metadata": {
        "title": "Knowledge Graph Embedding With Graph Convolutional Network and Bidirectional Gated Recurrent Unit for Fault Diagnosis of Industrial Processes",
        "authors": [
          "Jie Dong",
          "Cuiping Chen",
          "Chi Zhang",
          "Jinyao Ma",
          "Kai-xiang Peng"
        ],
        "published_date": "2025",
        "abstract": "The stability and reliability of modern industrial processes are key factors in ensuring production safety and achieving high quality and efficiency. Complex industrial processes are characterized by dynamics, nonlinearity, and multivariable coupling, which present challenges for fault diagnosis. To address the limitations of traditional fault diagnosis methods in handling complex industrial processes, a fault diagnosis framework of knowledge graph embedded graph convolutional network and bidirectional gated recurrent unit (KG-GCBiGCN) is proposed in this article. First, a domain knowledge graph of the industrial process is constructed in a top-down manner, leveraging the expertise of domain specialists. Next, node correlation analysis is performed using the maximum information coefficient (MIC) alongside the knowledge graph, resulting in the formation of an adjacency matrix. Then, the knowledge graph is embedded within the graph convolutional network (GCN), and a bidirectional gated recurrent unit (BiGRU) is introduced to capture the dynamic correlations present in time-series data, thereby enhancing the performance of the fault diagnosis model. Finally, fault diagnosis is achieved based on the predicted residuals. By integrating the deep information from the knowledge graph, the framework facilitates inference of the root cause of faults and identifies the propagation paths of the faults. The effectiveness of the proposed method is validated using actual data from the hot strip mill process (HSMP), achieving a fault detection accuracy of 99.23%.",
        "file_path": "paper_data/knowledge_graph_embedding/info/3ff6b617cd839c9d85cb7b58aa6ad56e95b6cf69.pdf",
        "venue": "IEEE Sensors Journal",
        "citationCount": 3,
        "score": 3.0,
        "summary": "The stability and reliability of modern industrial processes are key factors in ensuring production safety and achieving high quality and efficiency. Complex industrial processes are characterized by dynamics, nonlinearity, and multivariable coupling, which present challenges for fault diagnosis. To address the limitations of traditional fault diagnosis methods in handling complex industrial processes, a fault diagnosis framework of knowledge graph embedded graph convolutional network and bidirectional gated recurrent unit (KG-GCBiGCN) is proposed in this article. First, a domain knowledge graph of the industrial process is constructed in a top-down manner, leveraging the expertise of domain specialists. Next, node correlation analysis is performed using the maximum information coefficient (MIC) alongside the knowledge graph, resulting in the formation of an adjacency matrix. Then, the knowledge graph is embedded within the graph convolutional network (GCN), and a bidirectional gated recurrent unit (BiGRU) is introduced to capture the dynamic correlations present in time-series data, thereby enhancing the performance of the fault diagnosis model. Finally, fault diagnosis is achieved based on the predicted residuals. By integrating the deep information from the knowledge graph, the framework facilitates inference of the root cause of faults and identifies the propagation paths of the faults. The effectiveness of the proposed method is validated using actual data from the hot strip mill process (HSMP), achieving a fault detection accuracy of 99.23%.",
        "keywords": []
      },
      "file_name": "3ff6b617cd839c9d85cb7b58aa6ad56e95b6cf69.pdf"
    },
    {
      "success": true,
      "doc_id": "96445e309202adc055ce84ddec93f260",
      "summary": "The significance of Temporal Knowledge Graphs (TKGs) in Artificial Intelligence (AI) lies in their capacity to incorporate time-dimensional information, support complex reasoning and prediction, optimize decision-making processes, enhance the accuracy of recommendation systems, promote multimodal data integration, and strengthen knowledge management and updates. This provides a robust foundation for various AI applications. To effectively learn and apply both static and dynamic temporal patterns for reasoning, a range of embedding methods and large language models (LLMs) have been proposed in the literature. However, these methods often rely on a single underlying embedding space, whose geometric properties severely limit their ability to model intricate temporal patterns, such as hierarchical and ring structures. To address this limitation, this paper proposes embedding TKGs into projective geometric space and leverages LLMs technology to extract crucial temporal node information, thereby constructing the 5EL model. By embedding TKGs into projective geometric space and utilizing Mbius Group transformations, we effectively model various temporal patterns. Subsequently, LLMs technology is employed to process the trained TKGs. We adopt a parameter-efficient fine-tuning strategy to align LLMs with specific task requirements, thereby enhancing the model's ability to recognize structural information of key nodes in historical chains and enriching the representation of central entities. Experimental results on five advanced TKG datasets demonstrate that our proposed 5EL model significantly outperforms existing models.",
      "intriguing_abstract": "The significance of Temporal Knowledge Graphs (TKGs) in Artificial Intelligence (AI) lies in their capacity to incorporate time-dimensional information, support complex reasoning and prediction, optimize decision-making processes, enhance the accuracy of recommendation systems, promote multimodal data integration, and strengthen knowledge management and updates. This provides a robust foundation for various AI applications. To effectively learn and apply both static and dynamic temporal patterns for reasoning, a range of embedding methods and large language models (LLMs) have been proposed in the literature. However, these methods often rely on a single underlying embedding space, whose geometric properties severely limit their ability to model intricate temporal patterns, such as hierarchical and ring structures. To address this limitation, this paper proposes embedding TKGs into projective geometric space and leverages LLMs technology to extract crucial temporal node information, thereby constructing the 5EL model. By embedding TKGs into projective geometric space and utilizing Mbius Group transformations, we effectively model various temporal patterns. Subsequently, LLMs technology is employed to process the trained TKGs. We adopt a parameter-efficient fine-tuning strategy to align LLMs with specific task requirements, thereby enhancing the model's ability to recognize structural information of key nodes in historical chains and enriching the representation of central entities. Experimental results on five advanced TKG datasets demonstrate that our proposed 5EL model significantly outperforms existing models.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/9560ca767022020ccf414a2a8514f25b89f78cb3.pdf",
      "citation_key": "zhang2025ebv",
      "metadata": {
        "title": "Integrating Large Language Models and Mbius Group Transformations for Temporal Knowledge Graph Embedding on the Riemann Sphere",
        "authors": [
          "Sensen Zhang",
          "Xun Liang",
          "Simin Niu",
          "Zhendong Niu",
          "Bo Wu",
          "Gengxin Hua",
          "Long Wang",
          "Zhenyu Guan",
          "Hanyu Wang",
          "Xuan Zhang",
          "Zhiyu Li",
          "Yuefeng Ma"
        ],
        "published_date": "2025",
        "abstract": "The significance of Temporal Knowledge Graphs (TKGs) in Artificial Intelligence (AI) lies in their capacity to incorporate time-dimensional information, support complex reasoning and prediction, optimize decision-making processes, enhance the accuracy of recommendation systems, promote multimodal data integration, and strengthen knowledge management and updates. This provides a robust foundation for various AI applications. To effectively learn and apply both static and dynamic temporal patterns for reasoning, a range of embedding methods and large language models (LLMs) have been proposed in the literature. However, these methods often rely on a single underlying embedding space, whose geometric properties severely limit their ability to model intricate temporal patterns, such as hierarchical and ring structures. To address this limitation, this paper proposes embedding TKGs into projective geometric space and leverages LLMs technology to extract crucial temporal node information, thereby constructing the 5EL model. By embedding TKGs into projective geometric space and utilizing Mbius Group transformations, we effectively model various temporal patterns. Subsequently, LLMs technology is employed to process the trained TKGs. We adopt a parameter-efficient fine-tuning strategy to align LLMs with specific task requirements, thereby enhancing the model's ability to recognize structural information of key nodes in historical chains and enriching the representation of central entities. Experimental results on five advanced TKG datasets demonstrate that our proposed 5EL model significantly outperforms existing models.",
        "file_path": "paper_data/knowledge_graph_embedding/info/9560ca767022020ccf414a2a8514f25b89f78cb3.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 3,
        "score": 3.0,
        "summary": "The significance of Temporal Knowledge Graphs (TKGs) in Artificial Intelligence (AI) lies in their capacity to incorporate time-dimensional information, support complex reasoning and prediction, optimize decision-making processes, enhance the accuracy of recommendation systems, promote multimodal data integration, and strengthen knowledge management and updates. This provides a robust foundation for various AI applications. To effectively learn and apply both static and dynamic temporal patterns for reasoning, a range of embedding methods and large language models (LLMs) have been proposed in the literature. However, these methods often rely on a single underlying embedding space, whose geometric properties severely limit their ability to model intricate temporal patterns, such as hierarchical and ring structures. To address this limitation, this paper proposes embedding TKGs into projective geometric space and leverages LLMs technology to extract crucial temporal node information, thereby constructing the 5EL model. By embedding TKGs into projective geometric space and utilizing Mbius Group transformations, we effectively model various temporal patterns. Subsequently, LLMs technology is employed to process the trained TKGs. We adopt a parameter-efficient fine-tuning strategy to align LLMs with specific task requirements, thereby enhancing the model's ability to recognize structural information of key nodes in historical chains and enriching the representation of central entities. Experimental results on five advanced TKG datasets demonstrate that our proposed 5EL model significantly outperforms existing models.",
        "keywords": []
      },
      "file_name": "9560ca767022020ccf414a2a8514f25b89f78cb3.pdf"
    },
    {
      "success": true,
      "doc_id": "d0b0045f556e93aa32cb6856cfcf61af",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/d5c8dcc8f5c87c269780c7011a355b9202858847.pdf",
      "citation_key": "liu20242zm",
      "metadata": {
        "title": "Temporal Knowledge Graph Reasoning with Dynamic Hypergraph Embedding",
        "authors": [
          "Xinyue Liu",
          "Jianan Zhang",
          "Chi Ma",
          "Wenxin Liang",
          "Bo Xu",
          "Linlin Zong"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/d5c8dcc8f5c87c269780c7011a355b9202858847.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "d5c8dcc8f5c87c269780c7011a355b9202858847.pdf"
    },
    {
      "success": true,
      "doc_id": "08e6a1b0c436d22caa1fd03d3be334a5",
      "summary": "Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors.",
      "intriguing_abstract": "Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/a77b3c5f532e61af63a9d95e671ce02d8065ee24.pdf",
      "citation_key": "yang2024lwa",
      "metadata": {
        "title": "SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding",
        "authors": [
          "Ruiyi Yang",
          "Flora D. Salim",
          "Hao Xue"
        ],
        "published_date": "2024",
        "abstract": "Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors.",
        "file_path": "paper_data/knowledge_graph_embedding/info/a77b3c5f532e61af63a9d95e671ce02d8065ee24.pdf",
        "venue": "The Web Conference",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Knowledge graphs (KGs) have been increasingly employed for link prediction and recommendation using real-world datasets. However, the majority of current methods rely on static data, neglecting the dynamic nature and the hidden spatio-temporal attributes of real-world scenarios. This often results in suboptimal predictions and recommendations. Although there are effective spatio-temporal inference methods, they face challenges such as scalability with large datasets and inadequate semantic understanding, which impede their performance. To address these limitations, this paper introduces a novel framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing and exploring spatio-temporal KGs. To integrate spatial and temporal data into KGs, our framework exploited through a new 3-step embedding method. Output embeddings can be used for future temporal sequence prediction and spatial information recommendation, providing valuable insights for various applications such as retail sales forecasting and traffic volume prediction. Our framework offers a simple but comprehensive way to understand the underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of predictions and the relevance of recommendations. This work paves the way for more effective utilization of spatio-temporal data in KGs, with potential impacts across a wide range of sectors.",
        "keywords": []
      },
      "file_name": "a77b3c5f532e61af63a9d95e671ce02d8065ee24.pdf"
    },
    {
      "success": true,
      "doc_id": "8f88140ebf4adbb47007a2b78fd66afe",
      "summary": "The prosperity of software applications brings fierce market competition to developers. Employing third-party libraries (TPLs) to add new features to projects under development and to reduce the time to market has become a popular way in the community. However, given the tremendous TPLs ready for use, it is challenging for developers to effectively and efficiently identify the most suitable TPLs. To tackle this obstacle, we propose an innovative approach named PyRec to recommend potentially useful TPLs to developers for their projects. Taking Python project development as a use case, PyRec embeds Python projects, TPLs, contextual information, and relations between those entities into a knowledge graph. Then, it employs a graph neural network to capture useful information from the graph to make TPL recommendations. Different from existing approaches, PyRec can make full use of not only project-library interaction information but also contextual information to make more accurate TPL recommendations. Comprehensive evaluations are conducted based on 12,421 Python projects involving 963 TPLs, 9,675 extra entities, 121,474 library usage records, and 73,277 contextual records. Compared with five representative approaches, PyRec improves the recommendation performance significantly in all cases.",
      "intriguing_abstract": "The prosperity of software applications brings fierce market competition to developers. Employing third-party libraries (TPLs) to add new features to projects under development and to reduce the time to market has become a popular way in the community. However, given the tremendous TPLs ready for use, it is challenging for developers to effectively and efficiently identify the most suitable TPLs. To tackle this obstacle, we propose an innovative approach named PyRec to recommend potentially useful TPLs to developers for their projects. Taking Python project development as a use case, PyRec embeds Python projects, TPLs, contextual information, and relations between those entities into a knowledge graph. Then, it employs a graph neural network to capture useful information from the graph to make TPL recommendations. Different from existing approaches, PyRec can make full use of not only project-library interaction information but also contextual information to make more accurate TPL recommendations. Comprehensive evaluations are conducted based on 12,421 Python projects involving 963 TPLs, 9,675 extra entities, 121,474 library usage records, and 73,277 contextual records. Compared with five representative approaches, PyRec improves the recommendation performance significantly in all cases.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/2d12d1cec23e1c26c65de52100db70d91ca90035.pdf",
      "citation_key": "li20246qx",
      "metadata": {
        "title": "Neural Library Recommendation by Embedding Project-Library Knowledge Graph",
        "authors": [
          "Bo Li",
          "Haowei Quan",
          "Jiawei Wang",
          "Pei Liu",
          "Haipeng Cai",
          "Yuan Miao",
          "Yun Yang",
          "Li Li"
        ],
        "published_date": "2024",
        "abstract": "The prosperity of software applications brings fierce market competition to developers. Employing third-party libraries (TPLs) to add new features to projects under development and to reduce the time to market has become a popular way in the community. However, given the tremendous TPLs ready for use, it is challenging for developers to effectively and efficiently identify the most suitable TPLs. To tackle this obstacle, we propose an innovative approach named PyRec to recommend potentially useful TPLs to developers for their projects. Taking Python project development as a use case, PyRec embeds Python projects, TPLs, contextual information, and relations between those entities into a knowledge graph. Then, it employs a graph neural network to capture useful information from the graph to make TPL recommendations. Different from existing approaches, PyRec can make full use of not only project-library interaction information but also contextual information to make more accurate TPL recommendations. Comprehensive evaluations are conducted based on 12,421 Python projects involving 963 TPLs, 9,675 extra entities, 121,474 library usage records, and 73,277 contextual records. Compared with five representative approaches, PyRec improves the recommendation performance significantly in all cases.",
        "file_path": "paper_data/knowledge_graph_embedding/info/2d12d1cec23e1c26c65de52100db70d91ca90035.pdf",
        "venue": "IEEE Transactions on Software Engineering",
        "citationCount": 3,
        "score": 3.0,
        "summary": "The prosperity of software applications brings fierce market competition to developers. Employing third-party libraries (TPLs) to add new features to projects under development and to reduce the time to market has become a popular way in the community. However, given the tremendous TPLs ready for use, it is challenging for developers to effectively and efficiently identify the most suitable TPLs. To tackle this obstacle, we propose an innovative approach named PyRec to recommend potentially useful TPLs to developers for their projects. Taking Python project development as a use case, PyRec embeds Python projects, TPLs, contextual information, and relations between those entities into a knowledge graph. Then, it employs a graph neural network to capture useful information from the graph to make TPL recommendations. Different from existing approaches, PyRec can make full use of not only project-library interaction information but also contextual information to make more accurate TPL recommendations. Comprehensive evaluations are conducted based on 12,421 Python projects involving 963 TPLs, 9,675 extra entities, 121,474 library usage records, and 73,277 contextual records. Compared with five representative approaches, PyRec improves the recommendation performance significantly in all cases.",
        "keywords": []
      },
      "file_name": "2d12d1cec23e1c26c65de52100db70d91ca90035.pdf"
    },
    {
      "success": true,
      "doc_id": "5e9540495b4c6c753bd7fa8f98824f5b",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/4b1d0cf2b99aec85cdedceaef88c3a074de79832.pdf",
      "citation_key": "liu2024mji",
      "metadata": {
        "title": "SecKG2vec: A novel security knowledge graph relational reasoning method based on semantic and structural fusion embedding",
        "authors": [
          "Xiaojian Liu",
          "Xinwei Guo",
          "Wen Gu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/4b1d0cf2b99aec85cdedceaef88c3a074de79832.pdf",
        "venue": "Computers & security",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "4b1d0cf2b99aec85cdedceaef88c3a074de79832.pdf"
    },
    {
      "success": true,
      "doc_id": "fd69b40cdc34ff858120a3ff6b895145",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/0845cea58467d372eb296fa1f184ecabe02be18b.pdf",
      "citation_key": "chen2024efo",
      "metadata": {
        "title": "Quality assessment of cyber threat intelligence knowledge graph based on adaptive joining of embedding model",
        "authors": [
          "Bin Chen",
          "Hongyi Li",
          "Di Zhao",
          "Yitang Yang",
          "Chengwei Pan"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/0845cea58467d372eb296fa1f184ecabe02be18b.pdf",
        "venue": "Complex &amp; Intelligent Systems",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "0845cea58467d372eb296fa1f184ecabe02be18b.pdf"
    },
    {
      "success": true,
      "doc_id": "692d9494c92368e6aebf1ee866cd5876",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/6a9caace1919b0e7bb247f0ecb585068c1ec4ff8.pdf",
      "citation_key": "chen2024uld",
      "metadata": {
        "title": "Embedding dynamic graph attention mechanism into Clinical Knowledge Graph for enhanced diagnostic accuracy",
        "authors": [
          "Deng Chen",
          "Weiwei Zhang",
          "Zuohua Ding"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/6a9caace1919b0e7bb247f0ecb585068c1ec4ff8.pdf",
        "venue": "Expert systems with applications",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "6a9caace1919b0e7bb247f0ecb585068c1ec4ff8.pdf"
    },
    {
      "success": true,
      "doc_id": "e4ccf3134604b8cb905828ed3cc138ca",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/30321b036607a7936221235ea8ec7cf7c1627100.pdf",
      "citation_key": "wang2017zm5",
      "metadata": {
        "title": "Knowledge Graph Embedding: A Survey of Approaches and Applications",
        "authors": [
          "Quan Wang",
          "Zhendong Mao",
          "Bin Wang",
          "Li Guo"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/knowledge_graph_embedding/info/30321b036607a7936221235ea8ec7cf7c1627100.pdf",
        "venue": "IEEE Transactions on Knowledge and Data Engineering",
        "citationCount": 2284,
        "score": 285.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "30321b036607a7936221235ea8ec7cf7c1627100.pdf"
    },
    {
      "success": true,
      "doc_id": "19f99704e5a08f0c76b81e1f78825f78",
      "summary": "Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relation-path. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.",
      "intriguing_abstract": "Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relation-path. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/knowledge_graph_embedding/e03b8e02ddda86eafb54cafc5c44d231992be95a.pdf",
      "citation_key": "li2021qr0",
      "metadata": {
        "title": "Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks",
        "authors": [
          "Zhifei Li",
          "Hai Liu",
          "Zhaoli Zhang",
          "Tingting Liu",
          "N. Xiong"
        ],
        "published_date": "2021",
        "abstract": "Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relation-path. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.",
        "file_path": "paper_data/knowledge_graph_embedding/info/e03b8e02ddda86eafb54cafc5c44d231992be95a.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 246,
        "score": 61.5,
        "summary": "Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relation-path. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "e03b8e02ddda86eafb54cafc5c44d231992be95a.pdf"
    }
  ]
}