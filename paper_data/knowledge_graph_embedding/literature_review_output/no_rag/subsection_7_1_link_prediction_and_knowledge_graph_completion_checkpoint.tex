\subsection{Link Prediction and Knowledge Graph Completion}
Link prediction (LP) and knowledge graph completion (KGC) represent the fundamental applications of knowledge graph embedding (KGE), aiming to infer missing facts and enhance the completeness of knowledge graphs. These tasks are crucial for making KGs more robust and informative for downstream AI systems by automatically inferring unobserved facts within the graph structure. The evolution of KGE models for LP/KGC reflects a continuous effort to improve accuracy, handle complex relational patterns, and address practical challenges.

Early KGE models primarily leveraged geometric transformations to represent entities and relations. Translational models, such as TransH \cite{wang2014} and TransD \cite{ji2015}, extended the foundational TransE by modeling relations as translations on hyperplanes or through dynamic mapping matrices, respectively. TransH notably improved the handling of one-to-many and many-to-one relations by allowing entity projections, while TransD further refined this by considering the diversity of both entities and relations. However, these models, while efficient, often struggled to capture more intricate logical patterns like symmetry, antisymmetry, inversion, and composition. This limitation spurred the development of rotational models, with RotatE \cite{sun2018} defining relations as rotations in complex vector spaces. RotatE demonstrated superior expressiveness for these complex patterns, significantly outperforming its translational predecessors in link prediction tasks. Further geometric innovations include embedding entities on Lie groups (e.g., TorusE \cite{ebisu2017}) to address regularization issues, exploring alternative metrics like the Cycle metric \cite{yang2021} for enhanced expressiveness, and introducing powerful transformations such as Householder parameterization (HousE \cite{li2022}) or compound operations (CompoundE \cite{ge2022}, CompoundE3D \cite{ge2023}) to capture a broader spectrum of relational semantics. More recently, models like HolmE \cite{zheng2024} have focused on ensuring closure under composition, a theoretical property vital for modeling under-represented compositional patterns, while MQuinE \cite{liu2024} addresses specific theoretical deficiencies ("Z-paradox") in existing models to enhance expressiveness. While these geometric models offer mathematical elegance and interpretability, their ability to capture highly complex, non-linear interactions can be limited compared to deep learning approaches.

The advent of deep learning architectures marked a significant paradigm shift in KGE for LP/KGC. Convolutional Neural Networks (CNNs) have been widely adopted to extract local features and model interactions between entity and relation embeddings. Models like AcrE \cite{ren2020} and ReInceptionE \cite{xie2020} utilize various convolutional layers and attention mechanisms to capture complex relation patterns and aggregate entity-specific features, achieving state-of-the-art results. More recent CNN-based methods, such as CNN-ECFA \cite{hu2024} and SEConv \cite{yang2025}, continue to refine feature aggregation and interaction. Graph Neural Networks (GNNs) and attention mechanisms further leverage the graph's topology, with models like DisenKGAT \cite{wu2021} employing disentangled graph attention networks for diverse representations and GAATs \cite{wang2020} incorporating graph attenuated attention to capture rich neighborhood information. Transformer-based architectures, including CoKE \cite{wang2019}, Knowformer \cite{li2023}, and TGformer \cite{shi2025}, treat KGs as sequences or integrate graph structures into Transformer frameworks, leveraging self-attention to capture long-range dependencies and contextualized representations. These deep learning models excel at learning intricate, non-linear, and context-dependent features, often surpassing purely geometric approaches in accuracy, but typically incur higher computational costs and can be less interpretable.

Beyond structural information, KGE models for LP/KGC have been enriched by incorporating auxiliary information, logical rules, and multi-modal data. Integrating entity types, as seen in TransET \cite{wang2021} and TaKE \cite{he2023}, provides semantic guidance that significantly improves KG completion, especially in low-resource settings. Hyper-relational KGE models like HINGE \cite{rosso2020} move beyond simple triplets to incorporate associated key-value pairs, capturing richer data semantics. Rule-based KGE approaches, such as RUGE \cite{guo2017} and RulE \cite{tang2022}, iteratively guide embedding learning with soft logical rules, enhancing reasoning capabilities and ensuring semantic consistency, though rule extraction and balancing rule adherence with flexibility remain challenges. Furthermore, multi-modal KGEs, exemplified by SSP \cite{xiao2016} which projects text descriptions into semantic space, and Joint Language Semantic and Structure Embedding \cite{shen2022} which integrates pre-trained language models, enrich embeddings with external semantic context, crucial for overcoming data sparsity and improving understanding.

The practical effectiveness of KGE for LP/KGC also heavily relies on training optimizations and robustness. Negative sampling strategies, such as Confidence-Aware Negative Sampling \cite{shan2018} and NSCaching \cite{zhang2018}, are critical for efficient and effective training, though the "true" negative distribution remains a theoretical gap. Some approaches, like Efficient Non-Sampling KGE \cite{li2021}, even attempt to avoid negative sampling entirely to achieve more stable performance, albeit with increased computational complexity. Robustness against noisy data is addressed by methods like those using multi-task reinforcement learning \cite{zhang2021} to filter out erroneous triples or weighted training schemes (e.g., WeightE \cite{zhang2023}) to handle data imbalance. The continuous development across these diverse methodologies underscores the central role of link prediction and knowledge graph completion in advancing the utility and reliability of knowledge graphs for a wide array of AI applications \cite{dai2020, cao2022, ge2023, rossi2020}.