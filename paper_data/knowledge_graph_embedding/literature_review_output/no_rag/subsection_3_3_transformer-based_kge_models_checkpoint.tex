\subsection*{Transformer-based KGE Models}
The emergence of Transformer architectures, initially lauded for their unparalleled success in natural language processing, has profoundly influenced the landscape of knowledge graph embedding (KGE). These models, characterized by their self-attention mechanisms, offer a powerful paradigm for capturing long-range dependencies and generating highly contextualized representations, capabilities that were often limited in earlier KGE approaches. The adaptation of Transformers to knowledge graphs represents a significant methodological evolution within the "Deep Learning Architectures for KGE" subgroup, pushing the boundaries of expressiveness beyond traditional geometric or simpler neural network models.

Early adaptations of Transformers to KGE primarily treated knowledge graphs as sequences of entities and relations. A pioneering example is CoKE (Contextualized Knowledge Graph Embedding) \cite{wang2019}, which frames edges and paths within a KG as sequences. By feeding these sequences into a Transformer encoder, CoKE learns dynamic, flexible, and fully contextualized embeddings for entities and relations. This approach marked a critical shift from static, context-independent embeddings, allowing the model to capture varying properties of entities and relations based on their surrounding graph context. While CoKE demonstrated superior performance in link prediction and path query answering, its sequence-centric view inherently grappled with the graph's non-sequential nature, potentially overlooking intricate topological structures that are not easily linearized. This limitation highlights a theoretical gap: how to reconcile the order-invariance of self-attention with the directed, ordered nature of relational facts in KGs.

Subsequent research has focused on explicitly integrating graph structures into Transformer frameworks, moving beyond simple sequence linearization. Knowformer \cite{li2023} directly addresses the challenge of order invariance inherent in the self-attention mechanism, which struggles to distinguish between a valid triplet (subject, relation, object) and its shuffled variants. Knowformer innovatively incorporates relational compositions into entity representations, explicitly injecting semantics and capturing the role of an entity based on its position (subject or object) within a relation triplet. This design choice allows the Transformer to correctly capture relational semantics by distinguishing entity roles, a crucial advancement for modeling complex relational patterns that simpler translation-based models like TransD \cite{ji2015} or even rotational models like RotatE \cite{sun2018} might struggle to fully express without explicit positional encoding. Knowformer's ability to integrate positional awareness and relational semantics directly into the self-attention mechanism significantly enhances its expressiveness, particularly for modeling complex contextual information.

The latest advancements, such as TGformer \cite{shi2025}, further refine the integration of Transformer architectures with graph structures, proposing a general graph Transformer framework for KGE. TGformer is notable for being the first to explicitly leverage a graph Transformer to build knowledge embeddings that incorporate both triplet-level and graph-level structural features. This comprehensive approach addresses a critical limitation of previous methods: triplet-based models often ignore the broader graph structure, while some graph-based methods (e.g., certain GNNs like DisenKGAT \cite{wu2021}) might overlook the specific contextual information of individual nodes within a triplet. By constructing context-level subgraphs for each predicted triplet and employing a Knowledge Graph Transformer Network (KGTN), TGformer fully explores multi-structural features, boosting the model's understanding of entities and relations in diverse contexts. Furthermore, TGformer extends its capabilities to temporal knowledge graphs, a significant step towards handling the dynamic nature of real-world knowledge, aligning with the broader development direction of building "more powerful and comprehensive models that leverage advanced neural architectures to capture increasingly complex structural and contextual information, including temporal dynamics."

The primary strength of Transformer-based KGE models lies in their ability to capture global and local semantic relationships through self-attention, leading to highly contextualized representations. This contrasts with CNN-based KGE models like AcrE \cite{ren2020} or ReInceptionE \cite{xie2020}, which excel at extracting local features and interactions but may require additional mechanisms to capture long-range dependencies effectively. While CNN-ECFA \cite{hu2024} and SEConv \cite{yang2025} demonstrate the continued refinement of CNNs for feature aggregation, Transformers offer a more inherent capability for global context. Compared to foundational models like TorusE \cite{ebisu2017} or CyclE \cite{yang2021} that focus on refining the embedding space's geometry, Transformers provide a data-driven approach to learn complex, non-linear transformations and interactions, often achieving state-of-the-art performance in link prediction.

However, Transformer-based models also present trade-offs. Their computational complexity and high parameter count can pose scalability challenges for extremely large knowledge graphs, a practical constraint that needs careful consideration. While models like Knowformer address the initial order-invariance issue, the fundamental assumption of treating graph elements as sequences, even with sophisticated positional encodings, can sometimes be an oversimplification of the rich, multi-relational graph topology. The experimental setups for these models typically involve standard benchmark datasets, and while results are often superior, the generalizability to highly sparse or domain-specific KGs with limited data might still be a concern, requiring extensive pre-training or specialized fine-tuning. Despite these challenges, the innovative application of Transformers to graph structures, particularly in integrating multi-structural features and handling temporal dynamics, signifies a robust and adaptive direction for KGE research, continually pushing the state-of-the-art in capturing the intricate semantics of knowledge graphs.