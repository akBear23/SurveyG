\subsection{Rule-based and Constraint-driven KGE}
While purely data-driven knowledge graph embedding (KGE) models excel at capturing statistical patterns from observed triples, they often struggle with ensuring logical consistency, facilitating explicit reasoning, and providing interpretability aligned with human understanding. This subsection delves into approaches that integrate logical rules and explicit constraints directly into the KGE learning process, thereby injecting prior knowledge to address these limitations.

One foundational approach to injecting semantic consistency is exemplified by "Semantically Smooth Knowledge Graph Embedding" (SSE) \cite{guo2015}. This method enforces a "semantically smooth" embedding space, where entities belonging to the same semantic category are encouraged to lie close to each other. By employing manifold learning techniques, such as Laplacian Eigenmaps and Locally Linear Embedding, as regularization terms, SSE guides the embedding process to discover intrinsic geometric structures that reflect categorical semantics. While effective in promoting semantic coherence, SSE primarily relies on entity categories as a form of soft constraint, which is less explicit than logical rules and might not directly enhance complex reasoning capabilities. Its strength lies in its generality, as the smoothness assumption can be applied to various embedding models and constructed from diverse information beyond just entity categories.

A significant advancement in this domain is the explicit incorporation of logical rules. Early rule-based methods often relied on hard rules, which are rigid and require extensive manual curation. However, real-world knowledge graphs are often noisy and incomplete, making hard rules brittle. "Knowledge Graph Embedding with Iterative Guidance from Soft Rules" (RUGE) \cite{guo2017} introduced a novel paradigm to iteratively integrate soft rules (rules associated with confidence levels) into the embedding learning process. RUGE simultaneously learns from observed triples, unlabeled triples (whose labels are iteratively predicted), and automatically extracted soft rules. This iterative guidance allows the knowledge embodied in rules to be progressively transferred into the learned embeddings, leading to more robust representations. The key advantage of RUGE is its ability to leverage abundant, albeit uncertain, automatically extracted rules, moving beyond the limitations of manually curated hard rules. This iterative feedback loop between rule inference and embedding updates represents a crucial step towards deeply intertwining symbolic logic with subsymbolic representations.

Complementing complex rule integration, simpler structural constraints can also significantly enhance KGE models. "Improving Knowledge Graph Embedding Using Simple Constraints" \cite{ding2018} demonstrated that even straightforward constraints can yield substantial improvements in interpretability and structure without adding significant computational overhead. Specifically, this work explored non-negativity constraints on entity representations, which help learn compact and interpretable features, and approximate entailment constraints on relation representations. These entailment constraints encode regularities of logical entailment between relations, structuring the embedding space to reflect hierarchical or inferential relationships. While these constraints are less expressive than full first-order logic rules, their simplicity makes them highly efficient and broadly applicable, offering a practical trade-off between model complexity and the benefits of injected prior knowledge.

More recent contributions have further refined the integration of soft rules. "Knowledge Graph Embedding Preserving Soft Logical Regularity" \cite{guo2020} focused on imposing soft rule constraints directly on relation representations. By representing relations as bilinear forms and mapping entity representations into a non-negative and bounded space, the method derives a rule-based regularization that enforces relation representations to satisfy rule-introduced constraints. A notable strength of this approach is its improved scalability, as the complexity of rule learning becomes independent of the entity set size, making it more feasible for large-scale KGs. This direct regularization of relations ensures that logical patterns are preserved in the relational space, which is crucial for consistent reasoning.

Building on these foundations, RulE ("Rule Embedding") \cite{tang2022} presents a principled framework that learns rule embeddings jointly with entity and relation embeddings within a unified vector space. Unlike previous methods that might treat rules as external regularization, RulE explicitly represents rules as vectors, allowing for soft logical inference directly within the embedding space. This deep integration enables rule embeddings to regularize and enrich entity/relation embeddings, leading to more coherent and reasoning-capable representations. RulE's ability to calculate a confidence score for each rule based on its consistency with observed triples further refines the "softness" of logical inference, alleviating the brittleness often associated with strict logic. This joint learning paradigm represents a sophisticated approach to intertwining symbolic knowledge with subsymbolic representations, pushing the boundaries of reasoning capabilities within KGEs.

Collectively, these rule-based and constraint-driven methods highlight a critical evolution in KGE research. They move beyond purely data-driven models (like those in the "Core KGE Model Architectures" subgroup) by leveraging explicit logical knowledge to enhance robustness, improve reasoning, and increase interpretability. The progression from general semantic smoothness \cite{guo2015} to iterative soft rule guidance \cite{guo2017}, the application of simple yet effective structural constraints \cite{ding2018}, and finally to the joint embedding of rules themselves \cite{tang2022} demonstrates a continuous effort to bridge the gap between symbolic logic and continuous vector spaces.

However, several challenges persist. A primary limitation across many rule-based approaches is the reliance on the availability and quality of logical rules. While methods like RUGE can leverage automatically extracted soft rules, the efficiency and accuracy of such rule extraction remain a practical hurdle. Furthermore, balancing the strict adherence to logical rules with the flexibility to capture exceptions or nuanced patterns not covered by rules is a delicate trade-off. Over-constraining the embedding space with rules might inadvertently reduce its expressiveness or ability to generalize to unseen, complex scenarios. The scalability of managing and applying very large and complex rule sets, particularly for higher-order logic, also poses a significant challenge, despite efforts like \cite{guo2020} to optimize rule learning complexity. Future research needs to explore more robust and automated rule discovery mechanisms, develop adaptive frameworks that can dynamically weigh rule adherence against data-driven insights, and investigate more principled theoretical foundations for combining probabilistic logic with continuous embeddings.