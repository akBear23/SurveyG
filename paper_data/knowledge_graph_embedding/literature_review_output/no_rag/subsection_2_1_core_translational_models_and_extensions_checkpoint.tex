\subsection{Core Translational Models and Extensions}
The advent of knowledge graph embedding (KGE) marked a significant paradigm shift from purely symbolic knowledge representation to continuous vector spaces, offering enhanced efficiency and expressiveness for tasks such as link prediction and knowledge graph completion. At the forefront of this transformation were the translational models, which posited that a relation could be represented as a translation operation in an embedding space, moving a head entity vector closer to a tail entity vector. This foundational idea was first popularized by TransE, a pioneering model for its simplicity and computational efficiency \cite{wang2014}. TransE models a triple $(h, r, t)$ by enforcing the constraint $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$, where $\mathbf{h}, \mathbf{r}, \mathbf{t}$ are the embeddings of the head entity, relation, and tail entity, respectively. While remarkably effective for its time, TransE exhibited limitations in handling complex relational patterns, particularly one-to-many, many-to-one, and reflexive relations, where a single relation vector could not adequately distinguish between multiple valid tail entities for a given head, or vice-versa.

To address these inherent limitations, subsequent models extended the translational paradigm by introducing more sophisticated mechanisms. TransH emerged as a notable improvement, proposing to model relations as translations on relation-specific hyperplanes rather than directly in the entity embedding space \cite{wang2014}. Specifically, for a triple $(h, r, t)$, TransH projects the head and tail entity embeddings ($\mathbf{h}, \mathbf{t}$) onto a hyperplane defined by the relation $\mathbf{r}$'s normal vector $\mathbf{w}_r$, resulting in projected entities $\mathbf{h}_{\perp}$ and $\mathbf{t}_{\perp}$. The translational assumption then applies to these projected vectors: $\mathbf{h}_{\perp} + \mathbf{d}_r \approx \mathbf{t}_{\perp}$, where $\mathbf{d}_r$ is the relation-specific translation vector on the hyperplane. This mechanism allows TransH to better distinguish entities involved in one-to-many or many-to-one relations, as different entity pairs can be projected onto different points on the hyperplane while sharing the same relation vector. For instance, if a person has multiple children, TransH can project the person and each child onto the 'has\_child' hyperplane, allowing distinct representations for each child while maintaining the 'has\_child' relation. This approach offered a crucial trade-off, significantly improving expressiveness for complex relation types with almost the same model complexity as TransE, thereby maintaining scalability \cite{wang2014}. The recent review by \cite{asmara2023} further underscores TransH's importance in addressing these early challenges.

Building upon the concept of relation-specific transformations, TransD further refined the translational approach by introducing dynamic mapping matrices for entities and relations \cite{ji2015}. Unlike TransH, which uses a single hyperplane per relation, TransD employs two vectors for each entity and relation: one representing its meaning and another for constructing a dynamic mapping matrix. This allows for more fine-grained, entity-specific projections, where the projection matrix for a relation is dynamically constructed based on both the entity and relation vectors. The core idea is that different entities might interact with a relation in different ways, and a static projection (as in TransH) might not capture this diversity. TransD's dynamic mapping matrices provide a more adaptive mechanism to project entities into relation-specific spaces, thereby accounting for the diversity of both relations and entities. A significant advantage of TransD over its predecessors like TransR/CTransR (which used static, larger projection matrices) is its reduced parameter count and avoidance of computationally intensive matrix-vector multiplication operations, making it more scalable for large knowledge graphs \cite{ji2015}. This efficiency gain, while increasing expressiveness, was a critical step in making KGE models practical for real-world applications.

These core translational models and their extensions collectively established a fundamental paradigm for KGE. They demonstrated that representing symbolic knowledge in continuous vector spaces could not only be efficient but also expressive enough to capture intricate relational semantics. While TransH and TransD significantly improved upon TransE's ability to model one-to-many/many-to-one relations, they still operated within the limitations of a Euclidean embedding space and relatively simple geometric transformations. This inherent simplicity, while beneficial for efficiency, meant they struggled with more complex logical patterns such as symmetry, antisymmetry, inversion, and composition, which later models like RotatE would address more elegantly through rotational transformations in complex spaces \cite{sun2018}. Nevertheless, the foundational work of TransE, TransH, and TransD laid the essential groundwork, proving the viability of the embedding approach and setting the stage for the diverse array of KGE models that continue to influence modern research, as highlighted in various surveys \cite{dai2020, cao2022}. Their emphasis on balancing model capacity with computational efficiency remains a crucial design principle in the field.