\subsection{Explainability, Interpretability, and Trustworthiness}
The increasing deployment of Knowledge Graph Embedding (KGE) models in sensitive and high-stakes domains, such as healthcare, finance, and scientific discovery, necessitates a profound shift from merely achieving high predictive accuracy to ensuring transparency, understandability, and reliability. Despite their impressive performance, many KGE models often function as "black boxes," making it challenging to discern the rationale behind their inferences. This critical need has spurred research into three interconnected yet distinct concepts: interpretability, explainability, and trustworthiness.

**Interpretability** refers to the degree to which a human can comprehend the cause of a model's decision or the meaning encoded within its internal representations. In KGE, this involves understanding the semantic properties or relational patterns captured by the learned embedding dimensions. For instance, \cite{tran2019j42} explored the semantic structures within KGE spaces, proposing methods to analyze these structures and perform algebraic operations (e.g., similarity, analogy queries) to shed light on the relationships between entities. Their work aims to make the underlying knowledge more accessible for data exploration, providing insights into what the model has learned. Extending this, \cite{li2021} delved into *how* KGE models extrapolate to unseen data, proposing a "Semantic Evidence" view. They identified three levels of evidence (relation-level, entity-level, triple-level) observable from the training set that strongly correlate with extrapolation ability. By understanding these data-centric factors, researchers can interpret the mechanisms of generalization, thereby making the model's behavior more predictable and understandable. However, a significant challenge remains in directly mapping dense, high-dimensional embedding vectors to genuinely human-understandable features without losing fidelity, often requiring complex dimensionality reduction or projection techniques whose own interpretability can be questioned.

**Explainability**, in contrast, focuses on providing post-hoc justifications for specific predictions or recommendations, addressing the question of *why* a particular inference was made. A prevalent approach involves extracting symbolic explanations from the underlying knowledge graph. For example, \cite{islam2023} developed an ensemble KGE approach for drug repurposing that not only delivered robust predictions but also generated *rule-based explanations*. These explanations, derived from specific meta-paths or logical rules within the knowledge graph, elucidate the predicted drug-disease association. Such explicit justifications are paramount in critical applications like medicine, enabling domain experts to validate the model's reasoning and fostering confidence. Beyond single KGE models, \cite{kurokawa2021f4f} proposed an explainable knowledge reasoning framework that combines *multiple* KGE techniques with corresponding explainable AI methods, highlighting the complexity and multi-faceted nature of providing explanations, especially when integrating diverse knowledge sources. The primary challenge for explainability lies in ensuring that these explanations are both faithful to the model's internal, often subsymbolic, reasoning and genuinely comprehensible and actionable for human users, particularly for complex multi-hop inferences or those involving non-Euclidean embedding spaces.

**Trustworthiness** is a broader, overarching concept encompassing the reliability, robustness, validity, and confidence one can place in a KGE system's outputs. It integrates aspects of both interpretability and explainability, alongside rigorous validation, theoretical guarantees, and uncertainty quantification.

A foundational pillar of trustworthiness is **robust validation and reproducibility**. The KGE field has historically faced challenges regarding the consistency of reported results. \cite{ali2020} addressed this by developing PyKEEN, a unified, open-source framework designed to enable fair and consistent comparison of diverse KGE models. This initiative was vital for establishing reliable baselines and fostering trust in research findings through independent verification. Building on this, \cite{lloyd2022} critically investigated the impact of hyperparameters on KGE quality and, crucially, identified and rectified data leakage issues in benchmark datasets like UMLS. Their contribution of a leakage-robust variant (UMLS-43) is fundamental for ensuring that KGE performance claims are based on sound and unbiased evaluations. Despite these efforts, maintaining universal reproducibility across diverse hardware and software environments remains an ongoing challenge, requiring continuous community vigilance.

Trustworthiness is also significantly bolstered by **strong theoretical foundations and expressiveness guarantees**. \cite{zheng2024} (HolmE) introduced a Riemannian KGE model that is "closed under composition," offering stronger theoretical guarantees for modeling complex relational patterns and unifying several existing models. Such theoretical soundness contributes to a more predictable and fundamentally understandable KGE behavior, increasing trust in its inferences. A critical advancement in this area is the identification and resolution of fundamental expressiveness paradoxes. \cite{liu2024} identified the "Z-paradox," a fundamental deficiency causing many popular KGE models to incorrectly infer relationships based on specific graph patterns, leading to false positives and significant accuracy drops. They proposed MQuinE (Matrix Quin tuple Embedding), a novel KGE model theoretically proven to circumvent this paradox while maintaining strong expressiveness. This work directly enhances the validity and reliability of KGE predictions by addressing an inherent flaw. Furthermore, \cite{gebhart2021gtp} introduced a sheaf-theoretic framework for KGE, offering a generalized mathematical language for reasoning about KGE models and expressing prior constraints, which can lead to more consistent and trustworthy embeddings by providing a deeper theoretical understanding of their underlying structure.

Another critical component of trustworthiness is **uncertainty quantification**. Providing confidence scores or probability distributions for predictions allows users to gauge the reliability of an inference. \cite{chen2021i5t} (PASSLEAF) directly tackled this by proposing a framework for embedding uncertain knowledge graphs, where each relation is associated with a confidence score. PASSLEAF incorporates different scoring functions to predict relation confidence scores and leverages a semi-supervised learning model to augment learning. This approach explicitly models and predicts the confidence associated with relations, thereby enhancing trustworthiness by enabling users to understand the degree of certainty in a prediction. Probabilistic KGE models, such as those employing Gaussian embeddings or Bayesian approaches, inherently provide uncertainty estimates, which are invaluable for decision-making in sensitive applications. However, integrating robust uncertainty quantification into all KGE architectures and effectively communicating these uncertainties to end-users remains an active research area.

Finally, trustworthiness is also intrinsically linked to a model's **robustness against real-world data imperfections** (discussed in detail in Section 5.3) and its **performance in privacy-preserving distributed settings** (explored in Section 7.5). For instance, methods that handle data imbalance \cite{zhang2023} or mitigate the impact of noisy triples \cite{zhang2024} directly contribute to the reliability and trustworthiness of KGE predictions. Similarly, frameworks like PFedEG \cite{zhang2024}, which enable personalized federated KGE while addressing semantic disparities among clients, are crucial for fostering trust in KGE systems deployed in privacy-sensitive, collaborative environments.

In summary, the evolution of KGE research reflects a growing commitment to moving beyond mere predictive accuracy towards systems that are transparent, interpretable, and trustworthy. While significant progress has been made in establishing robust evaluation practices, mitigating fundamental expressiveness limitations, providing explicit explanations, and building theoretically sound models, future work must focus on developing standardized metrics for evaluating the faithfulness and utility of explanations for complex multi-hop relational paths. Further research is needed to explore user-centric explainable AI, investigating the cognitive load and decision-making impact of different explanation types (e.g., path-based vs. counterfactual) on domain experts. Moreover, integrating uncertainty quantification more pervasively across diverse KGE architectures and applications, and exploring intrinsically interpretable KGE models, are crucial to truly unlock their potential in high-stakes real-world scenarios.