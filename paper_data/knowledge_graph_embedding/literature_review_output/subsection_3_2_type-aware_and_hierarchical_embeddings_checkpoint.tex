\subsection{Type-Aware and Hierarchical Embeddings}

Knowledge graphs (KGs) often possess rich ontological schemas, including semantic type information and hierarchical structures, which are crucial for enhancing the logical consistency and accuracy of learned embeddings. This subsection explores Knowledge Graph Embedding (KGE) models that explicitly leverage entity types, class hierarchies, or logical rules to constrain and refine the embedding process, ensuring that learned representations respect these inherent semantic categories and structural relationships.

Early efforts to incorporate semantic information focused on ensuring a smooth embedding space. \cite{guo2015} introduced Semantically Smooth Embedding (SSE), which leverages entity categories to enforce a "semantically smooth" embedding space, ensuring entities within the same category are embedded closely. Building on this idea of context-dependent representations, \cite{yoon2016} proposed Logical Property Preserving (lpp) variants of TransE, TransR, and TransD. These models utilize role-specific projections to map entities to distinct representations based on their position (head or tail) in a triple, thereby preserving logical properties like transitivity and symmetricity, which are foundational to hierarchical relations.

A significant advancement in explicitly differentiating semantic categories came with \cite{lv2018}'s TransC (Translating Concepts). TransC innovatively models concepts as spheres and instances as vectors in the embedding space, capturing the inherent difference between a category and its members. This geometric distinction allows for the natural preservation of `instanceOf` and `subClassOf` transitivity by requiring instance vectors to lie within concept spheres, and sub-concept spheres within super-concept spheres. Extending the idea of structured relations, \cite{zhang2018} introduced a three-layer Hierarchical Relation Structure (HRS) to enrich KGE models. This approach categorizes relations into clusters, individual relations, and fine-grained sub-relations, allowing KGE models to learn more nuanced representations by explicitly leveraging these hierarchical dependencies within the relational schema.

Another powerful avenue for incorporating semantic constraints is through logical rules, which can implicitly define types and hierarchies. \cite{guo2017} proposed RUGE (RUle-Guided Embedding), an iterative framework that combines KGE with automatically extracted soft logical rules. By iteratively predicting soft labels for unlabeled triples and rectifying embeddings based on these rule-guided labels, RUGE enhances knowledge acquisition and representation. Following this, \cite{guo2020} developed Soft Logical Rule Embedding (SLRE), a scalable method for preserving soft logical regularities. SLRE represents relations as bilinear forms and maps entities to a non-negative bounded space, deriving a novel rule-based regularization that directly enforces constraints on relation representations, making its complexity independent of the entity set size. Further advancing rule-guided learning, \cite{tang2022} introduced RulE, which learns explicit rule embeddings and jointly represents entities, relations, and logical rules in a unified continuous space. RulE calculates confidence scores for rules and aggregates them with KGE scores for soft, context-dependent inference, mitigating the brittleness of traditional logic.

The concept of projecting entities onto relation-specific hyperplanes, initially explored by \cite{wang2014} in TransH to handle complex relation mapping properties, has been adapted to incorporate type information. \cite{he2023} presented TaKE (Type-augmented Knowledge graph Embedding), a model-agnostic framework that learns implicit type features without explicit supervision. TaKE employs a relation-specific hyperplane mechanism to project an entity's type representation onto different hyperplanes, capturing diverse type features relevant to specific relations. Similarly, \cite{wang2021} proposed TransET, which explicitly leverages entity types to learn more semantic features. TransET uses circle convolution based on the embeddings of both entities and their types to generate type-specific representations for head and tail entities, which are then used in a translation-based score function. In a domain-specific application, \cite{zhu2022} integrated *category* embeddings (a form of type information) with structural and descriptive embeddings using a reverse-hyperplane projection for multimodal reasoning in Specific Disease Knowledge Graphs (SDKGs), demonstrating the utility of type-awareness in specialized hierarchical contexts.

More recently, hyperbolic geometry has emerged as a natural fit for modeling hierarchical structures due to its exponentially increasing volume with distance. \cite{pan2021} introduced a hyperbolic hierarchy-aware KGE model that leverages an extended Poincar√© Ball and a polar coordinate system to effectively represent both hierarchical structures and logical patterns. Building on this, \cite{liang2024} proposed Fully Hyperbolic Rotation (FHRE), which defines KGE operations entirely within the Lorentz hyperbolic space. FHRE models relations as Lorentz rotations, eliminating the need for complex and potentially unstable logarithmic and exponential mappings between hyperbolic and tangent spaces during training, thereby offering a more stable and direct approach to embedding hierarchical data.

Despite these advancements, challenges remain in seamlessly integrating diverse forms of type and hierarchical information while maintaining scalability and interpretability. Future research could focus on developing more adaptive mechanisms for inferring and utilizing implicit type information in highly dynamic and sparse KGs, exploring novel geometric spaces that can simultaneously capture multiple facets of semantic and structural hierarchies, and designing methods for more transparently explaining how type and hierarchical constraints influence embedding outcomes.