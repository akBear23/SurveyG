\subsection{Tensor Factorization and Semantic Matching}
Knowledge Graph Embedding (KGE) models often frame the task of learning entity and relation representations as a tensor factorization problem or employ direct semantic matching functions to assess the plausibility of a triple $(h, r, t)$. This paradigm focuses on capturing intricate interactions between entities and relations through algebraic operations, moving beyond purely geometric translations.

A foundational model in this category is RESCAL \cite{nickel2016}, which represents relations as full matrices, allowing for a direct, bilinear interaction between the head entity vector $\mathbf{h}$ and the tail entity vector $\mathbf{t}$ through the relation matrix $\mathbf{M}_r$. Specifically, the plausibility score for a triple $(h, r, t)$ is computed as $\mathbf{h}^\top \mathbf{M}_r \mathbf{t}$. This approach captures complex, direct interactions and can model various relational patterns effectively, but it is notably parameter-heavy, leading to high computational costs and scalability challenges for large knowledge graphs. The general class of such models is often referred to as tensor decomposition models in comprehensive surveys \cite{rossi2020}.

Building upon the insights of bilinear models, ComplEx \cite{trouillon2016} introduced a significant advancement by employing complex-valued embeddings for entities and relations. It defines a scoring function using a Hermitian dot product, which naturally models both symmetric and antisymmetric relations without requiring explicit constraints. ComplEx offered a more elegant and parameter-efficient solution for capturing these specific relational patterns compared to the more parameter-heavy RESCAL, demonstrating the power of higher-dimensional, algebraic representations.

The success of these semantic matching approaches spurred further research into generalizing and optimizing scoring functions. AutoSF \cite{zhang2019} proposed an automated machine learning (AutoML) framework to search for optimal scoring functions tailored to specific knowledge graphs. By identifying a unified representation for popular bilinear models, AutoSF could discover novel, KG-dependent scoring functions that consistently outperformed human-designed baselines, highlighting the diversity of optimal semantic matching strategies.

Further extending the expressiveness of semantic matching, models began to incorporate more dynamic and compound operations. TransD \cite{ji2015}, while rooted in the translational paradigm, introduced dynamic mapping matrices for both entities and relations. This allowed for a more fine-grained, relation-aware projection of entities into relation-specific spaces, effectively enabling a more flexible semantic interaction than fixed bilinear forms, while maintaining parameter efficiency. Similarly, TransMS \cite{yang2019} enhanced semantic interaction by modeling multidirectional semantic transmission (e.g., from head to tail entity via the relation, and from entities to relations) using nonlinear transformations, capturing richer semantic flows with remarkable parameter efficiency.

More sophisticated algebraic and geometric transformations have also been explored. CompoundE \cite{ge2022} generalized geometric operations by combining translation, rotation, and scaling into a cascaded "compound operation." This powerful form of semantic matching allows for a richer modeling of complex relation types and provides a unifying framework for several prior distance-based models. HousE \cite{li2022} further advanced this by introducing Householder parameterization for high-dimensional rotations and invertible projections. This unified framework can simultaneously model diverse intrinsic relation patterns (like symmetry, antisymmetry, composition) and complex relation mapping properties (e.g., 1-to-N, N-to-N), offering superior expressiveness through advanced linear algebra. In non-Euclidean spaces, models like TorusE \cite{ebisu2017} embedded entities and relations on a Lie group (torus) to resolve regularization conflicts in translation-based models, providing a novel mathematical space for semantic matching. Similarly, Fully Hyperbolic Rotation (FHRE) \cite{liang2024} performs direct rotational semantic matching within Lorentz hyperbolic space, leveraging its properties for hierarchical data without the overhead of frequent space mappings. Ji et al. \cite{ji2024} employed quaternion embeddings, representing relations as rotations in a complex space, to model multihop fuzzy spatiotemporal knowledge graphs, effectively capturing noncommutative compositional patterns.

The advent of neural networks further expanded the capabilities of semantic matching models by allowing for highly flexible and non-linear scoring functions. M-DCN \cite{zhang2020} utilized multi-scale dynamic convolutional networks with relation-specific filters to extract richer feature interactions from concatenated entity and relation embeddings. ReInceptionE \cite{xie2020} combined an Inception network for deep interaction learning with a relation-aware attention mechanism to integrate both local neighborhood and global entity structural information into the query embedding, enhancing semantic matching with contextual awareness. SEConv \cite{yang2025} applied self-attention and multilayer CNNs to learn deeper structural features from triplets, particularly for medical knowledge graphs, demonstrating the power of neural architectures in specialized domains. Graph Neural Networks (GNNs) and Transformers have also been adapted for semantic matching. LAN \cite{wang2018} introduced a Logic Attention Network for inductive KGE, aggregating neighborhood information with attention. SE-GNN \cite{li2021} explicitly modeled different levels of "semantic evidence" (relation, entity, triple) using GNNs and attention mechanisms for improved extrapolation. MorsE \cite{chen2021} used a GNN modulator within a meta-learning framework to transfer meta-knowledge for inductive entity embedding. More recently, CPa-WAC \cite{modak2024} proposed a weighted aggregation composition GCN, and TGformer \cite{shi2025} leveraged a Graph Transformer framework to capture multi-structural and contextual features for robust entity and relation understanding. MGTCA \cite{shang2024} introduced mixed geometry messages and a trainable convolutional attention network for adaptive GNN switching, further enhancing the flexibility of neural semantic matching. For specific applications, CKGE \cite{yang2023} employed a KG-based Transformer with relational attention for explainable talent training course recommendations, while RKGE \cite{sun2018} used recurrent networks to learn path semantics for effective recommendation. SpherE \cite{li2024} introduced a novel semantic matching function by embedding entities as spheres and relations as rotations, defining plausibility through sphere overlap, which directly addresses the problem of set retrieval for many-to-many relations.

In conclusion, the evolution of tensor factorization and semantic matching models has progressed from simple bilinear interactions to highly expressive neural architectures and multi-geometric spaces. While early models like RESCAL established the foundation for direct algebraic interaction, later works introduced complex-valued embeddings, dynamic projections, compound geometric operations, and sophisticated neural networks to capture increasingly nuanced relational patterns and contextual information. A persistent challenge remains in balancing model expressiveness with interpretability and computational efficiency, as the optimal semantic matching strategy often depends heavily on the specific characteristics of the knowledge graph and the downstream application.