\subsection{Context-Augmented and Multi-view Embeddings}

Traditional Knowledge Graph Embedding (KGE) models primarily derive representations from the intrinsic (head, relation, tail) triple structure, often struggling with data sparsity and the nuanced semantics of real-world knowledge graphs. To overcome these limitations, a significant line of research has focused on enriching entity and relation embeddings by integrating auxiliary information, such as textual descriptions, entity attributes, logical rules, or multi-modal data. These "multi-view" embeddings combine diverse feature types, often through specialized neural networks or attention mechanisms, to create more robust and semantically rich representations.

Early efforts to augment KGE models incorporated supplementary textual information. \cite{xiao2016} introduced Semantic Space Projection (SSP), which leverages textual descriptions by projecting the triple loss vector onto a dynamically generated semantic hyperplane, thereby modeling strong correlations between symbolic facts and text. Similarly, \cite{guo2015} proposed Semantically Smooth Embedding (SSE), which enforces a "semantically smooth" embedding space by constraining entities belonging to the same semantic category to be close in the vector space, utilizing manifold learning algorithms like Laplacian Eigenmaps. Building on the utility of entity types, \cite{wang2021} developed TransET, a model that employs circle convolution based on both entity and type embeddings to generate type-specific representations, enhancing the capture of semantic features. More recently, \cite{he2023} presented TaKE, a model-agnostic framework that automatically learns implicit type features through a relation-specific hyperplane mechanism, addressing the common issue of missing explicit type supervision in real-world KGs.

Beyond static attributes and descriptions, logical rules offer a powerful form of auxiliary information. \cite{guo2017} introduced RUGE, a novel paradigm that iteratively guides KGE learning with soft logical rules, demonstrating that even uncertain, automatically extracted rules can significantly enhance link prediction. To improve the scalability of rule integration, \cite{guo2020} proposed Soft Logical Rule Embedding (SLRE), which regularizes relation representations directly, making the complexity of rule learning independent of the entity set size. Further advancing rule-guided reasoning, \cite{tang2022} developed RulE, a framework that learns explicit rule embeddings and jointly represents entities, relations, and logical rules in a unified space, enabling soft rule inference and mutual regularization between KGE and rule-based components.

Other approaches have focused on leveraging deeper structural context or hyper-relational information. \cite{zhang2018} proposed a method to incorporate a three-layer Hierarchical Relation Structure (HRS) into existing KGE models, enriching embeddings by categorizing relations into clusters, individual relations, and fine-grained sub-relations. Moving beyond simple triplets, \cite{rosso2020} introduced HINGE, a hyper-relational KGE model that directly learns from facts augmented with key-value pairs, capturing both the primary triplet structure and the correlations with associated attributes, which was previously oversimplified.

The integration of multiple, heterogeneous data sources, often termed "multi-view" or "multi-modal" embeddings, has led to more sophisticated models. \cite{shen2022} presented LASS, a method that jointly embeds language semantics from pre-trained language models (LMs) and structural information from knowledge graphs by fine-tuning LMs with a probabilistic structured loss, demonstrating superior performance in low-resource settings. For entity alignment, \cite{zhang2019} developed MultiKE, a framework that unifies multiple entity views (name, relation, attribute) through view-specific embeddings and novel cross-KG inference, including a soft alignment mechanism for relations and attributes. In the biomedical domain, \cite{zhu2022} proposed a multimodal reasoning approach for disease-specific knowledge graphs, integrating structural, categorical, and descriptive embeddings via a reverse-hyperplane projection method to discover new knowledge. Similarly, \cite{yang2025} introduced SEConv for healthcare prediction, combining a resource-efficient self-attention mechanism with a multilayer convolutional neural network to learn deeper structural features from medical KGs.

Advanced neural architectures, particularly attention mechanisms and Transformers, have played a crucial role in effectively combining these diverse feature types. \cite{wang2020} proposed Graph Attenuated Attention networks (GAATs), which use an attenuated attention mechanism to assign different weights to relation paths and acquire information from neighboring nodes, leading to more robust entity and relation representations. For explainable recommendation, \cite{yang2023} developed CKGE, which uses a KG-based Transformer with relational attention and structural encoding to process motivation-aware meta-graphs, integrating contextualized neighbor semantics and high-order connections. \cite{shi2025} introduced TGformer, a graph transformer framework that constructs context-level subgraphs and employs a Knowledge Graph Transformer Network (KGTN) to explore multi-structural (triplet-level and graph-level) and contextual features. Furthermore, \cite{shang2024} presented MGTCA, which combines messages from mixed geometric spaces (hyperbolic, hypersphere, Euclidean) with a trainable convolutional attention network, allowing for adaptive switching between different Graph Neural Network (GNN) types to capture diverse local graph structures. In the realm of Knowledge Graph Question Answering (KGQA), \cite{zhou2023} developed a system for chemistry that operates on hybrid multi-embedding spaces, leveraging BERT-based entity linking and a joint numerical embedding model to handle complex queries.

In conclusion, the evolution of KGE has progressively moved towards comprehensive representations that integrate auxiliary information beyond basic triples. These context-augmented and multi-view embedding approaches, ranging from explicit semantic projections and rule-guided learning to sophisticated multi-modal fusion and attention-based neural architectures, have significantly mitigated data sparsity and improved performance in tasks like link prediction, entity alignment, and recommendation. Future research directions include developing more dynamic and adaptive integration strategies for evolving multi-modal data, enhancing interpretability in complex multi-view models, and exploring novel methods for automatically discovering and leveraging latent auxiliary information from diverse data sources.