\subsection*{Link Prediction and Knowledge Graph Completion}

Link prediction is a primary and fundamental application of knowledge graph embeddings (KGEs), aiming to infer missing facts and complete incomplete knowledge graphs by predicting the plausibility of unobserved triples $(h, r, t)$. KGE models achieve this by embedding entities and relations into a continuous vector space, where geometric or algebraic operations are used to score the likelihood of a triple being valid. This process is crucial for enriching and maintaining the integrity of large-scale knowledge bases.

Early advancements in KGE for link prediction focused on developing efficient models capable of capturing diverse relational patterns. Building upon the foundational TransE model, \textcite{wang2014} introduced \texttt{TransH}, which addressed TransE's limitations with complex relations (e.g., one-to-many, many-to-one) by projecting entities onto relation-specific hyperplanes. This allowed entities to have different "roles" depending on the relation, significantly improving accuracy while maintaining computational efficiency. Further refining this concept, \textcite{ji2015} proposed \texttt{TransD}, which considered the diversity of both entities and relations through dynamically constructed mapping matrices, leading to more fine-grained projections and enhanced scalability for large KGs. A significant leap in geometric models came with \textcite{sun2018}, who introduced \texttt{RotatE}. This model elegantly represented relations as element-wise rotations in a complex vector space, enabling it to simultaneously capture symmetry, antisymmetry, inversion, and composition patternsâ€”a capability that eluded prior models. \texttt{RotatE} also contributed a novel self-adversarial negative sampling technique, generating more informative training examples.

As KGE models grew in complexity and number, the need for rigorous evaluation and reproducibility became paramount. \textcite{rossi2020} provided a comprehensive survey and comparative analysis of KGE methods for link prediction, highlighting the impact of various design choices and proposing new, more informative evaluation practices beyond standard metrics like Hits@k and Mean Reciprocal Rank (MRR). Directly addressing a "reproducibility crisis," \textcite{ali2020} re-implemented 21 KGE models within a unified framework, demonstrating that training configurations (e.g., loss functions, optimizers) are as critical as model architecture for achieving state-of-the-art performance. This work underscored the importance of a fair and consistent framework for comparing diverse KGE models. Delving deeper into model behavior, \textcite{lloyd2022} quantified the dataset-specific importance of hyperparameters using Sobol sensitivity analysis and identified data leakage issues in existing benchmarks, further refining the understanding of robust KGE training. To tackle the pervasive data imbalance (long-tail distribution) in KGs, \textcite{zhang2023} introduced \texttt{WeightE}, a novel approach using bilevel optimization to adaptively assign higher weights to infrequent entities and relations during training, thereby improving the reliability of link predictions for sparse parts of the knowledge graph.

The increasing scale of knowledge graphs necessitated advancements in KGE scalability and efficiency. \textcite{kochsiek2021} empirically investigated and improved parallel training techniques for KGE models, demonstrating how to scale training without compromising embedding quality. \textcite{zheng2024} introduced \texttt{GE2}, a general and efficient system for KGE learning that optimizes multi-GPU training and negative sampling, crucial for handling massive KGs. For computationally intensive Graph Neural Network (GNN)-based KGEs, \textcite{modak2024} proposed \texttt{CPa-WAC}, a scalable framework featuring a topology-preserving partitioning algorithm and a novel global decoder to effectively combine embeddings from independently trained subgraphs, achieving significant speed-ups without accuracy loss. Furthermore, \textcite{di2023} advanced automated model design by introducing a search space for GNN message functions, allowing KGE models to adapt to diverse KG forms (e.g., n-ary, hyper-relational data) and thus broadening the applicability of link prediction to more complex relational structures.

Beyond core link prediction, KGEs have been extended to more nuanced forms of KG completion. \textcite{dasgupta2018} introduced \texttt{HyTE}, a hyperplane-based approach that explicitly incorporates temporal information into KGEs, enabling time-aware link prediction and the inference of temporal scopes for facts. This is vital for dynamic knowledge graphs where fact validity changes over time. The utility of robust KGEs extends to various downstream tasks, such as entity alignment \textcite{sun2018, zhang2019}, question answering \textcite{huang2019}, and recommendation systems \textcite{sun2018}. Recent trends, as highlighted by \textcite{ge2023}, emphasize the integration of KGEs with pre-trained language models (PLMs) to leverage rich textual context for enhanced link prediction. The development of personalized federated KGE \textcite{zhang2024} also addresses the challenge of distributed and privacy-preserving link prediction across heterogeneous client KGs.

In conclusion, the field of link prediction and knowledge graph completion has evolved significantly, moving from foundational geometric models to sophisticated, scalable, and robust architectures. The continuous drive for higher accuracy, efficiency, and the ability to handle diverse KG characteristics has led to innovations in model expressiveness, rigorous evaluation methodologies, and system-level optimizations. While substantial progress has been made, challenges remain in handling extremely sparse KGs, performing complex multi-hop reasoning for link prediction, and effectively integrating multi-modal information to infer new facts with even greater precision and context.