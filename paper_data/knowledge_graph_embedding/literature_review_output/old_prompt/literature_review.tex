\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 377 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:1._introduction}

\section*{1. Introduction}

Knowledge Graphs (KGs) have emerged as a cornerstone of modern Artificial Intelligence, providing a structured and interconnected representation of real-world entities and their relationships. They are instrumental in organizing vast, heterogeneous information, powering diverse applications from semantic search and question answering to recommendation systems and scientific discovery \cite{wang2017zm5, dai2020, choudhary2021, yan2022, ge2023}. However, the inherent symbolic nature of KGs, while offering interpretability, presents significant challenges for large-scale computational tasks, scalability, and seamless integration with contemporary machine learning paradigms. This foundational limitation has spurred the development of Knowledge Graph Embedding (KGE) techniques, which transform symbolic entities and relations into low-dimensional, continuous vector spaces. KGEs address the critical need for efficient computation, improved scalability, and enhanced compatibility with data-driven AI models, thereby unlocking the full potential of KGs in complex analytical and predictive tasks \cite{cao2022, chen2023}. This literature review provides a comprehensive overview of the KGE landscape, tracing its evolution from initial translational and semantic matching models to advanced neural architectures, temporal embeddings, and robust learning strategies. It aims to synthesize the core principles, critically analyze methodological advancements, identify persistent challenges, and highlight future research trajectories in this rapidly evolving field.

\subsection*{1.1. Background: The Rise of Knowledge Graphs}
Knowledge Graphs (KGs) have become indispensable assets in the landscape of modern Artificial Intelligence, serving as powerful frameworks for organizing and representing complex, real-world information \cite{wang2017zm5, dai2020}. By structuring data as interconnected entities and their relationships (triples of the form (head, relation, tail)), KGs provide a rich, semantic layer that underpins a multitude of advanced AI applications. Their prevalence stems from their ability to integrate vast amounts of structured, semi-structured, and even unstructured data into a coherent, machine-readable format, enabling more intelligent systems \cite{choudhary2021, yan2022}. For instance, KGs are central to enhancing search engine capabilities by providing semantic context \cite{xiong2017zqu}, powering sophisticated recommendation systems by modeling intricate user-item interactions \cite{sun2018, liu2019e1u, sha2019plw, yang2023}, facilitating precise question answering \cite{huang2019, zhou2023}, and accelerating scientific discovery in domains such as drug repurposing and biomedical research \cite{mohamed2020, sosa2019ih0, islam2023, djeddi2023g71}. The sheer scale of contemporary KGs, often comprising millions of entities and billions of facts (e.g., Freebase, Wikidata), underscores their importance but also highlights the inherent challenges in their manipulation and reasoning. This structured yet expansive nature makes KGs a vital component for building explainable, robust, and context-aware AI systems, moving beyond purely statistical correlations to leverage explicit knowledge \cite{daruna2022dmk, yang2023}.

\subsection*{1.2. The Imperative for Knowledge Graph Embedding}
Despite their undeniable utility, the symbolic nature of Knowledge Graphs presents inherent limitations when confronted with the demands of modern AI systems, particularly concerning computational efficiency, scalability, and seamless integration with statistical machine learning models. Traditional symbolic reasoning, while offering high interpretability and logical precision, often suffers from combinatorial explosion, making inference over large KGs computationally prohibitive \cite{gutirrezbasulto2018oi0}. Moreover, the discrete, sparse representations of symbolic KGs are ill-suited for direct input into many neural network architectures, which thrive on dense, continuous vector inputs. This fundamental impedance mismatch necessitates a transformation of KGs into a more amenable format. Knowledge Graph Embedding (KGE) emerges as the critical solution, projecting entities and relations into low-dimensional, continuous vector spaces (embeddings) \cite{wang2017zm5, cao2022}.

The core motivation behind KGE is multifaceted: Firstly, \textbf{efficient computation}: Representing entities and relations as vectors allows for arithmetic operations (e.g., vector addition, dot products) to model relationships, drastically reducing the computational cost of tasks like link prediction compared to symbolic rule inference \cite{zhu2020, wang2021}. For instance, translational models like TransE interpret relations as translations between entity embeddings, enabling efficient similarity calculations \cite{jia2015}. Secondly, \textbf{scalability}: KGE models can handle vast KGs by learning compact representations, avoiding the sparsity issues and memory overhead associated with one-hot encodings or large adjacency matrices. Recent advancements even focus on parameter-efficient KGEs, where the model parameters do not scale linearly with the number of entities, making deployment on resource-constrained devices feasible \cite{chen2023}. Thirdly, \textbf{seamless integration with machine learning}: Dense vector embeddings serve as powerful features that can be directly fed into various downstream machine learning models, enhancing tasks such as recommendation \cite{sun2018, yang2023}, question answering \cite{huang2019}, and entity alignment \cite{sun2018, zhang2019}. This integration bridges the gap between structured knowledge and data-driven learning, allowing models to leverage both explicit facts and learned patterns. Furthermore, KGEs can implicitly capture complex relational patterns, including temporal dynamics \cite{dasgupta2018, xu2019} and multi-faceted entity semantics \cite{wu2021}, which are challenging for purely symbolic systems to model without extensive manual rule engineering. The ability to handle noisy or incomplete data, a common characteristic of real-world KGs, through robust embedding learning further solidifies the imperative for KGEs \cite{shan2018}.

\subsection*{1.3. Scope and Structure of the Review}
This comprehensive literature review aims to provide a structured and critical analysis of the field of Knowledge Graph Embedding (KGE), encompassing its foundational theories, methodological advancements, practical applications, and future challenges. The pedagogical progression of this review is designed to guide the reader from the fundamental principles of KGE to its most cutting-edge developments. We begin by categorizing KGE models based on their underlying representation spaces and scoring functions, examining seminal works that introduced translational models (e.g., TransE and its variants like TransA \cite{jia2015}), semantic matching models (e.g., DistMult, ComplEx), and their extensions.

A significant portion of this review will synthesize and critically analyze the evolution of KGE research along several key dimensions. We will explore how models have enhanced their \textbf{expressivity} to capture complex relational patterns, including symmetric, antisymmetric, and hierarchical relations, moving beyond the limitations of early models \cite{sun2018}. The persistent challenge of \textbf{scalability and efficiency} will be examined, discussing innovations ranging from optimized training strategies and negative sampling techniques \cite{shan2018, zhang2018} to parameter-efficient architectures \cite{chen2023} and federated learning approaches \cite{zhang2024}. The increasing importance of \textbf{temporal dynamics} in KGs will be a focal point, with a critical evaluation of models that integrate time into embeddings to capture evolving knowledge and predict future events \cite{dasgupta2018, xu2019}. We will also delve into methods that enhance \textbf{robustness} against noisy data and adversarial attacks, and those that improve \textbf{interpretability} and \textbf{explainability}, particularly in application-driven contexts like recommendation systems \cite{yang2023, daruna2022dmk}.

Furthermore, this review will critically compare different KGE paradigms, evaluating their methodological strengths and weaknesses, identifying underlying assumptions, and discussing trade-offs (e.g., expressivity vs. efficiency, accuracy vs. robustness). For instance, while some models achieve high accuracy on specific tasks, they might suffer from parameter explosion \cite{chen2023} or struggle with certain relation types \cite{wu2021}. We will highlight instances where experimental setups might affect generalizability \cite{lloyd2022} and discuss theoretical gaps that hinder further progress. By synthesizing disparate studies, we aim to identify evolutionary trends, potential contradictions in findings, and emerging research directions, such as the integration of KGEs with large language models \cite{liu2024q3q} and the exploration of novel geometric embedding spaces (e.g., hyperbolic, quaternion) \cite{liang2024, li2024}. Ultimately, this review seeks to provide a comprehensive, analytical foundation for researchers and practitioners navigating the complex and dynamic field of Knowledge Graph Embedding.

\label{sec:2._foundational_concepts_and_early_geometric_models}

\section*{2. Foundational Concepts and Early Geometric Models}

The advent of Knowledge Graph Embedding (KGE) marked a pivotal shift in how Knowledge Graphs (KGs) are processed and leveraged within Artificial Intelligence. This paradigm fundamentally transforms symbolic entities and relations into continuous, low-dimensional vector representations, often referred to as embeddings \cite{wang2017zm5, dai2020, cao2022, ge2023}. This transformation addresses critical challenges inherent in symbolic KGs, such as computational inefficiency, scalability limitations, and the difficulty of integrating discrete knowledge with statistical machine learning models. The core objective of KGE is to learn these embeddings such that the structural and semantic properties of the KG are preserved in the vector space, allowing for arithmetic operations to infer new facts or assess the plausibility of existing ones. This section lays the groundwork by elucidating the core principles of KGE, then delves into the pioneering 'translational distance models' that established the initial geometric paradigm, and finally explores their immediate extensions, highlighting the continuous drive for enhanced representational power in the field. These early models, while foundational, revealed inherent limitations that spurred subsequent research into more expressive and flexible embedding architectures, initiating what can be described as an "arms race" in KGE development.

\#\#\# 2.1. Core Principles of Knowledge Graph Embedding

At its heart, Knowledge Graph Embedding (KGE) is the process of mapping entities (nodes) and relations (edges) within a Knowledge Graph into a continuous vector space, typically $\mathbb{R}^d$, where $d$ is the embedding dimension \cite{wang2017zm5, cao2022}. Each entity $e$ is represented by a vector $\mathbf{e} \in \mathbb{R}^d$, and each relation $r$ by a vector $\mathbf{r} \in \mathbb{R}^d$ or a matrix $\mathbf{M}\_r \in \mathbb{R}^{d \times d}$. The fundamental goal is to capture the semantic and structural information of the KG such that plausible facts (triples $(h, r, t)$) are assigned high scores (or low "energy"), while implausible or false facts receive low scores (or high energy). This is achieved through a scoring function $f(h, r, t)$ that quantifies the plausibility of a triple based on the embeddings of its head entity $h$, relation $r$, and tail entity $t$. The design of this scoring function is central to distinguishing different KGE models, as it dictates how relational patterns are geometrically or semantically interpreted in the embedding space.

The training of KGE models typically involves minimizing a loss function that encourages positive triples to have better scores than negative (false) triples. A common approach is margin-based ranking loss, where the score of a positive triple $(h, r, t)$ is pushed below that of a corrupted negative triple $(h', r, t')$ or $(h, r, t'')$ by a certain margin $\gamma$. Negative sampling strategies, such as uniform negative sampling or more advanced techniques like $\epsilon$-truncated uniform negative sampling \cite{sun2018} and confidence-aware negative sampling \cite{shan2018}, are crucial for generating informative negative examples that challenge the model to learn finer distinctions. Without effective negative sampling, models risk collapsing all entities and relations into similar embeddings, failing to learn meaningful representations.

The benefits of this embedding approach are manifold. Firstly, it enables \textit{efficient computation} for tasks like link prediction and triple classification, as vector operations are significantly faster than symbolic reasoning \cite{zhu2020}. For instance, calculating the plausibility of a triple using vector distances or dot products is orders of magnitude faster than traversing complex symbolic rules. Secondly, KGEs offer enhanced \textit{scalability}, as the fixed dimensionality $d$ allows for compact representations of even massive KGs, mitigating the sparsity issues of one-hot encodings. This has led to research into parameter-efficient KGEs where the parameter count does not scale linearly with the number of entities \cite{chen2023}. Thirdly, these dense vector representations provide a \textit{seamless interface} for integrating structured knowledge with various deep learning architectures, powering applications in recommendation systems \cite{sun2018, yang2023}, question answering \cite{huang2019}, and entity alignment \cite{sun2018}.

However, the core principle of capturing relational patterns in a continuous space also introduces inherent assumptions and limitations. While KGEs excel at statistical inference and pattern recognition, they often struggle with the explicit, symbolic reasoning capabilities of traditional AI, such as multi-hop inference or logical deduction, which are not directly encoded in vector arithmetic. The choice of embedding dimension $d$ itself presents a trade-off: a smaller $d$ offers greater efficiency but risks losing fine-grained semantic distinctions, while a larger $d$ can capture more information but increases computational cost and memory footprint. This fundamental tension between representational capacity and computational efficiency laid the foundation for the "arms race" in developing increasingly expressive and sophisticated geometric and neural models.

\#\#\# 2.2. Translational Models: From TransE to TransH and TransR

The early landscape of KGE was largely defined by 'translational distance models,' which conceptualized relations as translation operations between entity embeddings in a low-dimensional vector space. These models were pioneering in their simplicity and efficiency, establishing a foundational paradigm for geometric KGE.

\textbf{TransE (Translating Embeddings)} is arguably the most influential early translational model, proposing that if a triple $(h, r, t)$ holds, then the embedding of the head entity $\mathbf{h}$ plus the embedding of the relation $\mathbf{r}$ should be approximately equal to the embedding of the tail entity $\mathbf{t}$, i.e., $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ \cite{bordes2013transe}. The plausibility of a triple is measured by a distance-based scoring function, typically the $L\_1$ or $L\_2$ norm, such as $f(h,r,t) = ||\mathbf{h} + \mathbf{r} - \mathbf{t}||$.
*   \textbf{Strengths:} TransE's simplicity makes it computationally efficient and easy to implement, performing remarkably well on 1-to-1 relations. Its low parameter count (only one vector per entity and relation) contributes to its efficiency.
*   \textbf{Weaknesses:} Its rigid geometric assumption struggles significantly with complex relation types, specifically 1-to-N, N-to-1, and N-to-N relations \cite{asmara2023}. For instance, if an entity is the head of multiple triples with the same relation but different tails (e.g., (Barack Obama, spouse, Michelle Obama) and (Barack Obama, child, Malia Obama)), TransE would assign the same relation vector $\mathbf{r}$ to both, forcing $\mathbf{h} + \mathbf{r} \approx \mathbf{t\_1}$ and $\mathbf{h} + \mathbf{r} \approx \mathbf{t\_2}$. This implies $\mathbf{t\_1} \approx \mathbf{t\_2}$, effectively collapsing distinct entities into similar representations, leading to poor performance. This limitation stems from using a single, fixed relation vector $\mathbf{r}$ for all entities, which cannot capture the diverse contexts in which a relation might manifest or the different facets of an entity. It also cannot effectively model symmetric relations (e.g., (sibling, sibling)) or antisymmetric relations, as $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ implies $\mathbf{t} - \mathbf{r} \approx \mathbf{h}$, which is problematic for relations where the inverse is not simply the negative of the relation vector.

To address TransE's shortcomings with complex relations, \textbf{TransH (Translating on Hyperplanes)} was introduced \cite{wang2014transh}. Unlike TransE, which operates in a single global space, TransH projects entities onto a relation-specific hyperplane before performing the translation. Each relation $r$ is associated with two vectors: a translation vector $\mathbf{r}$ and a normal vector $\mathbf{w}\_r$ defining its hyperplane. The projected entity embeddings are $\mathbf{h}\_{\perp} = \mathbf{h} - \mathbf{w}\_r^T \mathbf{h} \mathbf{w}\_r$ and $\mathbf{t}\_{\perp} = \mathbf{t} - \mathbf{w}\_r^T \mathbf{t} \mathbf{w}\_r$, and the scoring function becomes $f(h,r,t) = ||\mathbf{h}\_{\perp} + \mathbf{r} - \mathbf{t}\_{\perp}||$.
*   \textbf{Strengths:} By allowing entities to have different representations (projections) on different hyperplanes, TransH significantly improves the handling of 1-to-N and N-to-1 relations. A single entity can now have distinct contextualized embeddings for different relations, mitigating the entity collapse problem observed in TransE. For example, 'Barack Obama' can be projected differently for 'spouse' and 'child' relations.
\textit{   \textbf{Weaknesses:} While better, TransH still uses a single relation vector $\mathbf{r}$ for translation }on the hyperplane*, which might not be sufficient for highly complex N-to-N relations where the translation itself needs to be more nuanced. The introduction of relation-specific hyperplanes also increases the parameter count (two vectors per relation instead of one), making it more prone to overfitting, especially on sparse KGs where there isn't enough data to learn robust hyperplane normal vectors.

Building further on this idea, \textbf{TransR (Translating in Relation-Specific Spaces)} proposed projecting entities into relation-specific vector spaces rather than just hyperplanes \cite{lin2015transr}. In contrast to TransH's projection onto a hyperplane, TransR uses a full projection matrix. Each relation $r$ is associated with a translation vector $\mathbf{r}$ and a projection matrix $\mathbf{M}\_r$. Entities $\mathbf{h}$ and $\mathbf{t}$ are projected into the relation space as $\mathbf{h}\_r = \mathbf{h}\mathbf{M}\_r$ and $\mathbf{t}\_r = \mathbf{t}\mathbf{M}\_r$, and the scoring function is $f(h,r,t) = ||\mathbf{h}\_r + \mathbf{r} - \mathbf{t}\_r||$.
*   \textbf{Strengths:} TransR offers higher expressiveness by allowing entities to have entirely different representations in different relation spaces, making it more effective for N-to-N relations and capturing more nuanced relational semantics. This is a significant improvement over TransH, which only projects entities onto a hyperplane within the original embedding space, whereas TransR transforms them into an entirely new relation-specific space.
*   \textbf{Weaknesses:} The significant increase in parameters due to the projection matrices $\mathbf{M}\_r$ (one $d \times k$ matrix for each relation, where $k$ is the relation space dimension) leads to higher computational complexity and memory requirements. This can be particularly problematic for KGs with a large number of relations, potentially leading to overfitting and difficulties in training, especially given that many relations might be sparse. The experimental setups for these models often require careful hyperparameter tuning, which can be dataset-dependent \cite{lloyd2022}.

The evolution from TransE to TransH and TransR demonstrates a clear pattern: each subsequent model aimed to overcome the expressiveness limitations of its predecessor by introducing more complex geometric transformations. This came with an explicit trade-off: increased representational power was achieved at the cost of higher parameter counts and computational overhead. While TransE prioritized efficiency and simplicity, TransR prioritized expressiveness, with TransH offering a middle ground. This highlights a fundamental constraint in KGE: balancing the ability to model complex relational patterns with the practical demands of scalability and training stability.

\#\#\# 2.3. Early Extensions: Dynamic Mappings, Manifold Embeddings, and Beyond

The limitations of the initial translational models, particularly their rigid geometric assumptions and fixed relation representations, spurred immediate extensions that sought greater expressiveness and flexibility. These early extensions explored dynamic mappings, expanded entity representations beyond simple points, and introduced more sophisticated ways to handle complex relations.

\textbf{TransD (Translating with Dynamic Mapping Matrix)} \cite{ji2015transd} aimed to address the parameter explosion of TransR while retaining its expressiveness. Unlike TransR, which uses a full projection matrix $\mathbf{M}\_r$ for each relation, TransD proposes dynamic mapping matrices constructed from two vectors: a relation-specific projection vector $\mathbf{r}\_p$ and an entity-specific projection vector $\mathbf{e}\_p$. The projected entity embeddings are computed as $\mathbf{h}\_r = \mathbf{h} + \mathbf{r}\_p \mathbf{h}^T \mathbf{e}\_p$ and $\mathbf{t}\_r = \mathbf{t} + \mathbf{r}\_p \mathbf{t}^T \mathbf{e}\_p$. This allows each entity-relation pair to have a unique projection, making the mapping dynamic and context-dependent, without incurring the high parameter cost of full matrices.
*   \textbf{Strengths:} TransD significantly reduces the number of parameters compared to TransR (from $O(N\_r \cdot d^2)$ to $O(N\_r \cdot d + N\_e \cdot d)$), while maintaining or even improving expressiveness. It allows for more fine-grained, entity-specific projections within each relation space, making it more scalable than TransR for KGs with many relations.
*   \textbf{Weaknesses:} Despite its improvements, TransD still fundamentally adheres to the translational assumption, limiting its ability to capture highly complex or non-linear relational patterns that cannot be reduced to a translation in some projected space. The dynamic mapping, while efficient, might still struggle with extremely diverse entity types or highly polysemous relations where a simple vector-based projection might be insufficient.

Another notable extension, \textbf{TransA (An Adaptive Approach for Knowledge Graph Embedding)} \cite{xiao2015transa}, focused on improving the training objective rather than the geometric model itself. TransA introduced an adaptive margin for the loss function, moving beyond the fixed, global margin used in TransE and its variants. It calculates an optimal margin dynamically, based on entity-specific and relation-specific characteristics, reflecting the "locality" of different parts of the knowledge graph.
*   \textbf{Strengths:} By adaptively determining the margin, TransA can achieve better generalization and performance by tailoring the loss function to the specific structural properties of different KGs or their subgraphs. This addresses a critical methodological limitation of previous models that relied on arbitrary, experimentally determined global margins.
*   \textbf{Weaknesses:} While improving the training process and potentially leading to more robust embeddings, TransA does not fundamentally alter the underlying translational geometric model. Its expressiveness is still bound by the $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ paradigm, meaning it inherits some of the core limitations regarding complex relation types. The calculation of adaptive margins can also introduce its own computational overhead, although optimizations exist. In contrast to TransD which enhances the geometric model, TransA refines the learning process for existing geometric models.

Beyond point-based representations, \textbf{ManifoldE (A Manifold-based Approach for Knowledge Graph Embedding)} \cite{xiao2016manifold} represented a conceptual leap by expanding entity representations from single points to geometric manifolds, such as hyperplanes or spheres. In ManifoldE, entities are no longer strict points but rather regions or subspaces, and relations are modeled as transformations between these manifolds. For example, a relation might transform a head entity's manifold into a tail entity's manifold.
*   \textbf{Strengths:} Representing entities as manifolds offers significantly increased representational power and flexibility. It can naturally capture uncertainty, fuzziness, or multiple facets of an entity, addressing the rigidity of strict point-based models. This allows for more precise link prediction by considering the "region" an entity occupies rather than just a single point.
*   \textbf{Weaknesses:} The complexity of defining and manipulating manifolds and their transformations is substantially higher than point-based models. This leads to increased computational cost and parameterization challenges. Furthermore, the choice of manifold type (e.g., hyperplane, sphere) is a modeling assumption that might not universally fit all entities or relations, and determining the optimal manifold for different contexts remains a challenge. ManifoldE diverges from the strict translational paradigm by allowing more complex geometric transformations, but this comes at a significant increase in model complexity.

Another approach to enhance translational models for complex relations is \textbf{TransE-MTP (TransE with Multi-Translation Principles)}. This model, building directly on TransE, introduces multiple translation principles (MTPs) for different relation types (1-to-1, 1-to-N, N-to-1, N-to-N) \cite{additional\_paper\_1}. Instead of projecting entities or relations into new spaces, TransE-MTP adapts the translation rule itself based on the relation's cardinality. For example, a 1-to-N relation might use a different translation mechanism than a 1-to-1 relation.
*   \textbf{Strengths:} By defining specific translation rules for different relation types, TransE-MTP improves performance on complex relations compared to vanilla TransE and even TransH, as demonstrated on datasets like Freebase and Wordnet. It offers a more direct way to handle cardinality issues without the heavy parameter burden of TransR.
*   \textbf{Weaknesses:} This approach requires explicit knowledge or inference of relation types, which might not always be readily available or perfectly accurate in real-world KGs. It still fundamentally relies on the translation paradigm, potentially limiting its ability to capture highly abstract or non-linear relational patterns.

Finally, some extensions focused on integrating semantic constraints. \textbf{KRC (Knowledge Graph Embedding with Relational Constraints)} proposes a general framework for enhancing translation-based models by encoding regularities between a relation and its arguments into the embedding space \cite{additional\_paper\_3}. This moves beyond purely geometric considerations by incorporating semantic rules.
*   \textbf{Strengths:} KRC improves knowledge graph completion by leveraging semantic constraints, which can guide the learning process and lead to more semantically coherent embeddings. It also proposes a soft margin-based ranking loss, further refining the training objective.
*   \textbf{Weaknesses:} The effectiveness of KRC depends on the quality and availability of these relational constraints. Defining and integrating such constraints can be a non-trivial task, and the model's performance is still bound by the underlying translation-based architecture.

These early extensions collectively underscore the continuous drive for enhanced representational power. They demonstrate a progression from fixed, global parameters to dynamic, context-aware mappings (TransD), from rigid point representations to more flexible geometric objects (ManifoldE), and from generic translation rules to type-specific or semantically constrained translations (TransE-MTP, KRC). This evolution highlighted the need for models that could capture the inherent heterogeneity and complexity of real-world knowledge, moving beyond the simplistic assumptions of the earliest models.

\#\#\# 2.4. Limitations of Early Geometric Models and the Drive for Enhanced Expressiveness

While the pioneering translational models and their immediate extensions laid a crucial foundation for Knowledge Graph Embedding, they also exposed significant limitations that became catalysts for subsequent research. The core geometric assumptions of these early models, primarily relying on vector addition for relations, proved to be overly simplistic for the rich and diverse semantics found in real-world Knowledge Graphs.

A primary limitation was their inherent difficulty in capturing \textit{complex relation types} \cite{wu2021}. TransE, with its fixed relation vector, notoriously struggled with 1-to-N, N-to-1, and N-to-N relations, effectively collapsing distinct entities into similar representations. This limitation exists because the model's mechanism of $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ implies that for a given head $\mathbf{h}$ and relation $\mathbf{r}$, there can only be one unique tail $\mathbf{t}$. When multiple tails exist, the model is forced to learn an $\mathbf{r}$ that averages across them, leading to a loss of specificity. While TransH and TransR introduced relation-specific hyperplanes and spaces to mitigate this, they still faced challenges with highly polysemous relations or those exhibiting intricate logical properties like symmetry, antisymmetry, or transitivity \cite{asmara2023}. For instance, a symmetric relation like "sibling" would ideally imply $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ and $\mathbf{t} + \mathbf{r} \approx \mathbf{h}$, which is difficult to enforce with a single translation vector without forcing $\mathbf{r}$ to be approximately zero. The rigid geometric interpretation of relations as simple translations or projections often failed to capture the nuanced, multi-faceted nature of how entities interact.

Furthermore, these models often lacked the capacity to represent \textit{multi-faceted entity semantics} or \textit{context-dependent relations} \cite{wu2021}. An entity might have different meanings or roles depending on the relation it participates in, a subtlety that fixed point embeddings (even with projections like in TransH/R) struggle to encode fully. For example, "apple" as a fruit and "Apple" as a company require distinct semantic representations that simple translational models find hard to disentangle. The early models also largely ignored the \textit{temporal dimension} of knowledge, treating all facts as static truths, which is a significant oversight for dynamic KGs where facts have limited validity periods \cite{dasgupta2018, xu2019}. This led to inaccurate reasoning when temporal information was critical, as a fact true at one time might be false at another, a concept not captured by static vector representations.

The pursuit of greater expressiveness in models like TransR and ManifoldE, while beneficial, often led to a \textit{parameter explosion} and increased computational complexity \cite{chen2023}. This trade-off between expressiveness and efficiency became a central challenge, particularly for large-scale KGs. For example, TransR's use of full projection matrices for each relation can lead to millions of parameters for KGs with thousands of relations, making training slow and memory-intensive, and increasing the risk of overfitting on sparse data. This is a direct consequence of attempting to model complex transformations with high-dimensional matrices. The reliance on distance-based scoring functions also imposed implicit constraints on the types of relational patterns that could be learned, favoring simple geometric relationships over more abstract semantic matches. Moreover, the sensitivity of these models to hyperparameters and negative sampling strategies highlighted their fragility, with optimal configurations often being highly dataset-dependent \cite{lloyd2022, shan2018}. A methodological critique is that many early evaluations often used uniform negative sampling, which might overestimate performance by providing "easy" negative examples, rather than challenging the model with more plausible but false triples.

These limitations underscored a continuous and urgent drive for enhanced representational power in KGE. The field recognized the need to move beyond strict geometric forms and simple translational assumptions towards models that could: (1) capture more intricate relational patterns, (2) handle context-dependent entity and relation semantics, (3) integrate auxiliary information (like time or text), and (4) achieve this expressiveness without sacrificing scalability or robustness. This critical analysis of early geometric models thus serves as a foundational understanding, setting the stage for the development of more sophisticated semantic matching models, neural architectures, and dynamic embedding approaches discussed in subsequent sections.

\label{sec:3._advancing_expressiveness:_rotational,_compound,_and_higher-dimensional_geometries}

\section*{3. Advancing Expressiveness: Rotational, Compound, and Higher-Dimensional Geometries}

Building upon the foundational translational models, the field of Knowledge Graph Embedding (KGE) rapidly progressed to explore more sophisticated geometric and algebraic approaches, driven by the inherent limitations of simple vector addition and projection. The "arms race" for enhanced expressiveness necessitated models capable of capturing intricate relational patterns, such as symmetry, antisymmetry, and compositionality, which proved challenging for TransE-like models \cite{asmara2023}. This section delves into these advanced paradigms, beginning with rotational models that redefine relations as rotations in complex or quaternion spaces. It then extends to compound operations, which synergistically combine multiple geometric transformations, and culminates in a significant shift towards non-Euclidean geometries, including hyperbolic and hyperspherical spaces, and the emerging concept of multi-curvature embeddings. These innovations collectively aim to provide a more faithful and nuanced representation of the diverse and often complex topological characteristics found in real-world knowledge graphs, moving beyond the Euclidean straight-line assumptions to model hierarchical structures and cyclic patterns more effectively.

\#\#\# 3.1. Relational Rotations in Complex and Quaternion Spaces

The limitations of translational models in capturing complex relational patterns, particularly symmetry, antisymmetry, and composition, became a significant bottleneck. For instance, a symmetric relation $(A, \text{sibling}, B)$ implies $(B, \text{sibling}, A)$, which is difficult to model with a single translation vector $\mathbf{r}$ (i.e., $\mathbf{a} + \mathbf{r} \approx \mathbf{b}$ and $\mathbf{b} + \mathbf{r} \approx \mathbf{a}$ implies $\mathbf{r} \approx -\mathbf{r}$, forcing $\mathbf{r} \approx \mathbf{0}$). This theoretical gap spurred the development of rotational models, which leverage the properties of complex numbers or quaternions to represent relations as rotations.

\textbf{RotatE (Knowledge Graph Embedding by Relational Rotation in Complex Space)} \cite{sun2018} emerged as a seminal work in this direction. It models entities as vectors in a complex vector space $\mathbb{C}^d$ and relations as element-wise rotations. For a triple $(h, r, t)$, RotatE proposes that $\mathbf{h} \circ \mathbf{r} \approx \mathbf{t}$, where $\circ$ denotes the Hadamard (element-wise) product, and each component of the relation vector $\mathbf{r}$ is a complex number with a modulus of 1 (i.e., $r\_i = e^{i\theta\_i}$). The scoring function is typically $f(h,r,t) = ||\mathbf{h} \circ \mathbf{r} - \mathbf{t}||$.
*   \textbf{Strengths:} RotatE elegantly addresses symmetry, antisymmetry, and compositionality. A symmetric relation can be modeled by a rotation of $\pi$ (i.e., $e^{i\pi} = -1$), such that $\mathbf{h} \circ (-1) \approx \mathbf{t}$ implies $\mathbf{t} \circ (-1) \approx \mathbf{h}$. Antisymmetry is naturally captured by distinct rotation angles. Crucially, compositionality (if $(A, r\_1, B)$ and $(B, r\_2, C)$ implies $(A, r\_3, C)$) is modeled by $\mathbf{r}\_3 \approx \mathbf{r}\_1 \circ \mathbf{r}\_2$, as rotations are compositional. This provides a more theoretically grounded and expressive framework for these properties than translational models.
*   \textbf{Weaknesses:} While powerful, RotatE operates in complex Euclidean space, which might still struggle with highly hierarchical or non-Euclidean structural patterns. The assumption of element-wise rotation might be too restrictive for some complex interactions. The model's performance can also be sensitive to the initialization of complex embeddings and the choice of negative sampling strategies \cite{lloyd2022, shan2018}.

Following RotatE, several extensions explored variations of rotational transformations. \textbf{Rotate3D} \cite{gao2020} extended the concept to 3D Euclidean space, representing relations as rotations around axes, while \textbf{TeRo (Time-aware Knowledge Graph Embedding via Temporal Rotation)} \cite{xu2020} and \textbf{ChronoR (Rotation Based Temporal Knowledge Graph Embedding)} \cite{sadeghian2021} incorporated time into the rotational framework, allowing relation embeddings to evolve as temporal rotations. Other works like \cite{huang2021u42, wang20213kg, le2022ji8, wei20215a7, le2022ybl} further explored relational and entity rotations, sometimes on hyperplanes, to enhance expressiveness. \textbf{Path-RotatE} \cite{zhou20216m0} extended rotational operations to paths in complex space, allowing for more complex multi-hop reasoning. \textbf{RotateCT (Rotation and Coordinate Transformation in Complex Space)} \cite{dong2022taz} combined rotation with coordinate transformations for greater flexibility. These models collectively demonstrate the versatility of rotation as a fundamental geometric operation for KGE.

A natural progression from complex numbers is to \textbf{quaternions}, which offer a higher-dimensional algebraic structure (hypercomplex numbers) that can represent 3D rotations more naturally and without gimbal lock issues. \textbf{Quaternion Knowledge Graph Embedding} \cite{zhang2019rlm} proposed representing entities and relations as quaternions, where relations act as quaternion multiplications. This allows for more expressive rotations and richer interactions than complex numbers.
*   \textbf{Strengths:} Quaternion embeddings can capture more intricate geometric transformations and potentially model higher-order interactions due to their non-commutative multiplication. They offer a compact representation for 3D rotations, which can be beneficial for certain types of relational semantics. Recent works like \textbf{Contextualized Quaternion Embedding} \cite{chen2025}, \textbf{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding} \cite{ji2024}, and models leveraging the \textbf{special orthogonal group in quaternion space} \cite{le2023hjy, liang2024z0q} further underscore their potential for complex and temporal KGE.
\textit{   \textbf{Weaknesses:} Quaternions introduce increased computational complexity and parameter count compared to complex numbers. Their non-commutative nature can make interpretation more challenging. The theoretical justification for }why* quaternion algebra is universally optimal for all types of KG relations is still an active area of research, and their benefits might be more pronounced for specific types of relational structures.

The shift to rotational models, particularly RotatE, marked a significant advancement by providing a principled geometric interpretation for fundamental relational properties. However, these models still largely operate within Euclidean or complex Euclidean spaces. While quaternions offer a richer algebraic structure, the core challenge remains: how to effectively model relations that are not purely rotational, or structures that do not conform to Euclidean geometry.

\#\#\# 3.2. Compound Geometric Transformations

While rotational models significantly enhanced expressiveness, the inherent complexity of real-world knowledge graphs often demands a combination of geometric operations. This led to the development of models that employ \textbf{compound geometric transformations}, synergistically combining translation, rotation, and scaling to achieve a more nuanced and flexible representation of relational patterns. This approach implicitly acknowledges that no single geometric operation is sufficient to capture the full spectrum of relational semantics.

\textbf{CompoundE (Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations)} \cite{ge2022} is a prime example of this paradigm. It proposes a scoring function that integrates translation, rotation, and scaling operations. For a triple $(h, r, t)$, CompoundE models the relation $r$ as a sequence of these transformations applied to the head entity $h$ to approximate the tail entity $t$. Specifically, it might involve scaling the head entity embedding, then rotating it, and finally translating it.
*   \textbf{Strengths:} By combining these fundamental operations, CompoundE offers significantly enhanced expressiveness compared to models relying on a single operation. Translation captures the "difference" aspect of relations (like TransE), rotation handles symmetry and compositionality (like RotatE), and scaling allows for modeling hierarchical or magnitude-based relationships (e.g., "is\_larger\_than"). This multi-faceted approach provides greater flexibility in fitting diverse relational patterns.
*   \textbf{Weaknesses:} The increased complexity of compound operations comes with several trade-offs. The model has a higher parameter count, leading to increased computational cost during training and inference. More critically, the interpretability of the learned embeddings can diminish, as disentangling the individual contributions of translation, rotation, and scaling for a given relation becomes challenging. Hyperparameter tuning for such models is also more intricate, as the interplay between different transformation components needs careful optimization \cite{lloyd2022}.

Further extending this concept, \textbf{CompoundE3D (Knowledge Graph Embedding with 3D Compound Geometric Transformations)} \cite{ge2023} explores these compound operations in a 3D embedding space, potentially offering richer spatial transformations. Similarly, \textbf{STaR (Knowledge Graph Embedding by Scaling, Translation and Rotation)} \cite{li2022du0} explicitly combines these three operations, demonstrating their collective power. These models highlight a key evolutionary trend: as KGE models strive for greater accuracy, they often move towards more complex, composite operations, reflecting the "arms race" dynamic where each new model attempts to capture a broader range of relational nuances.

The methodological limitation of these compound models often lies in their increased parameterization and potential for overfitting, especially on sparse knowledge graphs. While they theoretically offer greater flexibility, effectively learning and disentangling these multiple transformations from limited observed triples remains a challenge. The experimental setups for such models typically involve extensive hyperparameter searches, and their generalizability to vastly different KG structures without re-tuning is an open question. The core assumption is that relations can be adequately represented as a combination of these basic transformations, which may not hold for all abstract or highly contextual relations. Despite these challenges, compound geometric transformations represent a powerful step towards building KGE models that can adapt to the multifaceted nature of knowledge.

\#\#\# 3.3. Exploring Non-Euclidean and Multi-Curvature Embedding Spaces

A significant paradigm shift in KGE has been the exploration of \textbf{non-Euclidean geometries}, moving beyond the flat, zero-curvature assumption of traditional vector spaces. This shift is motivated by the observation that many real-world knowledge graphs exhibit inherent hierarchical or tree-like structures that are poorly represented in Euclidean space. In Euclidean space, distances grow linearly, making it inefficient to embed hierarchical data where exponentially more space is needed at lower levels of the hierarchy.

\textbf{Hyperbolic spaces} have emerged as particularly adept at representing hierarchical structures. In hyperbolic geometry, space expands exponentially, allowing for a more faithful embedding of tree-like graphs where nodes at lower levels of the hierarchy can be placed far apart while maintaining short path distances to their ancestors. \textbf{Hyperbolic Hierarchy-Aware Knowledge Graph Embedding} \cite{pan2021} and \textbf{Hierarchical-aware relation rotational knowledge graph embedding} \cite{wang20213kg} were among the early works to leverage hyperbolic embeddings for KGE, demonstrating superior performance in capturing hierarchical relations. Subsequent research, such as \textbf{Hierarchical Hyperbolic Neural Graph Embedding} \cite{wang2021dgy}, \textbf{Deep hyperbolic convolutional model} \cite{lu2024fsd}, \textbf{HyperCL (A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology)} \cite{lu202436n}, \textbf{Hierarchical hyperbolic embedding} \cite{fang20243a4}, and \textbf{HGCGE (hyperbolic graph convolutional networks-based knowledge graph embedding)} \cite{bao20249xp}, has further solidified the utility of hyperbolic spaces for hierarchical KGs. These models often integrate hyperbolic geometry into existing KGE frameworks or Graph Neural Networks (GNNs) to enhance their ability to model complex graph structures, including for temporal KGs \cite{jia2023krv}. \textbf{Fully Hyperbolic Rotation for Knowledge Graph Embedding} \cite{liang2024} even combines rotational operations within hyperbolic space, aiming to capture both hierarchical and compositional properties.
*   \textbf{Strengths:} Hyperbolic embeddings naturally capture hierarchical relationships with significantly fewer dimensions than their Euclidean counterparts, leading to more compact and accurate representations. They excel at modeling power-law distributions often observed in real-world graphs.
*   \textbf{Weaknesses:} Hyperbolic geometry introduces computational challenges, as operations like distance calculation are more complex than in Euclidean space. The choice of curvature is a critical hyperparameter, and a single global curvature might not be optimal for all parts of a heterogeneous KG. The intuition behind hyperbolic embeddings is less straightforward than Euclidean spaces, potentially hindering model interpretability.

Beyond hyperbolic spaces, other non-Euclidean geometries have been explored. \textbf{Hyperspherical spaces}, which have positive curvature, are suitable for modeling cyclic or periodic patterns, or for embedding data where angular relationships are more important than linear distances. \textbf{SpherE (Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval)} \cite{li2024} is an example of a hyperspherical embedding model. \textbf{MbiusE (Knowledge Graph Embedding on Mbius Ring)} \cite{chen20210ah} and approaches leveraging \textbf{Mbius Group Transformations for Temporal Knowledge Graph Embedding on the Riemann Sphere} \cite{zhang2025ebv} explore geometries that can capture complex topological patterns, including cyclic and periodic relationships, which are challenging for Euclidean models. These spaces offer unique advantages for specific data characteristics.

A cutting-edge development is \textbf{multi-curvature embedding}, which addresses the limitation of assuming a single, global curvature for an entire knowledge graph. Real-world KGs are often heterogeneous, containing both hierarchical (hyperbolic) and cyclic/Euclidean substructures. Models like \textbf{MADE (Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion)} \cite{wang2024} and \textbf{IME (Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion)} \cite{wang2024} propose to adaptively learn local curvatures for different parts of the graph or even for different entities and relations.
*   \textbf{Strengths:} Multi-curvature embeddings offer unparalleled flexibility by allowing the embedding space's geometry to adapt to the local structure of the knowledge graph. This provides a more faithful representation of diverse topological patterns, potentially overcoming the "one-size-fits-all" limitation of single-geometry embeddings. They are particularly promising for complex, real-world KGs that combine various structural motifs.
*   \textbf{Weaknesses:} The primary challenge with multi-curvature models is their increased complexity in terms of model design, optimization, and parameterization. Learning optimal local curvatures is a non-trivial task, often requiring sophisticated optimization techniques. The computational overhead can be substantial, and the theoretical guarantees for such adaptive geometries are still under active investigation. Furthermore, the generalizability of learned curvatures across different datasets or even within different domains of a single large KG remains an open question.

The exploration of non-Euclidean and multi-curvature embedding spaces represents a profound evolution in KGE, moving beyond the simplistic assumptions of Euclidean geometry to embrace the intrinsic topological complexity of knowledge graphs. This direction promises more accurate, compact, and semantically rich embeddings, particularly for tasks involving hierarchical reasoning or complex relational patterns. However, it also introduces significant methodological hurdles related to computational efficiency, interpretability, and robust optimization, which are critical areas for future research.

\label{sec:4._deep_learning_architectures_and_automated_design_for_kge}

\section*{4. Deep Learning Architectures and Automated Design for KGE}

The evolution of Knowledge Graph Embedding (KGE) has witnessed a profound paradigm shift with the integration of advanced deep learning architectures, moving beyond purely geometric or algebraic models to leverage the expressive power of neural networks. This transition marks a critical phase in the "arms race" for more accurate, robust, and adaptable KGE models, enabling them to capture increasingly complex and nuanced patterns within knowledge graphs. Deep learning methods, particularly Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), offer powerful mechanisms for local feature extraction, interaction modeling, and structural pattern recognition that were previously unattainable. Furthermore, the advent of Transformer-based models has introduced capabilities for modeling long-range dependencies and contextualized features, pushing the boundaries of KGE expressiveness. Recognizing the growing complexity of designing optimal deep learning architectures for diverse knowledge graphs, the field has also seen a significant push towards automated design and meta-learning approaches. These methods aim to reduce manual effort, enhance adaptability, and discover novel scoring functions or architectural components, collectively driving KGE research towards greater sophistication and practical applicability \cite{cao2022, ge2023, dai2020}.

\subsection*{4.1. Convolutional and Graph Neural Networks for KGE}

The application of deep learning to KGE initially saw the rise of Convolutional Neural Networks (CNNs) and subsequently, Graph Neural Networks (GNNs), each offering distinct advantages in capturing structural information. CNNs, traditionally successful in image processing, were adapted to KGE by treating entity-relation pairs as structured inputs amenable to convolutional filters. Models like ConvE \cite{dettmers2018conve} and ConvKB \cite{nguyen2018conve} represent triples as 2D matrices or tensors, applying convolutional filters to extract local features and interactions between entities and relations. This approach allows for the discovery of intricate patterns that might be missed by simpler translational or rotational models. For instance, \cite{hu2024} proposed a CNN-based entity-specific common feature aggregation, demonstrating its utility in learning KGEs. Similarly, \cite{ren2020} leveraged atrous convolution and residual learning to enhance feature extraction, while \cite{li2021ro5} introduced recalibration convolutional networks to improve interaction modeling. The strength of CNNs lies in their ability to learn rich, local interaction features, which can be particularly effective for capturing complex relation types. However, a methodological limitation of CNNs in KGE is their inherent assumption of grid-like input structures, which often requires reshaping graph data into fixed-size tensors, potentially losing some of the graph's irregular topology. This can limit their generalizability and expressiveness for highly diverse graph structures.

GNNs, on the other hand, are intrinsically designed to operate on graph-structured data, making them a more natural fit for KGE. They leverage message passing mechanisms to aggregate information from an entity's local neighborhood, thereby capturing complex structural patterns and multi-hop relationships. Early GNN-based KGE models, such as R-GCN \cite{schlichtkrull2018modeling}, adapted graph convolutional networks to handle the multi-relational nature of KGs by using relation-specific weight matrices. The evolution of GNNs in KGE has seen the incorporation of attention mechanisms to dynamically weigh the importance of different neighbors and relations. For example, \cite{wang2020} introduced Graph Attenuated Attention Networks, while \cite{wu2021} proposed DisenKGAT, a disentangled graph attention network that learns independent components for entities, allowing for adaptive and robust representations. DisenKGAT specifically addresses the entanglement of latent factors in entity representations by employing relation-aware aggregation for micro-disentanglement and mutual information regularization for macro-disentanglement, leading to enhanced interpretability and robustness \cite{wu2021}. This directly addresses a limitation of earlier GNNs that might learn static or entangled representations. Other works, such as \cite{sheikh20213qq}, also emphasize relation-aware attention in GCNs. Recent advancements include multi-view feature augmented neural networks \cite{jiang202235y}, multi-hierarchical aggregation GCNs \cite{liu2024tc2}, and decoupled semantic GNNs \cite{li2024bl5}, all aiming to capture richer structural and semantic contexts. While GNNs offer superior capabilities in modeling graph topology, they face challenges such as over-smoothing (where representations of distant nodes become indistinguishable after many layers) and scalability to very large KGs. The assumption that local neighborhood information is sufficient for all reasoning tasks might also be a limitation, especially for relations requiring global context.

\subsection*{4.2. Transformer-based KGE Models}

The success of Transformer architectures in natural language processing, particularly their ability to model long-range dependencies through self-attention mechanisms, has naturally extended their application to Knowledge Graph Embedding. Transformer-based KGE models represent a significant leap in capturing contextualized and multi-structural features, moving beyond the local aggregation focus of many CNN and GNN approaches. The core idea is to treat entities and relations, or paths within a knowledge graph, as sequences or sets, allowing the self-attention mechanism to weigh their interdependencies dynamically.

A notable development in this area is \textbf{TGformer (A Graph Transformer Framework for Knowledge Graph Embedding)} \cite{shi2025}, which explicitly adapts the Transformer architecture for KGE. TGformer aims to model the complex interactions between entities and relations by allowing each element to attend to all other relevant elements in its context, thereby capturing global dependencies that might be missed by local aggregation methods. This addresses a key theoretical gap in traditional GNNs, which often struggle with long-range dependencies and the potential for over-smoothing. Similarly, \cite{li2023} introduced a Position-Aware Relational Transformer, emphasizing the importance of positional information in relational sequences within KGs.

Another innovative application of Transformers is seen in \textbf{Contextualized Knowledge Graph Embedding (CKGE)} for explainable recommendations \cite{yang2023}. This work constructs specific meta-graphs for talent-course pairs, serializes entities and paths into a sequential input, and then processes this with a novel KG-based Transformer. This Transformer incorporates specialized relational attention and structural encoding to model global dependencies, and crucially, includes a local path mask prediction mechanism to reveal the saliency of meta-paths, thereby providing explainability. This highlights a critical strength of Transformers: their ability to integrate diverse contextual information and provide a degree of interpretability by quantifying attention weights. The experimental setup in \cite{yang2023} on real-world datasets demonstrated superior performance and interpretability, validating the approach.

Further advancements include \textbf{TracKGE (Transformer with Relation-pattern Adaptive Contrastive Learning for Knowledge Graph Embedding)} \cite{wang202490m}, which combines Transformer capabilities with contrastive learning to enhance embedding quality. The strength of Transformer-based models lies in their unparalleled ability to capture complex, non-local interactions and contextual dependencies, leading to highly expressive embeddings. They can dynamically adapt their focus to relevant parts of the graph, which is particularly beneficial for heterogeneous and sparse KGs where fixed neighborhood definitions might be suboptimal. However, these models come with significant trade-offs. Transformers are computationally intensive, requiring substantial memory and processing power, especially for very large knowledge graphs. Their parameter count can be high, posing challenges for deployment in resource-constrained environments, a problem that \cite{chen2023} attempts to address with entity-agnostic representation learning, though not specifically for Transformers. The assumption that graph structures can be effectively linearized or tokenized for sequential processing, while often effective, might not always perfectly align with the intrinsic non-sequential nature of graphs, potentially leading to information loss or increased complexity in input preparation.

\subsection*{4.3. Automated Search and Meta-Learning for KGE Architectures}

The proliferation of diverse deep learning architectures for KGE, each with its own scoring functions, aggregation mechanisms, and hyperparameters, has introduced a significant challenge: manually designing and tuning these models is a labor-intensive and often suboptimal process. This complexity has spurred the development of \textbf{automated search} and \textbf{meta-learning} approaches, aiming to discover optimal KGE architectures or components with reduced human intervention. This trend reflects a broader movement in machine learning towards automating model design, often referred to as Neural Architecture Search (NAS).

\textbf{Automated Search for Scoring Functions:} A pioneering work in this direction is \textbf{AutoSF (Searching Scoring Functions for Knowledge Graph Embedding)} \cite{zhang2019}. AutoSF proposes to automatically discover effective scoring functions for KGE by framing the search as a reinforcement learning problem. A controller (e.g., an RNN) generates candidate scoring functions, which are then evaluated on a KGE task (e.g., link prediction). The performance serves as a reward signal to update the controller, iteratively improving the search process. This approach directly addresses the limitation of manually selecting from a fixed set of scoring functions (e.g., TransE, DistMult, RotatE), which may not be optimal for a given KG. Building on this, \cite{di20210ib} presented an efficient relation-aware scoring function search, further refining the search space and efficiency. The strength of AutoSF-like methods is their ability to explore a vast space of potential scoring functions, potentially discovering novel and highly effective combinations tailored to specific KG characteristics. However, the search process itself can be computationally very expensive, requiring significant resources and time. The generalizability of a scoring function discovered on one KG to another remains an open question, often necessitating a new search.

\textbf{Automated Search for GNN Components:} Extending the automated design paradigm to GNNs, \textbf{Message Function Search for Knowledge Graph Embedding} \cite{di2023} focuses on automatically discovering optimal message functions within GNN architectures. In GNNs, the message function dictates how information is transformed and aggregated from neighbors. Manually designing these functions is challenging due to the intricate interplay of aggregation, transformation, and activation operations. By automating this search, \cite{di2023} aims to find message functions that are highly effective for KGE tasks, adapting to the specific structural patterns of different KGs. This directly tackles the methodological limitation of fixed GNN architectures, which may not be universally optimal. Other related works, such as \cite{zhang2020i7j}, explored searching for recurrent architectures for KGE, demonstrating the versatility of NAS in this domain.

\textbf{Meta-Learning for Adaptability:} Meta-learning, or "learning to learn," provides a framework for models to quickly adapt to new tasks or unseen entities/relations, which is crucial for inductive KGE and dynamic KGs. \textbf{Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding} \cite{chen2021} is a prime example, enabling KGE models to perform well on unseen entities by transferring knowledge learned from known entities. For dynamic KGs, \cite{sun2024} proposed learning dynamic KGE in evolving service ecosystems via meta-learning, allowing models to adapt to temporal changes without extensive retraining. Similarly, \cite{mao2024v2s} explored dynamic graph embedding via meta-learning. The core assumption here is that there are underlying meta-patterns or meta-parameters that can be learned across tasks, enabling rapid adaptation.
*   \textbf{Strengths:} Automated design and meta-learning significantly reduce manual effort in model design and hyperparameter tuning, leading to potentially superior and highly customized architectures. They enhance the adaptability of KGE models to diverse and evolving knowledge graphs, improving inductive capabilities and performance on cold-start scenarios.
\textit{   \textbf{Weaknesses:} The primary limitation is the substantial computational cost associated with the search process, which can be prohibitive for very large search spaces or KGs. The interpretability of automatically discovered architectures can also be low, making it difficult to understand }why* certain components or functions perform well. Furthermore, the effectiveness of meta-learning often depends on the similarity between the meta-training tasks and the target tasks; significant domain shifts can reduce its benefits. The theoretical guarantees for the optimality or convergence of these search processes are often limited, relying heavily on empirical validation. Despite these challenges, automated design and meta-learning represent a powerful frontier for pushing the boundaries of KGE expressiveness and adaptability, promising more intelligent and autonomous KGE systems in the future.

\label{sec:5._enriching_kge_with_semantic_context,_rules,_and_multi-modality}

\section*{5. Enriching KGE with Semantic Context, Rules, and Multi-modality}

Traditional Knowledge Graph Embedding (KGE) models primarily focus on learning representations from structural triples (head, relation, tail), treating entities and relations as atomic symbols. While effective for capturing direct relational patterns, this approach often falls short in capturing the nuanced semantics, intricate logical dependencies, and rich contextual information inherent in real-world knowledge graphs. The "arms race" in KGE research has thus shifted towards enriching these structural embeddings by incorporating auxiliary data, logical rules, and multi-modal information. This paradigm shift is driven by the need to overcome fundamental limitations such as data sparsity, ambiguity, and the inability to perform complex reasoning, leading to more discriminative, meaningful, and robust representations. By moving beyond mere structural patterns, these advanced KGE frameworks aim to inject deeper semantic understanding, enforce consistency, and leverage diverse information sources, thereby enhancing the expressiveness, interpretability, and applicability of knowledge graph embeddings across a wider range of downstream tasks \cite{cao2022, ge2023, dai2020}. This section explores three primary avenues for such enrichment: leveraging auxiliary semantic information like entity types and attributes, integrating logical rules and constraints, and fusing information from diverse modalities such as text and vision.

\subsection*{Incorporating Auxiliary Information and Entity Types}

The inherent heterogeneity and richness of knowledge graphs extend far beyond simple triples, encompassing various forms of auxiliary information such as entity types, attributes, and contextual descriptions. Incorporating this auxiliary data into KGE frameworks significantly enhances the semantic discriminability and meaningfulness of learned embeddings. Early approaches recognized that entities are not merely points in a vector space but belong to specific types or possess descriptive attributes, which can guide the embedding process. For instance, models like TransET \cite{wang2021} explicitly embed entity types alongside entities, allowing the model to distinguish between entities that might have similar structural connections but fundamentally different semantic roles. Similarly, \cite{he2023} proposed a type-augmented KGE framework for knowledge graph completion, demonstrating how type information can regularize embeddings and improve prediction accuracy. The benefit here is clear: types provide a coarse-grained semantic hierarchy, reducing ambiguity and guiding the placement of entities in the embedding space. \cite{lv2018} further refined this by differentiating concepts and instances, enriching the semantic context.

Beyond types, entity attributes (e.g., numerical values, textual descriptions) offer fine-grained semantic details. \cite{wu2018c4b} explored incorporating numeric attributes, while \cite{zhang2024} integrated entity attributes for error-aware KGE, showing how attribute consistency can help identify and correct errors. A more sophisticated form of auxiliary context is seen in \texttt{yang2023}'s Contextualized Knowledge Graph Embedding (CKGE), which integrates "motivation-aware information" by constructing meta-graphs that capture contextualized neighbor semantics and high-order connections. This approach moves beyond simple type/attribute embeddings to model complex, dynamic contexts, significantly improving explainable recommendations. Similarly, \texttt{chen2023}'s Entity-Agnostic Representation Learning (EARL) leverages "distinguishable information"including connected relations, k-nearest reserved entities, and multi-hop neighborsto compose entity embeddings, rather than relying on direct lookups. This compositional approach, while primarily aimed at parameter efficiency, inherently uses auxiliary structural context to build rich representations. Temporal information, as explored by \texttt{dasgupta2018} (HyTE) and \texttt{xu2019} (ATiSE), is another crucial form of auxiliary context, allowing KGE models to capture the dynamic validity of facts.

The strength of incorporating auxiliary information lies in its ability to alleviate data sparsity by providing alternative semantic signals and enhancing the discriminative power of embeddings. This is particularly valuable for long-tail entities with few structural connections. However, this approach is not without its limitations. A primary challenge is the availability and quality of auxiliary data, which can be inconsistent or incomplete across different KGs. The effective fusion of diverse auxiliary information (e.g., types, attributes, temporal stamps, contextual meta-graphs) without introducing noise or redundancy remains an active research area. Methodological limitations often arise from the assumption that all auxiliary features are equally relevant or can be simply concatenated, which may not hold in practice. Furthermore, the computational overhead of processing and integrating additional data, especially for large-scale KGs, can be substantial. The trade-off between richer semantics and increased model complexity, as well as the need for careful feature engineering, underscores the ongoing challenges in this domain.

\subsection*{Rule-based and Constraint-driven Embedding}

The integration of logical rules and constraints represents a crucial step towards infusing symbolic reasoning capabilities into the sub-symbolic world of knowledge graph embeddings. This approach is motivated by the fact that knowledge graphs often contain implicit knowledge, suffer from incompleteness, or harbor inconsistencies that standard KGE models, relying solely on observed triples, cannot address. By incorporating rules, KGE models can enforce consistency, guide the learning process, and inject valuable prior knowledge, thereby improving reasoning capabilities and embedding quality.

One prominent method involves integrating rules as \textbf{soft constraints} or regularization terms within the KGE loss function. This penalizes embedding configurations that violate known logical rules (e.g., transitivity, symmetry, inverse relations). For instance, \cite{guo2017} proposed an approach that iteratively guides KGE learning using soft rules, demonstrating improved link prediction performance. Similarly, \cite{ding2018} showed that even simple constraints, when appropriately integrated, can enhance KGE quality. \texttt{guo2020} further explored preserving soft logical regularity, highlighting the importance of balancing strict adherence to rules with the inherent noise and incompleteness of real-world KGs. This flexibility is crucial because hard constraints, while theoretically sound, can be too rigid for noisy data, potentially leading to overfitting or hindering the model's ability to learn from exceptions. \texttt{li2020ek4} also enhanced KGE with relational constraints, emphasizing their role in guiding embedding learning.

Another direction involves learning embeddings for the rules themselves, as seen in \texttt{tang2022}'s RulE, which embeds rules for knowledge graph reasoning. This allows for a more dynamic and potentially more expressive integration of logical knowledge. Rule-based methods can also be used for data augmentation, where new triples are generated based on existing facts and rules, effectively expanding the training data and addressing sparsity \cite{li2021tm6, zhao202095o}. The bootstrapping approach in \texttt{sun2018} (BootEA) for entity alignment, while not explicitly rule-based, employs a global optimal matching strategy and alignment editing to iteratively label new alignments under a one-to-one constraint. This can be viewed as a form of constraint-driven learning that mitigates error propagation, analogous to how rules enforce consistency. More recently, \texttt{zhang2024fy0} combined soft logical rules with contrastive learning, demonstrating a synergistic effect between explicit knowledge injection and modern representation learning techniques.

The primary strength of rule-based and constraint-driven embedding is its ability to inject prior knowledge and improve logical consistency, which is critical for robust reasoning and knowledge graph completion. This directly addresses the theoretical gap where purely statistical KGE models struggle with logical inferences. However, significant challenges persist. \textbf{Rule acquisition} is often a major bottleneck; manually defining comprehensive and accurate rule sets is labor-intensive and prone to errors. Automatic rule extraction, while promising, is still an active research area and can be computationally expensive. Furthermore, ensuring the \textbf{compatibility} between symbolic logical rules and continuous vector spaces is a non-trivial problem, as discussed by \texttt{gutirrezbasulto2018oi0}. The "why" behind these limitations often stems from the fundamental difference between discrete symbolic logic and continuous distributed representations. The computational complexity also increases with the number and complexity of rules, posing scalability issues for very large KGs. Despite these trade-offs, the trend towards neuro-symbolic AI suggests that the judicious integration of rules is essential for building more intelligent and reliable KGE systems.

\subsection*{Multi-modal KGE: Integrating Textual and Other Modalities}

The increasing availability of diverse data modalities associated with entities and relations in knowledge graphs has paved the way for multi-modal KGE, a powerful approach to overcome data sparsity and enhance semantic understanding. By fusing information from sources like textual descriptions, visual features, and even audio, multi-modal KGE aims to create richer, more comprehensive embeddings that capture a broader spectrum of knowledge.

Textual descriptions are perhaps the most common and impactful auxiliary modality. Entities and relations often have associated names, definitions, or descriptive texts that contain rich semantic information not explicitly captured in structural triples. Early works like \texttt{xiao2016}'s SSP (Semantic Space Projection) projected textual descriptions into the semantic embedding space. More recently, the advent of powerful pre-trained language models (PLMs) such as BERT and GPT has revolutionized this area. These models can generate highly contextualized embeddings for entity names and descriptions, capturing nuanced meanings and implicit relationships. \texttt{shen2022} proposed joint language semantic and structure embedding for knowledge graph completion, demonstrating how PLMs can provide a strong semantic signal. In specialized domains, such as chemistry, \texttt{zhou2023} utilized Marie and BERT for a knowledge graph embedding-based question answering system, highlighting the utility of textual context for domain-specific reasoning. The integration of KGE with Large Language Models (LLMs) is also emerging, as seen in \texttt{liu2024q3q} for fault diagnosis in aviation assembly, suggesting a future where KGE provides structured knowledge to ground LLM reasoning. This approach is particularly effective for long-tail entities or sparse KGs where structural information is limited, as textual descriptions can provide a wealth of external knowledge.

Beyond text, visual features offer another rich source of information for entities with associated images. \texttt{zhu2022o32} introduced DFMKE, a dual fusion multi-modal KGE framework for entity alignment, which effectively combines visual and structural cues. \texttt{zhang2023} further explored modality-aware negative sampling for multi-modal KGE, optimizing the learning process when diverse modalities are present. The fusion of modalities is crucial for specialized domains, such as healthcare, where \texttt{yang2025} proposed a semantic-enhanced KGE model with AIGC (AI-Generated Content) for healthcare prediction, leveraging multi-modal data for more accurate diagnoses and prognoses.

The primary strength of multi-modal KGE lies in its ability to significantly alleviate data sparsity by providing alternative, complementary semantic signals. It enriches semantic understanding, especially for abstract concepts or entities with limited structural connections, and enables cross-modal reasoning. The leveraging of pre-trained models, particularly PLMs, brings vast external knowledge, improving generalization capabilities. However, multi-modal KGE faces considerable challenges. \textbf{Data availability and alignment} are critical practical limitations; high-quality multimodal data that is correctly aligned with KG entities is often scarce. The \textbf{fusion strategy} is complex; simply concatenating embeddings from different modalities may not be optimal, and sophisticated attention or gating mechanisms are often required. The "why" behind this limitation is that different modalities capture different aspects of an entity, and their optimal combination is highly context-dependent. Furthermore, processing and embedding multimodal data, especially with large PLMs, introduces significant \textbf{computational cost} and memory requirements, posing a trade-off between richness and efficiency. The interpretability of fused multimodal embeddings can also be lower compared to purely structural ones. Despite these challenges, multi-modal KGE represents a vital frontier, bridging the gap between symbolic knowledge and the rich, diverse information found in the real world, promising more robust and semantically complete knowledge representations.

\label{sec:6._dynamic,_efficient,_and_robust_kge}

\section*{6. Dynamic, Efficient, and Robust KGE}

The evolution of Knowledge Graph Embedding (KGE) research has increasingly shifted from merely learning static representations to addressing the critical challenges posed by the dynamic nature, practical deployment, and reliability requirements of real-world knowledge graphs. This section delves into advanced KGE paradigms that enable models to adapt to evolving knowledge, operate efficiently on massive datasets, and maintain integrity in the face of noise and adversarial attacks. The core motivation for these advancements stems from the recognition that knowledge is not static but constantly changes, that computational resources are finite, and that real-world data is inherently imperfect. Consequently, the field has seen a proliferation of methods for Temporal Knowledge Graph Embedding (TKGE) to capture time-varying facts, inductive and continual learning to handle evolving KGs, and techniques for enhancing efficiency, compression, and robustness against data quality issues and malicious attacks. This collective effort aims to develop KGE models that are not only accurate but also agile, scalable, and trustworthy, pushing the boundaries towards truly intelligent and resilient knowledge systems.

\subsection*{Temporal, Spatiotemporal, and Fuzzy Knowledge Graph Embedding}

Traditional KGE models often treat facts as immutable, overlooking the crucial temporal dimension that governs their validity. This limitation severely restricts their applicability in dynamic environments where knowledge evolves over time. Temporal Knowledge Graph Embedding (TKGE) addresses this by explicitly modeling the time-varying nature of entities and relations. Early approaches to TKGE, such as HyTE \cite{dasgupta2018}, represent each timestamp as a hyperplane, allowing the model to capture the temporal validity of facts by projecting entity and relation embeddings onto these time-specific hyperplanes. While innovative for its geometric interpretation of time, HyTE's representation of time as discrete hyperplanes may struggle with continuous temporal dynamics or complex temporal patterns like periodicity.

More advanced TKGE models have moved towards more sophisticated representations of temporal evolution and uncertainty. ATiSE \cite{xu2019} models the evolution of entity and relation representations as multi-dimensional additive time series, decomposing them into trend, seasonal, and random components. Crucially, ATiSE represents entities and relations as multi-dimensional Gaussian distributions, explicitly capturing the \textit{temporal uncertainty} in their evolution. This probabilistic approach is a significant step beyond deterministic embeddings, acknowledging the inherent fuzziness of real-world knowledge. However, ATiSE's assumption of constant diagonal covariance matrices for computational efficiency might oversimplify the true complexity of temporal uncertainty. Other notable TKGE efforts include those leveraging tensor decomposition \cite{lin2020, he2022e37}, temporal rotations \cite{xu2020, sadeghian2021}, graph attention networks \cite{xie2023, hou20237gt}, and even hyperbolic embeddings for extrapolation \cite{jia2023krv, wang2024, wang2024}. These models, while diverse, share the common goal of integrating time as a first-class citizen, enabling tasks like temporal link prediction and future event forecasting \cite{li2023y5q}. A recent survey by \cite{zhang20243iw} provides a comprehensive overview of the models and applications in this rapidly expanding area.

Beyond pure temporal dynamics, the integration of spatial and fuzzy information further enhances the expressiveness of KGE. Spatiotemporal KGE models aim to capture facts that are valid within specific geographical regions and time periods. FSTRE \cite{ji2024} introduces a Fuzzy Spatiotemporal RDF Knowledge Graph Embedding model that uses uncertain dynamic vector projection and rotation, explicitly handling uncertainty with fuzzy logic. This model, along with its extension for multihop queries \cite{ji2024}, represents a significant advancement by addressing both spatial, temporal, and inherent data uncertainty simultaneously. The challenge here lies in the increased complexity of modeling and the intensive data requirements for accurate spatiotemporal annotations. The "why" behind these complex models is the need to mirror the granularity and inherent imprecision of real-world knowledge, where facts are rarely absolutely true or universally valid. However, the trade-off is often increased computational cost and the difficulty of acquiring sufficiently rich and clean spatiotemporal data, limiting their generalizability to domains with sparse annotations.

\subsection*{Inductive, Continual, and Federated Learning for KGE}

The static, transductive nature of many traditional KGE models poses significant limitations in dynamic, open-world scenarios where knowledge graphs are constantly evolving with new entities and relations, or where data is distributed and privacy-sensitive. This has spurred research into inductive, continual, and federated learning paradigms for KGE.

\textbf{Inductive KGE} focuses on generalizing to unseen entities or relations without retraining the entire model. This is crucial for handling the ever-expanding nature of real-world KGs. Approaches often leverage meta-learning or compositional representations. For instance, \cite{chen2021} proposed Meta-Knowledge Transfer for Inductive KGE, where a meta-learner acquires transferable knowledge from existing entities to embed new ones. This addresses the limitation of traditional models that require retraining for every new entity. Similarly, InGram \cite{lee202380l} learns inductive embeddings via relation graphs, allowing it to generalize to unseen entities by composing their representations from known relational patterns. Logic Attention Based Neighborhood Aggregation \cite{wang2018} also enables inductive capabilities by aggregating information from an entity's local neighborhood. The primary methodological limitation of inductive KGE often lies in the assumption that new entities' representations can be adequately composed from existing patterns, which may not hold for truly novel or out-of-distribution entities.

\textbf{Continual KGE} addresses the challenge of adapting KGE models to evolving knowledge without suffering from catastrophic forgetting of previously learned information. As KGs are continuously updated, models must incrementally learn new facts while retaining old ones. Recent works have explored parameter-efficient adaptation techniques, such as incremental LoRA \cite{liu2024}, which adapt pre-trained KGE models to new data with minimal parameter updates. Knowledge distillation is another promising avenue, where a "student" model learns from a "teacher" model trained on older data, combined with new data, to prevent forgetting \cite{liu2024to0}. Dynamic KGE via local embedding reconstructions \cite{krause2022th0} also offers a way to update embeddings incrementally. The "why" behind the difficulty of continual learning is the inherent plasticity-stability dilemma: models need to be flexible enough to learn new information (plasticity) but stable enough to retain old knowledge (stability).

\textbf{Federated KGE} tackles the problem of learning KGE models from distributed and private knowledge graphs without centralizing raw data, which is vital for privacy-sensitive domains. This emerging field leverages federated learning principles. \cite{chen20226e4} proposed federated knowledge graph completion via embedding-contrastive learning, allowing multiple clients to collaboratively train a KGE model. Communication efficiency is a major concern in federated settings, addressed by methods like entity-wise top-k sparsification \cite{zhang2024} and personalized federated KGE with client-wise relation graphs \cite{zhang2024}. However, federated KGE introduces new security and privacy challenges. For instance, \cite{zhou2024} demonstrated the vulnerability of federated KGE to poisoning attacks, while \cite{hu20230kr} quantified and defended against privacy threats. These studies highlight an "arms race" dynamic between privacy-preserving mechanisms and potential attacks. The trade-off between privacy, utility, and communication overhead remains a central challenge, often limited by theoretical gaps in robust aggregation and differential privacy guarantees for complex graph structures.

\subsection*{Efficiency, Compression, and Robustness in Training}

The sheer scale of modern knowledge graphs necessitates KGE models that are not only accurate but also efficient in terms of computation, memory, and storage, while simultaneously being robust to the inherent noise and imperfections of real-world data.

\textbf{Efficiency and Compression} are critical for practical deployment. Traditional KGE models often suffer from a parameter explosion problem, where the number of entity embeddings scales linearly with the number of entities. To counter this, \cite{chen2023} introduced Entity-Agnostic Representation Learning (EARL), which learns embeddings for a small set of "reserved entities" and uses universal encoders to compose representations for all other entities from their distinguishable information (e.g., connected relations, k-nearest reserved entities, multi-hop neighbors). This paradigm shift significantly reduces parameter storage costs, making KGE models deployable on resource-constrained devices. Other efficiency techniques include knowledge distillation, where a smaller, more efficient model learns from a larger, more complex one. DualDE \cite{zhu2020} dually distills knowledge graph embeddings for faster and cheaper reasoning. Compression techniques \cite{sachan2020} and lightweight frameworks for efficient inference and storage \cite{wang2021} are also crucial. For large-scale training, parallelization techniques \cite{kochsiek2021} and communication-efficient strategies like hotness-aware caching \cite{dong2022c6z} are essential to manage the computational burden. The methodological limitation of compression often lies in the trade-off between model size and expressiveness; overly compressed models may lose fine-grained semantic distinctions.

\textbf{Robustness} is paramount given the noisy and incomplete nature of real-world KGs. A key aspect of KGE training is negative sampling, which generates "false" triples to teach the model what is not true. The quality of negative samples significantly impacts model performance and robustness. Simple uniform random sampling often generates "easy" negatives, leading to suboptimal learning. More sophisticated strategies, such as confidence-aware negative sampling for noisy KGs \cite{shan2018}, aim to generate more informative, "harder" negatives. NSCaching \cite{zhang2018} and efficient non-sampling methods \cite{li2021} also contribute to more effective training. The choice of negative sampling strategy is critical, as highlighted by reviews \cite{qian2021, madushanka2024}.

Beyond negative sampling, models need to be inherently robust to noisy data and potential adversarial attacks. Data poisoning attacks, where malicious triples are injected into the training data, can significantly degrade KGE performance \cite{zhang20190zu, zhang20193g2}. Defenses include rule-enhanced error detection \cite{hong2020hyg} and multi-task reinforcement learning for robust KGE \cite{zhang2021}. The issue of bias in KGE models, whether from the data or the model itself, also affects robustness and fairness \cite{radstok2021yup, shomer2023imo}. Techniques like weighted KGE \cite{zhang2023} and confidence-aware embeddings \cite{huang20240su} attempt to mitigate the impact of varying data quality. Furthermore, the sensitivity of KGE quality to hyperparameters is substantial and varies across datasets \cite{lloyd2022}, implying that robust performance requires careful and often dataset-specific tuning. The "why" behind these robustness challenges often stems from the inherent difficulty of distinguishing genuine noise or adversarial perturbations from legitimate, albeit rare, patterns in high-dimensional embedding spaces. This leads to an ongoing need for robust optimization, better regularization, and more resilient model architectures.

\label{sec:7._applications_and_explainability_of_knowledge_graph_embedding}

\section*{7. Applications and Explainability of Knowledge Graph Embedding}

Knowledge Graph Embedding (KGE) has transcended its foundational role in merely representing entities and relations in continuous vector spaces to become a pivotal technology underpinning a wide array of advanced Artificial Intelligence tasks. While link prediction and knowledge graph completion remain the primary evaluation benchmarks for KGE models, their true utility and impact are realized in complex downstream applications where structured knowledge can significantly enhance AI system performance. This section delves into the practical utility of KGE, showcasing its effectiveness in bridging the gap between symbolic knowledge and neural computation across diverse domains. From enabling intelligent question answering and seamless knowledge integration through entity alignment, to powering sophisticated recommendation systems and driving critical domain-specific discoveries, KGE has proven to be a versatile and powerful tool. Furthermore, as AI systems become more pervasive, the demand for transparency and interpretability has grown, leading to an increasing emphasis on developing explainable KGE models that can provide actionable insights and foster user trust. This evolution reflects a broader trend in AI research towards not just performance, but also utility, robustness, and transparency.

\subsection*{Core Tasks: Link Prediction and Knowledge Graph Completion}

Link prediction and knowledge graph completion (KGC) serve as the fundamental tasks for evaluating the efficacy of Knowledge Graph Embedding (KGE) models. These tasks are crucial for the continuous growth and refinement of knowledge graphs (KGs), addressing their inherent incompleteness by inferring missing facts \cite{rossi2020, dai2020, yan2022, ge2023}. In link prediction, the goal is to predict either the missing head entity ($?, r, t$), tail entity ($h, r, ?$), or relation ($h, ?, t$) within a triple. KGE models achieve this by learning low-dimensional vector representations for entities and relations such that a scoring function can accurately estimate the plausibility of a given triple. Models like TransE \cite{wang2014}, RotatE \cite{sun2018}, and ComplEx \cite{trouillon2016} (not cited in provided list, but a common baseline) represent entities and relations in different geometric spaces (e.g., Euclidean, complex, hyperbolic \cite{pan2021, liang2024, lu2024fsd}), each with specific inductive biases to capture different relational patterns.

Despite significant advancements, these core tasks present persistent challenges. A major limitation stems from the inherent sparsity and heterogeneity of real-world KGs, which can lead to poor generalization for entities and relations with limited observed triples. The complexity of relations, including one-to-many, many-to-one, and many-to-many mappings, also poses difficulties for simpler models. For instance, while RotatE \cite{sun2018} excels at modeling symmetric and antisymmetric relations by representing relations as rotations in complex space, it may struggle with highly diverse or hierarchical relation types without further augmentation. The quality of negative sampling, which generates "false" triples to train the model, is another critical factor. Naive negative sampling can lead to "easy" negatives, hindering effective learning, prompting the development of more sophisticated strategies like confidence-aware negative sampling for noisy KGs \cite{shan2018} or methods that consider entity types \cite{wang2021, he2023}. The choice of negative sampling strategy is crucial, as highlighted by recent reviews \cite{qian2021, madushanka2024}.

Furthermore, the static nature of many KGE models means they often fail to account for the temporal dynamics of facts, where the validity of a triple changes over time. This limitation, addressed by temporal KGEs (as discussed in Section 6), underscores the need for models that can capture not just static relationships but also their evolution. The experimental setups and choice of evaluation metrics (e.g., Mean Reciprocal Rank (MRR), Hits@K) also significantly affect generalizability. As \cite{lloyd2022} empirically demonstrated, hyperparameter sensitivities vary substantially between knowledge graphs, suggesting that optimal tuning strategies are dataset-specific and that a model performing well on one benchmark may not generalize to another without extensive re-tuning. This variability highlights a methodological limitation: the reliance on specific benchmarks may not fully reflect real-world performance or the robustness of a model across diverse KG characteristics. Ultimately, robust and accurate performance in these core tasks is a prerequisite for KGE's successful deployment in more complex, real-world applications.

\subsection*{KGE for Question Answering and Entity Alignment}

Beyond fundamental completion tasks, KGE has proven instrumental in powering more complex AI applications, particularly Question Answering (QA) over Knowledge Graphs and Entity Alignment. These applications leverage KGE's ability to capture semantic relationships in a continuous space, bridging the gap between natural language and structured knowledge, or between disparate knowledge sources.

For \textbf{Question Answering over Knowledge Graphs (KGQA)}, KGE models provide a powerful mechanism to interpret natural language queries and retrieve relevant information from KGs. Traditional KGQA systems often rely on complex semantic parsing or rule-based methods, which struggle with ambiguity and scalability. KGE-based approaches, however, embed both questions and KG components (entities, relations, paths) into a shared vector space, allowing for semantic matching. For instance, \cite{huang2019} explored KGE-based QA, demonstrating how embedding representations can facilitate direct matching between question semantics and KG facts. More recently, hybrid approaches combining KGE with large language models (LLMs) or BERT-like architectures have emerged. \cite{zhou2023} presented "Marie and BERT," a KGE-based QA system specifically for chemistry, showcasing how KGE can provide structured knowledge to enhance language models' understanding of domain-specific queries. Similarly, \cite{do2021mw0} developed a BERT-based triple classification model using KGE for QA systems. The strength of KGE in QA lies in its ability to handle semantic variations and provide a robust similarity measure. However, a critical limitation is its struggle with complex, multi-hop reasoning or questions requiring logical inference, which often necessitate augmenting KGE with symbolic reasoning components \cite{tang2022}. The semantic gap between the nuanced expressions in natural language and the rigid structure of KGs remains a challenge, often requiring sophisticated contextualization techniques \cite{wang2019}. Experimental setups for KGQA often rely on benchmark datasets that may not fully capture the diversity and complexity of real-world user queries, affecting generalizability.

\textbf{Entity Alignment (EA)} is another crucial application, vital for integrating heterogeneous KGs and building more comprehensive knowledge bases. Different KGs often describe the same real-world entities using different identifiers or schemas, leading to data silos. KGE facilitates EA by mapping entities from multiple KGs into a shared embedding space, where aligned entities are expected to have similar representations. \cite{sun2018} proposed a novel bootstrapping approach, BootEA, to address the challenge of limited prior alignment (labeled training data) in embedding-based EA. BootEA iteratively labels likely entity alignments and refines alignment-oriented KGEs, crucially employing a global optimal labeling strategy based on max-weighted matching and an alignment editing method to mitigate error accumulation. This approach significantly advances the state-of-the-art by making EA more robust in low-resource settings. Other works, such as \cite{zhang2019} and \cite{fanourakis2022}, also explore multi-view KGE and experimental reviews for EA. Ontology-guided EA, like OntoEA \cite{xiang2021}, further enhances alignment by incorporating ontological constraints. While KGE-based EA is powerful, its methodological limitations include sensitivity to the quality of initial embeddings and the challenge of handling highly heterogeneous KGs where structural similarities are minimal. The assumption of a shared embedding space might not hold perfectly for KGs with vastly different underlying schemas or data distributions. Furthermore, scalability for very large KGs remains a practical constraint, although solutions like large-scale EA via merging, partitioning, and embedding \cite{xin2022dam} are emerging. The "why" behind these limitations often stems from the inherent difficulty of reconciling diverse semantic contexts and structural biases present in independently constructed KGs, making perfect, unambiguous alignment a theoretically challenging problem.

\subsection*{Recommendation Systems and Domain-Specific Applications}

The ability of Knowledge Graph Embedding (KGE) to capture intricate relationships and rich semantic information has made it a transformative technology in \textbf{Recommendation Systems} and various \textbf{Domain-Specific Applications}.

In recommendation systems, KGE addresses critical challenges like data sparsity and cold-start problems by enriching user and item representations with structured knowledge from KGs. By embedding users, items, and their attributes (e.g., genre, director, brand) into a unified vector space, KGE models can infer latent preferences and relationships that are not explicitly present in interaction data. Early works, such as \cite{gradgyenge2017xdy} and \cite{sun2018}, demonstrated how recurrent KGE and graph embedding techniques could enhance recommendation accuracy. The field has since evolved to include more sophisticated approaches, such as hierarchical attentive KGE for personalized recommendations \cite{sha2019i3a, sha2019plw}, which can capture user preferences at different levels of abstraction. Cross-domain recommendation systems, which leverage knowledge from one domain to improve recommendations in another, also benefit significantly from KGE \cite{liu2023, huang2023grx}. KGE helps bridge semantic gaps between domains by finding common latent features. Furthermore, context-aware recommendation systems, which consider the dynamic context of user interactions, have integrated KGE to model temporal and situational factors \cite{mezni20218ml, mezni2021ezn}. The main trade-off in KGE-based recommendation is often between accuracy and interpretability, as complex embedding models can act as black boxes. Moreover, the quality and completeness of the underlying KG are paramount; a noisy or incomplete KG can propagate errors and lead to suboptimal recommendations. The dynamic nature of user preferences and item characteristics also presents a challenge, requiring continual learning or adaptive KGE models to maintain relevance.

Beyond general recommendations, KGE has found critical deployment in specialized, domain-specific applications:
\begin{itemize}
    \item \textbf{Healthcare and Drug Discovery:} KGE is a powerful tool for uncovering hidden relationships in biomedical knowledge. It has been successfully applied to drug repurposing \cite{sosa2019ih0, islam2023}, where it identifies new therapeutic uses for existing drugs by analyzing their interactions with diseases and targets. KGE also aids in predicting drug-drug interactions (DDIs) \cite{elebi20182bd, elebi2019bzc, su2023v6e, li2024gar, hao2022cl4}, which is crucial for patient safety. Other applications include predicting adverse drug reactions \cite{zhang2021wg7, li2024sgp}, identifying disease-gene associations \cite{wang2024c8z}, and even herb-target prediction \cite{duan2024d3f}. The "why" behind KGE's success here lies in its ability to process vast, heterogeneous biological data and infer non-obvious connections that traditional methods might miss. However, a significant practical constraint is the construction and curation of high-quality, comprehensive biomedical KGs, which is often labor-intensive and requires expert knowledge.
    \item \textbf{Industrial and Manufacturing:} KGE is increasingly used in Industry 4.0 scenarios. It supports defect diagnosis in additive manufacturing \cite{wang2023s70, dong2025l9k}, where KGs capture complex relationships between manufacturing processes, materials, and defects. KGE also facilitates manufacturing knowledge recommendation for collaborative design \cite{jing2024nxw} and enhances cognitive intelligent manufacturing by aggregating multi-hierarchical information \cite{li2021x10, liu2024tc2}.
    \item \textbf{Patent Analysis:} KGE can measure knowledge proximity between patents, aiding in technology landscape analysis and innovation management \cite{li2022}. By embedding patent metadata into a KG, researchers can identify related inventions and emerging technological trends.
    \item \textbf{Other Diverse Applications:} KGE has been applied to academic search engines for explicit semantic ranking \cite{xiong2017zqu, mai2018u0h}, financial news analysis for stock price prediction \cite{liu2018kvd}, urban flow pattern mining \cite{liu2021wqa}, ecotoxicological effect prediction \cite{myklebust201941l}, and even in the context of security knowledge graphs for relational reasoning \cite{liu2024mji} and mineral prospectivity mapping \cite{yan2024joa}.
\end{itemize}
The success of KGE in these diverse domains underscores its versatility. However, a common methodological limitation across these applications is the heavy reliance on the quality, completeness, and domain-specificity of the underlying KGs. Constructing and maintaining such KGs is often a significant practical constraint, and the generalizability of KGE models trained on one domain-specific KG to another can be limited due to different vocabularies and relational structures.

\subsection*{Towards Explainable Knowledge Graph Embedding}

As Knowledge Graph Embedding (KGE) models become increasingly integrated into critical AI applications, the demand for \textbf{explainability} has surged. Explainable AI (XAI) aims to make AI systems more transparent, understandable, and trustworthy, which is particularly vital for KGE-driven predictions in sensitive domains like healthcare or finance. The goal is to move beyond mere prediction accuracy to provide insights into \textit{why} a particular prediction was made, \textit{what} underlying knowledge contributed, and \textit{how} the model arrived at its conclusion.

Early KGE models, especially simpler translational models like TransE, offered a degree of inherent interpretability due to their geometric intuition (e.g., $h + r \approx t$). However, as models grew in complexity (e.g., deep neural networks, graph attention networks), their black-box nature intensified, making direct interpretation challenging. This led to the development of methods specifically designed to enhance KGE explainability.

One prominent approach involves \textbf{path-based explanations}, where the model identifies and highlights relevant paths within the KG that support a prediction. For instance, \cite{jia201870f} and \cite{jia20207dd} explored path-specific KGE, where the semantic paths connecting entities are explicitly modeled and can be used to justify predictions. These methods provide local explanations by showing the chain of reasoning through the graph. Another direction leverages \textbf{attention mechanisms} within KGE models. Graph Attention Networks (GATs) in KGE, such as DisenKGAT \cite{wu2021}, can assign varying importance weights to different neighboring entities and relations during embedding aggregation. DisenKGAT, in particular, aims for disentangled entity representations, where different components capture distinct aspects of an entity. By analyzing which components and which neighbors are activated for a specific prediction, it offers insights into the contributing factors, thereby enhancing both accuracy and explainability. \cite{wang2020} also explored graph attenuated attention networks for KGE.

A powerful avenue for explainability is the \textbf{integration of KGE with symbolic rules}. Models like RulE \cite{tang2022} learn rule embeddings that can be combined with KGE for more transparent reasoning. Similarly, approaches that preserve soft logical regularity \cite{guo2017, guo2020, wang20199fe} aim to align numerical embeddings with symbolic rules, making the underlying logic more accessible. This hybrid approach attempts to combine the expressive power of embeddings with the interpretability of rules. For recommendation systems, Contextualized Knowledge Graph Embedding (CKGE) for explainable talent training course recommendation \cite{yang2023} introduces a local path mask prediction mechanism. This mechanism explicitly reveals the saliency of different meta-paths, providing direct, motivation-aware explanations for recommendations by highlighting \textit{why} a course is recommended (e.g., "because you have skills X and Y, and this course enhances Z"). Other applications, such as inference reconciliation for robot actions \cite{daruna2022dmk}, also demonstrate the utility of explainable KGE in providing transparent decision-making.

Despite these advancements, significant challenges remain. There is an inherent trade-off between model expressiveness/accuracy and interpretability; more complex models often yield better performance but are harder to explain. The definition and quantification of "good" explanations are also subjective and context-dependent, making objective evaluation difficult. Current methods often provide local explanations for individual predictions but struggle to offer a global understanding of the entire model's behavior. The "why" behind these limitations often stems from the high-dimensional, non-linear nature of embedding spaces, which makes it difficult to map learned features back to human-understandable concepts. Future research directions include developing more robust metrics for explainability, exploring counterfactual explanations, and integrating KGE with Large Language Models (LLMs) to generate natural language explanations that are more intuitive for human users \cite{liu2024q3q, nie202499i}. The ultimate goal is to achieve user-centric explainability, where insights are not just technically sound but also actionable and relevant to the end-user's needs.

\label{sec:8._conclusion_and_future_directions}

\section*{8. Conclusion and Future Directions}

The journey of Knowledge Graph Embedding (KGE) research has been marked by a remarkable intellectual trajectory, evolving from foundational geometric models to sophisticated deep learning architectures. This evolution reflects a collective endeavor to address the inherent complexities of knowledge representation, pushing the boundaries of expressiveness, efficiency, and adaptability. Initially, KGE models sought to embed symbolic knowledge into continuous vector spaces, enabling computational reasoning and overcoming the limitations of discrete representations. Over time, the field has witnessed an "arms race" dynamic, where new models are continually developed to capture increasingly nuanced relational patterns, temporal dynamics, and contextual information. This section synthesizes these extensive advancements, consolidating the core achievements across diverse methodological families and illustrating their interconnectedness. It critically examines how the field has tackled fundamental challenges in knowledge representation, setting the stage for a thorough exploration of remaining hurdles and promising future directions that will shape the next generation of intelligent systems.

\subsection*{8.1. Summary of Key Advancements}

The intellectual trajectory of KGE research has seen a profound evolution, starting with foundational geometric models and progressing to highly expressive deep learning architectures. Early KGE models, primarily translational and distance-based, such as TransE \cite{wang2014}, laid the groundwork by representing entities as points and relations as translations in a low-dimensional vector space. While offering intuitive interpretability and computational efficiency, these models struggled with complex relation types like one-to-many or symmetric relations. This limitation spurred the development of more sophisticated geometric models, including TransA \cite{jia2015} which introduced adaptive margins to better capture local graph characteristics, and RotatE \cite{sun2018}, which models relations as rotations in complex space, excelling at symmetric/antisymmetric patterns. Further explorations into diverse geometric spaces, such as hyperbolic \cite{pan2021, liang2024, lu2024fsd}, spherical \cite{li2024}, and quaternion spaces \cite{zhang2019rlm, chen2025, le2023hjy}, have significantly enhanced expressiveness, allowing for better representation of hierarchical structures and complex relational semantics. Models like CompoundE \cite{ge2022, ge2023} and HousE \cite{li2022} exemplify this trend by combining multiple geometric operations to capture richer relational properties.

Beyond static representations, a critical advancement has been the integration of temporal dynamics. Recognizing that facts in real-world KGs are time-sensitive, models like HyTE \cite{dasgupta2018} introduced time-specific hyperplanes, while ATiSE \cite{xu2019} pioneered the use of additive time series decomposition and Gaussian distributions to model the evolution and inherent uncertainty of entity/relation embeddings over time. This marked a significant shift from static to dynamic knowledge representation, enabling more accurate temporal reasoning \cite{xu2020, sadeghian2021, xie2023, wang2024}. Concurrently, the field has embraced deep learning architectures, particularly Graph Neural Networks (GNNs), to aggregate rich neighborhood information. DisenKGAT \cite{wu2021} stands out by introducing disentangled graph attention networks, allowing entities to have multiple, independent latent components, thereby better capturing their multi-faceted nature and enhancing interpretability. This addresses the limitation of static, monolithic entity representations that often fail to distinguish different semantic aspects.

The challenge of parameter efficiency, especially for massive KGs, has also driven innovation. Conventional KGE models often suffer from parameter explosion, where embedding parameters scale linearly with the number of entities. EARL \cite{chen2023} addressed this by proposing entity-agnostic representation learning, using universal encoders and a small set of "reserved entities" to achieve competitive performance with significantly fewer parameters. This is a crucial breakthrough for deploying KGE models in resource-constrained environments or federated learning settings \cite{zhang2024, chen20226e4}. Furthermore, robustness against noisy data has been improved through methods like confidence-aware negative sampling \cite{shan2018} and rule-guided embeddings \cite{guo2017, guo2020}, which explicitly incorporate logical constraints to refine embeddings. In entity alignment, BootEA \cite{sun2018} provided a robust bootstrapping framework that iteratively refines alignments, mitigating error accumulation in low-resource scenarios. These advancements collectively demonstrate a concerted effort to build KGE models that are not only more expressive and accurate but also more adaptable to the complexities, scale, and dynamic nature of real-world knowledge.

\subsection*{8.2. Open Challenges and Theoretical Gaps}

Despite the significant advancements, the KGE field grapples with several persistent open challenges and theoretical gaps that hinder its full potential. One primary concern is \textbf{scalability and efficiency}. While models like EARL \cite{chen2023} address parameter explosion, training and inference on truly massive, industrial-scale KGs remain computationally intensive, especially for deep learning architectures. The empirical study by \cite{lloyd2022} highlights the substantial variability in hyperparameter sensitivities across different KGs, implying that optimal tuning strategies are dataset-specific, making large-scale deployment and generalization difficult without extensive, costly re-tuning. This issue is exacerbated in dynamic or continually evolving KGs, where frequent updates necessitate efficient incremental learning mechanisms \cite{wei20215a7, liu2024, liu2024to0}.

\textbf{Robustness and adversarial attacks} pose another critical threat. KGE models are susceptible to data poisoning attacks \cite{zhang20190zu, zhou2024}, where malicious triples can subtly degrade embedding quality and lead to incorrect predictions. The "arms race" dynamic in adversarial machine learning suggests that defenses often lag behind attacks, necessitating more proactive and theoretically grounded robustness measures. Furthermore, the inherent \textbf{black-box nature} of many deep learning KGE models continues to be a major theoretical and practical gap, especially for explainability. While methods like DisenKGAT \cite{wu2021} and path-based explanations \cite{jia201870f, jia20207dd} offer glimpses into model reasoning, a universally accepted definition and robust quantification of "good" explanations remain elusive. The challenge lies in bridging the high-dimensional, non-linear embedding space with human-understandable symbolic logic, particularly for complex, multi-hop inferences.

The modeling of \textbf{dynamic and evolving KGs} still presents theoretical hurdles. While temporal KGEs like ATiSE \cite{xu2019} have introduced sophisticated time series decomposition, they often rely on simplifying assumptions (e.g., stationary covariance matrices) that may not fully capture the unpredictable and non-linear evolution of knowledge. Formalizing continuous temporal changes and event-based dynamics, rather than discrete timestamps, is an ongoing challenge \cite{liu201918i, xu2020, sadeghian2021}. Moreover, \textbf{heterogeneity and multimodality} remain significant issues. Integrating diverse data types (e.g., text, images, numerical attributes) into a coherent, unified embedding space without losing modality-specific information is complex \cite{wu2018c4b, shen2022, zhang2023}. The theoretical underpinnings for combining such disparate signals effectively are still developing.

A broader theoretical gap is the \textbf{lack of a unified framework} for KGE. Different models operate under distinct geometric assumptions (Euclidean, complex, hyperbolic) or architectural paradigms (translational, bilinear, GNNs), each with specific inductive biases. Understanding \textit{why} certain models excel under particular KG characteristics or relation types is often empirical rather than theoretically derived. This leads to contradictory findings where a model performing well on one benchmark (e.g., UMLS being "easier" as noted by \cite{lloyd2022}) may not generalize to others. The impact of negative sampling strategies \cite{qian2021, madushanka2024} also lacks a comprehensive theoretical understanding, often relying on heuristic choices. These limitations underscore the need for more foundational research to develop robust, generalizable, and interpretable KGE models that can handle the full spectrum of real-world knowledge.

\subsection*{8.3. Emerging Trends and Broader Societal Impact}

The field of Knowledge Graph Embedding is at the cusp of several transformative trends, poised to significantly broaden its societal impact. A paramount emerging trend is the \textbf{integration with Large Language Models (LLMs)}. KGE is increasingly seen as a crucial component for grounding LLMs in factual knowledge, mitigating their hallucination tendencies, and enhancing their reasoning capabilities \cite{liu2024q3q, nie202499i}. Conversely, LLMs are being leveraged for automated KG construction, completion, and generating natural language explanations for KGE predictions, bridging the gap between symbolic and neural AI. This synergy promises more robust, reliable, and interpretable AI systems.

Another critical direction is \textbf{continual and incremental learning}. As KGs are dynamic and constantly evolving, the ability to update KGE models efficiently without full retraining is vital. Approaches like incremental LoRA \cite{liu2024} and incremental distillation \cite{liu2024to0} are addressing this, alongside meta-learning techniques for adapting to evolving service ecosystems \cite{sun2024, mao2024v2s}. This ensures that KGE models remain current and relevant in rapidly changing knowledge environments. Research into \textbf{advanced geometric spaces} continues to flourish, with a focus on multi-curvature adaptive embeddings \cite{wang2024, wang2024} and novel transformations in complex spaces \cite{dong2022taz, zhang2025ebv}, aiming to capture even more intricate relational semantics and hierarchical structures. The trend towards \textbf{automated KGE design} is also gaining momentum, with efforts like AutoSF \cite{zhang2019} and message function search \cite{di2023} exploring automated ways to discover optimal scoring functions and architectures, reducing reliance on manual hyperparameter tuning \cite{zhang2022fpm}.

The broader societal impact of KGE is expanding rapidly across diverse domains. In \textbf{healthcare}, KGE is driving molecular-evaluated drug repurposing \cite{islam2023}, enhancing drug-drug interaction prediction \cite{su2023v6e, li2024gar}, and even enabling explainable healthcare prediction with AIGC-designed models \cite{yang2025}. These applications promise more personalized medicine and accelerated drug discovery. In \textbf{industrial and manufacturing sectors}, KGE is crucial for defect diagnosis \cite{wang2023s70, dong2025l9k}, knowledge recommendation for collaborative design \cite{jing2024nxw}, and advancing cognitive intelligent manufacturing \cite{liu2024tc2}. The ability to embed complex industrial processes and relationships facilitates smarter, more efficient operations. Furthermore, KGE contributes to \textbf{environmental sustainability} through applications like ecotoxicological effect prediction \cite{myklebust201941l} and marine wind speed forecasting \cite{dong2024ijo}.

Beyond these specific applications, the increasing demand for \textbf{explainable KGE} is driving a shift towards more transparent and trustworthy AI. As KGE models influence critical decisions, providing clear justifications for their outputs becomes paramount, fostering user trust and enabling responsible AI development. This extends to addressing \textbf{fairness and bias} in KGE models \cite{radstok2021yup}, ensuring that embedded knowledge does not perpetuate or amplify societal biases. The development of \textbf{federated KGE} \cite{zhang2024, zhou2024, hu20230kr, chen20226e4} is crucial for privacy-preserving knowledge sharing and collaborative learning, particularly in sensitive domains like healthcare, demonstrating KGE's role in ethical AI. Ultimately, KGE is not just a technical advancement in knowledge representation; it is a foundational technology that empowers intelligent systems to understand, reason, and interact with the world in increasingly sophisticated and responsible ways, shaping the future of AI and its integration into society.

\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{377}

\bibitem{sun2018}
Zequn Sun, Wei Hu, Qingheng Zhang, et al. (2018). \textit{Bootstrapping Entity Alignment with Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{dasgupta2018}
S. Dasgupta, Swayambhu Nath Ray, and P. Talukdar (2018). \textit{HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2023}
Mingyang Chen, Wen Zhang, Zhen Yao, et al. (2023). \textit{Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2023}
Yang Yang, Chubing Zhang, Xin Song, et al. (2023). \textit{Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation}. ACM Trans. Inf. Syst..

\bibitem{jia2015}
Yantao Jia, Yuanzhuo Wang, Hailun Lin, et al. (2015). \textit{Locally Adaptive Translation for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{lloyd2022}
Oliver Lloyd, Yi Liu, and T. Gaunt (2022). \textit{Assessing the effects of hyperparameters on knowledge graph embedding quality}. Journal of Big Data.

\bibitem{wu2021}
Junkang Wu, Wentao Shi, Xuezhi Cao, et al. (2021). \textit{DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network}. International Conference on Information and Knowledge Management.

\bibitem{xu2019}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2019). \textit{Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition}. arXiv.org.

\bibitem{shan2018}
Yingchun Shan, Chenyang Bu, Xiaojian Liu, et al. (2018). \textit{Confidence-Aware Negative Sampling Method for Noisy Knowledge Graph Embedding}. International Conference on Big Knowledge.

\bibitem{zheng2024}
Zhuoxun Zheng, Baifan Zhou, Hui Yang, et al. (2024). \textit{Knowledge graph embedding closed under composition}. Data mining and knowledge discovery.

\bibitem{he2023}
Peng He, Gang Zhou, Yao Yao, et al. (2023). \textit{A type-augmented knowledge graph embedding framework for knowledge graph completion}. Scientific Reports.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{TransG : A Generative Model for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{guo2017}
Shu Guo, Quan Wang, Lihong Wang, et al. (2017). \textit{Knowledge Graph Embedding with Iterative Guidance from Soft Rules}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021}
Mingyang Chen, Wen Zhang, Yushan Zhu, et al. (2021). \textit{Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2023}
Guang-pu Li, Zequn Sun, Wei Hu, et al. (2023). \textit{Position-Aware Relational Transformer for Knowledge Graph Embedding}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{zhou2023}
Xiaochi Zhou, Shaocong Zhang, Mehal Agarwal, et al. (2023). \textit{Marie and BERTA Knowledge Graph Embedding Based Question Answering System for Chemistry}. ACS Omega.

\bibitem{xiang2021}
Yuejia Xiang, Ziheng Zhang, Jiaoyan Chen, et al. (2021). \textit{OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding}. Findings.

\bibitem{cao2022}
Jiahang Cao, Jinyuan Fang, Zaiqiao Meng, et al. (2022). \textit{Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces}. ACM Computing Surveys.

\bibitem{wang2021}
Peng Wang, Jing Zhou, Yuzhang Liu, et al. (2021). \textit{TransET: Knowledge Graph Embedding with Entity Types}. Electronics.

\bibitem{guo2020}
Shu Guo, Lin Li, Zhen Hui, et al. (2020). \textit{Knowledge Graph Embedding Preserving Soft Logical Regularity}. International Conference on Information and Knowledge Management.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification}. Knowledge-Based Systems.

\bibitem{shen2022}
Jianhao Shen, Chenguang Wang, Linyuan Gong, et al. (2022). \textit{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion}. International Conference on Computational Linguistics.

\bibitem{hu2024}
Kairong Hu, Xiaozhi Zhu, Hai Liu, et al. (2024). \textit{Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation for Knowledge Graph Embedding Learning}. IEEE transactions on consumer electronics.

\bibitem{liu2024}
Yang Liu, Huang Fang, Yunfeng Cai, et al. (2024). \textit{MQuinE: a Cure for Z-paradox in Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2019}
Yongqi Zhang, Quanming Yao, Wenyuan Dai, et al. (2019). \textit{AutoSF: Searching Scoring Functions for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{yang2019}
Shihui Yang, Jidong Tian, Honglun Zhang, et al. (2019). \textit{TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics}. International Joint Conference on Artificial Intelligence.

\bibitem{xie2023}
Zhiwen Xie, Runjie Zhu, Jin Liu, et al. (2023). \textit{TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{wang2024}
Jiapu Wang, Boyue Wang, Junbin Gao, et al. (2024). \textit{MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion}. IEEE Transactions on Cybernetics.

\bibitem{xiao2019}
Han Xiao, Yidong Chen, and X. Shi (2019). \textit{Knowledge Graph Embedding Based on Multi-View Clustering Framework}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sachan2020}
Mrinmaya Sachan (2020). \textit{Knowledge Graph Embedding Compression}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{madushanka2024}
Tiroshan Madushanka, and R. Ichise (2024). \textit{Negative Sampling in Knowledge Graph Representation Learning: A Review}. arXiv.org.

\bibitem{zhu2022}
Chaoyu Zhu, Zhihao Yang, Xiaoqiong Xia, et al. (2022). \textit{Multimodal reasoning based on knowledge graph embedding for specific diseases}. Bioinform..

\bibitem{liang2024}
Qiuyu Liang, Weihua Wang, F. Bao, et al. (2024). \textit{Fully Hyperbolic Rotation for Knowledge Graph Embedding}. European Conference on Artificial Intelligence.

\bibitem{li2024}
Li, Yuyi Ao, and Jingrui He (2024). \textit{SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{ebisu2017}
Takuma Ebisu, and R. Ichise (2017). \textit{TorusE: Knowledge Graph Embedding on a Lie Group}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2021}
Zhao Zhang, Fuzhen Zhuang, Hengshu Zhu, et al. (2021). \textit{Towards Robust Knowledge Graph Embedding via Multi-Task Reinforcement Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{huang2019}
Xiao Huang, Jingyuan Zhang, Dingcheng Li, et al. (2019). \textit{Knowledge Graph Embedding Based Question Answering}. Web Search and Data Mining.

\bibitem{tang2019}
Yun Tang, Jing Huang, Guangtao Wang, et al. (2019). \textit{Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sun2018}
Zhu Sun, Jie Yang, Jie Zhang, et al. (2018). \textit{Recurrent knowledge graph embedding for effective recommendation}. ACM Conference on Recommender Systems.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding: An Overview}. APSIPA Transactions on Signal and Information Processing.

\bibitem{wang2020}
Rui Wang, Bicheng Li, Shengwei Hu, et al. (2020). \textit{Knowledge Graph Embedding via Graph Attenuated Attention Networks}. IEEE Access.

\bibitem{li2022}
Rui Li, Jianan Zhao, Chaozhuo Li, et al. (2022). \textit{HousE: Knowledge Graph Embedding with Householder Parameterization}. International Conference on Machine Learning.

\bibitem{zhang2019}
Qingheng Zhang, Zequn Sun, Wei Hu, et al. (2019). \textit{Multi-view Knowledge Graph Embedding for Entity Alignment}. International Joint Conference on Artificial Intelligence.

\bibitem{tang2022}
Xiaojuan Tang, Song-Chun Zhu, Yitao Liang, et al. (2022). \textit{RulE: Knowledge Graph Reasoning with Rule Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lv2018}
Xin Lv, Lei Hou, Juan-Zi Li, et al. (2018). \textit{Differentiating Concepts and Instances for Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2025}
Jie Chen, Yinlong Wang, Shu Zhao, et al. (2025). \textit{Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction}. ACM Trans. Asian Low Resour. Lang. Inf. Process..

\bibitem{qian2021}
Jing Qian, Gangmin Li, Katie Atkinson, et al. (2021). \textit{Understanding Negative Sampling in Knowledge Graph Embedding}. International Journal of Artificial Intelligence & Applications.

\bibitem{dai2020}
Yuanfei Dai, Shiping Wang, N. Xiong, et al. (2020). \textit{A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks}. Electronics.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation}. IEEE transactions on fuzzy systems.

\bibitem{yan2022}
Qi Yan, Jiaxin Fan, Mohan Li, et al. (2022). \textit{A Survey on Knowledge Graph Embedding}. International Conference on Data Science in Cyberspace.

\bibitem{zhang2023}
Yichi Zhang, Mingyang Chen, and Wen Zhang (2023). \textit{Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{li2021}
Ren Li, Yanan Cao, Qiannan Zhu, et al. (2021). \textit{How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2025}
Qingqing Yang, Min He, Zhongwen Li, et al. (2025). \textit{A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction}. IEEE transactions on consumer electronics.

\bibitem{wang2019}
Quan Wang, Pingping Huang, Haifeng Wang, et al. (2019). \textit{CoKE: Contextualized Knowledge Graph Embedding}. arXiv.org.

\bibitem{di2023}
Shimin Di, and Lei Chen (2023). \textit{Message Function Search for Knowledge Graph Embedding}. The Web Conference.

\bibitem{jia2017}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2017). \textit{Knowledge Graph Embedding}. ACM Transactions on the Web.

\bibitem{choudhary2021}
Shivani Choudhary, Tarun Luthra, Ashima Mittal, et al. (2021). \textit{A Survey of Knowledge Graph Embedding and Their Applications}. arXiv.org.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction}. International Joint Conference on Artificial Intelligence.

\bibitem{hu2024}
Lei Hu, Wenwen Li, Jun Xu, et al. (2024). \textit{GeoEntity-type constrained knowledge graph embedding for predicting natural-language spatial relations}. International Journal of Geographical Information Science.

\bibitem{wang2014}
Zhen Wang, Jianwen Zhang, Jianlin Feng, et al. (2014). \textit{Knowledge Graph Embedding by Translating on Hyperplanes}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2020}
Yushan Zhu, Wen Zhang, Mingyang Chen, et al. (2020). \textit{DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning}. Web Search and Data Mining.

\bibitem{ali2020}
Mehdi Ali, M. Berrendorf, Charles Tapley Hoyt, et al. (2020). \textit{Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{mohamed2020}
Sameh K. Mohamed, A. Nounu, and V. Novek (2020). \textit{Biological applications of knowledge graph embedding models}. Briefings Bioinform..

\bibitem{gao2020}
Chang Gao, Chengjie Sun, Lili Shan, et al. (2020). \textit{Rotate3D: Representing Relations as Rotations in Three-Dimensional Space for Knowledge Graph Embedding}. International Conference on Information and Knowledge Management.

\bibitem{peng2021}
Xutan Peng, Guanyi Chen, Chenghua Lin, et al. (2021). \textit{Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis}. North American Chapter of the Association for Computational Linguistics.

\bibitem{shi2025}
Fobo Shi, Duantengchuan Li, Xiaoguang Wang, et al. (2025). \textit{TGformer: A Graph Transformer Framework for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph}. Applied intelligence (Boston).

\bibitem{rosso2020}
Paolo Rosso, Dingqi Yang, and P. Cudr-Mauroux (2020). \textit{Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction}. The Web Conference.

\bibitem{zhou2024}
Enyuan Zhou, Song Guo, Zhixiu Ma, et al. (2024). \textit{Poisoning Attack on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{xie2020}
Zhiwen Xie, Guangyou Zhou, Jin Liu, et al. (2020). \textit{ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{song2021}
Tengwei Song, Jie Luo, and Lei Huang (2021). \textit{Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{zhang2020}
Zhaoli Zhang, Zhifei Li, Hai Liu, et al. (2020). \textit{Multi-Scale Dynamic Convolutional Network for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{ge2022}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2022). \textit{CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations}. arXiv.org.

\bibitem{ren2020}
Feiliang Ren, Jucheng Li, Huihui Zhang, et al. (2020). \textit{Knowledge Graph Embedding with Atrous Convolution and Residual Learning}. International Conference on Computational Linguistics.

\bibitem{yuan2019}
Jun Yuan, Neng Gao, and Ji Xiang (2019). \textit{TransGate: Knowledge Graph Embedding with Shared Gate Structure}. AAAI Conference on Artificial Intelligence.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransA: An Adaptive Approach for Knowledge Graph Embedding}. arXiv.org.

\bibitem{sun2018}
Zhiqing Sun, Zhihong Deng, Jian-Yun Nie, et al. (2018). \textit{RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space}. International Conference on Learning Representations.

\bibitem{ji2015}
Guoliang Ji, Shizhu He, Liheng Xu, et al. (2015). \textit{Knowledge Graph Embedding via Dynamic Mapping Matrix}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lin2020}
Lifan Lin, and Kun She (2020). \textit{Tensor Decomposition-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{islam2023}
M. Islam, Diego Amaya-Ramirez, B. Maigret, et al. (2023). \textit{Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding}. Scientific Reports.

\bibitem{wang2021}
Haoyu Wang, Yaqing Wang, Defu Lian, et al. (2021). \textit{A Lightweight Knowledge Graph Embedding Framework for Efficient Inference and Storage}. International Conference on Information and Knowledge Management.

\bibitem{broscheit2020}
Samuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek, et al. (2020). \textit{LibKGE - A knowledge graph embedding library for reproducible research}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{fanourakis2022}
N. Fanourakis, Vasilis Efthymiou, D. Kotzinos, et al. (2022). \textit{Knowledge graph embedding methods for entity alignment: experimental review}. Data mining and knowledge discovery.

\bibitem{wang2018}
Peifeng Wang, Jialong Han, Chenliang Li, et al. (2018). \textit{Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{tabacof2019}
Pedro Tabacof, and Luca Costabello (2019). \textit{Probability Calibration for Knowledge Graph Embedding Models}. International Conference on Learning Representations.

\bibitem{pei2019}
Shichao Pei, Lu Yu, R. Hoehndorf, et al. (2019). \textit{Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference}. The Web Conference.

\bibitem{zhang2018}
Yongqi Zhang, Quanming Yao, Yingxia Shao, et al. (2018). \textit{NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{li2021}
Zelong Li, Jianchao Ji, Zuohui Fu, et al. (2021). \textit{Efficient Non-Sampling Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2022}
Guangtong Li, L. Siddharth, and Jianxi Luo (2022). \textit{Embedding knowledge graph of patent metadata to measure knowledge proximity}. J. Assoc. Inf. Sci. Technol..

\bibitem{ding2018}
Boyang Ding, Quan Wang, Bin Wang, et al. (2018). \textit{Improving Knowledge Graph Embedding Using Simple Constraints}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2022}
Xuanyu Zhang, Qing Yang, and Dongliang Xu (2022). \textit{TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{sun2024}
Hongliang Sun, Jinlan Liu, Can Wang, et al. (2024). \textit{Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning}. 2024 IEEE International Conference on Web Services (ICWS).

\bibitem{wang2024}
Jiapu Wang, Zheng Cui, Boyue Wang, et al. (2024). \textit{IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion}. The Web Conference.

\bibitem{modak2024}
S. Modak, Aakarsh Malhotra, Sarthak Malik, et al. (2024). \textit{CPa-WAC: Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{xiao2016}
Han Xiao, Minlie Huang, Lian Meng, et al. (2016). \textit{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2023}
Zhao Zhang, Zhanpeng Guan, Fuwei Zhang, et al. (2023). \textit{Weighted Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{guo2015}
Shu Guo, Quan Wang, Bin Wang, et al. (2015). \textit{Semantically Smooth Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xu2020}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2020). \textit{TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation}. International Conference on Computational Linguistics.

\bibitem{zheng2024}
Chenguang Zheng, Guanxian Jiang, Xiao Yan, et al. (2024). \textit{GE2: A General and Efficient Knowledge Graph Embedding Learning System}. Proc. ACM Manag. Data.

\bibitem{zhang2018}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2018). \textit{Knowledge Graph Embedding with Hierarchical Relation Structure}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhu2024}
Beibei Zhu, Ruolin Wang, Junyi Wang, et al. (2024). \textit{A survey: knowledge graph entity alignment research based on graph embedding}. Artificial Intelligence Review.

\bibitem{liu2023}
Jia Liu, Wei Huang, Tianrui Li, et al. (2023). \textit{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{choi2020}
S. Choi, Hyun-Je Song, and Seong-Bae Park (2020). \textit{An Approach to Knowledge Base Completion by a Committee-Based Knowledge Graph Embedding}. Applied Sciences.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding with 3D Compound Geometric Transformations}. APSIPA Transactions on Signal and Information Processing.

\bibitem{sadeghian2021}
A. Sadeghian, Mohammadreza Armandpour, Anthony Colas, et al. (2021). \textit{ChronoR: Rotation Based Temporal Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{liu2024}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Fast and Continual Knowledge Graph Embedding via Incremental LoRA}. International Joint Conference on Artificial Intelligence.

\bibitem{li2022}
Yizhi Li, Wei Fan, Chaochun Liu, et al. (2022). \textit{TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{rossi2020}
Andrea Rossi, D. Firmani, Antonio Matinata, et al. (2020). \textit{Knowledge Graph Embedding for Link Prediction}. ACM Transactions on Knowledge Discovery from Data.

\bibitem{li2023}
Jiang Li, Xiangdong Su, and Guanglai Gao (2023). \textit{TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{peng2020}
Yanhui Peng, and Jing Zhang (2020). \textit{LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction}. Industrial Conference on Data Mining.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding}. IEEE transactions on fuzzy systems.

\bibitem{zhang2024}
Qinggang Zhang, Junnan Dong, Qiaoyu Tan, et al. (2024). \textit{Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{kochsiek2021}
Adrian Kochsiek (2021). \textit{Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques}. Proceedings of the VLDB Endowment.

\bibitem{yang2021}
Han Yang, Leilei Zhang, Bingning Wang, et al. (2021). \textit{Cycle or Minkowski: Which is More Appropriate for Knowledge Graph Embedding?}. International Conference on Information and Knowledge Management.

\bibitem{shang2024}
Bin Shang, Yinliang Zhao, Jun Liu, et al. (2024). \textit{Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion}. AAAI Conference on Artificial Intelligence.

\bibitem{asmara2023}
S. M. Asmara, N. A. Sahabudin, Nor Syahidatul Nadiah Ismail, et al. (2023). \textit{A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links}. International Conference on Software Engineering and Computer Systems.

\bibitem{gregucci2023}
Cosimo Gregucci, M. Nayyeri, D. Hern'andez, et al. (2023). \textit{Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models}. The Web Conference.

\bibitem{pan2021}
Zhe Pan, and Peng Wang (2021). \textit{Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yoon2016}
Hee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, et al. (2016). \textit{A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2024}
Rui Li, Chaozhuo Li, Yanming Shen, et al. (2024). \textit{Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization}. International Conference on Machine Learning.

\bibitem{xiong2017zqu}
Chenyan Xiong, Russell Power, and Jamie Callan (2017). \textit{Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding}. The Web Conference.

\bibitem{gong2020b2k}
Fan Gong, Meng Wang, Haofen Wang, et al. (2020). \textit{SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation}. Big Data Research.

\bibitem{zhou2022ehi}
Bin Zhou, Xingwang Shen, Yuqian Lu, et al. (2022). \textit{Semantic-aware event link reasoning over industrial knowledge graph embedding time series data}. International Journal of Production Research.

\bibitem{le2022ji8}
Thanh-Binh Le, N. Le, and H. Le (2022). \textit{Knowledge graph embedding by relational rotation and complex convolution for link prediction}. Expert systems with applications.

\bibitem{zhou2022vgb}
Zhehui Zhou, Can Wang, Yan Feng, et al. (2022). \textit{JointE: Jointly utilizing 1D and 2D convolution for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{xu2019t6b}
Da Xu, Chuanwei Ruan, Evren Krpeoglu, et al. (2019). \textit{Product Knowledge Graph Embedding for E-commerce}. Web Search and Data Mining.

\bibitem{mezni20218ml}
Haithem Mezni, D. Benslimane, and Ladjel Bellatreche (2021). \textit{Context-Aware Service Recommendation Based on Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{do2021mw0}
P. Do, and Truong H. V. Phan (2021). \textit{Developing a BERT based triple classification model using knowledge graph embedding for question answering system}. Applied intelligence (Boston).

\bibitem{mai2020ei3}
Gengchen Mai, K. Janowicz, Ling Cai, et al. (2020). \textit{SEKGE: A locationaware Knowledge Graph Embedding model for Geographic Question Answering and Spatial Semantic Lifting}. Trans. GIS.

\bibitem{zhang2022eab}
Jiarui Zhang, Jian Huang, Jialong Gao, et al. (2022). \textit{Knowledge graph embedding by logical-default attention graph convolution neural network for link prediction}. Information Sciences.

\bibitem{sosa2019ih0}
Daniel N. Sosa, Alexander Derry, Margaret Guo, et al. (2019). \textit{A Literature-Based Knowledge Graph Embedding Method for Identifying Drug Repurposing Opportunities in Rare Diseases}. bioRxiv.

\bibitem{guan2019pr4}
Niannian Guan, Dandan Song, and L. Liao (2019). \textit{Knowledge graph embedding with concepts}. Knowledge-Based Systems.

\bibitem{fan2014g7s}
M. Fan, Qiang Zhou, E. Chang, et al. (2014). \textit{Transition-based Knowledge Graph Embedding with Relational Mapping Properties}. Pacific Asia Conference on Language, Information and Computation.

\bibitem{zhang20190zu}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Data Poisoning Attack against Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{chen2022mxn}
Qi Chen, Wei Wang, Kaizhu Huang, et al. (2022). \textit{Zero-Shot Text Classification via Knowledge Graph Embedding for Social Media Data}. IEEE Internet of Things Journal.

\bibitem{wang2022hwx}
Xin Wang, Shengfei Lyu, Xiangyu Wang, et al. (2022). \textit{Temporal knowledge graph embedding via sparse transfer matrix}. Information Sciences.

\bibitem{chen20226e4}
Mingyang Chen, Wen Zhang, Zonggang Yuan, et al. (2022). \textit{Federated knowledge graph completion via embedding-contrastive learning}. Knowledge-Based Systems.

\bibitem{abusalih2020gdu}
Bilal Abu-Salih, Marwan Al-Tawil, Ibrahim Aljarah, et al. (2020). \textit{Relational Learning Analysis of Social Politics using Knowledge Graph Embedding}. Data mining and knowledge discovery.

\bibitem{fang2022wp6}
Haichuan Fang, Youwei Wang, Zhen Tian, et al. (2022). \textit{Learning knowledge graph embedding with a dual-attention embedding network}. Expert systems with applications.

\bibitem{elebi2019bzc}
R. elebi, Hseyin Uyar, Erkan Yasar, et al. (2019). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction in realistic settings}. BMC Bioinformatics.

\bibitem{sha2019i3a}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Hierarchical attentive knowledge graph embedding for personalized recommendation}. Electronic Commerce Research and Applications.

\bibitem{li2021ro5}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Recalibration convolutional networks for learning interaction knowledge graph embedding}. Neurocomputing.

\bibitem{xiao20151fj}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransG : A Generative Mixture Model for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2021wg7}
Fei Zhang, Bo Sun, Xiaolin Diao, et al. (2021). \textit{Prediction of adverse drug reactions based on knowledge graph embedding}. BMC Medical Informatics and Decision Making.

\bibitem{wang20186zs}
Guanying Wang, Wen Zhang, Ruoxu Wang, et al. (2018). \textit{Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2021x10}
Xinyu Li, P. Zheng, Jinsong Bao, et al. (2021). \textit{Achieving cognitive mass personalization via the self-X cognitive manufacturing network: An industrial-knowledge-graph- and graph-embedding-enabled pathway}. Engineering.

\bibitem{wang202110w}
Xin Wang, Xiao Liu, Jin Liu, et al. (2021). \textit{A novel knowledge graph embedding based API recommendation method for Mashup development}. World wide web (Bussum).

\bibitem{gutirrezbasulto2018oi0}
Vctor Gutirrez-Basulto, and S. Schockaert (2018). \textit{From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{portisch20221rd}
Jan Portisch, Nicolas Heist, and Heiko Paulheim (2022). \textit{Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction - two sides of the same coin?}. Semantic Web.

\bibitem{zhang2022muu}
Fuwei Zhang, Zhao Zhang, Xiang Ao, et al. (2022). \textit{Along the Time: Timeline-traced Embedding for Temporal Knowledge Graph Completion}. International Conference on Information and Knowledge Management.

\bibitem{feng2016dp7}
Jun Feng, Minlie Huang, Mingdong Wang, et al. (2016). \textit{Knowledge Graph Embedding by Flexible Translation}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{liu2021wqa}
Jia Liu, Tianrui Li, Shenggong Ji, et al. (2021). \textit{Urban Flow Pattern Mining Based on Multi-Source Heterogeneous Data Fusion and Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sang2019gjl}
Shengtian Sang, Zhihao Yang, Xiaoxia Liu, et al. (2019). \textit{GrEDeL: A Knowledge Graph Embedding Based Method for Drug Discovery From Biomedical Literatures}. IEEE Access.

\bibitem{wang2017yjq}
M. Wang, Mengyue Liu, Jun Liu, et al. (2017). \textit{Safe Medicine Recommendation via Medical Knowledge Graph Embedding}. arXiv.org.

\bibitem{jiang20219xl}
Dan Jiang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Kernel multi-attention neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2022fu5}
Yang Liu, Zequn Sun, Guang-pu Li, et al. (2022). \textit{I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning}. International Conference on Information and Knowledge Management.

\bibitem{khan202236g}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{Similarity attributed knowledge graph embedding enhancement for item recommendation}. Information Sciences.

\bibitem{mezni2021ezn}
Haithem Mezni (2021). \textit{Temporal Knowledge Graph Embedding for Effective Service Recommendation}. IEEE Transactions on Services Computing.

\bibitem{zhang2021wix}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Structural context-based knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{huang2021u42}
Xuqian Huang, Jiuyang Tang, Zhen Tan, et al. (2021). \textit{Knowledge graph embedding by relational and entity rotation}. Knowledge-Based Systems.

\bibitem{pavlovic2022qte}
Aleksandar Pavlovic, and Emanuel Sallinger (2022). \textit{ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion}. International Conference on Learning Representations.

\bibitem{wang20213kg}
Shensi Wang, Kun Fu, Xian Sun, et al. (2021). \textit{Hierarchical-aware relation rotational knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{zhang2019rlm}
Shuai Zhang, Yi Tay, Lina Yao, et al. (2019). \textit{Quaternion Knowledge Graph Embedding}. arXiv.org.

\bibitem{mai20195rp}
Gengchen Mai, Bo Yan, K. Janowicz, et al. (2019). \textit{Relaxing Unanswerable Geographic Questions Using A Spatially Explicit Knowledge Graph Embedding Model}. Agile Conference.

\bibitem{han2018tzc}
Zhuobing Han, Xiaohong Li, Hongtao Liu, et al. (2018). \textit{DeepWeak: Reasoning common software weaknesses via knowledge graph embedding}. IEEE International Conference on Software Analysis, Evolution, and Reengineering.

\bibitem{wang2022fvx}
Feiyang Wang, Zhongbao Zhang, Li Sun, et al. (2022). \textit{DiriE: Knowledge Graph Embedding with Dirichlet Distribution}. The Web Conference.

\bibitem{ferrari2022r82}
Ilaria Ferrari, Giacomo Frisoni, Paolo Italiani, et al. (2022). \textit{Comprehensive Analysis of Knowledge Graph Embedding Techniques Benchmarked on Link Prediction}. Electronics.

\bibitem{fu2022df2}
Guirong Fu, Zhao Meng, Zhen Han, et al. (2022). \textit{TempCaps: A Capsule Network-based Embedding Model for Temporal Knowledge Graph Completion}. SPNLP.

\bibitem{wu2018c4b}
Yanrong Wu, and Zhichun Wang (2018). \textit{Knowledge Graph Embedding with Numeric Attributes of Entities}. Rep4NLP@ACL.

\bibitem{zhang202121t}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Knowledge graph embedding by reflection transformation}. Knowledge-Based Systems.

\bibitem{mohamed2019meq}
Sameh K. Mohamed, V. Novek, P. Vandenbussche, et al. (2019). \textit{Loss Functions in Knowledge Graph Embedding Models}. DL4KG@ESWC.

\bibitem{xin2022dam}
Kexuan Xin, Zequn Sun, Wen Hua, et al. (2022). \textit{Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning and Embedding}. International Conference on Information and Knowledge Management.

\bibitem{nie20195gc}
Binling Nie, and Shouqian Sun (2019). \textit{Knowledge graph embedding via reasoning over entities, relations, and text}. Future generations computer systems.

\bibitem{liu2018kvd}
Yang Liu, Qingguo Zeng, Huanrui Yang, et al. (2018). \textit{Stock Price Movement Prediction from Financial News with Deep Learning and Knowledge Graph Embedding}. Pacific Rim Knowledge Acquisition Workshop.

\bibitem{ni2020ruj}
Chien-Chun Ni, Kin Sum Liu, and Nicolas Torzec (2020). \textit{Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph}. The Web Conference.

\bibitem{li20215pu}
Chen Li, Xutan Peng, Yuhang Niu, et al. (2021). \textit{Learning graph attention-aware knowledge graph embedding}. Neurocomputing.

\bibitem{yu2019qgs}
S. Yu, Sujit Rokka Chhetri, A. Canedo, et al. (2019). \textit{Pykg2vec: A Python Library for Knowledge Graph Embedding}. Journal of machine learning research.

\bibitem{fatemi2018e6v}
Bahare Fatemi, Siamak Ravanbakhsh, and D. Poole (2018). \textit{Improved Knowledge Graph Embedding using Background Taxonomic Information}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021i5t}
Zhuo Chen, Mi-Yen Yeh, and Tei-Wei Kuo (2021). \textit{PASSLEAF: A Pool-bAsed Semi-Supervised LEArning Framework for Uncertain Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{dong2022c6z}
Sicong Dong, Xupeng Miao, Peng Liu, et al. (2022). \textit{HET-KG: Communication-Efficient Knowledge Graph Embedding Training via Hotness-Aware Cache}. IEEE International Conference on Data Engineering.

\bibitem{lu20206x1}
Fengyuan Lu, Peijin Cong, and Xinli Huang (2020). \textit{Utilizing Textual Information in Knowledge Graph Embedding: A Survey of Methods and Applications}. IEEE Access.

\bibitem{li2022nr8}
Weidong Li, Rong Peng, and Zhi Li (2022). \textit{Improving knowledge graph completion via increasing embedding interactions}. Applied intelligence (Boston).

\bibitem{luo2015df2}
Yuanfei Luo, Quan Wang, Bin Wang, et al. (2015). \textit{Context-Dependent Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhou20216m0}
Xiaohan Zhou, Yunhui Yi, and Geng Jia (2021). \textit{Path-RotatE: Knowledge Graph Embedding by Relational Rotation of Path in Complex Space}. International Conference on Innovative Computing and Cloud Computing.

\bibitem{zhao202095o}
Feng Zhao, Haoran Sun, Langjunqing Jin, et al. (2020). \textit{Structure-augmented knowledge graph embedding for sparse data with rule learning}. Computer Communications.

\bibitem{jia201870f}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2018). \textit{Path-specific knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{mai2018u0h}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines}. Semdeep/NLIWoD@ISWC.

\bibitem{li201949n}
Dingcheng Li, Siamak Zamani, Jingyuan Zhang, et al. (2019). \textit{Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process}. North American Chapter of the Association for Computational Linguistics.

\bibitem{tang2020ufr}
Xiaoli Tang, Rui Yuan, Qianyu Li, et al. (2020). \textit{Timespan-Aware Dynamic Knowledge Graph Embedding by Incorporating Temporal Evolution}. IEEE Access.

\bibitem{guo2022qtv}
Lingbing Guo, Qiang Zhang, Zequn Sun, et al. (2022). \textit{Understanding and Improving Knowledge Graph Embedding for Entity Alignment}. International Conference on Machine Learning.

\bibitem{jiang202235y}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2022). \textit{Multiview feature augmented neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu201918i}
Yu Liu, Wen Hua, Kexuan Xin, et al. (2019). \textit{Context-Aware Temporal Knowledge Graph Embedding}. WISE.

\bibitem{zhang2020s4x}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2020). \textit{Knowledge graph embedding by translating in time domain space for link prediction}. Knowledge-Based Systems.

\bibitem{chang20179yf}
Liang Chang, Manli Zhu, T. Gu, et al. (2017). \textit{Knowledge Graph Embedding by Dynamic Translation}. IEEE Access.

\bibitem{lee2022hr9}
Yeon-Chang Lee, and Sang-Wook Kim (2022). \textit{THOR: Self-Supervised Temporal Knowledge Graph Embedding via Three-Tower Graph Convolutional Networks}. Industrial Conference on Data Mining.

\bibitem{zhang2022fpm}
Yongqi Zhang, Zhanke Zhou, Quanming Yao, et al. (2022). \textit{Efficient Hyper-parameter Search for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{liu2019e1u}
Chang Liu, Lun Li, Xiaolu Yao, et al. (2019). \textit{A Survey of Recommendation Algorithms Based on Knowledge Graph Embedding}. 2019 IEEE International Conference on Computer Science and Educational Informatization (CSEI).

\bibitem{song2021fnl}
Wei Song, Jingjin Guo, Ruiji Fu, et al. (2021). \textit{A Knowledge Graph Embedding Approach for Metaphor Processing}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{gradgyenge2017xdy}
Lszl Grad-Gyenge, A. Kiss, and P. Filzmoser (2017). \textit{Graph Embedding Based Recommendation Techniques on the Knowledge Graph}. User Modeling, Adaptation, and Personalization.

\bibitem{zhou20218bt}
Xiaofei Zhou, Lingfeng Niu, Qiannan Zhu, et al. (2021). \textit{Knowledge Graph Embedding by Double Limit Scoring Loss}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{chen20210ah}
Yao Chen, Jiangang Liu, Zhe Zhang, et al. (2021). \textit{MbiusE: Knowledge Graph Embedding on Mbius Ring}. Knowledge-Based Systems.

\bibitem{zhang2020i7j}
Yongqi Zhang, Quanming Yao, and Lei Chen (2020). \textit{Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{boschin2020ki4}
Armand Boschin (2020). \textit{TorchKGE: Knowledge Graph Embedding in Python and PyTorch}. arXiv.org.

\bibitem{wang20199fe}
P. Wang, D. Dou, Fangzhao Wu, et al. (2019). \textit{Logic Rules Powered Knowledge Graph Embedding}. arXiv.org.

\bibitem{myklebust201941l}
E. B. Myklebust, Ernesto Jimnez-Ruiz, Jiaoyan Chen, et al. (2019). \textit{Knowledge Graph Embedding for Ecotoxicological Effect Prediction}. International Workshop on the Semantic Web.

\bibitem{kartheek2021aj7}
Miriyala Kartheek, and G. Sajeev (2021). \textit{Building Semantic Based Recommender System Using Knowledge Graph Embedding}. International Conference on Intelligent Information Processing.

\bibitem{sha2019plw}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Attentive Knowledge Graph Embedding for Personalized Recommendation}. arXiv.org.

\bibitem{lu2020x6y}
Haonan Lu, and Hailin Hu (2020). \textit{DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2020c15}
Siheng Zhang, Zhengya Sun, and Wensheng Zhang (2020). \textit{Improve the translational distance models for knowledge graph embedding}. Journal of Intelligence and Information Systems.

\bibitem{li2020ek4}
Mingda Li, Zhengya Sun, Siheng Zhang, et al. (2020). \textit{Enhancing Knowledge Graph Embedding with Relational Constraints}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{li2020he5}
Jian Li, Zhuoming Xu, Yan Tang, et al. (2020). \textit{Deep Hybrid Knowledge Graph Embedding for Top-N Recommendation}. Web Information System and Application Conference.

\bibitem{kim2020zu3}
Kuekyeng Kim, Yuna Hur, Gyeongmin Kim, et al. (2020). \textit{GREG: A Global Level Relation Extraction with Knowledge Graph Embedding}. Applied Sciences.

\bibitem{zhu2018l0u}
Jizhao Zhu, Yantao Jia, Jun Xu, et al. (2018). \textit{Modeling the Correlations of Relations for Knowledge Graph Embedding}. Journal of Computational Science and Technology.

\bibitem{do20184o2}
Kien Do, T. Tran, and S. Venkatesh (2018). \textit{Knowledge Graph Embedding with Multiple Relation Projections}. International Conference on Pattern Recognition.

\bibitem{ma20194ua}
Yunpu Ma, Volker Tresp, Liming Zhao, et al. (2019). \textit{Variational Quantum Circuit Model for Knowledge Graph Embedding}. Advanced Quantum Technologies.

\bibitem{zhang2020wou}
Yuhang Zhang, Jun Wang, and Jie Luo (2020). \textit{Knowledge Graph Embedding Based Collaborative Filtering}. IEEE Access.

\bibitem{zhang2019hs5}
Wen Zhang, Shumin Deng, Han Wang, et al. (2019). \textit{XTransE: Explainable Knowledge Graph Embedding for Link Prediction with Lifestyles in e-Commerce}. Joint International Conference of Semantic Technology.

\bibitem{wang20198d2}
Zhihao Wang, and Xin Li (2019). \textit{Hybrid-TE: Hybrid Translation-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{tran20195x3}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective}. EDBT/ICDT Workshops.

\bibitem{xiong2018fof}
Shengwu Xiong, Weitao Huang, and P. Duan (2018). \textit{Knowledge Graph Embedding via Relation Paths and Dynamic Mapping Matrix}. ER Workshops.

\bibitem{radstok2021yup}
Wessel Radstok, M. Chekol, and M. Schfer (2021). \textit{Are Knowledge Graph Embedding Models Biased, or Is it the Data That They Are Trained on?}. Wikidata@ISWC.

\bibitem{zhao2020o6z}
Ling Zhao, Hanhan Deng, L. Qiu, et al. (2020). \textit{Urban Multi-Source Spatio-Temporal Data Analysis Aware Knowledge Graph Embedding}. Symmetry.

\bibitem{zhang20182ey}
Maoyuan Zhang, Qi Wang, Wukui Xu, et al. (2018). \textit{Discriminative Path-Based Knowledge Graph Embedding for Precise Link Prediction}. European Conference on Information Retrieval.

\bibitem{jia20207dd}
Ningning Jia, Xiang Cheng, and Sen Su (2020). \textit{Improving Knowledge Graph Embedding Using Locally and Globally Attentive Relation Paths}. European Conference on Information Retrieval.

\bibitem{zhu2019ir6}
Qiannan Zhu, Xiaofei Zhou, P. Zhang, et al. (2019). \textit{A neural translating general hyperplane for knowledge graph embedding}. Journal of Computer Science.

\bibitem{wang2021dgy}
Shen Wang, Xiaokai Wei, C. D. Santos, et al. (2021). \textit{Knowledge Graph Representation via Hierarchical Hyperbolic Neural Graph Embedding}. 2021 IEEE International Conference on Big Data (Big Data).

\bibitem{ning20219et}
Zhiyuan Ning, Ziyue Qiao, Hao Dong, et al. (2021). \textit{LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding}. Pacific-Asia Conference on Knowledge Discovery and Data Mining.

\bibitem{sheikh20213qq}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2021). \textit{Knowledge Graph Embedding using Graph Convolutional Networks with Relation-Aware Attention}. arXiv.org.

\bibitem{rim2021s9a}
Wiem Ben Rim, Carolin (Haas) Lawrence, Kiril Gashteovski, et al. (2021). \textit{Behavioral Testing of Knowledge Graph Embedding Models for Link Prediction}. Conference on Automated Knowledge Base Construction.

\bibitem{zhang20179i2}
Chunhong Zhang, Miao Zhou, Xiao Han, et al. (2017). \textit{Knowledge Graph Embedding for Hyper-Relational Data}. Unpublished manuscript.

\bibitem{elebi20182bd}
R. elebi, Erkan Yasar, Hseyin Uyar, et al. (2018). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction using Linked Open Data}. Workshop on Semantic Web Applications and Tools for Life Sciences.

\bibitem{garofalo20185g9}
Martina Garofalo, Maria Angela Pellegrino, Abdulrahman Altabba, et al. (2018). \textit{Leveraging Knowledge Graph Embedding Techniques for Industry 4.0 Use Cases}. arXiv.org.

\bibitem{wang201825m}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2018). \textit{Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network}. arXiv.org.

\bibitem{chung2021u2l}
Chanyoung Chung, and Joyce Jiyoung Whang (2021). \textit{Knowledge Graph Embedding via Metagraph Learning}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{tran2019j42}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space}. International Conference on Theory and Practice of Digital Libraries.

\bibitem{shi2017m2h}
Jun Shi, Huan Gao, G. Qi, et al. (2017). \textit{Knowledge Graph Embedding with Triple Context}. International Conference on Information and Knowledge Management.

\bibitem{zhang2017ixt}
Wen Zhang (2017). \textit{Knowledge Graph Embedding with Diversity of Structures}. The Web Conference.

\bibitem{zhu20196p1}
Ming-Yi Zhu, De-sheng Zhen, Ran Tao, et al. (2019). \textit{Top-N Collaborative Filtering Recommendation Algorithm Based on Knowledge Graph Embedding}. International Conference on Knowledge Management in Organizations.

\bibitem{kertkeidkachorn2019dkn}
Natthawut Kertkeidkachorn, Xin Liu, and R. Ichise (2019). \textit{GTransE: Generalizing Translation-Based Model on Uncertain Knowledge Graph Embedding}. JSAI.

\bibitem{zhu2019zqy}
Jia Zhu, Zetao Zheng, Min Yang, et al. (2019). \textit{A semi-supervised model for knowledge graph embedding}. Data mining and knowledge discovery.

\bibitem{zhang20193g2}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Towards Data Poisoning Attack against Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2019fcs}
Wenqiang Liu, Hongyun Cai, Xu Cheng, et al. (2019). \textit{Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{kanojia20171in}
Vibhor Kanojia, Hideyuki Maeda, Riku Togashi, et al. (2017). \textit{Enhancing Knowledge Graph Embedding with Probabilistic Negative Sampling}. The Web Conference.

\bibitem{gao2018di0}
Huan Gao, Jun Shi, G. Qi, et al. (2018). \textit{Triple Context-Based Knowledge Graph Embedding}. IEEE Access.

\bibitem{mai2018egi}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Support and Centrality: Learning Weights for Knowledge Graph Embedding Models}. International Conference Knowledge Engineering and Knowledge Management.

\bibitem{xiao2016bb9}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2016). \textit{Knowledge Semantic Representation: A Generative Model for Interpretable Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2024q3q}
Peifeng Liu, Lu Qian, Xingwei Zhao, et al. (2024). \textit{Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly}. IEEE Transactions on Industrial Informatics.

\bibitem{zhang2024cjl}
Jin-cheng Zhang, A. Zain, Kai Zhou, et al. (2024). \textit{A review of recommender systems based on knowledge graph embedding}. Expert systems with applications.

\bibitem{su2023v6e}
Xiao-Rui Su, Zhuhong You, Deshuang Huang, et al. (2023). \textit{Biomedical Knowledge Graph Embedding With Capsule Network for Multi-Label Drug-Drug Interaction Prediction}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhu2023bfj}
Xiangrong Zhu, Guang-pu Li, and Wei Hu (2023). \textit{Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning}. The Web Conference.

\bibitem{liu2024to0}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Towards Continual Knowledge Graph Embedding via Incremental Distillation}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2024vgj}
Wei Wang, Xiaoxuan Shen, Baolin Yi, et al. (2024). \textit{Knowledge-aware fine-grained attention networks with refined knowledge graph embedding for personalized recommendation}. Expert systems with applications.

\bibitem{li2024920}
Duantengchuan Li, Tao Xia, Jing Wang, et al. (2024). \textit{SDFormer: A shallow-to-deep feature interaction for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{lee202380l}
Jaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang (2023). \textit{InGram: Inductive Knowledge Graph Embedding via Relation Graphs}. International Conference on Machine Learning.

\bibitem{shokrzadeh2023twj}
Zeinab Shokrzadeh, M. Feizi-Derakhshi, M. Balafar, et al. (2023). \textit{Knowledge graph-based recommendation system enhanced by neural collaborative filtering and knowledge graph embedding}. Ain Shams Engineering Journal.

\bibitem{gao2023086}
Weibo Gao, Hao Wang, Qi Liu, et al. (2023). \textit{Leveraging Transferable Knowledge Concept Graph Embedding for Cold-Start Cognitive Diagnosis}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2024sgp}
Yufeng Li, Wenchao Zhao, Bo Dang, et al. (2024). \textit{Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning}. 2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE).

\bibitem{xue2023qi7}
Zengcan Xue, Zhao Zhang, Hai Liu, et al. (2023). \textit{Learning knowledge graph embedding with multi-granularity relational augmentation network}. Expert systems with applications.

\bibitem{duan2024d3f}
Pengbo Duan, Kuo Yang, Xin Su, et al. (2024). \textit{HTINet2: herbtarget prediction via knowledge graph embedding and residual-like graph neural network}. Briefings Bioinform..

\bibitem{chen20246rm}
Zhen Chen, Dalin Zhang, Shanshan Feng, et al. (2024). \textit{KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2022o32}
Jia Zhu, Changqin Huang, and P. D. Meo (2022). \textit{DFMKE: A dual fusion multi-modal knowledge graph embedding framework for entity alignment}. Information Fusion.

\bibitem{mitropoulou20235t0}
Katerina Mitropoulou, Panagiotis C. Kokkinos, P. Soumplis, et al. (2023). \textit{Anomaly Detection in Cloud Computing using Knowledge Graph Embedding and Machine Learning Mechanisms}. Journal of Grid Computing.

\bibitem{shomer2023imo}
Harry Shomer, Wei Jin, Wentao Wang, et al. (2023). \textit{Toward Degree Bias in Embedding-Based Knowledge Graph Completion}. The Web Conference.

\bibitem{wang202490m}
Mingjie Wang, Zijie Li, Jun Wang, et al. (2024). \textit{TracKGE: Transformer with Relation-pattern Adaptive Contrastive Learning for Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{li2024bl5}
Zhifei Li, Wei Huang, Xuchao Gong, et al. (2024). \textit{Decoupled semantic graph neural network for knowledge graph embedding}. Neurocomputing.

\bibitem{li2024y2a}
Mingqi Li, Wenming Ma, and Zihao Chu (2024). \textit{KGIE: Knowledge graph convolutional network for recommender system with interactive embedding}. Knowledge-Based Systems.

\bibitem{jia2023krv}
Yan Jia, Mengqi Lin, Yechen Wang, et al. (2023). \textit{Extrapolation over temporal knowledge graph via hyperbolic embedding}. CAAI Transactions on Intelligence Technology.

\bibitem{huang2023grx}
Wei Huang, Jia Liu, Tianrui Li, et al. (2023). \textit{FedCKE: Cross-Domain Knowledge Graph Embedding in Federated Learning}. IEEE Transactions on Big Data.

\bibitem{wang2023s70}
Ruoxin Wang, and C. F. Cheung (2023). \textit{Knowledge graph embedding learning system for defect diagnosis in additive manufacturing}. Computers in industry (Print).

\bibitem{hou20237gt}
Xiangning Hou, Ruizhe Ma, Li Yan, et al. (2023). \textit{T-GAE: A Timespan-aware Graph Attention-based Embedding Model for Temporal Knowledge Graph Completion}. Information Sciences.

\bibitem{jiang2023opm}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2023). \textit{Multisource hierarchical neural network for knowledge graph embedding}. Expert systems with applications.

\bibitem{lu2022bwo}
H. Lu, Hailin Hu, and Xiaodong Lin (2022). \textit{DensE: An enhanced non-commutative representation for knowledge graph embedding with adaptive semantic hierarchy}. Neurocomputing.

\bibitem{djeddi2023g71}
W. Djeddi, Khalil Hermi, S. Yahia, et al. (2023). \textit{Advancing drugtarget interaction prediction: a comprehensive graph-based approach integrating knowledge graph embedding and ProtBert pretraining}. BMC Bioinformatics.

\bibitem{zhang20243iw}
Yuchao Zhang, Xiangjie Kong, Zhehui Shen, et al. (2024). \textit{A survey on temporal knowledge graph embedding: Models and applications}. Knowledge-Based Systems.

\bibitem{le2023hjy}
Thanh-Binh Le, Huy Tran, and H. Le (2023). \textit{Knowledge graph embedding with the special orthogonal group in quaternion space for link prediction}. Knowledge-Based Systems.

\bibitem{yao2023y12}
Zhen Yao, Wen Zhang, Mingyang Chen, et al. (2023). \textit{Analogical Inference Enhanced Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{li2023y5q}
Zhipeng Li, Shanshan Feng, Jun Shi, et al. (2023). \textit{Future Event Prediction Based on Temporal Knowledge Graph Embedding}. Computer systems science and engineering.

\bibitem{yang2022j7z}
Shihan Yang, Weiya Zhang, R. Tang, et al. (2022). \textit{Approximate inferring with confidence predicting based on uncertain knowledge graph embedding}. Information Sciences.

\bibitem{banerjee2023fdi}
Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, et al. (2023). \textit{GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering}. Extended Semantic Web Conference.

\bibitem{hu20230kr}
Yuke Hu, Wei Liang, Ruofan Wu, et al. (2023). \textit{Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2023wgg}
Daiyi Li, Li Yan, Xiaowen Zhang, et al. (2023). \textit{EventKGE: Event knowledge graph embedding with event causal transfer}. Knowledge-Based Systems.

\bibitem{hao2022cl4}
Xinkun Hao, Qingfeng Chen, Haiming Pan, et al. (2022). \textit{Enhancing drugdrug interaction prediction by three-way decision and knowledge graph embedding}. Granular Computing.

\bibitem{khan20222j1}
Nasrullah Khan, Z. Ma, Li Yan, et al. (2022). \textit{Hashing-based semantic relevance attributed knowledge graph embedding enhancement for deep probabilistic recommendation}. Applied intelligence (Boston).

\bibitem{le2022ybl}
Thanh-Binh Le, Ngoc Huynh, and Bac Le (2022). \textit{Knowledge graph embedding by projection and rotation on hyperplanes for link prediction}. Applied intelligence (Boston).

\bibitem{liang202338l}
Shuang Liang (2023). \textit{Knowledge Graph Embedding Based on Graph Neural Network}. IEEE International Conference on Data Engineering.

\bibitem{khan2022ipv}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{DCA-IoMT: Knowledge-Graph-Embedding-Enhanced Deep Collaborative Alert Recommendation Against COVID-19}. IEEE Transactions on Industrial Informatics.

\bibitem{he2022e37}
Peng He, Gang Zhou, Mengli Zhang, et al. (2022). \textit{Improving temporal knowledge graph embedding using tensor factorization}. Applied intelligence (Boston).

\bibitem{shen2022d5j}
Linshan Shen, Rongbo He, and Shaobin Huang (2022). \textit{Entity alignment with adaptive margin learning knowledge graph embedding}. Data & Knowledge Engineering.

\bibitem{di20210ib}
Shimin Di, Quanming Yao, Yongqi Zhang, et al. (2021). \textit{Efficient Relation-aware Scoring Function Search for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{niu2020uyy}
Guanglin Niu, Bo Li, Yongfei Zhang, et al. (2020). \textit{AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding}. Findings.

\bibitem{nie2023ejz}
H. Nie, Xiangguo Zhao, Xin Bi, et al. (2023). \textit{Correlation embedding learning with dynamic semantic enhanced sampling for knowledge graph completion}. World wide web (Bussum).

\bibitem{li2022du0}
Jiayi Li, and Yujiu Yang (2022). \textit{STaR: Knowledge Graph Embedding by Scaling, Translation and Rotation}. Autonomous Infrastructure, Management and Security.

\bibitem{daruna2022dmk}
A. Daruna, Devleena Das, and S. Chernova (2022). \textit{Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions}. IEEE/RJS International Conference on Intelligent RObots and Systems.

\bibitem{zhou20210ma}
Xing-Chun Zhou, Peng Wang, Qi Luo, et al. (2021). \textit{Multi-hop Knowledge Graph Reasoning Based on Hyperbolic Knowledge Graph Embedding and Reinforcement Learning}. IJCKG.

\bibitem{kun202384f}
Kong Wei Kun, Xin Liu, Teeradaj Racharak, et al. (2023). \textit{WeExt: A Framework of Extending Deterministic Knowledge Graph Embedding Models for Embedding Weighted Knowledge Graphs}. IEEE Access.

\bibitem{dong2022taz}
Yao Dong, Lei Wang, Ji Xiang, et al. (2022). \textit{RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space}. International Conference on Computational Linguistics.

\bibitem{kamigaito20218jz}
Hidetaka Kamigaito, and Katsuhiko Hayashi (2021). \textit{Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{krause2022th0}
Franziska Krause (2022). \textit{Dynamic Knowledge Graph Embeddings via Local Embedding Reconstructions}. Extended Semantic Web Conference.

\bibitem{zhang20213h6}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2021). \textit{Knowledge graph embedding with shared latent semantic units}. Neural Networks.

\bibitem{li2021tm6}
Guang-pu Li, Zequn Sun, Lei Qian, et al. (2021). \textit{Rule-based data augmentation for knowledge graph embedding}. AI Open.

\bibitem{wang2020au0}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2020). \textit{Enhancing knowledge graph embedding by composite neighbors for link prediction}. Computing.

\bibitem{wei20215a7}
Yuyang Wei, Wei Chen, Zhixu Li, et al. (2021). \textit{Incremental Update of Knowledge Graph Embedding by Rotating on Hyperplanes}. 2021 IEEE International Conference on Web Services (ICWS).

\bibitem{zhang2021rjh}
Yongqi Zhang, Quanming Yao, and Lei Chen (2021). \textit{Simple and automated negative sampling for knowledge graph embedding}. The VLDB journal.

\bibitem{sheikh202245c}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2022). \textit{Scaling knowledge graph embedding models for link prediction}. EuroMLSys@EuroSys.

\bibitem{ren2021muc}
Chao Ren, Le Zhang, Lintao Fang, et al. (2021). \textit{Ontological Concept Structure Aware Knowledge Transfer for Inductive Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{eyharabide2021wx4}
Victoria Eyharabide, I. E. I. Bekkouch, and Nicolae Drago Constantin (2021). \textit{Knowledge Graph Embedding-Based Domain Adaptation for Musical Instrument Recognition}. De Computis.

\bibitem{hong2020hyg}
Y. Hong, Chenyang Bu, and Tingting Jiang (2020). \textit{Rule-enhanced Noisy Knowledge Graph Embedding via Low-quality Error Detection}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{huang2020sqc}
Yan Huang, Haili Sun, Xu Ke, et al. (2020). \textit{CoRelatE: Learning the correlation in multi-fold relations for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{kurokawa2021f4f}
M. Kurokawa (2021). \textit{Explainable Knowledge Reasoning Framework Using Multiple Knowledge Graph Embedding}. IJCKG.

\bibitem{mohamed2021dwg}
Sameh K. Mohamed, Emir Muoz, and V. Novek (2021). \textit{On Training Knowledge Graph Embedding Models}. Inf..

\bibitem{gebhart2021gtp}
Thomas Gebhart, J. Hansen, and Paul Schrater (2021). \textit{Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding}. International Conference on Artificial Intelligence and Statistics.

\bibitem{deng2024643}
Weibin Deng, Yiteng Zhang, Hong Yu, et al. (2024). \textit{Knowledge graph embedding based on dynamic adaptive atrous convolution and attention mechanism for link prediction}. Information Processing & Management.

\bibitem{liu2024zr9}
Jin Liu, Hao Du, R. Guo, et al. (2024). \textit{MMGK: Multimodality Multiview Graph Representations and Knowledge Embedding for Mild Cognitive Impairment Diagnosis}. IEEE Transactions on Computational Social Systems.

\bibitem{zhang2024zmq}
Chengcheng Zhang, Tianyi Zang, and Tianyi Zhao (2024). \textit{KGE-UNIT: toward the unification of molecular interactions prediction based on knowledge graph and multi-task learning on drug discovery}. Briefings Bioinform..

\bibitem{he2024vks}
Mingsheng He, Lin Zhu, and Luyi Bai (2024). \textit{ConvTKG: A query-aware convolutional neural network-based embedding model for temporal knowledge graph completion}. Neurocomputing.

\bibitem{zhang2024fy0}
Dong Zhang, Zhe Rong, Chengyuan Xue, et al. (2024). \textit{SimRE: Simple contrastive learning with soft logical rule for knowledge graph embedding}. Information Sciences.

\bibitem{zhang2024ivc}
Dong Zhang, Wenlong Feng, Zonghang Wu, et al. (2024). \textit{CDRGN-SDE: Cross-Dimensional Recurrent Graph Network with neural Stochastic Differential Equation for temporal knowledge graph embedding}. Expert systems with applications.

\bibitem{jing2024nxw}
Yanzhen Jing, Guanghui Zhou, Chao Zhang, et al. (2024). \textit{XMKR: Explainable manufacturing knowledge recommendation for collaborative design with graph embedding learning}. Advanced Engineering Informatics.

\bibitem{jiang2024zlc}
Pengcheng Jiang, Lang Cao, Cao Xiao, et al. (2024). \textit{KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge}. Neural Information Processing Systems.

\bibitem{han2024u0t}
Zhulin Han, and Jian Wang (2024). \textit{Knowledge enhanced graph inference network based entity-relation extraction and knowledge graph construction for industrial domain}. Frontiers of Engineering Management.

\bibitem{quan2024o2a}
Huafeng Quan, Yiting Li, Dashuai Liu, et al. (2024). \textit{Protection of Guizhou Miao batik culture based on knowledge graph and deep learning}. Heritage Science.

\bibitem{liu2024tc2}
Bufan Liu, Chun-Hsien Chen, and Zuoxu Wang (2024). \textit{A multi-hierarchical aggregation-based graph convolutional network for industrial knowledge graph embedding towards cognitive intelligent manufacturing}. Journal of manufacturing systems.

\bibitem{hello2024hgf}
Nour Hello, P. Lorenzo, and E. Strinati (2024). \textit{Semantic Communication Enhanced by Knowledge Graph Representation Learning}. International Workshop on Signal Processing Advances in Wireless Communications.

\bibitem{li2024z0e}
Jinpeng Li, Hang Yu, Xiangfeng Luo, et al. (2024). \textit{COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion}. North American Chapter of the Association for Computational Linguistics.

\bibitem{yan2024joa}
Qun Yan, Juan Zhao, Linfu Xue, et al. (2024). \textit{Mineral Prospectivity Mapping Based on Spatial Feature Classification with Geological Map Knowledge Graph Embedding: Case Study of Gold Ore Prediction at Wulonggou, Qinghai Province (Western China)}. Natural Resources Research.

\bibitem{liu2024tn0}
Jhih-Chen Liu, Chiao-Ting Chen, Chi Lee, et al. (2024). \textit{Evolving Knowledge Graph Representation Learning with Multiple Attention Strategies for Citation Recommendation System}. ACM Transactions on Intelligent Systems and Technology.

\bibitem{wang20245dw}
Chuanghui Wang, Yunqing Yang, Jinshuai Song, et al. (2024). \textit{Research Progresses and Applications of Knowledge Graph Embedding Technique in Chemistry}. Journal of Chemical Information and Modeling.

\bibitem{long2024soi}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{KGDM: A Diffusion Model to Capture Multiple Relation Semantics for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2024ayq}
Qihui Zhou, Peiqi Yin, Xiao Yan, et al. (2024). \textit{Atom: An Efficient Query Serving System for Embedding-based Knowledge Graph Reasoning with Operator-level Batching}. Proc. ACM Manag. Data.

\bibitem{huang2024t19}
Chen Huang, Deshan Chen, Tengze Fan, et al. (2024). \textit{Incorporating environmental knowledge embedding and spatial-temporal graph attention networks for inland vessel traffic flow prediction}. Engineering applications of artificial intelligence.

\bibitem{lu2024fsd}
Ming Lu, Yancong Li, Jiangxiao Zhang, et al. (2024). \textit{Deep hyperbolic convolutional model for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2024yar}
Qi Liu, Qinghua Zhang, Fan Zhao, et al. (2024). \textit{Uncertain knowledge graph embedding: an effective method combining multi-relation and multi-path}. Frontiers Comput. Sci..

\bibitem{khan20242y2}
Nasrullah Khan, Zongmin Ma, Ruizhe Ma, et al. (2024). \textit{Continual knowledge graph embedding enhancement for joint interaction-based next click recommendation}. Knowledge-Based Systems.

\bibitem{xue2025ee8}
Zengcan Xue, Zhaoli Zhang, Hai Liu, et al. (2025). \textit{MHRN: A multi-perspective hierarchical relation network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{long20248vt}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{Fact Embedding through Diffusion Model for Knowledge Graph Completion}. The Web Conference.

\bibitem{huang20240su}
Chen Huang, Fei Yu, Zhiguo Wan, et al. (2024). \textit{Knowledge graph confidence-aware embedding for recommendation}. Neural Networks.

\bibitem{wang2024nej}
Yuzhuo Wang, Hongzhi Wang, Xianglong Liu, et al. (2024). \textit{GFedKG: GNN-based federated embedding model for knowledge graph completion}. Knowledge-Based Systems.

\bibitem{wang2024c8z}
Xinyan Wang, Kuo Yang, Ting Jia, et al. (2024). \textit{KDGene: knowledge graph completion for disease gene prediction using interactional tensor decomposition}. Briefings Bioinform..

\bibitem{liu2024x0k}
Yuhan Liu, Zelin Cao, Xing Gao, et al. (2024). \textit{Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport}. The Web Conference.

\bibitem{li2024uio}
Yongfang Li, and Chunhua Zhu (2024). \textit{TransE-MTP: A New Representation Learning Method for Knowledge Graph Embedding with Multi-Translation Principles and TransE}. Electronics.

\bibitem{zhang2024z78}
Qianjin Zhang, and Yandan Xu (2024). \textit{Knowledge graph embedding with inverse function representation for link prediction}. Engineering applications of artificial intelligence.

\bibitem{wang2024534}
Hao Wang, Dandan Song, Zhijing Wu, et al. (2024). \textit{A collaborative learning framework for knowledge graph embedding and reasoning}. Knowledge-Based Systems.

\bibitem{ni202438q}
Shengkun Ni, Xiangtai Kong, Yingying Zhang, et al. (2024). \textit{Identifying compound-protein interactions with knowledge graph embedding of perturbation transcriptomics}. Cell Genomics.

\bibitem{nie202499i}
Jixuan Nie, Xia Hou, Wenfeng Song, et al. (2024). \textit{Knowledge Graph Efficient Construction: Embedding Chain-of-Thought into LLMs}. VLDB Workshops.

\bibitem{wang2024d52}
Jingchao Wang, Weimin Li, Fangfang Liu, et al. (2024). \textit{ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion}. Expert systems with applications.

\bibitem{mao2024v2s}
Yuren Mao, Yu Hao, Xin Cao, et al. (2024). \textit{Dynamic Graph Embedding via Meta-Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{jafarzadeh202468v}
Parastoo Jafarzadeh, F. Ensan, Mahdiyar Ali Akbar Alavi, et al. (2024). \textit{A Knowledge Graph Embedding Model for Answering Factoid Entity Questions}. ACM Trans. Inf. Syst..

\bibitem{wang2024dea}
Yalin Wang, Yubin Peng, and Jingyu Guo (2024). \textit{Enhancing knowledge graph embedding with structure and semantic features}. Applied intelligence (Boston).

\bibitem{lu202436n}
Yuhuan Lu, Weijian Yu, Xin Jing, et al. (2024). \textit{HyperCL: A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{han2024gaq}
Yadan Han, Guangquan Lu, Shichao Zhang, et al. (2024). \textit{A Temporal Knowledge Graph Embedding Model Based on Variable Translation}. Tsinghua Science and Technology.

\bibitem{liu2024jz8}
Bingchen Liu, Shifu Hou, Weiyi Zhong, et al. (2024). \textit{Enhancing Temporal Knowledge Graph Alignment in News Domain With Box Embedding}. IEEE Transactions on Computational Social Systems.

\bibitem{he2024y6o}
Yunjie He, Daniel Hernndez, M. Nayyeri, et al. (2024). \textit{Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning}. Unpublished manuscript.

\bibitem{fang20243a4}
Yan Fang, Xiaodong Liu, Wei Lu, et al. (2024). \textit{Knowledge graph completion with low-dimensional gated hierarchical hyperbolic embedding}. Knowledge-Based Systems.

\bibitem{zhang2024h9k}
Mingtao Zhang, Guoli Yang, Yi Liu, et al. (2024). \textit{Knowledge graph accuracy evaluation: an LLM-enhanced embedding approach}. International Journal of Data Science and Analysis.

\bibitem{li2024wyh}
Yicong Li, Yu Yang, Jiannong Cao, et al. (2024). \textit{Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach}. Knowledge Discovery and Data Mining.

\bibitem{dong2024ijo}
Dibo Dong, Shangwei Wang, Qiaoying Guo, et al. (2024). \textit{Short-Term Marine Wind Speed Forecasting Based on Dynamic Graph Embedding and Spatiotemporal Information}. Journal of Marine Science and Engineering.

\bibitem{wang20246c7}
Tao Wang, Bo Shen, Jinglin Zhang, et al. (2024). \textit{Knowledge Graph Embedding via Triplet Component Interactions}. Neural Processing Letters.

\bibitem{zhang2024yjo}
Pengfei Zhang, Xiaoxue Zhang, Yang Fang, et al. (2024). \textit{Knowledge Graph Embedding for Hierarchical Entities Based on Auto-Embedding Size}. Mathematics.

\bibitem{liang20247wv}
K. Liang, Yue Liu, Hao Li, et al. (2024). \textit{Clustering then Propagation: Select Better Anchors for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{liu2024t05}
Qi Liu, Yuanyuan Jin, Xuefei Cao, et al. (2024). \textit{An Entity Ontology-Based Knowledge Graph Embedding Approach to News Credibility Assessment}. IEEE Transactions on Computational Social Systems.

\bibitem{pham20243mh}
H. V. Pham, Trung Tuan Nguyen, Luu Minh Tuan, et al. (2024). \textit{IDGCN: A Proposed Knowledge Graph Embedding With Graph Convolution Network For Context-Aware Recommendation Systems}. Journal of Organizational Computing and Electronic Commerce.

\bibitem{li2024gar}
Yu Li, Zhu-Hong You, Shu-Min Wang, et al. (2024). \textit{Attention-Based Learning for Predicting Drug-Drug Interactions in Knowledge Graph Embedding Based on Multisource Fusion Information}. International Journal of Intelligent Systems.

\bibitem{li2024nje}
Nan Li, Zhihao Yang, Jian Wang, et al. (2024). \textit{Drugtarget interaction prediction using knowledge graph embedding}. iScience.

\bibitem{bao20249xp}
Liming Bao, Yan Wang, Xiaoyu Song, et al. (2024). \textit{HGCGE: hyperbolic graph convolutional networks-based knowledge graph embedding for link prediction}. Knowledge and Information Systems.

\bibitem{xu2024fto}
Guoshun Xu, Guozheng Rao, Li Zhang, et al. (2024). \textit{Entity-relation aggregation mechanism graph neural network for knowledge graph embedding}. Applied intelligence (Boston).

\bibitem{liang2024z0q}
Qiuyu Liang, Weihua Wang, Jie Yu, et al. (2024). \textit{Effective Knowledge Graph Embedding with Quaternion Convolutional Networks}. Natural Language Processing and Chinese Computing.

\bibitem{liu2024ixy}
Jie Liu, Lizheng Zu, Yunbin Yan, et al. (2024). \textit{Multi-Filter soft shrinkage network for knowledge graph embedding}. Expert systems with applications.

\bibitem{dong2025l9k}
Jie Dong, Cuiping Chen, Chi Zhang, et al. (2025). \textit{Knowledge Graph Embedding With Graph Convolutional Network and Bidirectional Gated Recurrent Unit for Fault Diagnosis of Industrial Processes}. IEEE Sensors Journal.

\bibitem{zhang2025ebv}
Sensen Zhang, Xun Liang, Simin Niu, et al. (2025). \textit{Integrating Large Language Models and Mbius Group Transformations for Temporal Knowledge Graph Embedding on the Riemann Sphere}. AAAI Conference on Artificial Intelligence.

\bibitem{liu20242zm}
Xinyue Liu, Jianan Zhang, Chi Ma, et al. (2024). \textit{Temporal Knowledge Graph Reasoning with Dynamic Hypergraph Embedding}. International Conference on Language Resources and Evaluation.

\bibitem{yang2024lwa}
Ruiyi Yang, Flora D. Salim, and Hao Xue (2024). \textit{SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding}. The Web Conference.

\bibitem{li20246qx}
Bo Li, Haowei Quan, Jiawei Wang, et al. (2024). \textit{Neural Library Recommendation by Embedding Project-Library Knowledge Graph}. IEEE Transactions on Software Engineering.

\bibitem{liu2024mji}
Xiaojian Liu, Xinwei Guo, and Wen Gu (2024). \textit{SecKG2vec: A novel security knowledge graph relational reasoning method based on semantic and structural fusion embedding}. Computers & security.

\bibitem{chen2024efo}
Bin Chen, Hongyi Li, Di Zhao, et al. (2024). \textit{Quality assessment of cyber threat intelligence knowledge graph based on adaptive joining of embedding model}. Complex &amp; Intelligent Systems.

\bibitem{chen2024uld}
Deng Chen, Weiwei Zhang, and Zuohua Ding (2024). \textit{Embedding dynamic graph attention mechanism into Clinical Knowledge Graph for enhanced diagnostic accuracy}. Expert systems with applications.

\bibitem{wang2017zm5}
Quan Wang, Zhendong Mao, Bin Wang, et al. (2017). \textit{Knowledge Graph Embedding: A Survey of Approaches and Applications}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{li2021qr0}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks}. IEEE Transactions on Neural Networks and Learning Systems.

\end{thebibliography}

\end{document}