\section*{3. Advancing Expressiveness: Rotational, Compound, and Higher-Dimensional Geometries}

Building upon the foundational translational models, the field of Knowledge Graph Embedding (KGE) rapidly progressed to explore more sophisticated geometric and algebraic approaches, driven by the inherent limitations of simple vector addition and projection. The "arms race" for enhanced expressiveness necessitated models capable of capturing intricate relational patterns, such as symmetry, antisymmetry, and compositionality, which proved challenging for TransE-like models \cite{asmara2023}. This section delves into these advanced paradigms, beginning with rotational models that redefine relations as rotations in complex or quaternion spaces. It then extends to compound operations, which synergistically combine multiple geometric transformations, and culminates in a significant shift towards non-Euclidean geometries, including hyperbolic and hyperspherical spaces, and the emerging concept of multi-curvature embeddings. These innovations collectively aim to provide a more faithful and nuanced representation of the diverse and often complex topological characteristics found in real-world knowledge graphs, moving beyond the Euclidean straight-line assumptions to model hierarchical structures and cyclic patterns more effectively.

### 3.1. Relational Rotations in Complex and Quaternion Spaces

The limitations of translational models in capturing complex relational patterns, particularly symmetry, antisymmetry, and composition, became a significant bottleneck. For instance, a symmetric relation $(A, \text{sibling}, B)$ implies $(B, \text{sibling}, A)$, which is difficult to model with a single translation vector $\mathbf{r}$ (i.e., $\mathbf{a} + \mathbf{r} \approx \mathbf{b}$ and $\mathbf{b} + \mathbf{r} \approx \mathbf{a}$ implies $\mathbf{r} \approx -\mathbf{r}$, forcing $\mathbf{r} \approx \mathbf{0}$). This theoretical gap spurred the development of rotational models, which leverage the properties of complex numbers or quaternions to represent relations as rotations.

**RotatE (Knowledge Graph Embedding by Relational Rotation in Complex Space)** \cite{sun2018} emerged as a seminal work in this direction. It models entities as vectors in a complex vector space $\mathbb{C}^d$ and relations as element-wise rotations. For a triple $(h, r, t)$, RotatE proposes that $\mathbf{h} \circ \mathbf{r} \approx \mathbf{t}$, where $\circ$ denotes the Hadamard (element-wise) product, and each component of the relation vector $\mathbf{r}$ is a complex number with a modulus of 1 (i.e., $r_i = e^{i\theta_i}$). The scoring function is typically $f(h,r,t) = ||\mathbf{h} \circ \mathbf{r} - \mathbf{t}||$.
*   **Strengths:** RotatE elegantly addresses symmetry, antisymmetry, and compositionality. A symmetric relation can be modeled by a rotation of $\pi$ (i.e., $e^{i\pi} = -1$), such that $\mathbf{h} \circ (-1) \approx \mathbf{t}$ implies $\mathbf{t} \circ (-1) \approx \mathbf{h}$. Antisymmetry is naturally captured by distinct rotation angles. Crucially, compositionality (if $(A, r_1, B)$ and $(B, r_2, C)$ implies $(A, r_3, C)$) is modeled by $\mathbf{r}_3 \approx \mathbf{r}_1 \circ \mathbf{r}_2$, as rotations are compositional. This provides a more theoretically grounded and expressive framework for these properties than translational models.
*   **Weaknesses:** While powerful, RotatE operates in complex Euclidean space, which might still struggle with highly hierarchical or non-Euclidean structural patterns. The assumption of element-wise rotation might be too restrictive for some complex interactions. The model's performance can also be sensitive to the initialization of complex embeddings and the choice of negative sampling strategies \cite{lloyd2022, shan2018}.

Following RotatE, several extensions explored variations of rotational transformations. **Rotate3D** \cite{gao2020} extended the concept to 3D Euclidean space, representing relations as rotations around axes, while **TeRo (Time-aware Knowledge Graph Embedding via Temporal Rotation)** \cite{xu2020} and **ChronoR (Rotation Based Temporal Knowledge Graph Embedding)** \cite{sadeghian2021} incorporated time into the rotational framework, allowing relation embeddings to evolve as temporal rotations. Other works like \cite{huang2021u42, wang20213kg, le2022ji8, wei20215a7, le2022ybl} further explored relational and entity rotations, sometimes on hyperplanes, to enhance expressiveness. **Path-RotatE** \cite{zhou20216m0} extended rotational operations to paths in complex space, allowing for more complex multi-hop reasoning. **RotateCT (Rotation and Coordinate Transformation in Complex Space)** \cite{dong2022taz} combined rotation with coordinate transformations for greater flexibility. These models collectively demonstrate the versatility of rotation as a fundamental geometric operation for KGE.

A natural progression from complex numbers is to **quaternions**, which offer a higher-dimensional algebraic structure (hypercomplex numbers) that can represent 3D rotations more naturally and without gimbal lock issues. **Quaternion Knowledge Graph Embedding** \cite{zhang2019rlm} proposed representing entities and relations as quaternions, where relations act as quaternion multiplications. This allows for more expressive rotations and richer interactions than complex numbers.
*   **Strengths:** Quaternion embeddings can capture more intricate geometric transformations and potentially model higher-order interactions due to their non-commutative multiplication. They offer a compact representation for 3D rotations, which can be beneficial for certain types of relational semantics. Recent works like **Contextualized Quaternion Embedding** \cite{chen2025}, **Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding** \cite{ji2024}, and models leveraging the **special orthogonal group in quaternion space** \cite{le2023hjy, liang2024z0q} further underscore their potential for complex and temporal KGE.
*   **Weaknesses:** Quaternions introduce increased computational complexity and parameter count compared to complex numbers. Their non-commutative nature can make interpretation more challenging. The theoretical justification for *why* quaternion algebra is universally optimal for all types of KG relations is still an active area of research, and their benefits might be more pronounced for specific types of relational structures.

The shift to rotational models, particularly RotatE, marked a significant advancement by providing a principled geometric interpretation for fundamental relational properties. However, these models still largely operate within Euclidean or complex Euclidean spaces. While quaternions offer a richer algebraic structure, the core challenge remains: how to effectively model relations that are not purely rotational, or structures that do not conform to Euclidean geometry.

### 3.2. Compound Geometric Transformations

While rotational models significantly enhanced expressiveness, the inherent complexity of real-world knowledge graphs often demands a combination of geometric operations. This led to the development of models that employ **compound geometric transformations**, synergistically combining translation, rotation, and scaling to achieve a more nuanced and flexible representation of relational patterns. This approach implicitly acknowledges that no single geometric operation is sufficient to capture the full spectrum of relational semantics.

**CompoundE (Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations)** \cite{ge2022} is a prime example of this paradigm. It proposes a scoring function that integrates translation, rotation, and scaling operations. For a triple $(h, r, t)$, CompoundE models the relation $r$ as a sequence of these transformations applied to the head entity $h$ to approximate the tail entity $t$. Specifically, it might involve scaling the head entity embedding, then rotating it, and finally translating it.
*   **Strengths:** By combining these fundamental operations, CompoundE offers significantly enhanced expressiveness compared to models relying on a single operation. Translation captures the "difference" aspect of relations (like TransE), rotation handles symmetry and compositionality (like RotatE), and scaling allows for modeling hierarchical or magnitude-based relationships (e.g., "is_larger_than"). This multi-faceted approach provides greater flexibility in fitting diverse relational patterns.
*   **Weaknesses:** The increased complexity of compound operations comes with several trade-offs. The model has a higher parameter count, leading to increased computational cost during training and inference. More critically, the interpretability of the learned embeddings can diminish, as disentangling the individual contributions of translation, rotation, and scaling for a given relation becomes challenging. Hyperparameter tuning for such models is also more intricate, as the interplay between different transformation components needs careful optimization \cite{lloyd2022}.

Further extending this concept, **CompoundE3D (Knowledge Graph Embedding with 3D Compound Geometric Transformations)** \cite{ge2023} explores these compound operations in a 3D embedding space, potentially offering richer spatial transformations. Similarly, **STaR (Knowledge Graph Embedding by Scaling, Translation and Rotation)** \cite{li2022du0} explicitly combines these three operations, demonstrating their collective power. These models highlight a key evolutionary trend: as KGE models strive for greater accuracy, they often move towards more complex, composite operations, reflecting the "arms race" dynamic where each new model attempts to capture a broader range of relational nuances.

The methodological limitation of these compound models often lies in their increased parameterization and potential for overfitting, especially on sparse knowledge graphs. While they theoretically offer greater flexibility, effectively learning and disentangling these multiple transformations from limited observed triples remains a challenge. The experimental setups for such models typically involve extensive hyperparameter searches, and their generalizability to vastly different KG structures without re-tuning is an open question. The core assumption is that relations can be adequately represented as a combination of these basic transformations, which may not hold for all abstract or highly contextual relations. Despite these challenges, compound geometric transformations represent a powerful step towards building KGE models that can adapt to the multifaceted nature of knowledge.

### 3.3. Exploring Non-Euclidean and Multi-Curvature Embedding Spaces

A significant paradigm shift in KGE has been the exploration of **non-Euclidean geometries**, moving beyond the flat, zero-curvature assumption of traditional vector spaces. This shift is motivated by the observation that many real-world knowledge graphs exhibit inherent hierarchical or tree-like structures that are poorly represented in Euclidean space. In Euclidean space, distances grow linearly, making it inefficient to embed hierarchical data where exponentially more space is needed at lower levels of the hierarchy.

**Hyperbolic spaces** have emerged as particularly adept at representing hierarchical structures. In hyperbolic geometry, space expands exponentially, allowing for a more faithful embedding of tree-like graphs where nodes at lower levels of the hierarchy can be placed far apart while maintaining short path distances to their ancestors. **Hyperbolic Hierarchy-Aware Knowledge Graph Embedding** \cite{pan2021} and **Hierarchical-aware relation rotational knowledge graph embedding** \cite{wang20213kg} were among the early works to leverage hyperbolic embeddings for KGE, demonstrating superior performance in capturing hierarchical relations. Subsequent research, such as **Hierarchical Hyperbolic Neural Graph Embedding** \cite{wang2021dgy}, **Deep hyperbolic convolutional model** \cite{lu2024fsd}, **HyperCL (A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology)** \cite{lu202436n}, **Hierarchical hyperbolic embedding** \cite{fang20243a4}, and **HGCGE (hyperbolic graph convolutional networks-based knowledge graph embedding)** \cite{bao20249xp}, has further solidified the utility of hyperbolic spaces for hierarchical KGs. These models often integrate hyperbolic geometry into existing KGE frameworks or Graph Neural Networks (GNNs) to enhance their ability to model complex graph structures, including for temporal KGs \cite{jia2023krv}. **Fully Hyperbolic Rotation for Knowledge Graph Embedding** \cite{liang2024} even combines rotational operations within hyperbolic space, aiming to capture both hierarchical and compositional properties.
*   **Strengths:** Hyperbolic embeddings naturally capture hierarchical relationships with significantly fewer dimensions than their Euclidean counterparts, leading to more compact and accurate representations. They excel at modeling power-law distributions often observed in real-world graphs.
*   **Weaknesses:** Hyperbolic geometry introduces computational challenges, as operations like distance calculation are more complex than in Euclidean space. The choice of curvature is a critical hyperparameter, and a single global curvature might not be optimal for all parts of a heterogeneous KG. The intuition behind hyperbolic embeddings is less straightforward than Euclidean spaces, potentially hindering model interpretability.

Beyond hyperbolic spaces, other non-Euclidean geometries have been explored. **Hyperspherical spaces**, which have positive curvature, are suitable for modeling cyclic or periodic patterns, or for embedding data where angular relationships are more important than linear distances. **SpherE (Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval)** \cite{li2024} is an example of a hyperspherical embedding model. **MöbiusE (Knowledge Graph Embedding on Möbius Ring)** \cite{chen20210ah} and approaches leveraging **Möbius Group Transformations for Temporal Knowledge Graph Embedding on the Riemann Sphere** \cite{zhang2025ebv} explore geometries that can capture complex topological patterns, including cyclic and periodic relationships, which are challenging for Euclidean models. These spaces offer unique advantages for specific data characteristics.

A cutting-edge development is **multi-curvature embedding**, which addresses the limitation of assuming a single, global curvature for an entire knowledge graph. Real-world KGs are often heterogeneous, containing both hierarchical (hyperbolic) and cyclic/Euclidean substructures. Models like **MADE (Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion)** \cite{wang2024} and **IME (Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion)** \cite{wang2024} propose to adaptively learn local curvatures for different parts of the graph or even for different entities and relations.
*   **Strengths:** Multi-curvature embeddings offer unparalleled flexibility by allowing the embedding space's geometry to adapt to the local structure of the knowledge graph. This provides a more faithful representation of diverse topological patterns, potentially overcoming the "one-size-fits-all" limitation of single-geometry embeddings. They are particularly promising for complex, real-world KGs that combine various structural motifs.
*   **Weaknesses:** The primary challenge with multi-curvature models is their increased complexity in terms of model design, optimization, and parameterization. Learning optimal local curvatures is a non-trivial task, often requiring sophisticated optimization techniques. The computational overhead can be substantial, and the theoretical guarantees for such adaptive geometries are still under active investigation. Furthermore, the generalizability of learned curvatures across different datasets or even within different domains of a single large KG remains an open question.

The exploration of non-Euclidean and multi-curvature embedding spaces represents a profound evolution in KGE, moving beyond the simplistic assumptions of Euclidean geometry to embrace the intrinsic topological complexity of knowledge graphs. This direction promises more accurate, compact, and semantically rich embeddings, particularly for tasks involving hierarchical reasoning or complex relational patterns. However, it also introduces significant methodological hurdles related to computational efficiency, interpretability, and robust optimization, which are critical areas for future research.