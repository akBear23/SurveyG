\section*{2. Foundational Concepts and Early Geometric Models}

The advent of Knowledge Graph Embedding (KGE) marked a pivotal shift in how Knowledge Graphs (KGs) are processed and leveraged within Artificial Intelligence. This paradigm fundamentally transforms symbolic entities and relations into continuous, low-dimensional vector representations, often referred to as embeddings \cite{wang2017zm5, dai2020, cao2022, ge2023}. This transformation addresses critical challenges inherent in symbolic KGs, such as computational inefficiency, scalability limitations, and the difficulty of integrating discrete knowledge with statistical machine learning models. The core objective of KGE is to learn these embeddings such that the structural and semantic properties of the KG are preserved in the vector space, allowing for arithmetic operations to infer new facts or assess the plausibility of existing ones. This section lays the groundwork by elucidating the core principles of KGE, then delves into the pioneering 'translational distance models' that established the initial geometric paradigm, and finally explores their immediate extensions, highlighting the continuous drive for enhanced representational power in the field. These early models, while foundational, revealed inherent limitations that spurred subsequent research into more expressive and flexible embedding architectures, initiating what can be described as an "arms race" in KGE development.

### 2.1. Core Principles of Knowledge Graph Embedding

At its heart, Knowledge Graph Embedding (KGE) is the process of mapping entities (nodes) and relations (edges) within a Knowledge Graph into a continuous vector space, typically $\mathbb{R}^d$, where $d$ is the embedding dimension \cite{wang2017zm5, cao2022}. Each entity $e$ is represented by a vector $\mathbf{e} \in \mathbb{R}^d$, and each relation $r$ by a vector $\mathbf{r} \in \mathbb{R}^d$ or a matrix $\mathbf{M}_r \in \mathbb{R}^{d \times d}$. The fundamental goal is to capture the semantic and structural information of the KG such that plausible facts (triples $(h, r, t)$) are assigned high scores (or low "energy"), while implausible or false facts receive low scores (or high energy). This is achieved through a scoring function $f(h, r, t)$ that quantifies the plausibility of a triple based on the embeddings of its head entity $h$, relation $r$, and tail entity $t$. The design of this scoring function is central to distinguishing different KGE models, as it dictates how relational patterns are geometrically or semantically interpreted in the embedding space.

The training of KGE models typically involves minimizing a loss function that encourages positive triples to have better scores than negative (false) triples. A common approach is margin-based ranking loss, where the score of a positive triple $(h, r, t)$ is pushed below that of a corrupted negative triple $(h', r, t')$ or $(h, r, t'')$ by a certain margin $\gamma$. Negative sampling strategies, such as uniform negative sampling or more advanced techniques like $\epsilon$-truncated uniform negative sampling \cite{sun2018} and confidence-aware negative sampling \cite{shan2018}, are crucial for generating informative negative examples that challenge the model to learn finer distinctions. Without effective negative sampling, models risk collapsing all entities and relations into similar embeddings, failing to learn meaningful representations.

The benefits of this embedding approach are manifold. Firstly, it enables \textit{efficient computation} for tasks like link prediction and triple classification, as vector operations are significantly faster than symbolic reasoning \cite{zhu2020}. For instance, calculating the plausibility of a triple using vector distances or dot products is orders of magnitude faster than traversing complex symbolic rules. Secondly, KGEs offer enhanced \textit{scalability}, as the fixed dimensionality $d$ allows for compact representations of even massive KGs, mitigating the sparsity issues of one-hot encodings. This has led to research into parameter-efficient KGEs where the parameter count does not scale linearly with the number of entities \cite{chen2023}. Thirdly, these dense vector representations provide a \textit{seamless interface} for integrating structured knowledge with various deep learning architectures, powering applications in recommendation systems \cite{sun2018, yang2023}, question answering \cite{huang2019}, and entity alignment \cite{sun2018}.

However, the core principle of capturing relational patterns in a continuous space also introduces inherent assumptions and limitations. While KGEs excel at statistical inference and pattern recognition, they often struggle with the explicit, symbolic reasoning capabilities of traditional AI, such as multi-hop inference or logical deduction, which are not directly encoded in vector arithmetic. The choice of embedding dimension $d$ itself presents a trade-off: a smaller $d$ offers greater efficiency but risks losing fine-grained semantic distinctions, while a larger $d$ can capture more information but increases computational cost and memory footprint. This fundamental tension between representational capacity and computational efficiency laid the foundation for the "arms race" in developing increasingly expressive and sophisticated geometric and neural models.

### 2.2. Translational Models: From TransE to TransH and TransR

The early landscape of KGE was largely defined by 'translational distance models,' which conceptualized relations as translation operations between entity embeddings in a low-dimensional vector space. These models were pioneering in their simplicity and efficiency, establishing a foundational paradigm for geometric KGE.

**TransE (Translating Embeddings)** is arguably the most influential early translational model, proposing that if a triple $(h, r, t)$ holds, then the embedding of the head entity $\mathbf{h}$ plus the embedding of the relation $\mathbf{r}$ should be approximately equal to the embedding of the tail entity $\mathbf{t}$, i.e., $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ \cite{bordes2013transe}. The plausibility of a triple is measured by a distance-based scoring function, typically the $L_1$ or $L_2$ norm, such as $f(h,r,t) = ||\mathbf{h} + \mathbf{r} - \mathbf{t}||$.
*   **Strengths:** TransE's simplicity makes it computationally efficient and easy to implement, performing remarkably well on 1-to-1 relations. Its low parameter count (only one vector per entity and relation) contributes to its efficiency.
*   **Weaknesses:** Its rigid geometric assumption struggles significantly with complex relation types, specifically 1-to-N, N-to-1, and N-to-N relations \cite{asmara2023}. For instance, if an entity is the head of multiple triples with the same relation but different tails (e.g., (Barack Obama, spouse, Michelle Obama) and (Barack Obama, child, Malia Obama)), TransE would assign the same relation vector $\mathbf{r}$ to both, forcing $\mathbf{h} + \mathbf{r} \approx \mathbf{t_1}$ and $\mathbf{h} + \mathbf{r} \approx \mathbf{t_2}$. This implies $\mathbf{t_1} \approx \mathbf{t_2}$, effectively collapsing distinct entities into similar representations, leading to poor performance. This limitation stems from using a single, fixed relation vector $\mathbf{r}$ for all entities, which cannot capture the diverse contexts in which a relation might manifest or the different facets of an entity. It also cannot effectively model symmetric relations (e.g., (sibling, sibling)) or antisymmetric relations, as $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ implies $\mathbf{t} - \mathbf{r} \approx \mathbf{h}$, which is problematic for relations where the inverse is not simply the negative of the relation vector.

To address TransE's shortcomings with complex relations, **TransH (Translating on Hyperplanes)** was introduced \cite{wang2014transh}. Unlike TransE, which operates in a single global space, TransH projects entities onto a relation-specific hyperplane before performing the translation. Each relation $r$ is associated with two vectors: a translation vector $\mathbf{r}$ and a normal vector $\mathbf{w}_r$ defining its hyperplane. The projected entity embeddings are $\mathbf{h}_{\perp} = \mathbf{h} - \mathbf{w}_r^T \mathbf{h} \mathbf{w}_r$ and $\mathbf{t}_{\perp} = \mathbf{t} - \mathbf{w}_r^T \mathbf{t} \mathbf{w}_r$, and the scoring function becomes $f(h,r,t) = ||\mathbf{h}_{\perp} + \mathbf{r} - \mathbf{t}_{\perp}||$.
*   **Strengths:** By allowing entities to have different representations (projections) on different hyperplanes, TransH significantly improves the handling of 1-to-N and N-to-1 relations. A single entity can now have distinct contextualized embeddings for different relations, mitigating the entity collapse problem observed in TransE. For example, 'Barack Obama' can be projected differently for 'spouse' and 'child' relations.
*   **Weaknesses:** While better, TransH still uses a single relation vector $\mathbf{r}$ for translation *on the hyperplane*, which might not be sufficient for highly complex N-to-N relations where the translation itself needs to be more nuanced. The introduction of relation-specific hyperplanes also increases the parameter count (two vectors per relation instead of one), making it more prone to overfitting, especially on sparse KGs where there isn't enough data to learn robust hyperplane normal vectors.

Building further on this idea, **TransR (Translating in Relation-Specific Spaces)** proposed projecting entities into relation-specific vector spaces rather than just hyperplanes \cite{lin2015transr}. In contrast to TransH's projection onto a hyperplane, TransR uses a full projection matrix. Each relation $r$ is associated with a translation vector $\mathbf{r}$ and a projection matrix $\mathbf{M}_r$. Entities $\mathbf{h}$ and $\mathbf{t}$ are projected into the relation space as $\mathbf{h}_r = \mathbf{h}\mathbf{M}_r$ and $\mathbf{t}_r = \mathbf{t}\mathbf{M}_r$, and the scoring function is $f(h,r,t) = ||\mathbf{h}_r + \mathbf{r} - \mathbf{t}_r||$.
*   **Strengths:** TransR offers higher expressiveness by allowing entities to have entirely different representations in different relation spaces, making it more effective for N-to-N relations and capturing more nuanced relational semantics. This is a significant improvement over TransH, which only projects entities onto a hyperplane within the original embedding space, whereas TransR transforms them into an entirely new relation-specific space.
*   **Weaknesses:** The significant increase in parameters due to the projection matrices $\mathbf{M}_r$ (one $d \times k$ matrix for each relation, where $k$ is the relation space dimension) leads to higher computational complexity and memory requirements. This can be particularly problematic for KGs with a large number of relations, potentially leading to overfitting and difficulties in training, especially given that many relations might be sparse. The experimental setups for these models often require careful hyperparameter tuning, which can be dataset-dependent \cite{lloyd2022}.

The evolution from TransE to TransH and TransR demonstrates a clear pattern: each subsequent model aimed to overcome the expressiveness limitations of its predecessor by introducing more complex geometric transformations. This came with an explicit trade-off: increased representational power was achieved at the cost of higher parameter counts and computational overhead. While TransE prioritized efficiency and simplicity, TransR prioritized expressiveness, with TransH offering a middle ground. This highlights a fundamental constraint in KGE: balancing the ability to model complex relational patterns with the practical demands of scalability and training stability.

### 2.3. Early Extensions: Dynamic Mappings, Manifold Embeddings, and Beyond

The limitations of the initial translational models, particularly their rigid geometric assumptions and fixed relation representations, spurred immediate extensions that sought greater expressiveness and flexibility. These early extensions explored dynamic mappings, expanded entity representations beyond simple points, and introduced more sophisticated ways to handle complex relations.

**TransD (Translating with Dynamic Mapping Matrix)** \cite{ji2015transd} aimed to address the parameter explosion of TransR while retaining its expressiveness. Unlike TransR, which uses a full projection matrix $\mathbf{M}_r$ for each relation, TransD proposes dynamic mapping matrices constructed from two vectors: a relation-specific projection vector $\mathbf{r}_p$ and an entity-specific projection vector $\mathbf{e}_p$. The projected entity embeddings are computed as $\mathbf{h}_r = \mathbf{h} + \mathbf{r}_p \mathbf{h}^T \mathbf{e}_p$ and $\mathbf{t}_r = \mathbf{t} + \mathbf{r}_p \mathbf{t}^T \mathbf{e}_p$. This allows each entity-relation pair to have a unique projection, making the mapping dynamic and context-dependent, without incurring the high parameter cost of full matrices.
*   **Strengths:** TransD significantly reduces the number of parameters compared to TransR (from $O(N_r \cdot d^2)$ to $O(N_r \cdot d + N_e \cdot d)$), while maintaining or even improving expressiveness. It allows for more fine-grained, entity-specific projections within each relation space, making it more scalable than TransR for KGs with many relations.
*   **Weaknesses:** Despite its improvements, TransD still fundamentally adheres to the translational assumption, limiting its ability to capture highly complex or non-linear relational patterns that cannot be reduced to a translation in some projected space. The dynamic mapping, while efficient, might still struggle with extremely diverse entity types or highly polysemous relations where a simple vector-based projection might be insufficient.

Another notable extension, **TransA (An Adaptive Approach for Knowledge Graph Embedding)** \cite{xiao2015transa}, focused on improving the training objective rather than the geometric model itself. TransA introduced an adaptive margin for the loss function, moving beyond the fixed, global margin used in TransE and its variants. It calculates an optimal margin dynamically, based on entity-specific and relation-specific characteristics, reflecting the "locality" of different parts of the knowledge graph.
*   **Strengths:** By adaptively determining the margin, TransA can achieve better generalization and performance by tailoring the loss function to the specific structural properties of different KGs or their subgraphs. This addresses a critical methodological limitation of previous models that relied on arbitrary, experimentally determined global margins.
*   **Weaknesses:** While improving the training process and potentially leading to more robust embeddings, TransA does not fundamentally alter the underlying translational geometric model. Its expressiveness is still bound by the $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ paradigm, meaning it inherits some of the core limitations regarding complex relation types. The calculation of adaptive margins can also introduce its own computational overhead, although optimizations exist. In contrast to TransD which enhances the geometric model, TransA refines the learning process for existing geometric models.

Beyond point-based representations, **ManifoldE (A Manifold-based Approach for Knowledge Graph Embedding)** \cite{xiao2016manifold} represented a conceptual leap by expanding entity representations from single points to geometric manifolds, such as hyperplanes or spheres. In ManifoldE, entities are no longer strict points but rather regions or subspaces, and relations are modeled as transformations between these manifolds. For example, a relation might transform a head entity's manifold into a tail entity's manifold.
*   **Strengths:** Representing entities as manifolds offers significantly increased representational power and flexibility. It can naturally capture uncertainty, fuzziness, or multiple facets of an entity, addressing the rigidity of strict point-based models. This allows for more precise link prediction by considering the "region" an entity occupies rather than just a single point.
*   **Weaknesses:** The complexity of defining and manipulating manifolds and their transformations is substantially higher than point-based models. This leads to increased computational cost and parameterization challenges. Furthermore, the choice of manifold type (e.g., hyperplane, sphere) is a modeling assumption that might not universally fit all entities or relations, and determining the optimal manifold for different contexts remains a challenge. ManifoldE diverges from the strict translational paradigm by allowing more complex geometric transformations, but this comes at a significant increase in model complexity.

Another approach to enhance translational models for complex relations is **TransE-MTP (TransE with Multi-Translation Principles)**. This model, building directly on TransE, introduces multiple translation principles (MTPs) for different relation types (1-to-1, 1-to-N, N-to-1, N-to-N) \cite{additional_paper_1}. Instead of projecting entities or relations into new spaces, TransE-MTP adapts the translation rule itself based on the relation's cardinality. For example, a 1-to-N relation might use a different translation mechanism than a 1-to-1 relation.
*   **Strengths:** By defining specific translation rules for different relation types, TransE-MTP improves performance on complex relations compared to vanilla TransE and even TransH, as demonstrated on datasets like Freebase and Wordnet. It offers a more direct way to handle cardinality issues without the heavy parameter burden of TransR.
*   **Weaknesses:** This approach requires explicit knowledge or inference of relation types, which might not always be readily available or perfectly accurate in real-world KGs. It still fundamentally relies on the translation paradigm, potentially limiting its ability to capture highly abstract or non-linear relational patterns.

Finally, some extensions focused on integrating semantic constraints. **KRC (Knowledge Graph Embedding with Relational Constraints)** proposes a general framework for enhancing translation-based models by encoding regularities between a relation and its arguments into the embedding space \cite{additional_paper_3}. This moves beyond purely geometric considerations by incorporating semantic rules.
*   **Strengths:** KRC improves knowledge graph completion by leveraging semantic constraints, which can guide the learning process and lead to more semantically coherent embeddings. It also proposes a soft margin-based ranking loss, further refining the training objective.
*   **Weaknesses:** The effectiveness of KRC depends on the quality and availability of these relational constraints. Defining and integrating such constraints can be a non-trivial task, and the model's performance is still bound by the underlying translation-based architecture.

These early extensions collectively underscore the continuous drive for enhanced representational power. They demonstrate a progression from fixed, global parameters to dynamic, context-aware mappings (TransD), from rigid point representations to more flexible geometric objects (ManifoldE), and from generic translation rules to type-specific or semantically constrained translations (TransE-MTP, KRC). This evolution highlighted the need for models that could capture the inherent heterogeneity and complexity of real-world knowledge, moving beyond the simplistic assumptions of the earliest models.

### 2.4. Limitations of Early Geometric Models and the Drive for Enhanced Expressiveness

While the pioneering translational models and their immediate extensions laid a crucial foundation for Knowledge Graph Embedding, they also exposed significant limitations that became catalysts for subsequent research. The core geometric assumptions of these early models, primarily relying on vector addition for relations, proved to be overly simplistic for the rich and diverse semantics found in real-world Knowledge Graphs.

A primary limitation was their inherent difficulty in capturing \textit{complex relation types} \cite{wu2021}. TransE, with its fixed relation vector, notoriously struggled with 1-to-N, N-to-1, and N-to-N relations, effectively collapsing distinct entities into similar representations. This limitation exists because the model's mechanism of $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ implies that for a given head $\mathbf{h}$ and relation $\mathbf{r}$, there can only be one unique tail $\mathbf{t}$. When multiple tails exist, the model is forced to learn an $\mathbf{r}$ that averages across them, leading to a loss of specificity. While TransH and TransR introduced relation-specific hyperplanes and spaces to mitigate this, they still faced challenges with highly polysemous relations or those exhibiting intricate logical properties like symmetry, antisymmetry, or transitivity \cite{asmara2023}. For instance, a symmetric relation like "sibling" would ideally imply $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ and $\mathbf{t} + \mathbf{r} \approx \mathbf{h}$, which is difficult to enforce with a single translation vector without forcing $\mathbf{r}$ to be approximately zero. The rigid geometric interpretation of relations as simple translations or projections often failed to capture the nuanced, multi-faceted nature of how entities interact.

Furthermore, these models often lacked the capacity to represent \textit{multi-faceted entity semantics} or \textit{context-dependent relations} \cite{wu2021}. An entity might have different meanings or roles depending on the relation it participates in, a subtlety that fixed point embeddings (even with projections like in TransH/R) struggle to encode fully. For example, "apple" as a fruit and "Apple" as a company require distinct semantic representations that simple translational models find hard to disentangle. The early models also largely ignored the \textit{temporal dimension} of knowledge, treating all facts as static truths, which is a significant oversight for dynamic KGs where facts have limited validity periods \cite{dasgupta2018, xu2019}. This led to inaccurate reasoning when temporal information was critical, as a fact true at one time might be false at another, a concept not captured by static vector representations.

The pursuit of greater expressiveness in models like TransR and ManifoldE, while beneficial, often led to a \textit{parameter explosion} and increased computational complexity \cite{chen2023}. This trade-off between expressiveness and efficiency became a central challenge, particularly for large-scale KGs. For example, TransR's use of full projection matrices for each relation can lead to millions of parameters for KGs with thousands of relations, making training slow and memory-intensive, and increasing the risk of overfitting on sparse data. This is a direct consequence of attempting to model complex transformations with high-dimensional matrices. The reliance on distance-based scoring functions also imposed implicit constraints on the types of relational patterns that could be learned, favoring simple geometric relationships over more abstract semantic matches. Moreover, the sensitivity of these models to hyperparameters and negative sampling strategies highlighted their fragility, with optimal configurations often being highly dataset-dependent \cite{lloyd2022, shan2018}. A methodological critique is that many early evaluations often used uniform negative sampling, which might overestimate performance by providing "easy" negative examples, rather than challenging the model with more plausible but false triples.

These limitations underscored a continuous and urgent drive for enhanced representational power in KGE. The field recognized the need to move beyond strict geometric forms and simple translational assumptions towards models that could: (1) capture more intricate relational patterns, (2) handle context-dependent entity and relation semantics, (3) integrate auxiliary information (like time or text), and (4) achieve this expressiveness without sacrificing scalability or robustness. This critical analysis of early geometric models thus serves as a foundational understanding, setting the stage for the development of more sophisticated semantic matching models, neural architectures, and dynamic embedding approaches discussed in subsequent sections.