\section*{6. Dynamic, Efficient, and Robust KGE}

The evolution of Knowledge Graph Embedding (KGE) research has increasingly shifted from merely learning static representations to addressing the critical challenges posed by the dynamic nature, practical deployment, and reliability requirements of real-world knowledge graphs. This section delves into advanced KGE paradigms that enable models to adapt to evolving knowledge, operate efficiently on massive datasets, and maintain integrity in the face of noise and adversarial attacks. The core motivation for these advancements stems from the recognition that knowledge is not static but constantly changes, that computational resources are finite, and that real-world data is inherently imperfect. Consequently, the field has seen a proliferation of methods for Temporal Knowledge Graph Embedding (TKGE) to capture time-varying facts, inductive and continual learning to handle evolving KGs, and techniques for enhancing efficiency, compression, and robustness against data quality issues and malicious attacks. This collective effort aims to develop KGE models that are not only accurate but also agile, scalable, and trustworthy, pushing the boundaries towards truly intelligent and resilient knowledge systems.

\subsection*{Temporal, Spatiotemporal, and Fuzzy Knowledge Graph Embedding}

Traditional KGE models often treat facts as immutable, overlooking the crucial temporal dimension that governs their validity. This limitation severely restricts their applicability in dynamic environments where knowledge evolves over time. Temporal Knowledge Graph Embedding (TKGE) addresses this by explicitly modeling the time-varying nature of entities and relations. Early approaches to TKGE, such as HyTE \cite{dasgupta2018}, represent each timestamp as a hyperplane, allowing the model to capture the temporal validity of facts by projecting entity and relation embeddings onto these time-specific hyperplanes. While innovative for its geometric interpretation of time, HyTE's representation of time as discrete hyperplanes may struggle with continuous temporal dynamics or complex temporal patterns like periodicity.

More advanced TKGE models have moved towards more sophisticated representations of temporal evolution and uncertainty. ATiSE \cite{xu2019} models the evolution of entity and relation representations as multi-dimensional additive time series, decomposing them into trend, seasonal, and random components. Crucially, ATiSE represents entities and relations as multi-dimensional Gaussian distributions, explicitly capturing the *temporal uncertainty* in their evolution. This probabilistic approach is a significant step beyond deterministic embeddings, acknowledging the inherent fuzziness of real-world knowledge. However, ATiSE's assumption of constant diagonal covariance matrices for computational efficiency might oversimplify the true complexity of temporal uncertainty. Other notable TKGE efforts include those leveraging tensor decomposition \cite{lin2020, he2022e37}, temporal rotations \cite{xu2020, sadeghian2021}, graph attention networks \cite{xie2023, hou20237gt}, and even hyperbolic embeddings for extrapolation \cite{jia2023krv, wang2024, wang2024}. These models, while diverse, share the common goal of integrating time as a first-class citizen, enabling tasks like temporal link prediction and future event forecasting \cite{li2023y5q}. A recent survey by \cite{zhang20243iw} provides a comprehensive overview of the models and applications in this rapidly expanding area.

Beyond pure temporal dynamics, the integration of spatial and fuzzy information further enhances the expressiveness of KGE. Spatiotemporal KGE models aim to capture facts that are valid within specific geographical regions and time periods. FSTRE \cite{ji2024} introduces a Fuzzy Spatiotemporal RDF Knowledge Graph Embedding model that uses uncertain dynamic vector projection and rotation, explicitly handling uncertainty with fuzzy logic. This model, along with its extension for multihop queries \cite{ji2024}, represents a significant advancement by addressing both spatial, temporal, and inherent data uncertainty simultaneously. The challenge here lies in the increased complexity of modeling and the intensive data requirements for accurate spatiotemporal annotations. The "why" behind these complex models is the need to mirror the granularity and inherent imprecision of real-world knowledge, where facts are rarely absolutely true or universally valid. However, the trade-off is often increased computational cost and the difficulty of acquiring sufficiently rich and clean spatiotemporal data, limiting their generalizability to domains with sparse annotations.

\subsection*{Inductive, Continual, and Federated Learning for KGE}

The static, transductive nature of many traditional KGE models poses significant limitations in dynamic, open-world scenarios where knowledge graphs are constantly evolving with new entities and relations, or where data is distributed and privacy-sensitive. This has spurred research into inductive, continual, and federated learning paradigms for KGE.

**Inductive KGE** focuses on generalizing to unseen entities or relations without retraining the entire model. This is crucial for handling the ever-expanding nature of real-world KGs. Approaches often leverage meta-learning or compositional representations. For instance, \cite{chen2021} proposed Meta-Knowledge Transfer for Inductive KGE, where a meta-learner acquires transferable knowledge from existing entities to embed new ones. This addresses the limitation of traditional models that require retraining for every new entity. Similarly, InGram \cite{lee202380l} learns inductive embeddings via relation graphs, allowing it to generalize to unseen entities by composing their representations from known relational patterns. Logic Attention Based Neighborhood Aggregation \cite{wang2018} also enables inductive capabilities by aggregating information from an entity's local neighborhood. The primary methodological limitation of inductive KGE often lies in the assumption that new entities' representations can be adequately composed from existing patterns, which may not hold for truly novel or out-of-distribution entities.

**Continual KGE** addresses the challenge of adapting KGE models to evolving knowledge without suffering from catastrophic forgetting of previously learned information. As KGs are continuously updated, models must incrementally learn new facts while retaining old ones. Recent works have explored parameter-efficient adaptation techniques, such as incremental LoRA \cite{liu2024}, which adapt pre-trained KGE models to new data with minimal parameter updates. Knowledge distillation is another promising avenue, where a "student" model learns from a "teacher" model trained on older data, combined with new data, to prevent forgetting \cite{liu2024to0}. Dynamic KGE via local embedding reconstructions \cite{krause2022th0} also offers a way to update embeddings incrementally. The "why" behind the difficulty of continual learning is the inherent plasticity-stability dilemma: models need to be flexible enough to learn new information (plasticity) but stable enough to retain old knowledge (stability).

**Federated KGE** tackles the problem of learning KGE models from distributed and private knowledge graphs without centralizing raw data, which is vital for privacy-sensitive domains. This emerging field leverages federated learning principles. \cite{chen20226e4} proposed federated knowledge graph completion via embedding-contrastive learning, allowing multiple clients to collaboratively train a KGE model. Communication efficiency is a major concern in federated settings, addressed by methods like entity-wise top-k sparsification \cite{zhang2024} and personalized federated KGE with client-wise relation graphs \cite{zhang2024}. However, federated KGE introduces new security and privacy challenges. For instance, \cite{zhou2024} demonstrated the vulnerability of federated KGE to poisoning attacks, while \cite{hu20230kr} quantified and defended against privacy threats. These studies highlight an "arms race" dynamic between privacy-preserving mechanisms and potential attacks. The trade-off between privacy, utility, and communication overhead remains a central challenge, often limited by theoretical gaps in robust aggregation and differential privacy guarantees for complex graph structures.

\subsection*{Efficiency, Compression, and Robustness in Training}

The sheer scale of modern knowledge graphs necessitates KGE models that are not only accurate but also efficient in terms of computation, memory, and storage, while simultaneously being robust to the inherent noise and imperfections of real-world data.

**Efficiency and Compression** are critical for practical deployment. Traditional KGE models often suffer from a parameter explosion problem, where the number of entity embeddings scales linearly with the number of entities. To counter this, \cite{chen2023} introduced Entity-Agnostic Representation Learning (EARL), which learns embeddings for a small set of "reserved entities" and uses universal encoders to compose representations for all other entities from their distinguishable information (e.g., connected relations, k-nearest reserved entities, multi-hop neighbors). This paradigm shift significantly reduces parameter storage costs, making KGE models deployable on resource-constrained devices. Other efficiency techniques include knowledge distillation, where a smaller, more efficient model learns from a larger, more complex one. DualDE \cite{zhu2020} dually distills knowledge graph embeddings for faster and cheaper reasoning. Compression techniques \cite{sachan2020} and lightweight frameworks for efficient inference and storage \cite{wang2021} are also crucial. For large-scale training, parallelization techniques \cite{kochsiek2021} and communication-efficient strategies like hotness-aware caching \cite{dong2022c6z} are essential to manage the computational burden. The methodological limitation of compression often lies in the trade-off between model size and expressiveness; overly compressed models may lose fine-grained semantic distinctions.

**Robustness** is paramount given the noisy and incomplete nature of real-world KGs. A key aspect of KGE training is negative sampling, which generates "false" triples to teach the model what is not true. The quality of negative samples significantly impacts model performance and robustness. Simple uniform random sampling often generates "easy" negatives, leading to suboptimal learning. More sophisticated strategies, such as confidence-aware negative sampling for noisy KGs \cite{shan2018}, aim to generate more informative, "harder" negatives. NSCaching \cite{zhang2018} and efficient non-sampling methods \cite{li2021} also contribute to more effective training. The choice of negative sampling strategy is critical, as highlighted by reviews \cite{qian2021, madushanka2024}.

Beyond negative sampling, models need to be inherently robust to noisy data and potential adversarial attacks. Data poisoning attacks, where malicious triples are injected into the training data, can significantly degrade KGE performance \cite{zhang20190zu, zhang20193g2}. Defenses include rule-enhanced error detection \cite{hong2020hyg} and multi-task reinforcement learning for robust KGE \cite{zhang2021}. The issue of bias in KGE models, whether from the data or the model itself, also affects robustness and fairness \cite{radstok2021yup, shomer2023imo}. Techniques like weighted KGE \cite{zhang2023} and confidence-aware embeddings \cite{huang20240su} attempt to mitigate the impact of varying data quality. Furthermore, the sensitivity of KGE quality to hyperparameters is substantial and varies across datasets \cite{lloyd2022}, implying that robust performance requires careful and often dataset-specific tuning. The "why" behind these robustness challenges often stems from the inherent difficulty of distinguishing genuine noise or adversarial perturbations from legitimate, albeit rare, patterns in high-dimensional embedding spaces. This leads to an ongoing need for robust optimization, better regularization, and more resilient model architectures.