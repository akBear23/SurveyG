\subsection{Context-Augmented and Multi-view Embeddings}

Traditional Knowledge Graph Embedding (KGE) models, primarily relying on the structural information of (head, relation, tail) triples, often struggle with data sparsity and fail to capture the full semantic richness of entities and relations. Real-world knowledge graphs (KGs) are frequently accompanied by abundant auxiliary information, such as textual descriptions, entity attributes, and logical rules. This subsection investigates KGE approaches that enrich entity and relation representations by integrating these diverse data types, leading to "context-augmented" or "multi-view" embeddings. These methods aim to provide a more comprehensive understanding of entities, mitigate data sparsity, and enhance performance in tasks like link prediction and entity alignment by providing richer contextual cues \cite{choudhary2021, ge2023}.

One prominent direction involves leveraging **textual descriptions and linguistic context** associated with entities and relations. These methods bridge the gap between symbolic knowledge and natural language semantics. Early efforts, such as Semantic Space Projection (SSP) \cite{xiao2016}, augmented KGE by projecting the triple loss vector onto a semantic hyperplane derived from textual descriptions, thereby enforcing consistency between structural and textual embedding spaces. This approach highlights the potential of textual data to provide dense semantic cues that are often implicit in sparse triple structures. More recently, with the advent of powerful pre-trained language models (PLMs), methods like LASS \cite{shen2022} have emerged, which jointly embed language semantics from PLMs with structural information. LASS fine-tunes PLMs using a probabilistic structured loss, enhancing knowledge graph completion by integrating rich linguistic context directly into the embedding process. The integration of PLMs offers significant advantages in capturing nuanced semantics and handling out-of-vocabulary entities, but it also introduces challenges related to computational cost, the potential for noise from unstructured text, and effectively aligning the different semantic spaces \cite{ge2023}.

Beyond unstructured text, incorporating richer **structural context and logical rules** provides powerful inductive biases and enhances the logical consistency of embeddings. While basic KGE models only consider direct triples, context-augmented approaches look at the neighborhood or broader graph patterns. For instance, \cite{gao2018di0} introduced Triple Context-Based Knowledge Graph Embedding, which explicitly utilizes the context of each triple, composed of neighboring entities, their outgoing relations, and relation paths between target entities. This allows for a more informed inference process by considering local graph topology. Logical rules, often expressed as first-order logic, offer a declarative way to encode domain knowledge and enforce consistency. RUGE \cite{guo2017} proposed an iterative framework that guides KGE models with soft logical rules, enabling embeddings to learn from both labeled and rule-inferred unlabeled triples. This approach demonstrates how automatically extracted rules can improve embedding quality by providing additional supervision. Extending this, RulE \cite{tang2022} learns explicit rule embeddings and integrates them into a unified space with entities and relations, allowing for soft rule reasoning and mutual regularization. To improve scalability for rule integration, SLRE \cite{guo2020} directly regularizes relation representations based on soft logical regularities, making the complexity of rule learning independent of the entity set size. Furthermore, the inherent structure of relations can be leveraged, as seen in HRS \cite{zhang2018}, which utilizes a three-layer hierarchical relation structure (relation clusters, individual relations, sub-relations) to enrich knowledge representations, extending existing KGE models to better capture relational hierarchies. From a theoretical perspective, frameworks like Knowledge Sheaves \cite{gebhart2021gtp} offer a sophisticated way to incorporate prior constraints and schema-induced consistency, providing a generalized framework for reasoning about KGE models and adapting them for composite relations. While rule-based methods enhance logical consistency and explainability, they often suffer from the difficulty of extracting complete and accurate rule sets and can be brittle in the face of noisy data.

The concept of multi-view embeddings further extends to integrating **explicit entity attributes and truly multi-modal data**. This is crucial for creating comprehensive entity representations, especially in domains where entities have rich descriptive properties or non-textual information. Moving beyond simple triples, hyper-relational KGE models like HINGE \cite{rosso2020} directly learn from facts augmented with key-value pairs, simultaneously capturing the base triplet structure and its associated attributes. This allows for a richer representation of complex facts. For tasks like entity alignment, MultiKE \cite{zhang2019} developed a framework that unifies multiple views of entities—such as names, relations, and attributes—by learning view-specific embeddings and employing novel cross-KG inference and soft alignment methods. This approach is particularly effective in integrating heterogeneous information from different KGs. In specialized domains, multimodal reasoning has gained traction. For instance, \cite{zhu2022} constructed Specific Disease Knowledge Graphs (SDKGs) and performed multimodal reasoning by integrating structure, category, and description embeddings via a reverse-hyperplane projection method to discover new disease-specific knowledge. This demonstrates the power of combining diverse modalities for complex inference. In the context of explainable recommendations, CKGE \cite{yang2023} leverages contextualized neighbor semantics and high-order connections within meta-graphs to learn motivation-aware embeddings for talents and courses, providing transparent explanations by integrating rich user and item attributes. These multi-modal and attribute-rich approaches significantly enhance the representational power of KGEs, address cold-start problems, and facilitate cross-domain knowledge transfer, but they face challenges in data heterogeneity, effective fusion strategies, and managing noise across different modalities \cite{choudhary2021}.

In conclusion, the evolution of KGE from purely structural models to sophisticated context-augmented and multi-view approaches marks a critical advancement. By effectively integrating diverse auxiliary information—ranging from textual descriptions and linguistic context to richer structural patterns, logical rules, explicit attributes, and multi-modal data—these methods successfully mitigate data sparsity and significantly enhance the semantic richness and robustness of entity and relation representations. While substantial progress has been made, challenges remain in developing truly unified, adaptive frameworks that can seamlessly and efficiently combine an ever-growing variety of contextual cues, especially in dynamic and evolving knowledge graphs. Future research should focus on adaptive mechanisms for selecting and weighting different information views, ensuring interpretability in complex multi-modal models, and exploring more robust ways to handle noisy or incomplete auxiliary data.