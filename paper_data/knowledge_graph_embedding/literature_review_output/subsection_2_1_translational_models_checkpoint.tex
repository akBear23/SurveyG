\subsection{Translational Models}

Translational models constitute a foundational paradigm in knowledge graph embedding (KGE), conceptualizing relations as vector translations between entity embeddings within a continuous vector space. This elegant approach posits that for a valid triple $(\text{head, relation, tail})$, the embedding of the head entity ($\mathbf{h}$) plus the relation vector ($\mathbf{r}$) should approximate the embedding of the tail entity ($\mathbf{t}$), typically expressed as $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$. The plausibility of a triple is measured by a scoring function, often the squared L1 or L2-norm of the difference $||\mathbf{h} + \mathbf{r} - \mathbf{t}||$.

The pioneering model in this category is \textbf{TransE (Translating Embeddings)} \cite{bordes2013}, which established this core principle. TransE's appeal lay in its elegant simplicity and computational efficiency. However, its assumption of a single, fixed vector representation for each entity across all relations proved to be a significant limitation. This rigidity made it particularly challenging to model complex relation types such as one-to-many, many-to-one, and many-to-many. For instance, in a one-to-many relation like "person has_child child", if a person has multiple children, TransE would force the embeddings of all children to be clustered around $\mathbf{h} + \mathbf{r}$, making them indistinguishable or leading to suboptimal representations for the children. These foundational concepts and their initial challenges are comprehensively reviewed in works like \cite{asmara2023}.

To address TransE's limitations, \cite{wang2014} introduced \textbf{TransH (Translating on Hyperplanes)}. This model innovatively allows entities to have distributed representations by projecting them onto relation-specific hyperplanes. For a given relation $\mathbf{r}$, TransH defines a normal vector $\mathbf{w}_r$ for its hyperplane and a translation vector $\mathbf{d}_r$ that lies within this hyperplane. Entity embeddings $\mathbf{h}$ and $\mathbf{t}$ are first projected onto this relation-specific hyperplane, yielding $\mathbf{h}_{\perp} = \mathbf{h} - \mathbf{w}_r^T \mathbf{h} \mathbf{w}_r$ and $\mathbf{t}_{\perp} = \mathbf{t} - \mathbf{w}_r^T \mathbf{t} \mathbf{w}_r$. The plausibility of a triple is then measured by the squared L2-norm of the difference: $f_r(\mathbf{h},\mathbf{t}) = ||\mathbf{h}_{\perp} + \mathbf{d}_r - \mathbf{t}_{\perp}||_2^2$. This approach effectively handles complex relation types by allowing an entity's representation to adapt to the context of the relation it participates in. Furthermore, \cite{wang2014} incorporated an orthogonality constraint ($\mathbf{w}_r^T \mathbf{d}_r = 0$) to ensure the translation vector resides within the hyperplane and proposed an improved Bernoulli negative sampling strategy, which leverages relation mapping properties to generate more effective negative examples during training.

Building on the success of TransH, subsequent models further refined the concept of relation-specific representations. \cite{lin2015} proposed \textbf{TransR (Translating on Relations)} and \textbf{CTransR (Cluster-based Translating on Relations)}. TransR introduced the idea of mapping entities from a general entity space to a distinct relation-specific space using a projection matrix $\mathbf{M}_r$ for each relation $r$. Thus, the projected entity embeddings become $\mathbf{h}_r = \mathbf{h}\mathbf{M}_r$ and $\mathbf{t}_r = \mathbf{t}\mathbf{M}_r$, and the scoring function becomes $||\mathbf{h}_r + \mathbf{r} - \mathbf{t}_r||$. This allowed for more flexible and expressive representations, as entities could have different "appearances" depending on the relation they are involved in. CTransR extended TransR by recognizing that complex relations might exhibit multiple semantics. Instead of a single relation vector, CTransR clusters head-tail entity pairs for complex relations and learns multiple relation vectors and projection matrices for each cluster, capturing diverse semantic patterns within a single relation type.

Further enhancing this flexibility, \cite{ji2015} introduced \textbf{TransD (Translating with Dynamic Mapping Matrices)}. Unlike TransR, which uses a fixed projection matrix for each relation, TransD employs dynamic mapping matrices that are adaptively generated based on both the entity and the relation. Specifically, for a head entity $h$ and relation $r$, the projection matrix $\mathbf{M}_{rh}$ is constructed from $\mathbf{r}$ and $\mathbf{h}$ (and similarly $\mathbf{M}_{rt}$ for the tail entity). This allows for even greater adaptability, as the projection is tailored to the specific entity-relation pair, offering enhanced expressiveness in capturing diverse relational semantics.

Beyond these core extensions, the translational paradigm continues to evolve. \cite{li2020ek4} proposed the Knowledge Graph Embedding with Relational Constraints (KRC) framework, which enhances translation-based models by explicitly encoding regularities between a relation and its arguments into the embedding space. This allows for a more nuanced understanding of relational semantics beyond simple geometric translations. More recently, \cite{li2024uio} introduced TransE-MTP (TransE with Multi-Translation Principles), a direct advancement of TransE designed to better handle complex relations (one-to-many, many-to-one, many-to-many) by defining multiple translation principles for different relation types. This demonstrates ongoing efforts to refine the core translational idea to address persistent challenges.

Despite these significant advancements, translational models, even with their sophisticated extensions, primarily rely on predefined geometric operations. This reliance can limit their capacity to capture highly complex, implicit, or non-linear semantic patterns. A fundamental limitation stems from their inability to inherently model certain relational properties:
\begin{itemize}
    \item \textbf{Symmetry/Antisymmetry}: If $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ for a symmetric relation, it ideally implies $\mathbf{t} + \mathbf{r} \approx \mathbf{h}$. This only holds if $\mathbf{r} \approx \mathbf{0}$, which trivializes the relation, or if specific constraints are added. For antisymmetric relations, it's even more challenging.
    \item \textbf{Inverse Relations}: For relations like `parentOf` and `childOf`, translational models typically require learning two distinct relation vectors, $\mathbf{r}_{\text{parentOf}}$ and $\mathbf{r}_{\text{childOf}}$, without an inherent connection between them (e.g., $\mathbf{r}_{\text{parentOf}} \approx -\mathbf{r}_{\text{childOf}}$).
    \item \textbf{Compositional Relations}: Translational models struggle to capture compositional patterns (e.g., `fatherOf` + `fatherOf` = `grandfatherOf`). The sum of two relation vectors does not inherently represent the composite relation. This is related to the "Z-paradox," where certain logical inferences cannot be captured by simple vector addition.
\end{itemize}
While models like TransH, TransR, and TransD mitigate some of TransE's initial shortcomings by introducing relation-specific contexts, the core translational assumption still imposes constraints on their expressiveness. These inherent geometric limitations motivated the exploration of alternative paradigms, such as rotational models and tensor factorization, which offer different mathematical frameworks to capture richer and more diverse relational patterns.