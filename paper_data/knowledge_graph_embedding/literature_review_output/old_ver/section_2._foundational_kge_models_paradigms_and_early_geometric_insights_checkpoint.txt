\section{Foundational KGE Models: Paradigms and Early Geometric Insights}
The bedrock of Knowledge Graph Embedding (KGE) was laid by foundational models that sought to translate symbolic entities and relations into continuous vector spaces. These early paradigms established the core principles of KGE, interpreting relational facts through geometric operations within an embedding space. The initial approaches primarily focused on two distinct yet complementary interpretations: translational distance and semantic matching. Translational models conceptualized relations as vector translations between entities, while semantic matching models quantified the plausibility of a triple using scoring functions. As the field matured, researchers began to explore more expressive geometric transformations and alternative embedding spaces, moving beyond simple Euclidean assumptions to capture richer semantic nuances and address the limitations of earlier models. This evolution marked a critical step towards developing KGE models capable of handling the inherent complexity and diversity of real-world knowledge graphs, setting the stage for subsequent advancements in neural and context-aware embeddings \cite{cao2022, ge2023, yan2022}.

\subsection{Translational Distance Models}
Translational distance models represent one of the earliest and most intuitive paradigms in Knowledge Graph Embedding. The core idea behind these models is to interpret relations as translation operations between entity embeddings in a low-dimensional vector space. Specifically, for a given triple $(h, r, t)$, the embedding of the head entity $h$ plus the embedding of the relation $r$ should approximate the embedding of the tail entity $t$, i.e., $h + r \approx t$. This simple yet powerful geometric intuition laid the groundwork for a vast family of KGE models, which progressively introduced refinements to address the limitations of their predecessors, particularly in handling complex relational patterns. The development path in this area moved from basic linear translations to more sophisticated projections and adaptive mechanisms, aiming for enhanced expressiveness and accuracy \cite{asmara2023, cao2022}.

\subsubsection{Core Translational Approaches (TransE, TransH, TransR)}
The seminal work on translational models began with TransE (Translating Embeddings), which posited that if a triple $(h, r, t)$ holds, then the embedding of the head entity $\mathbf{h}$ plus the embedding of the relation $\mathbf{r}$ should be close to the embedding of the tail entity $\mathbf{t}$ in a vector space, i.e., $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ \cite{asmara2023, cao2022}. This model's simplicity and efficiency made it a popular baseline. However, TransE struggled with complex relation types, particularly one-to-many, many-to-one, and many-to-many relations, where a single relation vector could not adequately distinguish between multiple valid tail entities for a given head, or vice versa.

To address these limitations, TransH (Translating on Hyperplanes) was introduced \cite{wang2014}. TransH improved upon TransE by projecting entities onto a relation-specific hyperplane. For each relation $r$, TransH learns a normal vector $\mathbf{w}_r$ defining a hyperplane, and entities $h$ and $t$ are projected onto this plane as $\mathbf{h}_{\perp} = \mathbf{h} - \mathbf{w}_r^\top \mathbf{h} \mathbf{w}_r$ and $\mathbf{t}_{\perp} = \mathbf{t} - \mathbf{w}_r^\top \mathbf{t} \mathbf{w}_r$. The translational assumption then applies on this hyperplane: $\mathbf{h}_{\perp} + \mathbf{r} \approx \mathbf{t}_{\perp}$. This mechanism allowed TransH to represent different aspects of an entity for different relations, effectively handling one-to-many and many-to-one relations by allowing an entity to have distinct representations when viewed through different relational lenses. Building further, TransR (Translating on Relations) extended this idea by mapping entities and relations into distinct entity and relation spaces, respectively, and performing translations within the relation-specific space \cite{asmara2023, cao2022}. This hierarchical approach offered even greater flexibility in modeling complex relations by allowing for more nuanced transformations between different semantic contexts.

\subsubsection{Advanced Translational Models (TransA, TransD, TransMS, TranS)}
Building upon the core translational ideas, subsequent models introduced more sophisticated mechanisms to enhance expressiveness and address specific challenges. TransA (An Adaptive Approach for Knowledge Graph Embedding) \cite{jia2015, xiao2015} critically analyzed the fixed-margin loss functions prevalent in earlier translational models. It proposed an adaptive margin for each triple, arguing that a global, fixed margin is suboptimal for heterogeneous knowledge graphs. TransA introduced an adaptive weighting matrix for relations and a dynamically calculated margin, which improved performance by making the loss function more sensitive to the local characteristics of entities and relations. This addressed the limitation where different parts of a KG might require different optimal separation thresholds, as empirically shown by \cite{jia2015}.

TransD (Translating Embeddings with Dynamic Mapping Matrix) further refined the projection concept by introducing dynamic mapping matrices for both entities and relations \cite{cao2022}. Instead of a single relation-specific hyperplane (as in TransH) or separate spaces (as in TransR), TransD dynamically constructs projection vectors for head and tail entities based on both the entity and the relation, allowing for more flexible and fine-grained transformations. TransMS (Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics) \cite{yang2019} tackled complex relations by integrating multidirectional semantics, allowing relations to be interpreted in multiple directions or contexts, thereby enriching their representational capacity. TranS (Transition-based Knowledge Graph Embedding with Synthetic Relation Representation) \cite{zhang2022} proposed generating synthetic relation representations by combining head and tail entity information, aiming to capture more contextualized relational semantics. These advanced translational models collectively demonstrated a clear progression towards more adaptive, context-aware, and expressive representations, moving beyond simple vector additions to incorporate richer geometric transformations and dynamic projections.

\subsection{Semantic Matching Models}
In parallel to translational models, semantic matching models emerged as another foundational paradigm for Knowledge Graph Embedding. Instead of relying on a geometric translation property, these models quantify the plausibility of a triple $(h, r, t)$ by computing a scoring function $f(h, r, t)$. A higher score typically indicates a more plausible or valid fact. The core idea is to learn entity and relation embeddings such that valid triples receive high scores, while invalid (negative) triples receive low scores. This paradigm encompasses a variety of approaches, from tensor factorization methods that decompose the adjacency tensor of a knowledge graph to simpler linear and bilinear scoring functions. The development in this area focused on designing scoring functions that could effectively capture different types of relational semantics, including symmetry, asymmetry, and inverse relations, often balancing expressiveness with computational efficiency.

\subsubsection{Factorization-based Approaches (e.g., RESCAL, DistMult, ComplEx, HolE)}
Factorization-based models view the knowledge graph as a 3D adjacency tensor, where entries indicate the presence or absence of a relation between two entities. RESCAL (RElational Soft-Clustering Across all Link-types) was an early and influential model in this category \cite{cao2022}. It represented entities as vectors and relations as full matrices, where the score of a triple $(h, r, t)$ is computed as $\mathbf{h}^\top \mathbf{M}_r \mathbf{t}$. While powerful, RESCAL suffered from a large number of parameters due to the full relation matrices, leading to overfitting and computational challenges for very large KGs.

To address RESCAL's parameter complexity, DistMult (Diagonal Relation Matrices) simplified the relation matrices to diagonal matrices \cite{cao2022}. This reduced the scoring function to $\mathbf{h}^\top \text{diag}(\mathbf{r}) \mathbf{t}$, or equivalently, $\langle \mathbf{h}, \mathbf{r}, \mathbf{t} \rangle$, where $\mathbf{r}$ is a vector representing the relation. While highly efficient and effective for symmetric relations, DistMult inherently struggled with asymmetric relations (e.g., "parentOf" vs. "childOf") because the diagonal matrices cannot distinguish directionality. ComplEx (Complex Embeddings) \cite{cao2022} overcame this limitation by embedding entities and relations in complex vector spaces. The scoring function involves the real part of $\langle \mathbf{h}, \mathbf{r}, \overline{\mathbf{t}} \rangle$, where $\overline{\mathbf{t}}$ is the complex conjugate of $\mathbf{t}$. This allowed ComplEx to naturally handle asymmetric relations and achieve state-of-the-art performance. HolE (Holographic Embeddings) \cite{cao2022} introduced circular correlation to combine entity embeddings, offering a more compact and efficient way to capture interactions, with a scoring function based on $\mathbf{r} \cdot (\mathbf{h} \star \mathbf{t})$, where $\star$ denotes circular correlation. These factorization-based models demonstrated a clear progression from general tensor decomposition to specialized forms that balanced expressiveness with efficiency for different relational properties.

\subsubsection{Linear and Bilinear Scoring Functions (e.g., LineaRE)}
Beyond the more general factorization approaches, a class of semantic matching models directly employs linear or bilinear scoring functions to assess triple plausibility. These models often prioritize simplicity and interpretability while aiming to capture diverse relational patterns. The general form of a bilinear scoring function is $\mathbf{h}^\top \mathbf{M}_r \mathbf{t}$, where $\mathbf{M}_r$ is a relation-specific matrix. Different constraints on $\mathbf{M}_r$ lead to various models, including those discussed in factorization-based approaches (e.g., DistMult with diagonal $\mathbf{M}_r$).

A notable example in this category is LineaRE (Simple but Powerful Knowledge Graph Embedding for Link Prediction) \cite{peng2020}. LineaRE models KGE as a simple linear regression task, emphasizing both simplicity and scalability. Its key contribution lies in designing a model that explicitly captures a wide range of relational connectivity patterns, such as symmetry, antisymmetry, inversion, and composition, as well as mapping properties like one-to-one, one-to-many, many-to-one, and many-to-many. By carefully crafting its scoring function and regularization terms, LineaRE demonstrated that well-designed simpler models can achieve competitive performance by explicitly encoding diverse relational semantics. This approach contrasts with more complex neural models by showing that a clear, interpretable mathematical formulation can still yield powerful results, particularly when focusing on the explicit representation of relational properties. The development in this sub-area highlights a continuous effort to find the most efficient and interpretable mathematical forms for scoring triple plausibility.

\subsection{Early Geometric Refinements and Alternative Spaces}
While translational and semantic matching models established the initial paradigms, early research quickly identified limitations in their ability to fully capture the intricate semantics of knowledge graphs, particularly when confined to simple Euclidean vector spaces. This led to the exploration of more sophisticated geometric interpretations and alternative embedding spaces, challenging the initial assumptions about embedding space structure. These refinements aimed to improve expressiveness, handle complex relational properties more naturally, or enhance efficiency by leveraging the inherent properties of non-Euclidean geometries. This marked a significant step in the development of KGE, moving towards models that could better reflect the underlying structure and semantics of knowledge.

\subsubsection{Manifold-based Embeddings (ManifoldE)}
The concept of manifold-based embeddings emerged as a direct response to the limitations of representing entities as single points in Euclidean space, especially for relations that might imply a more complex, continuous transformation or a region rather than a precise point. ManifoldE (From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction) \cite{xiao2015} was a pioneering work in this direction. Instead of representing entities as points and relations as simple translations or matrices, ManifoldE proposed representing entities as points and relations as geometric manifolds, such as hyperplanes or spheres, in the embedding space.

This approach offered greater flexibility and expressiveness. For instance, representing a relation as a hyperplane allows for a more nuanced interpretation of "being related" than a fixed vector translation. A triple $(h, r, t)$ is considered valid if the tail entity $t$ lies on or close to the manifold defined by the head entity $h$ and relation $r$. This geometric interpretation can naturally capture complex relational patterns and their inherent uncertainty, as entities might not perfectly align with a point but rather fall within a region or along a surface. ManifoldE demonstrated that by moving beyond simple point-to-point mappings, KGE models could achieve more precise link prediction and better capture the continuous nature of certain relational semantics, paving the way for further exploration of non-Euclidean geometries.

\subsubsection{Toroidal and Cyclic Spaces (TorusE, CyclE)}
The exploration of alternative embedding spaces further pushed the boundaries of KGE, recognizing that Euclidean space might not always be the most natural or efficient geometry for representing certain types of knowledge. Toroidal and cyclic spaces offered intriguing properties for modeling relations, particularly those exhibiting periodic or cyclical characteristics. TorusE (Knowledge Graph Embedding on a Lie Group) \cite{ebisu2017} was a notable model that embedded entities and relations on a Lie group, specifically a torus. In this setup, relations are interpreted as rotations or translations on the torus. The cyclic nature of a torus can naturally capture periodic or compositional relations, and its compact nature can lead to more efficient representations.

Similarly, CyclE \cite{yang2021} explored the use of cyclic spaces for KGE, aiming to leverage their unique topological properties. A significant geometric refinement that gained prominence is RotatE (Knowledge Graph Embedding by Relational Rotation in Complex Space) \cite{sun2018}. RotatE represents entities and relations as vectors in a complex vector space. For a triple $(h, r, t)$, the model posits that the head entity embedding $\mathbf{h}$ should be approximately equal to the tail entity embedding $\mathbf{t}$ rotated by the relation embedding $\mathbf{r}$ in the complex plane, i.e., $\mathbf{h} \circ \mathbf{r} \approx \mathbf{t}$, where $\circ$ denotes element-wise product. This rotational mechanism inherently captures properties like symmetry (rotation by $\pi$), antisymmetry (rotation by an angle not $\pi$), and inversion (rotation by $-\theta$), making it highly expressive for diverse relational patterns. The shift to complex space and rotational transformations in RotatE represented a powerful geometric insight, demonstrating that non-Euclidean or extended Euclidean spaces could offer superior expressiveness and interpretability for KGE.