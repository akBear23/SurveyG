\section{Temporal and Spatiotemporal Knowledge Graph Embedding: Adapting Models for Dynamic Knowledge}
Knowledge Graphs (KGs) are increasingly recognized not as static repositories of facts, but as dynamic, evolving systems where the validity of information changes over time and, in many real-world scenarios, across space. Traditional Knowledge Graph Embedding (KGE) models, which primarily focus on static triple representations, are inherently limited in capturing these temporal dynamics, spatial relationships, and the inherent uncertainty that often accompanies real-world knowledge. This section delves into advanced KGE methodologies that explicitly address the dynamic and evolving nature of knowledge, reviewing models that adapt geometric spaces, leverage sophisticated temporal structures, and integrate spatial information with fuzziness. The rapid progression in this domain, particularly evident in recent 2024 publications, showcases a concerted effort to move beyond static representations towards more comprehensive and realistic models capable of reasoning over dynamic and uncertain knowledge. This evolution involves novel applications of geometric transformations, tensor decomposition techniques, multi-curvature spaces, and explicit time modeling, culminating in the challenging integration of spatial, temporal, and fuzzy information.

\subsection{Geometric Transformations for Temporal KGE}
Geometric transformations, particularly rotations and projections, have emerged as powerful mechanisms to model the temporal dynamics within Knowledge Graphs. Building upon the success of foundational rotation-based models like RotatE \cite{sun2018}, which represents relations as rotations in complex vector spaces, newer models adapt these operations to capture the evolving nature of facts. The core idea is that temporal changes can be effectively modeled as transformations in the embedding space, allowing entities and relations to shift or rotate through time.

TeRo (Time-aware Knowledge Graph Embedding via Temporal Rotation) \cite{xu2020} exemplifies this approach by modeling temporal relations as rotations. In TeRo, each timestamp is associated with a specific rotation matrix, allowing entities and relations to evolve their positions in the embedding space over time. This enables the model to capture temporal ordering and periodicity by applying different rotations for different time points. Similarly, ChronoR (Rotation Based Temporal Knowledge Graph Embedding) \cite{sadeghian2021} further refines the concept of temporal rotations, proposing a model that learns time-dependent rotations for relations. This allows for a more nuanced representation of how the relationship between two entities changes across various timestamps. These methods offer an intuitive way to capture the continuous evolution of knowledge, where the temporal context dictates the specific geometric transformation applied. By leveraging the compositional properties of rotations, they can model complex temporal patterns and infer missing facts at specific timestamps. However, a critical evaluation reveals that while effective for capturing smooth temporal transitions, these models might struggle with abrupt, non-linear changes or highly irregular temporal patterns, which are common in real-world dynamic KGs. Their expressiveness is also tied to the complexity of the rotation matrices, which can increase model parameters.

\subsection{Tensor Decomposition and Time-Aware Structures}
The inherent multi-dimensional nature of temporal knowledge graphs (entities, relations, time) makes tensor decomposition a natural fit for modeling time-aware structures. Tensor decomposition techniques can effectively capture the interactions between entities, relations, and time, providing a compact and interpretable representation of temporal facts.

One direct application is seen in the "Tensor Decomposition-Based Temporal Knowledge Graph Embedding" by \cite{lin2020}. This work leverages tensor factorization to decompose the 4-dimensional tensor representing (head, relation, tail, time) quadruplets into lower-dimensional embeddings for entities, relations, and time. By doing so, it can capture the time-varying nature of relationships and predict missing links at specific timestamps. A more advanced approach that explicitly models time-aware structures is ATiSE (Additive Time Series Embedding) \cite{xu2019}. ATiSE models the evolution of each entity and relation embedding as a multi-dimensional additive time series, comprising trend, seasonal, and random components. Crucially, ATiSE represents entities and relations as multi-dimensional Gaussian distributions, explicitly capturing the *temporal uncertainty* in their evolution, a significant departure from deterministic point embeddings. This allows the model to not only track changes but also quantify the confidence in those changes. Another innovative method, TeAST (Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline) \cite{li2023}, introduces a novel way to embed time by mapping it onto an Archimedean spiral. This geometric representation allows for capturing both the linear progression of time and cyclical patterns, offering a continuous and differentiable representation of temporal context. These tensor-based and time-series decomposition methods provide robust frameworks for handling the multi-faceted nature of temporal data, offering interpretability and the ability to model complex temporal patterns. However, the computational complexity of tensor decomposition can be a challenge for very large KGs, and the choice of decomposition method can significantly impact performance.

\subsection{Multi-Curvature Geometric Spaces for Temporal KGE}
The complex, often hierarchical and diverse, geometric structures inherent in real-world knowledge graphs, especially temporal ones, often cannot be adequately captured by a single Euclidean space. This limitation has driven the adoption of multi-curvature geometric spaces, including hyperbolic (negative curvature for hierarchies), hyperspherical (positive curvature for cyclic patterns), and Euclidean (zero curvature for grid-like structures) spaces. This approach allows models to adaptively choose the most suitable geometry for different parts of the KG or different types of temporal patterns.

Two prominent 2024 papers, MADE \cite{wang2024} and IME \cite{wang2024}, exemplify this trend for Temporal Knowledge Graph Completion (TKGC). MADE (Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion) proposes modeling TKGs in a combination of Euclidean, hyperbolic, and hyperspherical spaces. Its key innovation lies in a data-driven adaptive weighting mechanism that dynamically strengthens ideal spaces and weakens inappropriate ones based on the local data characteristics. This allows MADE to capture diverse geometric patterns (e.g., hierarchical, ring, chain) simultaneously, along with an innovative temporal regularization for smoothness. In parallel, IME (Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion) also leverages multi-curvature spaces but introduces a distinction between "space-shared" properties, which capture commonalities across different geometries, and "space-specific" properties, which capture unique features of each space. IME further employs an Adjustable Multi-curvature Pooling (AMP) approach to effectively integrate information from these heterogeneous spaces. Building on this, MGTCA (Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion) \cite{shang2024} integrates messages from hyperbolic, hypersphere, and Euclidean spaces within a graph neural network framework, further enhancing the ability to adapt to diverse local structures. These multi-curvature approaches represent a significant advancement, offering greater flexibility and expressiveness than single-geometry models, particularly for the intricate and evolving structures found in temporal KGs. However, the complexity of learning and integrating embeddings across multiple geometries, as well as the computational overhead, remain active areas of research.

\subsection{Explicit Time Modeling and Advanced Architectures}
Beyond embedding time implicitly through transformations or tensor structures, some models explicitly represent time as a distinct entity or through dedicated architectural components. This direct approach often provides clearer interpretability and can be more robust for certain temporal reasoning tasks.

HyTE (Hyperplane-based Temporally aware Knowledge Graph Embedding) \cite{dasgupta2018} is a pioneering work that explicitly incorporates time by associating each timestamp with a corresponding hyperplane in the entity-relation embedding space. This allows HyTE to not only perform temporally-guided KG inference but also predict temporal scopes for facts with missing time annotations, a crucial capability for incomplete KGs. As discussed earlier, ATiSE \cite{xu2019} also falls into this category by explicitly modeling entity and relation evolution as additive time series, capturing both trend and seasonality. More recently, advanced neural architectures have been adapted for explicit temporal modeling. TARGAT (Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding) \cite{xie2023} leverages a graph attention network to dynamically learn time-aware relational embeddings, allowing the model to focus on relevant temporal contexts. For dynamic KGs in evolving environments, MetaHG (Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning) \cite{sun2024} introduces a meta-learning strategy. This approach efficiently updates incremental knowledge and handles unseen entities by learning to adapt quickly to new temporal snapshots, mitigating spatial deformation through a hybrid GNN/HGNN framework. Looking ahead, TGformer (A Graph Transformer Framework for Knowledge Graph Embedding) \cite{shi2025} proposes a general graph transformer framework capable of handling both static and temporal KGs. It constructs context-level subgraphs and uses a Knowledge Graph Transformer Network (KGTN) to integrate triplet-level and graph-level structural features, offering a unifying architectural approach for dynamic knowledge. These explicit time modeling strategies and advanced architectures collectively enhance the ability of KGE models to understand, predict, and reason over the dynamic evolution of knowledge.

\subsection{Fuzzy Spatiotemporal KGE}
The most advanced challenge in dynamic KGE involves integrating not only temporal dynamics but also spatial information and the inherent uncertainty or fuzziness of real-world knowledge. This leads to the complex domain of fuzzy spatiotemporal Knowledge Graph Embedding, where models must represent facts that are valid within specific geographic regions, time intervals, and with varying degrees of certainty.

A groundbreaking contribution in this area is FSTRE (Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation) \cite{ji2024}. FSTRE introduces a strongly adaptive model for fuzzy spatiotemporal RDF embedding, explicitly addressing uncertain and dynamic knowledge. Its core lies in embedding spatial and temporal information using projection and rotation within a complex vector space. Crucially, FSTRE integrates fine-grained fuzziness into each element of spatiotemporal five-tuples through the modal length of anisotropic vectors, enabling it to capture rich interactions between crisp/static and fuzzy spatiotemporal knowledge using geometric operations. Building directly on this foundation, "Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding" \cite{ji2024} extends the paradigm to complex reasoning tasks. This work employs quaternion embedding to jointly represent spatiotemporal entities, modeling relations as rotations, and incorporates uncertainty through a scoring function's bias factor. By exploiting the noncommutative compositional pattern of quaternions, it constructs more accurate multihop paths, thereby improving path reasoning performance in fuzzy spatiotemporal contexts. Furthermore, the importance of spatial context is also highlighted in works like \cite{hu2024}'s GeoEntity-type constrained KGE, which focuses on predicting natural-language spatial relations. These models represent the cutting edge, moving KGE towards a more comprehensive representation of real-world knowledge, where facts are often uncertain, localized in space, and valid only for specific durations. The integration of fuzziness, spatiality, and temporality presents significant challenges in model design, computational efficiency, and interpretability, marking a fertile ground for future research.