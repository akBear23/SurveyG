\section{Leveraging Auxiliary Information for Enriched Embeddings}
While foundational Knowledge Graph Embedding (KGE) models primarily focus on learning representations from direct (head, relation, tail) triples, their expressiveness and reasoning capabilities are often limited by this narrow scope. Real-world knowledge graphs (KGs) are rich with diverse auxiliary information, including textual descriptions, explicit entity types and attributes, logical constraints, and complex multi-hop relational paths. Leveraging these supplementary data sources significantly enhances the quality and interpretability of learned embeddings, enabling KGE models to capture richer semantics, improve reasoning, and address challenges like data sparsity and ambiguity \cite{ge2023}. This section delves into advanced KGE methodologies that integrate such diverse auxiliary information, moving beyond simple structural triples to build more nuanced, context-aware, and robust representations. We explore the transformative impact of textual data and pre-trained language models, the utility of explicit type and attribute information, the guidance offered by logical rules, and the power of modeling broader contextual and path-based information.

\subsection{Textual Descriptions and Pre-trained Language Models (PLMs)}
The integration of textual descriptions and the transformative power of Pre-trained Language Models (PLMs) have revolutionized Knowledge Graph Embedding by infusing rich linguistic semantics into structural representations. Traditional KGE models, operating solely on symbolic triples, often struggle with capturing the nuanced meanings of entities and relations, especially for those with sparse connections. Textual descriptions, such as entity abstracts or Wikipedia snippets, provide a wealth of semantic context that can disambiguate entities and enrich their embeddings.

Early approaches to integrating text often involved simple concatenation of text-based embeddings (e.g., from word2vec) with structural embeddings, or using projection functions to map textual features into the embedding space. For instance, SSP (Semantic Space Projection) \cite{xiao2016} projected entity descriptions into the KGE space to enhance representations. However, these methods were limited by the shallow semantic understanding of earlier language models. The advent of large-scale PLMs, such as BERT, marked a paradigm shift. PLMs, pre-trained on vast text corpora, possess an unparalleled ability to capture deep contextualized semantic representations. Models like CoKE (Contextualized Knowledge Graph Embedding) \cite{wang2019} demonstrated the power of leveraging contextual information from entity descriptions and relational paths to enrich embeddings, moving beyond static representations. More recently, the synergy between KGE and PLMs has deepened. \cite{shen2022} proposed Joint Language Semantic and Structure Embedding for Knowledge Graph Completion, which effectively combines the linguistic understanding of PLMs with the structural insights from KGE. This allows for a more holistic representation that benefits from both symbolic and sub-symbolic knowledge. The application of PLMs extends to specialized domains, as seen in Marie and BERT \cite{zhou2023}, a KGE-based question answering system for chemistry that leverages BERT for enhanced semantic understanding. Furthermore, the concept of contextualized embeddings is crucial for applications like explainable recommendation, where CKGE (Contextualized Knowledge Graph Embedding) \cite{yang2023} integrates motivation-aware information using a novel KG-based Transformer to provide explainable talent training course recommendations. This evolution from simple text integration to sophisticated PLM-driven contextualization represents a significant step towards more semantically rich and human-like KGE.

\subsection{Entity Types and Attributes}
Beyond the core triple structure, explicit entity type and attribute information serve as invaluable auxiliary data for learning more nuanced and robust Knowledge Graph Embeddings. Entity types provide a categorical hierarchy, defining the semantic class of an entity (e.g., Person, Organization, Location), which can guide the embedding space to cluster similar entities and enforce type-aware constraints. Attributes, on the other hand, offer descriptive properties (e.g., 'age' for a 'Person', 'capital' for a 'Country'), enriching the entity's intrinsic semantic profile.

The integration of entity types helps in disambiguation and refining relational patterns. For example, TransET \cite{wang2021} explicitly incorporates entity types into the embedding process, allowing relations to be interpreted differently based on the types of the entities involved. Similarly, \cite{he2023} proposed a type-augmented KGE framework for knowledge graph completion, demonstrating how type information can provide a stronger inductive bias for learning. The concept of differentiating concepts and instances for KGE \cite{lv2018} also highlights the importance of type-like distinctions. Ontological information, which includes type hierarchies and class properties, is particularly potent. OntoEA \cite{xiang2021} is an ontology-guided entity alignment method that jointly embeds KGs and their associated ontologies, utilizing class hierarchy and disjointness constraints to enhance alignment accuracy. This shows a clear progression towards leveraging higher-level schema information. Furthermore, attributes provide fine-grained details that can improve embedding quality and address data imperfections. \cite{zhang2024} introduced AEKE (Attributed Error-aware Knowledge Embedding), a framework that leverages entity attributes to guide KGE learning and mitigate the impact of erroneous triples. By calculating confidence scores based on attribute-structure homogeneity, AEKE adaptively weights aggregation and loss functions, making embeddings more robust to noise. The use of two vectors in TransD \cite{ji2015}, one for entity meaning and another for constructing a dynamic mapping matrix, implicitly allows for capturing diverse entity characteristics that could be influenced by attributes. Even in specialized domains, type information is crucial, as seen in \cite{hu2024}'s GeoEntity-type constrained KGE for predicting natural-language spatial relations, where geographical entity types provide essential context. These methods collectively underscore the critical role of explicit type and attribute information in creating more semantically rich, accurate, and robust KGEs.

\subsection{Logical Rules and Constraints}
The inherent logical structure of knowledge graphs, often expressed through rules and constraints, provides a powerful form of auxiliary information that can guide and regularize the embedding learning process. Incorporating logical rules, such as transitivity (e.g., if A is a 'partOf' B and B is 'partOf' C, then A is 'partOf' C), symmetry, or inverse relations, helps to enforce consistency, improve reasoning capabilities, and alleviate data sparsity by inferring missing facts.

Early efforts to integrate logical rules often involved adding regularization terms to the KGE loss function. For instance, \cite{ding2018} showed how simple constraints could be used to improve KGE. RUGE \cite{guo2017} introduced an iterative guidance mechanism from soft rules, where rules are treated as soft constraints that influence the embedding updates. This approach allows the model to learn embeddings that are consistent with known logical patterns without imposing rigid, hard constraints that might conflict with noisy real-world data. Similarly, \cite{guo2020} explored Knowledge Graph Embedding Preserving Soft Logical Regularity, further demonstrating the benefits of incorporating logical patterns as soft guidance. The work by \cite{yoon2016} also focused on translation-based KGE models that preserve logical properties of relations. A more explicit approach is taken by RulE \cite{tang2022}, which directly learns rule embeddings to perform knowledge graph reasoning. This allows for a more direct integration of complex logical patterns into the embedding space. A significant theoretical advancement in this direction is HolmE (Knowledge graph embedding closed under composition) \cite{zheng2024}. While not explicitly a rule-based system, HolmE proposes a KGE model where the relation embedding space is explicitly "closed under composition." This property ensures that the composition of any two relation embeddings remains within the space, fundamentally enhancing the model's ability to handle under-represented composition patterns and extrapolate to unseen relations. This theoretical grounding provides a robust framework for compositional reasoning, a key aspect of logical inference. By leveraging these logical rules and constraints, KGE models move beyond mere statistical pattern recognition to incorporate a deeper, more consistent, and inferential understanding of knowledge.

\subsection{Path-based and Contextual Information}
Modeling multi-hop paths and broader contextual information is crucial for capturing complex relational semantics and improving reasoning capabilities in Knowledge Graph Embedding. While direct triples offer local insights, the true richness of a KG often lies in the indirect connections and the diverse paths linking entities. These paths reveal higher-order relationships, provide richer context for entity disambiguation, and are fundamental for complex reasoning tasks.

Graph Neural Networks (GNNs) and attention mechanisms have become instrumental in aggregating path-based and contextual information. GNNs iteratively propagate and aggregate information from an entity's neighborhood, allowing embeddings to capture multi-hop structural context. For instance, DisenKGAT (Knowledge Graph Embedding with Disentangled Graph Attention Network) \cite{wu2021} employs a relation-aware aggregation mechanism within a graph attention framework to learn disentangled entity representations. This allows different "components" of an entity's embedding to attend to different parts of its neighborhood, capturing context-dependent meanings. Similarly, models like Logic Attention Based Neighborhood Aggregation \cite{wang2018} use attention to selectively focus on relevant neighbors. Beyond direct neighbors, the concept of "graph context modeling" \cite{tang2019} emphasizes the importance of broader structural information. The advent of Transformer-based architectures has further pushed the boundaries of contextualized embeddings. The Position-Aware Relational Transformer \cite{li2023} explicitly incorporates positional information into its self-attention mechanism, allowing it to distinguish between different roles of entities and relations within a triple or path. This is crucial for understanding the flow of information along multi-hop paths. The most ambitious recent development is TGformer (A Graph Transformer Framework for Knowledge Graph Embedding) \cite{shi2025}, which integrates both triplet-level and graph-level structural features in static and temporal KGs. TGformer constructs context-level subgraphs for predicted triplets and employs a Knowledge Graph Transformer Network (KGTN) to fully explore these multi-structural features, aiming to overcome limitations of purely triplet-based or graph-based methods. For complex data types, multi-hop reasoning is also being tackled with advanced embedding spaces; for example, \cite{ji2024} uses quaternion embedding for multihop queries in fuzzy spatiotemporal RDF KGs. These advancements collectively demonstrate a strong trend towards KGE models that can effectively leverage the rich, interconnected nature of knowledge graphs to achieve deeper semantic understanding and more powerful reasoning.