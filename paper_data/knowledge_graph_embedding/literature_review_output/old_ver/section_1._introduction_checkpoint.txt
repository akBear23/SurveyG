\section{Introduction}
Knowledge Graphs (KGs) have emerged as a cornerstone paradigm for representing structured knowledge, providing a rich, interpretable framework for capturing real-world facts and their interconnections \cite{dai2020, yan2022, choudhary2021}. Comprising entities (nodes) and relations (edges) forming triples (subject, predicate, object), KGs serve as a foundational component for a myriad of artificial intelligence applications, ranging from semantic search and intelligent assistants to complex reasoning systems \cite{huang2019, zhou2023}. Their ability to organize vast amounts of heterogeneous information into a coherent, machine-readable format underscores their growing importance in advancing AI capabilities. However, the symbolic and sparse nature of traditional KGs presents significant computational challenges, particularly when dealing with large-scale graphs or integrating them into statistical machine learning models. This inherent sparsity and the sheer volume of data necessitate more efficient and continuous representations to unlock their full potential.

This challenge has propelled the development of Knowledge Graph Embedding (KGE), a crucial technique that transforms symbolic entities and relations into low-dimensional, continuous vector spaces, often referred to as embeddings \cite{cao2022, ge2023, dai2020}. KGE models aim to capture the semantic and structural properties of KGs within these dense vector representations, thereby facilitating computational efficiency and enabling a wide array of downstream AI tasks. The field has witnessed rapid evolution, moving from simpler geometric interpretations to sophisticated neural architectures and, more recently, integrating large pre-trained language models \cite{ge2023}. This literature review provides a timely and comprehensive synthesis of the major themes, developments, and future directions within this rapidly evolving KGE landscape, offering a structured understanding of its progression and current state-of-the-art.

\subsection{Background: Knowledge Graphs and Their Significance}
Knowledge Graphs are a powerful means of organizing and representing factual information, structured as a collection of triples $(h, r, t)$, where $h$ is the head entity, $r$ is the relation, and $t$ is the tail entity \cite{dai2020, yan2022}. These structured repositories of knowledge are vital for developing intelligent systems that can understand, reason, and interact with the world in a human-like manner. Their significance spans numerous AI applications, including enhancing search engine relevance, powering recommender systems \cite{sun2018, yang2023}, facilitating complex question answering \cite{huang2019, zhou2023}, and enabling semantic interoperability through entity alignment across disparate KGs \cite{sun2018, zhang2019, fanourakis2022, zhu2024}. For instance, in talent training course recommendation, KGs can capture intricate relationships between talents, courses, and motivations, leading to more explainable and personalized suggestions \cite{yang2023}. Despite their immense utility, KGs face inherent challenges such as data sparsity, incompleteness, and the difficulty of integrating heterogeneous information, which limit their direct applicability in many machine learning paradigms \cite{dai2020}. Furthermore, the dynamic nature of real-world knowledge, where facts have temporal validity, adds another layer of complexity that traditional static KGs often fail to capture \cite{dasgupta2018, xu2019}. Addressing these limitations is paramount for advancing the capabilities of AI systems built upon knowledge-rich foundations.

\subsection{The Role of Knowledge Graph Embedding}
Knowledge Graph Embedding (KGE) addresses the aforementioned challenges by transforming symbolic entities and relations into low-dimensional, continuous vector representations in a semantic space \cite{cao2022, ge2023}. This transformation is crucial for several reasons. Firstly, it significantly enhances computational efficiency, as dense vector operations are far more amenable to modern machine learning algorithms than sparse symbolic manipulations \cite{dai2020}. Secondly, KGE facilitates the integration of structural knowledge into various downstream AI applications, effectively bridging the gap between symbolic knowledge representation and statistical learning models. These applications include, but are not limited to, link prediction (or knowledge graph completion), where missing facts are inferred \cite{wu2021, rosso2020, rossi2020}, entity alignment, which matches equivalent entities across different KGs \cite{sun2018, zhang2019}, and question answering systems that leverage semantic understanding from embeddings \cite{huang2019, zhou2023}. KGE also plays a vital role in specialized domains like talent training course recommendation, where contextualized embeddings can provide explainable recommendations by capturing motivation-aware information \cite{yang2023}.

The evolution of KGE models reflects a continuous effort to capture increasingly complex semantic and structural patterns. Early models, like TransE and TransH \cite{jia2015}, focused on translational properties, representing relations as translations between entity embeddings. Subsequent advancements introduced more sophisticated geometric transformations, such as rotations in complex space (RotatE \cite{sun2018}) or compound operations (CompoundE \cite{ge2022}, CompoundE3D \cite{ge2023}). The field has also progressed to incorporate neural network-based approaches \cite{yan2022}, disentangled representations \cite{wu2021}, and, most recently, the powerful capabilities of pre-trained language models (PLMs) to leverage textual descriptions for enhanced embedding quality \cite{ge2023, shen2022}. Furthermore, challenges such as parameter efficiency \cite{chen2023}, the handling of temporal dynamics \cite{xu2019, dasgupta2018}, and the impact of hyperparameters on embedding quality \cite{lloyd2022} have driven innovative solutions, making KGE a dynamic and central area of research in modern AI.

\subsection{Scope and Organization of the Review}
This literature review aims to provide a comprehensive and structured overview of the rapidly evolving field of Knowledge Graph Embedding, synthesizing major themes and recent developments up to 2023. Drawing upon recent surveys \cite{ge2023, cao2022, yan2022, dai2020}, this review categorizes and critically evaluates prominent KGE models, highlighting their methodological contributions and the underlying theoretical perspectives. We acknowledge the foundational taxonomies that classify models based on their operational mechanisms, such as distance-based methods and semantic matching-based methods \cite{ge2023}, or by their mathematical representation spaces (algebraic, geometric, analytical) \cite{cao2022}.

Our review will systematically explore the progression of KGE techniques, from foundational translational models \cite{jia2015} to advanced neural architectures, including those that leverage disentangled representations \cite{wu2021} and integrate external information like textual descriptions and pre-trained language models \cite{ge2023, shen2022}. We will critically analyze how these models address challenges such as parameter efficiency \cite{chen2023}, the modeling of temporal dynamics \cite{xu2019, dasgupta2018}, and the need for explainable outputs in applications like recommendation systems \cite{yang2023}. Furthermore, we will discuss practical considerations, including the impact of negative sampling strategies \cite{shan2018, madushanka2024} and hyperparameter tuning \cite{lloyd2022} on embedding quality. By providing a timely synthesis and identifying key development directions, particularly the synergy between KGE and large language models, this review seeks to offer valuable insights for researchers and practitioners navigating the complex and promising landscape of Knowledge Graph Embedding.