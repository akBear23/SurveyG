\subsection{Early KGE Models}

The emergence of Knowledge Graph Embedding (KGE) models marks a significant advancement in the representation of complex relational data. Early models such as TransE, TransH, and TransR laid the foundational framework for representing relations as geometric transformations in vector spaces, addressing the inherent challenges of knowledge graph incompleteness.

TransE \cite{bordes2013} was one of the first models to propose a simple and effective method for knowledge graph embedding by representing relationships as translations in the embedding space. It assumes that for a given triplet $(h, r, t)$, the head entity $h$ plus the relation $r$ should approximate the tail entity $t$, formalized as $h + r \approx t$. However, TransE struggles with complex relation types, such as one-to-many and many-to-many relationships, due to its rigid representation that enforces a single vector per entity across all relations.

To address these limitations, TransH \cite{wang2014} introduced the concept of projecting entities onto relation-specific hyperplanes, allowing for more flexible representations. This model effectively captures the nuances of complex relations by enabling each entity to have different representations depending on the relation it participates in. The scoring function in TransH incorporates a normal vector to define the hyperplane, thus improving the model's ability to handle reflexive and one-to-many relations.

Building on the advancements of TransH, TransR \cite{lin2015b} further refined the approach by allowing entities to be represented in different vector spaces for different relations. This model employs a mapping matrix to project entities into relation-specific spaces, thus enhancing the expressiveness of the embeddings. However, while TransR improves upon the flexibility of representation, it introduces additional complexity and computational overhead, which can be a drawback for large-scale applications.

In a notable shift, the model RotatE \cite{sun2018} introduced a novel geometric paradigm by representing relations as rotations in complex space. This model is capable of capturing various relational patterns, including symmetry, inversion, and composition, simultaneously. By mapping entities and relations to a complex vector space, RotatE effectively addresses the shortcomings of earlier models, particularly in handling diverse relational semantics. It outperforms many existing models in link prediction tasks, demonstrating the effectiveness of its approach.

Despite these advancements, early KGE models still face challenges in fully leveraging the rich semantics present in knowledge graphs. For instance, while TransH and TransR improve upon TransE's limitations, they still rely on fixed representations that may not adapt well to the dynamic nature of knowledge graphs. Moreover, the reliance on specific geometric interpretations can limit the models' applicability to more complex relational structures.

In conclusion, early KGE models like TransE, TransH, and TransR established crucial methodologies for embedding knowledge graphs by representing relations as geometric transformations. However, subsequent innovations, particularly with RotatE, highlight the ongoing need for models that can flexibly and effectively capture the complexities of relational semantics. Future research may focus on integrating these geometric representations with additional contextual information or exploring hybrid approaches that combine the strengths of various KGE methodologies to enhance the robustness and expressiveness of knowledge graph embeddings.
```