\subsection*{Translational Models: From TransE to TransD}

The embedding of knowledge graphs into continuous vector spaces has been significantly advanced by translational models, which interpret relations as translation operations between entity embeddings. This paradigm, pioneered by TransE, posits that for a valid triple $(h, r, t)$, the embedding of the head entity $h$ plus the relation embedding $r$ should approximate the embedding of the tail entity $t$, i.e., $h + r \approx t$. While elegant and efficient, TransE struggled to accurately model complex relation patterns such as one-to-many, many-to-one, and many-to-many relationships, as it assigned a single, fixed vector representation to each entity regardless of the specific relation it participated in. This limitation often led to entities involved in such complex relations having nearly identical embeddings, diminishing the model's expressive power.

To address TransE's shortcomings, \textcite{wang2014} introduced \textbf{TransH}, a notable advancement that allows entities to have distinct representations when participating in different relations. TransH models each relation $r$ not merely as a translation vector, but as a combination of a relation-specific hyperplane (defined by its normal vector $w_r$) and a translation vector $d_r$ that lies within this hyperplane. For a given triple $(h, r, t)$, the head and tail entity embeddings are first projected onto the relation's hyperplane, yielding $h_{\perp}$ and $t_{\perp}$, before the translation $d_r$ is applied, with the scoring function measuring $||h_{\perp} + d_r - t_{\perp}||_2^2$. This innovative approach, coupled with a Bernoulli negative sampling strategy, significantly improved the handling of complex relations while maintaining computational efficiency comparable to TransE.

Building upon the concept of relation-specific transformations, \textcite{lin2015} further refined the translational paradigm with \textbf{TransR} and its extension, \textbf{CTransR}. Recognizing that entities and relations might reside in different semantic spaces, TransR proposed projecting entities from their original entity space into a relation-specific space before applying the translation. This is achieved through a relation-specific projection matrix $M_r$, such that the projected entities $M_r h$ and $M_r t$ are translated by $r$ within the relation space. CTransR extended this by clustering relations into multiple groups, each with its own projection matrix, to better capture the diverse semantics within a single relation type, thereby enhancing the model's ability to discriminate between fine-grained relational meanings. However, these models, particularly due to the use of explicit projection matrices, could incur higher computational costs and parameter counts.

The pursuit of greater expressiveness and efficiency culminated in \textbf{TransD} \textcite{ji2015}, which offered a more fine-grained and scalable solution. TransD addresses the diversity of *both* entities and relations by representing each with two distinct vectors: one for its intrinsic meaning and another for dynamically constructing a mapping matrix. This dynamic construction allows for flexible projections without the need for explicit, large projection matrices, significantly reducing the number of parameters and avoiding computationally intensive matrix-vector multiplications. By enabling entities and relations to adapt their projection based on their specific context, TransD demonstrated superior performance in tasks like triplet classification and link prediction, particularly for large-scale knowledge graphs, by effectively capturing the nuanced interactions between diverse entity and relation types.

In summary, the evolution from TransE to TransD showcases a progressive refinement of the translational embedding paradigm. Each subsequent model systematically addressed the limitations of its predecessors, moving from a simplistic universal translation to increasingly sophisticated, relation- and entity-aware transformations. This trajectory highlights a continuous effort to enhance the models' capacity to handle the inherent complexity and diversity of real-world knowledge graphs, laying a robust foundation for subsequent advancements in knowledge graph embedding.