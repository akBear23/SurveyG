\subsection*{Translational Models: Geometric Transformations in Vector Spaces}

Knowledge graph embedding (KGE) models often draw inspiration from geometric intuitions to represent entities and relations in continuous vector spaces, where relational patterns are captured through transformations. This subsection traces the evolution of translational distance models, which are predicated on the geometric intuition that a relation corresponds to a translation vector connecting head and tail entities, formalizing the notion that $h + r \approx t$.

The foundational model in this paradigm is \textit{TransE} (Translating Embeddings) \cite{bordes2013transe}. Introduced in 2013, TransE posits that for a valid triple $(h, r, t)$, the embedding of the head entity $h$ plus the embedding of the relation $r$ should approximately equal the embedding of the tail entity $t$ in a low-dimensional vector space. This simple yet effective formulation, typically optimized by minimizing a distance function like $||h + r - t||_L$, offered an intuitive and computationally efficient framework for knowledge graph completion. Despite its elegance and initial success, TransE faced inherent limitations, particularly in modeling complex relational patterns such as symmetric relations (e.g., "is\_sibling\_of") where $h+r \approx t$ and $t+r \approx h$ would imply $r \approx 0$, or 1-to-N, N-to-1, and N-to-N relations where multiple entities might share the same head or tail for a given relation, forcing their embeddings to be undesirably close.

To address TransE's shortcomings, a lineage of direct successors emerged, progressively increasing model complexity to gain expressiveness. \textit{TransH} (Translating on Hyperplanes) \cite{wang2014transh} was one of the first to tackle the issues with 1-to-N and N-to-1 relations. Instead of performing translations directly in the entity space, TransH projects entities onto a relation-specific hyperplane before the translation operation. For each relation $r$, a normal vector $w_r$ defines its hyperplane, and entities $h$ and $t$ are projected onto this plane, becoming $h_r$ and $t_r$, such that $h_r + r \approx t_r$. This mechanism allows an entity to have different representations for different relations, thereby mitigating the problem of multiple entities being forced into close proximity.

Building upon the idea of relation-specific contexts, \textit{TransR} (Translating on Relations) \cite{lin2015transr} further enhanced expressiveness by projecting entities into relation-specific \textit{spaces}, rather than just hyperplanes. TransR introduces distinct entity and relation spaces, along with a mapping matrix $M_r$ for each relation $r$ to project entity embeddings from the entity space to the relation space. The translation $h_r + r \approx t_r$ then occurs within this relation-specific space. This separation of entity and relation semantics allows for a more nuanced representation of complex relations, as entities can exhibit different aspects or roles depending on the relation in question. A variant, \textit{CTransR}, further refines this by clustering relations into types and learning different relation vectors and projection matrices for each type, capturing finer-grained semantics.

The concept of dynamic mapping matrices was introduced by \textit{TransD} (Translating with Dynamic Mapping Matrix) \cite{ji2015transd}, which aimed to improve upon TransR's parameter efficiency and adaptability. TransD constructs the projection matrices dynamically based on both the entity and relation vectors, rather than learning a fixed matrix for each relation. This approach allows for more flexible and adaptive projections, as the mapping from entity space to relation space is influenced by the specific entity involved, leading to a more parameter-efficient model while maintaining or improving expressiveness.

The culmination of this geometric paradigm is exemplified by \textit{RotatE} (Knowledge Graph Embedding by Relational Rotation in Complex Space) \cite{sun2018}. RotatE re-frames relations not as simple translations, but as element-wise rotations in a complex vector space. For a triple $(h, r, t)$, RotatE models the relation $r$ as an element-wise rotation from $h$ to $t$, such that $t \approx h \odot r$, where $\odot$ denotes the Hadamard (element-wise) product, and each component of $r$ is constrained to have a modulus of 1. This elegant formulation inherently captures key relational patterns that previous translational models struggled with: symmetry (e.g., $r$ corresponds to a rotation by $\pi$), antisymmetry (e.g., $r$ and $r^{-1}$ are distinct), and composition (e.g., if $r_1$ and $r_2$ are rotations, then $r_1 \odot r_2$ is also a rotation representing their composition). RotatE demonstrated state-of-the-art performance across various benchmarks, setting a new point of reference for its ability to model these diverse patterns simultaneously and efficiently, further enhanced by its proposed self-adversarial negative sampling strategy during training.

While these translational and geometric models have significantly advanced the field of KGE by providing intuitive and increasingly expressive frameworks, their reliance on predefined geometric operations (translation, projection, rotation) can still limit their capacity to capture highly intricate or implicit semantic patterns. The trade-off between model simplicity and expressiveness remains a central challenge, often leading to models with more parameters or complex training procedures. Future directions may involve hybrid approaches that combine geometric principles with more flexible, data-driven neural architectures to capture an even broader spectrum of relational semantics.