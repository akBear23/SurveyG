\subsection{Semantic Matching and Tensor Factorization Approaches}

Moving beyond the strict geometric transformations of translational models, an alternative foundational paradigm in knowledge graph embedding (KGE) focuses on direct semantic matching or tensor factorization to model complex relations. These approaches aim to capture richer semantic interactions by representing relations as more expressive mathematical structures that directly interact with entity embeddings.

A seminal contribution in this area is RESCAL \cite{Nickel2016}, which introduced a tensor factorization approach to KGE. In RESCAL, relations are represented as full matrices that directly interact with entity vectors, allowing for a richer, more direct semantic matching between entities through the relation. Specifically, for a triple $(h, r, t)$, its validity is scored by $h^T M_r t$, where $h$ and $t$ are entity vectors and $M_r$ is a relation-specific matrix. This was a key innovation in capturing complex interactions and non-linear patterns, moving beyond simple vector additions or projections. However, a significant limitation of RESCAL was its high computational cost and large number of parameters due to the full matrix representation for each relation, which hindered its scalability to very large knowledge graphs.

Building upon the semantic matching paradigm, ComplEx \cite{Trouillon2016} further advanced the field by introducing complex-valued embeddings for both entities and relations. ComplEx models the validity of a triple $(h, r, t)$ using a Hermitian dot product, $\text{Re}(\langle \mathbf{h}, \mathbf{r}, \bar{\mathbf{t}} \rangle)$, where $\mathbf{h}, \mathbf{r}, \mathbf{t}$ are complex-valued embeddings and $\bar{\mathbf{t}}$ is the conjugate of $\mathbf{t}$. This elegant formulation naturally models symmetric and antisymmetric relations without requiring explicit constraints, a significant advantage over many translational models. ComplEx offered a more efficient solution for capturing these specific relational patterns compared to the more parameter-heavy RESCAL, demonstrating how algebraic operations with higher-dimensional representations can capture nuanced semantic relationships more effectively.

The flexibility of semantic matching and tensor factorization has also enabled the integration of richer structural and logical information. For instance, \cite{zhang2018} extended models like DistMult (a simplified version of RESCAL where relation matrices are diagonal) by incorporating a Hierarchical Relation Structure (HRS). This approach leverages a three-layer hierarchy of relation clusters, individual relations, and fine-grained sub-relations to enrich the embedding process, allowing for more nuanced semantic interactions by explicitly modeling the structural context of relations. Similarly, the Soft Logical Rule Embedding (SLRE) framework proposed by \cite{guo2020} demonstrated how to preserve soft logical regularities by representing relations as bilinear forms and deriving a novel rule-based regularization that directly enforces constraints from soft rules. This method showcases the paradigm's ability to integrate uncertain logical knowledge, making the complexity of rule learning independent of the entity set size and improving scalability for rule-guided embedding.

Despite their expressiveness, models within this paradigm, particularly those involving matrix operations, often face challenges related to computational cost and memory footprint. Addressing these practical limitations, the Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE) framework \cite{li2021} was introduced. NS-KGE provides a method to train factorization-based KGE models (like DistMult, ComplEx, and SimplE) without relying on negative sampling, which traditionally introduces instability and suboptimal accuracy. By mathematically re-deriving and re-organizing the non-sampling square loss function, NS-KGE disentangles entity and relation embeddings, making full-data training computationally and space-efficient. This innovation directly tackles the "higher computational costs" often associated with tensor factorization approaches, enabling more stable and accurate embeddings by considering all negative instances.

In conclusion, semantic matching and tensor factorization approaches have significantly enriched knowledge graph embedding by providing mathematical frameworks that move beyond simple geometric transformations. Models like RESCAL and ComplEx, along with their extensions and efficiency improvements, offer greater expressiveness and the ability to capture complex relational semantics, including symmetry, anti-symmetry, and the integration of hierarchical or logical structures. While these approaches often entail higher computational demands, ongoing research continues to refine their efficiency and scalability, making them powerful tools for nuanced knowledge representation.