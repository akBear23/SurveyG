\subsection{Entity Alignment}
The integration of heterogeneous knowledge graphs (KGs) is a fundamental challenge in knowledge engineering, primarily revolving around the task of entity alignment (EA). This process involves identifying equivalent entities across different KGs to facilitate their seamless integration and enrichment. Knowledge Graph Embedding (KGE) has emerged as a powerful paradigm to address this, by projecting entities and relations into a continuous vector space where semantic similarity can be directly computed, thereby enabling the discovery of cross-KG correspondences.

Early KGE-based approaches for entity alignment often grappled with significant challenges, notably the scarcity of labeled training data, which frequently led to suboptimal precision and robustness. To mitigate this, \textcite{sun2018} introduced \textit{BootEA}, a pioneering bootstrapping approach. BootEA iteratively expands the training data by labeling likely alignments, employing a global optimization strategy based on max-weighted matching and an alignment editing method to resolve conflicts and reduce error accumulation. This method was innovative in its use of a limit-based objective function and $\epsilon$-truncated negative sampling to learn alignment-oriented embeddings, significantly improving performance in low-resource settings. However, like any bootstrapping technique, BootEA faced the inherent risk of noise amplification if initial embeddings or iteratively labeled data were imperfect, potentially propagating errors throughout the training process. Building on the semi-supervised learning paradigm, \textcite{pei2019} further addressed data scarcity with \textit{SEA}, which uniquely incorporated adversarial training to account for entity degree differences. This innovation enhanced the robustness of KGEs by ensuring more consistent alignment accuracy across entities with varying frequencies, a crucial aspect often overlooked by methods sensitive to data distribution imbalances, where high-degree entities might dominate embedding learning.

Recognizing that entities possess diverse characteristics beyond just their relational structure, \textcite{zhang2019} proposed \textit{MultiKE}, an innovative framework that unified multiple entity 'views'---namely, name, relation, and attribute---into a comprehensive embedding framework. MultiKE departed from previous approaches by jointly optimizing view-specific embeddings and, crucially, introduced "soft alignment" for relations and attributes, thereby reducing the heavy dependency on pre-existing seed alignments for these components. This multi-view approach significantly enhanced the richness of entity representations by integrating heterogeneous features. However, a critical challenge in such multi-view frameworks lies in effectively balancing and combining these diverse views, as improper weighting or potential negative transfer between views can hinder overall performance, adding complexity to feature engineering and model optimization. While these methods advanced the field by leveraging various entity features, they often overlooked higher-level semantic constraints. \textcite{xiang2021} presented \textit{OntoEA}, the first comprehensive framework to integrate ontological schema, including class hierarchies and disjointness axioms, into joint KG-ontology embedding. OntoEA directly addressed "class conflict" errors, a significant source of false positives in previous embedding-based EA methods, by explicitly modeling and learning inter-class conflicts through a novel Class Conflict Matrix and associated loss functions, thereby improving semantic consistency and leveraging richer background knowledge.

Leveraging the powerful neighborhood aggregation capabilities of Graph Neural Networks (GNNs), as detailed in Section 4.3, a new wave of entity alignment models has emerged to better capture structural similarities. GNNs inherently excel at aggregating multi-hop structural information from an entity's neighborhood, enabling them to learn more context-rich and discriminative embeddings for alignment. Unlike traditional KGEs that primarily focus on triple-level interactions, GNN-based models, such as RDGCN \cite{fanourakis2022}, can capture complex relational paths and structural patterns, leading to superior performance in identifying structural similarities between entities across KGs. This end-to-end learning of structural features offers a powerful alternative to manual feature engineering seen in methods like MultiKE. For instance, in the context of large-scale KGs, \textcite{xin2022dam} introduced a scalable GNN-based entity alignment approach that tackles the structure and alignment loss often incurred during KG partitioning. Their method proposes a centrality-based subgraph generation, self-supervised entity reconstruction, and cross-subgraph negative sampling to maintain structural integrity and alignment quality, demonstrating the adaptability of GNNs to real-world scalability challenges. Despite their strengths, GNNs can be sensitive to severe structural heterogeneity between KGs and may incur higher computational demands, particularly for very deep architectures or extremely large graphs, and can suffer from over-smoothing issues.

More recently, the landscape of entity alignment has been significantly impacted by the rise of Large Language Models (LLMs). These models, leveraging vast pre-trained knowledge and advanced natural language understanding capabilities, offer a distinct paradigm for EA, particularly in scenarios rich in textual descriptions or where structural information is sparse. As highlighted by \textcite{ge2023}, the integration of KGE methods with Pre-trained Language Models (PLMs) is a promising direction for KG completion, extending naturally to EA. LLM-based approaches can directly compare entity names, descriptions, and attributes by encoding them into a shared semantic space, often outperforming structure-focused GNNs in low-resource or zero-shot alignment settings where seed alignments are scarce. They excel at leveraging external, general-world knowledge implicitly encoded during pre-training, which is a significant advantage over models relying solely on the internal structure of the KGs. Furthermore, \textcite{zhu2024} emphasizes that the integration of global structural embedding with local semantic information (e.g., attributes, images) is crucial for enhancing alignment accuracy, a task where LLMs, with their multimodal capabilities, are increasingly proving effective. This paradigm shift allows for more flexible and robust alignment, especially when dealing with KGs that have limited overlap in their structural patterns but share rich textual content.

A comprehensive experimental review by \textcite{fanourakis2022} provides critical insights into the comparative performance and trade-offs of various embedding-based EA methods. Their meta-level analysis revealed statistically significant correlations between method performance and dataset characteristics, such as KG density and factual information richness. For instance, unsupervised and semi-supervised methods exploiting literal similarity (e.g., AttrE, KDCoE) were found to outperform supervised relation-based methods (like RDGCN) on datasets with decreasing density but rich factual information. This highlights that while GNNs generally excel at structural learning, methods leveraging textual or attribute information can be more robust in specific data sparsity scenarios or when structural information is limited, underscoring the importance of choosing the right approach based on data characteristics. The study also underscored the critical trade-off between effectiveness and efficiency, emphasizing that more complex models, while potentially more accurate, incur higher computational overhead, a practical consideration for real-world deployment.

These advancements collectively demonstrate a clear intellectual trajectory in leveraging KGE for entity alignment. The field has progressed from foundational KGE models that primarily focused on relational structures to increasingly sophisticated, problem-specific solutions that mitigate data scarcity, enhance robustness, integrate richer, heterogeneous information (e.g., attributes, ontological schema), and leverage advanced neural architectures like GNNs for deeper structural understanding, culminating in the integration of powerful LLMs for semantic matching. This evolution reflects a continuous effort to integrate diverse information sources and advanced learning paradigms to achieve more accurate and robust entity alignment across heterogeneous knowledge bases.