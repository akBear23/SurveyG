\subsection{Graph Neural Networks and Transformers for Structural Learning}

The inherent limitations of traditional Knowledge Graph Embedding (KGE) models, particularly their transductive nature and struggle to capture rich, multi-hop structural information, have driven the integration of Graph Neural Networks (GNNs) and Transformer architectures. These advanced neural models explicitly leverage graph topology and complex relational paths, providing a powerful framework for learning context-rich embeddings, enabling inductive learning for unseen entities and relations, and significantly enhancing generalization capabilities.

The paradigm shift towards GNNs in KGE began with models that adapted message-passing mechanisms to the relational nature of knowledge graphs. A seminal work, R-GCN \cite{schlichtkrull2018modeling}, introduced relation-specific transformations within a standard GCN framework. Each relation type was assigned a distinct weight matrix, allowing entities to aggregate information from their neighbors based on the type of incoming and outgoing edges. This approach effectively captured local graph structure and multi-hop dependencies through iterative message passing, where an entity's embedding is updated by aggregating transformed embeddings of its neighbors. However, R-GCN faced challenges with parameter explosion for KGs with many relation types and struggled to model complex relational patterns beyond simple aggregation.

To address the parameter efficiency and enhance expressiveness, CompGCN \cite{vashishth2020compositional} proposed a compositional approach. It unified entity and relation embeddings into a single framework, using composition operations (e.g., subtraction, multiplication) to define how relation embeddings transform entity messages during aggregation. This allowed for a more compact representation of relations and improved the modeling of inverse and symmetric relation patterns. Building on the GNN message-passing paradigm, GAATs \cite{wang2020} further refined neighbor aggregation by introducing an attenuated attention mechanism. Unlike earlier GCNs that often assigned uniform weights, GAATs dynamically weighted different relation paths and actively acquired information from neighboring nodes, leading to more nuanced and comprehensive entity and relation representations by capturing the varying importance of different structural contexts.

A critical challenge for KGE is inductive learning, where models must generalize to entities or even entire knowledge graphs unseen during training. SE-GNN \cite{li2021} advanced this by explicitly modeling "Semantic Evidence" at relation, entity, and triple levels through distinct neighbor aggregation patterns within a GNN. This provided crucial insights into *why* KGE models generalize, by demonstrating how different types of semantic evidence contribute to the inductive capacity. Taking generalization a step further, MorsE \cite{chen2021} introduced a meta-learning framework. MorsE learns "meta-knowledge" (transferable structural patterns) using a GNN modulator, enabling it to produce general entity embeddings for entirely unseen entities in new KGs. This represents a higher level of generalization compared to models that primarily focus on inductive relation prediction, as MorsE can adapt to novel graph structures.

The increasing complexity and heterogeneity of KGs have also spurred the development of more adaptive GNN architectures. MGTCA \cite{shang2024} introduced a novel approach by generating richer neighbor messages through the integration of spatial information from hyperbolic, hypersphere, and Euclidean spaces. It further incorporated a trainable convolutional attention network that autonomously switches between different GNN types (GCN, GAT, and a novel KGCAT) to adaptively capture diverse local structural patterns, overcoming the data dependence of single-GNN models. This highlights a trend towards multi-geometric and adaptive GNN designs. Complementing this, research into automated GNN design, such as the message function search proposed by \cite{di2023}, allows for the discovery of data-dependent message functions. This approach builds a flexible search space for GNN message functions, enabling the system to automatically find optimal structures and operators that adapt to diverse KG forms (e.g., traditional KGs, n-ary relational data, hyper-relational KGs), thereby enhancing adaptability and performance.

Deploying GNNs on large-scale KGs presents significant computational and memory challenges. CPa-WAC \cite{modak2024} addressed this by developing a Constellation Partitioning-based Scalable Weighted Aggregation Composition framework. It employs a novel constellation partitioning algorithm to divide KGs into topological clusters and uses a global decoder to merge embeddings from independently trained partitions, significantly reducing training time and memory costs while maintaining prediction accuracy. Other efforts in scaling GNNs for link prediction include algorithmic strategies like self-sufficient partitions, constraint-based negative sampling, and edge mini-batch training, which have demonstrated substantial speedups while preserving performance \cite{sheikh202245c}. Furthermore, empirical studies on parallel training techniques for KGE models, such as those by \cite{kochsiek2021}, have shown that while many existing methods negatively impact embedding quality, careful choices like variations of stratification and suitable random partitioning can enable efficient and effective large-scale training. For dynamic KGs, MetaHG \cite{sun2024} proposed a meta-learning strategy that efficiently updates incremental knowledge by integrating both local and potential global structural information through a hybrid GNN and Hypergraph Neural Network (HGNN) framework.

More recently, the power of Transformer architectures, renowned for their ability to capture long-range dependencies, has been harnessed for structural learning in KGE. TGformer \cite{shi2025} introduced a novel Graph Transformer Framework for KGE. It constructs context-level subgraphs for predicted triplets and employs a Knowledge Graph Transformer Network (KGTN) to comprehensively explore multi-structural features (both triplet-level and graph-level) and contextual information. By leveraging self-attention mechanisms, TGformer captures global and local structural features by allowing each node to attend to all other nodes within its contextual subgraph, enabling state-of-the-art link prediction through explicit modeling of complex relational paths. In a domain-specific application, SEConv \cite{yang2025} proposed a model for healthcare prediction that combines a resource-efficient self-attention mechanism with a multilayer Convolutional Neural Network (CNN) to learn deeper and more expressive structural features from medical KGs, illustrating hybrid approaches that merge the strengths of different architectures.

In conclusion, the integration of GNNs and Transformers marks a significant paradigm shift in KGE, moving from local, transductive approaches to models capable of inductive learning, multi-hop reasoning, and capturing rich contextual and structural information. While these advanced architectures offer superior expressiveness and generalization, challenges remain in optimizing their computational efficiency for extremely large and sparse KGs, ensuring interpretability of complex attention mechanisms, and effectively fusing diverse structural signals from multi-modal or multi-geometric spaces. Future research will likely focus on developing more scalable Graph Transformers, exploring novel attention mechanisms tailored for heterogeneous KGs, and designing hybrid models that combine the strengths of various architectures for robust and efficient structural learning, potentially through automated design processes.