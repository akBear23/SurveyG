\subsection*{Recommendation Systems and Question Answering}

Knowledge Graph Embeddings (KGE) play a pivotal role in enhancing the capabilities of intelligent information retrieval and decision-making processes, particularly in recommendation systems and Knowledge Graph-based Question Answering (KGQA). By representing entities and relations in continuous vector spaces, KGE models facilitate the discovery of intricate semantic relationships that are crucial for personalized suggestions and accurate query responses.

In the realm of recommendation systems, KGE models have evolved to capture increasingly nuanced user and item interactions. Early approaches often relied on labor-intensive, hand-engineered features derived from knowledge graphs. Addressing this, Recurrent Knowledge Graph Embedding (RKGE) \cite{sun2018} introduced a novel recurrent network architecture to automatically learn semantic representations for both entities and the paths connecting them. This approach, featuring a batch of recurrent networks and a pooling operator, effectively models the diverse semantics of multiple paths between entity pairs, thereby characterizing user preferences and providing meaningful explanations for recommendations. Building upon the foundation of single-domain recommenders, the challenge of cross-domain item recommendations and cold start problems emerged. To tackle this, \cite{liu2023} proposed a Cross-Domain Knowledge Graph Chiasmal Embedding approach, which efficiently interacts items across multiple domains through a novel binding rule. This method frames multi-domain item-item recommendation as a link prediction task within a cross-domain knowledge graph, demonstrating superior performance in both link prediction and multi-domain recommendation results. Further advancing the field, the demand for explainable recommendations has grown significantly. Contextualized Knowledge Graph Embedding (CKGE) \cite{yang2023} addresses this by integrating motivation-aware contextual information and high-order connections within a knowledge graph. CKGE employs a specialized KG-based Transformer, which processes meta-graphs constructed for each talent-course pair, incorporating relational attention and structural encoding. A key innovation is its local path mask prediction mechanism, which reveals the saliency of meta-paths, offering explicit explanations for recommendations and characterizing user preferences.

For Knowledge Graph-based Question Answering (KGQA), KGE models are instrumental in bridging the gap between natural language queries and structured knowledge. Initial KGE-based QA systems focused on simpler query types. For instance, the Knowledge Embedding based Question Answering (KEQA) framework \cite{huang2019} targets "simple questions" by jointly recovering the question's head entity, predicate, and tail entity representations within the KG embedding spaces. It then derives an answer by identifying the closest fact in the KG using a carefully designed joint distance metric, outperforming state-of-the-art methods on relevant benchmarks. Recognizing the need for domain-specific and more complex query handling, Marie and BERT \cite{zhou2023} developed a KGQA system specifically for chemistry. This system leverages hybrid knowledge graph embeddings, operating on multiple embedding spaces queried in parallel, to handle deep ontologies, numerical filtering, and intricate chemical reaction mechanisms. It introduces a score alignment model, an implicit multihop relation algorithm, and a BERT-based bidirectional entity-linking model, significantly advancing domain-specific KGQA. A more recent development involves the synergistic integration of KGE with Large Language Models (LLMs). A knowledge-enhanced joint model \cite{liu2024q3q} incorporates aviation assembly knowledge graph embeddings into LLMs for fault diagnosis. This model utilizes graph-structured big data from KGs to conduct prefix-tuning of LLMs, enabling online reconfiguration and strengthening specialized knowledge within the aviation assembly domain. By generating knowledge subgraphs and fusing knowledge through retrieval augmentation, it provides robust, knowledge-based reasoning responses, achieving high accuracy in practical industrial scenarios.

The progression in this area highlights a continuous effort to move beyond basic relational modeling towards more sophisticated, context-aware, and explainable systems. While significant strides have been made in automating feature learning, handling multi-domain interactions, and integrating with advanced language models, challenges remain. Future research could focus on developing more robust cross-domain knowledge transfer mechanisms that adapt to highly dynamic environments, enhancing the interpretability of complex multi-modal KGE-LLM systems, and exploring novel ways to dynamically update KGEs to reflect real-time changes in knowledge graphs for both recommendation and question answering tasks.