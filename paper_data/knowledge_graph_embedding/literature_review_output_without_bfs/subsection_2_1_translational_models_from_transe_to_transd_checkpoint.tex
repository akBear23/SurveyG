\subsection{Translational Models: From TransE to TransD}

The inherent incompleteness of knowledge graphs necessitates robust methods for link prediction and knowledge graph completion. The translational paradigm emerged as a foundational approach, modeling relations as transformations in a continuous embedding space, offering intuitive and computationally efficient initial solutions. These models interpret relations as operations that translate a head entity embedding to a tail entity embedding, aiming to satisfy the geometric principle $h + r \approx t$.

The pioneering work in this area was TransE \cite{bordes2013}, which posited that a relation $r$ acts as a simple translation from a head entity $h$ to a tail entity $t$ in a shared low-dimensional vector space. This model was groundbreaking for its simplicity and efficiency, establishing a strong baseline for knowledge graph embedding. However, TransE struggled significantly with complex relation types, particularly 1-to-N, N-to-1, and N-to-N relations \cite{asmara2023}. This limitation arose because TransE assigned a single, fixed vector representation to each entity, regardless of the specific relation it participated in. Consequently, for a 1-to-N relation (e.g., "capitalOf(Paris, France), capitalOf(Paris, Germany)"), the model would attempt to map "Paris" to both "France" and "Germany" via the same "capitalOf" vector. This forces "France" and "Germany" to occupy very similar locations in the embedding space, which is semantically inaccurate and severely limits discriminative power \cite{asmara2023}. Furthermore, TransE's common regularization strategy, which often involves projecting embeddings onto a unit sphere, was later identified to conflict with its core translation principle. This conflict could warp embeddings, adversely affecting link prediction accuracy and preventing the model from fully realizing its potential \cite{ebisu2017}.

To address TransE's shortcomings, subsequent models introduced more sophisticated mechanisms to handle relational diversity and improve expressiveness. TransH \cite{wang2014} advanced the paradigm by projecting entities onto relation-specific hyperplanes. Instead of a single entity vector, TransH allowed an entity to have different "distributed" representations (projections) depending on the specific relation it participated in. This innovation provided a more flexible way to model an entity's role in different contexts, thereby better accommodating 1-to-N, N-to-1, and N-to-N relations while maintaining computational efficiency comparable to TransE. By decoupling an entity's general representation from its relation-specific manifestation, TransH offered a significant step towards capturing the polysemy of entities in different relational contexts.

Building upon the concept of relation-specific representations, TransR and CTransR \cite{lin2015} further extended this idea by proposing the use of separate entity and relation spaces. In TransR, entities are first projected from their entity-specific space into a relation-specific space using a relation-specific projection matrix $M_r$, before the translation operation is applied ($h M_r + r \approx t M_r$). This approach provided even greater expressiveness than TransH's hyperplane projections, as it allowed entities to be represented distinctly and transformed across various relation types, improving the modeling of complex semantic patterns. However, TransR introduced a significant increase in parameter count. Each relation required its own full projection matrix, leading to a parameter complexity of $O(k_e \cdot k_r)$ per relation (where $k_e$ and $k_r$ are entity and relation embedding dimensions). This posed scalability challenges for knowledge graphs with a large number of relations, as the computational cost and memory footprint grew substantially.

TransD \cite{ji2015} was then introduced to refine the translational paradigm, aiming to achieve the expressiveness of TransR while mitigating its parameter explosion. TransD employed dynamic mapping matrices, a more efficient approach to projection. Unlike TransR's fixed, full projection matrices, TransD introduced a dual-vector representation for each entity ($h, h_p$) and relation ($r, r_p$). One vector captured the intrinsic meaning (e.g., $h$), while the second ($h_p$ or $r_p$) dynamically constructed a mapping matrix for projection. Specifically, the projection matrix for a head entity $h$ given relation $r$ is constructed as $M_{rh} = r_p h_p^T + I$, and similarly for the tail entity. This innovation significantly improved expressiveness by considering the diversity of *both* entities and relations in a more adaptive and efficient manner. Crucially, by constructing projection matrices from lower-dimensional vectors, TransD dramatically reduced the parameter count compared to TransR (e.g., $O(k_e + k_r)$ per relation for the projection vectors, plus the entity/relation embeddings), offering a superior trade-off between expressivity and scalability while achieving enhanced performance. This concept of dynamic translation, where parameter vectors are introduced to support flexible translation, was also explored in models like TransE-DT and TransR-DT \cite{chang20179yf}, demonstrating consistent improvements by making the translation principle more adaptive.

These models collectively established the translational paradigm, offering initial and progressively refined solutions for link prediction and knowledge graph completion. While highly influential and forming the bedrock of KGE research, these early translational models, even with their advancements, primarily relied on predefined geometric operations in Euclidean space. They often struggled to fully capture highly complex or implicit semantic patterns, such as those involving relational composition beyond simple vector addition, or the nuanced interplay of diverse entity types and textual contexts. Their limitations, particularly in modeling intricate logical patterns and scaling efficiently to extremely large and diverse knowledge graphs, paved the way for more expressive models, including those that generalize geometric transformations (e.g., rotations) or leverage neural architectures, which are discussed in subsequent sections.