\subsection{Lie Group and Compound Operations for Enhanced Expressiveness}

The quest for more expressive knowledge graph embedding (KGE) models has propelled research beyond simple Euclidean translations, leading to the exploration of advanced geometric and algebraic structures. This subsection examines models that leverage Lie groups and sophisticated compound geometric operations, offering a richer framework for representing entities and relations by capturing intricate relational semantics and diverse mapping properties.

An early conceptual step towards more flexible geometric representations was \textcite{xiao2015}'s \textbf{ManifoldE}, which moved beyond the "one-point" mapping principle of models like TransE. ManifoldE proposed embedding triples onto relation-specific manifolds (e.g., spheres or hyperplanes) rather than single points. This manifold-based approach provided greater flexibility for complex relations (e.g., one-to-many, many-to-many) and addressed the ill-posed algebraic system inherent in point-wise models, paving the way for more sophisticated geometric interpretations.

Building on this idea of structured geometric spaces, \textcite{ebisu2017} introduced \textbf{TorusE}, a pioneering model that embeds knowledge graphs directly on a Lie group, specifically a torus. This innovative approach ingeniously resolves a fundamental regularization conflict prevalent in earlier translational models. In Euclidean space, forcing embeddings onto a unit sphere (a common regularization technique) can distort learned representations, especially for relations modeled as translations. By embedding on a compact Abelian Lie group like the torus, TorusE inherently bounds the embeddings, eliminating the need for explicit regularization and leading to more accurate and efficient learning. This geometric shift offered a novel perspective, demonstrating the benefits of leveraging the inherent properties of Lie groups for KGE.

Further enhancing geometric transformations, subsequent research focused on combining multiple operations to achieve greater expressiveness. \textcite{ge2022} introduced \textbf{CompoundE}, a model that significantly generalizes geometric transformations by combining translation, rotation, and scaling operations in a cascaded, relation-specific manner. Unlike previous models often restricted to a single type of operation, CompoundE frames these compound operations within the affine group. This allows relations to not only shift and turn entity embeddings but also expand or contract them, providing a more versatile and powerful approach for capturing diverse relation patterns and mapping properties, such as symmetry, anti-symmetry, inversion, and various N-to-N relationships. As highlighted by the survey \textcite{ge2023}, CompoundE mathematically demonstrates its ability to encompass and generalize several existing distance-based KGE models, including TransE and RotatE, thereby offering a unifying perspective on affine operation-based techniques.

Extending the capabilities of high-dimensional geometric transformations, \textcite{li2022} proposed \textbf{HousE}, which utilizes Householder parameterization for high-dimensional rotations and invertible projections. While earlier rotation-based models like RotatE \cite{sun2019} were effective for capturing symmetric and anti-symmetric patterns, their distance-preserving nature limited their ability to model complex relation mapping properties (RMPs) such as one-to-many or many-to-many. HousE addresses this by representing relations as compositions of Householder reflections for arbitrary high-dimensional rotations (generalizing RotatE to k-dimensions) and introduces novel invertible Householder projections that can flexibly adjust relative distances. This unified framework allows HousE to simultaneously model all crucial relation patterns and RMPs, offering superior expressiveness and generalizing previous rotation-based models.

Building upon the Householder parameterization, \textcite{li2024} introduced \textbf{GoldE} (Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization). GoldE presents a framework that generalizes existing KGE approaches in *both dimension and geometry* for orthogonal relation transformations. It achieves this through a universal orthogonal parameterization based on a generalized Householder reflection, capable of operating in arbitrary dimensions and across Euclidean, elliptic, and hyperbolic geometries. For this subsection's focus, GoldE's elliptic orthogonal parameterization is particularly relevant, as it generalizes Euclidean models by incorporating relation-specific scaling operations, effectively extending the "compound operations" paradigm by allowing adaptive adjustments to entity embeddings within a generalized orthogonal transformation framework. This represents a significant step towards breaking the limitations of rigid, homogeneous geometric spaces.

The progression towards these more complex, structured operations highlights a broader search for unifying mathematical principles in KGE. This goal was later formalized by \textcite{zheng2024} with \textbf{HolmE}, which provides a unifying Riemannian framework for KGE models. HolmE introduces the crucial property of being "closed under composition," ensuring that the composition of any two relation embeddings remains within the embedding space. The authors theoretically prove that prominent KGE models like TransE and RotatE are special cases of HolmE, providing a deeper theoretical understanding of their underlying geometric properties and demonstrating how these advanced geometric operations contribute to a more robust and theoretically sound representation of relational composition.

In summary, models leveraging Lie groups and compound operations, from the regularization-resolving TorusE to the generalized affine transformations of CompoundE, the high-dimensional Householder parameterizations of HousE and GoldE, and the unifying Riemannian framework of HolmE, represent a significant advancement in KGE. They move beyond the limitations of simpler geometric operations by leveraging the inherent properties of advanced algebraic and geometric structures. This allows them to capture intricate relational semantics, including diverse relation patterns and complex mapping properties, with enhanced expressiveness. While these approaches offer superior modeling capacity, a continuing challenge lies in balancing their increased mathematical complexity and computational demands with interpretability and scalability for real-world, large-scale knowledge graphs. Future research may focus on optimizing these complex operations and integrating them with neural architectures for even more robust and adaptable KGE solutions.