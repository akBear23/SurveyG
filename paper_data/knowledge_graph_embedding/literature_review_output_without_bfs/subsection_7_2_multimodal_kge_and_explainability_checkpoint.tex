\subsection{Multimodal KGE and Explainability}

The evolution of Knowledge Graph Embedding (KGE) is increasingly characterized by a progressive move beyond purely symbolic representations towards richer, multimodal information integration. This paradigm shift is critically intertwined with the growing demand for explainability in AI systems, as the inherent complexity introduced by diverse data types necessitates enhanced transparency and interpretability. The integration of multimodal data aims to create more comprehensive and robust knowledge representations, while explainable KGE methods are crucial for fostering trust and understanding, particularly in sensitive domains where model transparency is paramount for adoption and ethical deployment.

\subsubsection{Multimodal Knowledge Graph Embedding}
Integrating multimodal information into KGE seeks to enrich entity and relation representations by combining symbolic facts with features extracted from various data types, including text, images, and potentially audio or video. Early efforts primarily focused on leveraging diverse *textual* information. For instance, \cite{zhang2019multike} introduced MultiKE, a pioneering approach for entity alignment that unified multiple entity "views" (name, relation, attribute) derived from textual descriptions. MultiKE innovated by designing cross-KG inference methods and a "soft alignment" for relations and attributes, thereby reducing reliance on costly seed alignments and demonstrating the benefits of heterogeneous textual feature integration for robust entity matching. While significant, these approaches largely remained within the linguistic domain.

The true frontier of multimodal KGE lies in the direct integration of *non-textual* modalities, such as visual features, with symbolic knowledge. Foundational models began to bridge this gap by extending existing KGE frameworks. For example, Image-embodied Knowledge Representation Learning (IKRL) \cite{xie2016image} integrated visual information by projecting image features into the same embedding space as entities and relations, allowing visual similarities to influence the learning of symbolic relationships. Similarly, Multimodal Knowledge Base Embedding (MKBE) \cite{guo2018multimodal} utilized CNNs to extract visual features from images associated with entities, fusing them with textual descriptions and structural information to enhance entity representations. These early methods demonstrated the potential of combining visual cues with relational facts, often by modifying scoring functions or introducing auxiliary loss terms.

More recent advancements leverage advanced neural architectures. For example, \cite{liang2023hypernode} proposed Hyper-node Relational Graph Attention Network (HRGAT) as a customized Graph Neural Network (GNN) for multimodal knowledge graphs. HRGAT combines different modal information (e.g., visual, textual) with graph structure information, using a hyper-node concept to represent entities that aggregate features from various modalities, thereby yielding a more precise and interpretable representation. This approach highlights the utility of GNNs in processing heterogeneous features for multimodal KGE. Another concrete application is seen in domain adaptation for tasks like musical instrument recognition, where \cite{eyharabide2021wx4} presented a method that incorporates KGE-derived semantic vector spaces as a key ingredient to guide the domain adaptation process, combining these semantic embeddings with visual embeddings from images. By training a neural network with these combined embeddings as anchors, their method demonstrates how KGE can enhance visual recognition tasks, effectively fusing symbolic knowledge with visual features to improve performance in data-scarce cultural heritage datasets. A recent survey by \cite{zhu2024survey} further underscores the growing importance of multimodal entity alignment, highlighting the integration of diverse modalities beyond text, such as images and even video, as a critical future direction.

These approaches typically employ various fusion techniques, such as joint embedding spaces where features from different modalities are projected into a common vector space, or attention mechanisms that selectively weigh the importance of different modal inputs. Despite these advancements, challenges remain in developing universally applicable multimodal KGE models. Key issues include the inherent heterogeneity of data (e.g., varying dimensions, noise levels), the complexity of aligning information across vastly different modalities, and the scarcity of large-scale, richly annotated multimodal knowledge graph datasets for training and evaluation. Furthermore, the increased dimensionality and complexity introduced by multimodal data can significantly exacerbate the "black box" nature of KGE models, intensifying the demand for robust explainability.

\subsubsection{Explainable Knowledge Graph Embedding}
The growing importance of explainability in KGE is crucial for building trust and understanding in AI systems, especially as models become more complex and operate in sensitive applications. Explainable KGE methods aim to provide insights into model predictions, learned representations, and underlying reasoning processes. We can categorize these methods into two main types: ante-hoc (intrinsic) explainability, where models are designed to be inherently interpretable, and post-hoc explainability, where explanations are generated after model training.

**Ante-hoc/Intrinsic Explainability**: These models embed interpretability directly into their design, often through intuitive geometric or statistical properties of their representations or by integrating explicit symbolic knowledge. \cite{li2024sphere} proposed SpherE, which embeds entities as spheres where the radius intuitively correlates with an entity's "universality" or prevalence in the knowledge graph. This sphere-based modeling not only enhances expressiveness for many-to-many relations but also offers an interpretable parameter that provides insights into the entity's role and scope within the KG. Similarly, \cite{pavlovic2022expressive} introduced ExpressivE, a spatio-functional KGE model that embeds entity pairs as points and relations as hyper-parallelograms. A key contribution of ExpressivE is its explicit focus on providing an "intuitive interpretation" and "consistent geometric interpretation" of captured inference patterns (e.g., composition, hierarchy) through the spatial relationships of these hyper-parallelograms, making the model's internal logic more transparent and directly understandable.

However, relying solely on geometric intuition for explainability can be insufficient for complex logical reasoning. \cite{gutierrezbasulto2018ontology} critically analyzed the compatibility between vector space representations and ontological rules, demonstrating that many popular embedding methods are incapable of modeling even simple rule types, thus failing to capture the dependencies inherent in symbolic knowledge. This highlights a limitation of purely geometric approaches. In contrast, models that integrate explicit logic rules offer a more formal and robust form of intrinsic explainability. For instance, \cite{wang2019logic} proposed a logic rule-enhanced method that can be integrated with translational KGE models. By automatically mining logic rules and representing triples as first-order logic, this approach transfers human knowledge into embeddings, making the reasoning process more aligned with symbolic logic and thus inherently more interpretable. Such rule-aware designs move beyond mere geometric analogies to provide explanations grounded in logical consistency.

**Post-hoc Explainability**: These methods aim to explain model behavior after training, often by analyzing predictions or learned features. This category can be further broken down into different levels of explanation:

*   **Meta-level Understanding of Generalization**: This focuses on explaining *why* KGE models generalize or extrapolate. \cite{li2021semantic} addresses this by introducing the concept of "Semantic Evidence" (relation-level, entity-level, and triple-level) to explain how KGE models extrapolate to unseen data, providing crucial insights into their underlying mechanisms and contributing to a meta-level understanding of model behavior. Expanding on this, \cite{kurokawa2021explainable} proposed an explainable knowledge reasoning framework that combines multiple KGE techniques with corresponding explainable AI (XAI) techniques. This framework aims to integrate various methods to achieve complex reasoning with explanations, demonstrating a broader approach to understanding KGE behavior in a multi-model context.

*   **Path-based Explanations (Local Explanations)**: These methods provide insights into *which* specific paths or facts contribute to a particular prediction. \cite{jia2020pconvkb} proposed PConvKB, a convolutional neural network-based embedding model that incorporates relation paths. By employing attention mechanisms to measure the local importance of relation paths and a measure called DIPF for global importance, PConvKB provides insights into *which* paths contribute most significantly to a fact's inference, thereby offering a form of explainability by highlighting the evidential paths in the graph. While effective for local explanations, the computational cost of enumerating and evaluating paths can be high for dense or very large KGs, limiting scalability.

*   **Application-specific Explanations**: In downstream applications, explainability often translates to providing concrete reasons for a model's output. \cite{sun2018rkge} introduced RKGE, a recurrent knowledge graph embedding model designed for recommendation systems. RKGE learns path semantics within KGs and provides "meaningful explanations" for its recommendation results, demonstrating early efforts in transparent recommendation systems. Building on this, \cite{yang2023ckge} further advanced explainable KGE with CKGE, a contextualized approach for explainable talent training course recommendation. CKGE constructs motivation-aware meta-graphs and employs a novel KG-based Transformer with a "local path mask prediction" mechanism to explicitly reveal the saliency of different meta-paths, offering concrete, motivation-driven explanations for specific recommendations. In sensitive domains like healthcare, explainability is paramount. For instance, SEConv \cite{yang2025seconv} is designed to provide transparent and interpretable predictions in healthcare applications, where understanding the rationale behind a model's output (e.g., drug-disease association) is critical for clinical trust and decision-making. Such models are crucial for bridging the gap between complex AI systems and human experts.

In conclusion, the field of KGE is actively pursuing richer, multimodal representations to capture the complexity of real-world knowledge, moving from integrating diverse symbolic and textual features towards a broader, though still challenging, integration of non-textual modalities. Concurrently, there is a strong emphasis on developing explainable KGE methods, ranging from understanding model generalization and providing intrinsically interpretable representations (e.g., geometric or rule-based) to offering concrete, prediction-specific explanations. A significant gap remains, however, in models that are *both* truly multimodal and inherently explainable. The increased complexity from multimodal inputs often deepens the "black box" nature of KGE models, making it challenging to attribute predictions to specific modalities or explain cross-modal interactions. Future work will likely focus on developing sophisticated multimodal fusion techniques that are designed with explainability in mind, formalizing evaluation metrics for multimodal explanations, and advancing human-centered explainable AI for KGE, ensuring that as KGE models become more powerful, they also become more transparent and trustworthy.