\subsection{Automated KGE and Meta-Learning}
The increasing complexity of Knowledge Graph Embedding (KGE) model design and the dynamic nature of real-world knowledge graphs necessitate a shift towards automated model selection and adaptive learning paradigms. This section explores the growing trend towards automating the KGE model design process (AutoML) and leveraging meta-learning for dynamic and adaptive embeddings. These approaches collectively aim to reduce manual effort, improve model adaptability to diverse KG characteristics, and enable continuous learning in evolving environments, pushing KGE towards more intelligent, autonomous, and self-optimizing systems.

A significant challenge in KGE has been the manual design of effective scoring functions and model architectures, which often exhibit dataset-specific optimal performance. To address this, \textcite{zhang2019} introduced \texttt{AutoSF}, an AutoML framework designed to automatically search for optimal scoring functions for diverse knowledge graphs. By identifying a unified representation for bilinear models and incorporating domain-specific insights into relation properties, \texttt{AutoSF} employs a progressive greedy search algorithm with a predictor to efficiently discover novel, KG-dependent scoring functions, thereby significantly reducing manual effort and improving adaptability. Building on this, \textcite{di20210ib} further refined the concept of automated scoring function search by proposing an efficient relation-aware approach. Recognizing that different semantic patterns within a KG might benefit from distinct scoring mechanisms, their method encodes the search space as a supernet and utilizes a one-shot alternative minimization algorithm to efficiently discover relation-aware scoring functions, leading to improved performance over general, non-relation-aware searches. Extending the scope of AutoML beyond scoring functions to neural architectures, \textcite{shang2024} proposed \texttt{MGTCA}, a framework that addresses the data dependence of Graph Neural Network (GNN) types in Knowledge Graph Completion. \texttt{MGTCA} introduces a Trainable Convolutional Attention Network that allows for autonomous switching between GNN types (GCN, GAT, and a novel KGCAT) and integrates a Mixed Geometry Message Function to capture richer structural information from hyperbolic, hypersphere, and Euclidean spaces. This approach automates the model design process by adaptively selecting the most suitable GNN architecture for local graph structures, eliminating the need for expensive pre-validation. Similarly, \textcite{di2023} focused on a more granular level of GNN design by building a search space for the *message function* itself. Their innovation lies in allowing both the structure and operators of the message function to be searched, making it highly adaptable to different KG forms (traditional KGs, n-ary relational data, hyper-relational KGs) and datasets. While \texttt{AutoSF} and \textcite{di20210ib} primarily focus on the scoring function, \texttt{MGTCA} and \textcite{di2023} extend AutoML to the architectural components of GNNs, offering a more comprehensive automation of KGE model construction. The trade-off often lies between the computational cost of the search process and the expressiveness and adaptability of the automatically discovered models.

Beyond automating static model design, a critical aspect of modern KGE is the ability to continuously learn and adapt to evolving knowledge and unseen entities. Traditional KGE models are often transductive, struggling with emerging entities and dynamic knowledge updates. \textcite{sun2024} presented \texttt{MetaHG}, a meta-learning strategy specifically tailored for dynamic updates in evolving service ecosystems. \texttt{MetaHG} addresses the inefficiency of updating incremental knowledge by incorporating both local and potential global structural information from current KG snapshots via a hybrid GNN and Hypergraph Neural Network (HGNN) framework. This enables continuous learning and adaptation to changing knowledge, mitigating issues like spatial deformation. Complementing this, \textcite{chen2021} introduced \texttt{MorsE}, another meta-learning framework for inductive KGE that focuses on generating general entity embeddings for *entirely new entities in unseen KGs*. \texttt{MorsE} learns "meta-knowledge" through entity-independent modules (an Entity Initializer and a GNN Modulator) that capture transferable structural patterns, allowing it to produce high-quality embeddings for novel entities without retraining from scratch. While \texttt{MorsE} focuses on generalizing to unseen entities, \textcite{lee202380l}'s \texttt{InGram} extends this inductive capability to also generate embeddings for *new relations* at inference time, leveraging relation graphs and attention mechanisms to aggregate neighboring embeddings. These inductive approaches are crucial for handling the constant influx of new information in real-world KGs.

Furthermore, the challenge of continually updating KGE models with new knowledge while preserving old knowledge (Continual Knowledge Graph Embedding, CKGE) is a key area where meta-learning principles are applied. \textcite{liu2024to0} proposed Incremental Distillation (IncDE) for CKGE, which explicitly considers the graph structure of new knowledge. IncDE introduces a hierarchical strategy to optimize the learning order of new triples and devises an incremental distillation mechanism to seamlessly transfer entity representations, thereby promoting old knowledge preservation and mitigating catastrophic forgetting. Building on efficiency, \textcite{liu2024} introduced \texttt{FastKGE} with an Incremental Low-Rank Adapter (IncLoRA) mechanism. Inspired by parameter-efficient fine-tuning in large language models, IncLoRA efficiently learns and stores new knowledge by dividing new entities and relations into layers and assigning incremental low-rank adapters. This approach significantly reduces training costs for new knowledge acquisition while maintaining competitive performance in preserving old knowledge. While \texttt{MetaHG} focuses on dynamic updates within an evolving system, \texttt{MorsE} and \texttt{InGram} tackle the more general problem of inductive learning for entirely new entities and relations, and IncDE and FastKGE specifically address the efficiency and forgetting challenges inherent in continual learning scenarios.

In conclusion, the trend towards automated KGE model design and meta-learning for dynamic embeddings marks a significant step towards more intelligent, autonomous, and self-optimizing KGE systems. AutoML frameworks like \texttt{AutoSF}, \textcite{di20210ib}, \texttt{MGTCA}, and \textcite{di2023} are reducing manual effort by automating the search for optimal scoring functions and GNN architectures, leading to more adaptable models. Concurrently, meta-learning approaches such as \texttt{MetaHG}, \texttt{MorsE}, \texttt{InGram}, IncDE, and \texttt{FastKGE} are enabling KGE models to continuously learn, adapt to evolving knowledge, and generalize to unseen entities and relations, addressing the inherent transductive limitations of many traditional models. Future research must continue to balance the increasing complexity and computational cost of these adaptive and automated models with efficiency, robustness, and interpretability, especially as knowledge graphs grow in size and dynamism.