\subsection{Efficiency, Scalability, and Robustness in Training}

The successful deployment of Knowledge Graph Embedding (KGE) models in real-world applications hinges on their ability to operate efficiently, scale to massive knowledge graphs (KGs), and maintain robustness against noise and incompleteness. Beyond the design of expressive scoring functions, the choice and optimization of training components significantly impact model performance and practical viability \cite{mohamed2021dwg}. This subsection delves into key advancements addressing these critical practical challenges.

A primary area of focus for training efficiency is negative sampling, a ubiquitous technique for learning from sparse positive triples. Traditional random negative sampling often suffers from the "vanishing gradient problem" by frequently selecting "easy" negatives, which provide little learning signal \cite{zhang2018, qian2021}. To overcome this, \textbf{NSCaching} \cite{zhang2018} introduced an efficient cache-based approach to dynamically store and sample high-quality, "hard" negative triplets, proving more effective than complex generative adversarial network (GAN)-based methods. For KGE models integrating multimodal data, \textbf{Modality-Aware Negative Sampling (MANS)} \cite{zhang2023} further refines this by introducing modal-level sampling, particularly for visual embeddings, to explicitly learn modality alignment, a crucial aspect overlooked by prior entity-level strategies. Addressing the inherent noise in real-world KGs, \textbf{Confidence-Aware Negative Sampling} \cite{shan2018} supports robust training by leveraging negative triple confidence, mitigating issues like zero loss and false detection. An alternative approach, the \textbf{Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE)} framework \cite{li2021}, mathematically re-derives the non-sampling square loss function to enable full-data training, achieving improved accuracy and stability without prohibitive computational or space costs for a broad class of models. Theoretically, the relationship between negative sampling and softmax cross-entropy loss functions has been unified using Bregman divergence, offering insights for fair comparisons and understanding their underlying mechanisms \cite{kamigaito20218jz}.

Beyond sampling, efforts to reduce model complexity and resource footprint are crucial. \textbf{TransGate} \cite{yuan2019} enhanced parameter efficiency by introducing a shared gate structure, inspired by LSTMs, to discriminate relation-specific information with significantly fewer parameters and comparable time complexity to simpler models like TransE. For deploying high-performing but resource-intensive KGEs, \textbf{DualDE} \cite{zhu2020} proposed a knowledge distillation framework that dually distills knowledge from a high-dimensional teacher KGE to a low-dimensional student. This approach can achieve substantial parameter reduction (up to 15x) and inference speedup (up to 6x) with minimal accuracy loss, demonstrating a practical trade-off between model capacity and efficiency. Complementing this, \textbf{LightKG} \cite{wang2021} introduced a lightweight framework utilizing codebooks and codewords for efficient storage and inference, dramatically reducing memory consumption and search time while maintaining high approximate search accuracy.

Scalability for truly massive KGs often necessitates distributed or partitioned training approaches. \textbf{CPa-WAC} \cite{modak2024} introduced a Constellation Partitioning-based Scalable Weighted Aggregation Composition framework for GNN-based KGE. This method partitions KGs into topological clusters using Louvain clustering and then aggregates embeddings from these partitions for global inference, reducing training time significantly (up to five times) without substantial accuracy degradation. However, a potential limitation of partitioning lies in the challenge of maintaining global graph coherence and preventing information loss across partition boundaries. For privacy-preserving and distributed training, \textbf{FedS} \cite{zhang2024} proposed a communication-efficient federated KGE framework. FedS leverages entity-wise Top-K sparsification to reduce the size of transmitted parameters per communication round, enhancing communication efficiency with negligible performance degradation. Federated KGE, however, introduces its own set of challenges, including data heterogeneity across clients, which \textbf{FedLU} \cite{zhu2023bfj} addresses through mutual knowledge distillation. Furthermore, privacy threats, such as inference attacks that can reveal sensitive triple existence, require robust defenses like differentially private mechanisms with private selection (\textbf{DP-Flames} \cite{hu20230kr}). The vulnerability to poisoning attacks, where malicious clients inject biased updates, has also been demonstrated, highlighting the need for secure aggregation and anomaly detection in federated settings \cite{zhou2024}. More broadly, empirical studies on parallel KGE training have revealed that many existing methods can negatively impact embedding quality, emphasizing the importance of careful technique selection, with variations of stratification and random partitioning showing promise \cite{kochsiek2021}.

Robustness to noise and incompleteness is another critical aspect. Beyond confidence-aware negative sampling, \textbf{Multi-Task Reinforcement Learning (MTRL)} \cite{zhang2021} offers a general framework for robust KGE by training RL agents to filter noisy triples, leveraging multi-task learning for semantically similar relations. While not primarily a training efficiency technique, the integration of auxiliary information like logical rules (as discussed in Section 4.1) inherently enhances robustness by providing constraints and reducing reliance on potentially noisy observed facts. To ensure the reliability of KGE model predictions, \textbf{Probability Calibration for KGE Models} \cite{tabacof2019} demonstrated that KGE models are often uncalibrated, proposing methods to ensure predicted probabilities are trustworthy, which is vital for high-stakes applications. Furthermore, \textbf{Committee-based KGE} \cite{choi2020} showed that an ensemble of diverse KGE models can achieve more robust knowledge base completion than any single model, by aggregating multiple perspectives. Finally, efficient hyperparameter optimization is crucial for practical deployment. \textbf{KGTuner} \cite{zhang2022fpm} proposes a two-stage search algorithm that efficiently explores hyperparameter configurations on small subgraphs and then fine-tunes on the full graph, consistently finding better hyperparameters within the same time budget.

In summary, operationalizing KGE models reveals a fundamental tension between achieving high expressiveness and ensuring practical deployability. While advancements in negative sampling, knowledge distillation, and lightweight architectures significantly improve training efficiency and reduce resource footprints, they often involve trade-offs in model capacity or require careful tuning to maintain performance. Similarly, scaling via partitioning or federated learning improves throughput and addresses privacy concerns but introduces complexities like communication bottlenecks, challenges in maintaining global graph coherence, and vulnerabilities to attacks. Future research must therefore focus on developing holistic solutions that co-design models and training regimes to be inherently efficient, scalable, and robust, rather than relying on isolated optimizations.