\subsection{Role of KG Embedding}
Knowledge Graph Embedding (KGE) stands as a pivotal paradigm, forging a crucial bridge between the explicit, symbolic representations of knowledge graphs (KGs) and the implicit, continuous representations favored by modern neural approaches. Building upon the challenges of data sparsity, computational overhead, and difficulty in statistical inference inherent to purely symbolic KGs, KGE transforms discrete entities and relations into continuous, low-dimensional vector spaces \cite{choudhary2021, yan2022}. This transformation is fundamental for enabling efficient computation, facilitating seamless integration with machine learning models, and, critically, for capturing latent semantic relationships and structural patterns that are often elusive in discrete formats.

At its core, KGE operates on the principle that entities and relations can be represented as vectors or matrices within an embedding space, where semantic similarity and relational patterns are encoded by their geometric positions and transformations. For instance, in the foundational translational model TransE, a relation is conceptualized as a translation vector from a head entity to a tail entity, formalized as $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ \cite{bordes2013}. This elegant vector arithmetic allows for the inference of missing links based on proximity in the embedding space. Such a continuous representation inherently addresses data sparsity by enabling generalization through shared vector space properties; entities with similar contexts or relations tend to cluster together, facilitating the discovery of nuanced semantic relationships not explicitly defined in the symbolic graph \cite{choudhary2021}. For example, if "Paris" and "France" have a similar vector relationship to "Berlin" and "Germany," the model implicitly learns the "capital city of" concept, even if not explicitly encoded as a rule. Models like Triple Context-Based KGE \cite{gao2018di0} and PConvKB \cite{jia20207dd} further enhance this by explicitly incorporating local and global contextual information, such as neighboring entities and relation paths, to enrich the learned embeddings and capture finer-grained semantics.

The continuous nature of KGE representations offers several significant advantages. Firstly, it enables highly efficient computation during inference. Instead of complex symbolic matching or rule application, KGE models perform vector operations (e.g., dot products, distance calculations) that are computationally inexpensive and highly parallelizable. This makes them scalable for querying large-scale KGs \cite{yan2022}. However, it is crucial to acknowledge that while inference is efficient, the *training* of these models on massive KGs can be computationally intensive, a challenge addressed by techniques focusing on efficiency and scalability in later sections (e.g., \cite{do20184o2} for handling many relations). Secondly, KGE acts as a crucial interface for integrating symbolic knowledge with modern machine learning and deep learning models. The learned embeddings serve as rich, pre-trained features for various neural architectures, allowing for end-to-end learning and leveraging the strengths of both symbolic and sub-symbolic AI paradigms \cite{choudhary2021}. This integration is vital for tasks requiring nuanced understanding and prediction, where traditional rule-based systems often fall short. Furthermore, KGE models can incorporate background taxonomic information, such as subclasses and subproperties, to enhance learning and make more robust predictions, as demonstrated by approaches like \cite{fatemi2018e6v}.

KGE plays a dual, yet interconnected, role in practical applications: providing an encoding for data mining tasks and enabling robust link prediction \cite{portisch20221rd}. For data mining, the embeddings serve as rich feature vectors for entities, which can then be directly utilized in tasks like clustering, classification, or recommendation systems. For link prediction, KGE models learn a scoring function that quantifies the plausibility of a given triple $(h, r, t)$, allowing for the completion of incomplete KGs by predicting missing entities or relations. This capability is fundamental for a wide array of tasks:
\begin{itemize}
    \item \textbf{Link Prediction}: Identifying missing facts within a KG, crucial for knowledge graph completion and enrichment.
    \item \textbf{Entity Alignment}: Discovering equivalent entities across different KGs, facilitating the integration and merging of heterogeneous knowledge sources.
    \item \textbf{Question Answering}: Enhancing natural language understanding by providing semantic context from KGs to answer complex queries.
    \item \textbf{Recommendation Systems}: Improving personalized recommendations by leveraging relational information between users, items, and their attributes.
\end{itemize}

However, this transformation from symbolic to continuous representations is not without trade-offs. The resulting dense vectors often lack the direct interpretability and logical guarantees inherent in symbolic systems, making it challenging to understand *why* a particular prediction was made. This limitation has spurred significant research into explainable KGE models \cite{kurokawa2021f4f} and methods that integrate logical rules directly into the embedding process, such as RulE \cite{tang2022}, which learns explicit rule embeddings to provide soft, context-dependent inference.

The evolution of KGE has seen models progress significantly. Early translational models like TransE, while groundbreaking, faced limitations, such as conflicts between their core principle and regularization strategies, leading to warped embeddings \cite{ebisu2017}. This spurred advancements like TorusE \cite{ebisu2017}, which embeds entities and relations on a Lie group (a torus) to inherently bound embeddings and resolve regularization conflicts, improving accuracy and efficiency. More recent models, such as HolmE \cite{zheng2024}, have introduced novel Riemannian KGE frameworks designed to be "closed under composition," allowing for robust modeling of complex, under-represented relational patterns and providing a unifying theoretical framework for many existing models. Other models, like TransF \cite{do20184o2}, address scalability challenges by explicitly modeling basis subspaces for projection matrices, making them robust to KGs with thousands of relations. This continuous innovation aims to enhance expressiveness, capture diverse relational patterns (e.g., symmetry, anti-symmetry, hierarchy, composition), and improve the robustness and generalizability of embeddings. Ultimately, KGE transforms complex, discrete information into a machine-readable, statistically analyzable format, offering scalability and improved performance across a wide spectrum of AI applications by effectively bridging the gap between symbolic knowledge and neural reasoning.