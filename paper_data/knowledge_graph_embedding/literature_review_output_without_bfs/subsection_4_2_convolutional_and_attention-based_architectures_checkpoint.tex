\subsection{Convolutional and Attention-based Architectures}

The evolution of Knowledge Graph Embedding (KGE) has seen a significant shift from models relying on fixed scoring functions to sophisticated neural architectures that enable data-driven feature learning, particularly through Convolutional Neural Networks (CNNs) and attention mechanisms. These architectures excel at capturing complex interactions and relational patterns, leading to more expressive and contextually aware embeddings by moving beyond simplistic, fixed scoring functions. CNNs primarily focus on extracting local, non-linear features from structured input, while attention mechanisms dynamically weigh and aggregate information, proving particularly effective for the irregular structures of knowledge graphs.

Initial convolutional approaches, such as ConvE \cite{dettmers2018}, marked a departure from simpler models by applying convolutional filters to concatenated entity and relation embeddings. This allowed for the learning of rich, non-linear feature maps, significantly improving the expressiveness for triplet plausibility prediction. ConvE's strength lies in its parameter efficiency and ability to capture intricate interaction patterns, but its fixed filters can struggle with the diverse and often complex relational patterns (e.g., 1-to-N, N-to-1, N-to-N) inherent in KGs, as it treats all relations uniformly. Building on this foundation, the Multi-Scale Dynamic Convolutional Network (M-DCN) \cite{zhang2020} addressed these limitations by introducing dynamic, multi-scale convolutional filters whose weights are specifically tailored to each relation. This innovation enabled more nuanced feature extraction and better handling of complex relation patterns compared to ConvE's static filters. Further advancing the integration of deep learning, ReInceptionE \cite{xie2020} employed an Inception network to deepen the interaction learning between head and relation embeddings, enhancing the model's capacity to capture complex features. Crucially, ReInceptionE also incorporated a relation-aware attention mechanism to integrate both local neighborhood and global entity structural information, demonstrating an early synergy between CNNs and attention for comprehensive structural awareness. Another hybrid approach, PConvKB \cite{jia20207dd}, improved upon ConvKB (a CNN-based model) by explicitly incorporating relation paths. It utilized an attention mechanism to measure the local importance of these paths and a global measure (DIPF) for their overall significance, thereby enriching the convolutional features with path-level context. In specialized domains, SEConv \cite{yang2025} for healthcare prediction demonstrated the synergy of a resource-efficient self-attention mechanism with a multi-layer CNN to learn deeper and more expressive structural features, highlighting the adaptability of these architectures to specific application needs and resource constraints. While CNNs are effective for local feature extraction and can be parameter-efficient, their inherent grid-like operations can be less intuitive for the irregular, graph-structured data of KGs, and they may struggle with modeling long-range dependencies without significant architectural depth.

To overcome the limitations of fixed feature extraction and uniform information aggregation, attention mechanisms emerged to dynamically weigh and aggregate information, particularly from an entity's neighborhood, to create more contextually rich embeddings. Pioneering inductive KGE, the Logic Attention Network (LAN) \cite{wang2018} employed a double-view attention mechanism that combined logical rule-based weighting with neural network attention. This innovative approach allowed for permutation-invariant, redundancy-aware, and query-relation-aware aggregation of neighborhood information, which is crucial for generalizing to unseen entities by focusing on relevant facts. Similarly, Graph Attenuated Attention networks (GAATs) \cite{wang2020} introduced an attenuated attention mechanism to assign varying importance to different relation paths and actively acquire information from neighboring nodes. This mechanism directly addressed the uniform weighting limitations of earlier graph-based models, allowing for a more fine-grained understanding of relational semantics by emphasizing more informative paths and neighbors. These attention-based models offer superior flexibility in capturing the varying importance of different parts of a knowledge graph, leading to more adaptive and context-sensitive embeddings. However, their computational cost can be higher than simpler models, and the interpretability of complex attention patterns remains a challenge.

In summary, convolutional and attention-based architectures have significantly advanced KGE by moving beyond fixed scoring functions to data-driven feature learning. CNNs like ConvE, M-DCN, ReInceptionE, PConvKB, and SEConv have enabled the extraction of rich, non-linear, and dynamic features from entity-relation interactions. Concurrently, attention mechanisms in models like LAN and GAATs have empowered KGE models to dynamically aggregate neighborhood information and capture complex relational patterns with varying importance. While these architectures offer superior expressiveness and contextual awareness, challenges remain in balancing their increased computational complexity with scalability for extremely large KGs, and in enhancing the interpretability of their learned features. The principles of dynamic information aggregation via attention, particularly on graph structures, laid crucial groundwork for the more formalized message-passing frameworks of Graph Neural Networks and Transformers, which are explored in the subsequent section for their ability to capture multi-hop structural dependencies more comprehensively.