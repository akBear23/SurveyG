\subsection*{Semantic Matching Models: RESCAL and ComplEx}
Moving beyond the geometric translations of early knowledge graph embedding models, alternative paradigms emerged that focused on semantic matching and tensor factorization to capture richer, more direct interactions between entities and relations. This shift introduced different mathematical frameworks for scoring triple validity, enhancing expressiveness for specific relational patterns and demonstrating the power of algebraic operations in capturing nuanced semantic relationships.

A pioneering model in this direction was RESCAL (Relational Embedding of Simple Components for ALgebraic Link prediction) \cite{Nickel_2016}. RESCAL introduced a tensor factorization approach, representing relations as full matrices that directly interact with entity vectors. For a given triple $(h, r, t)$, the model scores its validity using a bilinear form: $f(h, r, t) = \mathbf{h}^\top \mathbf{M}_r \mathbf{t}$, where $\mathbf{h}$ and $\mathbf{t}$ are vector embeddings for the head and tail entities, respectively, and $\mathbf{M}_r$ is a relation-specific matrix. This formulation allows for a richer, more direct semantic matching between entities through the relation, as the matrix $\mathbf{M}_r$ can capture complex, non-linear interactions. While RESCAL offered significant expressive power, particularly for capturing diverse relational patterns, it faced substantial scalability challenges due to the large number of parameters required for each relation matrix, leading to high computational costs and memory consumption.

Building upon the principles of tensor factorization, ComplEx (Complex Embeddings for Simple Link Prediction) \cite{Trouillon_2016} further advanced semantic matching by introducing complex-valued embeddings. In ComplEx, both entities and relations are embedded into a complex vector space, where each embedding $\mathbf{e} \in \mathbb{C}^d$ has a real part $\text{Re}(\mathbf{e})$ and an imaginary part $\text{Im}(\mathbf{e})$. The scoring function for a triple $(h, r, t)$ is defined using a Hermitian dot product: $f(h, r, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \bar{\mathbf{t}} \rangle)$, where $\bar{\mathbf{t}}$ denotes the complex conjugate of the tail entity embedding. This elegant mathematical framework naturally models symmetric and antisymmetric relations without requiring explicit constraints, a significant advantage over many translational models. For instance, if a relation is symmetric, its complex embedding can be purely real, and if it is antisymmetric, its embedding can be purely imaginary. ComplEx thus provided a more elegant and computationally efficient solution for capturing these specific relational patterns compared to the more parameter-heavy RESCAL, offering a better trade-off between expressiveness and efficiency for certain types of relations.

Both RESCAL and ComplEx represent a crucial intellectual contribution by moving beyond the strict geometric translations and demonstrating the power of algebraic operations and higher-dimensional representations in capturing nuanced semantic relationships. RESCAL laid the groundwork for tensor factorization in KGE, showcasing the potential of relation-specific matrices to model intricate interactions. ComplEx then refined this direction by leveraging complex numbers, offering a more compact and principled way to handle relational properties like symmetry and antisymmetry that are prevalent in knowledge graphs. While RESCAL's scalability remained a practical hurdle, ComplEx offered a more viable path for certain relational patterns, albeit potentially lacking the general expressiveness of more advanced neural models for highly diverse or compositional relation types. The exploration of these algebraic frameworks highlighted the diverse mathematical tools available for knowledge graph representation, paving the way for future innovations in capturing complex semantic structures.