# A Comprehensive Literature Review with Self-Reflection

**Generated on:** 2025-10-07T19:27:46.901389
**Papers analyzed:** 377

## Papers Included:
1. d899e434a7f2eecf33a90053df84cf32842fbca9.pdf [sun2018]
2. 83d58bc46b7adb92d8750da52313f060b10f201d.pdf [dasgupta2018]
3. 10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf [chen2023]
4. b1d807fc6b184d757ebdea67acd81132d8298ff6.pdf [yang2023]
5. abea782b5d0bdb4cd90ec42f672711613e71e43e.pdf [jia2015]
6. 658702b2fa647ae7eaf1255058105da9eefe6f52.pdf [lloyd2022]
7. 29eb99518d16ccf8ac306d92f4a6377ae109d9be.pdf [wu2021]
8. 58e1b93b18370433633152cb8825917edc2f16a6.pdf [xu2019]
9. d4220644ef94fa4c2e5138a619cfcd86508d2ea1.pdf [shan2018]
10. 15710515bae025372f298570267d234d4a3141cb.pdf [zheng2024]
11. 354fb91810c6d3756600c99ad84d2e6ef4136021.pdf [he2023]
12. 67cab3bafc8fa9e1ae3ff89791ad43c81441d271.pdf [xiao2015]
13. 405a7a7464cfe175333d6f04703ac272e00a85b4.pdf [guo2017]
14. 8b717c4dfb309638307fcc7d2c798b1c20927a3e.pdf [chen2021]
15. 29052ddd048acb1afa2c42613068b63bb7428a34.pdf [li2023]
16. 23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff.pdf [zhou2023]
17. af051c87cecca64c2de4ad9110608f7579766653.pdf [xiang2021]
18. 85064a4b1b96863af4fccff9ad34ce484945ad7b.pdf [cao2022]
19. 06315f8b2633a54b087c6094cdb281f01dd06482.pdf [wang2021]
20. a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0.pdf [guo2020]
21. 3ac716ac5d47d4420010678fda766ebb5b882ba9.pdf [zhang2024]
22. 933cb8bf1cd50d6d5833a627683327b15db28836.pdf [shen2022]
23. bb3e135757bfb82c4de202c807c9e381caecb623.pdf [hu2024]
24. 398978c84ca8dab093d0b7fa73c6d380f5fa914c.pdf [liu2024]
25. b594b21557395c6a8fa8356249373f8e318c2df2.pdf [zhang2019]
26. 3e3a84bbceba79843ca1105939b2eb438c149e9e.pdf [yang2019]
27. b3f0cdc217a3d192d2671e44913542903c94105b.pdf [xie2023]
28. 52eb7f27cdfbf359096b8b5ef56b2c2826beb660.pdf [wang2024]
29. ecb80d1e5507e163be4a6757b00c8809a2de4863.pdf [xiao2019]
30. 33d469c6d9fc09b59522d91b7696b15dc60a9a93.pdf [sachan2020]
31. 4801db5c5cb24a9069f2d264252fa26986ceefa9.pdf [madushanka2024]
32. a166957ec488cd20e61360d630568b3b81af3397.pdf [zhu2022]
33. bcffbb40e7922d2a34e752f8faaa4fe99649e21a.pdf [liang2024]
34. 7029ecb5d5fc04f54e1e25e739db2e993fb147c8.pdf [li2024]
35. 990334cf76845e2da64d3baa10b0a671e433d4b6.pdf [ebisu2017]
36. 0367603c0197ab48eeba29aa6af391584a5077c0.pdf [zhang2021]
37. 7572aefcd241ec76341addcb2e2e417587cb2e4c.pdf [huang2019]
38. c2c6edc5750a438bddd1217481832d38df6336de.pdf [tang2019]
39. a6a735f8e218f772e5b9dac411fa4abea87fdb9c.pdf [sun2018]
40. f2b924e69735fb7fd6fd95c6a032954480862029.pdf [ge2023]
41. e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a.pdf [wang2020]
42. 63836e669416668744c3676a831060e8de3f58a1.pdf [li2022]
43. 11e402c699bcb54d57da1a5fdbc57076d7255baf.pdf [zhang2019]
44. 191815e4109ee392b9120b61642c0e859fb662a1.pdf [tang2022]
45. d3c287ff061f295ddf8dc3cb02a6f39e301cae3b.pdf [lv2018]
46. c64433657869ecdaaa7988a029eabfe774d3ac47.pdf [chen2025]
47. 8fef3f8bb8bcd254898b5d24f3d78beab09e99d4.pdf [qian2021]
48. 68f34ed64fdf07bb1325097c93576658e061231e.pdf [dai2020]
49. efea0197c956e981e98c4d2532fa720c58954492.pdf [ji2024]
50. f470e11faa6200026cf39e248510070c078e509a.pdf [yan2022]
51. 5dc88d795cbcd01e6e99ba673e91e9024f0c3318.pdf [zhang2023]
52. 0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf [li2021]
53. 33f3f53c957c4a8832b1dcb095a4ac967bd89897.pdf [yang2025]
54. 2e925a02db26a60ee1cc022f3923e09f3fae7b39.pdf [wang2019]
55. 040fe47af8f4870bf681f34861c42b3ea46d76cf.pdf [di2023]
56. c762e198b0239313ee50476021b1939390c4ef9d.pdf [jia2017]
57. 1f20378d2820fdf1c1bb09ce22f739ab77b14e82.pdf [choudhary2021]
58. 991b64748dfeecf026a27030c16fe1743aa20167.pdf [xiao2015]
59. 6a2f26cece133b0aa52843be0f149a65e78374f7.pdf [hu2024]
60. 2a3f862199883ceff5e3c74126f0c80770653e05.pdf [wang2014]
61. 21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3.pdf [zhu2020]
62. acc855d74431537b98de5185e065e4eacbab7b26.pdf [ali2020]
63. 2a25540e3ce0baba56ee71da7ca938f0264f790d.pdf [mohamed2020]
64. d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4.pdf [gao2020]
65. d7ef14459674b75807cd9be549f1e12d53849ead.pdf [peng2021]
66. 3f170af3566f055e758fa3bdf2bfd3a0e8787e58.pdf [shi2025]
67. 5b5b3face4be1cf131d0cb9c40ae5adcd0c16408.pdf [zhang2024]
68. f4e39a4f8fd8f8453372b74fda17047b9860d870.pdf [rosso2020]
69. 6a86594566fc9fa2e92afb6f0229d63a45fe25e6.pdf [zhou2024]
70. 1620a20881b572b5ffc6f9cb3cf39f6090cee19f.pdf [xie2020]
71. 83a46afaeb520abcd9b0138507a253f6d4d8bff7.pdf [song2021]
72. f44ee7932aacd054101b00f37d4c26c27630c557.pdf [zhang2020]
73. 44ce738296c3148c6593324773706cdc228614d4.pdf [ge2022]
74. bcdb8914550df02bfe1f69348c9830d775f6590a.pdf [ren2020]
75. 77dc07c92c37586f94a6f5ac3de103b218931578.pdf [yuan2019]
76. d1a525c16a53b94200029df1037f2c9c7c244d7b.pdf [xiao2015]
77. 8f096071a09701012c9c279aee2a88143a295935.pdf [sun2018]
78. 18bd7cd489874ed9976b4f87a6a558f9533316e0.pdf [ji2015]
79. 0364e17da01358e2705524cd781ef8cc928256f5.pdf [lin2020]
80. fda63b289d4c0c332f88975994114fb61b514ced.pdf [islam2023]
81. 3f0d5aa7a637d2c0bb3d768c99cc203430b4481e.pdf [wang2021]
82. 2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83.pdf [broscheit2020]
83. 84aa127dc5ca3080385439cb10edc50b5d2c04e4.pdf [fanourakis2022]
84. 727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf [wang2018]
85. 19a672bdf29367b7509586a4be27c6843af903b1.pdf [tabacof2019]
86. ecc04e9285f016090697a1a8f9e96ce01e94e742.pdf [pei2019]
87. beade097ff41c62a8d8d29065be0e1339be39f30.pdf [zhang2018]
88. bbb89d88ad5b8279709ff089d3c00cd2750cd26b.pdf [li2021]
89. d605a7628b2a7ff8ce04fc27111626e2d734cab4.pdf [li2022]
90. 322aa32b2a409d2e135dbb14736d9aeb497f1c52.pdf [ding2018]
91. b2d2ad9a458bdcb0523d22be659eb013ca2d3c67.pdf [zhang2022]
92. ce7291c5cd919a97ced6369ca697db9849848688.pdf [sun2024]
93. 780bc77fac1aaf460ba191daa218f3c111119092.pdf [wang2024]
94. 6205f75cb6db1503c94386441ca68c63c9cbd456.pdf [modak2024]
95. e379f7c85441df5d8ddc1565cabf4b4290c22f1f.pdf [xiao2016]
96. c180564160d0788a82df203f9e5f61380d9846aa.pdf [zhang2023]
97. 69418ff5d4eac106c72130e152b807004e2b979c.pdf [guo2015]
98. 552bfaca30af29647c083993fbe406867fc70d4c.pdf [xu2020]
99. 33a7b7abf006d22de24c1471e6f6c93842a497b6.pdf [zheng2024]
100. 86ac98157da100a529ca65fe6e1da064b0a651e8.pdf [zhang2018]
101. 52b167a90a10cde25309e40d7f6e6b5e14ec3261.pdf [zhu2024]
102. 145fa4ea1567a6b9d981fdea0e183140d99aeb97.pdf [liu2023]
103. e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c.pdf [choi2020]
104. 4085a5cf49c193fe3d3ff19ff2d696fe20a5a596.pdf [ge2023]
105. 4e52607397a96fb2104a99c570c9cec29c9ca519.pdf [sadeghian2021]
106. eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf [liu2024]
107. 7e5f318bf5b9c986ca82d2d97e11f50d58ee6680.pdf [li2022]
108. 8c93f3cecf79bd9f8d021f589d095305e281dd2f.pdf [rossi2020]
109. cab5194d13c1ce89a96322adaac754b2cb630d87.pdf [li2023]
110. 95c3d25b40f963eb248136555bd9b9e35817cc09.pdf [peng2020]
111. 12cc4b65644a84a16ef7dfe7bdd70172cd38cffd.pdf [ji2024]
112. 40479fd70115e545d21c01853aad56e6922280ac.pdf [zhang2024]
113. 5515fd5d14ac7b19806294119560a8c74f7fa4b2.pdf [kochsiek2021]
114. e5c851867af5587466f7cd9c22f8b2c84f8c6b63.pdf [yang2021]
115. eb14b24b329a6cc80747644616e15491ef49596f.pdf [shang2024]
116. 9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf [asmara2023]
117. d9802a67b326fe89bbd761c261937ee1e4d4d674.pdf [gregucci2023]
118. b307e96f59fde63567cd0beb30c9e36d968fad8e.pdf [pan2021]
119. e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d.pdf [yoon2016]
120. c075a84356b529464df2e06a02bf9b524a815152.pdf [li2024]
121. b30481dd5467a187b7e1a5a2dd326d97cafd95ac.pdf [xiong2017zqu]
122. 2930168f3be575781939a57f4bb92e6b29c33b08.pdf [gong2020b2k]
123. da60d33d007681743d939861ae24f4cdac15667e.pdf [zhou2022ehi]
124. bb65c0898647c57c87a72e80d97a53576e3034ca.pdf [le2022ji8]
125. c03965d00865074ae66d0324c7145bf59aec73e6.pdf [zhou2022vgb]
126. 4b0e3d0721ea9324e9950b3bb98d917da8acb222.pdf [xu2019t6b]
127. 8df10fa4eca07dbb5fe2fe2ecc1e546cb8a8c947.pdf [mezni20218ml]
128. d6cc2a58df29d3e3fe4c55902880908dde32ee60.pdf [do2021mw0]
129. a57af41c3845a6d15ffbe5bd278e971ca9b8124a.pdf [mai2020ei3]
130. 8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf [zhang2022eab]
131. 23ae48cdb8b7985e5a32fc79b6aae0de3230fe4f.pdf [sosa2019ih0]
132. 87ccb0d6c3e9f6367cd753538f4e906838cea8c2.pdf [guan2019pr4]
133. 0dddf37145689e5f2899f8081d9971882e6ff1e9.pdf [fan2014g7s]
134. 4be29e1cd866ab31f83f03723e2f307cdc1faab0.pdf [zhang20190zu]
135. 2a81032e5bb4b29f6e1423b6083b9a04bb54b605.pdf [chen2022mxn]
136. c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf [wang2022hwx]
137. d97ec8a07cea1a18edf0a20981aad7e3dfe351e6.pdf [chen20226e4]
138. 389935511c395526817cf4ae62dae8913845ebdf.pdf [abusalih2020gdu]
139. ba524aa0ae24971b56eef6e92491de07d097a233.pdf [fang2022wp6]
140. a264af122f9f2ea5df46c030beb8ec0c25d6e907.pdf [elebi2019bzc]
141. 90450fe686c0fa645a1954950adffc5b2401e4b7.pdf [sha2019i3a]
142. 2257eb642e9ecae24f455a58dc807ee2a843081f.pdf [li2021ro5]
143. d77de3a4ddfa62f8105c0591fd41e549edcfd95f.pdf [xiao20151fj]
144. 52457f574780c53c68ad645fcdc86e2492b5074a.pdf [zhang2021wg7]
145. ac79b551ca16f98c1c3a5592c22d8093a492c4f3.pdf [wang20186zs]
146. 0abee37fe165b86753b306ffcc59a77e89de0599.pdf [li2021x10]
147. 512177d6b1e643b49b1d5ab1ad389666750144a9.pdf [wang202110w]
148. 60347869db7d1940958ee465b3010b3a612bf791.pdf [gutirrezbasulto2018oi0]
149. 9f7731d72e2aa251d2994eb1729c22aa78d0f718.pdf [portisch20221rd]
150. c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a.pdf [zhang2022muu]
151. 4ac5f7ad786fbee89b04023383a4fbe095ccc779.pdf [feng2016dp7]
152. 9fc2fd3d53a04d082edc80bafa470a66acdebb14.pdf [liu2021wqa]
153. 747dff7b9cd0d6feb16c340b684b1923034e8777.pdf [sang2019gjl]
154. 3e76e90180fc8300ecdeb5b543015cc68e0fd249.pdf [wang2017yjq]
155. 547dfe2a9d6a1bb1023f2208fb31f3a0671bf9ca.pdf [jiang20219xl]
156. 39eb51ae87c168ad4339214de6b91e2e2fdcfaa1.pdf [liu2022fu5]
157. fee5ac3604ccdefee2b65275fed47503234099e2.pdf [khan202236g]
158. 154fac5040865b4d74cf5a2cad39381c134a8b7d.pdf [mezni2021ezn]
159. 543497b1e551ad6473ddb9aa46697db28bccd3f5.pdf [zhang2021wix]
160. 6cc55dec26f5c078c6872d612c1561b1646d459a.pdf [huang2021u42]
161. ee5ceab9fa5f3bad231469923a03ad16184b51b9.pdf [pavlovic2022qte]
162. 3705cfe0d7dab8881518cb932f2465ca432d3f24.pdf [wang20213kg]
163. 882d6fe22a093ff95a8106a215bca37603ada710.pdf [zhang2019rlm]
164. 92ef8ff6715733697ca915c65cb18b160a764da6.pdf [mai20195rp]
165. a0ca7d39296d8d31dbbf300f58e7e375fb879492.pdf [han2018tzc]
166. 9155e1340e9263cf042d144681acccfc0c9d194b.pdf [wang2022fvx]
167. b5167990eda7d48f1a70a1fcb900ed5d46c40985.pdf [ferrari2022r82]
168. 0a8faa6c0e6dc9f743e96f276239d02d8839aca2.pdf [fu2022df2]
169. 71245f9d9ba0317f78151698dc1ddba7583a3afd.pdf [wu2018c4b]
170. f0499c2123e17106039e8e772878aad073ccf916.pdf [zhang202121t]
171. 2bdb9985208a7c7805676029300e3ba648125bd1.pdf [mohamed2019meq]
172. 7ccb05062f9ea7179532fd3355cf984b0102cfc5.pdf [xin2022dam]
173. c8214cac9c841f7b295a78c5bf71b6ed37c40eec.pdf [nie20195gc]
174. dab87bce4ac8c6033f5836f575b57c4a665b4f49.pdf [liu2018kvd]
175. 7ae22798887ff4e19033a8028007e1780b53ba8c.pdf [ni2020ruj]
176. 01c1e7830031b25410ed70965d239ac439a6fb68.pdf [li20215pu]
177. 021cbcd59c0438ac8a50c511be7634b0c00a1b89.pdf [yu2019qgs]
178. f211a2123e28d60cd8cdc05449c3cb7da2610b0a.pdf [fatemi2018e6v]
179. 3646e2947827c0a9314443e5cbb15575fafaf4ba.pdf [chen2021i5t]
180. 67c03d7a477059dc20faa02e3b45ca7055433615.pdf [dong2022c6z]
181. 91d8e1339eddee3217a6897cebdeb526b4bb1f72.pdf [lu20206x1]
182. b1464e3f0c82e21e23dfd9bc28e423856754b3d6.pdf [li2022nr8]
183. 57a7804d4e4e57de9a5c096ce7ea3e50d2c86f0f.pdf [luo2015df2]
184. 678dacdf029becac1116f345520f8e4afff5a873.pdf [zhou20216m0]
185. 1a25c8afacb6d36d4d8635eb9e3f8b8cf2e2122c.pdf [zhao202095o]
186. 60ad3ce0492a004020ff55653a51d6bfc457f12d.pdf [jia201870f]
187. 434b32d34b5d21071fc78a081741757f263c14ae.pdf [mai2018u0h]
188. 4a96636d1fc92221f2232d2d74be6e303cd0642a.pdf [li201949n]
189. 9c17d3f1837ae9f10f57c0b07c8288137d84026b.pdf [tang2020ufr]
190. e740a9aa753fcc926857ef4b90c1f91dd086e08d.pdf [guo2022qtv]
191. 315b239040f73063076014fdfabcc621b2719d83.pdf [jiang202235y]
192. 96b1f6fb6e904a674aef5cd32efee3edfa1c8ee2.pdf [liu201918i]
193. 5d6b4c5e48ec0585facea96a746bcbf7225d424c.pdf [zhang2020s4x]
194. 441f124d48662d6bd4f8e3190633371aa1b034eb.pdf [chang20179yf]
195. 5f9ea28be0d3bb9a73d62512190a772b10e92db0.pdf [lee2022hr9]
196. 836d1d1c94f0fd0713c77b86ce136fffd059dbc0.pdf [zhang2022fpm]
197. 0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf [liu2019e1u]
198. 00529345e4a604674477f8a1dc1333114883b8d9.pdf [song2021fnl]
199. f0d5351c76448e28626177ece5ce97715087a0f9.pdf [gradgyenge2017xdy]
200. 9866a21c0ada20b62b28b3722c975595be819e24.pdf [zhou20218bt]
201. 50e7017c7768b7b2f5215a35539db1490ddc37ab.pdf [chen20210ah]
202. 95a501bfe4b09323e6e178edd64dc24a6935c23f.pdf [zhang2020i7j]
203. 46b5198a535dfcaf1cc7d57d471ad9ec050e46cf.pdf [boschin2020ki4]
204. cda7a1bdce2bfa77c2d463b631ff84b69ce3c9ed.pdf [wang20199fe]
205. f76a6e8f059820667af53edbd42d33fc4bca85fd.pdf [myklebust201941l]
206. 40667a731593a44d4e2f9391f1d14f368321b751.pdf [kartheek2021aj7]
207. 6bf53a97f5a3f5b0375f4702cbec28d8e9ab61c0.pdf [sha2019plw]
208. 4ae2631fb5e99cb64ff7d6e7ed3a1e6b0bedd269.pdf [lu2020x6y]
209. d76b3bf29366b4f0902ea145a3f7c020a35f084f.pdf [zhang2020c15]
210. 151c9bb547306d66ba252be7c20e35f711e9f330.pdf [li2020ek4]
211. c0827be29366be4b8cfa0dfbef4ead3f7b08f562.pdf [li2020he5]
212. 2d38cdaf2e232b5d1cb1dce388aa0fe75babcf29.pdf [kim2020zu3]
213. d6508e8825a6a1281ad415de47a2f108d98df87d.pdf [zhu2018l0u]
214. 18101998fb57704b79eb4c4c37891144ede8f8b9.pdf [do20184o2]
215. 23830bb104b25103162ec9f9f463624d9a434194.pdf [ma20194ua]
216. 77e23cd2437c6afb16082793badbb02842442e13.pdf [zhang2020wou]
217. 92351a799555df8d49465c2d4959118030339cc0.pdf [zhang2019hs5]
218. 6de535eb1b0024887227f7987e6eb22478af2a95.pdf [wang20198d2]
219. be7b102315ce70a7e01eb87c1140dd6850148e8b.pdf [tran20195x3]
220. 5b6a24ea3ffdccb14ce0267a815845c62ef026c9.pdf [xiong2018fof]
221. 75f7e3459e53fa0775c941cb703f049797851ef0.pdf [radstok2021yup]
222. 3ea066e35fdd45162a7fa94e995abe0beeceb532.pdf [zhao2020o6z]
223. c7a630751e45e3a74691bd0fc0880b4bf87be101.pdf [zhang20182ey]
224. a2a7f85d2ba28750725c4956eb14d53f6a90f003.pdf [jia20207dd]
225. bb0613ea0d39e35901aa0018de40deaf35cbbd5d.pdf [zhu2019ir6]
226. 509fa029989e89a4b82dd01ab75734aed937d684.pdf [wang2021dgy]
227. 4f2cc26b689cdac36ceb2037338eac65e7e5a193.pdf [ning20219et]
228. 7bb4cd36de648ca44cc390fe886ee70a4b2ad1ac.pdf [sheikh20213qq]
229. 93db6077c12cc83ea165a0d8851efb69b0055f3a.pdf [rim2021s9a]
230. 2f700be8a387101411a84199adfe30636e331752.pdf [zhang20179i2]
231. 2dba03d338170bde4e965909230256086bafa9f8.pdf [elebi20182bd]
232. c2648a294ef2fc299e1dd959bc1f92973f9c9ebc.pdf [garofalo20185g9]
233. 62c50e300ee87b185401ce27323bbb3f5262fdff.pdf [wang201825m]
234. 66f19b09f644578f808e69f38d3e76f8b972f813.pdf [chung2021u2l]
235. 9b68475f787be0999e7d09456003e664d37c2155.pdf [tran2019j42]
236. f0ac0c2f82886700dc7e7a178d597d33deebfc88.pdf [shi2017m2h]
237. a5aeca7ef265b27ff6d9ea08873c9499632b6439.pdf [zhang2017ixt]
238. 8412cc4dd7c8d309d7a84573637d4daaad8d33b5.pdf [zhu20196p1]
239. 8be21591c29d68d99e89a71fc7755f09f5eed3a1.pdf [kertkeidkachorn2019dkn]
240. 6493e6d563282fcb65029162a71cd2cb8168765b.pdf [zhu2019zqy]
241. d5eabc89e2346411134569a603e63a143d1d6552.pdf [zhang20193g2]
242. 89cf9719b97e69f5bb7d715d5a16609676c14e86.pdf [liu2019fcs]
243. 1c1b5fd282d3a1fe03a671f7d13092d49cb31139.pdf [kanojia20171in]
244. 7f7137d3e1de7e0e801c27d5e8b963dfd6d94eb4.pdf [gao2018di0]
245. 49899fd94cd272914f7d1e81b0915058c25bb665.pdf [mai2018egi]
246. e64557514ab856d22ddbb34bc23ffb7085d5d6b0.pdf [xiao2016bb9]
247. 7eece37709dceba5086f48dc43ac1a69d0427486.pdf [liu2024q3q]
248. 83424a4fea2e75311632059914bf358bc045435f.pdf [zhang2024cjl]
249. 3f8b13ede9f4d3a770ec8b4771b6036b9f603bfa.pdf [su2023v6e]
250. ac0c9afa9c19f0700d903e00a92e83e41587add3.pdf [zhu2023bfj]
251. f42d060fb530a11daecd90695211c01a5c264f8d.pdf [liu2024to0]
252. 7aca91d068d20d3389b28b8277ebc3d488be459f.pdf [wang2024vgj]
253. fa07384402f5c9d5b789edf7667bbcc555f381e3.pdf [li2024920]
254. 48c2e0d87b84efca7f11462bbdac1be1177e2433.pdf [lee202380l]
255. 51c18009b2c566d7cddc934b2cf9a1bca813f58f.pdf [shokrzadeh2023twj]
256. 5cbf9bc26b3d0471cb37c3f4a931990b1260d82d.pdf [gao2023086]
257. 4383242be5bdfb30ffa84e58cc252acfb58d4878.pdf [li2024sgp]
258. f26d45a806d1f1319f37eb41b8aa87d768a1d656.pdf [xue2023qi7]
259. 7b569aecc97f5fe57ce19ca0670a6b1bc62c7f7c.pdf [duan2024d3f]
260. 8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf [chen20246rm]
261. e83b693a44ec32ddfb084d13138e8d7ebc85a7c3.pdf [zhu2022o32]
262. f284977aa917be0ff15b835b538294b827135d19.pdf [mitropoulou20235t0]
263. f3fa1ef467c996b30242124a298b5b9d031e9ed5.pdf [shomer2023imo]
264. 61ef322fba87ccfd36c004afc875542a290fe879.pdf [wang202490m]
265. 5bef4d28d12dd578ce8a971d88d2779ec01c7ec5.pdf [li2024bl5]
266. c441b2833db8bd96b4ad133679a68f79d464ef59.pdf [li2024y2a]
267. edfbe0b62b9f628858d05b64bd830cf9b0a1ab74.pdf [jia2023krv]
268. 88e700e9fd6c14f3aa4502176a60512ca4020e35.pdf [huang2023grx]
269. 942541df1b97a9d1e46059c7c2d11503adc51c4c.pdf [wang2023s70]
270. abc424e17642df01e0e056427250526bc624f762.pdf [hou20237gt]
271. 825d7339eadadd2baf962f7d3c8fe7dc0cdc9819.pdf [jiang2023opm]
272. b6839f89a59132f0e62011a218ec229a27ffff6b.pdf [lu2022bwo]
273. 59116a07dbdb3cdeebb20085fdfde8b899de8f6a.pdf [djeddi2023g71]
274. 3cab78074e79122fd28cd76f37fd8805e8e4fc31.pdf [zhang20243iw]
275. ed21098804490b98899bcb7195084983ce69ed6c.pdf [le2023hjy]
276. 354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf [yao2023y12]
277. c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf [li2023y5q]
278. dc949e502e35307753a1acbcdf937f0cd866e63b.pdf [yang2022j7z]
279. a64167fcaa7a487575c6479510e57795afc9974e.pdf [banerjee2023fdi]
280. f9a575349133b2d4bf512cfb7754fca6d13b0a81.pdf [hu20230kr]
281. 5f850f1f522f959e2d3dcad263d05b0fdbb187c3.pdf [li2023wgg]
282. 4c68ee32d3db73d4d05803c1b3f2f4b929a88b78.pdf [hao2022cl4]
283. 2ac47be80b02a3ff1b87c46cf2b8c27e739c2873.pdf [khan20222j1]
284. b5aedc0464d04aa3fed7e59a141d9be7ee18c217.pdf [le2022ybl]
285. 463c7e28be13eb02620ad7e29b562bf6e5014ba2.pdf [liang202338l]
286. 7009fd9eb533df6882644a1c8e1019dc034b9cc5.pdf [khan2022ipv]
287. e186e5000174ea70729c90d465e60279c5f88646.pdf [he2022e37]
288. 70dc4c1ec4cda0a7c88751fb9a6b0c648e48e11f.pdf [shen2022d5j]
289. 5a8c6890e524b708dc262d3f456c985e8a46d7d1.pdf [di20210ib]
290. 86631a005e1a88a66926ac0c364ed0101a02b7e7.pdf [niu2020uyy]
291. 92b9aeabaaac0f20f66c5a68fbb4fc268c5eaae5.pdf [nie2023ejz]
292. ce494973ceefe5ac011f7e9879843530395fa9db.pdf [li2022du0]
293. 25edfb99d3b8377a11433cf7be2bcd9f8bfbdb87.pdf [daruna2022dmk]
294. 709a128e752414c973613814ddc2509f2abe092f.pdf [zhou20210ma]
295. 18fd8982051fc1de652a9882c2c52db11bca646b.pdf [kun202384f]
296. a7f0b4776d3df11cf0d0e72785c3035cc744726c.pdf [dong2022taz]
297. e2783f8aa4c61443760a8754cd6d88165d50b213.pdf [kamigaito20218jz]
298. 77fedfa533871c6c4218285493f725d5df4e74e5.pdf [krause2022th0]
299. 695ef4cf57b4fd0c7ec17a6e10dffade51f38179.pdf [zhang20213h6]
300. 90d5e74b18d03f733c6086418bfe9b20bb6a0a69.pdf [li2021tm6]
301. c495b2780accfbb53a932181e3c9fd957d16895d.pdf [wang2020au0]
302. 85bfec413860c072529ab8399676ab4b072f2e34.pdf [wei20215a7]
303. a89f61021e5382912aaeb3f69a6d8a6265787af4.pdf [zhang2021rjh]
304. b3cbbc1f34a20c22853f3dd347fd635b2e414fd5.pdf [sheikh202245c]
305. df7265b4652b21bc690497b3967a708d811ddd23.pdf [ren2021muc]
306. f6182d5c14c6047d197f1af842862653a13238f2.pdf [eyharabide2021wx4]
307. 082856e9b36fac60b9b9400abffaff0e74552fe1.pdf [hong2020hyg]
308. b25744d3c5d93e49b1906991dc8b5426ea2cf51d.pdf [huang2020sqc]
309. 18bcad2521cbe8df9d84b1adff1dd57c72c68a9d.pdf [kurokawa2021f4f]
310. bdd6c1a6695e3d201b70f4a913ffc758b74216e7.pdf [mohamed2021dwg]
311. e93565f447a42b158df27ba75385f5e2fc30dde7.pdf [gebhart2021gtp]
312. cf436f34ca6aabe1971c3531d465ecaa3d480d68.pdf [deng2024643]
313. 76016197d7d4f2213a4ace29988c93285793e154.pdf [liu2024zr9]
314. 9730f484b84074c1d61c154211ea06cc6ff20940.pdf [zhang2024zmq]
315. 10c388fa25dd6f07707a414946e5b7a674e7155b.pdf [he2024vks]
316. 7e6a50b70223dc00c712a17537fb7e23f8fd5ad4.pdf [zhang2024fy0]
317. ae58ebc99f67eed0de7f4ba2ca6f7ceb9ab056fb.pdf [zhang2024ivc]
318. 6ad02ad36e7a2c7d72d1a0b15ffc61dae2be1d7a.pdf [jing2024nxw]
319. 75ba0b92bcf095e7cd1544425f1818fed195f83f.pdf [jiang2024zlc]
320. 905d27e361c50da406439bdac25807dd38258fd8.pdf [han2024u0t]
321. b2646d9ee88c3dd6822b039a38c9604932aaaf47.pdf [quan2024o2a]
322. c7666fbaa49da21c465dbfabcf5fdd768b8c7b9e.pdf [liu2024tc2]
323. f1b7682df472a88fbaac3e6049f638ecec6937e7.pdf [hello2024hgf]
324. d66622beef468f7b934a5bf601cb8a3fcefe78f3.pdf [li2024z0e]
325. 20486c2fb358730ee99ae39b5e0a88d7b39ca720.pdf [yan2024joa]
326. b49f6029d681ac286ab929238f5aef5f352767c8.pdf [liu2024tn0]
327. c5a19440511a741edd1581d41d37d3e9b7088186.pdf [wang20245dw]
328. 822ad7c33316202a2511d300c6b8a263b758ad1a.pdf [long2024soi]
329. ba61c59abb560ff47a8dd780c8ccffb0af5e14c2.pdf [zhou2024ayq]
330. b3c340aa22bcd183c41836ef7265d656f741911f.pdf [huang2024t19]
331. 7c82aa0ae4b4e027a2df8afe9bbeccf88368c62f.pdf [lu2024fsd]
332. 0d9a788260e3abff4794d79f72b2b5ab2fb5abe5.pdf [liu2024yar]
333. 6cba788eea4fdb3bd0d1db4ecdd8a70040b81e62.pdf [khan20242y2]
334. 6c195ec2d5a491ffca9ab893968c4d44a6d0ce7d.pdf [xue2025ee8]
335. 37b274eb6fa68dede9f4aaad6dec1e2ea56095ce.pdf [long20248vt]
336. 9be88067bd7351b36bb0c698f5559ced3918a1d5.pdf [huang20240su]
337. e0d17f8b2fffff6c5eaf3f13bc45126196ddd128.pdf [wang2024nej]
338. a4b6e13efa80bedf8e588ac69f91fdaecc8e5077.pdf [wang2024c8z]
339. ccb6674576de48f8cfd99374c3b737a94dc3cb98.pdf [liu2024x0k]
340. 75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf [li2024uio]
341. 8ff387296878f23632a588076823b160673866ab.pdf [zhang2024z78]
342. 6a66b459955959c4b8a67bd298ed291506923b7a.pdf [wang2024534]
343. 6b69c8848a1cc50ed8775beb483c71cfc314c66b.pdf [ni202438q]
344. d57e01d80c7f0f86b5e3f096b193ab9210e9095f.pdf [nie202499i]
345. a9bfb9ab236553768782f2b90a69c5625f033186.pdf [wang2024d52]
346. 6903aea3553a449257388580028e0bddf119d021.pdf [mao2024v2s]
347. 767d56fe80f7681b97943a8bff39f0b580e4acd8.pdf [jafarzadeh202468v]
348. 9e7799ef313143aa9c0669a7d1918fcfd5d21359.pdf [wang2024dea]
349. 563b3d57927b688e59322dbbfc973e5f1b269584.pdf [lu202436n]
350. 984c18fa61b10b6d1c34affc98f27ca8344d4224.pdf [han2024gaq]
351. 4a0048f1942a68e7c39adac43588d1604af26fc7.pdf [liu2024jz8]
352. 49dfd47177fa3aeab8a6bea82a77ec8bdb93bf1e.pdf [he2024y6o]
353. 2a5c888b2df4fd8c49aef46ee065422b00b178c0.pdf [fang20243a4]
354. 48c07506022634f332b410fb59dca9f61f89b032.pdf [zhang2024h9k]
355. 575af1587dea578d48eb27f45f008203565d9170.pdf [li2024wyh]
356. 7bd50842503e23e6479447b98912ac482ef43adc.pdf [dong2024ijo]
357. 4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf [wang20246c7]
358. c3861a930a65e8d9ee7ab9f0a6ee71e0e59df7ed.pdf [zhang2024yjo]
359. 217a4712feae7d7590d813d23e88f5fbb4f2c37f.pdf [liang20247wv]
360. cf696a919b8476a4d74b8b726e919812a2f05779.pdf [liu2024t05]
361. 91d5aa3d43237ec60266563ec6e8079f86532cfa.pdf [pham20243mh]
362. 58480444670ff933fe644563f7e2948a79503442.pdf [li2024gar]
363. 9b836b4764d4f6947ac684fd4ba3e8c3597d95bd.pdf [li2024nje]
364. bd0e8d6db97111686d02b51134f87439f8f1acfa.pdf [bao20249xp]
365. bea79d59ab3d203d06c88ebf67ac47cb34adeaa7.pdf [xu2024fto]
366. 241904795d94dcb1946ad46c9184c59899783af1.pdf [liang2024z0q]
367. 55dab161c25d1dd04fbeecdeca085274bfe8463f.pdf [liu2024ixy]
368. 3ff6b617cd839c9d85cb7b58aa6ad56e95b6cf69.pdf [dong2025l9k]
369. 9560ca767022020ccf414a2a8514f25b89f78cb3.pdf [zhang2025ebv]
370. d5c8dcc8f5c87c269780c7011a355b9202858847.pdf [liu20242zm]
371. a77b3c5f532e61af63a9d95e671ce02d8065ee24.pdf [yang2024lwa]
372. 2d12d1cec23e1c26c65de52100db70d91ca90035.pdf [li20246qx]
373. 4b1d0cf2b99aec85cdedceaef88c3a074de79832.pdf [liu2024mji]
374. 0845cea58467d372eb296fa1f184ecabe02be18b.pdf [chen2024efo]
375. 6a9caace1919b0e7bb247f0ecb585068c1ec4ff8.pdf [chen2024uld]
376. 30321b036607a7936221235ea8ec7cf7c1627100.pdf [wang2017zm5]
377. e03b8e02ddda86eafb54cafc5c44d231992be95a.pdf [li2021qr0]

## Literature Review

### Introduction

\section{Introduction}
\label{sec:introduction}



\subsection{Background: Knowledge Graphs}
\label{sec:1_1_background:_knowledge_graphs}


Knowledge Graphs (KGs) represent a foundational paradigm for structuring and organizing factual information, serving as a backbone for numerous intelligent systems and advanced analytical applications [choudhary2021, yan2022]. At their essence, KGs model world knowledge as a collection of interconnected entities and their relationships, typically expressed in the form of (head entity, relation, tail entity) triples. For example, a triple such as (\textit{Barack Obama, bornIn, Honolulu}) explicitly captures a factual relationship between two distinct entities, providing a machine-readable assertion of knowledge.

The conceptual lineage of knowledge graphs can be traced back to early semantic networks, which emerged in artificial intelligence research during the 1960s with the goal of representing human knowledge in a structured, machine-interpretable format. This evolutionary path progressed through various stages, including the development of expert systems and formal ontologies. A significant milestone in this journey was the advent of the Semantic Web vision, which introduced foundational technologies such as the Resource Description Framework (RDF) and Web Ontology Language (OWL) to enable data interoperability and machine-understandable content across the web. These efforts laid the groundwork for the large-scale, publicly available knowledge bases that characterize modern KGs. Prominent examples include Freebase, a collaborative knowledge base acquired by Google; DBpedia, which systematically extracts structured information from Wikipedia; and Wikidata, serving as a central, multilingual repository for the structured data of Wikimedia projects [rossi2020]. These contemporary KGs are indispensable resources, playing a crucial role in organizing vast amounts of world knowledge, enhancing web search capabilities, powering sophisticated question-answering systems [huang2019], and facilitating personalized recommender systems [sun2018].

Despite their immense utility in organizing and querying structured information, traditional symbolic representations within knowledge graphs inherently present several critical challenges that limit their full potential and scalability. Firstly, KGs frequently suffer from **data sparsity** and incompleteness. Real-world knowledge is vast and constantly evolving, making it practically impossible to explicitly enumerate every possible fact. This incompleteness means that many potential relationships are missing, hindering comprehensive reasoning and accurate inference, as the absence of an explicit triple does not necessarily imply the absence of a relationship.

Secondly, performing **statistical inference** directly on discrete, symbolic representations is inherently difficult and often inefficient. Traditional rule-based reasoning, while offering precision and interpretability, struggles to generalize effectively over noisy, uncertain, or incomplete data. It operates on an "all or nothing" principle, where a rule either fires or it doesn't, making it brittle and unable to capture probabilistic relationships, nuanced semantic similarities, or exceptions [tang2022]. This rigidity prevents the discovery of latent patterns or implicit connections that are not explicitly codified by rules or triples. For instance, determining that two entities are semantically similar based on their shared context or indirect relationships is challenging with purely symbolic methods.

Thirdly, the **computational overhead of rule-based reasoning** can be substantial, particularly for large-scale KGs containing billions of triples. Deriving new facts through logical inference rules often involves complex combinatorial searches across the graph, which is computationally expensive and challenging to scale. The explicit representation of every fact and rule, coupled with the need for exhaustive search, makes traditional reasoning impractical for dynamic and massive knowledge bases.

These collective challenges—data sparsity, the inherent difficulty of statistical inference on discrete symbols, and the computational burden of rule-based reasoning—highlight a critical need for more flexible, robust, and continuous representations of knowledge. This fundamental motivation spurred the development of Knowledge Graph Embedding (KGE) techniques. KGE aims to address these limitations by transforming discrete entities and relations into low-dimensional, continuous vector spaces, thereby enabling the capture of latent semantic information, facilitating efficient statistical inference, and allowing for more scalable computation [choudhary2021, yan2022]. This paradigm shift from discrete symbols to dense, continuous vectors forms the conceptual foundation for a new generation of knowledge graph processing, bridging symbolic knowledge representation with modern machine learning techniques.
\subsection{Role of KG Embedding}
\label{sec:1_2_role_of_kg_embedding}

Knowledge Graph Embedding (KGE) stands as a pivotal paradigm, forging a crucial bridge between the explicit, symbolic representations of knowledge graphs (KGs) and the implicit, continuous representations favored by modern neural approaches. Building upon the challenges of data sparsity, computational overhead, and difficulty in statistical inference inherent to purely symbolic KGs, KGE transforms discrete entities and relations into continuous, low-dimensional vector spaces [choudhary2021, yan2022]. This transformation is fundamental for enabling efficient computation, facilitating seamless integration with machine learning models, and, critically, for capturing latent semantic relationships and structural patterns that are often elusive in discrete formats.

At its core, KGE operates on the principle that entities and relations can be represented as vectors or matrices within an embedding space, where semantic similarity and relational patterns are encoded by their geometric positions and transformations. For instance, in the foundational translational model TransE, a relation is conceptualized as a translation vector from a head entity to a tail entity, formalized as $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ [bordes2013]. This elegant vector arithmetic allows for the inference of missing links based on proximity in the embedding space. Such a continuous representation inherently addresses data sparsity by enabling generalization through shared vector space properties; entities with similar contexts or relations tend to cluster together, facilitating the discovery of nuanced semantic relationships not explicitly defined in the symbolic graph [choudhary2021]. For example, if "Paris" and "France" have a similar vector relationship to "Berlin" and "Germany," the model implicitly learns the "capital city of" concept, even if not explicitly encoded as a rule. Models like Triple Context-Based KGE [gao2018di0] and PConvKB [jia20207dd] further enhance this by explicitly incorporating local and global contextual information, such as neighboring entities and relation paths, to enrich the learned embeddings and capture finer-grained semantics.

The continuous nature of KGE representations offers several significant advantages. Firstly, it enables highly efficient computation during inference. Instead of complex symbolic matching or rule application, KGE models perform vector operations (e.g., dot products, distance calculations) that are computationally inexpensive and highly parallelizable. This makes them scalable for querying large-scale KGs [yan2022]. However, it is crucial to acknowledge that while inference is efficient, the *training* of these models on massive KGs can be computationally intensive, a challenge addressed by techniques focusing on efficiency and scalability in later sections (e.g., [do20184o2] for handling many relations). Secondly, KGE acts as a crucial interface for integrating symbolic knowledge with modern machine learning and deep learning models. The learned embeddings serve as rich, pre-trained features for various neural architectures, allowing for end-to-end learning and leveraging the strengths of both symbolic and sub-symbolic AI paradigms [choudhary2021]. This integration is vital for tasks requiring nuanced understanding and prediction, where traditional rule-based systems often fall short. Furthermore, KGE models can incorporate background taxonomic information, such as subclasses and subproperties, to enhance learning and make more robust predictions, as demonstrated by approaches like [fatemi2018e6v].

KGE plays a dual, yet interconnected, role in practical applications: providing an encoding for data mining tasks and enabling robust link prediction [portisch20221rd]. For data mining, the embeddings serve as rich feature vectors for entities, which can then be directly utilized in tasks like clustering, classification, or recommendation systems. For link prediction, KGE models learn a scoring function that quantifies the plausibility of a given triple $(h, r, t)$, allowing for the completion of incomplete KGs by predicting missing entities or relations. This capability is fundamental for a wide array of tasks:
\begin{itemize}
    \item \textbf{Link Prediction}: Identifying missing facts within a KG, crucial for knowledge graph completion and enrichment.
    \item \textbf{Entity Alignment}: Discovering equivalent entities across different KGs, facilitating the integration and merging of heterogeneous knowledge sources.
    \item \textbf{Question Answering}: Enhancing natural language understanding by providing semantic context from KGs to answer complex queries.
    \item \textbf{Recommendation Systems}: Improving personalized recommendations by leveraging relational information between users, items, and their attributes.
\end{itemize}

However, this transformation from symbolic to continuous representations is not without trade-offs. The resulting dense vectors often lack the direct interpretability and logical guarantees inherent in symbolic systems, making it challenging to understand *why* a particular prediction was made. This limitation has spurred significant research into explainable KGE models [kurokawa2021f4f] and methods that integrate logical rules directly into the embedding process, such as RulE [tang2022], which learns explicit rule embeddings to provide soft, context-dependent inference.

The evolution of KGE has seen models progress significantly. Early translational models like TransE, while groundbreaking, faced limitations, such as conflicts between their core principle and regularization strategies, leading to warped embeddings [ebisu2017]. This spurred advancements like TorusE [ebisu2017], which embeds entities and relations on a Lie group (a torus) to inherently bound embeddings and resolve regularization conflicts, improving accuracy and efficiency. More recent models, such as HolmE [zheng2024], have introduced novel Riemannian KGE frameworks designed to be "closed under composition," allowing for robust modeling of complex, under-represented relational patterns and providing a unifying theoretical framework for many existing models. Other models, like TransF [do20184o2], address scalability challenges by explicitly modeling basis subspaces for projection matrices, making them robust to KGs with thousands of relations. This continuous innovation aims to enhance expressiveness, capture diverse relational patterns (e.g., symmetry, anti-symmetry, hierarchy, composition), and improve the robustness and generalizability of embeddings. Ultimately, KGE transforms complex, discrete information into a machine-readable, statistically analyzable format, offering scalability and improved performance across a wide spectrum of AI applications by effectively bridging the gap between symbolic knowledge and neural reasoning.
\subsection{Scope and Organization of the Review}
\label{sec:1_3_scope__and__organization_of_the_review}


This literature review offers a comprehensive and pedagogically structured exploration of Knowledge Graph Embedding (KGE) research, guiding the reader through its intellectual evolution from foundational models to advanced techniques, diverse applications, and emerging future directions. The review is meticulously organized to provide a clear understanding of the field's boundaries, objectives, and the overarching themes that have shaped its development, critically informed by an analysis of identified research communities and their evolutionary paths. Our approach emphasizes a progression from core theoretical underpinnings to practical implementations and future challenges, ensuring a coherent narrative for both newcomers and seasoned researchers.

The review begins in **Section 1: Introduction**, which establishes the foundational context for KGE. Following an overview of knowledge graphs and the motivation for embedding methods, this section delineates the scope and organization of the entire document, setting the stage for the subsequent detailed discussions.

**Section 2: Foundational KGE Paradigms** delves into the initial breakthroughs and core models that established the field. We trace the evolution from early geometric approaches, such as translational models like TransE, TransH [wang2014], and TransD [ji2015], which interpret relations as vector translations, to semantic matching models like RESCAL and ComplEx that leverage algebraic operations. The section culminates with rotational models, exemplified by RotatE [sun2019rotate], which generalize earlier geometric intuitions by modeling relations as rotations in complex vector spaces. This progression highlights the increasing sophistication in capturing diverse relational patterns, as comprehensively benchmarked and categorized by studies like [ferrari2022r82], which compare these foundational techniques across various datasets and evaluation criteria.

Building upon these foundations, **Section 3: Advanced Geometric and Non-Euclidean Embeddings** critically examines the evolution of embedding techniques beyond basic Euclidean spaces. This section explores models that leverage non-Euclidean geometries, such as hyperbolic spaces for hierarchical data [pan2021hyperbolic], Lie groups for efficient representation, and compound operations that combine multiple geometric transformations. It also covers spherical and quaternion embeddings, demonstrating how mathematically richer frameworks enhance model expressiveness for intricate graph structures and diverse semantics.

The discussion then transitions to **Section 4: Contextual and Graph Neural Network-based KGE**, focusing on models that enhance representations by incorporating richer contextual information and leveraging advanced neural network architectures. This includes integrating auxiliary information like textual descriptions, entity types [he2023take], and logical rules [tang2022rule]. We then explore the application of convolutional and attention-based architectures [dettmers2018conve], culminating in a detailed examination of Graph Neural Networks (GNNs) and Transformers [shi2025tgformer] for capturing multi-hop structural dependencies and enabling inductive learning.

Recognizing the dynamic nature of real-world knowledge, **Section 5: Temporal Knowledge Graph Embedding** is dedicated to methodologies that address the crucial challenge of modeling knowledge evolving over time. This section covers early approaches to temporal integration, such as HyTE [dasgupta2018], followed by rotation-based models like TeRo [xu2020tero] and ChronoR [sadeghian2021chronor], and advanced techniques leveraging multi-curvature spaces and GNNs for evolving KGs. This specialized focus underscores the importance of capturing temporal dependencies for predictive tasks.

**Section 6: KGE for Downstream Applications and Practical Considerations** highlights the practical utility of KGEs by showcasing their impact across various downstream tasks. We detail their application in crucial areas such as entity alignment [sun2018bootea, zhang2019multike], recommendation systems [sun2018rkge, liu2023cross], and question answering [huang2019keqa]. Furthermore, we broaden the scope to include semantic queries and data exploration [tran2019j42] and discuss how KGEs serve as encodings for general data mining tasks, not just link prediction [portisch20221rd]. This section also addresses critical practical aspects, including efficiency, scalability for large-scale KGs, and robustness in training.

Finally, **Section 7: Emerging Trends and Future Directions** looks ahead, identifying cutting-edge research avenues and unresolved challenges. This includes the move towards automated KGE model design and meta-learning [sun2024metahg], the integration of multimodal information, and the increasing demand for explainable and ethical AI in KGE [yang2025seconv]. The section synthesizes theoretical gaps and practical hurdles, providing a roadmap for future research and development.

The review concludes in **Section 8: Conclusion**, which synthesizes the key advancements and intellectual trajectory of KGE research, reiterating the significant progress made and outlining persistent open research questions and promising future directions. This structured journey through the diverse and rapidly evolving landscape of knowledge graph embedding research ensures a clear understanding of the review's boundaries, objectives, and the overarching themes that guide the discussion, emphasizing the continuous drive towards more expressive, efficient, and context-aware knowledge representations.


### Foundational KGE Paradigms

\section{Foundational KGE Paradigms}
\label{sec:foundational_kge_paradigms}



\subsection{Translational Models: From TransE to TransD}
\label{sec:2_1_translational_models:_from_transe_to_transd}


The inherent incompleteness of knowledge graphs necessitates robust methods for link prediction and knowledge graph completion. The translational paradigm emerged as a foundational approach, modeling relations as transformations in a continuous embedding space, offering intuitive and computationally efficient initial solutions. These models interpret relations as operations that translate a head entity embedding to a tail entity embedding, aiming to satisfy the geometric principle $h + r \approx t$.

The pioneering work in this area was TransE [bordes2013], which posited that a relation $r$ acts as a simple translation from a head entity $h$ to a tail entity $t$ in a shared low-dimensional vector space. This model was groundbreaking for its simplicity and efficiency, establishing a strong baseline for knowledge graph embedding. However, TransE struggled significantly with complex relation types, particularly 1-to-N, N-to-1, and N-to-N relations [asmara2023]. This limitation arose because TransE assigned a single, fixed vector representation to each entity, regardless of the specific relation it participated in. Consequently, for a 1-to-N relation (e.g., "capitalOf(Paris, France), capitalOf(Paris, Germany)"), the model would attempt to map "Paris" to both "France" and "Germany" via the same "capitalOf" vector. This forces "France" and "Germany" to occupy very similar locations in the embedding space, which is semantically inaccurate and severely limits discriminative power [asmara2023]. Furthermore, TransE's common regularization strategy, which often involves projecting embeddings onto a unit sphere, was later identified to conflict with its core translation principle. This conflict could warp embeddings, adversely affecting link prediction accuracy and preventing the model from fully realizing its potential [ebisu2017].

To address TransE's shortcomings, subsequent models introduced more sophisticated mechanisms to handle relational diversity and improve expressiveness. TransH [wang2014] advanced the paradigm by projecting entities onto relation-specific hyperplanes. Instead of a single entity vector, TransH allowed an entity to have different "distributed" representations (projections) depending on the specific relation it participated in. This innovation provided a more flexible way to model an entity's role in different contexts, thereby better accommodating 1-to-N, N-to-1, and N-to-N relations while maintaining computational efficiency comparable to TransE. By decoupling an entity's general representation from its relation-specific manifestation, TransH offered a significant step towards capturing the polysemy of entities in different relational contexts.

Building upon the concept of relation-specific representations, TransR and CTransR [lin2015] further extended this idea by proposing the use of separate entity and relation spaces. In TransR, entities are first projected from their entity-specific space into a relation-specific space using a relation-specific projection matrix $M_r$, before the translation operation is applied ($h M_r + r \approx t M_r$). This approach provided even greater expressiveness than TransH's hyperplane projections, as it allowed entities to be represented distinctly and transformed across various relation types, improving the modeling of complex semantic patterns. However, TransR introduced a significant increase in parameter count. Each relation required its own full projection matrix, leading to a parameter complexity of $O(k_e \cdot k_r)$ per relation (where $k_e$ and $k_r$ are entity and relation embedding dimensions). This posed scalability challenges for knowledge graphs with a large number of relations, as the computational cost and memory footprint grew substantially.

TransD [ji2015] was then introduced to refine the translational paradigm, aiming to achieve the expressiveness of TransR while mitigating its parameter explosion. TransD employed dynamic mapping matrices, a more efficient approach to projection. Unlike TransR's fixed, full projection matrices, TransD introduced a dual-vector representation for each entity ($h, h_p$) and relation ($r, r_p$). One vector captured the intrinsic meaning (e.g., $h$), while the second ($h_p$ or $r_p$) dynamically constructed a mapping matrix for projection. Specifically, the projection matrix for a head entity $h$ given relation $r$ is constructed as $M_{rh} = r_p h_p^T + I$, and similarly for the tail entity. This innovation significantly improved expressiveness by considering the diversity of *both* entities and relations in a more adaptive and efficient manner. Crucially, by constructing projection matrices from lower-dimensional vectors, TransD dramatically reduced the parameter count compared to TransR (e.g., $O(k_e + k_r)$ per relation for the projection vectors, plus the entity/relation embeddings), offering a superior trade-off between expressivity and scalability while achieving enhanced performance. This concept of dynamic translation, where parameter vectors are introduced to support flexible translation, was also explored in models like TransE-DT and TransR-DT [chang20179yf], demonstrating consistent improvements by making the translation principle more adaptive.

These models collectively established the translational paradigm, offering initial and progressively refined solutions for link prediction and knowledge graph completion. While highly influential and forming the bedrock of KGE research, these early translational models, even with their advancements, primarily relied on predefined geometric operations in Euclidean space. They often struggled to fully capture highly complex or implicit semantic patterns, such as those involving relational composition beyond simple vector addition, or the nuanced interplay of diverse entity types and textual contexts. Their limitations, particularly in modeling intricate logical patterns and scaling efficiently to extremely large and diverse knowledge graphs, paved the way for more expressive models, including those that generalize geometric transformations (e.g., rotations) or leverage neural architectures, which are discussed in subsequent sections.
\subsection{Semantic Matching Models: RESCAL and ComplEx}
\label{sec:2_2_semantic_matching_models:_rescal__and__complex}

Moving beyond the geometric translations of early knowledge graph embedding models, alternative paradigms emerged that focused on semantic matching and tensor factorization to capture richer, more direct interactions between entities and relations. This shift introduced different mathematical frameworks for scoring triple validity, enhancing expressiveness for specific relational patterns and demonstrating the power of algebraic operations in capturing nuanced semantic relationships.

A pioneering model in this direction was RESCAL (Relational Embedding of Simple Components for ALgebraic Link prediction) [Nickel_2016]. RESCAL introduced a tensor factorization approach, representing relations as full matrices that directly interact with entity vectors. For a given triple $(h, r, t)$, the model scores its validity using a bilinear form: $f(h, r, t) = \mathbf{h}^\top \mathbf{M}_r \mathbf{t}$, where $\mathbf{h}$ and $\mathbf{t}$ are vector embeddings for the head and tail entities, respectively, and $\mathbf{M}_r$ is a relation-specific matrix. This formulation allows for a richer, more direct semantic matching between entities through the relation, as the matrix $\mathbf{M}_r$ can capture complex, non-linear interactions. While RESCAL offered significant expressive power, particularly for capturing diverse relational patterns, it faced substantial scalability challenges due to the large number of parameters required for each relation matrix, leading to high computational costs and memory consumption.

Building upon the principles of tensor factorization, ComplEx (Complex Embeddings for Simple Link Prediction) [Trouillon_2016] further advanced semantic matching by introducing complex-valued embeddings. In ComplEx, both entities and relations are embedded into a complex vector space, where each embedding $\mathbf{e} \in \mathbb{C}^d$ has a real part $\text{Re}(\mathbf{e})$ and an imaginary part $\text{Im}(\mathbf{e})$. The scoring function for a triple $(h, r, t)$ is defined using a Hermitian dot product: $f(h, r, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \bar{\mathbf{t}} \rangle)$, where $\bar{\mathbf{t}}$ denotes the complex conjugate of the tail entity embedding. This elegant mathematical framework naturally models symmetric and antisymmetric relations without requiring explicit constraints, a significant advantage over many translational models. For instance, if a relation is symmetric, its complex embedding can be purely real, and if it is antisymmetric, its embedding can be purely imaginary. ComplEx thus provided a more elegant and computationally efficient solution for capturing these specific relational patterns compared to the more parameter-heavy RESCAL, offering a better trade-off between expressiveness and efficiency for certain types of relations.

Both RESCAL and ComplEx represent a crucial intellectual contribution by moving beyond the strict geometric translations and demonstrating the power of algebraic operations and higher-dimensional representations in capturing nuanced semantic relationships. RESCAL laid the groundwork for tensor factorization in KGE, showcasing the potential of relation-specific matrices to model intricate interactions. ComplEx then refined this direction by leveraging complex numbers, offering a more compact and principled way to handle relational properties like symmetry and antisymmetry that are prevalent in knowledge graphs. While RESCAL's scalability remained a practical hurdle, ComplEx offered a more viable path for certain relational patterns, albeit potentially lacking the general expressiveness of more advanced neural models for highly diverse or compositional relation types. The exploration of these algebraic frameworks highlighted the diverse mathematical tools available for knowledge graph representation, paving the way for future innovations in capturing complex semantic structures.
\subsection{Rotational Models: Capturing Complex Relation Patterns}
\label{sec:2_3_rotational_models:_capturing_complex_relation_patterns}


The accurate representation of diverse relational patterns within knowledge graphs (KGs) has been a persistent challenge for embedding models. While early translational models offered an efficient paradigm for learning entity and relation embeddings, they often struggled to simultaneously capture complex logical properties such as symmetry, anti-symmetry, inversion, and composition within a unified framework. Initial attempts to enhance the expressiveness of translational models, such as TransH [wang2014], addressed the limitations of TransE by projecting entities onto relation-specific hyperplanes. This allowed entities to have varying representations depending on the relation, thereby better handling one-to-many and many-to-one mapping properties. Further advancements, like TransD [ji2015], introduced dynamic mapping matrices for both entities and relations, aiming for a more fine-grained representation of diversity and improved efficiency by avoiding computationally intensive matrix-vector multiplications.

Despite these improvements in handling complex mapping properties, a single, elegant mechanism capable of unifying all fundamental relational patterns remained elusive. Semantic matching models, such as ComplEx [trouillon2016], which employed complex-valued embeddings and a Hermitian dot product, successfully modeled symmetric and anti-symmetric relations. However, ComplEx, like other semantic matching approaches, did not inherently capture compositional patterns with the same elegance or effectiveness. This gap highlighted the need for a more generalized geometric operation that could intrinsically model a broader spectrum of relational semantics.

A significant breakthrough in this area was the introduction of RotatE [sun2018], which represents a profound generalization of the translational embedding paradigm. RotatE elegantly interprets relations as element-wise rotations in a complex vector space, where the head entity embedding is rotated to align with the tail entity embedding. Specifically, for a valid triple $(h, r, t)$, RotatE models the relationship as $t = h \odot r$, where $h, r, t$ are complex-valued embeddings and $\odot$ denotes the Hadamard (element-wise) product, with the modulus of each element of $r$ constrained to $|r_i|=1$. This geometric operation allows RotatE to inherently and simultaneously capture diverse relational patterns.

The power of RotatE lies in its ability to model complex logical properties within a unified framework. For instance, symmetry is captured when a relation $r$ corresponds to a rotation by an angle of $\pi$ (or $0$), meaning $r = r^{-1}$. Anti-symmetry is naturally handled when $r \neq r^{-1}$. Inversion is directly supported, as the inverse relation $r^{-1}$ is simply the conjugate of $r$ in complex space. Most notably, RotatE can infer composition patterns, where if $(h, r_1, m)$ and $(m, r_2, t)$ are true, then $(h, r_1 \odot r_2, t)$ is also likely true, with $r_1 \odot r_2$ representing the composite relation. This capability to infer composition was a critical advantage over models like ComplEx [trouillon2016], which struggled with such patterns.

RotatE's novel approach, coupled with a self-adversarial negative sampling strategy, demonstrated superior performance across various benchmark datasets, including those specifically designed to test compositionality, such as the "Countries" dataset [sun2018]. Its ability to model complex logical properties more effectively than earlier translational or semantic matching approaches marked a significant contribution to improving KGE expressiveness. By offering a mathematically elegant and unified framework for diverse relational patterns, RotatE set a new standard for capturing intricate relational semantics, paving the way for more robust and logically consistent knowledge graph reasoning. The success of rotational models underscores the potential of leveraging richer geometric transformations in higher-dimensional spaces to unlock deeper semantic understanding within knowledge graphs.


### Advanced Geometric and Non-Euclidean Embeddings

\section{Advanced Geometric and Non-Euclidean Embeddings}
\label{sec:advanced_geometric__and__non-euclidean_embeddings}



\subsection{Hyperbolic Embeddings for Hierarchical Structures}
\label{sec:3_1_hyperbolic_embeddings_for_hierarchical_structures}


Knowledge graphs frequently exhibit intricate hierarchical or tree-like structures, which traditional Euclidean embedding spaces struggle to represent efficiently due to their inherent uniform curvature and limited capacity for exponential growth. Hyperbolic geometry, with its natural negative curvature and exponentially increasing volume, offers a compelling alternative for embedding such hierarchical data, allowing for more faithful and compact representations.

Early explorations into hyperbolic Knowledge Graph Embedding (KGE) models demonstrated their potential for capturing these complex hierarchies. \textcite{pan2021} introduced a novel KGE model that leverages an extended Poincaré Ball and a polar coordinate system within hyperbolic space. This approach specifically addressed the challenge of simultaneously modeling hierarchical structures and logical patterns by employing tangent space and exponential transformations for mapping vectors into the extended Poincaré Ball, along with a unique method for handling boundary conditions by expanding the modulus length. Their model achieved state-of-the-art results on certain link prediction tasks, showcasing the expressiveness of hyperbolic spaces for hierarchical knowledge. However, many initial hyperbolic KGE models, including those preceding \textcite{pan2021}, often relied on frequent and computationally intensive mappings between hyperbolic and tangent spaces during training. These "hybrid" approaches, while conceptually sound, introduced overhead and potential numerical instability due to the complex logarithmic and exponential functions involved.

Addressing these limitations, \textcite{liang2024} proposed the Fully Hyperbolic Rotation model (FHRE), which innovates by performing operations directly and entirely within hyperbolic space, specifically utilizing the Lorentz model. Unlike its predecessors, FHRE eliminates the need for iterative logarithmic and exponential mappings during training, instead performing a single mapping at initialization. In FHRE, relations are conceptualized as Lorentz rotations that transform head entity embeddings to tail entity embeddings, with triplet plausibility measured by a Lorentzian distance-based scoring function. This "fully hyperbolic" paradigm enhances both expressiveness and efficiency by fully leveraging the intrinsic properties of hyperbolic geometry, leading to improved stability and reduced computational burden. Experimental validation demonstrated that FHRE achieves state-of-the-art performance on challenging benchmarks, particularly those with diverse and complex relational patterns, by capturing fine-grained relational semantics more effectively than previous Euclidean, complex, and hybrid hyperbolic models.

The theoretical underpinnings of such geometric approaches are further explored in broader contexts. For instance, \textcite{zheng2024} introduced HolmE, a general Riemannian KGE framework that discusses the property of being "closed under composition" and leverages Riemannian geometry, including hyperbolic space, for efficient computation through extensions of Möbius addition. While HolmE provides a unifying perspective for models like TransE and RotatE, its direct application to the specific challenges of hierarchical representation in hyperbolic space is more about providing a theoretical umbrella rather than direct methodological innovation in hyperbolic geometry itself. Nonetheless, it underscores the growing recognition of non-Euclidean geometries in KGE.

In conclusion, hyperbolic embeddings have emerged as a powerful paradigm for representing hierarchical structures in knowledge graphs, offering a natural fit that Euclidean spaces lack. Models like \textcite{pan2021} and \textcite{liang2024} have progressively refined this approach, moving from hybrid mapping-based methods to fully hyperbolic operations, thereby enhancing expressiveness, efficiency, and stability. Despite these advancements, challenges remain in scaling these models to extremely large and dynamic knowledge graphs, integrating more diverse and complex relation types beyond simple hierarchies, and developing more interpretable hyperbolic operations. Future research could explore adaptive hyperbolic curvature, dynamic hyperbolic embeddings for evolving KGs, and novel ways to combine hyperbolic geometry with advanced neural architectures for even richer and more robust knowledge representation.
\subsection{Lie Group and Compound Operations for Enhanced Expressiveness}
\label{sec:3_2_lie_group__and__compound_operations_for_enhanced_expressiveness}


The quest for more expressive knowledge graph embedding (KGE) models has propelled research beyond simple Euclidean translations, leading to the exploration of advanced geometric and algebraic structures. This subsection examines models that leverage Lie groups and sophisticated compound geometric operations, offering a richer framework for representing entities and relations by capturing intricate relational semantics and diverse mapping properties.

An early conceptual step towards more flexible geometric representations was \textcite{xiao2015}'s \textbf{ManifoldE}, which moved beyond the "one-point" mapping principle of models like TransE. ManifoldE proposed embedding triples onto relation-specific manifolds (e.g., spheres or hyperplanes) rather than single points. This manifold-based approach provided greater flexibility for complex relations (e.g., one-to-many, many-to-many) and addressed the ill-posed algebraic system inherent in point-wise models, paving the way for more sophisticated geometric interpretations.

Building on this idea of structured geometric spaces, \textcite{ebisu2017} introduced \textbf{TorusE}, a pioneering model that embeds knowledge graphs directly on a Lie group, specifically a torus. This innovative approach ingeniously resolves a fundamental regularization conflict prevalent in earlier translational models. In Euclidean space, forcing embeddings onto a unit sphere (a common regularization technique) can distort learned representations, especially for relations modeled as translations. By embedding on a compact Abelian Lie group like the torus, TorusE inherently bounds the embeddings, eliminating the need for explicit regularization and leading to more accurate and efficient learning. This geometric shift offered a novel perspective, demonstrating the benefits of leveraging the inherent properties of Lie groups for KGE.

Further enhancing geometric transformations, subsequent research focused on combining multiple operations to achieve greater expressiveness. \textcite{ge2022} introduced \textbf{CompoundE}, a model that significantly generalizes geometric transformations by combining translation, rotation, and scaling operations in a cascaded, relation-specific manner. Unlike previous models often restricted to a single type of operation, CompoundE frames these compound operations within the affine group. This allows relations to not only shift and turn entity embeddings but also expand or contract them, providing a more versatile and powerful approach for capturing diverse relation patterns and mapping properties, such as symmetry, anti-symmetry, inversion, and various N-to-N relationships. As highlighted by the survey \textcite{ge2023}, CompoundE mathematically demonstrates its ability to encompass and generalize several existing distance-based KGE models, including TransE and RotatE, thereby offering a unifying perspective on affine operation-based techniques.

Extending the capabilities of high-dimensional geometric transformations, \textcite{li2022} proposed \textbf{HousE}, which utilizes Householder parameterization for high-dimensional rotations and invertible projections. While earlier rotation-based models like RotatE [sun2019] were effective for capturing symmetric and anti-symmetric patterns, their distance-preserving nature limited their ability to model complex relation mapping properties (RMPs) such as one-to-many or many-to-many. HousE addresses this by representing relations as compositions of Householder reflections for arbitrary high-dimensional rotations (generalizing RotatE to k-dimensions) and introduces novel invertible Householder projections that can flexibly adjust relative distances. This unified framework allows HousE to simultaneously model all crucial relation patterns and RMPs, offering superior expressiveness and generalizing previous rotation-based models.

Building upon the Householder parameterization, \textcite{li2024} introduced \textbf{GoldE} (Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization). GoldE presents a framework that generalizes existing KGE approaches in *both dimension and geometry* for orthogonal relation transformations. It achieves this through a universal orthogonal parameterization based on a generalized Householder reflection, capable of operating in arbitrary dimensions and across Euclidean, elliptic, and hyperbolic geometries. For this subsection's focus, GoldE's elliptic orthogonal parameterization is particularly relevant, as it generalizes Euclidean models by incorporating relation-specific scaling operations, effectively extending the "compound operations" paradigm by allowing adaptive adjustments to entity embeddings within a generalized orthogonal transformation framework. This represents a significant step towards breaking the limitations of rigid, homogeneous geometric spaces.

The progression towards these more complex, structured operations highlights a broader search for unifying mathematical principles in KGE. This goal was later formalized by \textcite{zheng2024} with \textbf{HolmE}, which provides a unifying Riemannian framework for KGE models. HolmE introduces the crucial property of being "closed under composition," ensuring that the composition of any two relation embeddings remains within the embedding space. The authors theoretically prove that prominent KGE models like TransE and RotatE are special cases of HolmE, providing a deeper theoretical understanding of their underlying geometric properties and demonstrating how these advanced geometric operations contribute to a more robust and theoretically sound representation of relational composition.

In summary, models leveraging Lie groups and compound operations, from the regularization-resolving TorusE to the generalized affine transformations of CompoundE, the high-dimensional Householder parameterizations of HousE and GoldE, and the unifying Riemannian framework of HolmE, represent a significant advancement in KGE. They move beyond the limitations of simpler geometric operations by leveraging the inherent properties of advanced algebraic and geometric structures. This allows them to capture intricate relational semantics, including diverse relation patterns and complex mapping properties, with enhanced expressiveness. While these approaches offer superior modeling capacity, a continuing challenge lies in balancing their increased mathematical complexity and computational demands with interpretability and scalability for real-world, large-scale knowledge graphs. Future research may focus on optimizing these complex operations and integrating them with neural architectures for even more robust and adaptable KGE solutions.
\subsection{Spherical and Quaternion Embeddings for Diverse Semantics}
\label{sec:3_3_spherical__and__quaternion_embeddings_for_diverse_semantics}


Traditional Knowledge Graph Embedding (KGE) models often represent entities as single points in Euclidean space, a simplification that can limit their capacity to capture the rich, multi-faceted semantic properties inherent in real-world knowledge graphs. This limitation becomes particularly apparent when modeling complex relations, entity extents, or multi-dimensional semantic relationships. To overcome these challenges, recent research has explored alternative, more expressive embedding spaces, notably spherical and quaternion representations, which leverage distinct mathematical properties to enhance KGE expressiveness and applicability to specialized tasks [cao2022].

The evolution of KGE models has seen a progression from simple vector operations to more sophisticated geometric and algebraic structures. Building upon the success of rotational models like RotatE [sun2019], which interprets relations as 2D rotations in complex vector space, quaternion embeddings offer a natural extension to higher dimensions. Quaternions, a 4D number system, provide a powerful algebraic framework for representing rotations in 3D and 4D space. The seminal work in this area is \textbf{QuatE} [zhang2019quat], which extends the concept of complex-valued embeddings by representing entities and relations as quaternions. QuatE models relations as rotations using the Hamilton product, allowing it to capture a wider array of relational patterns, including symmetry, anti-symmetry, inversion, and composition, within a unified 4D space. This higher-dimensional algebraic structure inherently provides greater expressiveness for intricate relational semantics compared to its 2D complex-valued counterparts.

Further advancements in quaternion embeddings demonstrate their versatility for diverse semantic challenges. \textbf{ConQuatE} [chen2025] leverages quaternion rotations to address the critical "polysemy issue," where entities exhibit different semantic characteristics depending on their relational contexts. By efficiently incorporating contextual cues from various connected relations through quaternion transformations, ConQuatE enriches entity representations across multiple semantic dimensions without requiring auxiliary information, thereby improving link prediction performance. Similarly, for dynamic knowledge, \textbf{TimeLine-Traced KGE (TLT-KGE)} [zhang2022muu] utilizes quaternion vectors to embed entities and relations with timestamps. TLT-KGE innovatively models semantic and temporal information as distinct axes within the quaternion space, enabling it to distinguish and connect these independent yet related aspects of temporal facts. This approach effectively addresses the challenge of representing evolving knowledge, outperforming state-of-the-art competitors in temporal knowledge graph completion. The broader utility of quaternion algebra in KGE is also highlighted by analyses that propose new multi-embedding models based on this framework [tran20195x3].

In a conceptually distinct but equally impactful direction, spherical embeddings move beyond point-based representations to model entities as geometric volumes. \textbf{SpherE} [li2024] proposes embedding entities not as single points, but as \textit{spheres} in Euclidean space, each defined by a center vector and a radius, while relations are modeled as rotations. This novel representation allows SpherE to explicitly capture entity \textit{extents} and \textit{overlaps}, which is crucial for tasks like knowledge graph set retrieval where precise sets of answers are required. By representing entities with a "spread" rather than a singular location, SpherE robustly models one-to-many, many-to-one, and especially many-to-many relations, a long-standing challenge for traditional point-based models. The interpretability of the sphere's radius, which correlates with an entity's universality, further enhances its utility. Theoretically, SpherE demonstrates high expressiveness for various relation patterns and mapping properties by checking sphere intersection after rotation, providing a powerful mechanism to understand how entities relate in terms of their scope and potential intersections.

These advancements in spherical and quaternion embedding spaces represent a significant leap in Knowledge Graph Embedding expressiveness, moving beyond the inherent limitations of traditional vector representations. Quaternion embeddings, by extending rotational models to 4D algebraic structures, offer enhanced capabilities for capturing complex relational patterns and contextual semantics, particularly useful for polysemy and temporal dynamics. Spherical embeddings, on the other hand, introduce volumetric representations that explicitly model entity extents and overlaps, proving highly effective for set-based retrieval and intricate many-to-many relationships. As highlighted by [cao2022], the choice of representation space profoundly influences the types of KG properties that can be effectively modeled. By leveraging the unique algebraic properties of quaternions and the geometric advantages of spheres, these models provide novel and powerful ways to capture complex entity and relation properties, pushing the boundaries of KGE expressiveness and expanding their applicability to specialized tasks in diverse real-world applications. Future research may explore the integration of these advanced geometric and algebraic spaces with neural architectures, or develop adaptive strategies for selecting optimal embedding spaces based on specific KG characteristics and task requirements.


### Contextual and Graph Neural Network-based KGE

\section{Contextual and Graph Neural Network-based KGE}
\label{sec:contextual__and__graph_neural_network-based_kge}



\subsection{Integrating Auxiliary Information: Text, Types, and Rules}
\label{sec:4_1_integrating_auxiliary_information:_text,_types,__and__rules}

Knowledge graph embedding (KGE) models traditionally focus on learning representations solely from observed (head, relation, tail) triples. However, real-world knowledge graphs are often incomplete and noisy, necessitating the integration of auxiliary information to enrich embeddings, improve semantic consistency, and enhance predictive performance. This section reviews KGE models that leverage diverse external data sources, including textual descriptions, semantic categories, explicit or implicit entity types, and logical rules.

Early efforts to incorporate textual information aimed to bridge the gap between symbolic triples and natural language semantics. [xiao2016] proposed Semantic Space Projection (SSP), a model that jointly learns from triples and textual descriptions by projecting the triple's loss vector onto a semantic hyperplane. This approach models strong correlations between texts and triples, guiding the embedding topology. Building on this, [shen2022] introduced LASS (Joint Language Semantic and Structure Embedding), which fine-tunes pre-trained language models with a probabilistic structured loss to simultaneously capture semantics from textual descriptions and reconstruct KG structures. LASS addresses the limitations of earlier text-aware models by providing a unified framework for deeper integration of language semantics, demonstrating superior performance, especially in low-resource settings.

Beyond free-form text, structured semantic information like entity categories and types offers valuable context. [guo2015] presented Semantically Smooth Embedding (SSE), which enforces a "semantically smooth" embedding space by constraining entities belonging to the same semantic category to lie close to each other, using manifold learning algorithms like Laplacian Eigenmaps as regularization. Taking this a step further, [lv2018] proposed TransC (Translating Concepts), which fundamentally differentiates concepts (represented as spheres) from instances (represented as vectors) in the embedding space. This geometric modeling inherently preserves the transitivity of `instanceOf` and `subClassOf` relations, a crucial property often overlooked by models treating all entities uniformly. More recently, [wang2021] introduced TransET, a model that leverages explicit entity types by employing circle convolution based on entity and type embeddings to generate type-specific representations, thereby learning more semantic features. Addressing the common challenge of incomplete or unavailable explicit type information, [he2023] developed TaKE (Type-augmented Knowledge graph Embedding), a model-agnostic framework that automatically captures *implicit* type features. TaKE further models the diversity of entity types using a relation-specific hyperplane mechanism and introduces a novel type-constrained negative sampling strategy, making it highly flexible and effective without requiring explicit type supervision.

Another powerful form of auxiliary information comes from logical rules, which encode explicit knowledge and constraints. [guo2017] proposed RUGE (RUle-Guided Embedding), an iterative paradigm that guides embedding learning with automatically extracted soft rules. RUGE addresses the limitation of one-time rule injection by alternating between soft label prediction for unlabeled triples and embedding rectification, maximizing the utility of uncertain logical knowledge. Expanding on the integration of soft rules, [guo2020] introduced a scalable method for Soft Logical Rule Embedding (SLRE), which represents relations as bilinear forms and entities in a non-negative bounded space. Their key innovation is a novel rule-based regularization that directly enforces relation representations to satisfy soft rule constraints, making its complexity independent of the entity set size and significantly improving scalability. More comprehensively, [tang2022] presented RulE (Rule Embedding), a neural-symbolic framework that learns explicit *rule embeddings* and jointly represents entities, relations, and logical rules in a unified continuous space. RulE calculates confidence scores for rules and employs a soft rule reasoning mechanism, effectively mitigating the brittleness of traditional logical inference and allowing for mutual regularization between KGE and rule-based components.

In conclusion, the integration of auxiliary information, whether from textual descriptions, semantic categories, entity types, or logical rules, has profoundly enhanced KGE models. This progression reflects a shift from purely structural embeddings to richer, context-aware representations that capture more nuanced semantic relationships and logical consistencies. While significant strides have been made in leveraging these diverse data sources, future research could explore more sophisticated fusion mechanisms for multi-modal auxiliary information, investigate methods for automatically discovering and validating rules from noisy data, and develop frameworks that offer greater interpretability of how auxiliary knowledge influences learned representations.
\subsection{Convolutional and Attention-based Architectures}
\label{sec:4_2_convolutional__and__attention-based_architectures}


The evolution of Knowledge Graph Embedding (KGE) has seen a significant shift from models relying on fixed scoring functions to sophisticated neural architectures that enable data-driven feature learning, particularly through Convolutional Neural Networks (CNNs) and attention mechanisms. These architectures excel at capturing complex interactions and relational patterns, leading to more expressive and contextually aware embeddings by moving beyond simplistic, fixed scoring functions. CNNs primarily focus on extracting local, non-linear features from structured input, while attention mechanisms dynamically weigh and aggregate information, proving particularly effective for the irregular structures of knowledge graphs.

Initial convolutional approaches, such as ConvE [dettmers2018], marked a departure from simpler models by applying convolutional filters to concatenated entity and relation embeddings. This allowed for the learning of rich, non-linear feature maps, significantly improving the expressiveness for triplet plausibility prediction. ConvE's strength lies in its parameter efficiency and ability to capture intricate interaction patterns, but its fixed filters can struggle with the diverse and often complex relational patterns (e.g., 1-to-N, N-to-1, N-to-N) inherent in KGs, as it treats all relations uniformly. Building on this foundation, the Multi-Scale Dynamic Convolutional Network (M-DCN) [zhang2020] addressed these limitations by introducing dynamic, multi-scale convolutional filters whose weights are specifically tailored to each relation. This innovation enabled more nuanced feature extraction and better handling of complex relation patterns compared to ConvE's static filters. Further advancing the integration of deep learning, ReInceptionE [xie2020] employed an Inception network to deepen the interaction learning between head and relation embeddings, enhancing the model's capacity to capture complex features. Crucially, ReInceptionE also incorporated a relation-aware attention mechanism to integrate both local neighborhood and global entity structural information, demonstrating an early synergy between CNNs and attention for comprehensive structural awareness. Another hybrid approach, PConvKB [jia20207dd], improved upon ConvKB (a CNN-based model) by explicitly incorporating relation paths. It utilized an attention mechanism to measure the local importance of these paths and a global measure (DIPF) for their overall significance, thereby enriching the convolutional features with path-level context. In specialized domains, SEConv [yang2025] for healthcare prediction demonstrated the synergy of a resource-efficient self-attention mechanism with a multi-layer CNN to learn deeper and more expressive structural features, highlighting the adaptability of these architectures to specific application needs and resource constraints. While CNNs are effective for local feature extraction and can be parameter-efficient, their inherent grid-like operations can be less intuitive for the irregular, graph-structured data of KGs, and they may struggle with modeling long-range dependencies without significant architectural depth.

To overcome the limitations of fixed feature extraction and uniform information aggregation, attention mechanisms emerged to dynamically weigh and aggregate information, particularly from an entity's neighborhood, to create more contextually rich embeddings. Pioneering inductive KGE, the Logic Attention Network (LAN) [wang2018] employed a double-view attention mechanism that combined logical rule-based weighting with neural network attention. This innovative approach allowed for permutation-invariant, redundancy-aware, and query-relation-aware aggregation of neighborhood information, which is crucial for generalizing to unseen entities by focusing on relevant facts. Similarly, Graph Attenuated Attention networks (GAATs) [wang2020] introduced an attenuated attention mechanism to assign varying importance to different relation paths and actively acquire information from neighboring nodes. This mechanism directly addressed the uniform weighting limitations of earlier graph-based models, allowing for a more fine-grained understanding of relational semantics by emphasizing more informative paths and neighbors. These attention-based models offer superior flexibility in capturing the varying importance of different parts of a knowledge graph, leading to more adaptive and context-sensitive embeddings. However, their computational cost can be higher than simpler models, and the interpretability of complex attention patterns remains a challenge.

In summary, convolutional and attention-based architectures have significantly advanced KGE by moving beyond fixed scoring functions to data-driven feature learning. CNNs like ConvE, M-DCN, ReInceptionE, PConvKB, and SEConv have enabled the extraction of rich, non-linear, and dynamic features from entity-relation interactions. Concurrently, attention mechanisms in models like LAN and GAATs have empowered KGE models to dynamically aggregate neighborhood information and capture complex relational patterns with varying importance. While these architectures offer superior expressiveness and contextual awareness, challenges remain in balancing their increased computational complexity with scalability for extremely large KGs, and in enhancing the interpretability of their learned features. The principles of dynamic information aggregation via attention, particularly on graph structures, laid crucial groundwork for the more formalized message-passing frameworks of Graph Neural Networks and Transformers, which are explored in the subsequent section for their ability to capture multi-hop structural dependencies more comprehensively.
\subsection{Graph Neural Networks and Transformers for Structural Learning}
\label{sec:4_3_graph_neural_networks__and__transformers_for_structural_learning}


The inherent limitations of traditional Knowledge Graph Embedding (KGE) models, particularly their transductive nature and struggle to capture rich, multi-hop structural information, have driven the integration of Graph Neural Networks (GNNs) and Transformer architectures. These advanced neural models explicitly leverage graph topology and complex relational paths, providing a powerful framework for learning context-rich embeddings, enabling inductive learning for unseen entities and relations, and significantly enhancing generalization capabilities.

The paradigm shift towards GNNs in KGE began with models that adapted message-passing mechanisms to the relational nature of knowledge graphs. A seminal work, R-GCN [schlichtkrull2018modeling], introduced relation-specific transformations within a standard GCN framework. Each relation type was assigned a distinct weight matrix, allowing entities to aggregate information from their neighbors based on the type of incoming and outgoing edges. This approach effectively captured local graph structure and multi-hop dependencies through iterative message passing, where an entity's embedding is updated by aggregating transformed embeddings of its neighbors. However, R-GCN faced challenges with parameter explosion for KGs with many relation types and struggled to model complex relational patterns beyond simple aggregation.

To address the parameter efficiency and enhance expressiveness, CompGCN [vashishth2020compositional] proposed a compositional approach. It unified entity and relation embeddings into a single framework, using composition operations (e.g., subtraction, multiplication) to define how relation embeddings transform entity messages during aggregation. This allowed for a more compact representation of relations and improved the modeling of inverse and symmetric relation patterns. Building on the GNN message-passing paradigm, GAATs [wang2020] further refined neighbor aggregation by introducing an attenuated attention mechanism. Unlike earlier GCNs that often assigned uniform weights, GAATs dynamically weighted different relation paths and actively acquired information from neighboring nodes, leading to more nuanced and comprehensive entity and relation representations by capturing the varying importance of different structural contexts.

A critical challenge for KGE is inductive learning, where models must generalize to entities or even entire knowledge graphs unseen during training. SE-GNN [li2021] advanced this by explicitly modeling "Semantic Evidence" at relation, entity, and triple levels through distinct neighbor aggregation patterns within a GNN. This provided crucial insights into *why* KGE models generalize, by demonstrating how different types of semantic evidence contribute to the inductive capacity. Taking generalization a step further, MorsE [chen2021] introduced a meta-learning framework. MorsE learns "meta-knowledge" (transferable structural patterns) using a GNN modulator, enabling it to produce general entity embeddings for entirely unseen entities in new KGs. This represents a higher level of generalization compared to models that primarily focus on inductive relation prediction, as MorsE can adapt to novel graph structures.

The increasing complexity and heterogeneity of KGs have also spurred the development of more adaptive GNN architectures. MGTCA [shang2024] introduced a novel approach by generating richer neighbor messages through the integration of spatial information from hyperbolic, hypersphere, and Euclidean spaces. It further incorporated a trainable convolutional attention network that autonomously switches between different GNN types (GCN, GAT, and a novel KGCAT) to adaptively capture diverse local structural patterns, overcoming the data dependence of single-GNN models. This highlights a trend towards multi-geometric and adaptive GNN designs. Complementing this, research into automated GNN design, such as the message function search proposed by [di2023], allows for the discovery of data-dependent message functions. This approach builds a flexible search space for GNN message functions, enabling the system to automatically find optimal structures and operators that adapt to diverse KG forms (e.g., traditional KGs, n-ary relational data, hyper-relational KGs), thereby enhancing adaptability and performance.

Deploying GNNs on large-scale KGs presents significant computational and memory challenges. CPa-WAC [modak2024] addressed this by developing a Constellation Partitioning-based Scalable Weighted Aggregation Composition framework. It employs a novel constellation partitioning algorithm to divide KGs into topological clusters and uses a global decoder to merge embeddings from independently trained partitions, significantly reducing training time and memory costs while maintaining prediction accuracy. Other efforts in scaling GNNs for link prediction include algorithmic strategies like self-sufficient partitions, constraint-based negative sampling, and edge mini-batch training, which have demonstrated substantial speedups while preserving performance [sheikh202245c]. Furthermore, empirical studies on parallel training techniques for KGE models, such as those by [kochsiek2021], have shown that while many existing methods negatively impact embedding quality, careful choices like variations of stratification and suitable random partitioning can enable efficient and effective large-scale training. For dynamic KGs, MetaHG [sun2024] proposed a meta-learning strategy that efficiently updates incremental knowledge by integrating both local and potential global structural information through a hybrid GNN and Hypergraph Neural Network (HGNN) framework.

More recently, the power of Transformer architectures, renowned for their ability to capture long-range dependencies, has been harnessed for structural learning in KGE. TGformer [shi2025] introduced a novel Graph Transformer Framework for KGE. It constructs context-level subgraphs for predicted triplets and employs a Knowledge Graph Transformer Network (KGTN) to comprehensively explore multi-structural features (both triplet-level and graph-level) and contextual information. By leveraging self-attention mechanisms, TGformer captures global and local structural features by allowing each node to attend to all other nodes within its contextual subgraph, enabling state-of-the-art link prediction through explicit modeling of complex relational paths. In a domain-specific application, SEConv [yang2025] proposed a model for healthcare prediction that combines a resource-efficient self-attention mechanism with a multilayer Convolutional Neural Network (CNN) to learn deeper and more expressive structural features from medical KGs, illustrating hybrid approaches that merge the strengths of different architectures.

In conclusion, the integration of GNNs and Transformers marks a significant paradigm shift in KGE, moving from local, transductive approaches to models capable of inductive learning, multi-hop reasoning, and capturing rich contextual and structural information. While these advanced architectures offer superior expressiveness and generalization, challenges remain in optimizing their computational efficiency for extremely large and sparse KGs, ensuring interpretability of complex attention mechanisms, and effectively fusing diverse structural signals from multi-modal or multi-geometric spaces. Future research will likely focus on developing more scalable Graph Transformers, exploring novel attention mechanisms tailored for heterogeneous KGs, and designing hybrid models that combine the strengths of various architectures for robust and efficient structural learning, potentially through automated design processes.


### Temporal Knowledge Graph Embedding

\section{Temporal Knowledge Graph Embedding}
\label{sec:temporal_knowledge_graph_embedding}



\subsection{Early Approaches to Temporal Integration}
\label{sec:5_1_early_approaches_to_temporal_integration}


The dynamic nature of real-world knowledge necessitates that Knowledge Graph Embedding (KGE) models move beyond static representations to explicitly incorporate temporal information. Early research in temporal integration laid the foundational groundwork, exploring diverse methodologies to embed the temporal dimension directly into knowledge graphs, thereby enabling time-aware reasoning and prediction.

One of the pioneering geometric approaches was HyTE [dasgupta2018], which introduced a novel method for embedding temporal knowledge by associating each timestamp with a distinct hyperplane in the embedding space. This allowed HyTE to perform temporally-guided inference and predict the temporal scopes for relational facts, a crucial capability for incomplete knowledge graphs where temporal validity might be missing. In a different vein, tensor decomposition methods emerged as a powerful paradigm for inherently capturing time within KGEs [lin2020]. This approach represented facts as higher-order tensors, explicitly including the time dimension alongside entities and relations, offering a generalizable framework for modeling dynamic knowledge. While these models provided initial explicit temporal handling, they often treated temporal evolution deterministically.

A significant departure from deterministic temporal modeling was introduced by ATiSE (Additive Time Series Embedding) [xu2019]. ATiSE innovated by modeling the evolution of each entity and relation representation as a multi-dimensional additive time series, composed of trend, seasonal, and random components. Crucially, it represented entities and relations as multi-dimensional Gaussian distributions at each time step, explicitly accounting for temporal uncertainty during their evolution, a feature largely overlooked by prior models. This statistical perspective provided a more nuanced understanding of how knowledge changes over time.

More recently, TeAST (Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline) [li2023] presented a novel structural mapping for time. TeAST introduced the concept of an Archimedean spiral timeline for relations, which ensures that relations occurring simultaneously are placed on the same timeline and that all relations evolve in a structured manner. This approach transformed the quadruple completion problem into a 3rd-order tensor completion task, specifically designed to avoid direct entity evolution and enhance interpretability, building upon the idea of structured temporal representation with a creative geometric twist.

These early models collectively established diverse foundational methodologies for integrating temporal information into KGEs, ranging from geometric interpretations and tensor-based structural embeddings to time series analysis for uncertainty and novel timeline mappings. While they successfully moved beyond static representations, a shared limitation across some of these initial approaches, particularly the earlier ones, was their potential struggle with highly complex, non-linear temporal patterns or the scalability challenges associated with explicit temporal indexing or extensive tensor operations. These challenges highlighted the need for more sophisticated and efficient temporal modeling techniques, paving the way for subsequent advancements in the field.
\subsection{Rotation-based Models for Temporal Dynamics}
\label{sec:5_2_rotation-based_models_for_temporal_dynamics}


Building on the success of rotation-based embedding models like RotatE in capturing diverse relation patterns within static knowledge graphs, a significant line of research has emerged that leverages rotations in complex or k-dimensional spaces to elegantly model temporal evolution. These models offer a highly expressive framework for representing time-dependent changes and their intricate interactions.

A foundational contribution in this area is \textcite{xu2020} TeRo (Temporal Rotation), which introduced the concept of temporal rotation in a complex vector space to model entity evolution. TeRo represents time-specific entity embeddings as element-wise rotations of their time-independent counterparts, allowing it to effectively capture diverse relation patterns, including temporary, asymmetric, and reflexive relations, and robustly handle various time annotations such as discrete time points and continuous time intervals through dual relation embeddings.

Building upon TeRo's pioneering work, \textcite{sadeghian2021} ChronoR (Chronological Rotation) generalized the rotation mechanism to k-dimensional spaces, offering a more flexible and powerful framework for temporal knowledge graph embedding. ChronoR proposed an inner product scoring function, which it theoretically demonstrated to generalize complex-domain models like ComplEx, thereby offering a robust alternative to Euclidean distance in high-dimensional spaces. Furthermore, ChronoR introduced advanced regularization techniques, including a tensor nuclear norm-inspired regularization and a 4-norm based temporal smoothness objective, to encourage consistent transformations for chronologically closer timestamps, enhancing model generalizability and capturing the smooth evolution of entities over time.

More recently, \textcite{ji2024} FSTRE (Fuzzy Spatiotemporal RDF Embedding) further extended this rotation-based paradigm by integrating fuzziness and spatial information alongside temporal rotation within a complex vector space. FSTRE addresses the challenge of modeling uncertain and dynamic knowledge by uniquely employing projection for spatial embedding and rotation for temporal embedding, while incorporating fine-grained fuzziness through the modal length of anisotropic vectors. This comprehensive approach demonstrates the versatility of geometric operations in complex space to represent multifaceted dynamics beyond just temporal evolution.

Collectively, these rotation-based models showcase high expressiveness for diverse relation patterns and temporal dynamics, providing an elegant and theoretically grounded method to capture time-dependent changes and their interactions within knowledge graphs. While their inherent complexity can increase with higher dimensions or the integration of additional factors like fuzziness and spatial information, they represent a powerful advancement in modeling the evolving nature of real-world knowledge. Future research could explore more adaptive rotation mechanisms, investigate their applicability to forecasting tasks, or integrate them with other advanced temporal modeling techniques to handle even more intricate forms of uncertainty and continuous time.
\subsection{Multi-Curvature and GNNs for Evolving KGs}
\label{sec:5_3_multi-curvature__and__gnns_for_evolving_kgs}


The inherent geometric complexity and intricate interactions within Temporal Knowledge Graphs (TKGs) pose significant challenges for traditional embedding methods. To address these limitations, recent advancements have explored sophisticated geometric spaces and Graph Neural Network (GNN) architectures, offering enhanced flexibility and expressiveness for dynamic knowledge representation. These cutting-edge approaches aim to capture the diverse underlying structures and complex temporal dependencies that characterize evolving KGs.

One prominent direction involves modeling TKGs in multi-curvature spaces, moving beyond the constraints of a single Euclidean space. \textcite{wang2024} introduced MADE (Multicurvature Adaptive Embedding), a novel model for Temporal Knowledge Graph Completion (TKGC) that embeds TKGs into a combination of Euclidean, hyperbolic, and hyperspherical spaces. MADE leverages a data-driven weighting mechanism to dynamically assign importance to each curvature space, allowing it to adaptively capture various geometric structures such as hierarchies, rings, and chains present in TKGs. It further incorporates a quadruplet distributor for information interaction and an innovative temporal regularization to ensure the smoothness of timestamp embeddings [wang2024].

Building upon this multi-curvature paradigm, \textcite{wang2024} further refined the approach with IME (Integrating Multi-curvature Shared and Specific Embedding). IME addresses the limitations of earlier multi-curvature methods, which often overlook the "spatial gap" and heterogeneity between different curvature spaces. It innovatively integrates both "space-shared" properties to capture commonalities across spaces and "space-specific" properties to model unique features of each curvature, effectively bridging inter-space semantic gaps. Furthermore, IME introduces an Adjustable Multi-curvature Pooling (AMP) mechanism that learns optimal pooling weights for superior information fusion and employs novel similarity, difference, and structure loss functions to guide the learning process, demonstrating superior performance in TKGC tasks [wang2024].

Complementary to these geometric embedding advancements, other research focuses on leveraging Graph Neural Networks (GNNs) to explicitly capture complex multi-fact interactions across different timestamps. \textcite{xie2023} proposed TARGAT (A Time-Aware Relational Graph Attention Model), which tackles the challenge of GNN-based models struggling to directly capture interactions among multiple facts occurring at varying timestamps. TARGAT treats multi-facts across different timestamps as a unified graph and introduces a dynamic time-aware relational generator that creates time-aware relational message transformation matrices. These matrices are then used for time-aware feature projection and aggregation, enabling the model to explicitly capture intricate multi-fact interactions and achieve state-of-the-art results on several benchmarks [xie2023].

In summary, both multi-curvature embedding models like MADE and IME, and GNN-based approaches such as TARGAT, represent significant strides in handling the complex temporal dependencies and geometric structures of evolving KGs. MADE and IME offer geometrically adaptive embeddings that can represent diverse structural patterns more accurately than single-space models, with IME further enhancing inter-space interaction and adaptive fusion. TARGAT, on the other hand, provides a robust GNN-centric framework for modeling dynamic, time-aware relational interactions. While these methods offer enhanced flexibility and expressiveness, they often introduce increased computational overhead due to multi-space embeddings or complex GNN architectures, and interpreting the precise contributions of different curvature spaces or attention mechanisms remains a challenge for future research.


### KGE for Downstream Applications and Practical Considerations

\section{KGE for Downstream Applications and Practical Considerations}
\label{sec:kge_for_downstream_applications__and__practical_considerations}



\subsection{Entity Alignment}
\label{sec:6_1_entity_alignment}

The integration of heterogeneous knowledge graphs (KGs) is a fundamental challenge in knowledge engineering, primarily revolving around the task of entity alignment (EA). This process involves identifying equivalent entities across different KGs to facilitate their seamless integration and enrichment. Knowledge Graph Embedding (KGE) has emerged as a powerful paradigm to address this, by projecting entities and relations into a continuous vector space where semantic similarity can be directly computed, thereby enabling the discovery of cross-KG correspondences.

Early KGE-based approaches for entity alignment often grappled with significant challenges, notably the scarcity of labeled training data, which frequently led to suboptimal precision and robustness. To mitigate this, \textcite{sun2018} introduced \textit{BootEA}, a pioneering bootstrapping approach. BootEA iteratively expands the training data by labeling likely alignments, employing a global optimization strategy based on max-weighted matching and an alignment editing method to resolve conflicts and reduce error accumulation. This method was innovative in its use of a limit-based objective function and $\epsilon$-truncated negative sampling to learn alignment-oriented embeddings, significantly improving performance in low-resource settings. However, like any bootstrapping technique, BootEA faced the inherent risk of noise amplification if initial embeddings or iteratively labeled data were imperfect, potentially propagating errors throughout the training process. Building on the semi-supervised learning paradigm, \textcite{pei2019} further addressed data scarcity with \textit{SEA}, which uniquely incorporated adversarial training to account for entity degree differences. This innovation enhanced the robustness of KGEs by ensuring more consistent alignment accuracy across entities with varying frequencies, a crucial aspect often overlooked by methods sensitive to data distribution imbalances, where high-degree entities might dominate embedding learning.

Recognizing that entities possess diverse characteristics beyond just their relational structure, \textcite{zhang2019} proposed \textit{MultiKE}, an innovative framework that unified multiple entity 'views'---namely, name, relation, and attribute---into a comprehensive embedding framework. MultiKE departed from previous approaches by jointly optimizing view-specific embeddings and, crucially, introduced "soft alignment" for relations and attributes, thereby reducing the heavy dependency on pre-existing seed alignments for these components. This multi-view approach significantly enhanced the richness of entity representations by integrating heterogeneous features. However, a critical challenge in such multi-view frameworks lies in effectively balancing and combining these diverse views, as improper weighting or potential negative transfer between views can hinder overall performance, adding complexity to feature engineering and model optimization. While these methods advanced the field by leveraging various entity features, they often overlooked higher-level semantic constraints. \textcite{xiang2021} presented \textit{OntoEA}, the first comprehensive framework to integrate ontological schema, including class hierarchies and disjointness axioms, into joint KG-ontology embedding. OntoEA directly addressed "class conflict" errors, a significant source of false positives in previous embedding-based EA methods, by explicitly modeling and learning inter-class conflicts through a novel Class Conflict Matrix and associated loss functions, thereby improving semantic consistency and leveraging richer background knowledge.

Leveraging the powerful neighborhood aggregation capabilities of Graph Neural Networks (GNNs), as detailed in Section 4.3, a new wave of entity alignment models has emerged to better capture structural similarities. GNNs inherently excel at aggregating multi-hop structural information from an entity's neighborhood, enabling them to learn more context-rich and discriminative embeddings for alignment. Unlike traditional KGEs that primarily focus on triple-level interactions, GNN-based models, such as RDGCN [fanourakis2022], can capture complex relational paths and structural patterns, leading to superior performance in identifying structural similarities between entities across KGs. This end-to-end learning of structural features offers a powerful alternative to manual feature engineering seen in methods like MultiKE. For instance, in the context of large-scale KGs, \textcite{xin2022dam} introduced a scalable GNN-based entity alignment approach that tackles the structure and alignment loss often incurred during KG partitioning. Their method proposes a centrality-based subgraph generation, self-supervised entity reconstruction, and cross-subgraph negative sampling to maintain structural integrity and alignment quality, demonstrating the adaptability of GNNs to real-world scalability challenges. Despite their strengths, GNNs can be sensitive to severe structural heterogeneity between KGs and may incur higher computational demands, particularly for very deep architectures or extremely large graphs, and can suffer from over-smoothing issues.

More recently, the landscape of entity alignment has been significantly impacted by the rise of Large Language Models (LLMs). These models, leveraging vast pre-trained knowledge and advanced natural language understanding capabilities, offer a distinct paradigm for EA, particularly in scenarios rich in textual descriptions or where structural information is sparse. As highlighted by \textcite{ge2023}, the integration of KGE methods with Pre-trained Language Models (PLMs) is a promising direction for KG completion, extending naturally to EA. LLM-based approaches can directly compare entity names, descriptions, and attributes by encoding them into a shared semantic space, often outperforming structure-focused GNNs in low-resource or zero-shot alignment settings where seed alignments are scarce. They excel at leveraging external, general-world knowledge implicitly encoded during pre-training, which is a significant advantage over models relying solely on the internal structure of the KGs. Furthermore, \textcite{zhu2024} emphasizes that the integration of global structural embedding with local semantic information (e.g., attributes, images) is crucial for enhancing alignment accuracy, a task where LLMs, with their multimodal capabilities, are increasingly proving effective. This paradigm shift allows for more flexible and robust alignment, especially when dealing with KGs that have limited overlap in their structural patterns but share rich textual content.

A comprehensive experimental review by \textcite{fanourakis2022} provides critical insights into the comparative performance and trade-offs of various embedding-based EA methods. Their meta-level analysis revealed statistically significant correlations between method performance and dataset characteristics, such as KG density and factual information richness. For instance, unsupervised and semi-supervised methods exploiting literal similarity (e.g., AttrE, KDCoE) were found to outperform supervised relation-based methods (like RDGCN) on datasets with decreasing density but rich factual information. This highlights that while GNNs generally excel at structural learning, methods leveraging textual or attribute information can be more robust in specific data sparsity scenarios or when structural information is limited, underscoring the importance of choosing the right approach based on data characteristics. The study also underscored the critical trade-off between effectiveness and efficiency, emphasizing that more complex models, while potentially more accurate, incur higher computational overhead, a practical consideration for real-world deployment.

These advancements collectively demonstrate a clear intellectual trajectory in leveraging KGE for entity alignment. The field has progressed from foundational KGE models that primarily focused on relational structures to increasingly sophisticated, problem-specific solutions that mitigate data scarcity, enhance robustness, integrate richer, heterogeneous information (e.g., attributes, ontological schema), and leverage advanced neural architectures like GNNs for deeper structural understanding, culminating in the integration of powerful LLMs for semantic matching. This evolution reflects a continuous effort to integrate diverse information sources and advanced learning paradigms to achieve more accurate and robust entity alignment across heterogeneous knowledge bases.
\subsection{Recommendation Systems and Question Answering}
\label{sec:6_2_recommendation_systems__and__question_answering}


Knowledge Graph Embeddings (KGE) play a pivotal role in enhancing the capabilities of intelligent information retrieval and decision-making processes, particularly in recommendation systems and Knowledge Graph-based Question Answering (KGQA). By representing entities and relations in continuous vector spaces, KGE models facilitate the discovery of intricate semantic relationships that are crucial for personalized suggestions and accurate query responses.

In the realm of recommendation systems, KGE models have evolved to capture increasingly nuanced user and item interactions. Early approaches often relied on labor-intensive, hand-engineered features derived from knowledge graphs. Addressing this, Recurrent Knowledge Graph Embedding (RKGE) [sun2018] introduced a novel recurrent network architecture to automatically learn semantic representations for both entities and the paths connecting them. This approach, featuring a batch of recurrent networks and a pooling operator, effectively models the diverse semantics of multiple paths between entity pairs, thereby characterizing user preferences and providing meaningful explanations for recommendations. Building upon the foundation of single-domain recommenders, the challenge of cross-domain item recommendations and cold start problems emerged. To tackle this, [liu2023] proposed a Cross-Domain Knowledge Graph Chiasmal Embedding approach, which efficiently interacts items across multiple domains through a novel binding rule. This method frames multi-domain item-item recommendation as a link prediction task within a cross-domain knowledge graph, demonstrating superior performance in both link prediction and multi-domain recommendation results. Further advancing the field, the demand for explainable recommendations has grown significantly. Contextualized Knowledge Graph Embedding (CKGE) [yang2023] addresses this by integrating motivation-aware contextual information and high-order connections within a knowledge graph. CKGE employs a specialized KG-based Transformer, which processes meta-graphs constructed for each talent-course pair, incorporating relational attention and structural encoding. A key innovation is its local path mask prediction mechanism, which reveals the saliency of meta-paths, offering explicit explanations for recommendations and characterizing user preferences.

For Knowledge Graph-based Question Answering (KGQA), KGE models are instrumental in bridging the gap between natural language queries and structured knowledge. Initial KGE-based QA systems focused on simpler query types. For instance, the Knowledge Embedding based Question Answering (KEQA) framework [huang2019] targets "simple questions" by jointly recovering the question's head entity, predicate, and tail entity representations within the KG embedding spaces. It then derives an answer by identifying the closest fact in the KG using a carefully designed joint distance metric, outperforming state-of-the-art methods on relevant benchmarks. Recognizing the need for domain-specific and more complex query handling, Marie and BERT [zhou2023] developed a KGQA system specifically for chemistry. This system leverages hybrid knowledge graph embeddings, operating on multiple embedding spaces queried in parallel, to handle deep ontologies, numerical filtering, and intricate chemical reaction mechanisms. It introduces a score alignment model, an implicit multihop relation algorithm, and a BERT-based bidirectional entity-linking model, significantly advancing domain-specific KGQA. A more recent development involves the synergistic integration of KGE with Large Language Models (LLMs). A knowledge-enhanced joint model [liu2024q3q] incorporates aviation assembly knowledge graph embeddings into LLMs for fault diagnosis. This model utilizes graph-structured big data from KGs to conduct prefix-tuning of LLMs, enabling online reconfiguration and strengthening specialized knowledge within the aviation assembly domain. By generating knowledge subgraphs and fusing knowledge through retrieval augmentation, it provides robust, knowledge-based reasoning responses, achieving high accuracy in practical industrial scenarios.

The progression in this area highlights a continuous effort to move beyond basic relational modeling towards more sophisticated, context-aware, and explainable systems. While significant strides have been made in automating feature learning, handling multi-domain interactions, and integrating with advanced language models, challenges remain. Future research could focus on developing more robust cross-domain knowledge transfer mechanisms that adapt to highly dynamic environments, enhancing the interpretability of complex multi-modal KGE-LLM systems, and exploring novel ways to dynamically update KGEs to reflect real-time changes in knowledge graphs for both recommendation and question answering tasks.
\subsection{Efficiency, Scalability, and Robustness in Training}
\label{sec:6_3_efficiency,_scalability,__and__robustness_in_training}


The successful deployment of Knowledge Graph Embedding (KGE) models in real-world applications hinges on their ability to operate efficiently, scale to massive knowledge graphs (KGs), and maintain robustness against noise and incompleteness. Beyond the design of expressive scoring functions, the choice and optimization of training components significantly impact model performance and practical viability [mohamed2021dwg]. This subsection delves into key advancements addressing these critical practical challenges.

A primary area of focus for training efficiency is negative sampling, a ubiquitous technique for learning from sparse positive triples. Traditional random negative sampling often suffers from the "vanishing gradient problem" by frequently selecting "easy" negatives, which provide little learning signal [zhang2018, qian2021]. To overcome this, \textbf{NSCaching} [zhang2018] introduced an efficient cache-based approach to dynamically store and sample high-quality, "hard" negative triplets, proving more effective than complex generative adversarial network (GAN)-based methods. For KGE models integrating multimodal data, \textbf{Modality-Aware Negative Sampling (MANS)} [zhang2023] further refines this by introducing modal-level sampling, particularly for visual embeddings, to explicitly learn modality alignment, a crucial aspect overlooked by prior entity-level strategies. Addressing the inherent noise in real-world KGs, \textbf{Confidence-Aware Negative Sampling} [shan2018] supports robust training by leveraging negative triple confidence, mitigating issues like zero loss and false detection. An alternative approach, the \textbf{Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE)} framework [li2021], mathematically re-derives the non-sampling square loss function to enable full-data training, achieving improved accuracy and stability without prohibitive computational or space costs for a broad class of models. Theoretically, the relationship between negative sampling and softmax cross-entropy loss functions has been unified using Bregman divergence, offering insights for fair comparisons and understanding their underlying mechanisms [kamigaito20218jz].

Beyond sampling, efforts to reduce model complexity and resource footprint are crucial. \textbf{TransGate} [yuan2019] enhanced parameter efficiency by introducing a shared gate structure, inspired by LSTMs, to discriminate relation-specific information with significantly fewer parameters and comparable time complexity to simpler models like TransE. For deploying high-performing but resource-intensive KGEs, \textbf{DualDE} [zhu2020] proposed a knowledge distillation framework that dually distills knowledge from a high-dimensional teacher KGE to a low-dimensional student. This approach can achieve substantial parameter reduction (up to 15x) and inference speedup (up to 6x) with minimal accuracy loss, demonstrating a practical trade-off between model capacity and efficiency. Complementing this, \textbf{LightKG} [wang2021] introduced a lightweight framework utilizing codebooks and codewords for efficient storage and inference, dramatically reducing memory consumption and search time while maintaining high approximate search accuracy.

Scalability for truly massive KGs often necessitates distributed or partitioned training approaches. \textbf{CPa-WAC} [modak2024] introduced a Constellation Partitioning-based Scalable Weighted Aggregation Composition framework for GNN-based KGE. This method partitions KGs into topological clusters using Louvain clustering and then aggregates embeddings from these partitions for global inference, reducing training time significantly (up to five times) without substantial accuracy degradation. However, a potential limitation of partitioning lies in the challenge of maintaining global graph coherence and preventing information loss across partition boundaries. For privacy-preserving and distributed training, \textbf{FedS} [zhang2024] proposed a communication-efficient federated KGE framework. FedS leverages entity-wise Top-K sparsification to reduce the size of transmitted parameters per communication round, enhancing communication efficiency with negligible performance degradation. Federated KGE, however, introduces its own set of challenges, including data heterogeneity across clients, which \textbf{FedLU} [zhu2023bfj] addresses through mutual knowledge distillation. Furthermore, privacy threats, such as inference attacks that can reveal sensitive triple existence, require robust defenses like differentially private mechanisms with private selection (\textbf{DP-Flames} [hu20230kr]). The vulnerability to poisoning attacks, where malicious clients inject biased updates, has also been demonstrated, highlighting the need for secure aggregation and anomaly detection in federated settings [zhou2024]. More broadly, empirical studies on parallel KGE training have revealed that many existing methods can negatively impact embedding quality, emphasizing the importance of careful technique selection, with variations of stratification and random partitioning showing promise [kochsiek2021].

Robustness to noise and incompleteness is another critical aspect. Beyond confidence-aware negative sampling, \textbf{Multi-Task Reinforcement Learning (MTRL)} [zhang2021] offers a general framework for robust KGE by training RL agents to filter noisy triples, leveraging multi-task learning for semantically similar relations. While not primarily a training efficiency technique, the integration of auxiliary information like logical rules (as discussed in Section 4.1) inherently enhances robustness by providing constraints and reducing reliance on potentially noisy observed facts. To ensure the reliability of KGE model predictions, \textbf{Probability Calibration for KGE Models} [tabacof2019] demonstrated that KGE models are often uncalibrated, proposing methods to ensure predicted probabilities are trustworthy, which is vital for high-stakes applications. Furthermore, \textbf{Committee-based KGE} [choi2020] showed that an ensemble of diverse KGE models can achieve more robust knowledge base completion than any single model, by aggregating multiple perspectives. Finally, efficient hyperparameter optimization is crucial for practical deployment. \textbf{KGTuner} [zhang2022fpm] proposes a two-stage search algorithm that efficiently explores hyperparameter configurations on small subgraphs and then fine-tunes on the full graph, consistently finding better hyperparameters within the same time budget.

In summary, operationalizing KGE models reveals a fundamental tension between achieving high expressiveness and ensuring practical deployability. While advancements in negative sampling, knowledge distillation, and lightweight architectures significantly improve training efficiency and reduce resource footprints, they often involve trade-offs in model capacity or require careful tuning to maintain performance. Similarly, scaling via partitioning or federated learning improves throughput and addresses privacy concerns but introduces complexities like communication bottlenecks, challenges in maintaining global graph coherence, and vulnerabilities to attacks. Future research must therefore focus on developing holistic solutions that co-design models and training regimes to be inherently efficient, scalable, and robust, rather than relying on isolated optimizations.


### Emerging Trends and Future Directions

\section{Emerging Trends and Future Directions}
\label{sec:emerging_trends__and__future_directions}



\subsection{Automated KGE and Meta-Learning}
\label{sec:7_1_automated_kge__and__meta-learning}

The increasing complexity of Knowledge Graph Embedding (KGE) model design and the dynamic nature of real-world knowledge graphs necessitate a shift towards automated model selection and adaptive learning paradigms. This section explores the growing trend towards automating the KGE model design process (AutoML) and leveraging meta-learning for dynamic and adaptive embeddings. These approaches collectively aim to reduce manual effort, improve model adaptability to diverse KG characteristics, and enable continuous learning in evolving environments, pushing KGE towards more intelligent, autonomous, and self-optimizing systems.

A significant challenge in KGE has been the manual design of effective scoring functions and model architectures, which often exhibit dataset-specific optimal performance. To address this, \textcite{zhang2019} introduced \texttt{AutoSF}, an AutoML framework designed to automatically search for optimal scoring functions for diverse knowledge graphs. By identifying a unified representation for bilinear models and incorporating domain-specific insights into relation properties, \texttt{AutoSF} employs a progressive greedy search algorithm with a predictor to efficiently discover novel, KG-dependent scoring functions, thereby significantly reducing manual effort and improving adaptability. Building on this, \textcite{di20210ib} further refined the concept of automated scoring function search by proposing an efficient relation-aware approach. Recognizing that different semantic patterns within a KG might benefit from distinct scoring mechanisms, their method encodes the search space as a supernet and utilizes a one-shot alternative minimization algorithm to efficiently discover relation-aware scoring functions, leading to improved performance over general, non-relation-aware searches. Extending the scope of AutoML beyond scoring functions to neural architectures, \textcite{shang2024} proposed \texttt{MGTCA}, a framework that addresses the data dependence of Graph Neural Network (GNN) types in Knowledge Graph Completion. \texttt{MGTCA} introduces a Trainable Convolutional Attention Network that allows for autonomous switching between GNN types (GCN, GAT, and a novel KGCAT) and integrates a Mixed Geometry Message Function to capture richer structural information from hyperbolic, hypersphere, and Euclidean spaces. This approach automates the model design process by adaptively selecting the most suitable GNN architecture for local graph structures, eliminating the need for expensive pre-validation. Similarly, \textcite{di2023} focused on a more granular level of GNN design by building a search space for the *message function* itself. Their innovation lies in allowing both the structure and operators of the message function to be searched, making it highly adaptable to different KG forms (traditional KGs, n-ary relational data, hyper-relational KGs) and datasets. While \texttt{AutoSF} and \textcite{di20210ib} primarily focus on the scoring function, \texttt{MGTCA} and \textcite{di2023} extend AutoML to the architectural components of GNNs, offering a more comprehensive automation of KGE model construction. The trade-off often lies between the computational cost of the search process and the expressiveness and adaptability of the automatically discovered models.

Beyond automating static model design, a critical aspect of modern KGE is the ability to continuously learn and adapt to evolving knowledge and unseen entities. Traditional KGE models are often transductive, struggling with emerging entities and dynamic knowledge updates. \textcite{sun2024} presented \texttt{MetaHG}, a meta-learning strategy specifically tailored for dynamic updates in evolving service ecosystems. \texttt{MetaHG} addresses the inefficiency of updating incremental knowledge by incorporating both local and potential global structural information from current KG snapshots via a hybrid GNN and Hypergraph Neural Network (HGNN) framework. This enables continuous learning and adaptation to changing knowledge, mitigating issues like spatial deformation. Complementing this, \textcite{chen2021} introduced \texttt{MorsE}, another meta-learning framework for inductive KGE that focuses on generating general entity embeddings for *entirely new entities in unseen KGs*. \texttt{MorsE} learns "meta-knowledge" through entity-independent modules (an Entity Initializer and a GNN Modulator) that capture transferable structural patterns, allowing it to produce high-quality embeddings for novel entities without retraining from scratch. While \texttt{MorsE} focuses on generalizing to unseen entities, \textcite{lee202380l}'s \texttt{InGram} extends this inductive capability to also generate embeddings for *new relations* at inference time, leveraging relation graphs and attention mechanisms to aggregate neighboring embeddings. These inductive approaches are crucial for handling the constant influx of new information in real-world KGs.

Furthermore, the challenge of continually updating KGE models with new knowledge while preserving old knowledge (Continual Knowledge Graph Embedding, CKGE) is a key area where meta-learning principles are applied. \textcite{liu2024to0} proposed Incremental Distillation (IncDE) for CKGE, which explicitly considers the graph structure of new knowledge. IncDE introduces a hierarchical strategy to optimize the learning order of new triples and devises an incremental distillation mechanism to seamlessly transfer entity representations, thereby promoting old knowledge preservation and mitigating catastrophic forgetting. Building on efficiency, \textcite{liu2024} introduced \texttt{FastKGE} with an Incremental Low-Rank Adapter (IncLoRA) mechanism. Inspired by parameter-efficient fine-tuning in large language models, IncLoRA efficiently learns and stores new knowledge by dividing new entities and relations into layers and assigning incremental low-rank adapters. This approach significantly reduces training costs for new knowledge acquisition while maintaining competitive performance in preserving old knowledge. While \texttt{MetaHG} focuses on dynamic updates within an evolving system, \texttt{MorsE} and \texttt{InGram} tackle the more general problem of inductive learning for entirely new entities and relations, and IncDE and FastKGE specifically address the efficiency and forgetting challenges inherent in continual learning scenarios.

In conclusion, the trend towards automated KGE model design and meta-learning for dynamic embeddings marks a significant step towards more intelligent, autonomous, and self-optimizing KGE systems. AutoML frameworks like \texttt{AutoSF}, \textcite{di20210ib}, \texttt{MGTCA}, and \textcite{di2023} are reducing manual effort by automating the search for optimal scoring functions and GNN architectures, leading to more adaptable models. Concurrently, meta-learning approaches such as \texttt{MetaHG}, \texttt{MorsE}, \texttt{InGram}, IncDE, and \texttt{FastKGE} are enabling KGE models to continuously learn, adapt to evolving knowledge, and generalize to unseen entities and relations, addressing the inherent transductive limitations of many traditional models. Future research must continue to balance the increasing complexity and computational cost of these adaptive and automated models with efficiency, robustness, and interpretability, especially as knowledge graphs grow in size and dynamism.
\subsection{Multimodal KGE and Explainability}
\label{sec:7_2_multimodal_kge__and__explainability}


The evolution of Knowledge Graph Embedding (KGE) is increasingly characterized by a progressive move beyond purely symbolic representations towards richer, multimodal information integration. This paradigm shift is critically intertwined with the growing demand for explainability in AI systems, as the inherent complexity introduced by diverse data types necessitates enhanced transparency and interpretability. The integration of multimodal data aims to create more comprehensive and robust knowledge representations, while explainable KGE methods are crucial for fostering trust and understanding, particularly in sensitive domains where model transparency is paramount for adoption and ethical deployment.

\subsubsection{Multimodal Knowledge Graph Embedding}
Integrating multimodal information into KGE seeks to enrich entity and relation representations by combining symbolic facts with features extracted from various data types, including text, images, and potentially audio or video. Early efforts primarily focused on leveraging diverse *textual* information. For instance, [zhang2019multike] introduced MultiKE, a pioneering approach for entity alignment that unified multiple entity "views" (name, relation, attribute) derived from textual descriptions. MultiKE innovated by designing cross-KG inference methods and a "soft alignment" for relations and attributes, thereby reducing reliance on costly seed alignments and demonstrating the benefits of heterogeneous textual feature integration for robust entity matching. While significant, these approaches largely remained within the linguistic domain.

The true frontier of multimodal KGE lies in the direct integration of *non-textual* modalities, such as visual features, with symbolic knowledge. Foundational models began to bridge this gap by extending existing KGE frameworks. For example, Image-embodied Knowledge Representation Learning (IKRL) [xie2016image] integrated visual information by projecting image features into the same embedding space as entities and relations, allowing visual similarities to influence the learning of symbolic relationships. Similarly, Multimodal Knowledge Base Embedding (MKBE) [guo2018multimodal] utilized CNNs to extract visual features from images associated with entities, fusing them with textual descriptions and structural information to enhance entity representations. These early methods demonstrated the potential of combining visual cues with relational facts, often by modifying scoring functions or introducing auxiliary loss terms.

More recent advancements leverage advanced neural architectures. For example, [liang2023hypernode] proposed Hyper-node Relational Graph Attention Network (HRGAT) as a customized Graph Neural Network (GNN) for multimodal knowledge graphs. HRGAT combines different modal information (e.g., visual, textual) with graph structure information, using a hyper-node concept to represent entities that aggregate features from various modalities, thereby yielding a more precise and interpretable representation. This approach highlights the utility of GNNs in processing heterogeneous features for multimodal KGE. Another concrete application is seen in domain adaptation for tasks like musical instrument recognition, where [eyharabide2021wx4] presented a method that incorporates KGE-derived semantic vector spaces as a key ingredient to guide the domain adaptation process, combining these semantic embeddings with visual embeddings from images. By training a neural network with these combined embeddings as anchors, their method demonstrates how KGE can enhance visual recognition tasks, effectively fusing symbolic knowledge with visual features to improve performance in data-scarce cultural heritage datasets. A recent survey by [zhu2024survey] further underscores the growing importance of multimodal entity alignment, highlighting the integration of diverse modalities beyond text, such as images and even video, as a critical future direction.

These approaches typically employ various fusion techniques, such as joint embedding spaces where features from different modalities are projected into a common vector space, or attention mechanisms that selectively weigh the importance of different modal inputs. Despite these advancements, challenges remain in developing universally applicable multimodal KGE models. Key issues include the inherent heterogeneity of data (e.g., varying dimensions, noise levels), the complexity of aligning information across vastly different modalities, and the scarcity of large-scale, richly annotated multimodal knowledge graph datasets for training and evaluation. Furthermore, the increased dimensionality and complexity introduced by multimodal data can significantly exacerbate the "black box" nature of KGE models, intensifying the demand for robust explainability.

\subsubsection{Explainable Knowledge Graph Embedding}
The growing importance of explainability in KGE is crucial for building trust and understanding in AI systems, especially as models become more complex and operate in sensitive applications. Explainable KGE methods aim to provide insights into model predictions, learned representations, and underlying reasoning processes. We can categorize these methods into two main types: ante-hoc (intrinsic) explainability, where models are designed to be inherently interpretable, and post-hoc explainability, where explanations are generated after model training.

**Ante-hoc/Intrinsic Explainability**: These models embed interpretability directly into their design, often through intuitive geometric or statistical properties of their representations or by integrating explicit symbolic knowledge. [li2024sphere] proposed SpherE, which embeds entities as spheres where the radius intuitively correlates with an entity's "universality" or prevalence in the knowledge graph. This sphere-based modeling not only enhances expressiveness for many-to-many relations but also offers an interpretable parameter that provides insights into the entity's role and scope within the KG. Similarly, [pavlovic2022expressive] introduced ExpressivE, a spatio-functional KGE model that embeds entity pairs as points and relations as hyper-parallelograms. A key contribution of ExpressivE is its explicit focus on providing an "intuitive interpretation" and "consistent geometric interpretation" of captured inference patterns (e.g., composition, hierarchy) through the spatial relationships of these hyper-parallelograms, making the model's internal logic more transparent and directly understandable.

However, relying solely on geometric intuition for explainability can be insufficient for complex logical reasoning. [gutierrezbasulto2018ontology] critically analyzed the compatibility between vector space representations and ontological rules, demonstrating that many popular embedding methods are incapable of modeling even simple rule types, thus failing to capture the dependencies inherent in symbolic knowledge. This highlights a limitation of purely geometric approaches. In contrast, models that integrate explicit logic rules offer a more formal and robust form of intrinsic explainability. For instance, [wang2019logic] proposed a logic rule-enhanced method that can be integrated with translational KGE models. By automatically mining logic rules and representing triples as first-order logic, this approach transfers human knowledge into embeddings, making the reasoning process more aligned with symbolic logic and thus inherently more interpretable. Such rule-aware designs move beyond mere geometric analogies to provide explanations grounded in logical consistency.

**Post-hoc Explainability**: These methods aim to explain model behavior after training, often by analyzing predictions or learned features. This category can be further broken down into different levels of explanation:

*   **Meta-level Understanding of Generalization**: This focuses on explaining *why* KGE models generalize or extrapolate. [li2021semantic] addresses this by introducing the concept of "Semantic Evidence" (relation-level, entity-level, and triple-level) to explain how KGE models extrapolate to unseen data, providing crucial insights into their underlying mechanisms and contributing to a meta-level understanding of model behavior. Expanding on this, [kurokawa2021explainable] proposed an explainable knowledge reasoning framework that combines multiple KGE techniques with corresponding explainable AI (XAI) techniques. This framework aims to integrate various methods to achieve complex reasoning with explanations, demonstrating a broader approach to understanding KGE behavior in a multi-model context.

*   **Path-based Explanations (Local Explanations)**: These methods provide insights into *which* specific paths or facts contribute to a particular prediction. [jia2020pconvkb] proposed PConvKB, a convolutional neural network-based embedding model that incorporates relation paths. By employing attention mechanisms to measure the local importance of relation paths and a measure called DIPF for global importance, PConvKB provides insights into *which* paths contribute most significantly to a fact's inference, thereby offering a form of explainability by highlighting the evidential paths in the graph. While effective for local explanations, the computational cost of enumerating and evaluating paths can be high for dense or very large KGs, limiting scalability.

*   **Application-specific Explanations**: In downstream applications, explainability often translates to providing concrete reasons for a model's output. [sun2018rkge] introduced RKGE, a recurrent knowledge graph embedding model designed for recommendation systems. RKGE learns path semantics within KGs and provides "meaningful explanations" for its recommendation results, demonstrating early efforts in transparent recommendation systems. Building on this, [yang2023ckge] further advanced explainable KGE with CKGE, a contextualized approach for explainable talent training course recommendation. CKGE constructs motivation-aware meta-graphs and employs a novel KG-based Transformer with a "local path mask prediction" mechanism to explicitly reveal the saliency of different meta-paths, offering concrete, motivation-driven explanations for specific recommendations. In sensitive domains like healthcare, explainability is paramount. For instance, SEConv [yang2025seconv] is designed to provide transparent and interpretable predictions in healthcare applications, where understanding the rationale behind a model's output (e.g., drug-disease association) is critical for clinical trust and decision-making. Such models are crucial for bridging the gap between complex AI systems and human experts.

In conclusion, the field of KGE is actively pursuing richer, multimodal representations to capture the complexity of real-world knowledge, moving from integrating diverse symbolic and textual features towards a broader, though still challenging, integration of non-textual modalities. Concurrently, there is a strong emphasis on developing explainable KGE methods, ranging from understanding model generalization and providing intrinsically interpretable representations (e.g., geometric or rule-based) to offering concrete, prediction-specific explanations. A significant gap remains, however, in models that are *both* truly multimodal and inherently explainable. The increased complexity from multimodal inputs often deepens the "black box" nature of KGE models, making it challenging to attribute predictions to specific modalities or explain cross-modal interactions. Future work will likely focus on developing sophisticated multimodal fusion techniques that are designed with explainability in mind, formalizing evaluation metrics for multimodal explanations, and advancing human-centered explainable AI for KGE, ensuring that as KGE models become more powerful, they also become more transparent and trustworthy.
\subsection{Open Challenges and Ethical Considerations}
\label{sec:7_3_open_challenges__and__ethical_considerations}


Despite remarkable progress in Knowledge Graph Embedding (KGE) research, the field continues to grapple with fundamental, persistent challenges that impede the robust, scalable, and ethically responsible deployment of KGE-powered AI systems. This subsection critically assesses these unresolved technical hurdles and delves into the crucial ethical implications, outlining essential areas for future investigation to ensure KGE techniques align with societal values.

A significant, overarching challenge remains **extreme scalability and continuous adaptation for dynamic Knowledge Graphs (KGs)**. While Section 6.3 discusses various solutions for improving KGE efficiency and handling dynamic updates, these often address specific facets rather than the holistic problem of KGs that are simultaneously massive, highly dynamic, and non-stationary. For instance, lightweight frameworks like LightKG [wang2021] reduce storage and inference costs, and knowledge distillation methods like DualDE [zhu2020] enable faster reasoning. Similarly, graph partitioning strategies such as CPa-WAC [modak2024] enhance the scalability of GNN-based KGEs. However, these solutions frequently involve trade-offs, such as simplified representations or reliance on discrete updates, which may not suffice for KGs where topology and semantics evolve continuously and rapidly. Meta-learning approaches, exemplified by MetaHG [sun2024] and [mao2024v2s], offer promise for adapting to incremental knowledge and emerging entities. Yet, the challenge of maintaining high-quality embeddings and efficient inference for KGs experiencing continuous, high-velocity changes across billions of entities and relations remains a grand research problem. The entire training pipeline, including loss functions, hyperparameters, and negative sampling strategies, requires holistic optimization to achieve true scalability and accuracy [mohamed2021dwg], rather than focusing solely on scoring functions.

Another critical area is the **robust handling of inherent uncertainty, incompleteness, and adversarial threats in real-world data**. KGs are intrinsically noisy, incomplete, and can be subject to manipulation. While methods like confidence-aware negative sampling [shan2018] and reinforcement learning frameworks [zhang2021] improve robustness against noise, and techniques like committee-based models [choi2020] or logical rule integration [guo2017, guo2020] address incompleteness, fundamental limitations persist. Real-world knowledge often contains fuzzy, conflicting, or inherently uncertain facts, as seen in complex spatiotemporal KGs [ji2024]. Current KGE models frequently struggle to reliably quantify the certainty of their predictions, with studies showing that KGE models are often uncalibrated [tabacof2019]. This lack of reliable confidence scores undermines the trustworthiness of KGE-powered systems in critical applications. Furthermore, KGE models are vulnerable to *data poisoning attacks* [zhang20190zu], where malicious actors can manipulate the plausibility of facts by injecting or deleting triples, posing a significant threat to the integrity and reliability of learned representations and downstream tasks. Developing KGEs that are inherently resilient to such adversarial manipulations and can robustly model epistemic uncertainty remains a profound challenge.

Beyond these technical limitations, KGE research faces critical **ethical implications** that demand proactive attention.
\begin{enumerate*}[label=(\roman*)]
    \item \textbf{Bias and Fairness in Learned Representations}: KGE models learn from existing KGs, which are often constructed from real-world data reflecting historical, societal, or data collection biases. If these biases are embedded in learned representations, they can be amplified when deployed in sensitive applications, leading to unfair or discriminatory outcomes. For instance, KGEs used in healthcare (e.g., SEConv [yang2025], multimodal reasoning for diseases [zhu2022]) could perpetuate biases in medical records, affecting diagnoses or treatment recommendations for specific demographic groups. A specific technical challenge is *degree bias*, where entities with fewer connections (low-degree nodes) receive poorer representations, leading to performance disparities [shomer2023imo]. While solutions like KG-Mixup [shomer2023imo] mitigate degree bias in static KGs and FairDGE [li2024wyh] addresses *structural fairness* in dynamic graphs, the broader challenge lies in developing comprehensive, domain-agnostic debiasing strategies that account for intersectional biases without compromising model utility. The field requires more robust methods to detect, quantify, and mitigate various forms of bias, ensuring equitable performance across diverse entity groups.
    \item \textbf{Data Privacy and Confidentiality Concerns}: Training KGE models often requires access to vast amounts of data, some of which may be sensitive or proprietary. Sharing such data or models trained on it can pose significant privacy risks. Federated KGE (FKGE) offers a promising direction by enabling collaborative learning from distributed KGs without centralizing raw data, as seen in FedS [zhang2024], personalized FKGE (PFedEG) [zhang2024], and cross-domain FKGE (FedCKE) [huang2023grx]. However, FKGE is not immune to privacy threats. *Inference attacks* and *model inversion attacks* can still reveal sensitive information about clients' private KGs, even with distributed training [hu20230kr]. Advanced defenses, such as differentially private FKGE (DP-Flames) [hu20230kr], aim to provide stronger privacy guarantees but often come with inherent *privacy-utility trade-offs*. The ongoing challenge is to develop robust privacy-preserving mechanisms that can effectively balance strong confidentiality with high model performance, especially in heterogeneous and dynamic federated environments.
    \item \textbf{Responsible Deployment of KGE-powered AI Systems}: KGEs are increasingly integrated into critical AI systems, from question answering in chemistry [zhou2023] to fault diagnosis in aviation assembly [liu2024q3q]. In such high-stakes applications, ensuring fairness, accountability, and transparency (FAT) is paramount. The aforementioned issues of uncalibrated predictions [tabacof2019], vulnerability to adversarial attacks [zhang20190zu], and embedded biases directly undermine the trustworthiness and safety of these systems. Responsible deployment necessitates moving beyond purely performance-driven metrics to a holistic consideration of ethical dimensions. This includes developing comprehensive frameworks for auditing KGE systems for bias, ensuring robust defenses against adversarial manipulation, and providing reliable confidence scores for predictions. The field must prioritize the development of KGEs that are not only powerful but also inherently trustworthy, fair, and beneficial for society, requiring interdisciplinary collaboration to establish ethical guidelines and regulatory standards.
\end{enumerate*}

In conclusion, while KGE research has made remarkable strides, significant challenges persist in achieving extreme scalability for dynamic KGs, ensuring robustness against inherent data imperfections and adversarial threats, and critically, addressing the profound ethical implications of bias, privacy, and responsible deployment. Future investigations must prioritize the development of KGE techniques that are not only performant but also inherently robust, fair, and interpretable (as discussed in Section 7.2), aligning with societal values and ensuring the trustworthy application of KGE-powered AI systems in an increasingly complex world.


### Conclusion

\section{Conclusion}
\label{sec:conclusion}



\subsection{Synthesis of Key Developments}
\label{sec:8_1_synthesis_of_key_developments}


The evolution of Knowledge Graph Embedding (KGE) research represents a continuous intellectual journey, driven by the imperative to capture increasingly rich semantics, model complex structural patterns, and adapt to the dynamic nature of real-world knowledge. This progression has transformed the field from rudimentary geometric models to sophisticated neural architectures and specialized temporal frameworks, significantly enhancing the expressiveness and utility of KGE across diverse applications. The overarching theme is a relentless pursuit of more powerful and versatile knowledge representation techniques, moving from simple, explicit assumptions about relations to implicitly learned, context-aware, and adaptable embeddings.

The foundational phase of KGE was characterized by geometric interpretations, primarily focusing on modeling relations as transformations in vector spaces. Early translational models, exemplified by TransE [bordes2013], established the viability of embedding entities and relations into continuous spaces. However, their simplicity led to limitations in handling complex relational patterns like 1-to-N, N-to-1, and N-to-N. This spurred advancements such as TransH [wang2014] and TransD [ji2015], which introduced relation-specific hyperplanes and dynamic mapping matrices, respectively, to enhance expressiveness for diverse entities and relations. Simultaneously, semantic matching models like RESCAL [nickel2016] and ComplEx [trouillon2016] offered alternative algebraic frameworks, using tensor factorization and complex-valued embeddings to capture richer, often symmetric or antisymmetric, semantic interactions. A pivotal conceptual leap came with rotational models like RotatE [sun2019], which elegantly modeled relations as rotations in complex space, providing a unified mechanism to capture symmetry, anti-symmetry, inversion, and composition simultaneously. This marked a significant departure from the limitations of purely translational or bilinear models, as highlighted by surveys classifying KGEs based on their underlying mathematical spaces [cao2022] and geometric transformations [ge2023]. This foundational understanding culminated in unifying frameworks such as HolmE [zheng2024], which theoretically demonstrated that prominent models like TransE and RotatE are special cases of a more general Riemannian KGE, inherently closed under composition and robustly modeling long-tail composition patterns.

Building on these geometric foundations, the field expanded into advanced non-Euclidean spaces and generalized transformations to address specific structural challenges. The inherent capacity of hyperbolic spaces to represent hierarchical structures more efficiently than Euclidean spaces led to models like those leveraging the Poincaré Ball [pan2021] and Lorentz hyperbolic space [liang2024], which perform operations directly in these spaces to capture fine-grained hierarchical semantics. The exploration of Lie groups, such as embedding on a torus in TorusE [ebisu2017], aimed to resolve regularization conflicts and improve efficiency. The quest for universal expressiveness further led to compound operations, as seen in CompoundE [ge2022], which generalized geometric transformations by combining translation, rotation, and scaling. This trajectory towards more versatile geometric modeling reached new heights with frameworks like GoldE [li2024], which introduced universal orthogonal parameterization based on generalized Householder reflections. GoldE represents a significant advancement by generalizing orthogonal transformations across both dimensions and geometries (Euclidean, elliptic, hyperbolic), allowing for superior modeling of the inherent topological heterogeneity of real-world KGs. This continuous push for mathematically richer frameworks underscores the commitment to accurately represent intricate relational semantics and complex graph structures. Even more abstract theoretical explorations, such as Knowledge Sheaves [gebhart2021gtp], demonstrate the depth of analytical inquiry into the fundamental nature of KGE.

Beyond purely geometric transformations, a major paradigm shift involved integrating richer contextual information and leveraging advanced neural architectures. Early efforts recognized the limitations of relying solely on triple-level interactions, leading to models that incorporated auxiliary information such as textual descriptions (e.g., SSP [xiao2016]), entity types (e.g., TransET [wang2021], TaKE [he2023]), and logical rules (e.g., RUGE [guo2017], RulE [tang2022]). These integrations provided valuable context, improved semantic consistency, and addressed data sparsity, particularly in incomplete KGs. The advent of deep learning architectures further revolutionized KGE. Convolutional Neural Networks (CNNs), as adopted by ConvE [dettmers2018], enabled the extraction of rich, non-linear features, moving beyond fixed scoring functions to data-driven feature learning. Graph Neural Networks (GNNs) became pivotal for aggregating multi-hop structural information from an entity's neighborhood, exemplified by R-GCN [zhang2020] and Logic Attention Network (LAN) [wang2018], which enabled inductive learning for unseen entities. More recently, Transformer architectures, such as TGformer [shi2025], have been adapted to capture global and local structural features through self-attention mechanisms, further enhancing context-rich embeddings and generalization capabilities. The synergy with Large Language Models (LLMs) represents a cutting-edge trend, integrating language semantics with structural embeddings for even deeper contextual understanding [shen2022].

The field has also made significant strides in addressing the dynamic nature of knowledge and the practical challenges of real-world deployment. Temporal Knowledge Graph Embedding (TKGE) emerged as a critical sub-field, moving from early geometric approaches like HyTE [dasgupta2018] (using hyperplanes for timestamps) and tensor decomposition methods [lin2020] to more sophisticated rotation-based models like TeRo [xu2020] and ChronoR [sadeghian2021], which elegantly capture temporal evolution in complex or k-dimensional spaces. Recent innovations have pushed into multi-curvature spaces (e.g., MADE [wang2024], IME [wang2024]) and GNN-based approaches (e.g., TARGAT [xie2023]) to model intricate temporal dependencies and geometric complexities. Concurrently, substantial efforts have focused on improving training robustness, efficiency, and scalability for massive KGs. This includes advanced negative sampling strategies (e.g., NSCaching [zhang2018], MANS [zhang2023]), model compression techniques (e.g., DualDE [zhu2020]), and distributed learning paradigms like Federated KGE [zhang2024]. The utility of KGE has expanded far beyond simple link prediction, demonstrating its versatility in crucial downstream applications such as entity alignment (e.g., BootEA [sun2018], MultiKE [zhang2019]), recommendation systems (e.g., RKGE [sun2018], Cross-Domain KGE [liu2023]), and question answering (e.g., KEQA [huang2019]). Furthermore, KGEs have proven valuable for broader data mining tasks [portisch20221rd] and semantic querying on scholarly data [tran2019j42], showcasing their role in enabling intelligent information retrieval and decision-making across diverse domains.

In essence, the journey of KGE research reflects a profound commitment to developing increasingly powerful and versatile knowledge representation techniques. From initial geometric intuitions, the field has embraced the complexity of non-Euclidean spaces, the power of neural architectures for contextual learning, and the necessity of modeling dynamic knowledge. This continuous evolution, driven by theoretical advancements and practical demands, underscores KGE's pivotal role in bridging symbolic knowledge with modern machine learning, pushing towards more robust, intelligent, and context-aware knowledge systems.
\subsection{Remaining Challenges and Future Research Avenues}
\label{sec:8_2_remaining_challenges__and__future_research_avenues}

The journey of Knowledge Graph Embedding research, from foundational geometric models to advanced neural architectures and temporal extensions, has profoundly enhanced our ability to represent and reason with complex knowledge. However, the field now stands at a critical juncture, defined by a set of interconnected grand challenges and transformative future research avenues. The persistent demand for **robust scalability** for truly massive, dynamic, and distributed KGs remains paramount. While parallel training techniques [kochsiek2021] and federated learning paradigms [zhang2024, hu20230kr, huang2023grx] offer promising paths, they introduce inherent trade-offs. For instance, distributed training, while addressing data volume and privacy concerns, complicates the management of semantic disparity across clients [zhang2024] and necessitates novel defenses against privacy threats [hu20230kr]. This highlights a fundamental tension: solutions for scalability often introduce new vulnerabilities or complexities in other areas, such as managing communication overhead or ensuring data consistency.

Simultaneously, **enhancing robustness against noise, incompleteness, and adversarial attacks** is crucial for trustworthy AI. Real-world KGs are imperfect, and while methods like logical rule integration [tang2022] and consistency constraints derived from knowledge sheaves [gebhart2021gtp] improve resilience, models must become intrinsically more robust. The challenge here is to achieve this without sacrificing model expressiveness or interpretability, as highly robust models can sometimes be more opaque or require more complex training regimes, as seen in the impact of hyperparameters on embedding quality [lloyd2022]. This directly links to the **persistent demand for greater interpretability and explainability**. Moving beyond post-hoc explanations, the goal is to develop *inherently interpretable* models [daruna2022dmk, kurokawa2021f4f] where predictions are transparent and logic-grounded, perhaps by deeper integration of symbolic reasoning with neural approaches [tang2022, gutirrezbasulto2018oi0]. This often implies a trade-off with the complexity required for capturing nuanced, diverse semantics or integrating multimodal information.

Looking ahead, the future of KGE will be shaped by **unified perceptual-symbolic reasoning**, integrating diverse modalities like text, images, and audio. This requires sophisticated semantic alignment and joint representation learning, which in turn demands more **autonomous and adaptive KGE systems**. Such systems, leveraging insights from the mathematical properties of representation spaces [cao2022] and hyperparameter sensitivities [lloyd2022], could self-optimize architectures and learning strategies, reducing manual effort. However, the pursuit of such advanced capabilities must be balanced with the **crucial need for ethically aligned, fair, and privacy-preserving embedding techniques**. The integration of multimodal data, for example, can exacerbate existing biases, while automated systems might inadvertently optimize for performance at the expense of fairness or privacy. Federated learning, while offering privacy benefits, still requires robust defenses [hu20230kr] and careful consideration of structural fairness across clients [zhang2024].

Therefore, the next generation of knowledge graph intelligence demands a holistic approach. It requires navigating the intricate interdependencies and inherent trade-offs between scalability, robustness, and interpretability, while simultaneously pushing the boundaries of multimodal integration, autonomous adaptation, and ethical responsibility. This synthesis underscores that impactful contributions will emerge from interdisciplinary efforts that not only advance theoretical foundations but also rigorously address the practical, societal implications of deploying KGE in real-world intelligent systems.


