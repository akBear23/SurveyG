\subsection*{Background: Knowledge Graphs}

Knowledge Graphs (KGs) represent a foundational paradigm for structuring and organizing factual information, serving as a backbone for numerous intelligent systems and advanced analytical applications \cite{choudhary2021, yan2022}. At their essence, KGs model world knowledge as a collection of interconnected entities and their relationships, typically expressed in the form of (head entity, relation, tail entity) triples. For example, a triple such as (\textit{Barack Obama, bornIn, Honolulu}) explicitly captures a factual relationship between two distinct entities, providing a machine-readable assertion of knowledge.

The conceptual lineage of knowledge graphs can be traced back to early semantic networks, which emerged in artificial intelligence research during the 1960s with the goal of representing human knowledge in a structured, machine-interpretable format. This evolutionary path progressed through various stages, including the development of expert systems and formal ontologies. A significant milestone in this journey was the advent of the Semantic Web vision, which introduced foundational technologies such as the Resource Description Framework (RDF) and Web Ontology Language (OWL) to enable data interoperability and machine-understandable content across the web. These efforts laid the groundwork for the large-scale, publicly available knowledge bases that characterize modern KGs. Prominent examples include Freebase, a collaborative knowledge base acquired by Google; DBpedia, which systematically extracts structured information from Wikipedia; and Wikidata, serving as a central, multilingual repository for the structured data of Wikimedia projects \cite{rossi2020}. These contemporary KGs are indispensable resources, playing a crucial role in organizing vast amounts of world knowledge, enhancing web search capabilities, powering sophisticated question-answering systems \cite{huang2019}, and facilitating personalized recommender systems \cite{sun2018}.

Despite their immense utility in organizing and querying structured information, traditional symbolic representations within knowledge graphs inherently present several critical challenges that limit their full potential and scalability. Firstly, KGs frequently suffer from **data sparsity** and incompleteness. Real-world knowledge is vast and constantly evolving, making it practically impossible to explicitly enumerate every possible fact. This incompleteness means that many potential relationships are missing, hindering comprehensive reasoning and accurate inference, as the absence of an explicit triple does not necessarily imply the absence of a relationship.

Secondly, performing **statistical inference** directly on discrete, symbolic representations is inherently difficult and often inefficient. Traditional rule-based reasoning, while offering precision and interpretability, struggles to generalize effectively over noisy, uncertain, or incomplete data. It operates on an "all or nothing" principle, where a rule either fires or it doesn't, making it brittle and unable to capture probabilistic relationships, nuanced semantic similarities, or exceptions \cite{tang2022}. This rigidity prevents the discovery of latent patterns or implicit connections that are not explicitly codified by rules or triples. For instance, determining that two entities are semantically similar based on their shared context or indirect relationships is challenging with purely symbolic methods.

Thirdly, the **computational overhead of rule-based reasoning** can be substantial, particularly for large-scale KGs containing billions of triples. Deriving new facts through logical inference rules often involves complex combinatorial searches across the graph, which is computationally expensive and challenging to scale. The explicit representation of every fact and rule, coupled with the need for exhaustive search, makes traditional reasoning impractical for dynamic and massive knowledge bases.

These collective challenges—data sparsity, the inherent difficulty of statistical inference on discrete symbols, and the computational burden of rule-based reasoning—highlight a critical need for more flexible, robust, and continuous representations of knowledge. This fundamental motivation spurred the development of Knowledge Graph Embedding (KGE) techniques. KGE aims to address these limitations by transforming discrete entities and relations into low-dimensional, continuous vector spaces, thereby enabling the capture of latent semantic information, facilitating efficient statistical inference, and allowing for more scalable computation \cite{choudhary2021, yan2022}. This paradigm shift from discrete symbols to dense, continuous vectors forms the conceptual foundation for a new generation of knowledge graph processing, bridging symbolic knowledge representation with modern machine learning techniques.