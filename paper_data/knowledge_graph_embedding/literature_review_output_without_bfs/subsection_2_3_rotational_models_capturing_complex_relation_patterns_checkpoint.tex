\subsection*{Rotational Models: Capturing Complex Relation Patterns}

The accurate representation of diverse relational patterns within knowledge graphs (KGs) has been a persistent challenge for embedding models. While early translational models offered an efficient paradigm for learning entity and relation embeddings, they often struggled to simultaneously capture complex logical properties such as symmetry, anti-symmetry, inversion, and composition within a unified framework. Initial attempts to enhance the expressiveness of translational models, such as TransH \cite{wang2014}, addressed the limitations of TransE by projecting entities onto relation-specific hyperplanes. This allowed entities to have varying representations depending on the relation, thereby better handling one-to-many and many-to-one mapping properties. Further advancements, like TransD \cite{ji2015}, introduced dynamic mapping matrices for both entities and relations, aiming for a more fine-grained representation of diversity and improved efficiency by avoiding computationally intensive matrix-vector multiplications.

Despite these improvements in handling complex mapping properties, a single, elegant mechanism capable of unifying all fundamental relational patterns remained elusive. Semantic matching models, such as ComplEx \cite{trouillon2016}, which employed complex-valued embeddings and a Hermitian dot product, successfully modeled symmetric and anti-symmetric relations. However, ComplEx, like other semantic matching approaches, did not inherently capture compositional patterns with the same elegance or effectiveness. This gap highlighted the need for a more generalized geometric operation that could intrinsically model a broader spectrum of relational semantics.

A significant breakthrough in this area was the introduction of RotatE \cite{sun2018}, which represents a profound generalization of the translational embedding paradigm. RotatE elegantly interprets relations as element-wise rotations in a complex vector space, where the head entity embedding is rotated to align with the tail entity embedding. Specifically, for a valid triple $(h, r, t)$, RotatE models the relationship as $t = h \odot r$, where $h, r, t$ are complex-valued embeddings and $\odot$ denotes the Hadamard (element-wise) product, with the modulus of each element of $r$ constrained to $|r_i|=1$. This geometric operation allows RotatE to inherently and simultaneously capture diverse relational patterns.

The power of RotatE lies in its ability to model complex logical properties within a unified framework. For instance, symmetry is captured when a relation $r$ corresponds to a rotation by an angle of $\pi$ (or $0$), meaning $r = r^{-1}$. Anti-symmetry is naturally handled when $r \neq r^{-1}$. Inversion is directly supported, as the inverse relation $r^{-1}$ is simply the conjugate of $r$ in complex space. Most notably, RotatE can infer composition patterns, where if $(h, r_1, m)$ and $(m, r_2, t)$ are true, then $(h, r_1 \odot r_2, t)$ is also likely true, with $r_1 \odot r_2$ representing the composite relation. This capability to infer composition was a critical advantage over models like ComplEx \cite{trouillon2016}, which struggled with such patterns.

RotatE's novel approach, coupled with a self-adversarial negative sampling strategy, demonstrated superior performance across various benchmark datasets, including those specifically designed to test compositionality, such as the "Countries" dataset \cite{sun2018}. Its ability to model complex logical properties more effectively than earlier translational or semantic matching approaches marked a significant contribution to improving KGE expressiveness. By offering a mathematically elegant and unified framework for diverse relational patterns, RotatE set a new standard for capturing intricate relational semantics, paving the way for more robust and logically consistent knowledge graph reasoning. The success of rotational models underscores the potential of leveraging richer geometric transformations in higher-dimensional spaces to unlock deeper semantic understanding within knowledge graphs.