\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 377 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction}

\section{Introduction}
\label{sec:introduction}

\subsection{Background: Knowledge Graphs}
\label{sec:1\_1\_background:\_knowledge\_graphs}

Knowledge Graphs (KGs) represent a foundational paradigm for structuring and organizing factual information, serving as a backbone for numerous intelligent systems and advanced analytical applications \cite{choudhary2021, yan2022}. At their essence, KGs model world knowledge as a collection of interconnected entities and their relationships, typically expressed in the form of (head entity, relation, tail entity) triples. For example, a triple such as (\textit{Barack Obama, bornIn, Honolulu}) explicitly captures a factual relationship between two distinct entities, providing a machine-readable assertion of knowledge.

The conceptual lineage of knowledge graphs can be traced back to early semantic networks, which emerged in artificial intelligence research during the 1960s with the goal of representing human knowledge in a structured, machine-interpretable format. This evolutionary path progressed through various stages, including the development of expert systems and formal ontologies. A significant milestone in this journey was the advent of the Semantic Web vision, which introduced foundational technologies such as the Resource Description Framework (RDF) and Web Ontology Language (OWL) to enable data interoperability and machine-understandable content across the web. These efforts laid the groundwork for the large-scale, publicly available knowledge bases that characterize modern KGs. Prominent examples include Freebase, a collaborative knowledge base acquired by Google; DBpedia, which systematically extracts structured information from Wikipedia; and Wikidata, serving as a central, multilingual repository for the structured data of Wikimedia projects \cite{rossi2020}. These contemporary KGs are indispensable resources, playing a crucial role in organizing vast amounts of world knowledge, enhancing web search capabilities, powering sophisticated question-answering systems \cite{huang2019}, and facilitating personalized recommender systems \cite{sun2018}.

Despite their immense utility in organizing and querying structured information, traditional symbolic representations within knowledge graphs inherently present several critical challenges that limit their full potential and scalability. Firstly, KGs frequently suffer from \textbf{data sparsity} and incompleteness. Real-world knowledge is vast and constantly evolving, making it practically impossible to explicitly enumerate every possible fact. This incompleteness means that many potential relationships are missing, hindering comprehensive reasoning and accurate inference, as the absence of an explicit triple does not necessarily imply the absence of a relationship.

Secondly, performing \textbf{statistical inference} directly on discrete, symbolic representations is inherently difficult and often inefficient. Traditional rule-based reasoning, while offering precision and interpretability, struggles to generalize effectively over noisy, uncertain, or incomplete data. It operates on an "all or nothing" principle, where a rule either fires or it doesn't, making it brittle and unable to capture probabilistic relationships, nuanced semantic similarities, or exceptions \cite{tang2022}. This rigidity prevents the discovery of latent patterns or implicit connections that are not explicitly codified by rules or triples. For instance, determining that two entities are semantically similar based on their shared context or indirect relationships is challenging with purely symbolic methods.

Thirdly, the \textbf{computational overhead of rule-based reasoning} can be substantial, particularly for large-scale KGs containing billions of triples. Deriving new facts through logical inference rules often involves complex combinatorial searches across the graph, which is computationally expensive and challenging to scale. The explicit representation of every fact and rule, coupled with the need for exhaustive search, makes traditional reasoning impractical for dynamic and massive knowledge bases.

These collective challenges—data sparsity, the inherent difficulty of statistical inference on discrete symbols, and the computational burden of rule-based reasoning—highlight a critical need for more flexible, robust, and continuous representations of knowledge. This fundamental motivation spurred the development of Knowledge Graph Embedding (KGE) techniques. KGE aims to address these limitations by transforming discrete entities and relations into low-dimensional, continuous vector spaces, thereby enabling the capture of latent semantic information, facilitating efficient statistical inference, and allowing for more scalable computation \cite{choudhary2021, yan2022}. This paradigm shift from discrete symbols to dense, continuous vectors forms the conceptual foundation for a new generation of knowledge graph processing, bridging symbolic knowledge representation with modern machine learning techniques.
\subsection{Role of KG Embedding}
\label{sec:1\_2\_role\_of\_kg\_embedding}

Knowledge Graph Embedding (KGE) stands as a pivotal paradigm, forging a crucial bridge between the explicit, symbolic representations of knowledge graphs (KGs) and the implicit, continuous representations favored by modern neural approaches. Building upon the challenges of data sparsity, computational overhead, and difficulty in statistical inference inherent to purely symbolic KGs, KGE transforms discrete entities and relations into continuous, low-dimensional vector spaces \cite{choudhary2021, yan2022}. This transformation is fundamental for enabling efficient computation, facilitating seamless integration with machine learning models, and, critically, for capturing latent semantic relationships and structural patterns that are often elusive in discrete formats.

At its core, KGE operates on the principle that entities and relations can be represented as vectors or matrices within an embedding space, where semantic similarity and relational patterns are encoded by their geometric positions and transformations. For instance, in the foundational translational model TransE, a relation is conceptualized as a translation vector from a head entity to a tail entity, formalized as $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ \cite{bordes2013}. This elegant vector arithmetic allows for the inference of missing links based on proximity in the embedding space. Such a continuous representation inherently addresses data sparsity by enabling generalization through shared vector space properties; entities with similar contexts or relations tend to cluster together, facilitating the discovery of nuanced semantic relationships not explicitly defined in the symbolic graph \cite{choudhary2021}. For example, if "Paris" and "France" have a similar vector relationship to "Berlin" and "Germany," the model implicitly learns the "capital city of" concept, even if not explicitly encoded as a rule. Models like Triple Context-Based KGE \cite{gao2018di0} and PConvKB \cite{jia20207dd} further enhance this by explicitly incorporating local and global contextual information, such as neighboring entities and relation paths, to enrich the learned embeddings and capture finer-grained semantics.

The continuous nature of KGE representations offers several significant advantages. Firstly, it enables highly efficient computation during inference. Instead of complex symbolic matching or rule application, KGE models perform vector operations (e.g., dot products, distance calculations) that are computationally inexpensive and highly parallelizable. This makes them scalable for querying large-scale KGs \cite{yan2022}. However, it is crucial to acknowledge that while inference is efficient, the \textit{training} of these models on massive KGs can be computationally intensive, a challenge addressed by techniques focusing on efficiency and scalability in later sections (e.g., \cite{do20184o2} for handling many relations). Secondly, KGE acts as a crucial interface for integrating symbolic knowledge with modern machine learning and deep learning models. The learned embeddings serve as rich, pre-trained features for various neural architectures, allowing for end-to-end learning and leveraging the strengths of both symbolic and sub-symbolic AI paradigms \cite{choudhary2021}. This integration is vital for tasks requiring nuanced understanding and prediction, where traditional rule-based systems often fall short. Furthermore, KGE models can incorporate background taxonomic information, such as subclasses and subproperties, to enhance learning and make more robust predictions, as demonstrated by approaches like \cite{fatemi2018e6v}.

KGE plays a dual, yet interconnected, role in practical applications: providing an encoding for data mining tasks and enabling robust link prediction \cite{portisch20221rd}. For data mining, the embeddings serve as rich feature vectors for entities, which can then be directly utilized in tasks like clustering, classification, or recommendation systems. For link prediction, KGE models learn a scoring function that quantifies the plausibility of a given triple $(h, r, t)$, allowing for the completion of incomplete KGs by predicting missing entities or relations. This capability is fundamental for a wide array of tasks:
\begin{itemize}
    \item \textbf{Link Prediction}: Identifying missing facts within a KG, crucial for knowledge graph completion and enrichment.
    \item \textbf{Entity Alignment}: Discovering equivalent entities across different KGs, facilitating the integration and merging of heterogeneous knowledge sources.
    \item \textbf{Question Answering}: Enhancing natural language understanding by providing semantic context from KGs to answer complex queries.
    \item \textbf{Recommendation Systems}: Improving personalized recommendations by leveraging relational information between users, items, and their attributes.
\end{itemize}

However, this transformation from symbolic to continuous representations is not without trade-offs. The resulting dense vectors often lack the direct interpretability and logical guarantees inherent in symbolic systems, making it challenging to understand \textit{why} a particular prediction was made. This limitation has spurred significant research into explainable KGE models \cite{kurokawa2021f4f} and methods that integrate logical rules directly into the embedding process, such as RulE \cite{tang2022}, which learns explicit rule embeddings to provide soft, context-dependent inference.

The evolution of KGE has seen models progress significantly. Early translational models like TransE, while groundbreaking, faced limitations, such as conflicts between their core principle and regularization strategies, leading to warped embeddings \cite{ebisu2017}. This spurred advancements like TorusE \cite{ebisu2017}, which embeds entities and relations on a Lie group (a torus) to inherently bound embeddings and resolve regularization conflicts, improving accuracy and efficiency. More recent models, such as HolmE \cite{zheng2024}, have introduced novel Riemannian KGE frameworks designed to be "closed under composition," allowing for robust modeling of complex, under-represented relational patterns and providing a unifying theoretical framework for many existing models. Other models, like TransF \cite{do20184o2}, address scalability challenges by explicitly modeling basis subspaces for projection matrices, making them robust to KGs with thousands of relations. This continuous innovation aims to enhance expressiveness, capture diverse relational patterns (e.g., symmetry, anti-symmetry, hierarchy, composition), and improve the robustness and generalizability of embeddings. Ultimately, KGE transforms complex, discrete information into a machine-readable, statistically analyzable format, offering scalability and improved performance across a wide spectrum of AI applications by effectively bridging the gap between symbolic knowledge and neural reasoning.
\subsection{Scope and Organization of the Review}
\label{sec:1\_3\_scope\_\_and\_\_organization\_of\_the\_review}

This literature review offers a comprehensive and pedagogically structured exploration of Knowledge Graph Embedding (KGE) research, guiding the reader through its intellectual evolution from foundational models to advanced techniques, diverse applications, and emerging future directions. The review is meticulously organized to provide a clear understanding of the field's boundaries, objectives, and the overarching themes that have shaped its development, critically informed by an analysis of identified research communities and their evolutionary paths. Our approach emphasizes a progression from core theoretical underpinnings to practical implementations and future challenges, ensuring a coherent narrative for both newcomers and seasoned researchers.

The review begins in \textbf{Section 1: Introduction}, which establishes the foundational context for KGE. Following an overview of knowledge graphs and the motivation for embedding methods, this section delineates the scope and organization of the entire document, setting the stage for the subsequent detailed discussions.

\textbf{Section 2: Foundational KGE Paradigms} delves into the initial breakthroughs and core models that established the field. We trace the evolution from early geometric approaches, such as translational models like TransE, TransH \cite{wang2014}, and TransD \cite{ji2015}, which interpret relations as vector translations, to semantic matching models like RESCAL and ComplEx that leverage algebraic operations. The section culminates with rotational models, exemplified by RotatE \cite{sun2019rotate}, which generalize earlier geometric intuitions by modeling relations as rotations in complex vector spaces. This progression highlights the increasing sophistication in capturing diverse relational patterns, as comprehensively benchmarked and categorized by studies like \cite{ferrari2022r82}, which compare these foundational techniques across various datasets and evaluation criteria.

Building upon these foundations, \textbf{Section 3: Advanced Geometric and Non-Euclidean Embeddings} critically examines the evolution of embedding techniques beyond basic Euclidean spaces. This section explores models that leverage non-Euclidean geometries, such as hyperbolic spaces for hierarchical data \cite{pan2021hyperbolic}, Lie groups for efficient representation, and compound operations that combine multiple geometric transformations. It also covers spherical and quaternion embeddings, demonstrating how mathematically richer frameworks enhance model expressiveness for intricate graph structures and diverse semantics.

The discussion then transitions to \textbf{Section 4: Contextual and Graph Neural Network-based KGE}, focusing on models that enhance representations by incorporating richer contextual information and leveraging advanced neural network architectures. This includes integrating auxiliary information like textual descriptions, entity types \cite{he2023take}, and logical rules \cite{tang2022rule}. We then explore the application of convolutional and attention-based architectures \cite{dettmers2018conve}, culminating in a detailed examination of Graph Neural Networks (GNNs) and Transformers \cite{shi2025tgformer} for capturing multi-hop structural dependencies and enabling inductive learning.

Recognizing the dynamic nature of real-world knowledge, \textbf{Section 5: Temporal Knowledge Graph Embedding} is dedicated to methodologies that address the crucial challenge of modeling knowledge evolving over time. This section covers early approaches to temporal integration, such as HyTE \cite{dasgupta2018}, followed by rotation-based models like TeRo \cite{xu2020tero} and ChronoR \cite{sadeghian2021chronor}, and advanced techniques leveraging multi-curvature spaces and GNNs for evolving KGs. This specialized focus underscores the importance of capturing temporal dependencies for predictive tasks.

\textbf{Section 6: KGE for Downstream Applications and Practical Considerations} highlights the practical utility of KGEs by showcasing their impact across various downstream tasks. We detail their application in crucial areas such as entity alignment \cite{sun2018bootea, zhang2019multike}, recommendation systems \cite{sun2018rkge, liu2023cross}, and question answering \cite{huang2019keqa}. Furthermore, we broaden the scope to include semantic queries and data exploration \cite{tran2019j42} and discuss how KGEs serve as encodings for general data mining tasks, not just link prediction \cite{portisch20221rd}. This section also addresses critical practical aspects, including efficiency, scalability for large-scale KGs, and robustness in training.

Finally, \textbf{Section 7: Emerging Trends and Future Directions} looks ahead, identifying cutting-edge research avenues and unresolved challenges. This includes the move towards automated KGE model design and meta-learning \cite{sun2024metahg}, the integration of multimodal information, and the increasing demand for explainable and ethical AI in KGE \cite{yang2025seconv}. The section synthesizes theoretical gaps and practical hurdles, providing a roadmap for future research and development.

The review concludes in \textbf{Section 8: Conclusion}, which synthesizes the key advancements and intellectual trajectory of KGE research, reiterating the significant progress made and outlining persistent open research questions and promising future directions. This structured journey through the diverse and rapidly evolving landscape of knowledge graph embedding research ensures a clear understanding of the review's boundaries, objectives, and the overarching themes that guide the discussion, emphasizing the continuous drive towards more expressive, efficient, and context-aware knowledge representations.


\label{sec:foundational_kge_paradigms}

\section{Foundational KGE Paradigms}
\label{sec:foundational\_kge\_paradigms}

\subsection{Translational Models: From TransE to TransD}
\label{sec:2\_1\_translational\_models:\_from\_transe\_to\_transd}

The inherent incompleteness of knowledge graphs necessitates robust methods for link prediction and knowledge graph completion. The translational paradigm emerged as a foundational approach, modeling relations as transformations in a continuous embedding space, offering intuitive and computationally efficient initial solutions. These models interpret relations as operations that translate a head entity embedding to a tail entity embedding, aiming to satisfy the geometric principle $h + r \approx t$.

The pioneering work in this area was TransE \cite{bordes2013}, which posited that a relation $r$ acts as a simple translation from a head entity $h$ to a tail entity $t$ in a shared low-dimensional vector space. This model was groundbreaking for its simplicity and efficiency, establishing a strong baseline for knowledge graph embedding. However, TransE struggled significantly with complex relation types, particularly 1-to-N, N-to-1, and N-to-N relations \cite{asmara2023}. This limitation arose because TransE assigned a single, fixed vector representation to each entity, regardless of the specific relation it participated in. Consequently, for a 1-to-N relation (e.g., "capitalOf(Paris, France), capitalOf(Paris, Germany)"), the model would attempt to map "Paris" to both "France" and "Germany" via the same "capitalOf" vector. This forces "France" and "Germany" to occupy very similar locations in the embedding space, which is semantically inaccurate and severely limits discriminative power \cite{asmara2023}. Furthermore, TransE's common regularization strategy, which often involves projecting embeddings onto a unit sphere, was later identified to conflict with its core translation principle. This conflict could warp embeddings, adversely affecting link prediction accuracy and preventing the model from fully realizing its potential \cite{ebisu2017}.

To address TransE's shortcomings, subsequent models introduced more sophisticated mechanisms to handle relational diversity and improve expressiveness. TransH \cite{wang2014} advanced the paradigm by projecting entities onto relation-specific hyperplanes. Instead of a single entity vector, TransH allowed an entity to have different "distributed" representations (projections) depending on the specific relation it participated in. This innovation provided a more flexible way to model an entity's role in different contexts, thereby better accommodating 1-to-N, N-to-1, and N-to-N relations while maintaining computational efficiency comparable to TransE. By decoupling an entity's general representation from its relation-specific manifestation, TransH offered a significant step towards capturing the polysemy of entities in different relational contexts.

Building upon the concept of relation-specific representations, TransR and CTransR \cite{lin2015} further extended this idea by proposing the use of separate entity and relation spaces. In TransR, entities are first projected from their entity-specific space into a relation-specific space using a relation-specific projection matrix $M\_r$, before the translation operation is applied ($h M\_r + r \approx t M\_r$). This approach provided even greater expressiveness than TransH's hyperplane projections, as it allowed entities to be represented distinctly and transformed across various relation types, improving the modeling of complex semantic patterns. However, TransR introduced a significant increase in parameter count. Each relation required its own full projection matrix, leading to a parameter complexity of $O(k\_e \cdot k\_r)$ per relation (where $k\_e$ and $k\_r$ are entity and relation embedding dimensions). This posed scalability challenges for knowledge graphs with a large number of relations, as the computational cost and memory footprint grew substantially.

TransD \cite{ji2015} was then introduced to refine the translational paradigm, aiming to achieve the expressiveness of TransR while mitigating its parameter explosion. TransD employed dynamic mapping matrices, a more efficient approach to projection. Unlike TransR's fixed, full projection matrices, TransD introduced a dual-vector representation for each entity ($h, h\_p$) and relation ($r, r\_p$). One vector captured the intrinsic meaning (e.g., $h$), while the second ($h\_p$ or $r\_p$) dynamically constructed a mapping matrix for projection. Specifically, the projection matrix for a head entity $h$ given relation $r$ is constructed as $M\_{rh} = r\_p h\_p^T + I$, and similarly for the tail entity. This innovation significantly improved expressiveness by considering the diversity of \textit{both} entities and relations in a more adaptive and efficient manner. Crucially, by constructing projection matrices from lower-dimensional vectors, TransD dramatically reduced the parameter count compared to TransR (e.g., $O(k\_e + k\_r)$ per relation for the projection vectors, plus the entity/relation embeddings), offering a superior trade-off between expressivity and scalability while achieving enhanced performance. This concept of dynamic translation, where parameter vectors are introduced to support flexible translation, was also explored in models like TransE-DT and TransR-DT \cite{chang20179yf}, demonstrating consistent improvements by making the translation principle more adaptive.

These models collectively established the translational paradigm, offering initial and progressively refined solutions for link prediction and knowledge graph completion. While highly influential and forming the bedrock of KGE research, these early translational models, even with their advancements, primarily relied on predefined geometric operations in Euclidean space. They often struggled to fully capture highly complex or implicit semantic patterns, such as those involving relational composition beyond simple vector addition, or the nuanced interplay of diverse entity types and textual contexts. Their limitations, particularly in modeling intricate logical patterns and scaling efficiently to extremely large and diverse knowledge graphs, paved the way for more expressive models, including those that generalize geometric transformations (e.g., rotations) or leverage neural architectures, which are discussed in subsequent sections.
\subsection{Semantic Matching Models: RESCAL and ComplEx}
\label{sec:2\_2\_semantic\_matching\_models:\_rescal\_\_and\_\_complex}

Moving beyond the geometric translations of early knowledge graph embedding models, alternative paradigms emerged that focused on semantic matching and tensor factorization to capture richer, more direct interactions between entities and relations. This shift introduced different mathematical frameworks for scoring triple validity, enhancing expressiveness for specific relational patterns and demonstrating the power of algebraic operations in capturing nuanced semantic relationships.

A pioneering model in this direction was RESCAL (Relational Embedding of Simple Components for ALgebraic Link prediction) \cite{Nickel\_2016}. RESCAL introduced a tensor factorization approach, representing relations as full matrices that directly interact with entity vectors. For a given triple $(h, r, t)$, the model scores its validity using a bilinear form: $f(h, r, t) = \mathbf{h}^\top \mathbf{M}\_r \mathbf{t}$, where $\mathbf{h}$ and $\mathbf{t}$ are vector embeddings for the head and tail entities, respectively, and $\mathbf{M}\_r$ is a relation-specific matrix. This formulation allows for a richer, more direct semantic matching between entities through the relation, as the matrix $\mathbf{M}\_r$ can capture complex, non-linear interactions. While RESCAL offered significant expressive power, particularly for capturing diverse relational patterns, it faced substantial scalability challenges due to the large number of parameters required for each relation matrix, leading to high computational costs and memory consumption.

Building upon the principles of tensor factorization, ComplEx (Complex Embeddings for Simple Link Prediction) \cite{Trouillon\_2016} further advanced semantic matching by introducing complex-valued embeddings. In ComplEx, both entities and relations are embedded into a complex vector space, where each embedding $\mathbf{e} \in \mathbb{C}^d$ has a real part $\text{Re}(\mathbf{e})$ and an imaginary part $\text{Im}(\mathbf{e})$. The scoring function for a triple $(h, r, t)$ is defined using a Hermitian dot product: $f(h, r, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \bar{\mathbf{t}} \rangle)$, where $\bar{\mathbf{t}}$ denotes the complex conjugate of the tail entity embedding. This elegant mathematical framework naturally models symmetric and antisymmetric relations without requiring explicit constraints, a significant advantage over many translational models. For instance, if a relation is symmetric, its complex embedding can be purely real, and if it is antisymmetric, its embedding can be purely imaginary. ComplEx thus provided a more elegant and computationally efficient solution for capturing these specific relational patterns compared to the more parameter-heavy RESCAL, offering a better trade-off between expressiveness and efficiency for certain types of relations.

Both RESCAL and ComplEx represent a crucial intellectual contribution by moving beyond the strict geometric translations and demonstrating the power of algebraic operations and higher-dimensional representations in capturing nuanced semantic relationships. RESCAL laid the groundwork for tensor factorization in KGE, showcasing the potential of relation-specific matrices to model intricate interactions. ComplEx then refined this direction by leveraging complex numbers, offering a more compact and principled way to handle relational properties like symmetry and antisymmetry that are prevalent in knowledge graphs. While RESCAL's scalability remained a practical hurdle, ComplEx offered a more viable path for certain relational patterns, albeit potentially lacking the general expressiveness of more advanced neural models for highly diverse or compositional relation types. The exploration of these algebraic frameworks highlighted the diverse mathematical tools available for knowledge graph representation, paving the way for future innovations in capturing complex semantic structures.
\subsection{Rotational Models: Capturing Complex Relation Patterns}
\label{sec:2\_3\_rotational\_models:\_capturing\_complex\_relation\_patterns}

The accurate representation of diverse relational patterns within knowledge graphs (KGs) has been a persistent challenge for embedding models. While early translational models offered an efficient paradigm for learning entity and relation embeddings, they often struggled to simultaneously capture complex logical properties such as symmetry, anti-symmetry, inversion, and composition within a unified framework. Initial attempts to enhance the expressiveness of translational models, such as TransH \cite{wang2014}, addressed the limitations of TransE by projecting entities onto relation-specific hyperplanes. This allowed entities to have varying representations depending on the relation, thereby better handling one-to-many and many-to-one mapping properties. Further advancements, like TransD \cite{ji2015}, introduced dynamic mapping matrices for both entities and relations, aiming for a more fine-grained representation of diversity and improved efficiency by avoiding computationally intensive matrix-vector multiplications.

Despite these improvements in handling complex mapping properties, a single, elegant mechanism capable of unifying all fundamental relational patterns remained elusive. Semantic matching models, such as ComplEx \cite{trouillon2016}, which employed complex-valued embeddings and a Hermitian dot product, successfully modeled symmetric and anti-symmetric relations. However, ComplEx, like other semantic matching approaches, did not inherently capture compositional patterns with the same elegance or effectiveness. This gap highlighted the need for a more generalized geometric operation that could intrinsically model a broader spectrum of relational semantics.

A significant breakthrough in this area was the introduction of RotatE \cite{sun2018}, which represents a profound generalization of the translational embedding paradigm. RotatE elegantly interprets relations as element-wise rotations in a complex vector space, where the head entity embedding is rotated to align with the tail entity embedding. Specifically, for a valid triple $(h, r, t)$, RotatE models the relationship as $t = h \odot r$, where $h, r, t$ are complex-valued embeddings and $\odot$ denotes the Hadamard (element-wise) product, with the modulus of each element of $r$ constrained to $|r\_i|=1$. This geometric operation allows RotatE to inherently and simultaneously capture diverse relational patterns.

The power of RotatE lies in its ability to model complex logical properties within a unified framework. For instance, symmetry is captured when a relation $r$ corresponds to a rotation by an angle of $\pi$ (or $0$), meaning $r = r^{-1}$. Anti-symmetry is naturally handled when $r \neq r^{-1}$. Inversion is directly supported, as the inverse relation $r^{-1}$ is simply the conjugate of $r$ in complex space. Most notably, RotatE can infer composition patterns, where if $(h, r\_1, m)$ and $(m, r\_2, t)$ are true, then $(h, r\_1 \odot r\_2, t)$ is also likely true, with $r\_1 \odot r\_2$ representing the composite relation. This capability to infer composition was a critical advantage over models like ComplEx \cite{trouillon2016}, which struggled with such patterns.

RotatE's novel approach, coupled with a self-adversarial negative sampling strategy, demonstrated superior performance across various benchmark datasets, including those specifically designed to test compositionality, such as the "Countries" dataset \cite{sun2018}. Its ability to model complex logical properties more effectively than earlier translational or semantic matching approaches marked a significant contribution to improving KGE expressiveness. By offering a mathematically elegant and unified framework for diverse relational patterns, RotatE set a new standard for capturing intricate relational semantics, paving the way for more robust and logically consistent knowledge graph reasoning. The success of rotational models underscores the potential of leveraging richer geometric transformations in higher-dimensional spaces to unlock deeper semantic understanding within knowledge graphs.


\label{sec:advanced_geometric_and_non-euclidean_embeddings}

\section{Advanced Geometric and Non-Euclidean Embeddings}
\label{sec:advanced\_geometric\_\_and\_\_non-euclidean\_embeddings}

\subsection{Hyperbolic Embeddings for Hierarchical Structures}
\label{sec:3\_1\_hyperbolic\_embeddings\_for\_hierarchical\_structures}

Knowledge graphs frequently exhibit intricate hierarchical or tree-like structures, which traditional Euclidean embedding spaces struggle to represent efficiently due to their inherent uniform curvature and limited capacity for exponential growth. Hyperbolic geometry, with its natural negative curvature and exponentially increasing volume, offers a compelling alternative for embedding such hierarchical data, allowing for more faithful and compact representations.

Early explorations into hyperbolic Knowledge Graph Embedding (KGE) models demonstrated their potential for capturing these complex hierarchies. \textcite{pan2021} introduced a novel KGE model that leverages an extended Poincaré Ball and a polar coordinate system within hyperbolic space. This approach specifically addressed the challenge of simultaneously modeling hierarchical structures and logical patterns by employing tangent space and exponential transformations for mapping vectors into the extended Poincaré Ball, along with a unique method for handling boundary conditions by expanding the modulus length. Their model achieved state-of-the-art results on certain link prediction tasks, showcasing the expressiveness of hyperbolic spaces for hierarchical knowledge. However, many initial hyperbolic KGE models, including those preceding \textcite{pan2021}, often relied on frequent and computationally intensive mappings between hyperbolic and tangent spaces during training. These "hybrid" approaches, while conceptually sound, introduced overhead and potential numerical instability due to the complex logarithmic and exponential functions involved.

Addressing these limitations, \textcite{liang2024} proposed the Fully Hyperbolic Rotation model (FHRE), which innovates by performing operations directly and entirely within hyperbolic space, specifically utilizing the Lorentz model. Unlike its predecessors, FHRE eliminates the need for iterative logarithmic and exponential mappings during training, instead performing a single mapping at initialization. In FHRE, relations are conceptualized as Lorentz rotations that transform head entity embeddings to tail entity embeddings, with triplet plausibility measured by a Lorentzian distance-based scoring function. This "fully hyperbolic" paradigm enhances both expressiveness and efficiency by fully leveraging the intrinsic properties of hyperbolic geometry, leading to improved stability and reduced computational burden. Experimental validation demonstrated that FHRE achieves state-of-the-art performance on challenging benchmarks, particularly those with diverse and complex relational patterns, by capturing fine-grained relational semantics more effectively than previous Euclidean, complex, and hybrid hyperbolic models.

The theoretical underpinnings of such geometric approaches are further explored in broader contexts. For instance, \textcite{zheng2024} introduced HolmE, a general Riemannian KGE framework that discusses the property of being "closed under composition" and leverages Riemannian geometry, including hyperbolic space, for efficient computation through extensions of Möbius addition. While HolmE provides a unifying perspective for models like TransE and RotatE, its direct application to the specific challenges of hierarchical representation in hyperbolic space is more about providing a theoretical umbrella rather than direct methodological innovation in hyperbolic geometry itself. Nonetheless, it underscores the growing recognition of non-Euclidean geometries in KGE.

In conclusion, hyperbolic embeddings have emerged as a powerful paradigm for representing hierarchical structures in knowledge graphs, offering a natural fit that Euclidean spaces lack. Models like \textcite{pan2021} and \textcite{liang2024} have progressively refined this approach, moving from hybrid mapping-based methods to fully hyperbolic operations, thereby enhancing expressiveness, efficiency, and stability. Despite these advancements, challenges remain in scaling these models to extremely large and dynamic knowledge graphs, integrating more diverse and complex relation types beyond simple hierarchies, and developing more interpretable hyperbolic operations. Future research could explore adaptive hyperbolic curvature, dynamic hyperbolic embeddings for evolving KGs, and novel ways to combine hyperbolic geometry with advanced neural architectures for even richer and more robust knowledge representation.
\subsection{Lie Group and Compound Operations for Enhanced Expressiveness}
\label{sec:3\_2\_lie\_group\_\_and\_\_compound\_operations\_for\_enhanced\_expressiveness}

The quest for more expressive knowledge graph embedding (KGE) models has propelled research beyond simple Euclidean translations, leading to the exploration of advanced geometric and algebraic structures. This subsection examines models that leverage Lie groups and sophisticated compound geometric operations, offering a richer framework for representing entities and relations by capturing intricate relational semantics and diverse mapping properties.

An early conceptual step towards more flexible geometric representations was \textcite{xiao2015}'s \textbf{ManifoldE}, which moved beyond the "one-point" mapping principle of models like TransE. ManifoldE proposed embedding triples onto relation-specific manifolds (e.g., spheres or hyperplanes) rather than single points. This manifold-based approach provided greater flexibility for complex relations (e.g., one-to-many, many-to-many) and addressed the ill-posed algebraic system inherent in point-wise models, paving the way for more sophisticated geometric interpretations.

Building on this idea of structured geometric spaces, \textcite{ebisu2017} introduced \textbf{TorusE}, a pioneering model that embeds knowledge graphs directly on a Lie group, specifically a torus. This innovative approach ingeniously resolves a fundamental regularization conflict prevalent in earlier translational models. In Euclidean space, forcing embeddings onto a unit sphere (a common regularization technique) can distort learned representations, especially for relations modeled as translations. By embedding on a compact Abelian Lie group like the torus, TorusE inherently bounds the embeddings, eliminating the need for explicit regularization and leading to more accurate and efficient learning. This geometric shift offered a novel perspective, demonstrating the benefits of leveraging the inherent properties of Lie groups for KGE.

Further enhancing geometric transformations, subsequent research focused on combining multiple operations to achieve greater expressiveness. \textcite{ge2022} introduced \textbf{CompoundE}, a model that significantly generalizes geometric transformations by combining translation, rotation, and scaling operations in a cascaded, relation-specific manner. Unlike previous models often restricted to a single type of operation, CompoundE frames these compound operations within the affine group. This allows relations to not only shift and turn entity embeddings but also expand or contract them, providing a more versatile and powerful approach for capturing diverse relation patterns and mapping properties, such as symmetry, anti-symmetry, inversion, and various N-to-N relationships. As highlighted by the survey \textcite{ge2023}, CompoundE mathematically demonstrates its ability to encompass and generalize several existing distance-based KGE models, including TransE and RotatE, thereby offering a unifying perspective on affine operation-based techniques.

Extending the capabilities of high-dimensional geometric transformations, \textcite{li2022} proposed \textbf{HousE}, which utilizes Householder parameterization for high-dimensional rotations and invertible projections. While earlier rotation-based models like RotatE \cite{sun2019} were effective for capturing symmetric and anti-symmetric patterns, their distance-preserving nature limited their ability to model complex relation mapping properties (RMPs) such as one-to-many or many-to-many. HousE addresses this by representing relations as compositions of Householder reflections for arbitrary high-dimensional rotations (generalizing RotatE to k-dimensions) and introduces novel invertible Householder projections that can flexibly adjust relative distances. This unified framework allows HousE to simultaneously model all crucial relation patterns and RMPs, offering superior expressiveness and generalizing previous rotation-based models.

Building upon the Householder parameterization, \textcite{li2024} introduced \textbf{GoldE} (Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization). GoldE presents a framework that generalizes existing KGE approaches in \textit{both dimension and geometry} for orthogonal relation transformations. It achieves this through a universal orthogonal parameterization based on a generalized Householder reflection, capable of operating in arbitrary dimensions and across Euclidean, elliptic, and hyperbolic geometries. For this subsection's focus, GoldE's elliptic orthogonal parameterization is particularly relevant, as it generalizes Euclidean models by incorporating relation-specific scaling operations, effectively extending the "compound operations" paradigm by allowing adaptive adjustments to entity embeddings within a generalized orthogonal transformation framework. This represents a significant step towards breaking the limitations of rigid, homogeneous geometric spaces.

The progression towards these more complex, structured operations highlights a broader search for unifying mathematical principles in KGE. This goal was later formalized by \textcite{zheng2024} with \textbf{HolmE}, which provides a unifying Riemannian framework for KGE models. HolmE introduces the crucial property of being "closed under composition," ensuring that the composition of any two relation embeddings remains within the embedding space. The authors theoretically prove that prominent KGE models like TransE and RotatE are special cases of HolmE, providing a deeper theoretical understanding of their underlying geometric properties and demonstrating how these advanced geometric operations contribute to a more robust and theoretically sound representation of relational composition.

In summary, models leveraging Lie groups and compound operations, from the regularization-resolving TorusE to the generalized affine transformations of CompoundE, the high-dimensional Householder parameterizations of HousE and GoldE, and the unifying Riemannian framework of HolmE, represent a significant advancement in KGE. They move beyond the limitations of simpler geometric operations by leveraging the inherent properties of advanced algebraic and geometric structures. This allows them to capture intricate relational semantics, including diverse relation patterns and complex mapping properties, with enhanced expressiveness. While these approaches offer superior modeling capacity, a continuing challenge lies in balancing their increased mathematical complexity and computational demands with interpretability and scalability for real-world, large-scale knowledge graphs. Future research may focus on optimizing these complex operations and integrating them with neural architectures for even more robust and adaptable KGE solutions.
\subsection{Spherical and Quaternion Embeddings for Diverse Semantics}
\label{sec:3\_3\_spherical\_\_and\_\_quaternion\_embeddings\_for\_diverse\_semantics}

Traditional Knowledge Graph Embedding (KGE) models often represent entities as single points in Euclidean space, a simplification that can limit their capacity to capture the rich, multi-faceted semantic properties inherent in real-world knowledge graphs. This limitation becomes particularly apparent when modeling complex relations, entity extents, or multi-dimensional semantic relationships. To overcome these challenges, recent research has explored alternative, more expressive embedding spaces, notably spherical and quaternion representations, which leverage distinct mathematical properties to enhance KGE expressiveness and applicability to specialized tasks \cite{cao2022}.

The evolution of KGE models has seen a progression from simple vector operations to more sophisticated geometric and algebraic structures. Building upon the success of rotational models like RotatE \cite{sun2019}, which interprets relations as 2D rotations in complex vector space, quaternion embeddings offer a natural extension to higher dimensions. Quaternions, a 4D number system, provide a powerful algebraic framework for representing rotations in 3D and 4D space. The seminal work in this area is \textbf{QuatE} \cite{zhang2019quat}, which extends the concept of complex-valued embeddings by representing entities and relations as quaternions. QuatE models relations as rotations using the Hamilton product, allowing it to capture a wider array of relational patterns, including symmetry, anti-symmetry, inversion, and composition, within a unified 4D space. This higher-dimensional algebraic structure inherently provides greater expressiveness for intricate relational semantics compared to its 2D complex-valued counterparts.

Further advancements in quaternion embeddings demonstrate their versatility for diverse semantic challenges. \textbf{ConQuatE} \cite{chen2025} leverages quaternion rotations to address the critical "polysemy issue," where entities exhibit different semantic characteristics depending on their relational contexts. By efficiently incorporating contextual cues from various connected relations through quaternion transformations, ConQuatE enriches entity representations across multiple semantic dimensions without requiring auxiliary information, thereby improving link prediction performance. Similarly, for dynamic knowledge, \textbf{TimeLine-Traced KGE (TLT-KGE)} \cite{zhang2022muu} utilizes quaternion vectors to embed entities and relations with timestamps. TLT-KGE innovatively models semantic and temporal information as distinct axes within the quaternion space, enabling it to distinguish and connect these independent yet related aspects of temporal facts. This approach effectively addresses the challenge of representing evolving knowledge, outperforming state-of-the-art competitors in temporal knowledge graph completion. The broader utility of quaternion algebra in KGE is also highlighted by analyses that propose new multi-embedding models based on this framework \cite{tran20195x3}.

In a conceptually distinct but equally impactful direction, spherical embeddings move beyond point-based representations to model entities as geometric volumes. \textbf{SpherE} \cite{li2024} proposes embedding entities not as single points, but as \textit{spheres} in Euclidean space, each defined by a center vector and a radius, while relations are modeled as rotations. This novel representation allows SpherE to explicitly capture entity \textit{extents} and \textit{overlaps}, which is crucial for tasks like knowledge graph set retrieval where precise sets of answers are required. By representing entities with a "spread" rather than a singular location, SpherE robustly models one-to-many, many-to-one, and especially many-to-many relations, a long-standing challenge for traditional point-based models. The interpretability of the sphere's radius, which correlates with an entity's universality, further enhances its utility. Theoretically, SpherE demonstrates high expressiveness for various relation patterns and mapping properties by checking sphere intersection after rotation, providing a powerful mechanism to understand how entities relate in terms of their scope and potential intersections.

These advancements in spherical and quaternion embedding spaces represent a significant leap in Knowledge Graph Embedding expressiveness, moving beyond the inherent limitations of traditional vector representations. Quaternion embeddings, by extending rotational models to 4D algebraic structures, offer enhanced capabilities for capturing complex relational patterns and contextual semantics, particularly useful for polysemy and temporal dynamics. Spherical embeddings, on the other hand, introduce volumetric representations that explicitly model entity extents and overlaps, proving highly effective for set-based retrieval and intricate many-to-many relationships. As highlighted by \cite{cao2022}, the choice of representation space profoundly influences the types of KG properties that can be effectively modeled. By leveraging the unique algebraic properties of quaternions and the geometric advantages of spheres, these models provide novel and powerful ways to capture complex entity and relation properties, pushing the boundaries of KGE expressiveness and expanding their applicability to specialized tasks in diverse real-world applications. Future research may explore the integration of these advanced geometric and algebraic spaces with neural architectures, or develop adaptive strategies for selecting optimal embedding spaces based on specific KG characteristics and task requirements.


\label{sec:contextual_and_graph_neural_network-based_kge}

\section{Contextual and Graph Neural Network-based KGE}
\label{sec:contextual\_\_and\_\_graph\_neural\_network-based\_kge}

\subsection{Integrating Auxiliary Information: Text, Types, and Rules}
\label{sec:4\_1\_integrating\_auxiliary\_information:\_text,\_types,\_\_and\_\_rules}

Knowledge graph embedding (KGE) models traditionally focus on learning representations solely from observed (head, relation, tail) triples. However, real-world knowledge graphs are often incomplete and noisy, necessitating the integration of auxiliary information to enrich embeddings, improve semantic consistency, and enhance predictive performance. This section reviews KGE models that leverage diverse external data sources, including textual descriptions, semantic categories, explicit or implicit entity types, and logical rules.

Early efforts to incorporate textual information aimed to bridge the gap between symbolic triples and natural language semantics. \cite{xiao2016} proposed Semantic Space Projection (SSP), a model that jointly learns from triples and textual descriptions by projecting the triple's loss vector onto a semantic hyperplane. This approach models strong correlations between texts and triples, guiding the embedding topology. Building on this, \cite{shen2022} introduced LASS (Joint Language Semantic and Structure Embedding), which fine-tunes pre-trained language models with a probabilistic structured loss to simultaneously capture semantics from textual descriptions and reconstruct KG structures. LASS addresses the limitations of earlier text-aware models by providing a unified framework for deeper integration of language semantics, demonstrating superior performance, especially in low-resource settings.

Beyond free-form text, structured semantic information like entity categories and types offers valuable context. \cite{guo2015} presented Semantically Smooth Embedding (SSE), which enforces a "semantically smooth" embedding space by constraining entities belonging to the same semantic category to lie close to each other, using manifold learning algorithms like Laplacian Eigenmaps as regularization. Taking this a step further, \cite{lv2018} proposed TransC (Translating Concepts), which fundamentally differentiates concepts (represented as spheres) from instances (represented as vectors) in the embedding space. This geometric modeling inherently preserves the transitivity of \texttt{instanceOf} and \texttt{subClassOf} relations, a crucial property often overlooked by models treating all entities uniformly. More recently, \cite{wang2021} introduced TransET, a model that leverages explicit entity types by employing circle convolution based on entity and type embeddings to generate type-specific representations, thereby learning more semantic features. Addressing the common challenge of incomplete or unavailable explicit type information, \cite{he2023} developed TaKE (Type-augmented Knowledge graph Embedding), a model-agnostic framework that automatically captures \textit{implicit} type features. TaKE further models the diversity of entity types using a relation-specific hyperplane mechanism and introduces a novel type-constrained negative sampling strategy, making it highly flexible and effective without requiring explicit type supervision.

Another powerful form of auxiliary information comes from logical rules, which encode explicit knowledge and constraints. \cite{guo2017} proposed RUGE (RUle-Guided Embedding), an iterative paradigm that guides embedding learning with automatically extracted soft rules. RUGE addresses the limitation of one-time rule injection by alternating between soft label prediction for unlabeled triples and embedding rectification, maximizing the utility of uncertain logical knowledge. Expanding on the integration of soft rules, \cite{guo2020} introduced a scalable method for Soft Logical Rule Embedding (SLRE), which represents relations as bilinear forms and entities in a non-negative bounded space. Their key innovation is a novel rule-based regularization that directly enforces relation representations to satisfy soft rule constraints, making its complexity independent of the entity set size and significantly improving scalability. More comprehensively, \cite{tang2022} presented RulE (Rule Embedding), a neural-symbolic framework that learns explicit \textit{rule embeddings} and jointly represents entities, relations, and logical rules in a unified continuous space. RulE calculates confidence scores for rules and employs a soft rule reasoning mechanism, effectively mitigating the brittleness of traditional logical inference and allowing for mutual regularization between KGE and rule-based components.

In conclusion, the integration of auxiliary information, whether from textual descriptions, semantic categories, entity types, or logical rules, has profoundly enhanced KGE models. This progression reflects a shift from purely structural embeddings to richer, context-aware representations that capture more nuanced semantic relationships and logical consistencies. While significant strides have been made in leveraging these diverse data sources, future research could explore more sophisticated fusion mechanisms for multi-modal auxiliary information, investigate methods for automatically discovering and validating rules from noisy data, and develop frameworks that offer greater interpretability of how auxiliary knowledge influences learned representations.
\subsection{Convolutional and Attention-based Architectures}
\label{sec:4\_2\_convolutional\_\_and\_\_attention-based\_architectures}

The evolution of Knowledge Graph Embedding (KGE) has seen a significant shift from models relying on fixed scoring functions to sophisticated neural architectures that enable data-driven feature learning, particularly through Convolutional Neural Networks (CNNs) and attention mechanisms. These architectures excel at capturing complex interactions and relational patterns, leading to more expressive and contextually aware embeddings by moving beyond simplistic, fixed scoring functions. CNNs primarily focus on extracting local, non-linear features from structured input, while attention mechanisms dynamically weigh and aggregate information, proving particularly effective for the irregular structures of knowledge graphs.

Initial convolutional approaches, such as ConvE \cite{dettmers2018}, marked a departure from simpler models by applying convolutional filters to concatenated entity and relation embeddings. This allowed for the learning of rich, non-linear feature maps, significantly improving the expressiveness for triplet plausibility prediction. ConvE's strength lies in its parameter efficiency and ability to capture intricate interaction patterns, but its fixed filters can struggle with the diverse and often complex relational patterns (e.g., 1-to-N, N-to-1, N-to-N) inherent in KGs, as it treats all relations uniformly. Building on this foundation, the Multi-Scale Dynamic Convolutional Network (M-DCN) \cite{zhang2020} addressed these limitations by introducing dynamic, multi-scale convolutional filters whose weights are specifically tailored to each relation. This innovation enabled more nuanced feature extraction and better handling of complex relation patterns compared to ConvE's static filters. Further advancing the integration of deep learning, ReInceptionE \cite{xie2020} employed an Inception network to deepen the interaction learning between head and relation embeddings, enhancing the model's capacity to capture complex features. Crucially, ReInceptionE also incorporated a relation-aware attention mechanism to integrate both local neighborhood and global entity structural information, demonstrating an early synergy between CNNs and attention for comprehensive structural awareness. Another hybrid approach, PConvKB \cite{jia20207dd}, improved upon ConvKB (a CNN-based model) by explicitly incorporating relation paths. It utilized an attention mechanism to measure the local importance of these paths and a global measure (DIPF) for their overall significance, thereby enriching the convolutional features with path-level context. In specialized domains, SEConv \cite{yang2025} for healthcare prediction demonstrated the synergy of a resource-efficient self-attention mechanism with a multi-layer CNN to learn deeper and more expressive structural features, highlighting the adaptability of these architectures to specific application needs and resource constraints. While CNNs are effective for local feature extraction and can be parameter-efficient, their inherent grid-like operations can be less intuitive for the irregular, graph-structured data of KGs, and they may struggle with modeling long-range dependencies without significant architectural depth.

To overcome the limitations of fixed feature extraction and uniform information aggregation, attention mechanisms emerged to dynamically weigh and aggregate information, particularly from an entity's neighborhood, to create more contextually rich embeddings. Pioneering inductive KGE, the Logic Attention Network (LAN) \cite{wang2018} employed a double-view attention mechanism that combined logical rule-based weighting with neural network attention. This innovative approach allowed for permutation-invariant, redundancy-aware, and query-relation-aware aggregation of neighborhood information, which is crucial for generalizing to unseen entities by focusing on relevant facts. Similarly, Graph Attenuated Attention networks (GAATs) \cite{wang2020} introduced an attenuated attention mechanism to assign varying importance to different relation paths and actively acquire information from neighboring nodes. This mechanism directly addressed the uniform weighting limitations of earlier graph-based models, allowing for a more fine-grained understanding of relational semantics by emphasizing more informative paths and neighbors. These attention-based models offer superior flexibility in capturing the varying importance of different parts of a knowledge graph, leading to more adaptive and context-sensitive embeddings. However, their computational cost can be higher than simpler models, and the interpretability of complex attention patterns remains a challenge.

In summary, convolutional and attention-based architectures have significantly advanced KGE by moving beyond fixed scoring functions to data-driven feature learning. CNNs like ConvE, M-DCN, ReInceptionE, PConvKB, and SEConv have enabled the extraction of rich, non-linear, and dynamic features from entity-relation interactions. Concurrently, attention mechanisms in models like LAN and GAATs have empowered KGE models to dynamically aggregate neighborhood information and capture complex relational patterns with varying importance. While these architectures offer superior expressiveness and contextual awareness, challenges remain in balancing their increased computational complexity with scalability for extremely large KGs, and in enhancing the interpretability of their learned features. The principles of dynamic information aggregation via attention, particularly on graph structures, laid crucial groundwork for the more formalized message-passing frameworks of Graph Neural Networks and Transformers, which are explored in the subsequent section for their ability to capture multi-hop structural dependencies more comprehensively.
\subsection{Graph Neural Networks and Transformers for Structural Learning}
\label{sec:4\_3\_graph\_neural\_networks\_\_and\_\_transformers\_for\_structural\_learning}

The inherent limitations of traditional Knowledge Graph Embedding (KGE) models, particularly their transductive nature and struggle to capture rich, multi-hop structural information, have driven the integration of Graph Neural Networks (GNNs) and Transformer architectures. These advanced neural models explicitly leverage graph topology and complex relational paths, providing a powerful framework for learning context-rich embeddings, enabling inductive learning for unseen entities and relations, and significantly enhancing generalization capabilities.

The paradigm shift towards GNNs in KGE began with models that adapted message-passing mechanisms to the relational nature of knowledge graphs. A seminal work, R-GCN \cite{schlichtkrull2018modeling}, introduced relation-specific transformations within a standard GCN framework. Each relation type was assigned a distinct weight matrix, allowing entities to aggregate information from their neighbors based on the type of incoming and outgoing edges. This approach effectively captured local graph structure and multi-hop dependencies through iterative message passing, where an entity's embedding is updated by aggregating transformed embeddings of its neighbors. However, R-GCN faced challenges with parameter explosion for KGs with many relation types and struggled to model complex relational patterns beyond simple aggregation.

To address the parameter efficiency and enhance expressiveness, CompGCN \cite{vashishth2020compositional} proposed a compositional approach. It unified entity and relation embeddings into a single framework, using composition operations (e.g., subtraction, multiplication) to define how relation embeddings transform entity messages during aggregation. This allowed for a more compact representation of relations and improved the modeling of inverse and symmetric relation patterns. Building on the GNN message-passing paradigm, GAATs \cite{wang2020} further refined neighbor aggregation by introducing an attenuated attention mechanism. Unlike earlier GCNs that often assigned uniform weights, GAATs dynamically weighted different relation paths and actively acquired information from neighboring nodes, leading to more nuanced and comprehensive entity and relation representations by capturing the varying importance of different structural contexts.

A critical challenge for KGE is inductive learning, where models must generalize to entities or even entire knowledge graphs unseen during training. SE-GNN \cite{li2021} advanced this by explicitly modeling "Semantic Evidence" at relation, entity, and triple levels through distinct neighbor aggregation patterns within a GNN. This provided crucial insights into \textit{why} KGE models generalize, by demonstrating how different types of semantic evidence contribute to the inductive capacity. Taking generalization a step further, MorsE \cite{chen2021} introduced a meta-learning framework. MorsE learns "meta-knowledge" (transferable structural patterns) using a GNN modulator, enabling it to produce general entity embeddings for entirely unseen entities in new KGs. This represents a higher level of generalization compared to models that primarily focus on inductive relation prediction, as MorsE can adapt to novel graph structures.

The increasing complexity and heterogeneity of KGs have also spurred the development of more adaptive GNN architectures. MGTCA \cite{shang2024} introduced a novel approach by generating richer neighbor messages through the integration of spatial information from hyperbolic, hypersphere, and Euclidean spaces. It further incorporated a trainable convolutional attention network that autonomously switches between different GNN types (GCN, GAT, and a novel KGCAT) to adaptively capture diverse local structural patterns, overcoming the data dependence of single-GNN models. This highlights a trend towards multi-geometric and adaptive GNN designs. Complementing this, research into automated GNN design, such as the message function search proposed by \cite{di2023}, allows for the discovery of data-dependent message functions. This approach builds a flexible search space for GNN message functions, enabling the system to automatically find optimal structures and operators that adapt to diverse KG forms (e.g., traditional KGs, n-ary relational data, hyper-relational KGs), thereby enhancing adaptability and performance.

Deploying GNNs on large-scale KGs presents significant computational and memory challenges. CPa-WAC \cite{modak2024} addressed this by developing a Constellation Partitioning-based Scalable Weighted Aggregation Composition framework. It employs a novel constellation partitioning algorithm to divide KGs into topological clusters and uses a global decoder to merge embeddings from independently trained partitions, significantly reducing training time and memory costs while maintaining prediction accuracy. Other efforts in scaling GNNs for link prediction include algorithmic strategies like self-sufficient partitions, constraint-based negative sampling, and edge mini-batch training, which have demonstrated substantial speedups while preserving performance \cite{sheikh202245c}. Furthermore, empirical studies on parallel training techniques for KGE models, such as those by \cite{kochsiek2021}, have shown that while many existing methods negatively impact embedding quality, careful choices like variations of stratification and suitable random partitioning can enable efficient and effective large-scale training. For dynamic KGs, MetaHG \cite{sun2024} proposed a meta-learning strategy that efficiently updates incremental knowledge by integrating both local and potential global structural information through a hybrid GNN and Hypergraph Neural Network (HGNN) framework.

More recently, the power of Transformer architectures, renowned for their ability to capture long-range dependencies, has been harnessed for structural learning in KGE. TGformer \cite{shi2025} introduced a novel Graph Transformer Framework for KGE. It constructs context-level subgraphs for predicted triplets and employs a Knowledge Graph Transformer Network (KGTN) to comprehensively explore multi-structural features (both triplet-level and graph-level) and contextual information. By leveraging self-attention mechanisms, TGformer captures global and local structural features by allowing each node to attend to all other nodes within its contextual subgraph, enabling state-of-the-art link prediction through explicit modeling of complex relational paths. In a domain-specific application, SEConv \cite{yang2025} proposed a model for healthcare prediction that combines a resource-efficient self-attention mechanism with a multilayer Convolutional Neural Network (CNN) to learn deeper and more expressive structural features from medical KGs, illustrating hybrid approaches that merge the strengths of different architectures.

In conclusion, the integration of GNNs and Transformers marks a significant paradigm shift in KGE, moving from local, transductive approaches to models capable of inductive learning, multi-hop reasoning, and capturing rich contextual and structural information. While these advanced architectures offer superior expressiveness and generalization, challenges remain in optimizing their computational efficiency for extremely large and sparse KGs, ensuring interpretability of complex attention mechanisms, and effectively fusing diverse structural signals from multi-modal or multi-geometric spaces. Future research will likely focus on developing more scalable Graph Transformers, exploring novel attention mechanisms tailored for heterogeneous KGs, and designing hybrid models that combine the strengths of various architectures for robust and efficient structural learning, potentially through automated design processes.


\label{sec:temporal_knowledge_graph_embedding}

\section{Temporal Knowledge Graph Embedding}
\label{sec:temporal\_knowledge\_graph\_embedding}

\subsection{Early Approaches to Temporal Integration}
\label{sec:5\_1\_early\_approaches\_to\_temporal\_integration}

The dynamic nature of real-world knowledge necessitates that Knowledge Graph Embedding (KGE) models move beyond static representations to explicitly incorporate temporal information. Early research in temporal integration laid the foundational groundwork, exploring diverse methodologies to embed the temporal dimension directly into knowledge graphs, thereby enabling time-aware reasoning and prediction.

One of the pioneering geometric approaches was HyTE \cite{dasgupta2018}, which introduced a novel method for embedding temporal knowledge by associating each timestamp with a distinct hyperplane in the embedding space. This allowed HyTE to perform temporally-guided inference and predict the temporal scopes for relational facts, a crucial capability for incomplete knowledge graphs where temporal validity might be missing. In a different vein, tensor decomposition methods emerged as a powerful paradigm for inherently capturing time within KGEs \cite{lin2020}. This approach represented facts as higher-order tensors, explicitly including the time dimension alongside entities and relations, offering a generalizable framework for modeling dynamic knowledge. While these models provided initial explicit temporal handling, they often treated temporal evolution deterministically.

A significant departure from deterministic temporal modeling was introduced by ATiSE (Additive Time Series Embedding) \cite{xu2019}. ATiSE innovated by modeling the evolution of each entity and relation representation as a multi-dimensional additive time series, composed of trend, seasonal, and random components. Crucially, it represented entities and relations as multi-dimensional Gaussian distributions at each time step, explicitly accounting for temporal uncertainty during their evolution, a feature largely overlooked by prior models. This statistical perspective provided a more nuanced understanding of how knowledge changes over time.

More recently, TeAST (Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline) \cite{li2023} presented a novel structural mapping for time. TeAST introduced the concept of an Archimedean spiral timeline for relations, which ensures that relations occurring simultaneously are placed on the same timeline and that all relations evolve in a structured manner. This approach transformed the quadruple completion problem into a 3rd-order tensor completion task, specifically designed to avoid direct entity evolution and enhance interpretability, building upon the idea of structured temporal representation with a creative geometric twist.

These early models collectively established diverse foundational methodologies for integrating temporal information into KGEs, ranging from geometric interpretations and tensor-based structural embeddings to time series analysis for uncertainty and novel timeline mappings. While they successfully moved beyond static representations, a shared limitation across some of these initial approaches, particularly the earlier ones, was their potential struggle with highly complex, non-linear temporal patterns or the scalability challenges associated with explicit temporal indexing or extensive tensor operations. These challenges highlighted the need for more sophisticated and efficient temporal modeling techniques, paving the way for subsequent advancements in the field.
\subsection{Rotation-based Models for Temporal Dynamics}
\label{sec:5\_2\_rotation-based\_models\_for\_temporal\_dynamics}

Building on the success of rotation-based embedding models like RotatE in capturing diverse relation patterns within static knowledge graphs, a significant line of research has emerged that leverages rotations in complex or k-dimensional spaces to elegantly model temporal evolution. These models offer a highly expressive framework for representing time-dependent changes and their intricate interactions.

A foundational contribution in this area is \textcite{xu2020} TeRo (Temporal Rotation), which introduced the concept of temporal rotation in a complex vector space to model entity evolution. TeRo represents time-specific entity embeddings as element-wise rotations of their time-independent counterparts, allowing it to effectively capture diverse relation patterns, including temporary, asymmetric, and reflexive relations, and robustly handle various time annotations such as discrete time points and continuous time intervals through dual relation embeddings.

Building upon TeRo's pioneering work, \textcite{sadeghian2021} ChronoR (Chronological Rotation) generalized the rotation mechanism to k-dimensional spaces, offering a more flexible and powerful framework for temporal knowledge graph embedding. ChronoR proposed an inner product scoring function, which it theoretically demonstrated to generalize complex-domain models like ComplEx, thereby offering a robust alternative to Euclidean distance in high-dimensional spaces. Furthermore, ChronoR introduced advanced regularization techniques, including a tensor nuclear norm-inspired regularization and a 4-norm based temporal smoothness objective, to encourage consistent transformations for chronologically closer timestamps, enhancing model generalizability and capturing the smooth evolution of entities over time.

More recently, \textcite{ji2024} FSTRE (Fuzzy Spatiotemporal RDF Embedding) further extended this rotation-based paradigm by integrating fuzziness and spatial information alongside temporal rotation within a complex vector space. FSTRE addresses the challenge of modeling uncertain and dynamic knowledge by uniquely employing projection for spatial embedding and rotation for temporal embedding, while incorporating fine-grained fuzziness through the modal length of anisotropic vectors. This comprehensive approach demonstrates the versatility of geometric operations in complex space to represent multifaceted dynamics beyond just temporal evolution.

Collectively, these rotation-based models showcase high expressiveness for diverse relation patterns and temporal dynamics, providing an elegant and theoretically grounded method to capture time-dependent changes and their interactions within knowledge graphs. While their inherent complexity can increase with higher dimensions or the integration of additional factors like fuzziness and spatial information, they represent a powerful advancement in modeling the evolving nature of real-world knowledge. Future research could explore more adaptive rotation mechanisms, investigate their applicability to forecasting tasks, or integrate them with other advanced temporal modeling techniques to handle even more intricate forms of uncertainty and continuous time.
\subsection{Multi-Curvature and GNNs for Evolving KGs}
\label{sec:5\_3\_multi-curvature\_\_and\_\_gnns\_for\_evolving\_kgs}

The inherent geometric complexity and intricate interactions within Temporal Knowledge Graphs (TKGs) pose significant challenges for traditional embedding methods. To address these limitations, recent advancements have explored sophisticated geometric spaces and Graph Neural Network (GNN) architectures, offering enhanced flexibility and expressiveness for dynamic knowledge representation. These cutting-edge approaches aim to capture the diverse underlying structures and complex temporal dependencies that characterize evolving KGs.

One prominent direction involves modeling TKGs in multi-curvature spaces, moving beyond the constraints of a single Euclidean space. \textcite{wang2024} introduced MADE (Multicurvature Adaptive Embedding), a novel model for Temporal Knowledge Graph Completion (TKGC) that embeds TKGs into a combination of Euclidean, hyperbolic, and hyperspherical spaces. MADE leverages a data-driven weighting mechanism to dynamically assign importance to each curvature space, allowing it to adaptively capture various geometric structures such as hierarchies, rings, and chains present in TKGs. It further incorporates a quadruplet distributor for information interaction and an innovative temporal regularization to ensure the smoothness of timestamp embeddings \cite{wang2024}.

Building upon this multi-curvature paradigm, \textcite{wang2024} further refined the approach with IME (Integrating Multi-curvature Shared and Specific Embedding). IME addresses the limitations of earlier multi-curvature methods, which often overlook the "spatial gap" and heterogeneity between different curvature spaces. It innovatively integrates both "space-shared" properties to capture commonalities across spaces and "space-specific" properties to model unique features of each curvature, effectively bridging inter-space semantic gaps. Furthermore, IME introduces an Adjustable Multi-curvature Pooling (AMP) mechanism that learns optimal pooling weights for superior information fusion and employs novel similarity, difference, and structure loss functions to guide the learning process, demonstrating superior performance in TKGC tasks \cite{wang2024}.

Complementary to these geometric embedding advancements, other research focuses on leveraging Graph Neural Networks (GNNs) to explicitly capture complex multi-fact interactions across different timestamps. \textcite{xie2023} proposed TARGAT (A Time-Aware Relational Graph Attention Model), which tackles the challenge of GNN-based models struggling to directly capture interactions among multiple facts occurring at varying timestamps. TARGAT treats multi-facts across different timestamps as a unified graph and introduces a dynamic time-aware relational generator that creates time-aware relational message transformation matrices. These matrices are then used for time-aware feature projection and aggregation, enabling the model to explicitly capture intricate multi-fact interactions and achieve state-of-the-art results on several benchmarks \cite{xie2023}.

In summary, both multi-curvature embedding models like MADE and IME, and GNN-based approaches such as TARGAT, represent significant strides in handling the complex temporal dependencies and geometric structures of evolving KGs. MADE and IME offer geometrically adaptive embeddings that can represent diverse structural patterns more accurately than single-space models, with IME further enhancing inter-space interaction and adaptive fusion. TARGAT, on the other hand, provides a robust GNN-centric framework for modeling dynamic, time-aware relational interactions. While these methods offer enhanced flexibility and expressiveness, they often introduce increased computational overhead due to multi-space embeddings or complex GNN architectures, and interpreting the precise contributions of different curvature spaces or attention mechanisms remains a challenge for future research.


\label{sec:kge_for_downstream_applications_and_practical_considerations}

\section{KGE for Downstream Applications and Practical Considerations}
\label{sec:kge\_for\_downstream\_applications\_\_and\_\_practical\_considerations}

\subsection{Entity Alignment}
\label{sec:6\_1\_entity\_alignment}

The integration of heterogeneous knowledge graphs (KGs) is a fundamental challenge in knowledge engineering, primarily revolving around the task of entity alignment (EA). This process involves identifying equivalent entities across different KGs to facilitate their seamless integration and enrichment. Knowledge Graph Embedding (KGE) has emerged as a powerful paradigm to address this, by projecting entities and relations into a continuous vector space where semantic similarity can be directly computed, thereby enabling the discovery of cross-KG correspondences.

Early KGE-based approaches for entity alignment often grappled with significant challenges, notably the scarcity of labeled training data, which frequently led to suboptimal precision and robustness. To mitigate this, \textcite{sun2018} introduced \textit{BootEA}, a pioneering bootstrapping approach. BootEA iteratively expands the training data by labeling likely alignments, employing a global optimization strategy based on max-weighted matching and an alignment editing method to resolve conflicts and reduce error accumulation. This method was innovative in its use of a limit-based objective function and $\epsilon$-truncated negative sampling to learn alignment-oriented embeddings, significantly improving performance in low-resource settings. However, like any bootstrapping technique, BootEA faced the inherent risk of noise amplification if initial embeddings or iteratively labeled data were imperfect, potentially propagating errors throughout the training process. Building on the semi-supervised learning paradigm, \textcite{pei2019} further addressed data scarcity with \textit{SEA}, which uniquely incorporated adversarial training to account for entity degree differences. This innovation enhanced the robustness of KGEs by ensuring more consistent alignment accuracy across entities with varying frequencies, a crucial aspect often overlooked by methods sensitive to data distribution imbalances, where high-degree entities might dominate embedding learning.

Recognizing that entities possess diverse characteristics beyond just their relational structure, \textcite{zhang2019} proposed \textit{MultiKE}, an innovative framework that unified multiple entity 'views'---namely, name, relation, and attribute---into a comprehensive embedding framework. MultiKE departed from previous approaches by jointly optimizing view-specific embeddings and, crucially, introduced "soft alignment" for relations and attributes, thereby reducing the heavy dependency on pre-existing seed alignments for these components. This multi-view approach significantly enhanced the richness of entity representations by integrating heterogeneous features. However, a critical challenge in such multi-view frameworks lies in effectively balancing and combining these diverse views, as improper weighting or potential negative transfer between views can hinder overall performance, adding complexity to feature engineering and model optimization. While these methods advanced the field by leveraging various entity features, they often overlooked higher-level semantic constraints. \textcite{xiang2021} presented \textit{OntoEA}, the first comprehensive framework to integrate ontological schema, including class hierarchies and disjointness axioms, into joint KG-ontology embedding. OntoEA directly addressed "class conflict" errors, a significant source of false positives in previous embedding-based EA methods, by explicitly modeling and learning inter-class conflicts through a novel Class Conflict Matrix and associated loss functions, thereby improving semantic consistency and leveraging richer background knowledge.

Leveraging the powerful neighborhood aggregation capabilities of Graph Neural Networks (GNNs), as detailed in Section 4.3, a new wave of entity alignment models has emerged to better capture structural similarities. GNNs inherently excel at aggregating multi-hop structural information from an entity's neighborhood, enabling them to learn more context-rich and discriminative embeddings for alignment. Unlike traditional KGEs that primarily focus on triple-level interactions, GNN-based models, such as RDGCN \cite{fanourakis2022}, can capture complex relational paths and structural patterns, leading to superior performance in identifying structural similarities between entities across KGs. This end-to-end learning of structural features offers a powerful alternative to manual feature engineering seen in methods like MultiKE. For instance, in the context of large-scale KGs, \textcite{xin2022dam} introduced a scalable GNN-based entity alignment approach that tackles the structure and alignment loss often incurred during KG partitioning. Their method proposes a centrality-based subgraph generation, self-supervised entity reconstruction, and cross-subgraph negative sampling to maintain structural integrity and alignment quality, demonstrating the adaptability of GNNs to real-world scalability challenges. Despite their strengths, GNNs can be sensitive to severe structural heterogeneity between KGs and may incur higher computational demands, particularly for very deep architectures or extremely large graphs, and can suffer from over-smoothing issues.

More recently, the landscape of entity alignment has been significantly impacted by the rise of Large Language Models (LLMs). These models, leveraging vast pre-trained knowledge and advanced natural language understanding capabilities, offer a distinct paradigm for EA, particularly in scenarios rich in textual descriptions or where structural information is sparse. As highlighted by \textcite{ge2023}, the integration of KGE methods with Pre-trained Language Models (PLMs) is a promising direction for KG completion, extending naturally to EA. LLM-based approaches can directly compare entity names, descriptions, and attributes by encoding them into a shared semantic space, often outperforming structure-focused GNNs in low-resource or zero-shot alignment settings where seed alignments are scarce. They excel at leveraging external, general-world knowledge implicitly encoded during pre-training, which is a significant advantage over models relying solely on the internal structure of the KGs. Furthermore, \textcite{zhu2024} emphasizes that the integration of global structural embedding with local semantic information (e.g., attributes, images) is crucial for enhancing alignment accuracy, a task where LLMs, with their multimodal capabilities, are increasingly proving effective. This paradigm shift allows for more flexible and robust alignment, especially when dealing with KGs that have limited overlap in their structural patterns but share rich textual content.

A comprehensive experimental review by \textcite{fanourakis2022} provides critical insights into the comparative performance and trade-offs of various embedding-based EA methods. Their meta-level analysis revealed statistically significant correlations between method performance and dataset characteristics, such as KG density and factual information richness. For instance, unsupervised and semi-supervised methods exploiting literal similarity (e.g., AttrE, KDCoE) were found to outperform supervised relation-based methods (like RDGCN) on datasets with decreasing density but rich factual information. This highlights that while GNNs generally excel at structural learning, methods leveraging textual or attribute information can be more robust in specific data sparsity scenarios or when structural information is limited, underscoring the importance of choosing the right approach based on data characteristics. The study also underscored the critical trade-off between effectiveness and efficiency, emphasizing that more complex models, while potentially more accurate, incur higher computational overhead, a practical consideration for real-world deployment.

These advancements collectively demonstrate a clear intellectual trajectory in leveraging KGE for entity alignment. The field has progressed from foundational KGE models that primarily focused on relational structures to increasingly sophisticated, problem-specific solutions that mitigate data scarcity, enhance robustness, integrate richer, heterogeneous information (e.g., attributes, ontological schema), and leverage advanced neural architectures like GNNs for deeper structural understanding, culminating in the integration of powerful LLMs for semantic matching. This evolution reflects a continuous effort to integrate diverse information sources and advanced learning paradigms to achieve more accurate and robust entity alignment across heterogeneous knowledge bases.
\subsection{Recommendation Systems and Question Answering}
\label{sec:6\_2\_recommendation\_systems\_\_and\_\_question\_answering}

Knowledge Graph Embeddings (KGE) play a pivotal role in enhancing the capabilities of intelligent information retrieval and decision-making processes, particularly in recommendation systems and Knowledge Graph-based Question Answering (KGQA). By representing entities and relations in continuous vector spaces, KGE models facilitate the discovery of intricate semantic relationships that are crucial for personalized suggestions and accurate query responses.

In the realm of recommendation systems, KGE models have evolved to capture increasingly nuanced user and item interactions. Early approaches often relied on labor-intensive, hand-engineered features derived from knowledge graphs. Addressing this, Recurrent Knowledge Graph Embedding (RKGE) \cite{sun2018} introduced a novel recurrent network architecture to automatically learn semantic representations for both entities and the paths connecting them. This approach, featuring a batch of recurrent networks and a pooling operator, effectively models the diverse semantics of multiple paths between entity pairs, thereby characterizing user preferences and providing meaningful explanations for recommendations. Building upon the foundation of single-domain recommenders, the challenge of cross-domain item recommendations and cold start problems emerged. To tackle this, \cite{liu2023} proposed a Cross-Domain Knowledge Graph Chiasmal Embedding approach, which efficiently interacts items across multiple domains through a novel binding rule. This method frames multi-domain item-item recommendation as a link prediction task within a cross-domain knowledge graph, demonstrating superior performance in both link prediction and multi-domain recommendation results. Further advancing the field, the demand for explainable recommendations has grown significantly. Contextualized Knowledge Graph Embedding (CKGE) \cite{yang2023} addresses this by integrating motivation-aware contextual information and high-order connections within a knowledge graph. CKGE employs a specialized KG-based Transformer, which processes meta-graphs constructed for each talent-course pair, incorporating relational attention and structural encoding. A key innovation is its local path mask prediction mechanism, which reveals the saliency of meta-paths, offering explicit explanations for recommendations and characterizing user preferences.

For Knowledge Graph-based Question Answering (KGQA), KGE models are instrumental in bridging the gap between natural language queries and structured knowledge. Initial KGE-based QA systems focused on simpler query types. For instance, the Knowledge Embedding based Question Answering (KEQA) framework \cite{huang2019} targets "simple questions" by jointly recovering the question's head entity, predicate, and tail entity representations within the KG embedding spaces. It then derives an answer by identifying the closest fact in the KG using a carefully designed joint distance metric, outperforming state-of-the-art methods on relevant benchmarks. Recognizing the need for domain-specific and more complex query handling, Marie and BERT \cite{zhou2023} developed a KGQA system specifically for chemistry. This system leverages hybrid knowledge graph embeddings, operating on multiple embedding spaces queried in parallel, to handle deep ontologies, numerical filtering, and intricate chemical reaction mechanisms. It introduces a score alignment model, an implicit multihop relation algorithm, and a BERT-based bidirectional entity-linking model, significantly advancing domain-specific KGQA. A more recent development involves the synergistic integration of KGE with Large Language Models (LLMs). A knowledge-enhanced joint model \cite{liu2024q3q} incorporates aviation assembly knowledge graph embeddings into LLMs for fault diagnosis. This model utilizes graph-structured big data from KGs to conduct prefix-tuning of LLMs, enabling online reconfiguration and strengthening specialized knowledge within the aviation assembly domain. By generating knowledge subgraphs and fusing knowledge through retrieval augmentation, it provides robust, knowledge-based reasoning responses, achieving high accuracy in practical industrial scenarios.

The progression in this area highlights a continuous effort to move beyond basic relational modeling towards more sophisticated, context-aware, and explainable systems. While significant strides have been made in automating feature learning, handling multi-domain interactions, and integrating with advanced language models, challenges remain. Future research could focus on developing more robust cross-domain knowledge transfer mechanisms that adapt to highly dynamic environments, enhancing the interpretability of complex multi-modal KGE-LLM systems, and exploring novel ways to dynamically update KGEs to reflect real-time changes in knowledge graphs for both recommendation and question answering tasks.
\subsection{Efficiency, Scalability, and Robustness in Training}
\label{sec:6\_3\_efficiency,\_scalability,\_\_and\_\_robustness\_in\_training}

The successful deployment of Knowledge Graph Embedding (KGE) models in real-world applications hinges on their ability to operate efficiently, scale to massive knowledge graphs (KGs), and maintain robustness against noise and incompleteness. Beyond the design of expressive scoring functions, the choice and optimization of training components significantly impact model performance and practical viability \cite{mohamed2021dwg}. This subsection delves into key advancements addressing these critical practical challenges.

A primary area of focus for training efficiency is negative sampling, a ubiquitous technique for learning from sparse positive triples. Traditional random negative sampling often suffers from the "vanishing gradient problem" by frequently selecting "easy" negatives, which provide little learning signal \cite{zhang2018, qian2021}. To overcome this, \textbf{NSCaching} \cite{zhang2018} introduced an efficient cache-based approach to dynamically store and sample high-quality, "hard" negative triplets, proving more effective than complex generative adversarial network (GAN)-based methods. For KGE models integrating multimodal data, \textbf{Modality-Aware Negative Sampling (MANS)} \cite{zhang2023} further refines this by introducing modal-level sampling, particularly for visual embeddings, to explicitly learn modality alignment, a crucial aspect overlooked by prior entity-level strategies. Addressing the inherent noise in real-world KGs, \textbf{Confidence-Aware Negative Sampling} \cite{shan2018} supports robust training by leveraging negative triple confidence, mitigating issues like zero loss and false detection. An alternative approach, the \textbf{Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE)} framework \cite{li2021}, mathematically re-derives the non-sampling square loss function to enable full-data training, achieving improved accuracy and stability without prohibitive computational or space costs for a broad class of models. Theoretically, the relationship between negative sampling and softmax cross-entropy loss functions has been unified using Bregman divergence, offering insights for fair comparisons and understanding their underlying mechanisms \cite{kamigaito20218jz}.

Beyond sampling, efforts to reduce model complexity and resource footprint are crucial. \textbf{TransGate} \cite{yuan2019} enhanced parameter efficiency by introducing a shared gate structure, inspired by LSTMs, to discriminate relation-specific information with significantly fewer parameters and comparable time complexity to simpler models like TransE. For deploying high-performing but resource-intensive KGEs, \textbf{DualDE} \cite{zhu2020} proposed a knowledge distillation framework that dually distills knowledge from a high-dimensional teacher KGE to a low-dimensional student. This approach can achieve substantial parameter reduction (up to 15x) and inference speedup (up to 6x) with minimal accuracy loss, demonstrating a practical trade-off between model capacity and efficiency. Complementing this, \textbf{LightKG} \cite{wang2021} introduced a lightweight framework utilizing codebooks and codewords for efficient storage and inference, dramatically reducing memory consumption and search time while maintaining high approximate search accuracy.

Scalability for truly massive KGs often necessitates distributed or partitioned training approaches. \textbf{CPa-WAC} \cite{modak2024} introduced a Constellation Partitioning-based Scalable Weighted Aggregation Composition framework for GNN-based KGE. This method partitions KGs into topological clusters using Louvain clustering and then aggregates embeddings from these partitions for global inference, reducing training time significantly (up to five times) without substantial accuracy degradation. However, a potential limitation of partitioning lies in the challenge of maintaining global graph coherence and preventing information loss across partition boundaries. For privacy-preserving and distributed training, \textbf{FedS} \cite{zhang2024} proposed a communication-efficient federated KGE framework. FedS leverages entity-wise Top-K sparsification to reduce the size of transmitted parameters per communication round, enhancing communication efficiency with negligible performance degradation. Federated KGE, however, introduces its own set of challenges, including data heterogeneity across clients, which \textbf{FedLU} \cite{zhu2023bfj} addresses through mutual knowledge distillation. Furthermore, privacy threats, such as inference attacks that can reveal sensitive triple existence, require robust defenses like differentially private mechanisms with private selection (\textbf{DP-Flames} \cite{hu20230kr}). The vulnerability to poisoning attacks, where malicious clients inject biased updates, has also been demonstrated, highlighting the need for secure aggregation and anomaly detection in federated settings \cite{zhou2024}. More broadly, empirical studies on parallel KGE training have revealed that many existing methods can negatively impact embedding quality, emphasizing the importance of careful technique selection, with variations of stratification and random partitioning showing promise \cite{kochsiek2021}.

Robustness to noise and incompleteness is another critical aspect. Beyond confidence-aware negative sampling, \textbf{Multi-Task Reinforcement Learning (MTRL)} \cite{zhang2021} offers a general framework for robust KGE by training RL agents to filter noisy triples, leveraging multi-task learning for semantically similar relations. While not primarily a training efficiency technique, the integration of auxiliary information like logical rules (as discussed in Section 4.1) inherently enhances robustness by providing constraints and reducing reliance on potentially noisy observed facts. To ensure the reliability of KGE model predictions, \textbf{Probability Calibration for KGE Models} \cite{tabacof2019} demonstrated that KGE models are often uncalibrated, proposing methods to ensure predicted probabilities are trustworthy, which is vital for high-stakes applications. Furthermore, \textbf{Committee-based KGE} \cite{choi2020} showed that an ensemble of diverse KGE models can achieve more robust knowledge base completion than any single model, by aggregating multiple perspectives. Finally, efficient hyperparameter optimization is crucial for practical deployment. \textbf{KGTuner} \cite{zhang2022fpm} proposes a two-stage search algorithm that efficiently explores hyperparameter configurations on small subgraphs and then fine-tunes on the full graph, consistently finding better hyperparameters within the same time budget.

In summary, operationalizing KGE models reveals a fundamental tension between achieving high expressiveness and ensuring practical deployability. While advancements in negative sampling, knowledge distillation, and lightweight architectures significantly improve training efficiency and reduce resource footprints, they often involve trade-offs in model capacity or require careful tuning to maintain performance. Similarly, scaling via partitioning or federated learning improves throughput and addresses privacy concerns but introduces complexities like communication bottlenecks, challenges in maintaining global graph coherence, and vulnerabilities to attacks. Future research must therefore focus on developing holistic solutions that co-design models and training regimes to be inherently efficient, scalable, and robust, rather than relying on isolated optimizations.


\label{sec:emerging_trends_and_future_directions}

\section{Emerging Trends and Future Directions}
\label{sec:emerging\_trends\_\_and\_\_future\_directions}

\subsection{Automated KGE and Meta-Learning}
\label{sec:7\_1\_automated\_kge\_\_and\_\_meta-learning}

The increasing complexity of Knowledge Graph Embedding (KGE) model design and the dynamic nature of real-world knowledge graphs necessitate a shift towards automated model selection and adaptive learning paradigms. This section explores the growing trend towards automating the KGE model design process (AutoML) and leveraging meta-learning for dynamic and adaptive embeddings. These approaches collectively aim to reduce manual effort, improve model adaptability to diverse KG characteristics, and enable continuous learning in evolving environments, pushing KGE towards more intelligent, autonomous, and self-optimizing systems.

A significant challenge in KGE has been the manual design of effective scoring functions and model architectures, which often exhibit dataset-specific optimal performance. To address this, \textcite{zhang2019} introduced \texttt{AutoSF}, an AutoML framework designed to automatically search for optimal scoring functions for diverse knowledge graphs. By identifying a unified representation for bilinear models and incorporating domain-specific insights into relation properties, \texttt{AutoSF} employs a progressive greedy search algorithm with a predictor to efficiently discover novel, KG-dependent scoring functions, thereby significantly reducing manual effort and improving adaptability. Building on this, \textcite{di20210ib} further refined the concept of automated scoring function search by proposing an efficient relation-aware approach. Recognizing that different semantic patterns within a KG might benefit from distinct scoring mechanisms, their method encodes the search space as a supernet and utilizes a one-shot alternative minimization algorithm to efficiently discover relation-aware scoring functions, leading to improved performance over general, non-relation-aware searches. Extending the scope of AutoML beyond scoring functions to neural architectures, \textcite{shang2024} proposed \texttt{MGTCA}, a framework that addresses the data dependence of Graph Neural Network (GNN) types in Knowledge Graph Completion. \texttt{MGTCA} introduces a Trainable Convolutional Attention Network that allows for autonomous switching between GNN types (GCN, GAT, and a novel KGCAT) and integrates a Mixed Geometry Message Function to capture richer structural information from hyperbolic, hypersphere, and Euclidean spaces. This approach automates the model design process by adaptively selecting the most suitable GNN architecture for local graph structures, eliminating the need for expensive pre-validation. Similarly, \textcite{di2023} focused on a more granular level of GNN design by building a search space for the \textit{message function} itself. Their innovation lies in allowing both the structure and operators of the message function to be searched, making it highly adaptable to different KG forms (traditional KGs, n-ary relational data, hyper-relational KGs) and datasets. While \texttt{AutoSF} and \textcite{di20210ib} primarily focus on the scoring function, \texttt{MGTCA} and \textcite{di2023} extend AutoML to the architectural components of GNNs, offering a more comprehensive automation of KGE model construction. The trade-off often lies between the computational cost of the search process and the expressiveness and adaptability of the automatically discovered models.

Beyond automating static model design, a critical aspect of modern KGE is the ability to continuously learn and adapt to evolving knowledge and unseen entities. Traditional KGE models are often transductive, struggling with emerging entities and dynamic knowledge updates. \textcite{sun2024} presented \texttt{MetaHG}, a meta-learning strategy specifically tailored for dynamic updates in evolving service ecosystems. \texttt{MetaHG} addresses the inefficiency of updating incremental knowledge by incorporating both local and potential global structural information from current KG snapshots via a hybrid GNN and Hypergraph Neural Network (HGNN) framework. This enables continuous learning and adaptation to changing knowledge, mitigating issues like spatial deformation. Complementing this, \textcite{chen2021} introduced \texttt{MorsE}, another meta-learning framework for inductive KGE that focuses on generating general entity embeddings for \textit{entirely new entities in unseen KGs}. \texttt{MorsE} learns "meta-knowledge" through entity-independent modules (an Entity Initializer and a GNN Modulator) that capture transferable structural patterns, allowing it to produce high-quality embeddings for novel entities without retraining from scratch. While \texttt{MorsE} focuses on generalizing to unseen entities, \textcite{lee202380l}'s \texttt{InGram} extends this inductive capability to also generate embeddings for \textit{new relations} at inference time, leveraging relation graphs and attention mechanisms to aggregate neighboring embeddings. These inductive approaches are crucial for handling the constant influx of new information in real-world KGs.

Furthermore, the challenge of continually updating KGE models with new knowledge while preserving old knowledge (Continual Knowledge Graph Embedding, CKGE) is a key area where meta-learning principles are applied. \textcite{liu2024to0} proposed Incremental Distillation (IncDE) for CKGE, which explicitly considers the graph structure of new knowledge. IncDE introduces a hierarchical strategy to optimize the learning order of new triples and devises an incremental distillation mechanism to seamlessly transfer entity representations, thereby promoting old knowledge preservation and mitigating catastrophic forgetting. Building on efficiency, \textcite{liu2024} introduced \texttt{FastKGE} with an Incremental Low-Rank Adapter (IncLoRA) mechanism. Inspired by parameter-efficient fine-tuning in large language models, IncLoRA efficiently learns and stores new knowledge by dividing new entities and relations into layers and assigning incremental low-rank adapters. This approach significantly reduces training costs for new knowledge acquisition while maintaining competitive performance in preserving old knowledge. While \texttt{MetaHG} focuses on dynamic updates within an evolving system, \texttt{MorsE} and \texttt{InGram} tackle the more general problem of inductive learning for entirely new entities and relations, and IncDE and FastKGE specifically address the efficiency and forgetting challenges inherent in continual learning scenarios.

In conclusion, the trend towards automated KGE model design and meta-learning for dynamic embeddings marks a significant step towards more intelligent, autonomous, and self-optimizing KGE systems. AutoML frameworks like \texttt{AutoSF}, \textcite{di20210ib}, \texttt{MGTCA}, and \textcite{di2023} are reducing manual effort by automating the search for optimal scoring functions and GNN architectures, leading to more adaptable models. Concurrently, meta-learning approaches such as \texttt{MetaHG}, \texttt{MorsE}, \texttt{InGram}, IncDE, and \texttt{FastKGE} are enabling KGE models to continuously learn, adapt to evolving knowledge, and generalize to unseen entities and relations, addressing the inherent transductive limitations of many traditional models. Future research must continue to balance the increasing complexity and computational cost of these adaptive and automated models with efficiency, robustness, and interpretability, especially as knowledge graphs grow in size and dynamism.
\subsection{Multimodal KGE and Explainability}
\label{sec:7\_2\_multimodal\_kge\_\_and\_\_explainability}

The evolution of Knowledge Graph Embedding (KGE) is increasingly characterized by a progressive move beyond purely symbolic representations towards richer, multimodal information integration. This paradigm shift is critically intertwined with the growing demand for explainability in AI systems, as the inherent complexity introduced by diverse data types necessitates enhanced transparency and interpretability. The integration of multimodal data aims to create more comprehensive and robust knowledge representations, while explainable KGE methods are crucial for fostering trust and understanding, particularly in sensitive domains where model transparency is paramount for adoption and ethical deployment.

\subsubsection{Multimodal Knowledge Graph Embedding}
Integrating multimodal information into KGE seeks to enrich entity and relation representations by combining symbolic facts with features extracted from various data types, including text, images, and potentially audio or video. Early efforts primarily focused on leveraging diverse \textit{textual} information. For instance, \cite{zhang2019multike} introduced MultiKE, a pioneering approach for entity alignment that unified multiple entity "views" (name, relation, attribute) derived from textual descriptions. MultiKE innovated by designing cross-KG inference methods and a "soft alignment" for relations and attributes, thereby reducing reliance on costly seed alignments and demonstrating the benefits of heterogeneous textual feature integration for robust entity matching. While significant, these approaches largely remained within the linguistic domain.

The true frontier of multimodal KGE lies in the direct integration of \textit{non-textual} modalities, such as visual features, with symbolic knowledge. Foundational models began to bridge this gap by extending existing KGE frameworks. For example, Image-embodied Knowledge Representation Learning (IKRL) \cite{xie2016image} integrated visual information by projecting image features into the same embedding space as entities and relations, allowing visual similarities to influence the learning of symbolic relationships. Similarly, Multimodal Knowledge Base Embedding (MKBE) \cite{guo2018multimodal} utilized CNNs to extract visual features from images associated with entities, fusing them with textual descriptions and structural information to enhance entity representations. These early methods demonstrated the potential of combining visual cues with relational facts, often by modifying scoring functions or introducing auxiliary loss terms.

More recent advancements leverage advanced neural architectures. For example, \cite{liang2023hypernode} proposed Hyper-node Relational Graph Attention Network (HRGAT) as a customized Graph Neural Network (GNN) for multimodal knowledge graphs. HRGAT combines different modal information (e.g., visual, textual) with graph structure information, using a hyper-node concept to represent entities that aggregate features from various modalities, thereby yielding a more precise and interpretable representation. This approach highlights the utility of GNNs in processing heterogeneous features for multimodal KGE. Another concrete application is seen in domain adaptation for tasks like musical instrument recognition, where \cite{eyharabide2021wx4} presented a method that incorporates KGE-derived semantic vector spaces as a key ingredient to guide the domain adaptation process, combining these semantic embeddings with visual embeddings from images. By training a neural network with these combined embeddings as anchors, their method demonstrates how KGE can enhance visual recognition tasks, effectively fusing symbolic knowledge with visual features to improve performance in data-scarce cultural heritage datasets. A recent survey by \cite{zhu2024survey} further underscores the growing importance of multimodal entity alignment, highlighting the integration of diverse modalities beyond text, such as images and even video, as a critical future direction.

These approaches typically employ various fusion techniques, such as joint embedding spaces where features from different modalities are projected into a common vector space, or attention mechanisms that selectively weigh the importance of different modal inputs. Despite these advancements, challenges remain in developing universally applicable multimodal KGE models. Key issues include the inherent heterogeneity of data (e.g., varying dimensions, noise levels), the complexity of aligning information across vastly different modalities, and the scarcity of large-scale, richly annotated multimodal knowledge graph datasets for training and evaluation. Furthermore, the increased dimensionality and complexity introduced by multimodal data can significantly exacerbate the "black box" nature of KGE models, intensifying the demand for robust explainability.

\subsubsection{Explainable Knowledge Graph Embedding}
The growing importance of explainability in KGE is crucial for building trust and understanding in AI systems, especially as models become more complex and operate in sensitive applications. Explainable KGE methods aim to provide insights into model predictions, learned representations, and underlying reasoning processes. We can categorize these methods into two main types: ante-hoc (intrinsic) explainability, where models are designed to be inherently interpretable, and post-hoc explainability, where explanations are generated after model training.

\textbf{Ante-hoc/Intrinsic Explainability}: These models embed interpretability directly into their design, often through intuitive geometric or statistical properties of their representations or by integrating explicit symbolic knowledge. \cite{li2024sphere} proposed SpherE, which embeds entities as spheres where the radius intuitively correlates with an entity's "universality" or prevalence in the knowledge graph. This sphere-based modeling not only enhances expressiveness for many-to-many relations but also offers an interpretable parameter that provides insights into the entity's role and scope within the KG. Similarly, \cite{pavlovic2022expressive} introduced ExpressivE, a spatio-functional KGE model that embeds entity pairs as points and relations as hyper-parallelograms. A key contribution of ExpressivE is its explicit focus on providing an "intuitive interpretation" and "consistent geometric interpretation" of captured inference patterns (e.g., composition, hierarchy) through the spatial relationships of these hyper-parallelograms, making the model's internal logic more transparent and directly understandable.

However, relying solely on geometric intuition for explainability can be insufficient for complex logical reasoning. \cite{gutierrezbasulto2018ontology} critically analyzed the compatibility between vector space representations and ontological rules, demonstrating that many popular embedding methods are incapable of modeling even simple rule types, thus failing to capture the dependencies inherent in symbolic knowledge. This highlights a limitation of purely geometric approaches. In contrast, models that integrate explicit logic rules offer a more formal and robust form of intrinsic explainability. For instance, \cite{wang2019logic} proposed a logic rule-enhanced method that can be integrated with translational KGE models. By automatically mining logic rules and representing triples as first-order logic, this approach transfers human knowledge into embeddings, making the reasoning process more aligned with symbolic logic and thus inherently more interpretable. Such rule-aware designs move beyond mere geometric analogies to provide explanations grounded in logical consistency.

\textbf{Post-hoc Explainability}: These methods aim to explain model behavior after training, often by analyzing predictions or learned features. This category can be further broken down into different levels of explanation:

\textit{   \textbf{Meta-level Understanding of Generalization}: This focuses on explaining }why* KGE models generalize or extrapolate. \cite{li2021semantic} addresses this by introducing the concept of "Semantic Evidence" (relation-level, entity-level, and triple-level) to explain how KGE models extrapolate to unseen data, providing crucial insights into their underlying mechanisms and contributing to a meta-level understanding of model behavior. Expanding on this, \cite{kurokawa2021explainable} proposed an explainable knowledge reasoning framework that combines multiple KGE techniques with corresponding explainable AI (XAI) techniques. This framework aims to integrate various methods to achieve complex reasoning with explanations, demonstrating a broader approach to understanding KGE behavior in a multi-model context.

\textit{   \textbf{Path-based Explanations (Local Explanations)}: These methods provide insights into }which\textit{ specific paths or facts contribute to a particular prediction. \cite{jia2020pconvkb} proposed PConvKB, a convolutional neural network-based embedding model that incorporates relation paths. By employing attention mechanisms to measure the local importance of relation paths and a measure called DIPF for global importance, PConvKB provides insights into }which* paths contribute most significantly to a fact's inference, thereby offering a form of explainability by highlighting the evidential paths in the graph. While effective for local explanations, the computational cost of enumerating and evaluating paths can be high for dense or very large KGs, limiting scalability.

*   \textbf{Application-specific Explanations}: In downstream applications, explainability often translates to providing concrete reasons for a model's output. \cite{sun2018rkge} introduced RKGE, a recurrent knowledge graph embedding model designed for recommendation systems. RKGE learns path semantics within KGs and provides "meaningful explanations" for its recommendation results, demonstrating early efforts in transparent recommendation systems. Building on this, \cite{yang2023ckge} further advanced explainable KGE with CKGE, a contextualized approach for explainable talent training course recommendation. CKGE constructs motivation-aware meta-graphs and employs a novel KG-based Transformer with a "local path mask prediction" mechanism to explicitly reveal the saliency of different meta-paths, offering concrete, motivation-driven explanations for specific recommendations. In sensitive domains like healthcare, explainability is paramount. For instance, SEConv \cite{yang2025seconv} is designed to provide transparent and interpretable predictions in healthcare applications, where understanding the rationale behind a model's output (e.g., drug-disease association) is critical for clinical trust and decision-making. Such models are crucial for bridging the gap between complex AI systems and human experts.

In conclusion, the field of KGE is actively pursuing richer, multimodal representations to capture the complexity of real-world knowledge, moving from integrating diverse symbolic and textual features towards a broader, though still challenging, integration of non-textual modalities. Concurrently, there is a strong emphasis on developing explainable KGE methods, ranging from understanding model generalization and providing intrinsically interpretable representations (e.g., geometric or rule-based) to offering concrete, prediction-specific explanations. A significant gap remains, however, in models that are \textit{both} truly multimodal and inherently explainable. The increased complexity from multimodal inputs often deepens the "black box" nature of KGE models, making it challenging to attribute predictions to specific modalities or explain cross-modal interactions. Future work will likely focus on developing sophisticated multimodal fusion techniques that are designed with explainability in mind, formalizing evaluation metrics for multimodal explanations, and advancing human-centered explainable AI for KGE, ensuring that as KGE models become more powerful, they also become more transparent and trustworthy.
\subsection{Open Challenges and Ethical Considerations}
\label{sec:7\_3\_open\_challenges\_\_and\_\_ethical\_considerations}

Despite remarkable progress in Knowledge Graph Embedding (KGE) research, the field continues to grapple with fundamental, persistent challenges that impede the robust, scalable, and ethically responsible deployment of KGE-powered AI systems. This subsection critically assesses these unresolved technical hurdles and delves into the crucial ethical implications, outlining essential areas for future investigation to ensure KGE techniques align with societal values.

A significant, overarching challenge remains \textbf{extreme scalability and continuous adaptation for dynamic Knowledge Graphs (KGs)}. While Section 6.3 discusses various solutions for improving KGE efficiency and handling dynamic updates, these often address specific facets rather than the holistic problem of KGs that are simultaneously massive, highly dynamic, and non-stationary. For instance, lightweight frameworks like LightKG \cite{wang2021} reduce storage and inference costs, and knowledge distillation methods like DualDE \cite{zhu2020} enable faster reasoning. Similarly, graph partitioning strategies such as CPa-WAC \cite{modak2024} enhance the scalability of GNN-based KGEs. However, these solutions frequently involve trade-offs, such as simplified representations or reliance on discrete updates, which may not suffice for KGs where topology and semantics evolve continuously and rapidly. Meta-learning approaches, exemplified by MetaHG \cite{sun2024} and \cite{mao2024v2s}, offer promise for adapting to incremental knowledge and emerging entities. Yet, the challenge of maintaining high-quality embeddings and efficient inference for KGs experiencing continuous, high-velocity changes across billions of entities and relations remains a grand research problem. The entire training pipeline, including loss functions, hyperparameters, and negative sampling strategies, requires holistic optimization to achieve true scalability and accuracy \cite{mohamed2021dwg}, rather than focusing solely on scoring functions.

Another critical area is the \textbf{robust handling of inherent uncertainty, incompleteness, and adversarial threats in real-world data}. KGs are intrinsically noisy, incomplete, and can be subject to manipulation. While methods like confidence-aware negative sampling \cite{shan2018} and reinforcement learning frameworks \cite{zhang2021} improve robustness against noise, and techniques like committee-based models \cite{choi2020} or logical rule integration \cite{guo2017, guo2020} address incompleteness, fundamental limitations persist. Real-world knowledge often contains fuzzy, conflicting, or inherently uncertain facts, as seen in complex spatiotemporal KGs \cite{ji2024}. Current KGE models frequently struggle to reliably quantify the certainty of their predictions, with studies showing that KGE models are often uncalibrated \cite{tabacof2019}. This lack of reliable confidence scores undermines the trustworthiness of KGE-powered systems in critical applications. Furthermore, KGE models are vulnerable to \textit{data poisoning attacks} \cite{zhang20190zu}, where malicious actors can manipulate the plausibility of facts by injecting or deleting triples, posing a significant threat to the integrity and reliability of learned representations and downstream tasks. Developing KGEs that are inherently resilient to such adversarial manipulations and can robustly model epistemic uncertainty remains a profound challenge.

Beyond these technical limitations, KGE research faces critical \textbf{ethical implications} that demand proactive attention.
\begin{enumerate\textit{}[label=(\roman})]
    \item \textbf{Bias and Fairness in Learned Representations}: KGE models learn from existing KGs, which are often constructed from real-world data reflecting historical, societal, or data collection biases. If these biases are embedded in learned representations, they can be amplified when deployed in sensitive applications, leading to unfair or discriminatory outcomes. For instance, KGEs used in healthcare (e.g., SEConv \cite{yang2025}, multimodal reasoning for diseases \cite{zhu2022}) could perpetuate biases in medical records, affecting diagnoses or treatment recommendations for specific demographic groups. A specific technical challenge is \textit{degree bias}, where entities with fewer connections (low-degree nodes) receive poorer representations, leading to performance disparities \cite{shomer2023imo}. While solutions like KG-Mixup \cite{shomer2023imo} mitigate degree bias in static KGs and FairDGE \cite{li2024wyh} addresses \textit{structural fairness} in dynamic graphs, the broader challenge lies in developing comprehensive, domain-agnostic debiasing strategies that account for intersectional biases without compromising model utility. The field requires more robust methods to detect, quantify, and mitigate various forms of bias, ensuring equitable performance across diverse entity groups.
    \item \textbf{Data Privacy and Confidentiality Concerns}: Training KGE models often requires access to vast amounts of data, some of which may be sensitive or proprietary. Sharing such data or models trained on it can pose significant privacy risks. Federated KGE (FKGE) offers a promising direction by enabling collaborative learning from distributed KGs without centralizing raw data, as seen in FedS \cite{zhang2024}, personalized FKGE (PFedEG) \cite{zhang2024}, and cross-domain FKGE (FedCKE) \cite{huang2023grx}. However, FKGE is not immune to privacy threats. \textit{Inference attacks} and \textit{model inversion attacks} can still reveal sensitive information about clients' private KGs, even with distributed training \cite{hu20230kr}. Advanced defenses, such as differentially private FKGE (DP-Flames) \cite{hu20230kr}, aim to provide stronger privacy guarantees but often come with inherent \textit{privacy-utility trade-offs}. The ongoing challenge is to develop robust privacy-preserving mechanisms that can effectively balance strong confidentiality with high model performance, especially in heterogeneous and dynamic federated environments.
    \item \textbf{Responsible Deployment of KGE-powered AI Systems}: KGEs are increasingly integrated into critical AI systems, from question answering in chemistry \cite{zhou2023} to fault diagnosis in aviation assembly \cite{liu2024q3q}. In such high-stakes applications, ensuring fairness, accountability, and transparency (FAT) is paramount. The aforementioned issues of uncalibrated predictions \cite{tabacof2019}, vulnerability to adversarial attacks \cite{zhang20190zu}, and embedded biases directly undermine the trustworthiness and safety of these systems. Responsible deployment necessitates moving beyond purely performance-driven metrics to a holistic consideration of ethical dimensions. This includes developing comprehensive frameworks for auditing KGE systems for bias, ensuring robust defenses against adversarial manipulation, and providing reliable confidence scores for predictions. The field must prioritize the development of KGEs that are not only powerful but also inherently trustworthy, fair, and beneficial for society, requiring interdisciplinary collaboration to establish ethical guidelines and regulatory standards.
\end{enumerate*}

In conclusion, while KGE research has made remarkable strides, significant challenges persist in achieving extreme scalability for dynamic KGs, ensuring robustness against inherent data imperfections and adversarial threats, and critically, addressing the profound ethical implications of bias, privacy, and responsible deployment. Future investigations must prioritize the development of KGE techniques that are not only performant but also inherently robust, fair, and interpretable (as discussed in Section 7.2), aligning with societal values and ensuring the trustworthy application of KGE-powered AI systems in an increasingly complex world.


\label{sec:conclusion}

\section{Conclusion}
\label{sec:conclusion}

\subsection{Synthesis of Key Developments}
\label{sec:8\_1\_synthesis\_of\_key\_developments}

The evolution of Knowledge Graph Embedding (KGE) research represents a continuous intellectual journey, driven by the imperative to capture increasingly rich semantics, model complex structural patterns, and adapt to the dynamic nature of real-world knowledge. This progression has transformed the field from rudimentary geometric models to sophisticated neural architectures and specialized temporal frameworks, significantly enhancing the expressiveness and utility of KGE across diverse applications. The overarching theme is a relentless pursuit of more powerful and versatile knowledge representation techniques, moving from simple, explicit assumptions about relations to implicitly learned, context-aware, and adaptable embeddings.

The foundational phase of KGE was characterized by geometric interpretations, primarily focusing on modeling relations as transformations in vector spaces. Early translational models, exemplified by TransE \cite{bordes2013}, established the viability of embedding entities and relations into continuous spaces. However, their simplicity led to limitations in handling complex relational patterns like 1-to-N, N-to-1, and N-to-N. This spurred advancements such as TransH \cite{wang2014} and TransD \cite{ji2015}, which introduced relation-specific hyperplanes and dynamic mapping matrices, respectively, to enhance expressiveness for diverse entities and relations. Simultaneously, semantic matching models like RESCAL \cite{nickel2016} and ComplEx \cite{trouillon2016} offered alternative algebraic frameworks, using tensor factorization and complex-valued embeddings to capture richer, often symmetric or antisymmetric, semantic interactions. A pivotal conceptual leap came with rotational models like RotatE \cite{sun2019}, which elegantly modeled relations as rotations in complex space, providing a unified mechanism to capture symmetry, anti-symmetry, inversion, and composition simultaneously. This marked a significant departure from the limitations of purely translational or bilinear models, as highlighted by surveys classifying KGEs based on their underlying mathematical spaces \cite{cao2022} and geometric transformations \cite{ge2023}. This foundational understanding culminated in unifying frameworks such as HolmE \cite{zheng2024}, which theoretically demonstrated that prominent models like TransE and RotatE are special cases of a more general Riemannian KGE, inherently closed under composition and robustly modeling long-tail composition patterns.

Building on these geometric foundations, the field expanded into advanced non-Euclidean spaces and generalized transformations to address specific structural challenges. The inherent capacity of hyperbolic spaces to represent hierarchical structures more efficiently than Euclidean spaces led to models like those leveraging the Poincaré Ball \cite{pan2021} and Lorentz hyperbolic space \cite{liang2024}, which perform operations directly in these spaces to capture fine-grained hierarchical semantics. The exploration of Lie groups, such as embedding on a torus in TorusE \cite{ebisu2017}, aimed to resolve regularization conflicts and improve efficiency. The quest for universal expressiveness further led to compound operations, as seen in CompoundE \cite{ge2022}, which generalized geometric transformations by combining translation, rotation, and scaling. This trajectory towards more versatile geometric modeling reached new heights with frameworks like GoldE \cite{li2024}, which introduced universal orthogonal parameterization based on generalized Householder reflections. GoldE represents a significant advancement by generalizing orthogonal transformations across both dimensions and geometries (Euclidean, elliptic, hyperbolic), allowing for superior modeling of the inherent topological heterogeneity of real-world KGs. This continuous push for mathematically richer frameworks underscores the commitment to accurately represent intricate relational semantics and complex graph structures. Even more abstract theoretical explorations, such as Knowledge Sheaves \cite{gebhart2021gtp}, demonstrate the depth of analytical inquiry into the fundamental nature of KGE.

Beyond purely geometric transformations, a major paradigm shift involved integrating richer contextual information and leveraging advanced neural architectures. Early efforts recognized the limitations of relying solely on triple-level interactions, leading to models that incorporated auxiliary information such as textual descriptions (e.g., SSP \cite{xiao2016}), entity types (e.g., TransET \cite{wang2021}, TaKE \cite{he2023}), and logical rules (e.g., RUGE \cite{guo2017}, RulE \cite{tang2022}). These integrations provided valuable context, improved semantic consistency, and addressed data sparsity, particularly in incomplete KGs. The advent of deep learning architectures further revolutionized KGE. Convolutional Neural Networks (CNNs), as adopted by ConvE \cite{dettmers2018}, enabled the extraction of rich, non-linear features, moving beyond fixed scoring functions to data-driven feature learning. Graph Neural Networks (GNNs) became pivotal for aggregating multi-hop structural information from an entity's neighborhood, exemplified by R-GCN \cite{zhang2020} and Logic Attention Network (LAN) \cite{wang2018}, which enabled inductive learning for unseen entities. More recently, Transformer architectures, such as TGformer \cite{shi2025}, have been adapted to capture global and local structural features through self-attention mechanisms, further enhancing context-rich embeddings and generalization capabilities. The synergy with Large Language Models (LLMs) represents a cutting-edge trend, integrating language semantics with structural embeddings for even deeper contextual understanding \cite{shen2022}.

The field has also made significant strides in addressing the dynamic nature of knowledge and the practical challenges of real-world deployment. Temporal Knowledge Graph Embedding (TKGE) emerged as a critical sub-field, moving from early geometric approaches like HyTE \cite{dasgupta2018} (using hyperplanes for timestamps) and tensor decomposition methods \cite{lin2020} to more sophisticated rotation-based models like TeRo \cite{xu2020} and ChronoR \cite{sadeghian2021}, which elegantly capture temporal evolution in complex or k-dimensional spaces. Recent innovations have pushed into multi-curvature spaces (e.g., MADE \cite{wang2024}, IME \cite{wang2024}) and GNN-based approaches (e.g., TARGAT \cite{xie2023}) to model intricate temporal dependencies and geometric complexities. Concurrently, substantial efforts have focused on improving training robustness, efficiency, and scalability for massive KGs. This includes advanced negative sampling strategies (e.g., NSCaching \cite{zhang2018}, MANS \cite{zhang2023}), model compression techniques (e.g., DualDE \cite{zhu2020}), and distributed learning paradigms like Federated KGE \cite{zhang2024}. The utility of KGE has expanded far beyond simple link prediction, demonstrating its versatility in crucial downstream applications such as entity alignment (e.g., BootEA \cite{sun2018}, MultiKE \cite{zhang2019}), recommendation systems (e.g., RKGE \cite{sun2018}, Cross-Domain KGE \cite{liu2023}), and question answering (e.g., KEQA \cite{huang2019}). Furthermore, KGEs have proven valuable for broader data mining tasks \cite{portisch20221rd} and semantic querying on scholarly data \cite{tran2019j42}, showcasing their role in enabling intelligent information retrieval and decision-making across diverse domains.

In essence, the journey of KGE research reflects a profound commitment to developing increasingly powerful and versatile knowledge representation techniques. From initial geometric intuitions, the field has embraced the complexity of non-Euclidean spaces, the power of neural architectures for contextual learning, and the necessity of modeling dynamic knowledge. This continuous evolution, driven by theoretical advancements and practical demands, underscores KGE's pivotal role in bridging symbolic knowledge with modern machine learning, pushing towards more robust, intelligent, and context-aware knowledge systems.
\subsection{Remaining Challenges and Future Research Avenues}
\label{sec:8\_2\_remaining\_challenges\_\_and\_\_future\_research\_avenues}

The journey of Knowledge Graph Embedding research, from foundational geometric models to advanced neural architectures and temporal extensions, has profoundly enhanced our ability to represent and reason with complex knowledge. However, the field now stands at a critical juncture, defined by a set of interconnected grand challenges and transformative future research avenues. The persistent demand for \textbf{robust scalability} for truly massive, dynamic, and distributed KGs remains paramount. While parallel training techniques \cite{kochsiek2021} and federated learning paradigms \cite{zhang2024, hu20230kr, huang2023grx} offer promising paths, they introduce inherent trade-offs. For instance, distributed training, while addressing data volume and privacy concerns, complicates the management of semantic disparity across clients \cite{zhang2024} and necessitates novel defenses against privacy threats \cite{hu20230kr}. This highlights a fundamental tension: solutions for scalability often introduce new vulnerabilities or complexities in other areas, such as managing communication overhead or ensuring data consistency.

Simultaneously, \textbf{enhancing robustness against noise, incompleteness, and adversarial attacks} is crucial for trustworthy AI. Real-world KGs are imperfect, and while methods like logical rule integration \cite{tang2022} and consistency constraints derived from knowledge sheaves \cite{gebhart2021gtp} improve resilience, models must become intrinsically more robust. The challenge here is to achieve this without sacrificing model expressiveness or interpretability, as highly robust models can sometimes be more opaque or require more complex training regimes, as seen in the impact of hyperparameters on embedding quality \cite{lloyd2022}. This directly links to the \textbf{persistent demand for greater interpretability and explainability}. Moving beyond post-hoc explanations, the goal is to develop \textit{inherently interpretable} models \cite{daruna2022dmk, kurokawa2021f4f} where predictions are transparent and logic-grounded, perhaps by deeper integration of symbolic reasoning with neural approaches \cite{tang2022, gutirrezbasulto2018oi0}. This often implies a trade-off with the complexity required for capturing nuanced, diverse semantics or integrating multimodal information.

Looking ahead, the future of KGE will be shaped by \textbf{unified perceptual-symbolic reasoning}, integrating diverse modalities like text, images, and audio. This requires sophisticated semantic alignment and joint representation learning, which in turn demands more \textbf{autonomous and adaptive KGE systems}. Such systems, leveraging insights from the mathematical properties of representation spaces \cite{cao2022} and hyperparameter sensitivities \cite{lloyd2022}, could self-optimize architectures and learning strategies, reducing manual effort. However, the pursuit of such advanced capabilities must be balanced with the \textbf{crucial need for ethically aligned, fair, and privacy-preserving embedding techniques}. The integration of multimodal data, for example, can exacerbate existing biases, while automated systems might inadvertently optimize for performance at the expense of fairness or privacy. Federated learning, while offering privacy benefits, still requires robust defenses \cite{hu20230kr} and careful consideration of structural fairness across clients \cite{zhang2024}.

Therefore, the next generation of knowledge graph intelligence demands a holistic approach. It requires navigating the intricate interdependencies and inherent trade-offs between scalability, robustness, and interpretability, while simultaneously pushing the boundaries of multimodal integration, autonomous adaptation, and ethical responsibility. This synthesis underscores that impactful contributions will emerge from interdisciplinary efforts that not only advance theoretical foundations but also rigorously address the practical, societal implications of deploying KGE in real-world intelligent systems.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{377}

\bibitem{sun2018}
Zequn Sun, Wei Hu, Qingheng Zhang, et al. (2018). \textit{Bootstrapping Entity Alignment with Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{dasgupta2018}
S. Dasgupta, Swayambhu Nath Ray, and P. Talukdar (2018). \textit{HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2023}
Mingyang Chen, Wen Zhang, Zhen Yao, et al. (2023). \textit{Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2023}
Yang Yang, Chubing Zhang, Xin Song, et al. (2023). \textit{Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation}. ACM Trans. Inf. Syst..

\bibitem{jia2015}
Yantao Jia, Yuanzhuo Wang, Hailun Lin, et al. (2015). \textit{Locally Adaptive Translation for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{lloyd2022}
Oliver Lloyd, Yi Liu, and T. Gaunt (2022). \textit{Assessing the effects of hyperparameters on knowledge graph embedding quality}. Journal of Big Data.

\bibitem{wu2021}
Junkang Wu, Wentao Shi, Xuezhi Cao, et al. (2021). \textit{DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network}. International Conference on Information and Knowledge Management.

\bibitem{xu2019}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2019). \textit{Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition}. arXiv.org.

\bibitem{shan2018}
Yingchun Shan, Chenyang Bu, Xiaojian Liu, et al. (2018). \textit{Confidence-Aware Negative Sampling Method for Noisy Knowledge Graph Embedding}. International Conference on Big Knowledge.

\bibitem{zheng2024}
Zhuoxun Zheng, Baifan Zhou, Hui Yang, et al. (2024). \textit{Knowledge graph embedding closed under composition}. Data mining and knowledge discovery.

\bibitem{he2023}
Peng He, Gang Zhou, Yao Yao, et al. (2023). \textit{A type-augmented knowledge graph embedding framework for knowledge graph completion}. Scientific Reports.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{TransG : A Generative Model for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{guo2017}
Shu Guo, Quan Wang, Lihong Wang, et al. (2017). \textit{Knowledge Graph Embedding with Iterative Guidance from Soft Rules}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021}
Mingyang Chen, Wen Zhang, Yushan Zhu, et al. (2021). \textit{Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2023}
Guang-pu Li, Zequn Sun, Wei Hu, et al. (2023). \textit{Position-Aware Relational Transformer for Knowledge Graph Embedding}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{zhou2023}
Xiaochi Zhou, Shaocong Zhang, Mehal Agarwal, et al. (2023). \textit{Marie and BERT—A Knowledge Graph Embedding Based Question Answering System for Chemistry}. ACS Omega.

\bibitem{xiang2021}
Yuejia Xiang, Ziheng Zhang, Jiaoyan Chen, et al. (2021). \textit{OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding}. Findings.

\bibitem{cao2022}
Jiahang Cao, Jinyuan Fang, Zaiqiao Meng, et al. (2022). \textit{Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces}. ACM Computing Surveys.

\bibitem{wang2021}
Peng Wang, Jing Zhou, Yuzhang Liu, et al. (2021). \textit{TransET: Knowledge Graph Embedding with Entity Types}. Electronics.

\bibitem{guo2020}
Shu Guo, Lin Li, Zhen Hui, et al. (2020). \textit{Knowledge Graph Embedding Preserving Soft Logical Regularity}. International Conference on Information and Knowledge Management.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification}. Knowledge-Based Systems.

\bibitem{shen2022}
Jianhao Shen, Chenguang Wang, Linyuan Gong, et al. (2022). \textit{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion}. International Conference on Computational Linguistics.

\bibitem{hu2024}
Kairong Hu, Xiaozhi Zhu, Hai Liu, et al. (2024). \textit{Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation for Knowledge Graph Embedding Learning}. IEEE transactions on consumer electronics.

\bibitem{liu2024}
Yang Liu, Huang Fang, Yunfeng Cai, et al. (2024). \textit{MQuinE: a Cure for “Z-paradox” in Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2019}
Yongqi Zhang, Quanming Yao, Wenyuan Dai, et al. (2019). \textit{AutoSF: Searching Scoring Functions for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{yang2019}
Shihui Yang, Jidong Tian, Honglun Zhang, et al. (2019). \textit{TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics}. International Joint Conference on Artificial Intelligence.

\bibitem{xie2023}
Zhiwen Xie, Runjie Zhu, Jin Liu, et al. (2023). \textit{TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{wang2024}
Jiapu Wang, Boyue Wang, Junbin Gao, et al. (2024). \textit{MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion}. IEEE Transactions on Cybernetics.

\bibitem{xiao2019}
Han Xiao, Yidong Chen, and X. Shi (2019). \textit{Knowledge Graph Embedding Based on Multi-View Clustering Framework}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sachan2020}
Mrinmaya Sachan (2020). \textit{Knowledge Graph Embedding Compression}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{madushanka2024}
Tiroshan Madushanka, and R. Ichise (2024). \textit{Negative Sampling in Knowledge Graph Representation Learning: A Review}. arXiv.org.

\bibitem{zhu2022}
Chaoyu Zhu, Zhihao Yang, Xiaoqiong Xia, et al. (2022). \textit{Multimodal reasoning based on knowledge graph embedding for specific diseases}. Bioinform..

\bibitem{liang2024}
Qiuyu Liang, Weihua Wang, F. Bao, et al. (2024). \textit{Fully Hyperbolic Rotation for Knowledge Graph Embedding}. European Conference on Artificial Intelligence.

\bibitem{li2024}
Li, Yuyi Ao, and Jingrui He (2024). \textit{SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{ebisu2017}
Takuma Ebisu, and R. Ichise (2017). \textit{TorusE: Knowledge Graph Embedding on a Lie Group}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2021}
Zhao Zhang, Fuzhen Zhuang, Hengshu Zhu, et al. (2021). \textit{Towards Robust Knowledge Graph Embedding via Multi-Task Reinforcement Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{huang2019}
Xiao Huang, Jingyuan Zhang, Dingcheng Li, et al. (2019). \textit{Knowledge Graph Embedding Based Question Answering}. Web Search and Data Mining.

\bibitem{tang2019}
Yun Tang, Jing Huang, Guangtao Wang, et al. (2019). \textit{Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sun2018}
Zhu Sun, Jie Yang, Jie Zhang, et al. (2018). \textit{Recurrent knowledge graph embedding for effective recommendation}. ACM Conference on Recommender Systems.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding: An Overview}. APSIPA Transactions on Signal and Information Processing.

\bibitem{wang2020}
Rui Wang, Bicheng Li, Shengwei Hu, et al. (2020). \textit{Knowledge Graph Embedding via Graph Attenuated Attention Networks}. IEEE Access.

\bibitem{li2022}
Rui Li, Jianan Zhao, Chaozhuo Li, et al. (2022). \textit{HousE: Knowledge Graph Embedding with Householder Parameterization}. International Conference on Machine Learning.

\bibitem{zhang2019}
Qingheng Zhang, Zequn Sun, Wei Hu, et al. (2019). \textit{Multi-view Knowledge Graph Embedding for Entity Alignment}. International Joint Conference on Artificial Intelligence.

\bibitem{tang2022}
Xiaojuan Tang, Song-Chun Zhu, Yitao Liang, et al. (2022). \textit{RulE: Knowledge Graph Reasoning with Rule Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lv2018}
Xin Lv, Lei Hou, Juan-Zi Li, et al. (2018). \textit{Differentiating Concepts and Instances for Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2025}
Jie Chen, Yinlong Wang, Shu Zhao, et al. (2025). \textit{Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction}. ACM Trans. Asian Low Resour. Lang. Inf. Process..

\bibitem{qian2021}
Jing Qian, Gangmin Li, Katie Atkinson, et al. (2021). \textit{Understanding Negative Sampling in Knowledge Graph Embedding}. International Journal of Artificial Intelligence & Applications.

\bibitem{dai2020}
Yuanfei Dai, Shiping Wang, N. Xiong, et al. (2020). \textit{A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks}. Electronics.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation}. IEEE transactions on fuzzy systems.

\bibitem{yan2022}
Qi Yan, Jiaxin Fan, Mohan Li, et al. (2022). \textit{A Survey on Knowledge Graph Embedding}. International Conference on Data Science in Cyberspace.

\bibitem{zhang2023}
Yichi Zhang, Mingyang Chen, and Wen Zhang (2023). \textit{Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{li2021}
Ren Li, Yanan Cao, Qiannan Zhu, et al. (2021). \textit{How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2025}
Qingqing Yang, Min He, Zhongwen Li, et al. (2025). \textit{A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction}. IEEE transactions on consumer electronics.

\bibitem{wang2019}
Quan Wang, Pingping Huang, Haifeng Wang, et al. (2019). \textit{CoKE: Contextualized Knowledge Graph Embedding}. arXiv.org.

\bibitem{di2023}
Shimin Di, and Lei Chen (2023). \textit{Message Function Search for Knowledge Graph Embedding}. The Web Conference.

\bibitem{jia2017}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2017). \textit{Knowledge Graph Embedding}. ACM Transactions on the Web.

\bibitem{choudhary2021}
Shivani Choudhary, Tarun Luthra, Ashima Mittal, et al. (2021). \textit{A Survey of Knowledge Graph Embedding and Their Applications}. arXiv.org.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction}. International Joint Conference on Artificial Intelligence.

\bibitem{hu2024}
Lei Hu, Wenwen Li, Jun Xu, et al. (2024). \textit{GeoEntity-type constrained knowledge graph embedding for predicting natural-language spatial relations}. International Journal of Geographical Information Science.

\bibitem{wang2014}
Zhen Wang, Jianwen Zhang, Jianlin Feng, et al. (2014). \textit{Knowledge Graph Embedding by Translating on Hyperplanes}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2020}
Yushan Zhu, Wen Zhang, Mingyang Chen, et al. (2020). \textit{DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning}. Web Search and Data Mining.

\bibitem{ali2020}
Mehdi Ali, M. Berrendorf, Charles Tapley Hoyt, et al. (2020). \textit{Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{mohamed2020}
Sameh K. Mohamed, A. Nounu, and V. Nováček (2020). \textit{Biological applications of knowledge graph embedding models}. Briefings Bioinform..

\bibitem{gao2020}
Chang Gao, Chengjie Sun, Lili Shan, et al. (2020). \textit{Rotate3D: Representing Relations as Rotations in Three-Dimensional Space for Knowledge Graph Embedding}. International Conference on Information and Knowledge Management.

\bibitem{peng2021}
Xutan Peng, Guanyi Chen, Chenghua Lin, et al. (2021). \textit{Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis}. North American Chapter of the Association for Computational Linguistics.

\bibitem{shi2025}
Fobo Shi, Duantengchuan Li, Xiaoguang Wang, et al. (2025). \textit{TGformer: A Graph Transformer Framework for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph}. Applied intelligence (Boston).

\bibitem{rosso2020}
Paolo Rosso, Dingqi Yang, and P. Cudré-Mauroux (2020). \textit{Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction}. The Web Conference.

\bibitem{zhou2024}
Enyuan Zhou, Song Guo, Zhixiu Ma, et al. (2024). \textit{Poisoning Attack on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{xie2020}
Zhiwen Xie, Guangyou Zhou, Jin Liu, et al. (2020). \textit{ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{song2021}
Tengwei Song, Jie Luo, and Lei Huang (2021). \textit{Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{zhang2020}
Zhaoli Zhang, Zhifei Li, Hai Liu, et al. (2020). \textit{Multi-Scale Dynamic Convolutional Network for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{ge2022}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2022). \textit{CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations}. arXiv.org.

\bibitem{ren2020}
Feiliang Ren, Jucheng Li, Huihui Zhang, et al. (2020). \textit{Knowledge Graph Embedding with Atrous Convolution and Residual Learning}. International Conference on Computational Linguistics.

\bibitem{yuan2019}
Jun Yuan, Neng Gao, and Ji Xiang (2019). \textit{TransGate: Knowledge Graph Embedding with Shared Gate Structure}. AAAI Conference on Artificial Intelligence.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransA: An Adaptive Approach for Knowledge Graph Embedding}. arXiv.org.

\bibitem{sun2018}
Zhiqing Sun, Zhihong Deng, Jian-Yun Nie, et al. (2018). \textit{RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space}. International Conference on Learning Representations.

\bibitem{ji2015}
Guoliang Ji, Shizhu He, Liheng Xu, et al. (2015). \textit{Knowledge Graph Embedding via Dynamic Mapping Matrix}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lin2020}
Lifan Lin, and Kun She (2020). \textit{Tensor Decomposition-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{islam2023}
M. Islam, Diego Amaya-Ramirez, B. Maigret, et al. (2023). \textit{Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding}. Scientific Reports.

\bibitem{wang2021}
Haoyu Wang, Yaqing Wang, Defu Lian, et al. (2021). \textit{A Lightweight Knowledge Graph Embedding Framework for Efficient Inference and Storage}. International Conference on Information and Knowledge Management.

\bibitem{broscheit2020}
Samuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek, et al. (2020). \textit{LibKGE - A knowledge graph embedding library for reproducible research}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{fanourakis2022}
N. Fanourakis, Vasilis Efthymiou, D. Kotzinos, et al. (2022). \textit{Knowledge graph embedding methods for entity alignment: experimental review}. Data mining and knowledge discovery.

\bibitem{wang2018}
Peifeng Wang, Jialong Han, Chenliang Li, et al. (2018). \textit{Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{tabacof2019}
Pedro Tabacof, and Luca Costabello (2019). \textit{Probability Calibration for Knowledge Graph Embedding Models}. International Conference on Learning Representations.

\bibitem{pei2019}
Shichao Pei, Lu Yu, R. Hoehndorf, et al. (2019). \textit{Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference}. The Web Conference.

\bibitem{zhang2018}
Yongqi Zhang, Quanming Yao, Yingxia Shao, et al. (2018). \textit{NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{li2021}
Zelong Li, Jianchao Ji, Zuohui Fu, et al. (2021). \textit{Efficient Non-Sampling Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2022}
Guangtong Li, L. Siddharth, and Jianxi Luo (2022). \textit{Embedding knowledge graph of patent metadata to measure knowledge proximity}. J. Assoc. Inf. Sci. Technol..

\bibitem{ding2018}
Boyang Ding, Quan Wang, Bin Wang, et al. (2018). \textit{Improving Knowledge Graph Embedding Using Simple Constraints}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2022}
Xuanyu Zhang, Qing Yang, and Dongliang Xu (2022). \textit{TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{sun2024}
Hongliang Sun, Jinlan Liu, Can Wang, et al. (2024). \textit{Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning}. 2024 IEEE International Conference on Web Services (ICWS).

\bibitem{wang2024}
Jiapu Wang, Zheng Cui, Boyue Wang, et al. (2024). \textit{IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion}. The Web Conference.

\bibitem{modak2024}
S. Modak, Aakarsh Malhotra, Sarthak Malik, et al. (2024). \textit{CPa-WAC: Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{xiao2016}
Han Xiao, Minlie Huang, Lian Meng, et al. (2016). \textit{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2023}
Zhao Zhang, Zhanpeng Guan, Fuwei Zhang, et al. (2023). \textit{Weighted Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{guo2015}
Shu Guo, Quan Wang, Bin Wang, et al. (2015). \textit{Semantically Smooth Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xu2020}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2020). \textit{TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation}. International Conference on Computational Linguistics.

\bibitem{zheng2024}
Chenguang Zheng, Guanxian Jiang, Xiao Yan, et al. (2024). \textit{GE2: A General and Efficient Knowledge Graph Embedding Learning System}. Proc. ACM Manag. Data.

\bibitem{zhang2018}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2018). \textit{Knowledge Graph Embedding with Hierarchical Relation Structure}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhu2024}
Beibei Zhu, Ruolin Wang, Junyi Wang, et al. (2024). \textit{A survey: knowledge graph entity alignment research based on graph embedding}. Artificial Intelligence Review.

\bibitem{liu2023}
Jia Liu, Wei Huang, Tianrui Li, et al. (2023). \textit{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{choi2020}
S. Choi, Hyun-Je Song, and Seong-Bae Park (2020). \textit{An Approach to Knowledge Base Completion by a Committee-Based Knowledge Graph Embedding}. Applied Sciences.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding with 3D Compound Geometric Transformations}. APSIPA Transactions on Signal and Information Processing.

\bibitem{sadeghian2021}
A. Sadeghian, Mohammadreza Armandpour, Anthony Colas, et al. (2021). \textit{ChronoR: Rotation Based Temporal Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{liu2024}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Fast and Continual Knowledge Graph Embedding via Incremental LoRA}. International Joint Conference on Artificial Intelligence.

\bibitem{li2022}
Yizhi Li, Wei Fan, Chaochun Liu, et al. (2022). \textit{TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{rossi2020}
Andrea Rossi, D. Firmani, Antonio Matinata, et al. (2020). \textit{Knowledge Graph Embedding for Link Prediction}. ACM Transactions on Knowledge Discovery from Data.

\bibitem{li2023}
Jiang Li, Xiangdong Su, and Guanglai Gao (2023). \textit{TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{peng2020}
Yanhui Peng, and Jing Zhang (2020). \textit{LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction}. Industrial Conference on Data Mining.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding}. IEEE transactions on fuzzy systems.

\bibitem{zhang2024}
Qinggang Zhang, Junnan Dong, Qiaoyu Tan, et al. (2024). \textit{Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{kochsiek2021}
Adrian Kochsiek (2021). \textit{Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques}. Proceedings of the VLDB Endowment.

\bibitem{yang2021}
Han Yang, Leilei Zhang, Bingning Wang, et al. (2021). \textit{Cycle or Minkowski: Which is More Appropriate for Knowledge Graph Embedding?}. International Conference on Information and Knowledge Management.

\bibitem{shang2024}
Bin Shang, Yinliang Zhao, Jun Liu, et al. (2024). \textit{Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion}. AAAI Conference on Artificial Intelligence.

\bibitem{asmara2023}
S. M. Asmara, N. A. Sahabudin, Nor Syahidatul Nadiah Ismail, et al. (2023). \textit{A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links}. International Conference on Software Engineering and Computer Systems.

\bibitem{gregucci2023}
Cosimo Gregucci, M. Nayyeri, D. Hern'andez, et al. (2023). \textit{Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models}. The Web Conference.

\bibitem{pan2021}
Zhe Pan, and Peng Wang (2021). \textit{Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yoon2016}
Hee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, et al. (2016). \textit{A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2024}
Rui Li, Chaozhuo Li, Yanming Shen, et al. (2024). \textit{Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization}. International Conference on Machine Learning.

\bibitem{xiong2017zqu}
Chenyan Xiong, Russell Power, and Jamie Callan (2017). \textit{Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding}. The Web Conference.

\bibitem{gong2020b2k}
Fan Gong, Meng Wang, Haofen Wang, et al. (2020). \textit{SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation}. Big Data Research.

\bibitem{zhou2022ehi}
Bin Zhou, Xingwang Shen, Yuqian Lu, et al. (2022). \textit{Semantic-aware event link reasoning over industrial knowledge graph embedding time series data}. International Journal of Production Research.

\bibitem{le2022ji8}
Thanh-Binh Le, N. Le, and H. Le (2022). \textit{Knowledge graph embedding by relational rotation and complex convolution for link prediction}. Expert systems with applications.

\bibitem{zhou2022vgb}
Zhehui Zhou, Can Wang, Yan Feng, et al. (2022). \textit{JointE: Jointly utilizing 1D and 2D convolution for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{xu2019t6b}
Da Xu, Chuanwei Ruan, Evren Körpeoglu, et al. (2019). \textit{Product Knowledge Graph Embedding for E-commerce}. Web Search and Data Mining.

\bibitem{mezni20218ml}
Haithem Mezni, D. Benslimane, and Ladjel Bellatreche (2021). \textit{Context-Aware Service Recommendation Based on Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{do2021mw0}
P. Do, and Truong H. V. Phan (2021). \textit{Developing a BERT based triple classification model using knowledge graph embedding for question answering system}. Applied intelligence (Boston).

\bibitem{mai2020ei3}
Gengchen Mai, K. Janowicz, Ling Cai, et al. (2020). \textit{SE‐KGE: A location‐aware Knowledge Graph Embedding model for Geographic Question Answering and Spatial Semantic Lifting}. Trans. GIS.

\bibitem{zhang2022eab}
Jiarui Zhang, Jian Huang, Jialong Gao, et al. (2022). \textit{Knowledge graph embedding by logical-default attention graph convolution neural network for link prediction}. Information Sciences.

\bibitem{sosa2019ih0}
Daniel N. Sosa, Alexander Derry, Margaret Guo, et al. (2019). \textit{A Literature-Based Knowledge Graph Embedding Method for Identifying Drug Repurposing Opportunities in Rare Diseases}. bioRxiv.

\bibitem{guan2019pr4}
Niannian Guan, Dandan Song, and L. Liao (2019). \textit{Knowledge graph embedding with concepts}. Knowledge-Based Systems.

\bibitem{fan2014g7s}
M. Fan, Qiang Zhou, E. Chang, et al. (2014). \textit{Transition-based Knowledge Graph Embedding with Relational Mapping Properties}. Pacific Asia Conference on Language, Information and Computation.

\bibitem{zhang20190zu}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Data Poisoning Attack against Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{chen2022mxn}
Qi Chen, Wei Wang, Kaizhu Huang, et al. (2022). \textit{Zero-Shot Text Classification via Knowledge Graph Embedding for Social Media Data}. IEEE Internet of Things Journal.

\bibitem{wang2022hwx}
Xin Wang, Shengfei Lyu, Xiangyu Wang, et al. (2022). \textit{Temporal knowledge graph embedding via sparse transfer matrix}. Information Sciences.

\bibitem{chen20226e4}
Mingyang Chen, Wen Zhang, Zonggang Yuan, et al. (2022). \textit{Federated knowledge graph completion via embedding-contrastive learning}. Knowledge-Based Systems.

\bibitem{abusalih2020gdu}
Bilal Abu-Salih, Marwan Al-Tawil, Ibrahim Aljarah, et al. (2020). \textit{Relational Learning Analysis of Social Politics using Knowledge Graph Embedding}. Data mining and knowledge discovery.

\bibitem{fang2022wp6}
Haichuan Fang, Youwei Wang, Zhen Tian, et al. (2022). \textit{Learning knowledge graph embedding with a dual-attention embedding network}. Expert systems with applications.

\bibitem{elebi2019bzc}
R. Çelebi, Hüseyin Uyar, Erkan Yasar, et al. (2019). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction in realistic settings}. BMC Bioinformatics.

\bibitem{sha2019i3a}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Hierarchical attentive knowledge graph embedding for personalized recommendation}. Electronic Commerce Research and Applications.

\bibitem{li2021ro5}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Recalibration convolutional networks for learning interaction knowledge graph embedding}. Neurocomputing.

\bibitem{xiao20151fj}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransG : A Generative Mixture Model for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2021wg7}
Fei Zhang, Bo Sun, Xiaolin Diao, et al. (2021). \textit{Prediction of adverse drug reactions based on knowledge graph embedding}. BMC Medical Informatics and Decision Making.

\bibitem{wang20186zs}
Guanying Wang, Wen Zhang, Ruoxu Wang, et al. (2018). \textit{Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2021x10}
Xinyu Li, P. Zheng, Jinsong Bao, et al. (2021). \textit{Achieving cognitive mass personalization via the self-X cognitive manufacturing network: An industrial-knowledge-graph- and graph-embedding-enabled pathway}. Engineering.

\bibitem{wang202110w}
Xin Wang, Xiao Liu, Jin Liu, et al. (2021). \textit{A novel knowledge graph embedding based API recommendation method for Mashup development}. World wide web (Bussum).

\bibitem{gutirrezbasulto2018oi0}
Víctor Gutiérrez-Basulto, and S. Schockaert (2018). \textit{From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{portisch20221rd}
Jan Portisch, Nicolas Heist, and Heiko Paulheim (2022). \textit{Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction - two sides of the same coin?}. Semantic Web.

\bibitem{zhang2022muu}
Fuwei Zhang, Zhao Zhang, Xiang Ao, et al. (2022). \textit{Along the Time: Timeline-traced Embedding for Temporal Knowledge Graph Completion}. International Conference on Information and Knowledge Management.

\bibitem{feng2016dp7}
Jun Feng, Minlie Huang, Mingdong Wang, et al. (2016). \textit{Knowledge Graph Embedding by Flexible Translation}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{liu2021wqa}
Jia Liu, Tianrui Li, Shenggong Ji, et al. (2021). \textit{Urban Flow Pattern Mining Based on Multi-Source Heterogeneous Data Fusion and Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sang2019gjl}
Shengtian Sang, Zhihao Yang, Xiaoxia Liu, et al. (2019). \textit{GrEDeL: A Knowledge Graph Embedding Based Method for Drug Discovery From Biomedical Literatures}. IEEE Access.

\bibitem{wang2017yjq}
M. Wang, Mengyue Liu, Jun Liu, et al. (2017). \textit{Safe Medicine Recommendation via Medical Knowledge Graph Embedding}. arXiv.org.

\bibitem{jiang20219xl}
Dan Jiang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Kernel multi-attention neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2022fu5}
Yang Liu, Zequn Sun, Guang-pu Li, et al. (2022). \textit{I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning}. International Conference on Information and Knowledge Management.

\bibitem{khan202236g}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{Similarity attributed knowledge graph embedding enhancement for item recommendation}. Information Sciences.

\bibitem{mezni2021ezn}
Haithem Mezni (2021). \textit{Temporal Knowledge Graph Embedding for Effective Service Recommendation}. IEEE Transactions on Services Computing.

\bibitem{zhang2021wix}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Structural context-based knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{huang2021u42}
Xuqian Huang, Jiuyang Tang, Zhen Tan, et al. (2021). \textit{Knowledge graph embedding by relational and entity rotation}. Knowledge-Based Systems.

\bibitem{pavlovic2022qte}
Aleksandar Pavlovic, and Emanuel Sallinger (2022). \textit{ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion}. International Conference on Learning Representations.

\bibitem{wang20213kg}
Shensi Wang, Kun Fu, Xian Sun, et al. (2021). \textit{Hierarchical-aware relation rotational knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{zhang2019rlm}
Shuai Zhang, Yi Tay, Lina Yao, et al. (2019). \textit{Quaternion Knowledge Graph Embedding}. arXiv.org.

\bibitem{mai20195rp}
Gengchen Mai, Bo Yan, K. Janowicz, et al. (2019). \textit{Relaxing Unanswerable Geographic Questions Using A Spatially Explicit Knowledge Graph Embedding Model}. Agile Conference.

\bibitem{han2018tzc}
Zhuobing Han, Xiaohong Li, Hongtao Liu, et al. (2018). \textit{DeepWeak: Reasoning common software weaknesses via knowledge graph embedding}. IEEE International Conference on Software Analysis, Evolution, and Reengineering.

\bibitem{wang2022fvx}
Feiyang Wang, Zhongbao Zhang, Li Sun, et al. (2022). \textit{DiriE: Knowledge Graph Embedding with Dirichlet Distribution}. The Web Conference.

\bibitem{ferrari2022r82}
Ilaria Ferrari, Giacomo Frisoni, Paolo Italiani, et al. (2022). \textit{Comprehensive Analysis of Knowledge Graph Embedding Techniques Benchmarked on Link Prediction}. Electronics.

\bibitem{fu2022df2}
Guirong Fu, Zhao Meng, Zhen Han, et al. (2022). \textit{TempCaps: A Capsule Network-based Embedding Model for Temporal Knowledge Graph Completion}. SPNLP.

\bibitem{wu2018c4b}
Yanrong Wu, and Zhichun Wang (2018). \textit{Knowledge Graph Embedding with Numeric Attributes of Entities}. Rep4NLP@ACL.

\bibitem{zhang202121t}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Knowledge graph embedding by reflection transformation}. Knowledge-Based Systems.

\bibitem{mohamed2019meq}
Sameh K. Mohamed, V. Nováček, P. Vandenbussche, et al. (2019). \textit{Loss Functions in Knowledge Graph Embedding Models}. DL4KG@ESWC.

\bibitem{xin2022dam}
Kexuan Xin, Zequn Sun, Wen Hua, et al. (2022). \textit{Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning and Embedding}. International Conference on Information and Knowledge Management.

\bibitem{nie20195gc}
Binling Nie, and Shouqian Sun (2019). \textit{Knowledge graph embedding via reasoning over entities, relations, and text}. Future generations computer systems.

\bibitem{liu2018kvd}
Yang Liu, Qingguo Zeng, Huanrui Yang, et al. (2018). \textit{Stock Price Movement Prediction from Financial News with Deep Learning and Knowledge Graph Embedding}. Pacific Rim Knowledge Acquisition Workshop.

\bibitem{ni2020ruj}
Chien-Chun Ni, Kin Sum Liu, and Nicolas Torzec (2020). \textit{Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph}. The Web Conference.

\bibitem{li20215pu}
Chen Li, Xutan Peng, Yuhang Niu, et al. (2021). \textit{Learning graph attention-aware knowledge graph embedding}. Neurocomputing.

\bibitem{yu2019qgs}
S. Yu, Sujit Rokka Chhetri, A. Canedo, et al. (2019). \textit{Pykg2vec: A Python Library for Knowledge Graph Embedding}. Journal of machine learning research.

\bibitem{fatemi2018e6v}
Bahare Fatemi, Siamak Ravanbakhsh, and D. Poole (2018). \textit{Improved Knowledge Graph Embedding using Background Taxonomic Information}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021i5t}
Zhuo Chen, Mi-Yen Yeh, and Tei-Wei Kuo (2021). \textit{PASSLEAF: A Pool-bAsed Semi-Supervised LEArning Framework for Uncertain Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{dong2022c6z}
Sicong Dong, Xupeng Miao, Peng Liu, et al. (2022). \textit{HET-KG: Communication-Efficient Knowledge Graph Embedding Training via Hotness-Aware Cache}. IEEE International Conference on Data Engineering.

\bibitem{lu20206x1}
Fengyuan Lu, Peijin Cong, and Xinli Huang (2020). \textit{Utilizing Textual Information in Knowledge Graph Embedding: A Survey of Methods and Applications}. IEEE Access.

\bibitem{li2022nr8}
Weidong Li, Rong Peng, and Zhi Li (2022). \textit{Improving knowledge graph completion via increasing embedding interactions}. Applied intelligence (Boston).

\bibitem{luo2015df2}
Yuanfei Luo, Quan Wang, Bin Wang, et al. (2015). \textit{Context-Dependent Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhou20216m0}
Xiaohan Zhou, Yunhui Yi, and Geng Jia (2021). \textit{Path-RotatE: Knowledge Graph Embedding by Relational Rotation of Path in Complex Space}. International Conference on Innovative Computing and Cloud Computing.

\bibitem{zhao202095o}
Feng Zhao, Haoran Sun, Langjunqing Jin, et al. (2020). \textit{Structure-augmented knowledge graph embedding for sparse data with rule learning}. Computer Communications.

\bibitem{jia201870f}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2018). \textit{Path-specific knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{mai2018u0h}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines}. Semdeep/NLIWoD@ISWC.

\bibitem{li201949n}
Dingcheng Li, Siamak Zamani, Jingyuan Zhang, et al. (2019). \textit{Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process}. North American Chapter of the Association for Computational Linguistics.

\bibitem{tang2020ufr}
Xiaoli Tang, Rui Yuan, Qianyu Li, et al. (2020). \textit{Timespan-Aware Dynamic Knowledge Graph Embedding by Incorporating Temporal Evolution}. IEEE Access.

\bibitem{guo2022qtv}
Lingbing Guo, Qiang Zhang, Zequn Sun, et al. (2022). \textit{Understanding and Improving Knowledge Graph Embedding for Entity Alignment}. International Conference on Machine Learning.

\bibitem{jiang202235y}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2022). \textit{Multiview feature augmented neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu201918i}
Yu Liu, Wen Hua, Kexuan Xin, et al. (2019). \textit{Context-Aware Temporal Knowledge Graph Embedding}. WISE.

\bibitem{zhang2020s4x}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2020). \textit{Knowledge graph embedding by translating in time domain space for link prediction}. Knowledge-Based Systems.

\bibitem{chang20179yf}
Liang Chang, Manli Zhu, T. Gu, et al. (2017). \textit{Knowledge Graph Embedding by Dynamic Translation}. IEEE Access.

\bibitem{lee2022hr9}
Yeon-Chang Lee, and Sang-Wook Kim (2022). \textit{THOR: Self-Supervised Temporal Knowledge Graph Embedding via Three-Tower Graph Convolutional Networks}. Industrial Conference on Data Mining.

\bibitem{zhang2022fpm}
Yongqi Zhang, Zhanke Zhou, Quanming Yao, et al. (2022). \textit{Efficient Hyper-parameter Search for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{liu2019e1u}
Chang Liu, Lun Li, Xiaolu Yao, et al. (2019). \textit{A Survey of Recommendation Algorithms Based on Knowledge Graph Embedding}. 2019 IEEE International Conference on Computer Science and Educational Informatization (CSEI).

\bibitem{song2021fnl}
Wei Song, Jingjin Guo, Ruiji Fu, et al. (2021). \textit{A Knowledge Graph Embedding Approach for Metaphor Processing}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{gradgyenge2017xdy}
László Grad-Gyenge, A. Kiss, and P. Filzmoser (2017). \textit{Graph Embedding Based Recommendation Techniques on the Knowledge Graph}. User Modeling, Adaptation, and Personalization.

\bibitem{zhou20218bt}
Xiaofei Zhou, Lingfeng Niu, Qiannan Zhu, et al. (2021). \textit{Knowledge Graph Embedding by Double Limit Scoring Loss}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{chen20210ah}
Yao Chen, Jiangang Liu, Zhe Zhang, et al. (2021). \textit{MöbiusE: Knowledge Graph Embedding on Möbius Ring}. Knowledge-Based Systems.

\bibitem{zhang2020i7j}
Yongqi Zhang, Quanming Yao, and Lei Chen (2020). \textit{Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{boschin2020ki4}
Armand Boschin (2020). \textit{TorchKGE: Knowledge Graph Embedding in Python and PyTorch}. arXiv.org.

\bibitem{wang20199fe}
P. Wang, D. Dou, Fangzhao Wu, et al. (2019). \textit{Logic Rules Powered Knowledge Graph Embedding}. arXiv.org.

\bibitem{myklebust201941l}
E. B. Myklebust, Ernesto Jiménez-Ruiz, Jiaoyan Chen, et al. (2019). \textit{Knowledge Graph Embedding for Ecotoxicological Effect Prediction}. International Workshop on the Semantic Web.

\bibitem{kartheek2021aj7}
Miriyala Kartheek, and G. Sajeev (2021). \textit{Building Semantic Based Recommender System Using Knowledge Graph Embedding}. International Conference on Intelligent Information Processing.

\bibitem{sha2019plw}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Attentive Knowledge Graph Embedding for Personalized Recommendation}. arXiv.org.

\bibitem{lu2020x6y}
Haonan Lu, and Hailin Hu (2020). \textit{DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2020c15}
Siheng Zhang, Zhengya Sun, and Wensheng Zhang (2020). \textit{Improve the translational distance models for knowledge graph embedding}. Journal of Intelligence and Information Systems.

\bibitem{li2020ek4}
Mingda Li, Zhengya Sun, Siheng Zhang, et al. (2020). \textit{Enhancing Knowledge Graph Embedding with Relational Constraints}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{li2020he5}
Jian Li, Zhuoming Xu, Yan Tang, et al. (2020). \textit{Deep Hybrid Knowledge Graph Embedding for Top-N Recommendation}. Web Information System and Application Conference.

\bibitem{kim2020zu3}
Kuekyeng Kim, Yuna Hur, Gyeongmin Kim, et al. (2020). \textit{GREG: A Global Level Relation Extraction with Knowledge Graph Embedding}. Applied Sciences.

\bibitem{zhu2018l0u}
Jizhao Zhu, Yantao Jia, Jun Xu, et al. (2018). \textit{Modeling the Correlations of Relations for Knowledge Graph Embedding}. Journal of Computational Science and Technology.

\bibitem{do20184o2}
Kien Do, T. Tran, and S. Venkatesh (2018). \textit{Knowledge Graph Embedding with Multiple Relation Projections}. International Conference on Pattern Recognition.

\bibitem{ma20194ua}
Yunpu Ma, Volker Tresp, Liming Zhao, et al. (2019). \textit{Variational Quantum Circuit Model for Knowledge Graph Embedding}. Advanced Quantum Technologies.

\bibitem{zhang2020wou}
Yuhang Zhang, Jun Wang, and Jie Luo (2020). \textit{Knowledge Graph Embedding Based Collaborative Filtering}. IEEE Access.

\bibitem{zhang2019hs5}
Wen Zhang, Shumin Deng, Han Wang, et al. (2019). \textit{XTransE: Explainable Knowledge Graph Embedding for Link Prediction with Lifestyles in e-Commerce}. Joint International Conference of Semantic Technology.

\bibitem{wang20198d2}
Zhihao Wang, and Xin Li (2019). \textit{Hybrid-TE: Hybrid Translation-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{tran20195x3}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective}. EDBT/ICDT Workshops.

\bibitem{xiong2018fof}
Shengwu Xiong, Weitao Huang, and P. Duan (2018). \textit{Knowledge Graph Embedding via Relation Paths and Dynamic Mapping Matrix}. ER Workshops.

\bibitem{radstok2021yup}
Wessel Radstok, M. Chekol, and M. Schäfer (2021). \textit{Are Knowledge Graph Embedding Models Biased, or Is it the Data That They Are Trained on?}. Wikidata@ISWC.

\bibitem{zhao2020o6z}
Ling Zhao, Hanhan Deng, L. Qiu, et al. (2020). \textit{Urban Multi-Source Spatio-Temporal Data Analysis Aware Knowledge Graph Embedding}. Symmetry.

\bibitem{zhang20182ey}
Maoyuan Zhang, Qi Wang, Wukui Xu, et al. (2018). \textit{Discriminative Path-Based Knowledge Graph Embedding for Precise Link Prediction}. European Conference on Information Retrieval.

\bibitem{jia20207dd}
Ningning Jia, Xiang Cheng, and Sen Su (2020). \textit{Improving Knowledge Graph Embedding Using Locally and Globally Attentive Relation Paths}. European Conference on Information Retrieval.

\bibitem{zhu2019ir6}
Qiannan Zhu, Xiaofei Zhou, P. Zhang, et al. (2019). \textit{A neural translating general hyperplane for knowledge graph embedding}. Journal of Computer Science.

\bibitem{wang2021dgy}
Shen Wang, Xiaokai Wei, C. D. Santos, et al. (2021). \textit{Knowledge Graph Representation via Hierarchical Hyperbolic Neural Graph Embedding}. 2021 IEEE International Conference on Big Data (Big Data).

\bibitem{ning20219et}
Zhiyuan Ning, Ziyue Qiao, Hao Dong, et al. (2021). \textit{LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding}. Pacific-Asia Conference on Knowledge Discovery and Data Mining.

\bibitem{sheikh20213qq}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2021). \textit{Knowledge Graph Embedding using Graph Convolutional Networks with Relation-Aware Attention}. arXiv.org.

\bibitem{rim2021s9a}
Wiem Ben Rim, Carolin (Haas) Lawrence, Kiril Gashteovski, et al. (2021). \textit{Behavioral Testing of Knowledge Graph Embedding Models for Link Prediction}. Conference on Automated Knowledge Base Construction.

\bibitem{zhang20179i2}
Chunhong Zhang, Miao Zhou, Xiao Han, et al. (2017). \textit{Knowledge Graph Embedding for Hyper-Relational Data}. Unpublished manuscript.

\bibitem{elebi20182bd}
R. Çelebi, Erkan Yasar, Hüseyin Uyar, et al. (2018). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction using Linked Open Data}. Workshop on Semantic Web Applications and Tools for Life Sciences.

\bibitem{garofalo20185g9}
Martina Garofalo, Maria Angela Pellegrino, Abdulrahman Altabba, et al. (2018). \textit{Leveraging Knowledge Graph Embedding Techniques for Industry 4.0 Use Cases}. arXiv.org.

\bibitem{wang201825m}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2018). \textit{Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network}. arXiv.org.

\bibitem{chung2021u2l}
Chanyoung Chung, and Joyce Jiyoung Whang (2021). \textit{Knowledge Graph Embedding via Metagraph Learning}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{tran2019j42}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space}. International Conference on Theory and Practice of Digital Libraries.

\bibitem{shi2017m2h}
Jun Shi, Huan Gao, G. Qi, et al. (2017). \textit{Knowledge Graph Embedding with Triple Context}. International Conference on Information and Knowledge Management.

\bibitem{zhang2017ixt}
Wen Zhang (2017). \textit{Knowledge Graph Embedding with Diversity of Structures}. The Web Conference.

\bibitem{zhu20196p1}
Ming-Yi Zhu, De-sheng Zhen, Ran Tao, et al. (2019). \textit{Top-N Collaborative Filtering Recommendation Algorithm Based on Knowledge Graph Embedding}. International Conference on Knowledge Management in Organizations.

\bibitem{kertkeidkachorn2019dkn}
Natthawut Kertkeidkachorn, Xin Liu, and R. Ichise (2019). \textit{GTransE: Generalizing Translation-Based Model on Uncertain Knowledge Graph Embedding}. JSAI.

\bibitem{zhu2019zqy}
Jia Zhu, Zetao Zheng, Min Yang, et al. (2019). \textit{A semi-supervised model for knowledge graph embedding}. Data mining and knowledge discovery.

\bibitem{zhang20193g2}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Towards Data Poisoning Attack against Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2019fcs}
Wenqiang Liu, Hongyun Cai, Xu Cheng, et al. (2019). \textit{Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{kanojia20171in}
Vibhor Kanojia, Hideyuki Maeda, Riku Togashi, et al. (2017). \textit{Enhancing Knowledge Graph Embedding with Probabilistic Negative Sampling}. The Web Conference.

\bibitem{gao2018di0}
Huan Gao, Jun Shi, G. Qi, et al. (2018). \textit{Triple Context-Based Knowledge Graph Embedding}. IEEE Access.

\bibitem{mai2018egi}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Support and Centrality: Learning Weights for Knowledge Graph Embedding Models}. International Conference Knowledge Engineering and Knowledge Management.

\bibitem{xiao2016bb9}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2016). \textit{Knowledge Semantic Representation: A Generative Model for Interpretable Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2024q3q}
Peifeng Liu, Lu Qian, Xingwei Zhao, et al. (2024). \textit{Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly}. IEEE Transactions on Industrial Informatics.

\bibitem{zhang2024cjl}
Jin-cheng Zhang, A. Zain, Kai Zhou, et al. (2024). \textit{A review of recommender systems based on knowledge graph embedding}. Expert systems with applications.

\bibitem{su2023v6e}
Xiao-Rui Su, Zhuhong You, Deshuang Huang, et al. (2023). \textit{Biomedical Knowledge Graph Embedding With Capsule Network for Multi-Label Drug-Drug Interaction Prediction}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhu2023bfj}
Xiangrong Zhu, Guang-pu Li, and Wei Hu (2023). \textit{Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning}. The Web Conference.

\bibitem{liu2024to0}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Towards Continual Knowledge Graph Embedding via Incremental Distillation}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2024vgj}
Wei Wang, Xiaoxuan Shen, Baolin Yi, et al. (2024). \textit{Knowledge-aware fine-grained attention networks with refined knowledge graph embedding for personalized recommendation}. Expert systems with applications.

\bibitem{li2024920}
Duantengchuan Li, Tao Xia, Jing Wang, et al. (2024). \textit{SDFormer: A shallow-to-deep feature interaction for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{lee202380l}
Jaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang (2023). \textit{InGram: Inductive Knowledge Graph Embedding via Relation Graphs}. International Conference on Machine Learning.

\bibitem{shokrzadeh2023twj}
Zeinab Shokrzadeh, M. Feizi-Derakhshi, M. Balafar, et al. (2023). \textit{Knowledge graph-based recommendation system enhanced by neural collaborative filtering and knowledge graph embedding}. Ain Shams Engineering Journal.

\bibitem{gao2023086}
Weibo Gao, Hao Wang, Qi Liu, et al. (2023). \textit{Leveraging Transferable Knowledge Concept Graph Embedding for Cold-Start Cognitive Diagnosis}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2024sgp}
Yufeng Li, Wenchao Zhao, Bo Dang, et al. (2024). \textit{Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning}. 2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE).

\bibitem{xue2023qi7}
Zengcan Xue, Zhao Zhang, Hai Liu, et al. (2023). \textit{Learning knowledge graph embedding with multi-granularity relational augmentation network}. Expert systems with applications.

\bibitem{duan2024d3f}
Pengbo Duan, Kuo Yang, Xin Su, et al. (2024). \textit{HTINet2: herb–target prediction via knowledge graph embedding and residual-like graph neural network}. Briefings Bioinform..

\bibitem{chen20246rm}
Zhen Chen, Dalin Zhang, Shanshan Feng, et al. (2024). \textit{KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2022o32}
Jia Zhu, Changqin Huang, and P. D. Meo (2022). \textit{DFMKE: A dual fusion multi-modal knowledge graph embedding framework for entity alignment}. Information Fusion.

\bibitem{mitropoulou20235t0}
Katerina Mitropoulou, Panagiotis C. Kokkinos, P. Soumplis, et al. (2023). \textit{Anomaly Detection in Cloud Computing using Knowledge Graph Embedding and Machine Learning Mechanisms}. Journal of Grid Computing.

\bibitem{shomer2023imo}
Harry Shomer, Wei Jin, Wentao Wang, et al. (2023). \textit{Toward Degree Bias in Embedding-Based Knowledge Graph Completion}. The Web Conference.

\bibitem{wang202490m}
Mingjie Wang, Zijie Li, Jun Wang, et al. (2024). \textit{TracKGE: Transformer with Relation-pattern Adaptive Contrastive Learning for Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{li2024bl5}
Zhifei Li, Wei Huang, Xuchao Gong, et al. (2024). \textit{Decoupled semantic graph neural network for knowledge graph embedding}. Neurocomputing.

\bibitem{li2024y2a}
Mingqi Li, Wenming Ma, and Zihao Chu (2024). \textit{KGIE: Knowledge graph convolutional network for recommender system with interactive embedding}. Knowledge-Based Systems.

\bibitem{jia2023krv}
Yan Jia, Mengqi Lin, Yechen Wang, et al. (2023). \textit{Extrapolation over temporal knowledge graph via hyperbolic embedding}. CAAI Transactions on Intelligence Technology.

\bibitem{huang2023grx}
Wei Huang, Jia Liu, Tianrui Li, et al. (2023). \textit{FedCKE: Cross-Domain Knowledge Graph Embedding in Federated Learning}. IEEE Transactions on Big Data.

\bibitem{wang2023s70}
Ruoxin Wang, and C. F. Cheung (2023). \textit{Knowledge graph embedding learning system for defect diagnosis in additive manufacturing}. Computers in industry (Print).

\bibitem{hou20237gt}
Xiangning Hou, Ruizhe Ma, Li Yan, et al. (2023). \textit{T-GAE: A Timespan-aware Graph Attention-based Embedding Model for Temporal Knowledge Graph Completion}. Information Sciences.

\bibitem{jiang2023opm}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2023). \textit{Multisource hierarchical neural network for knowledge graph embedding}. Expert systems with applications.

\bibitem{lu2022bwo}
H. Lu, Hailin Hu, and Xiaodong Lin (2022). \textit{DensE: An enhanced non-commutative representation for knowledge graph embedding with adaptive semantic hierarchy}. Neurocomputing.

\bibitem{djeddi2023g71}
W. Djeddi, Khalil Hermi, S. Yahia, et al. (2023). \textit{Advancing drug–target interaction prediction: a comprehensive graph-based approach integrating knowledge graph embedding and ProtBert pretraining}. BMC Bioinformatics.

\bibitem{zhang20243iw}
Yuchao Zhang, Xiangjie Kong, Zhehui Shen, et al. (2024). \textit{A survey on temporal knowledge graph embedding: Models and applications}. Knowledge-Based Systems.

\bibitem{le2023hjy}
Thanh-Binh Le, Huy Tran, and H. Le (2023). \textit{Knowledge graph embedding with the special orthogonal group in quaternion space for link prediction}. Knowledge-Based Systems.

\bibitem{yao2023y12}
Zhen Yao, Wen Zhang, Mingyang Chen, et al. (2023). \textit{Analogical Inference Enhanced Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{li2023y5q}
Zhipeng Li, Shanshan Feng, Jun Shi, et al. (2023). \textit{Future Event Prediction Based on Temporal Knowledge Graph Embedding}. Computer systems science and engineering.

\bibitem{yang2022j7z}
Shihan Yang, Weiya Zhang, R. Tang, et al. (2022). \textit{Approximate inferring with confidence predicting based on uncertain knowledge graph embedding}. Information Sciences.

\bibitem{banerjee2023fdi}
Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, et al. (2023). \textit{GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering}. Extended Semantic Web Conference.

\bibitem{hu20230kr}
Yuke Hu, Wei Liang, Ruofan Wu, et al. (2023). \textit{Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2023wgg}
Daiyi Li, Li Yan, Xiaowen Zhang, et al. (2023). \textit{EventKGE: Event knowledge graph embedding with event causal transfer}. Knowledge-Based Systems.

\bibitem{hao2022cl4}
Xinkun Hao, Qingfeng Chen, Haiming Pan, et al. (2022). \textit{Enhancing drug–drug interaction prediction by three-way decision and knowledge graph embedding}. Granular Computing.

\bibitem{khan20222j1}
Nasrullah Khan, Z. Ma, Li Yan, et al. (2022). \textit{Hashing-based semantic relevance attributed knowledge graph embedding enhancement for deep probabilistic recommendation}. Applied intelligence (Boston).

\bibitem{le2022ybl}
Thanh-Binh Le, Ngoc Huynh, and Bac Le (2022). \textit{Knowledge graph embedding by projection and rotation on hyperplanes for link prediction}. Applied intelligence (Boston).

\bibitem{liang202338l}
Shuang Liang (2023). \textit{Knowledge Graph Embedding Based on Graph Neural Network}. IEEE International Conference on Data Engineering.

\bibitem{khan2022ipv}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{DCA-IoMT: Knowledge-Graph-Embedding-Enhanced Deep Collaborative Alert Recommendation Against COVID-19}. IEEE Transactions on Industrial Informatics.

\bibitem{he2022e37}
Peng He, Gang Zhou, Mengli Zhang, et al. (2022). \textit{Improving temporal knowledge graph embedding using tensor factorization}. Applied intelligence (Boston).

\bibitem{shen2022d5j}
Linshan Shen, Rongbo He, and Shaobin Huang (2022). \textit{Entity alignment with adaptive margin learning knowledge graph embedding}. Data & Knowledge Engineering.

\bibitem{di20210ib}
Shimin Di, Quanming Yao, Yongqi Zhang, et al. (2021). \textit{Efficient Relation-aware Scoring Function Search for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{niu2020uyy}
Guanglin Niu, Bo Li, Yongfei Zhang, et al. (2020). \textit{AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding}. Findings.

\bibitem{nie2023ejz}
H. Nie, Xiangguo Zhao, Xin Bi, et al. (2023). \textit{Correlation embedding learning with dynamic semantic enhanced sampling for knowledge graph completion}. World wide web (Bussum).

\bibitem{li2022du0}
Jiayi Li, and Yujiu Yang (2022). \textit{STaR: Knowledge Graph Embedding by Scaling, Translation and Rotation}. Autonomous Infrastructure, Management and Security.

\bibitem{daruna2022dmk}
A. Daruna, Devleena Das, and S. Chernova (2022). \textit{Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions}. IEEE/RJS International Conference on Intelligent RObots and Systems.

\bibitem{zhou20210ma}
Xing-Chun Zhou, Peng Wang, Qi Luo, et al. (2021). \textit{Multi-hop Knowledge Graph Reasoning Based on Hyperbolic Knowledge Graph Embedding and Reinforcement Learning}. IJCKG.

\bibitem{kun202384f}
Kong Wei Kun, Xin Liu, Teeradaj Racharak, et al. (2023). \textit{WeExt: A Framework of Extending Deterministic Knowledge Graph Embedding Models for Embedding Weighted Knowledge Graphs}. IEEE Access.

\bibitem{dong2022taz}
Yao Dong, Lei Wang, Ji Xiang, et al. (2022). \textit{RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space}. International Conference on Computational Linguistics.

\bibitem{kamigaito20218jz}
Hidetaka Kamigaito, and Katsuhiko Hayashi (2021). \textit{Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{krause2022th0}
Franziska Krause (2022). \textit{Dynamic Knowledge Graph Embeddings via Local Embedding Reconstructions}. Extended Semantic Web Conference.

\bibitem{zhang20213h6}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2021). \textit{Knowledge graph embedding with shared latent semantic units}. Neural Networks.

\bibitem{li2021tm6}
Guang-pu Li, Zequn Sun, Lei Qian, et al. (2021). \textit{Rule-based data augmentation for knowledge graph embedding}. AI Open.

\bibitem{wang2020au0}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2020). \textit{Enhancing knowledge graph embedding by composite neighbors for link prediction}. Computing.

\bibitem{wei20215a7}
Yuyang Wei, Wei Chen, Zhixu Li, et al. (2021). \textit{Incremental Update of Knowledge Graph Embedding by Rotating on Hyperplanes}. 2021 IEEE International Conference on Web Services (ICWS).

\bibitem{zhang2021rjh}
Yongqi Zhang, Quanming Yao, and Lei Chen (2021). \textit{Simple and automated negative sampling for knowledge graph embedding}. The VLDB journal.

\bibitem{sheikh202245c}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2022). \textit{Scaling knowledge graph embedding models for link prediction}. EuroMLSys@EuroSys.

\bibitem{ren2021muc}
Chao Ren, Le Zhang, Lintao Fang, et al. (2021). \textit{Ontological Concept Structure Aware Knowledge Transfer for Inductive Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{eyharabide2021wx4}
Victoria Eyharabide, I. E. I. Bekkouch, and Nicolae Dragoș Constantin (2021). \textit{Knowledge Graph Embedding-Based Domain Adaptation for Musical Instrument Recognition}. De Computis.

\bibitem{hong2020hyg}
Y. Hong, Chenyang Bu, and Tingting Jiang (2020). \textit{Rule-enhanced Noisy Knowledge Graph Embedding via Low-quality Error Detection}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{huang2020sqc}
Yan Huang, Haili Sun, Xu Ke, et al. (2020). \textit{CoRelatE: Learning the correlation in multi-fold relations for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{kurokawa2021f4f}
M. Kurokawa (2021). \textit{Explainable Knowledge Reasoning Framework Using Multiple Knowledge Graph Embedding}. IJCKG.

\bibitem{mohamed2021dwg}
Sameh K. Mohamed, Emir Muñoz, and V. Nováček (2021). \textit{On Training Knowledge Graph Embedding Models}. Inf..

\bibitem{gebhart2021gtp}
Thomas Gebhart, J. Hansen, and Paul Schrater (2021). \textit{Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding}. International Conference on Artificial Intelligence and Statistics.

\bibitem{deng2024643}
Weibin Deng, Yiteng Zhang, Hong Yu, et al. (2024). \textit{Knowledge graph embedding based on dynamic adaptive atrous convolution and attention mechanism for link prediction}. Information Processing & Management.

\bibitem{liu2024zr9}
Jin Liu, Hao Du, R. Guo, et al. (2024). \textit{MMGK: Multimodality Multiview Graph Representations and Knowledge Embedding for Mild Cognitive Impairment Diagnosis}. IEEE Transactions on Computational Social Systems.

\bibitem{zhang2024zmq}
Chengcheng Zhang, Tianyi Zang, and Tianyi Zhao (2024). \textit{KGE-UNIT: toward the unification of molecular interactions prediction based on knowledge graph and multi-task learning on drug discovery}. Briefings Bioinform..

\bibitem{he2024vks}
Mingsheng He, Lin Zhu, and Luyi Bai (2024). \textit{ConvTKG: A query-aware convolutional neural network-based embedding model for temporal knowledge graph completion}. Neurocomputing.

\bibitem{zhang2024fy0}
Dong Zhang, Zhe Rong, Chengyuan Xue, et al. (2024). \textit{SimRE: Simple contrastive learning with soft logical rule for knowledge graph embedding}. Information Sciences.

\bibitem{zhang2024ivc}
Dong Zhang, Wenlong Feng, Zonghang Wu, et al. (2024). \textit{CDRGN-SDE: Cross-Dimensional Recurrent Graph Network with neural Stochastic Differential Equation for temporal knowledge graph embedding}. Expert systems with applications.

\bibitem{jing2024nxw}
Yanzhen Jing, Guanghui Zhou, Chao Zhang, et al. (2024). \textit{XMKR: Explainable manufacturing knowledge recommendation for collaborative design with graph embedding learning}. Advanced Engineering Informatics.

\bibitem{jiang2024zlc}
Pengcheng Jiang, Lang Cao, Cao Xiao, et al. (2024). \textit{KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge}. Neural Information Processing Systems.

\bibitem{han2024u0t}
Zhulin Han, and Jian Wang (2024). \textit{Knowledge enhanced graph inference network based entity-relation extraction and knowledge graph construction for industrial domain}. Frontiers of Engineering Management.

\bibitem{quan2024o2a}
Huafeng Quan, Yiting Li, Dashuai Liu, et al. (2024). \textit{Protection of Guizhou Miao batik culture based on knowledge graph and deep learning}. Heritage Science.

\bibitem{liu2024tc2}
Bufan Liu, Chun-Hsien Chen, and Zuoxu Wang (2024). \textit{A multi-hierarchical aggregation-based graph convolutional network for industrial knowledge graph embedding towards cognitive intelligent manufacturing}. Journal of manufacturing systems.

\bibitem{hello2024hgf}
Nour Hello, P. Lorenzo, and E. Strinati (2024). \textit{Semantic Communication Enhanced by Knowledge Graph Representation Learning}. International Workshop on Signal Processing Advances in Wireless Communications.

\bibitem{li2024z0e}
Jinpeng Li, Hang Yu, Xiangfeng Luo, et al. (2024). \textit{COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion}. North American Chapter of the Association for Computational Linguistics.

\bibitem{yan2024joa}
Qun Yan, Juan Zhao, Linfu Xue, et al. (2024). \textit{Mineral Prospectivity Mapping Based on Spatial Feature Classification with Geological Map Knowledge Graph Embedding: Case Study of Gold Ore Prediction at Wulonggou, Qinghai Province (Western China)}. Natural Resources Research.

\bibitem{liu2024tn0}
Jhih-Chen Liu, Chiao-Ting Chen, Chi Lee, et al. (2024). \textit{Evolving Knowledge Graph Representation Learning with Multiple Attention Strategies for Citation Recommendation System}. ACM Transactions on Intelligent Systems and Technology.

\bibitem{wang20245dw}
Chuanghui Wang, Yunqing Yang, Jinshuai Song, et al. (2024). \textit{Research Progresses and Applications of Knowledge Graph Embedding Technique in Chemistry}. Journal of Chemical Information and Modeling.

\bibitem{long2024soi}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{KGDM: A Diffusion Model to Capture Multiple Relation Semantics for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2024ayq}
Qihui Zhou, Peiqi Yin, Xiao Yan, et al. (2024). \textit{Atom: An Efficient Query Serving System for Embedding-based Knowledge Graph Reasoning with Operator-level Batching}. Proc. ACM Manag. Data.

\bibitem{huang2024t19}
Chen Huang, Deshan Chen, Tengze Fan, et al. (2024). \textit{Incorporating environmental knowledge embedding and spatial-temporal graph attention networks for inland vessel traffic flow prediction}. Engineering applications of artificial intelligence.

\bibitem{lu2024fsd}
Ming Lu, Yancong Li, Jiangxiao Zhang, et al. (2024). \textit{Deep hyperbolic convolutional model for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2024yar}
Qi Liu, Qinghua Zhang, Fan Zhao, et al. (2024). \textit{Uncertain knowledge graph embedding: an effective method combining multi-relation and multi-path}. Frontiers Comput. Sci..

\bibitem{khan20242y2}
Nasrullah Khan, Zongmin Ma, Ruizhe Ma, et al. (2024). \textit{Continual knowledge graph embedding enhancement for joint interaction-based next click recommendation}. Knowledge-Based Systems.

\bibitem{xue2025ee8}
Zengcan Xue, Zhaoli Zhang, Hai Liu, et al. (2025). \textit{MHRN: A multi-perspective hierarchical relation network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{long20248vt}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{Fact Embedding through Diffusion Model for Knowledge Graph Completion}. The Web Conference.

\bibitem{huang20240su}
Chen Huang, Fei Yu, Zhiguo Wan, et al. (2024). \textit{Knowledge graph confidence-aware embedding for recommendation}. Neural Networks.

\bibitem{wang2024nej}
Yuzhuo Wang, Hongzhi Wang, Xianglong Liu, et al. (2024). \textit{GFedKG: GNN-based federated embedding model for knowledge graph completion}. Knowledge-Based Systems.

\bibitem{wang2024c8z}
Xinyan Wang, Kuo Yang, Ting Jia, et al. (2024). \textit{KDGene: knowledge graph completion for disease gene prediction using interactional tensor decomposition}. Briefings Bioinform..

\bibitem{liu2024x0k}
Yuhan Liu, Zelin Cao, Xing Gao, et al. (2024). \textit{Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport}. The Web Conference.

\bibitem{li2024uio}
Yongfang Li, and Chunhua Zhu (2024). \textit{TransE-MTP: A New Representation Learning Method for Knowledge Graph Embedding with Multi-Translation Principles and TransE}. Electronics.

\bibitem{zhang2024z78}
Qianjin Zhang, and Yandan Xu (2024). \textit{Knowledge graph embedding with inverse function representation for link prediction}. Engineering applications of artificial intelligence.

\bibitem{wang2024534}
Hao Wang, Dandan Song, Zhijing Wu, et al. (2024). \textit{A collaborative learning framework for knowledge graph embedding and reasoning}. Knowledge-Based Systems.

\bibitem{ni202438q}
Shengkun Ni, Xiangtai Kong, Yingying Zhang, et al. (2024). \textit{Identifying compound-protein interactions with knowledge graph embedding of perturbation transcriptomics}. Cell Genomics.

\bibitem{nie202499i}
Jixuan Nie, Xia Hou, Wenfeng Song, et al. (2024). \textit{Knowledge Graph Efficient Construction: Embedding Chain-of-Thought into LLMs}. VLDB Workshops.

\bibitem{wang2024d52}
Jingchao Wang, Weimin Li, Fangfang Liu, et al. (2024). \textit{ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion}. Expert systems with applications.

\bibitem{mao2024v2s}
Yuren Mao, Yu Hao, Xin Cao, et al. (2024). \textit{Dynamic Graph Embedding via Meta-Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{jafarzadeh202468v}
Parastoo Jafarzadeh, F. Ensan, Mahdiyar Ali Akbar Alavi, et al. (2024). \textit{A Knowledge Graph Embedding Model for Answering Factoid Entity Questions}. ACM Trans. Inf. Syst..

\bibitem{wang2024dea}
Yalin Wang, Yubin Peng, and Jingyu Guo (2024). \textit{Enhancing knowledge graph embedding with structure and semantic features}. Applied intelligence (Boston).

\bibitem{lu202436n}
Yuhuan Lu, Weijian Yu, Xin Jing, et al. (2024). \textit{HyperCL: A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{han2024gaq}
Yadan Han, Guangquan Lu, Shichao Zhang, et al. (2024). \textit{A Temporal Knowledge Graph Embedding Model Based on Variable Translation}. Tsinghua Science and Technology.

\bibitem{liu2024jz8}
Bingchen Liu, Shifu Hou, Weiyi Zhong, et al. (2024). \textit{Enhancing Temporal Knowledge Graph Alignment in News Domain With Box Embedding}. IEEE Transactions on Computational Social Systems.

\bibitem{he2024y6o}
Yunjie He, Daniel Hernández, M. Nayyeri, et al. (2024). \textit{Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning}. Unpublished manuscript.

\bibitem{fang20243a4}
Yan Fang, Xiaodong Liu, Wei Lu, et al. (2024). \textit{Knowledge graph completion with low-dimensional gated hierarchical hyperbolic embedding}. Knowledge-Based Systems.

\bibitem{zhang2024h9k}
Mingtao Zhang, Guoli Yang, Yi Liu, et al. (2024). \textit{Knowledge graph accuracy evaluation: an LLM-enhanced embedding approach}. International Journal of Data Science and Analysis.

\bibitem{li2024wyh}
Yicong Li, Yu Yang, Jiannong Cao, et al. (2024). \textit{Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach}. Knowledge Discovery and Data Mining.

\bibitem{dong2024ijo}
Dibo Dong, Shangwei Wang, Qiaoying Guo, et al. (2024). \textit{Short-Term Marine Wind Speed Forecasting Based on Dynamic Graph Embedding and Spatiotemporal Information}. Journal of Marine Science and Engineering.

\bibitem{wang20246c7}
Tao Wang, Bo Shen, Jinglin Zhang, et al. (2024). \textit{Knowledge Graph Embedding via Triplet Component Interactions}. Neural Processing Letters.

\bibitem{zhang2024yjo}
Pengfei Zhang, Xiaoxue Zhang, Yang Fang, et al. (2024). \textit{Knowledge Graph Embedding for Hierarchical Entities Based on Auto-Embedding Size}. Mathematics.

\bibitem{liang20247wv}
K. Liang, Yue Liu, Hao Li, et al. (2024). \textit{Clustering then Propagation: Select Better Anchors for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{liu2024t05}
Qi Liu, Yuanyuan Jin, Xuefei Cao, et al. (2024). \textit{An Entity Ontology-Based Knowledge Graph Embedding Approach to News Credibility Assessment}. IEEE Transactions on Computational Social Systems.

\bibitem{pham20243mh}
H. V. Pham, Trung Tuan Nguyen, Luu Minh Tuan, et al. (2024). \textit{IDGCN: A Proposed Knowledge Graph Embedding With Graph Convolution Network For Context-Aware Recommendation Systems}. Journal of Organizational Computing and Electronic Commerce.

\bibitem{li2024gar}
Yu Li, Zhu-Hong You, Shu-Min Wang, et al. (2024). \textit{Attention-Based Learning for Predicting Drug-Drug Interactions in Knowledge Graph Embedding Based on Multisource Fusion Information}. International Journal of Intelligent Systems.

\bibitem{li2024nje}
Nan Li, Zhihao Yang, Jian Wang, et al. (2024). \textit{Drug–target interaction prediction using knowledge graph embedding}. iScience.

\bibitem{bao20249xp}
Liming Bao, Yan Wang, Xiaoyu Song, et al. (2024). \textit{HGCGE: hyperbolic graph convolutional networks-based knowledge graph embedding for link prediction}. Knowledge and Information Systems.

\bibitem{xu2024fto}
Guoshun Xu, Guozheng Rao, Li Zhang, et al. (2024). \textit{Entity-relation aggregation mechanism graph neural network for knowledge graph embedding}. Applied intelligence (Boston).

\bibitem{liang2024z0q}
Qiuyu Liang, Weihua Wang, Jie Yu, et al. (2024). \textit{Effective Knowledge Graph Embedding with Quaternion Convolutional Networks}. Natural Language Processing and Chinese Computing.

\bibitem{liu2024ixy}
Jie Liu, Lizheng Zu, Yunbin Yan, et al. (2024). \textit{Multi-Filter soft shrinkage network for knowledge graph embedding}. Expert systems with applications.

\bibitem{dong2025l9k}
Jie Dong, Cuiping Chen, Chi Zhang, et al. (2025). \textit{Knowledge Graph Embedding With Graph Convolutional Network and Bidirectional Gated Recurrent Unit for Fault Diagnosis of Industrial Processes}. IEEE Sensors Journal.

\bibitem{zhang2025ebv}
Sensen Zhang, Xun Liang, Simin Niu, et al. (2025). \textit{Integrating Large Language Models and Möbius Group Transformations for Temporal Knowledge Graph Embedding on the Riemann Sphere}. AAAI Conference on Artificial Intelligence.

\bibitem{liu20242zm}
Xinyue Liu, Jianan Zhang, Chi Ma, et al. (2024). \textit{Temporal Knowledge Graph Reasoning with Dynamic Hypergraph Embedding}. International Conference on Language Resources and Evaluation.

\bibitem{yang2024lwa}
Ruiyi Yang, Flora D. Salim, and Hao Xue (2024). \textit{SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding}. The Web Conference.

\bibitem{li20246qx}
Bo Li, Haowei Quan, Jiawei Wang, et al. (2024). \textit{Neural Library Recommendation by Embedding Project-Library Knowledge Graph}. IEEE Transactions on Software Engineering.

\bibitem{liu2024mji}
Xiaojian Liu, Xinwei Guo, and Wen Gu (2024). \textit{SecKG2vec: A novel security knowledge graph relational reasoning method based on semantic and structural fusion embedding}. Computers & security.

\bibitem{chen2024efo}
Bin Chen, Hongyi Li, Di Zhao, et al. (2024). \textit{Quality assessment of cyber threat intelligence knowledge graph based on adaptive joining of embedding model}. Complex &amp; Intelligent Systems.

\bibitem{chen2024uld}
Deng Chen, Weiwei Zhang, and Zuohua Ding (2024). \textit{Embedding dynamic graph attention mechanism into Clinical Knowledge Graph for enhanced diagnostic accuracy}. Expert systems with applications.

\bibitem{wang2017zm5}
Quan Wang, Zhendong Mao, Bin Wang, et al. (2017). \textit{Knowledge Graph Embedding: A Survey of Approaches and Applications}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{li2021qr0}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks}. IEEE Transactions on Neural Networks and Learning Systems.

\end{thebibliography}

\end{document}