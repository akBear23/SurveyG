\subsection{Integrating Auxiliary Information: Text, Types, and Rules}
Knowledge graph embedding (KGE) models traditionally focus on learning representations solely from observed (head, relation, tail) triples. However, real-world knowledge graphs are often incomplete and noisy, necessitating the integration of auxiliary information to enrich embeddings, improve semantic consistency, and enhance predictive performance. This section reviews KGE models that leverage diverse external data sources, including textual descriptions, semantic categories, explicit or implicit entity types, and logical rules.

Early efforts to incorporate textual information aimed to bridge the gap between symbolic triples and natural language semantics. \cite{xiao2016} proposed Semantic Space Projection (SSP), a model that jointly learns from triples and textual descriptions by projecting the triple's loss vector onto a semantic hyperplane. This approach models strong correlations between texts and triples, guiding the embedding topology. Building on this, \cite{shen2022} introduced LASS (Joint Language Semantic and Structure Embedding), which fine-tunes pre-trained language models with a probabilistic structured loss to simultaneously capture semantics from textual descriptions and reconstruct KG structures. LASS addresses the limitations of earlier text-aware models by providing a unified framework for deeper integration of language semantics, demonstrating superior performance, especially in low-resource settings.

Beyond free-form text, structured semantic information like entity categories and types offers valuable context. \cite{guo2015} presented Semantically Smooth Embedding (SSE), which enforces a "semantically smooth" embedding space by constraining entities belonging to the same semantic category to lie close to each other, using manifold learning algorithms like Laplacian Eigenmaps as regularization. Taking this a step further, \cite{lv2018} proposed TransC (Translating Concepts), which fundamentally differentiates concepts (represented as spheres) from instances (represented as vectors) in the embedding space. This geometric modeling inherently preserves the transitivity of `instanceOf` and `subClassOf` relations, a crucial property often overlooked by models treating all entities uniformly. More recently, \cite{wang2021} introduced TransET, a model that leverages explicit entity types by employing circle convolution based on entity and type embeddings to generate type-specific representations, thereby learning more semantic features. Addressing the common challenge of incomplete or unavailable explicit type information, \cite{he2023} developed TaKE (Type-augmented Knowledge graph Embedding), a model-agnostic framework that automatically captures *implicit* type features. TaKE further models the diversity of entity types using a relation-specific hyperplane mechanism and introduces a novel type-constrained negative sampling strategy, making it highly flexible and effective without requiring explicit type supervision.

Another powerful form of auxiliary information comes from logical rules, which encode explicit knowledge and constraints. \cite{guo2017} proposed RUGE (RUle-Guided Embedding), an iterative paradigm that guides embedding learning with automatically extracted soft rules. RUGE addresses the limitation of one-time rule injection by alternating between soft label prediction for unlabeled triples and embedding rectification, maximizing the utility of uncertain logical knowledge. Expanding on the integration of soft rules, \cite{guo2020} introduced a scalable method for Soft Logical Rule Embedding (SLRE), which represents relations as bilinear forms and entities in a non-negative bounded space. Their key innovation is a novel rule-based regularization that directly enforces relation representations to satisfy soft rule constraints, making its complexity independent of the entity set size and significantly improving scalability. More comprehensively, \cite{tang2022} presented RulE (Rule Embedding), a neural-symbolic framework that learns explicit *rule embeddings* and jointly represents entities, relations, and logical rules in a unified continuous space. RulE calculates confidence scores for rules and employs a soft rule reasoning mechanism, effectively mitigating the brittleness of traditional logical inference and allowing for mutual regularization between KGE and rule-based components.

In conclusion, the integration of auxiliary information, whether from textual descriptions, semantic categories, entity types, or logical rules, has profoundly enhanced KGE models. This progression reflects a shift from purely structural embeddings to richer, context-aware representations that capture more nuanced semantic relationships and logical consistencies. While significant strides have been made in leveraging these diverse data sources, future research could explore more sophisticated fusion mechanisms for multi-modal auxiliary information, investigate methods for automatically discovering and validating rules from noisy data, and develop frameworks that offer greater interpretability of how auxiliary knowledge influences learned representations.