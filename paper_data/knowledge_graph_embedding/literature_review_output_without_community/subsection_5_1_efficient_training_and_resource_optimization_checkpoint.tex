\subsection{Efficient Training and Resource Optimization}

The practical deployment of Knowledge Graph Embedding (KGE) models, particularly in large-scale and resource-constrained environments, critically depends on their computational efficiency and optimized resource demands during both training and inference. This subsection delves into key advancements aimed at improving KGE training efficiency, reducing resource consumption, and enhancing stability through various strategies, including refined negative sampling, non-sampling frameworks, embedding compression, and lightweight model architectures.

A cornerstone of KGE training is negative sampling, a technique essential for generating unobserved triples that serve as negative examples, thereby enabling models to learn to distinguish true facts from false ones. The quality and efficiency of negative sampling significantly impact training stability and accuracy \cite{mohamed2021dwg, ali2020}. Early methods, such as uniform negative sampling, often suffered from generating false negatives or uninformative samples. Bernoulli negative sampling, introduced with TransH \cite{wang2014}, marked an improvement by considering relation mapping properties, selectively corrupting either the head or tail entity based on relation statistics. This approach reduced the likelihood of generating false negative labels, leading to more effective and stable training. Further advancing this, RotatE \cite{sun2018} proposed self-adversarial negative sampling, which dynamically generates more challenging and informative negative samples by leveraging the current state of the embedding model. This adaptive strategy makes the training process more robust and efficient compared to static or GAN-based approaches, as it focuses computational effort on samples that are most beneficial for learning. The persistent importance of this technique is underscored by comprehensive reviews, such as those by \cite{madushanka2024} and \cite{qian2021}, which systematically classify various negative sampling methods (e.g., static, dynamic, custom cluster-based), highlight their advantages and disadvantages, and identify ongoing research challenges. These reviews emphasize that the choice of negative sampling, alongside loss functions and hyperparameters, profoundly influences model scalability and accuracy \cite{mohamed2021dwg}. More recently, type-augmented frameworks like TaKE \cite{he2023} have contributed to training efficiency by introducing type-constrained negative sampling, which implicitly leverages entity type knowledge to generate more effective negative samples and improve the training signal.

While refining negative sampling addresses a crucial aspect of training efficiency, an alternative paradigm involves non-sampling KGE (NS-KGE) frameworks, which aim to circumvent the inherent instability and computational overhead associated with negative sampling altogether. These methods typically adopt a 1-vs-All training paradigm, where instead of sampling a small number of negative triples, the model explicitly scores all possible entities as potential tails (or heads) for a given head-relation (or relation-tail) pair. This approach, while potentially more computationally intensive during inference if not optimized (e.g., through approximate nearest neighbor search), eliminates the need for heuristic-driven negative sample generation, thereby offering greater training stability and potentially more comprehensive learning signals by evaluating all candidates. The trade-off lies in managing the increased computational burden of scoring all candidates, which often necessitates specialized optimization techniques.

Beyond training methodologies, resource optimization also encompasses reducing the memory footprint and computational cost of the KGE models themselves. Knowledge graph embedding compression techniques are vital for deploying models in memory-constrained environments. Approaches include quantization, pruning, and knowledge distillation. For instance, \cite{sachan2020} proposes a novel method to compress the KGE layer by representing each entity as a vector of discrete codes, from which the full continuous embeddings are then composed. This technique achieves substantial compression ratios, ranging from 50x to 1000x, with only a minor loss in performance. Such compression is critical for enabling the deployment of powerful KGE techniques in real-world applications where storage and memory are significant limitations, by effectively decoupling the storage size from the embedding dimension.

Furthermore, the development of lightweight KGE frameworks is crucial for efficient inference and storage, especially for edge devices or applications requiring real-time responses. These frameworks are designed with inherently lower parameter counts or simpler scoring functions to minimize computational overhead. An example of such an approach is LineaRE \cite{peng2020}, which interprets knowledge graph embedding as a simple linear regression task. By using real-valued vectors and element-wise products, LineaRE offers a remarkably simple and scalable architecture. Despite its simplicity, it has been mathematically proven to comprehensively model all four connectivity patterns (symmetry, antisymmetry, inversion, composition) and all four complex mapping properties (one-to-one, one-to-many, many-to-one, many-to-many), often outperforming more complex models. This demonstrates that high expressiveness and efficiency are not mutually exclusive, challenging the notion that complex architectures are always necessary for robust performance.

In summary, the pursuit of efficient KGE training and resource optimization is a multifaceted endeavor, driving significant advancements across various fronts. Innovations range from the meticulous refinement of negative sampling strategies \cite{wang2014, sun2018, he2023, madushanka2024, qian2021, mohamed2021dwg} and the exploration of non-sampling paradigms to the development of embedding compression techniques \cite{sachan2020} and inherently lightweight model architectures \cite{peng2020}. These collective efforts are indispensable for making KGE models practical, scalable, and viable for deployment in large-scale, dynamic, and resource-constrained real-world applications. Ongoing research continues to navigate the delicate balance between model expressiveness, computational efficiency, and robustness, pushing the boundaries of what KGE models can achieve.