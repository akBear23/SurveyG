\subsection{Continual Learning for Evolving KGs}

The inherent dynamism of real-world knowledge graphs (KGs), which are subject to constant updates with new entities, relations, and facts, poses a formidable challenge for knowledge graph embedding (KGE) models. The core problem lies in continually updating these representations without succumbing to catastrophic forgetting, where the model loses previously acquired knowledge upon learning new information. This necessitates the development of adaptive and efficient continual learning (CL) paradigms that can seamlessly integrate novel information while robustly preserving the integrity of existing embeddings.

Early efforts to address dynamic KGs often focused on incremental updates, laying the groundwork for more sophisticated continual learning strategies. For instance, \cite{jia2017} introduced \textit{iTransA}, an incremental algorithm designed to adapt the margin of the loss function in translation-based KGE models. Unlike static approaches, iTransA dynamically adjusts this margin, making it locally adaptive to the evolving characteristics of different knowledge graphs and allowing for efficient updates when new vertices and edges are added without requiring full re-training. This method represents an early regularization-inspired approach, where the learning process itself adapts to new data. Building on this, \cite{wei20215a7} proposed \textit{RotatH}, a novel KGE method that supports incremental updates by rotating entities on relation-specific hyperplanes. RotatH efficiently integrates new entities into the existing embedding space, maintaining timeliness and accuracy. Its design allows it to handle complex relations, such as many-to-many and symmetric relations, in both incremental and static environments, demonstrating a geometric approach to efficient continuous adaptation.

A significant advancement in mitigating catastrophic forgetting through architectural innovation is \textit{FastKGE} by \cite{liu2024}. This framework directly addresses continual learning in dynamic KGs by employing an Incremental Low-Rank Adapter (IncLoRA) mechanism. IncLoRA enables the efficient acquisition of new knowledge from evolving KGs while robustly preserving previously learned information. By introducing small, trainable adapter modules that are added to pre-trained layers, FastKGE can adapt to new data streams with minimal parameter updates, effectively preventing catastrophic forgetting without requiring complete retraining of the entire model. This parameter-efficient approach is crucial for maintaining up-to-date and accurate representations in large-scale, constantly changing KGs.

Beyond architectural modifications, knowledge distillation has emerged as a powerful technique for continual KGE. \cite{liu2024to0} proposed \textit{IncDE} (Incremental Distillation), a competitive method for continual KGE that explicitly leverages the explicit graph structure. IncDE introduces a hierarchical strategy for learning new triples, ranking them for layer-by-layer processing based on inter- and intra-hierarchical graph structure features. This structured learning order optimizes the integration of new knowledge. Crucially, IncDE devises a novel incremental distillation mechanism that facilitates the seamless transfer of entity representations from previous layers to subsequent ones, thereby promoting the preservation of old knowledge. A two-stage training paradigm further prevents the over-corruption of established knowledge by under-trained new information, demonstrating a comprehensive strategy for balancing new knowledge acquisition and old knowledge retention.

The challenges of continual learning are further compounded in distributed and privacy-sensitive environments. \cite{zhu2023bfj} introduced \textit{FedLU} (Federated Knowledge Graph Embedding Learning and Unlearning), a novel federated learning framework that addresses both heterogeneous data and knowledge forgetting in distributed KGE settings. FedLU employs mutual knowledge distillation to transfer local knowledge to a global model and absorb global knowledge back, effectively coping with the drift caused by data heterogeneity. Moreover, it incorporates a knowledge unlearning method inspired by cognitive neuroscience, combining retroactive interference and passive decay to selectively erase specific knowledge from local clients and propagate these changes to the global model. This work highlights the evolving landscape of continual learning, where privacy, distribution, and the ability to selectively forget become critical considerations.

In synthesizing these approaches, we observe a spectrum of continual learning strategies for KGEs:
\begin{itemize}
    \item \textbf{Adaptive Regularization/Loss-based}: Methods like iTransA \cite{jia2017} adapt the learning objective to new data, focusing on local properties.
    \item \textbf{Geometric/Incremental Update}: RotatH \cite{wei20215a7} leverages geometric transformations for efficient, localized updates.
    \item \textbf{Architectural/Parameter Isolation}: FastKGE with IncLoRA \cite{liu2024} modifies model architecture to isolate new learning, preventing interference with old knowledge.
    \item \textbf{Knowledge Distillation}: IncDE \cite{liu2024to0} and FedLU \cite{zhu2023bfj} transfer knowledge from an "old" model (or previous state) to a "new" one, often coupled with strategies for structured learning or distributed settings.
\end{itemize}
While significant progress has been made, the field of continual learning for KGEs still faces substantial challenges. A critical limitation is the trade-off between efficiency, adaptability to new information, and the robust prevention of catastrophic forgetting. Current benchmarks often do not fully capture the complexities of real-world evolving KGs, which can involve diverse types of changes (e.g., new entities, new relations, structural changes, concept drift). Future directions must include developing more diverse and robust continual learning strategies, potentially combining the strengths of architectural adaptations with sophisticated distillation or replay mechanisms. Furthermore, there is a crucial need for rigorous benchmarks that can effectively evaluate a model's ability to adapt to various forms of KG evolution over extended periods while preventing catastrophic forgetting. It is also important to distinguish continual learning, which focuses on the incremental *learning paradigm* to avoid forgetting, from models that merely *represent* temporal dynamics (as discussed in Subsection 4.1). The ultimate goal is to develop KGE models that can operate autonomously and reliably in perpetually evolving knowledge environments, balancing the acquisition of novel insights with the preservation of foundational knowledge.