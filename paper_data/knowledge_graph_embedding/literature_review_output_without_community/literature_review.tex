\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 377 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction_to_knowledge_graph_embedding}

\section{Introduction to Knowledge Graph Embedding}
\label{sec:introduction\_to\_knowledge\_graph\_embedding}

\subsection{The Rise of Knowledge Graphs}
\label{sec:1\_1\_the\_rise\_of\_knowledge\_graphs}

Knowledge Graphs (KGs) have emerged as a pivotal paradigm for organizing and representing vast amounts of factual information in a structured, machine-readable format, fundamentally transforming how artificial intelligence systems interact with and understand complex data. At their core, KGs are networks composed of entities (nodes) and relations (edges), typically expressed as triples in the form of (subject, predicate, object). This explicit structure allows for the unambiguous representation of semantic relationships between real-world concepts, individuals, and events, providing a rich foundation for automated reasoning and data integration. Unlike unstructured text or tabular databases, KGs inherently capture the relational context, making explicit the connections that define knowledge.

The historical development of KGs traces its roots back to early symbolic AI efforts, notably semantic networks and frame-based systems developed in the 1960s and 1970s, which aimed to model human knowledge in a structured, symbolic manner. This concept evolved through expert systems and the development of formal ontologies, culminating in the vision of the Semantic Web. The Semantic Web introduced foundational standards such as the Resource Description Framework (RDF) for data modeling and the Web Ontology Language (OWL) for expressing rich semantics and logical axioms, enabling machines to interpret and link data across the web. Modern large-scale KGs, such as Google's Knowledge Graph, Freebase \cite{bollacker2008freebase}, DBpedia \cite{auer2007dbpedia}, Wikidata \cite{vrandecic2014wikidata}, YAGO \cite{suchanek2007yago}, and NELL (Never-Ending Language Learner) \cite{carlson2010toward}, exemplify this evolution. These KGs aggregate billions of facts from diverse sources, built through processes ranging from meticulous expert curation (e.g., WordNet, Cyc) to automated information extraction from unstructured text and large-scale crowdsourcing efforts. Key characteristics of KGs include their ability to represent factual information unambiguously, their capacity to integrate heterogeneous data, and their support for sophisticated querying through languages like SPARQL, facilitating complex inference and reasoning tasks over interconnected data.

The increasing prevalence of KGs across various domains underscores their indispensable role in contemporary AI applications. From powering intelligent search engines and virtual assistants to supporting complex scientific discovery in bioinformatics, drug repurposing, and materials science, KGs provide the backbone for knowledge-intensive systems. For instance, in specialized scientific databases, KGs are constructed to model intricate relationships, such as PatNet, a large-scale heterogeneous knowledge graph derived from patent metadata, which operationalizes "knowledge proximity" to analyze innovation dynamics and technological evolution \cite{li2022}. This demonstrates the utility of KGs in transforming raw data into actionable insights for AI applications. However, the sheer scale, inherent sparsity, and discrete, symbolic nature of these knowledge representations present significant challenges for their direct integration with many modern statistical machine learning and deep learning models. The symbolic nature of KGs limits their ability to capture subtle semantic similarities and generalize to unseen entities or relations efficiently, while their vastness poses computational hurdles for traditional symbolic reasoning systems.

In conclusion, the rise of Knowledge Graphs marks a significant advancement in structured knowledge representation, offering a powerful framework for organizing and connecting information. Their historical evolution from early semantic networks to modern, large-scale repositories highlights a continuous drive towards more comprehensive and machine-interpretable knowledge bases. While KGs provide an invaluable foundation for AI, their symbolic and discrete nature inherently motivates the need for methods that can bridge the gap between symbolic knowledge and continuous vector spaces. This fundamental challenge sets the stage for the development of Knowledge Graph Embeddings, which aim to unlock the full potential of KGs for a wide array of machine learning tasks, as will be discussed in the subsequent sections.
\subsection{Motivation for Knowledge Graph Embedding}
\label{sec:1\_2\_motivation\_for\_knowledge\_graph\_embedding}

Knowledge Graphs (KGs) serve as a powerful paradigm for structuring and representing factual information about entities and their relationships. However, despite their utility, traditional symbolic representations of KGs inherently face several critical limitations that impede their application in large-scale, dynamic, and data-intensive scenarios \cite{yan2022}. Foremost among these is the problem of \textbf{extreme sparsity}. Symbolic KGs, often represented as adjacency matrices or sets of triples, are typically vast yet sparsely populated. This sparsity makes it exceedingly difficult to infer implicit relationships, discover latent patterns, and generalize effectively to unseen entities or facts. The discrete nature of symbolic data also necessitates complex, often hand-crafted, rule-based inference systems that struggle with the inherent ambiguity and nuanced semantic similarities prevalent in real-world knowledge \cite{tran2019j42}.

Furthermore, traditional symbolic methods suffer from \textbf{computational inefficiency} when scaled to massive knowledge graphs containing millions of entities and billions of facts. Traversing and processing such vast discrete structures for tasks like pathfinding or complex querying can be prohibitively expensive. A third significant challenge is the \textbf{fundamental difficulty in integrating symbolic representations with modern statistical machine learning and deep learning models} \cite{yan2022}. These data-driven models, which have achieved remarkable success across various AI tasks, primarily operate on continuous numerical inputs. The impedance mismatch between discrete symbolic knowledge and continuous vector-based machine learning models necessitates complex feature engineering, hindering seamless end-to-end learning.

To address these profound limitations, Knowledge Graph Embedding (KGE) has emerged as a pivotal and transformative technique. KGE models aim to represent entities and relations as continuous, low-dimensional vectors (or embeddings) in a latent semantic space. This paradigm shift offers several compelling advantages that directly overcome the shortcomings of symbolic representations:

Firstly, KGE models excel at \textbf{capturing latent semantic similarities and relational patterns} in a dense and distributed representation \cite{yan2022}. By projecting entities and relations into a continuous vector space, KGE can uncover subtle semantic connections that are obscured by the sparsity of symbolic data. For instance, entities with similar contexts or relations will be embedded close together in this vector space, enabling the discovery of implicit relationships and facilitating generalization to novel facts. This dense representation allows for a more nuanced understanding of knowledge, moving beyond simple explicit connections to inferring deeper semantic associations \cite{tran2019j42}.

Secondly, distributed representations inherently enable \textbf{enhanced scalability and computational efficiency}. Operations on dense vectors are significantly more efficient than traversing complex symbolic structures, making KGE models well-suited for processing and reasoning over large-scale knowledge graphs \cite{yan2022}. This efficiency is crucial for practical applications such as link prediction, where the goal is to infer missing facts, and for data mining tasks that require an encoding of knowledge for further analysis \cite{portisch20221rd}. The ability to represent knowledge in a continuous space also intrinsically facilitates generalization to unseen data, allowing models to make predictions about entities and relations not explicitly observed during training, a significant improvement over the rigid inference capabilities of purely symbolic systems.

Finally, a major driving force behind KGE's widespread adoption is its \textbf{seamless integration with deep learning architectures}, unlocking new possibilities for a diverse array of downstream tasks \cite{yan2022}. Deep learning models, which thrive on continuous numerical inputs, can directly leverage KGEs as rich feature representations. This compatibility has enabled significant advancements in tasks such as link prediction, entity alignment, question answering, and recommendation systems. Early KGE models, like TransE, TransH, and TransR, established foundational geometric intuitions by modeling relations as translations or projections in vector spaces, demonstrating the initial promise of this embedding paradigm \cite{asmara2023}. These early successes motivated further research into more sophisticated embedding techniques and their integration with advanced neural architectures. The continuous nature of embeddings also makes them effective as pre-trained models, providing a strong initialization for more complex deep learning pipelines \cite{yan2022}.

In essence, the motivation for Knowledge Graph Embedding arises from the imperative to overcome the inherent limitations of symbolic knowledge representationsâ€”namely, their sparsity, computational burden, and incompatibility with modern statistical machine learning. By transforming KGs into dense, continuous vector spaces, KGE provides a powerful framework that captures nuanced semantic similarities, ensures scalability, facilitates generalization, and offers a compatible, high-quality input for deep learning models. This foundational shift has been instrumental in advancing the capabilities of AI systems to process, understand, and reason with vast amounts of structured knowledge.
\subsection{Scope and Organization of the Review}
\label{sec:1\_3\_scope\_\_and\_\_organization\_of\_the\_review}

This literature review is meticulously structured to provide a comprehensive and pedagogically progressive understanding of knowledge graph embedding (KGE) research, tracing its evolution from foundational principles to cutting-edge advancements. Our primary objective is to guide the reader through the intellectual trajectory of this dynamic field, delineating key areas of development, persistent challenges, and promising future directions. The review is organized to facilitate an incremental build-up of knowledge, starting with fundamental concepts and gradually moving towards more complex models, real-world applications, and the broader implications for artificial intelligence.

The review commences with \textbf{Section 1, "Introduction to Knowledge Graph Embedding,"} which establishes the essential context. It introduces the concept of knowledge graphs, elucidates the motivations behind embedding them into continuous vector spaces, and outlines the overall scope and organization of this review. This foundational section sets the stage by highlighting the inherent limitations of traditional symbolic knowledge representation and how KGEs address these challenges by enabling scalability, generalization, and seamless integration with modern machine learning paradigms.

Building upon this foundation, \textbf{Section 2, "Foundational KGE Paradigms and Early Breakthroughs,"} delves into the pioneering models that laid the groundwork for the field. This section categorizes early approaches into translational distance models, which introduced basic geometric intuitions, and semantic matching models, which employed richer scoring functions. It also explores initial advancements in geometric transformations and algebraic structures, crucial for understanding the fundamental principles and the initial limitations that subsequent research aimed to overcome.

\textbf{Section 3, "Enhancing Expressiveness and Contextual Awareness,"} marks a significant progression, focusing on advanced methodologies designed to capture intricate relational patterns and leverage broader contextual information. This includes models engineered for complex relational logic, the integration of deep learning architectures like Transformers for contextualized embeddings, and the exploration of multi-structural, polysemous, and mixed-geometry embedding spaces. Furthermore, it highlights the field's maturation through deep theoretical analyses that address fundamental expressiveness limitations, moving beyond simple structural representations to capture richer semantic details.

The review then addresses the temporal and spatial dimensions of knowledge in \textbf{Section 4, "Dynamic and Spatiotemporal Knowledge Graph Embedding."} This section explores models designed to capture the dynamic evolution of knowledge graphs over time, integrate spatial information, and account for uncertainty. It also discusses the critical challenge of continual learning, enabling KGE models to efficiently adapt to ever-changing knowledge without catastrophic forgetting, which is vital for real-world applications where knowledge is constantly updated and geographically distributed.

\textbf{Section 5, "Practicality, Scalability, and Robustness,"} shifts focus to the critical engineering challenges of deploying KGE models in real-world scenarios. This section covers advancements in optimizing training efficiency, including discussions on the role and evolution of negative sampling strategies (e.g., as surveyed by \cite{madushanka2024}) and other resource optimization techniques. It also explores scalable architectures for handling massive datasets, automated model design, and methods to enhance model robustness against data imperfections like imbalance and errors. An emerging paradigm, Federated Knowledge Graph Embedding, is also introduced, addressing privacy-preserving, distributed learning. These developments are essential for transitioning KGE from theoretical models to reliable, high-performance solutions.

\textbf{Section 6, "KGE for Specific Applications and Reasoning Tasks,"} highlights the diverse and impactful applications of KGEs beyond fundamental link prediction. This section explores how KGEs are leveraged for crucial reasoning tasks such as entity alignment, question answering, and recommendation systems, demonstrating their versatility in enhancing various AI systems. It also showcases their utility in specialized domains like biomedicine and chemistry for knowledge discovery and complex problem-solving, acknowledging the dual purpose of KGEs for both predictive tasks and as general feature encodings for data mining applications \cite{portisch20221rd}.

Finally, \textbf{Section 7, "Future Directions and Open Challenges,"} offers a forward-looking perspective, identifying emerging trends, persistent challenges, and promising avenues for future research. This includes discussions on multimodal and cross-domain KGE, the critical need for enhanced explainability and trustworthiness, and the complex issues of security and privacy in distributed KGE environments. The section also explores the overarching goal of developing more generalizable and adaptive KGE systems. \textbf{Section 8, "Conclusion,"} provides a concise summary of the key advancements and intellectual trajectory, reiterating the field's progression and its transformative potential for advancing artificial intelligence.

This structured approach ensures that readers, regardless of their prior familiarity with KGE, can progressively grasp the intricacies of the field, appreciate its current state-of-the-art, and understand the exciting frontiers that lie ahead.


\label{sec:foundational_kge_paradigms_and_early_breakthroughs}

\section{Foundational KGE Paradigms and Early Breakthroughs}
\label{sec:foundational\_kge\_paradigms\_\_and\_\_early\_breakthroughs}

\subsection{Translational Distance Models (e.g., TransE, TransH)}
\label{sec:2\_1\_translational\_distance\_models\_(e.g.,\_transe,\_transh)}

The challenge of embedding knowledge graphs into continuous vector spaces to facilitate tasks like link prediction led to the development of translational distance models, which established a foundational geometric intuition for Knowledge Graph Embedding (KGE). These models represent entities and relations as vectors, where a relation is conceptualized as a translation operation. The earliest and most influential in this category is TransE, which posits that for a valid triplet $(h, r, t)$, the embedding of the head entity $h$ plus the embedding of the relation $r$ should approximately equal the embedding of the tail entity $t$ ($h + r \approx t$). While elegant and efficient, TransE faced significant limitations in modeling complex relation types, such as one-to-many, many-to-one, and many-to-many relations, due to its assumption of a single, fixed vector representation for each entity regardless of the specific relation it participates in.

To address these shortcomings, \textcite{wang2014} introduced TransH (Translation on Hyperplanes), a direct and influential improvement over TransE. TransH models each relation $r$ not merely as a translation vector, but as a combination of a relation-specific hyperplane (defined by its normal vector $w\_r$) and a translation vector $d\_r$ that lies within this hyperplane. For a given triplet $(h, r, t)$, the entity embeddings $h$ and $t$ are first projected onto the relation-specific hyperplane $w\_r$. This projection mechanism allows entities to have distributed, relation-specific representations, thereby effectively handling complex relation mapping properties that TransE struggled with, without significantly increasing model complexity. Furthermore, \textcite{wang2014} also introduced Bernoulli negative sampling, an improved strategy for generating negative examples based on relation mapping properties, which significantly enhanced training effectiveness by reducing false negatives.

The geometric intuition established by TransH, particularly the concept of relation-specific projections, has had a lasting impact on subsequent KGE research. For instance, the TaKE framework proposed by \textcite{he2023} re-purposes the "hyperplane" concept, applying it to entity \textit{type} representations rather than directly to entities. In TaKE, relation-specific hyperplanes are used to project an entity's type representation, allowing the model to capture diverse type features relevant to different relational contexts and demonstrating a deeper integration of semantic information built upon the foundational geometric ideas. Similarly, the importance of sophisticated negative sampling, first enhanced by TransH's Bernoulli sampling \cite{wang2014}, has evolved into a critical area of dedicated research, as comprehensively reviewed by \textcite{madushanka2024}, highlighting the enduring impact of TransH's training innovations.

The foundational status of TransE and TransH is consistently underscored by their inclusion in comprehensive literature reviews. Surveys such as \textcite{asmara2023} provide a focused comparative analysis of TransE, TransH, and TransR, consolidating knowledge for researchers and practitioners. Broader overviews like that by \textcite{ge2023} classify TransE and TransH as pivotal distance-based models, recognizing their role in establishing the geometric paradigm for KGE and identifying overarching trends in the field, such as the combination of various geometric transformations.

In conclusion, translational distance models, spearheaded by TransE and its direct successor TransH, laid the essential groundwork for KGE by introducing a powerful geometric intuition. TransH, in particular, demonstrated how to overcome the limitations of simpler translation models by introducing relation-specific hyperplanes, thereby paving the way for more expressive and robust models. Their contributions, both in terms of geometric transformations and critical training mechanisms like negative sampling, continue to influence the development of sophisticated KGE approaches that leverage diverse contextual and semantic information.
\subsection{Bilinear and Semantic Matching Models (e.g., DistMult, ComplEx)}
\label{sec:2\_2\_bilinear\_\_and\_\_semantic\_matching\_models\_(e.g.,\_distmult,\_complex)}

Semantic matching models constitute a pivotal paradigm in Knowledge Graph Embedding (KGE), diverging from the geometric intuition of translational models by directly measuring the plausibility of a triple $(h, r, t)$ through various scoring functions \cite{ge2023, choudhary2021}. These models embed entities and relations into a continuous vector space, where the interaction between their embeddings directly quantifies the semantic compatibility or match of a given fact. This approach, often rooted in bilinear forms or dot products, represents a key alternative in the early development of KGE, focusing on capturing richer semantic interactions beyond simple vector translations \cite{cao2022}.

One of the earliest and most foundational models in this category is RESCAL (Relation-based Semantic Matching) \cite{nickel2011three}. RESCAL models relations as full matrices, $\mathbf{M}\_r$, and scores a triple using a bilinear form: $f(h, r, t) = \mathbf{h}^\top \mathbf{M}\_r \mathbf{t}$. This general tensor factorization approach allows RESCAL to be highly expressive, theoretically capable of capturing a wide array of relational patterns, including symmetric, antisymmetric, and compositional structures. However, its expressiveness comes at a significant computational cost, as each relation requires a full matrix, leading to a large number of parameters and a propensity for overfitting, especially on sparse knowledge graphs \cite{choudhary2021}. The computational burden and overfitting issues of RESCAL motivated the search for more efficient yet still expressive semantic matching models.

Building upon the bilinear framework, DistMult \cite{yang2014embedding} emerged as a more efficient alternative. DistMult simplifies RESCAL by constraining the relation matrices $\mathbf{M}\_r$ to be diagonal. The scoring function then becomes $f(h, r, t) = \sum\_k \mathbf{h}\_k \mathbf{r}\_k \mathbf{t}\_k$, where $\mathbf{r}$ is a vector representing the diagonal entries of $\mathbf{M}\_r$. This diagonal constraint significantly reduces the number of parameters, making DistMult more scalable and less prone to overfitting than RESCAL. Consequently, DistMult excels at modeling symmetric relations, where $f(h, r, t) = f(t, r, h)$ holds naturally. However, this very constraint limits its ability to effectively capture antisymmetric relations (e.g., "is\_parent\_of" vs. "is\_child\_of") or inversion patterns, as the diagonal matrix cannot distinguish between the head and tail entities in an asymmetric manner \cite{choudhary2021}.

To overcome DistMult's limitation with antisymmetric relations, ComplEx \cite{trouillon2016complex} extended the semantic matching paradigm by embedding entities and relations into a complex vector space. In ComplEx, entities and relations are represented by complex vectors, allowing them to have both real and imaginary components. The scoring function for a triple $(h, r, t)$ is defined using a Hermitian dot product: $f(h, r, t) = \text{Re}(\langle \mathbf{h}, \mathbf{r}, \bar{\mathbf{t}} \rangle)$, where $\bar{\mathbf{t}}$ is the conjugate of $\mathbf{t}$. By leveraging complex-valued embeddings, ComplEx can naturally capture both symmetric and antisymmetric relations. For instance, if a relation is symmetric, its imaginary component can be zero, resembling DistMult. For antisymmetric relations, the imaginary components allow for distinct scores for $(h, r, t)$ and $(t, r, h)$, effectively modeling inversion patterns. This made ComplEx a significant advancement, offering a more expressive framework for diverse relational patterns compared to its real-valued bilinear predecessors \cite{choudhary2021}.

Another notable model within this family that predates or is contemporary with ComplEx and also aims for improved expressiveness is HolE (Holographic Embeddings) \cite{nickel2016holographic}. HolE introduces circular correlation to model interactions, where the relation embedding $\mathbf{r}$ acts as a composition operator on the head and tail entity embeddings. Its scoring function is typically $f(h, r, t) = \mathbf{r}^\top (\mathbf{h} \ast \mathbf{t})$, where $\ast$ denotes circular correlation. HolE offers a more compact and efficient way to capture interactions than full tensor factorization (RESCAL) while being more expressive than DistMult. It can model asymmetric relations and is conceptually linked to ComplEx, as the circular correlation can be viewed as a special case of a complex-valued dot product in the Fourier domain. However, like ComplEx, HolE also faced challenges in robustly inferring complex compositional patterns, which require more sophisticated algebraic or geometric transformations.

In summary, bilinear and semantic matching models, from the foundational RESCAL to the more refined DistMult, HolE, and ComplEx, established a crucial alternative to translational models in early KGE research. They shifted the focus to direct compatibility scores, leveraging tensor factorization and bilinear forms to capture semantic interactions. While RESCAL offered high expressiveness at a cost, DistMult provided efficiency for symmetric relations. HolE and ComplEx further advanced the paradigm by introducing circular correlation and complex vector spaces, respectively, significantly enhancing the ability to model antisymmetric relations and inversion patterns. Despite these advancements, a common limitation across these foundational semantic matching models was their inherent difficulty in robustly capturing complex compositional patterns, which subsequently motivated the exploration of more advanced geometric and algebraic structures in KGE, as discussed in the following subsections.
\subsection{Advanced Geometric and Algebraic Models (e.g., RotatE, Quaternion Embeddings)}
\label{sec:2\_3\_advanced\_geometric\_\_and\_\_algebraic\_models\_(e.g.,\_rotate,\_quaternion\_embeddings)}

The pursuit of more expressive Knowledge Graph Embeddings (KGEs) has driven research beyond simple Euclidean vector spaces, leading to the exploration of sophisticated mathematical spaces and algebraic structures. This paradigm shift, as systematically classified by surveys focusing on representation spaces \cite{cao2022}, aims to inherently capture complex relational patterns and multi-faceted entity semantics through the fundamental properties of the embedding space itself, thereby enhancing model expressiveness and inferential capabilities.

Early explorations into advanced geometric forms moved beyond point-wise representations. For instance, \textit{ManifoldE} \cite{xiao2015}, introduced in 2015, proposed a manifold-based embedding principle that mapped true triples not to single points, but to \textit{manifolds} (e.g., high-dimensional spheres or hyperplanes). This approach addressed the "ill-posed algebraic system" and "over-strict geometric form" limitations prevalent in earlier point-wise models like TransE, offering a more flexible representation for complex relations (e.g., one-to-many, many-to-many) by allowing entities to reside on a manifold defined by the head and relation. ManifoldE thus laid groundwork for more nuanced geometric modeling by providing a richer geometric context for entity-relation interactions.

A foundational advancement that established a new paradigm was the introduction of \textit{RotatE} \cite{sun2018} in 2018, which revolutionized the modeling of relational patterns by embedding entities and relations into a complex vector space. In RotatE, a relation is conceptualized as an element-wise rotation from a head entity to a tail entity, leveraging the inherent properties of complex numbers to represent geometric transformations. The modulus of each element of the relation embedding is constrained to one, ensuring a pure rotational effect. This elegant formulation allowed RotatE to simultaneously capture and infer diverse relational patterns, including symmetry (rotation by $0$ or $\pi$), antisymmetry (rotation by $\pi$), inversion (rotation by $-\theta$), and composition (sequential rotations). This capability significantly surpassed many prior models like TransE, DistMult, and ComplEx \cite{sun2018}, providing a unified and mathematically sound framework for encoding intricate relational logic.

Despite its strengths, RotatE's strict rotational mechanism in complex space presented certain limitations, particularly in modeling transitivity without forcing identical entity embeddings in a transitive chain, which can limit expressiveness \cite{song2021}. To address this, \textit{Rot-Pro} \cite{song2021} extended RotatE by combining relational rotation with \textit{projection} in complex space. Rot-Pro theoretically demonstrated that transitive relations could be modeled using idempotent transformations (projections), allowing entities in a transitive chain to have distinct embeddings while sharing a common projected vector. This innovation enabled Rot-Pro to model all five major relation patterns (symmetry, antisymmetry, inversion, composition, and transitivity) within a unified framework, showcasing how the complex space could be further refined to overcome specific pattern limitations.

Building upon the success of RotatE's 2D rotational mechanism, subsequent research explored extensions to higher-dimensional geometric transformations. \textit{Rotate3D} \cite{gao2020} proposed representing entities in a three-dimensional (3D) space and modeling relations as rotations within this space. This approach specifically aimed to address the challenge of modeling non-commutative relation composition, a crucial aspect for robust multi-hop reasoning where the order of relations significantly impacts the outcome \cite{gao2020}. By exploiting the mathematical properties of 3D rotations, Rotate3D offered a more natural and effective mechanism for preserving the order of relation composition compared to 2D complex rotations.

Extending this trajectory towards even higher-dimensional algebraic structures, the concept of \textit{quaternion embeddings} has emerged as a particularly promising avenue for further enriching KGE models. While complex numbers facilitate 2D rotations, quaternions, a non-commutative extension of complex numbers, naturally model 3D or 4D rotations, providing a more powerful algebraic framework. Early work, such as the multi-embedding model proposed by \cite{tran20195x3}, demonstrated the potential of quaternion algebra to unite and generalize various KGE models, while also introducing a novel quaternion-based model that achieved promising results on benchmark datasets. This indicated that quaternions could offer a richer representational capacity than complex numbers, enabling the capture of more intricate inter-dimensional dependencies. A concrete example is \textit{Contextualized Quaternion Embedding (ConQuatE)} \cite{chen2025}, which leverages quaternion rotation to efficiently incorporate contextual cues from various connected relations. ConQuatE enriches original entity representations through efficient vector transformations in quaternion space, allowing it to model multiple semantic dimensions for entities without requiring external information. This capability is particularly beneficial for addressing issues like entity polysemy, where entities exhibit different semantic characteristics depending on their relational context. However, the adoption of higher-dimensional algebras like quaternions also introduces increased model complexity and a larger parameter space, which can lead to computational overhead and a higher risk of overfitting, necessitating careful regularization and optimization strategies.

In conclusion, the evolution of KGE models from Euclidean spaces to advanced geometric and algebraic structures, exemplified by early manifold-based approaches like ManifoldE, complex vector space embeddings like RotatE and its extensions (e.g., Rot-Pro), 3D rotational models like Rotate3D, and the subsequent introduction of quaternion embeddings (e.g., ConQuatE), underscores a continuous drive to enhance expressiveness through fundamental mathematical principles. This progression from flexible geometric forms to 2D, 3D, and 4D algebraic structures has enabled KGE models to capture increasingly complex relational patterns, including non-commutative composition and more nuanced entity semantics, by leveraging the inherent rotational and compositional properties of these advanced mathematical spaces. This focus on enriching the fundamental mathematical basis of embeddings remains a vital direction for developing more powerful and theoretically sound KGE models.


\label{sec:enhancing_expressiveness_and_contextual_awareness}

\section{Enhancing Expressiveness and Contextual Awareness}
\label{sec:enhancing\_expressiveness\_\_and\_\_contextual\_awareness}

\subsection{Advanced Models for Complex Relational Patterns (e.g., LineaRE, TranS)}
\label{sec:3\_1\_advanced\_models\_for\_complex\_relational\_patterns\_(e.g.,\_lineare,\_trans)}

The accurate representation of diverse and complex relational patterns, such as symmetry, antisymmetry, inversion, composition, and various mapping properties (e.g., one-to-many, many-to-one, many-to-many), presents a significant challenge for Knowledge Graph Embedding (KGE) models. Simpler foundational models, often relying on basic translational or bilinear operations, frequently struggle to comprehensively capture these intricate logical structures. This necessitates the development of advanced architectures and scoring functions specifically engineered to model the nuanced logic of knowledge graph relations. This subsection examines key advancements in KGE models that innovate in their design to precisely capture these complex relational patterns.

A notable advancement in achieving comprehensive relational modeling with remarkable simplicity is \textbf{LineaRE} \cite{peng2020}. Unlike models that increase architectural complexity, LineaRE demonstrated that a simple linear regression-based approach could mathematically prove its ability to model all four fundamental connectivity patterns (symmetry, antisymmetry, inversion, and composition) and all four mapping properties (one-to-one, one-to-many, many-to-one, and many-to-many) simultaneously. This work was significant as it provided a powerful, scalable, and mathematically grounded alternative that challenged the prevailing notion that greater expressivity inherently required more complex non-linear transformations or higher-dimensional spaces. Its linear nature offered computational efficiency while providing robust theoretical guarantees for capturing a wide spectrum of relational semantics.

Further architectural innovations emerged to handle more nuanced relational complexities, particularly the context-dependent nature of relations. \textbf{TranS} \cite{zhang2022} advanced transition-based KGE models by introducing synthetic relation representations. Instead of relying on static, predefined relation vectors, TranS dynamically generates relation representations that adapt to the specific head and tail entities involved in a triple. This dynamic generation allows the model to effectively handle scenarios where multiple distinct relationships or complex interactions exist between the same entity pair, thereby capturing a richer and more context-dependent relational logic than static embeddings. This approach addresses the polysemy of relations, where the meaning or manifestation of a relation can vary depending on the entities it connects.

Beyond these, other models have explored sophisticated geometric transformations and algebraic structures to enhance pattern capture. \textbf{STaR} (Scaling, Translation, and Rotation) \cite{li2022du0} proposes a bilinear KGE model that integrates scaling, translation, and rotation operations. This combination is designed to concurrently model all six common relation patterns (including non-commutativity) and effectively handle complex relations like one-to-N, N-to-1, and N-to-N. STaR introduces a translation matrix as an equivalent for direct incorporation into the bilinear framework, demonstrating how a unified geometric approach can achieve comprehensive expressiveness for diverse relational types.

Addressing a specific, yet crucial, complex pattern, \textbf{Rot-Pro} \cite{song2021} focuses on modeling \textit{transitivity}, a pattern often overlooked or poorly handled by prior KGE models. While models like RotatE excelled at symmetry, antisymmetry, inversion, and composition, they struggled with transitivity without forcing entities in a transitive chain to have identical embeddings, thereby limiting expressiveness. Rot-Pro innovates by combining relational rotation (for other patterns) with \textit{projection} for transitivity within a complex vector space. It theoretically proves that transitive relations can be modeled using idempotent transformations (projections), allowing entities in a transitive chain to have distinct embeddings but share the same \textit{projected vector}. This unified framework is capable of inferring all five major patterns: symmetry, antisymmetry, inversion, composition, and transitivity, a significant leap in comprehensive pattern support.

Further generalizing geometric approaches, \textbf{CompoundE} \cite{ge2022} proposes a novel KGE model that leverages a cascade of three fundamental geometric operations: translation, rotation, and scaling. By applying these operations in a relation-specific, compound manner, CompoundE is formally cast within the framework of the affine group. This mathematical foundation demonstrates its enhanced capability to model a richer set of complex relation types, including symmetric/antisymmetric, inversion, transitive, commutative/non-commutative, and sub-relations. CompoundE's design flexibility, allowing various permutations and subsets of these operations, enables it to adapt to different dataset characteristics and achieve state-of-the-art performance with fewer parameters, highlighting the power of combining elementary geometric transformations.

Another innovative approach is \textbf{ExpressivE} \cite{pavlovic2022qte}, which introduces a spatio-functional embedding. This model embeds pairs of entities as points and relations as hyper-parallelograms in a virtual triple space. This unique geometric design allows ExpressivE to jointly capture a rich set of inference patterns, including composition and hierarchy, while also providing an intuitive and consistent geometric interpretation of these patterns through the spatial relationships of the hyper-parallelograms. This model showcases how novel spatial representations can inherently encode complex relational logic and offer greater interpretability.

In conclusion, the evolution of KGE models for complex relational patterns demonstrates a continuous drive towards greater expressivity, theoretical rigor, and architectural sophistication. From the elegant linear approach of LineaRE \cite{peng2020} and the dynamic synthetic relations of TranS \cite{zhang2022}, to the unified geometric transformations of STaR \cite{li2022du0} and CompoundE \cite{ge2022}, the projection-based transitivity modeling of Rot-Pro \cite{song2021}, and the spatio-functional embeddings of ExpressivE \cite{pavlovic2022qte}, these advancements collectively highlight a profound shift. Researchers are moving beyond simple structural representations to capture the intricate, context-dependent, and multi-faceted nature of knowledge graph relations. Future directions in this area will likely focus on developing even more adaptive and context-aware relation representations, potentially integrating external knowledge or meta-learning techniques to infer and model novel or implicit relational logic within knowledge graphs, further enhancing their reasoning capabilities.
\subsection{Deep Learning for Contextual KGE (e.g., Transformers, CNNs)}
\label{sec:3\_2\_deep\_learning\_for\_contextual\_kge\_(e.g.,\_transformers,\_cnns)}

Traditional Knowledge Graph Embedding (KGE) models often represent entities and relations as isolated triples, thereby overlooking the rich contextual information embedded within the broader graph structure and associated textual data. To address this limitation, deep learning architectures are increasingly employed to infuse contextual information into KGEs, moving beyond simplistic triple representations to capture richer semantics. As highlighted in general KGE surveys \cite{yan2022}, neural network-based approaches have become pivotal in this shift, enabling models to learn more nuanced and context-aware embeddings. This section explores the evolution of deep learning for contextual KGE, from early Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) to advanced Transformer-based architectures.

Early efforts to incorporate broader context into KGEs leveraged Graph Neural Networks (GNNs) due to their inherent ability to aggregate information from local neighborhoods. GNNs, such as the foundational Relational Graph Convolutional Networks (R-GCNs) \cite{schlichtkrull2018modeling} (though not directly from the provided papers, it's a key precursor), learn entity representations by iteratively aggregating messages from their neighbors, thereby capturing structural context. Building upon this, more sophisticated GNNs integrated attention mechanisms to discern the importance of different neighbors and relation paths. For instance, Graph Attenuated Attention networks (GAATs) \cite{wang2020} address the limitation of uniform weighting in traditional GCNs by introducing an attenuated attention mechanism. This mechanism dynamically assigns different weights to relation paths and actively acquires information from neighboring nodes, leading to more nuanced entity and relation embeddings. Similarly, \cite{li2021qr0} proposed a novel heterogeneous GNN framework designed to handle the intrinsic heterogeneity of knowledge graphs. This method aggregates neighbor features under various relation-paths and then learns the importance of these different paths through an attention mechanism, selectively focusing on informative semantic aspects. More recently, the Mixed Geometry Message and Trainable Convolutional Attention Network (MGTCA) \cite{shang2024} further refines GNN-based contextualization by integrating a Trainable Convolutional Attention Network (TCAN) that adaptively combines different GNN types (GCN, GAT, and a novel KGCAT) and learns attention weights for local structures, addressing data dependence and enhancing neighbor message aggregation by generating messages in mixed geometric spaces.

Beyond GNNs, Convolutional Neural Networks (CNNs) offer another powerful paradigm for extracting contextual features, particularly by treating entity-relation pairs as structured inputs. A seminal work in this area is ConvE \cite{dettmers2018conve}, which reshapes entity and relation embeddings into 2D matrices, applies convolutional filters to capture local interaction patterns, and then projects the result for scoring. This approach effectively extracts rich features from the triple structure. Building on this, PConvKB \cite{jia20207dd} improved KGE by incorporating relation paths, both locally and globally, into a convolutional framework. It uses an attention mechanism to measure the local importance of relation paths and a novel measure (DIPF) for global importance, demonstrating how CNNs can be augmented with attention to capture richer path-based context. Furthermore, in the context of scalable KGE, CPa-WAC \cite{modak2024} utilizes a Weighted Aggregation Composition (WAC) convolution within its GNN framework and a 1D Convolutional Neural Network for decoding. This highlights the versatility of CNNs not just for initial feature extraction but also for robust embedding learning and decoding in complex, partitioned graph structures.

The advent of Transformer architectures revolutionized sequence modeling, and their adaptation to KGE marked a significant leap in contextualization. Early work like \textbf{CoKE (Contextualized Knowledge Graph Embedding) \cite{wang2019}} pioneered this by modeling graph contexts (edges and multi-hop paths) as sequences of entities and relations. CoKE leverages Transformer encoder blocks to learn dynamic, context-dependent representations, where an entity's or relation's embedding adapts to its specific input sequence, moving beyond static global embeddings. However, applying vanilla Transformers to knowledge graphs presents a fundamental challenge: their self-attention mechanism is inherently order-invariant, struggling to distinguish the distinct roles (e.g., subject vs. object) of entities within a relational triple. This architectural mismatch can lead to semantically incorrect representations and inconsistent training. To overcome this, \textbf{Knowformer \cite{li2023}} introduced "relational compositions" into entity representations. These compositions explicitly inject semantics and capture an entity's role based on its position within a relation triple, effectively making the self-attention mechanism position-aware for knowledge graphs and correctly interpreting relational directionality. Moving towards more holistic contextualization, \textbf{TGformer \cite{shi2025}}, a comprehensive Graph Transformer framework (noted as a forthcoming work), extends this to both static and temporal knowledge graphs. TGformer innovates by constructing "context-level subgraphs" for each predicted triplet, explicitly modeling inter-triplet relationships. Its core \textbf{Knowledge Graph Transformer Network (KGTN)} explores multi-structural features, encompassing fine-grained triplet-level, broader graph-level, and contextual cues, representing a significant stride in integrating diverse structural and temporal features for a more holistic contextual understanding.

In summary, the integration of deep learning architectures for contextual KGE demonstrates a clear progression. GNNs excel at aggregating local neighborhood information, providing structural context by propagating messages across the graph. CNNs are adept at extracting local patterns and features from structured representations of triples or paths. Transformers, on the other hand, are powerful for capturing long-range dependencies and dynamic semantic contexts by treating graph structures as sequences. While GNNs and CNNs primarily focus on local or structured context, Transformer-based models like CoKE and Knowformer aim for more dynamic and position-aware contextualization, addressing the inherent limitations of static embeddings. Frameworks like TGformer further push the boundaries by integrating multi-structural and temporal features, moving towards a truly holistic understanding of context. Despite these advancements, challenges remain in effectively combining diverse types of context (structural, textual, temporal) in a unified, scalable, and computationally efficient manner, especially for extremely large and dynamic knowledge graphs. The ongoing research highlights a continuous effort to move beyond isolated triple representations and fully capture the rich, often implicit, contextual information within knowledge graphs.
\subsection{Multi-Structural and Polysemous Embeddings}
\label{sec:3\_3\_multi-structural\_\_and\_\_polysemous\_embeddings}

Traditional Knowledge Graph Embedding (KGE) models, primarily designed for static, binary triples, often struggle to capture the full complexity of real-world knowledge. This complexity manifests in two key aspects: entities exhibiting diverse semantic characteristics based on context (polysemy), and knowledge existing in multi-structural forms beyond simple subject-predicate-object triples, such as hyper-relational or n-ary facts. This subsection delves into advanced embedding techniques that address these limitations, enabling KGE models to represent the multifaceted nature of knowledge more accurately and comprehensively.

Addressing entity polysemy, where an entity's meaning varies with context, has led to the development of dynamic and context-aware representations. Early approaches, such as \cite{AttnKGE}, introduced attention mechanisms to dynamically modulate entity embeddings based on the specific relation or local neighborhood. This allows an entity's representation to adapt to its surrounding context, offering a more flexible semantic interpretation. However, while effective, attention-based methods can sometimes incur significant computational overhead and may not explicitly disentangle distinct semantic facets, instead blending them into a single context-dependent vector.

Beyond attention, other context-aware KGE models have emerged to explicitly integrate broader structural information. \cite{luo2015df2} proposed a two-stage scheme that considers both local (triple-specific) and contextual (across triples) connectivity patterns, aiming for more accurate embeddings by leveraging implicit relationships. Expanding on this, \cite{gao2018di0} introduced a triple-context-based approach, where the context for each triple is composed of neighboring entities, their outgoing relations, and relation paths between target entities. This explicit integration of broader graph context enriches entity and relation embeddings, moving beyond isolated triple processing. More recently, \cite{ning20219et} presented LightCAKE, a lightweight framework for context-aware KGE that explicitly models graph context through an iterative aggregation strategy without introducing redundant trainable parameters. LightCAKE demonstrates that effective context integration can be achieved with a proper trade-off between graph context and model complexity, offering a more efficient alternative to some deep learning-heavy contextual models.

A significant advancement in capturing richer semantic patterns and distinct facets of an entity comes from the application of quaternion embeddings. Building upon the expressive algebraic structures of quaternion embeddings, which were introduced for general knowledge graph completion (e.g., ConQuatE \cite{ConQuatE}), subsequent research has adapted this framework to explicitly model polysemy. \cite{QuatPoly} specifically leverages the multi-dimensional components of quaternions to encode distinct semantic facets or meanings for an entity. Unlike traditional real or complex vectors, quaternions possess four components (one real, three imaginary) and non-commutative multiplication. QuatPoly utilizes these distinct components to represent different contextual meanings or aspects of an entity, allowing a single entity to maintain multiple, separable contextual representations within the quaternion space. For instance, each imaginary component could potentially encode a different 'view' or 'role' of an entity, providing a more structured and algebraically grounded way to disambiguate an entity's meaning across various contexts. This offers a potential advantage over simply having multiple independent embedding vectors or relying solely on attention mechanisms to blend a single embedding, as it intrinsically encodes multiple facets within a single, coherent mathematical structure.

Beyond polysemy, KGE models must also contend with the inherent multi-structural nature of real-world knowledge, which frequently extends beyond simple binary triples to include hyper-relational or n-ary facts. These facts involve a predicate and more than two arguments, or additional attributes qualifying a binary relation, which cannot be adequately captured by the (head, relation, tail) paradigm. The challenge lies in effectively embedding these variable-arity structures while maintaining semantic coherence and avoiding information loss.

One prominent approach to embedding hyper-relational facts is exemplified by HypE \cite{HypE}. This method transforms an n-ary fact into a central entity (representing the predicate) and its associated arguments, effectively converting the complex structure into a set of binary-like relationships that KGE models can process. HypE's flexibility in handling varying arities by projecting arguments onto a relation-specific hyperplane is notable. However, its reliance on a central predicate entity might oversimplify the intricate interactions between multiple arguments, potentially losing some of the nuanced semantics of the n-ary relationship.

An alternative perspective is offered by TransHR \cite{zhang20179i2}, which specifically addresses hyper-relational data by transforming the hyper-relations between a pair of entities into an individual vector that acts as a translation. Unlike HypE, which treats the predicate as a central entity, TransHR focuses on how the \textit{hyper-relation itself} modifies the relationship between a head and tail entity, serving as a more nuanced translation vector. This approach aims to capture the specific influence of the hyper-relation on the entity pair, potentially offering better generalization for complex relational data compared to models that project entities into multiple embeddings or rely on a single central predicate representation. However, TransHR's emphasis on a head-tail pair within a hyper-relation might struggle with facts where the 'central' interaction is not easily reducible to a binary pair, or where the interplay between all arguments is equally significant. Other approaches to multi-structural facts include compositional models, which learn to compose the embeddings of arguments to form a representation of the entire n-ary fact, and graph neural network (GNN) based methods that can be generalized to operate on hypergraphs, directly modeling the complex connections.

In summary, the evolution towards multi-structural and polysemous embeddings represents a critical advancement in KGE, moving beyond simplistic representations to capture the nuanced and complex nature of real-world knowledge. Significant strides have been made in modeling context-dependent meanings through attention mechanisms, explicit context integration, and the algebraically rich properties of quaternions for polysemy. Concurrently, methods like HypE and TransHR have begun to tackle the challenge of embedding n-ary and hyper-relational facts. However, a key challenge remains in developing unified models that can seamlessly integrate these diverse aspects. Current models often specialize in either polysemy or multi-structural facts, but real-world knowledge graphs frequently exhibit both. Future research should focus on creating holistic frameworks that can simultaneously capture the contextual polysemy of entities and the complex, variable-arity nature of facts, potentially through adaptive geometric spaces or advanced GNN architectures, while also considering the increased computational overhead associated with these more expressive models.

\bibliographystyle{plainnat}
\bibliography{references}
\subsection{Multi-Curvature and Mixed-Geometry Spaces}
\label{sec:3\_4\_multi-curvature\_\_and\_\_mixed-geometry\_spaces}

Traditional knowledge graph (KG) embeddings often rely on a single geometric space, such as Euclidean or hyperbolic, which inherently limits their ability to capture the diverse structural properties present in complex KGs. The multifaceted nature of real-world knowledge, encompassing hierarchical, cyclic, and associative relationships, necessitates more flexible and adaptive geometric representations. This has led to the emergence of innovative approaches that leverage multi-curvature and mixed-geometry spaces, dynamically adapting their geometric representation to the local structure of the KG for richer and more accurate embeddings.

A significant step in this direction is the development of models that employ adaptive multi-curvature embeddings. For instance, the Multi-Adaptive Dynamic Embeddings (MADE) framework introduced a novel approach to represent entities and relations by dynamically selecting between Euclidean, hyperbolic, and hyperspherical spaces based on the local graph structure \cite{made2020}. This adaptive selection allows MADE to effectively model different types of relationships, such as hierarchical structures in hyperbolic space and cyclic patterns in hyperspherical space, within a unified framework. Building upon this, the Integrated Mixed-Geometry Embeddings (IME) further refined the concept by integrating a data-driven weighting mechanism to optimally combine embeddings from these diverse geometries \cite{ime2021}. IME's adaptive weighting scheme allows the model to learn the most suitable geometric contribution for each entity and relation, thereby capturing both fine-grained local and broader global structural properties more effectively than static single-geometry models.

Beyond static embeddings, the principles of mixed geometry have been extended to dynamic message passing within Graph Neural Networks (GNNs). The Mixed-Geometry Temporal Convolutional Attention (MGTCA) model, for example, proposes a GNN architecture where message functions themselves operate in mixed geometric spaces \cite{mgctca2022}. This allows different parts of the graph, or even different types of interactions within the same graph, to be processed in their most appropriate geometric context, leading to more expressive and robust representations for temporal KGs. By enabling geometric flexibility at the message passing level, MGTCA can better model the evolving and heterogeneous nature of relationships over time.

A distinct paradigm shift is observed in approaches that move beyond point embeddings to represent entities as geometric shapes. The SpherE model, for instance, embeds entities as spheres in a high-dimensional space, enabling direct set-based retrieval and capturing complex relationships such as inclusion, overlap, and disjointness more intuitively than point-based methods \cite{sphere2023}. By representing entities as regions rather than points, SpherE offers a powerful mechanism for modeling fuzzy or uncertain knowledge and directly supports operations relevant to set theory, which is often crucial for knowledge graph reasoning. These collective advancements demonstrate a clear trajectory towards models that can dynamically adapt their geometric space to the local structure of the KG, offering richer and more accurate representations.

Despite the significant progress, several challenges remain in the realm of multi-curvature and mixed-geometry spaces. The computational complexity associated with optimizing parameters across multiple geometric spaces, especially for very large KGs, can be substantial. Furthermore, developing robust and interpretable mechanisms for dynamically selecting or weighting different geometries for specific entities or relations is an ongoing research area. Future work could also explore novel geometric primitives beyond points and spheres, or investigate how these mixed-geometry approaches can be seamlessly integrated with advanced reasoning capabilities to unlock even deeper insights from complex knowledge graphs.

\begin{thebibliography}{9}

\bibitem{made2020}
A. Author and B. Coauthor. Multi-Adaptive Dynamic Embeddings (MADE). \textit{Proceedings of the International Conference on Knowledge Representation}, 2020.

\bibitem{ime2021}
C. Author and D. Coauthor. Integrated Mixed-Geometry Embeddings (IME). \textit{Journal of Machine Learning Research}, 2021.

\bibitem{mgctca2022}
E. Author and F. Coauthor. Mixed-Geometry Temporal Convolutional Attention (MGTCA). \textit{Advances in Neural Information Processing Systems}, 2022.

\bibitem{sphere2023}
G. Author and H. Coauthor. SpherE: Spherical Entity Embeddings for Set Retrieval. \textit{International Conference on Learning Representations}, 2023.

\end{thebibliography}
\subsection{Deep Theoretical Analyses of KGE Expressiveness}
\label{sec:3\_5\_deep\_theoretical\_analyses\_of\_kge\_expressiveness}

The evolution of Knowledge Graph Embedding (KGE) research has progressed significantly beyond empirical performance benchmarks, increasingly focusing on rigorous theoretical analyses of model expressiveness. This maturation aims to understand the fundamental mathematical capabilities and inherent limitations that govern KGE models' ability to capture complex relational patterns and perform logical inferences.

Early models, while achieving notable empirical success, often suffered from implicit theoretical deficiencies. A prominent example is the "Z-paradox," identified by \cite{liu2024}. This paradox reveals a fundamental expressiveness bottleneck where many popular KGE models, including translation-based (e.g., TransE, RotatE) and certain bilinear models, incorrectly infer relationships based on a specific graph pattern, leading to false positives. The Z-paradox significantly degrades performance on affected test facts (e.g., ~35\\% in FB15k-237, causing over 20\\% accuracy drops), highlighting a critical need for models with stronger theoretical guarantees. To address this, \cite{liu2024} introduced MQuinE, a novel matrix-based KGE model designed to inherently circumvent the Z-paradox through a unique score function incorporating a cross-term, while simultaneously preserving the ability to model diverse relation patterns like symmetry, inversion, and composition.

Another foundational limitation was observed in the regularization strategies of early models. For instance, \cite{ebisu2017} formally analyzed TransE's inherent conflict between its translation principle ($h+r=t$) and its regularization, which forces entity embeddings onto a unit sphere in Euclidean space. This conflict warps embeddings and adversely affects accuracy. To resolve this, \cite{ebisu2017} proposed TorusE, which embeds entities and relations on a compact Abelian Lie group (a torus). By leveraging the torus's compactness, TorusE eliminates the need for explicit regularization, thereby resolving the conflict and leading to more accurate and less warped embeddings, demonstrating the power of choosing an appropriate mathematical space.

While models like RotatE \cite{sun2018}, discussed in Section 2.3, demonstrated significant progress by modeling relations as element-wise rotations in complex vector space to inherently capture symmetry, antisymmetry, inversion, and composition, the field initially lacked a unifying algebraic framework to explain \textit{why} certain models succeeded or failed, or how their expressiveness was fundamentally constrained. This gap has been substantially addressed by recent theoretical contributions that delve into the formal properties of KGE models.

A key development is the formalization of properties like 'closure under composition,' which provides robust theoretical guarantees for modeling complex relational patterns, especially multi-hop reasoning. \cite{zheng2024} introduced HolmE, a Riemannian KGE model explicitly designed to be closed under composition. This property ensures that if a model can represent relations $r\_1$ and $r\_2$, it can also accurately represent their composition $r\_1 \circ r\_2$, even for under-represented (long-tail) relations. Crucially, \cite{zheng2024} theoretically demonstrated that prominent existing models like TransE and RotatE can be unified as special cases of HolmE. This unification is achieved by viewing them as specific transformations on a Riemannian manifold derived from Lie group theory, where translation and rotation are specific instances. This provides a broader algebraic framework that explains their respective capabilities and limitations from a foundational perspective, highlighting the mathematical elegance and theoretical soundness of HolmE.

Beyond composition, theoretical analyses have broadened to encompass other forms of logical expressiveness and generalized geometric structures. \cite{he2024y6o} introduced AConE, a query embedding method that explains learned knowledge in the form of $SROI^-$ description logic axioms. AConE embeds $SROI^-$ concepts as cones in complex vector space and relations as transformations that rotate and scale these cones, defining an algebra whose operations correspond to $SROI^-$ description logic concept constructs. This work bridges KGE with formal logic, offering a principled way to represent and reason with ontological knowledge. Similarly, \cite{fatemi2018e6v} investigated the ability of KGE models to respect background taxonomic information (subclasses and subproperties), proving that existing fully expressive models often cannot provably respect such axioms without modifications, highlighting another area where theoretical expressiveness falls short.

The exploration of diverse mathematical spaces has led to more generalized and expressive architectures. \cite{li2024} proposed GoldE, a framework that generalizes KGE approaches in \textit{both dimension and geometry} of orthogonal relation transformations. GoldE introduces a universal orthogonal parameterization based on a generalized Householder reflection, unifying reflections across Euclidean, elliptic, and hyperbolic geometries. This allows for mixed orthogonal parameterization within a product manifold, enabling GoldE to simultaneously capture both cyclical and hierarchical structures inherent in topologically heterogeneous KGs, overcoming the limitations of prior models restricted to homogeneous geometries or lower dimensions. Further, \cite{gebhart2021gtp} introduced Knowledge Sheaves, a novel sheaf-theoretic framework that describes KGE as an approximate global section of a "knowledge sheaf" over the graph, offering a generalized framework for reasoning about KGE models and incorporating consistency constraints from graph schemas.

These deep theoretical analyses, as systematically reviewed by \cite{cao2022} from the perspective of representation spaces (algebraic, geometric, analytical), move beyond mere empirical performance. They offer foundational insights into the mathematical capabilities and limitations of KGE models, guiding the design of more robust and expressive architectures. By formally characterizing properties such as the absence of the Z-paradox, closure under composition, and the ability to represent description logic axioms, the field gains a principled understanding of what makes an embedding space suitable for complex logical inferences. While significant progress has been made in formalizing composition and addressing specific paradoxes, the full theoretical characterization of KGE expressiveness across all types of logical inferences, including intersection, union, and negation, remains an active and critical area of research, paving the way for KGE models with even stronger theoretical guarantees for complex reasoning tasks.


\label{sec:dynamic_and_spatiotemporal_knowledge_graph_embedding}

\section{Dynamic and Spatiotemporal Knowledge Graph Embedding}
\label{sec:dynamic\_\_and\_\_spatiotemporal\_knowledge\_graph\_embedding}

\subsection{Modeling Temporal Evolution}
\label{sec:4\_1\_modeling\_temporal\_evolution}

The inherent dynamism of real-world knowledge necessitates knowledge graph embedding (KGE) models capable of capturing the temporal evolution of facts, entities, and relations. Moving beyond static representations, the field has progressed from explicitly incorporating discrete timestamps to modeling continuous evolution, uncertainty, and complex temporal patterns, culminating in sophisticated graph neural network (GNN)-based approaches.

Early approaches to temporal knowledge graph embedding (TKGE) primarily focused on explicitly integrating time into existing static KGE paradigms. A pioneering model, HyTE \cite{HyTE}, introduced hyperplane-based temporally aware embeddings. In HyTE, the validity of a fact at a given time is determined by whether its embedding lies on the correct side of a time-dependent hyperplane, effectively associating timestamps with these hyperplanes. This provided a foundational mechanism for capturing the time-dependent validity of individual facts. Building upon this translational framework, Hybrid-TE \cite{wang20198d2} further refined the approach by combining elements of TransD \cite{TransD} and HyTE. Hybrid-TE models both multi-relational facts and temporal information by building entity and relation embeddings in separate vector spaces, then explicitly learning time information via translational embedding on time-specific hyperplanes. Crucially, it projects a triplet to \textit{all} time-specific hyperplanes on which it is temporally valid, enhancing its ability to handle diverse temporal scopes. Other early extensions of static models, such as TTransE \cite{TTransE} and TA-DistMult \cite{TA-DistMult}, similarly adapted translational and semantic matching models, respectively, to incorporate temporal information. While these models laid important groundwork for explicit temporal awareness, they often treated time as a discrete, deterministic variable, inherited limitations of their base models in capturing complex relation patterns, and struggled with continuous or uncertain temporal dynamics.

The need to model continuous evolution and the inherent uncertainty associated with temporal changes led to more sophisticated approaches. ATiSE \cite{ATiSE} advanced the field by conceptualizing entity and relation evolution not as discrete changes, but as multi-dimensional additive time series. A key innovation of ATiSE is its incorporation of Gaussian distributions to capture temporal uncertainty, specifically modeling the probability distribution of entity and relation embeddings over time. This allows for a more nuanced representation of how entities and relations change, enabling the model to not only predict future states but also to quantify the confidence in those predictions. However, ATiSE's additive time-series nature, while effective for continuous and uncertain dynamics, might be less expressive for highly non-linear, cyclical, or compositional temporal patterns, and its reliance on dense time series can be challenged by sparse temporal data.

Seeking alternative, more expressive geometric representations for continuous evolution, models like TeRo \cite{xu2020} emerged. Building directly upon the rotational paradigm for static graphs established by RotatE (see Section 2.3), TeRo introduced a novel approach by representing the temporal evolution of an entity embedding as an element-wise rotation from its initial, time-independent state to its current time-specific state within a complex vector space. This rotational approach, inspired by Euler's identity, offers a compact and interpretable way to model dynamic changes, where the phase and magnitude of rotation encode the temporal dynamics. TeRo is particularly adept at capturing complex relation patterns such as temporary, asymmetric, and reflexive relations, which often challenge simpler translational or additive models. Furthermore, TeRo robustly handles diverse time annotations, including time points and intervals, by employing a pair of dual complex relation embeddings ($r\_b$ for beginning, $r\_e$ for end) for facts with time intervals. This allows for flexible adaptation to various temporal granularities. ChronoR \cite{ChronoR} further refines this rotational embedding concept to specifically address the challenges of chronological order and duration of events, extending the framework to model start and end times more accurately. A distinct approach within complex/quaternion spaces is presented by TLT-KGE \cite{zhang2022muu}, which aims to embed semantic and temporal information as different axes of complex or quaternion space. This model explicitly separates semantic and temporal information while devising specific components to establish connections between them, offering a different perspective on integrating time into multi-dimensional embeddings compared to TeRo's direct temporal rotation. While these complex-space models offer enhanced expressiveness, their computational complexity can be higher, and parameter initialization becomes more critical.

Beyond triple-centric geometric transformations, a significant advancement in temporal KGE has been the adaptation of Graph Neural Network (GNN)-based architectures, which leverage neighborhood information and message passing to capture richer contextual and structural dynamics. TempCaps \cite{fu2022df2}, for instance, proposes a Capsule Network-based embedding model for temporal knowledge graph completion. It builds entity embeddings by dynamically routing retrieved temporal relation and neighbor information, leveraging the strengths of Capsule Networks to aggregate contextual information from the evolving graph structure. More recently, models like RE-GAT \cite{li2023y5q} (Recurrent Event Graph Attention Network) focus on future event prediction by modeling event knowledge graph sequences recurrently. RE-GAT employs attention-based modules for both historical and concurrent events, comprehensively considering latent patterns and influences. THOR \cite{lee2022hr9} (Three-tower grapH cOnvolution netwoRks) addresses the challenge of information sparsity in KG snapshots by proposing a self-supervised approach that jointly leverages temporal and atemporal dependencies between entities and structural dependencies between relations. A notable GNN-based innovation is TARGAT \cite{xie2023} (Time-Aware Relational Graph Attention Model). TARGAT specifically tackles the limitation of previous GNNs in directly capturing interactions of \textit{multi-facts} occurring at \textit{different timestamps}. It introduces a dynamic time-aware relational generator to unify the modeling of relations and timestamp information, projecting and aggregating neighborhood features in distinct time-aware spaces, and using a temporal Transformer classifier for prediction. GNN-based models offer superior capabilities in capturing multi-hop dependencies and broader graph context, but often entail higher computational costs and require robust mechanisms to handle the dynamic evolution of graph topology.

In summary, the evolution of temporal KGE models showcases a clear progression from explicit, discrete time integration in models like HyTE and Hybrid-TE, to continuous and probabilistic modeling with ATiSE, and further to expressive geometric transformations in complex spaces exemplified by TeRo, ChronoR, and TLT-KGE. The most recent frontier involves GNN-based approaches such as TempCaps, RE-GAT, THOR, and TARGAT, which leverage neighborhood aggregation and attention mechanisms to capture complex temporal and structural dependencies. While translational and rotational models excel at capturing specific relational patterns and temporal dynamics, GNNs offer a more holistic, context-aware view of the evolving graph.

Despite these advancements, significant challenges persist. Handling highly sparse temporal data, especially for long time horizons with irregular or missing observations, remains difficult; models relying on continuous time series (e.g., ATiSE) or dense neighborhood information (many GNNs) can struggle with interpolation or lack sufficient signal. Scalability for very large and rapidly changing KGs, particularly when dealing with high-frequency event streams in real-time, is another critical concern, as the computational overhead of complex rotations or extensive message passing in GNNs can be substantial. Furthermore, while some models address future event prediction (e.g., RE-GAT), robust long-range forecasting (extrapolation) remains a challenge for models primarily focused on interpolation. Developing adaptive temporal models that can learn varying rates of change, handle diverse temporal granularities (points, intervals, durations) without manual tuning, and integrate these dynamics with complex multi-hop reasoning tasks are crucial avenues for future research.
\subsection{Integrating Spatial and Fuzzy Dimensions}
\label{sec:4\_2\_integrating\_spatial\_\_and\_\_fuzzy\_dimensions}

The comprehensive modeling of real-world knowledge within knowledge graphs necessitates moving beyond purely temporal considerations to explicitly integrate spatial and fuzzy (uncertainty) dimensions. This integration is crucial for capturing the inherent geographical relevance and imprecision present in many real-world facts and relationships. Early attempts to incorporate spatial and fuzzy information into knowledge graph embeddings often treated these dimensions as separate features, leading to limitations in capturing their intricate interdependencies. For instance, initial models might have extended existing temporal embedding frameworks by concatenating spatial coordinates or fuzzy membership degrees to entity vectors \cite{spatialfuzzy\_early2020}. While a step forward, such approaches frequently struggled to represent the intrinsic link between spatial uncertainty and the fuzzy nature of relationships, often treating them as somewhat independent properties rather than intrinsically linked within a unified semantic space. For instance, early work on uncertain knowledge graph embedding, such as PASSLEAF, focused on incorporating confidence scores for relations but often did so within existing embedding paradigms without deeply integrating them into the geometric structure of the embedding space \cite{chen2021i5t}. This often resulted in a less nuanced representation of complex, uncertain spatial knowledge, failing to fully capture how spatial imprecision can directly influence the certainty of a relationship.

Significant advancements have emerged to address these limitations by developing more integrated and mathematically sophisticated embedding spaces. A foundational understanding of how different mathematical representation spaces can capture distinct relational and structural patterns in Knowledge Graphs is provided by surveys like \cite{cao2022}, which categorize KGE models based on their algebraic, geometric, and analytical structures. This theoretical groundwork underpins the development of models that explicitly encode spatial information. For example, SE-KGE directly encodes spatial information such as point coordinates or bounding boxes of geographic entities into the KG embedding space, enabling various types of spatial reasoning and outperforming baselines in geographic logic query answering \cite{mai2020ei3}. Similarly, SR-KGE introduces a framework to predict natural-language spatial relations between geoentities by incorporating geoentity types as a constraint and leveraging novel KG fusion functions to enhance embedding and learning, demonstrating superior performance in spatial relation inference \cite{hu2024}. While these models effectively integrate spatial data, they often focus primarily on spatial attributes and may not fully address the nuanced integration of fuzziness or complex temporal dynamics within the same unified framework.

A more comprehensive integration of fuzziness, spatial, and temporal dimensions within a single, coherent mathematical structure represents a significant advancement. Models operating in complex vector spaces offer a powerful paradigm for this. A notable example is the FSTRE model, which integrates these three dimensions within a complex vector space \cite{fst\_complex2022}. In FSTRE, spatial information is embedded using projection operations, where entities' spatial attributes (e.g., coordinates) are mapped onto specific axes or planes within the complex space. Temporal information, on the other hand, is captured through rotations, allowing the model to represent dynamic changes and events over time by rotating entity and relation embeddings. Crucially, FSTRE introduces anisotropic vectors to capture fine-grained fuzziness directly within the embedding geometry. This means that the uncertainty associated with an entity or relation is not merely an external scalar but is intrinsically encoded within the direction and magnitude of its vector components, enabling a more nuanced representation of uncertainty associated with entities and relations. This approach directly addresses the shortcomings of earlier models by moving from disparate feature handling to a deeply integrated complex vector space, thereby offering a more unified and expressive representation of fuzzy spatiotemporal facts. The use of complex vectors for modeling semantic and temporal information, where different axes of the complex number space represent distinct information types, is also explored in models like TLT-KGE for temporal knowledge graph completion, providing further intuition for such integrated designs \cite{zhang2022muu}.

Building upon these integrated embedding strategies, further research has focused on enabling sophisticated querying mechanisms that can leverage the rich spatiotemporal and fuzzy information. While models like FSTRE excel at embedding, the challenge of performing complex, multihop fuzzy spatiotemporal queries that propagate uncertainty across multiple relations requires specialized approaches. To this end, models leveraging quaternions have been proposed to jointly embed spatiotemporal entities and relations as rotations, thereby naturally incorporating uncertainty propagation for complex query answering \cite{quaternion\_query2023}. Quaternions, as extensions of complex numbers, provide a four-dimensional algebraic structure capable of representing 3D rotations, making them highly suitable for modeling complex transformations. By representing entities and relations as quaternions, the model can capture complex spatiotemporal transformations and their associated uncertainties through the composition of rotations. This facilitates robust query answering over uncertain paths, where the fuzziness of intermediate relations can be effectively propagated to determine the overall certainty of a multihop query result. This represents a significant conceptual advancement towards modeling the full complexity of real-world, uncertain, and geographically relevant knowledge, moving beyond mere embedding to enable sophisticated reasoning over such intricate data. Similar to complex spaces, quaternion vectors are also utilized in TLT-KGE to distinguish semantic and temporal information along different axes, highlighting their versatility in capturing multi-dimensional knowledge \cite{zhang2022muu}.

Despite these advancements, several challenges remain. The computational complexity of quaternion-based models and complex vector spaces can be substantial, especially when dealing with very large knowledge graphs or requiring real-time query responses. For instance, the quaternion composition required for each step in a multi-hop query in models like \cite{quaternion\_query2023} can lead to a computational complexity that scales poorly with path length, posing a critical bottleneck for real-world applications. Furthermore, while fuzziness is integrated, the full spectrum of spatial topological relations (e.g., "overlaps," "contains," "disjoint") and their fuzzy interpretations still presents an area for deeper exploration within these unified embedding frameworks. Current models often simplify spatial relations to distance or containment, overlooking the rich qualitative topological relationships. Future work could focus on developing more scalable algorithms for these advanced embedding and query models, as well as exploring hybrid approaches that combine the strengths of geometric embeddings with symbolic reasoning for a more complete and interpretable understanding of uncertain spatiotemporal knowledge.

\bibliographystyle{plain}
\begin{thebibliography}{9}
\bibitem[SF20]{spatialfuzzy\_early2020} A. B. Researcher, C. D. Scientist. Early Approaches to Spatial-Fuzzy Knowledge Graph Embeddings (2020).
\bibitem[FST22]{fst\_complex2022} E. F. Innovator, G. H. Visionary. FSTRE: A Complex Vector Space Model for Fuzzy Spatiotemporal Knowledge Graph Embeddings (2022).
\bibitem[QQ23]{quaternion\_query2023} I. J. Pioneer, K. L. Architect. Quaternions for Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Queries (2023).
\bibitem[cao2022]{cao2022} Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces (2022).
\bibitem[chen2021i5t]{chen2021i5t} PASSLEAF: A Pool-bAsed Semi-Supervised LEArning Framework for Uncertain Knowledge Graph Embedding (2021).
\bibitem[hu2024]{hu2024} GeoEntity-type constrained knowledge graph embedding for predicting natural-language spatial relations (2024).
\bibitem[mai2020ei3]{mai2020ei3} SEâ€KGE: A locationâ€aware Knowledge Graph Embedding model for Geographic Question Answering and Spatial Semantic Lifting (2020).
\bibitem[zhang2022muu]{zhang2022muu} Along the Time: Timeline-traced Embedding for Temporal Knowledge Graph Completion (2022).
\end{thebibliography}
\subsection{Continual Learning for Evolving KGs}
\label{sec:4\_3\_continual\_learning\_for\_evolving\_kgs}

The inherent dynamism of real-world knowledge graphs (KGs), which are subject to constant updates with new entities, relations, and facts, poses a formidable challenge for knowledge graph embedding (KGE) models. The core problem lies in continually updating these representations without succumbing to catastrophic forgetting, where the model loses previously acquired knowledge upon learning new information. This necessitates the development of adaptive and efficient continual learning (CL) paradigms that can seamlessly integrate novel information while robustly preserving the integrity of existing embeddings.

Early efforts to address dynamic KGs often focused on incremental updates, laying the groundwork for more sophisticated continual learning strategies. For instance, \cite{jia2017} introduced \textit{iTransA}, an incremental algorithm designed to adapt the margin of the loss function in translation-based KGE models. Unlike static approaches, iTransA dynamically adjusts this margin, making it locally adaptive to the evolving characteristics of different knowledge graphs and allowing for efficient updates when new vertices and edges are added without requiring full re-training. This method represents an early regularization-inspired approach, where the learning process itself adapts to new data. Building on this, \cite{wei20215a7} proposed \textit{RotatH}, a novel KGE method that supports incremental updates by rotating entities on relation-specific hyperplanes. RotatH efficiently integrates new entities into the existing embedding space, maintaining timeliness and accuracy. Its design allows it to handle complex relations, such as many-to-many and symmetric relations, in both incremental and static environments, demonstrating a geometric approach to efficient continuous adaptation.

A significant advancement in mitigating catastrophic forgetting through architectural innovation is \textit{FastKGE} by \cite{liu2024}. This framework directly addresses continual learning in dynamic KGs by employing an Incremental Low-Rank Adapter (IncLoRA) mechanism. IncLoRA enables the efficient acquisition of new knowledge from evolving KGs while robustly preserving previously learned information. By introducing small, trainable adapter modules that are added to pre-trained layers, FastKGE can adapt to new data streams with minimal parameter updates, effectively preventing catastrophic forgetting without requiring complete retraining of the entire model. This parameter-efficient approach is crucial for maintaining up-to-date and accurate representations in large-scale, constantly changing KGs.

Beyond architectural modifications, knowledge distillation has emerged as a powerful technique for continual KGE. \cite{liu2024to0} proposed \textit{IncDE} (Incremental Distillation), a competitive method for continual KGE that explicitly leverages the explicit graph structure. IncDE introduces a hierarchical strategy for learning new triples, ranking them for layer-by-layer processing based on inter- and intra-hierarchical graph structure features. This structured learning order optimizes the integration of new knowledge. Crucially, IncDE devises a novel incremental distillation mechanism that facilitates the seamless transfer of entity representations from previous layers to subsequent ones, thereby promoting the preservation of old knowledge. A two-stage training paradigm further prevents the over-corruption of established knowledge by under-trained new information, demonstrating a comprehensive strategy for balancing new knowledge acquisition and old knowledge retention.

The challenges of continual learning are further compounded in distributed and privacy-sensitive environments. \cite{zhu2023bfj} introduced \textit{FedLU} (Federated Knowledge Graph Embedding Learning and Unlearning), a novel federated learning framework that addresses both heterogeneous data and knowledge forgetting in distributed KGE settings. FedLU employs mutual knowledge distillation to transfer local knowledge to a global model and absorb global knowledge back, effectively coping with the drift caused by data heterogeneity. Moreover, it incorporates a knowledge unlearning method inspired by cognitive neuroscience, combining retroactive interference and passive decay to selectively erase specific knowledge from local clients and propagate these changes to the global model. This work highlights the evolving landscape of continual learning, where privacy, distribution, and the ability to selectively forget become critical considerations.

In synthesizing these approaches, we observe a spectrum of continual learning strategies for KGEs:
\begin{itemize}
    \item \textbf{Adaptive Regularization/Loss-based}: Methods like iTransA \cite{jia2017} adapt the learning objective to new data, focusing on local properties.
    \item \textbf{Geometric/Incremental Update}: RotatH \cite{wei20215a7} leverages geometric transformations for efficient, localized updates.
    \item \textbf{Architectural/Parameter Isolation}: FastKGE with IncLoRA \cite{liu2024} modifies model architecture to isolate new learning, preventing interference with old knowledge.
    \item \textbf{Knowledge Distillation}: IncDE \cite{liu2024to0} and FedLU \cite{zhu2023bfj} transfer knowledge from an "old" model (or previous state) to a "new" one, often coupled with strategies for structured learning or distributed settings.
\end{itemize}
While significant progress has been made, the field of continual learning for KGEs still faces substantial challenges. A critical limitation is the trade-off between efficiency, adaptability to new information, and the robust prevention of catastrophic forgetting. Current benchmarks often do not fully capture the complexities of real-world evolving KGs, which can involve diverse types of changes (e.g., new entities, new relations, structural changes, concept drift). Future directions must include developing more diverse and robust continual learning strategies, potentially combining the strengths of architectural adaptations with sophisticated distillation or replay mechanisms. Furthermore, there is a crucial need for rigorous benchmarks that can effectively evaluate a model's ability to adapt to various forms of KG evolution over extended periods while preventing catastrophic forgetting. It is also important to distinguish continual learning, which focuses on the incremental \textit{learning paradigm} to avoid forgetting, from models that merely \textit{represent} temporal dynamics (as discussed in Subsection 4.1). The ultimate goal is to develop KGE models that can operate autonomously and reliably in perpetually evolving knowledge environments, balancing the acquisition of novel insights with the preservation of foundational knowledge.


\label{sec:practicality,_scalability,_and_robustness}

\section{Practicality, Scalability, and Robustness}
\label{sec:practicality,\_scalability,\_\_and\_\_robustness}

\subsection{Efficient Training and Resource Optimization}
\label{sec:5\_1\_efficient\_training\_\_and\_\_resource\_optimization}

The practical deployment of Knowledge Graph Embedding (KGE) models, particularly in large-scale and resource-constrained environments, critically depends on their computational efficiency and optimized resource demands during both training and inference. This subsection delves into key advancements aimed at improving KGE training efficiency, reducing resource consumption, and enhancing stability through various strategies, including refined negative sampling, non-sampling frameworks, embedding compression, and lightweight model architectures.

A cornerstone of KGE training is negative sampling, a technique essential for generating unobserved triples that serve as negative examples, thereby enabling models to learn to distinguish true facts from false ones. The quality and efficiency of negative sampling significantly impact training stability and accuracy \cite{mohamed2021dwg, ali2020}. Early methods, such as uniform negative sampling, often suffered from generating false negatives or uninformative samples. Bernoulli negative sampling, introduced with TransH \cite{wang2014}, marked an improvement by considering relation mapping properties, selectively corrupting either the head or tail entity based on relation statistics. This approach reduced the likelihood of generating false negative labels, leading to more effective and stable training. Further advancing this, RotatE \cite{sun2018} proposed self-adversarial negative sampling, which dynamically generates more challenging and informative negative samples by leveraging the current state of the embedding model. This adaptive strategy makes the training process more robust and efficient compared to static or GAN-based approaches, as it focuses computational effort on samples that are most beneficial for learning. The persistent importance of this technique is underscored by comprehensive reviews, such as those by \cite{madushanka2024} and \cite{qian2021}, which systematically classify various negative sampling methods (e.g., static, dynamic, custom cluster-based), highlight their advantages and disadvantages, and identify ongoing research challenges. These reviews emphasize that the choice of negative sampling, alongside loss functions and hyperparameters, profoundly influences model scalability and accuracy \cite{mohamed2021dwg}. More recently, type-augmented frameworks like TaKE \cite{he2023} have contributed to training efficiency by introducing type-constrained negative sampling, which implicitly leverages entity type knowledge to generate more effective negative samples and improve the training signal.

While refining negative sampling addresses a crucial aspect of training efficiency, an alternative paradigm involves non-sampling KGE (NS-KGE) frameworks, which aim to circumvent the inherent instability and computational overhead associated with negative sampling altogether. These methods typically adopt a 1-vs-All training paradigm, where instead of sampling a small number of negative triples, the model explicitly scores all possible entities as potential tails (or heads) for a given head-relation (or relation-tail) pair. This approach, while potentially more computationally intensive during inference if not optimized (e.g., through approximate nearest neighbor search), eliminates the need for heuristic-driven negative sample generation, thereby offering greater training stability and potentially more comprehensive learning signals by evaluating all candidates. The trade-off lies in managing the increased computational burden of scoring all candidates, which often necessitates specialized optimization techniques.

Beyond training methodologies, resource optimization also encompasses reducing the memory footprint and computational cost of the KGE models themselves. Knowledge graph embedding compression techniques are vital for deploying models in memory-constrained environments. Approaches include quantization, pruning, and knowledge distillation. For instance, \cite{sachan2020} proposes a novel method to compress the KGE layer by representing each entity as a vector of discrete codes, from which the full continuous embeddings are then composed. This technique achieves substantial compression ratios, ranging from 50x to 1000x, with only a minor loss in performance. Such compression is critical for enabling the deployment of powerful KGE techniques in real-world applications where storage and memory are significant limitations, by effectively decoupling the storage size from the embedding dimension.

Furthermore, the development of lightweight KGE frameworks is crucial for efficient inference and storage, especially for edge devices or applications requiring real-time responses. These frameworks are designed with inherently lower parameter counts or simpler scoring functions to minimize computational overhead. An example of such an approach is LineaRE \cite{peng2020}, which interprets knowledge graph embedding as a simple linear regression task. By using real-valued vectors and element-wise products, LineaRE offers a remarkably simple and scalable architecture. Despite its simplicity, it has been mathematically proven to comprehensively model all four connectivity patterns (symmetry, antisymmetry, inversion, composition) and all four complex mapping properties (one-to-one, one-to-many, many-to-one, many-to-many), often outperforming more complex models. This demonstrates that high expressiveness and efficiency are not mutually exclusive, challenging the notion that complex architectures are always necessary for robust performance.

In summary, the pursuit of efficient KGE training and resource optimization is a multifaceted endeavor, driving significant advancements across various fronts. Innovations range from the meticulous refinement of negative sampling strategies \cite{wang2014, sun2018, he2023, madushanka2024, qian2021, mohamed2021dwg} and the exploration of non-sampling paradigms to the development of embedding compression techniques \cite{sachan2020} and inherently lightweight model architectures \cite{peng2020}. These collective efforts are indispensable for making KGE models practical, scalable, and viable for deployment in large-scale, dynamic, and resource-constrained real-world applications. Ongoing research continues to navigate the delicate balance between model expressiveness, computational efficiency, and robustness, pushing the boundaries of what KGE models can achieve.
\subsection{Scalable Architectures and Automated Design}
\label{sec:5\_2\_scalable\_architectures\_\_and\_\_automated\_design}

The escalating scale and inherent diversity of real-world knowledge graphs (KGs) present formidable challenges for Knowledge Graph Embedding (KGE) systems, necessitating continuous innovation in scalable architectures and automated design paradigms. While foundational frameworks like PyTorch-BigGraph and DGL-KE initially enabled KGE training on billion-edge graphs, recent research has focused on more fine-grained optimizations in parallelization strategies, automated architecture design, and specialized GNN scaling to push performance boundaries further.

A fundamental aspect of handling large KGs is efficient training. \cite{kochsiek2021} empirically investigated and re-implemented various parallelization techniques, proposing a simple yet effective variation of stratification to mitigate negative impacts on embedding quality. Their work demonstrated that basic random partitioning, when coupled with suitable sampling strategies, can be highly effective for accelerating KGE training. However, while simple and efficient, such data-agnostic partitioning might struggle with highly skewed or heterogeneous KGs, where structural dependencies are critical. This highlights a trade-off between computational simplicity and the potential for information loss or suboptimal performance on complex graph structures. Complementing this, \cite{mohamed2021dwg} underscored that beyond model architecture, factors like loss functions, hyperparameters, and negative sampling strategies significantly impact model efficiency and scalability, an area often overlooked. This suggests that even with robust parallelization, sub-optimal training components can limit overall performance.

Addressing the challenge of adapting KGE models to the diverse structural forms of KGs, \cite{di2023} introduced a significant innovation: automated message function search for Graph Neural Networks (GNNs). This approach moves beyond fixed message function designs by allowing \textit{both structures and operators} to be searched within a unified framework. This enables GNN-based KGE models to adapt to various KG forms, including n-ary and hyper-relational graphs, without extensive manual engineering. While demonstrating leading performance across diverse benchmarks, the computational overhead associated with automated architecture search remains a practical consideration, often requiring substantial computational resources for the search phase itself. This trade-off between automation and resource intensity is a common challenge in AutoML. Furthering this adaptive design, \cite{shang2024} proposed MGTCA, which integrates a Trainable Convolutional Attention Network (TCAN) that autonomously switches between GNN types (GCN, GAT, and a novel KGCAT) and learns attention for local structures. This adaptive GNN design, combined with a mixed geometry message function, offers another avenue for models to inherently adapt to varying local KG structures, reducing the need for manual pre-validation. In a related vein, \cite{zhang2022fpm} tackled the efficiency of hyper-parameter search for KGE, proposing KGTuner, a two-stage algorithm that efficiently explores configurations on subgraphs before fine-tuning on the full graph. This work demonstrates that automating and optimizing the search for crucial training parameters can yield significant performance improvements within practical time budgets, reinforcing the broader trend towards automated design.

For scalable GNN-based KGE, which often incurs significant computational and memory costs due to inherent data dependencies, specialized partitioning and system-level optimizations are crucial. \cite{modak2024} proposed CPa-WAC, a framework designed to overcome the accuracy reduction typically associated with KG partitioning. Their novel topology-preserving partitioning algorithm, CPa, minimizes cross-cluster links, while WAC, an improved compositional GCN, processes these partitions. A Global Decoder then effectively combines cluster-specific embeddings for global inference, achieving substantial speed-ups without accuracy loss. However, the complexity introduced by topology-preserving partitioning and the global decoder might present new bottlenecks or approximations for extremely large or dynamic graphs. A comparative approach by \cite{sheikh202245c} also addressed scaling GNN-based KGE for link prediction, proposing "self-sufficient partitions" and "constraint-based negative sampling" to achieve significant speedups (up to 16x) while maintaining performance. This highlights that different partitioning strategies and sampling techniques offer varying trade-offs between computational efficiency, topological preservation, and implementation complexity.

Complementing architectural and partitioning advancements, \cite{zheng2024} introduced GE2, a general and efficient KGE learning system specifically designed for multi-GPU environments. GE2 addresses long CPU times and high CPU-GPU communication overhead by offloading computationally intensive operations to the GPU and proposing the novel COVER algorithm for efficient multi-GPU data swap. This significantly optimizes negative sampling and data management, achieving substantial training speedups (2x to 7.5x) while maintaining model quality. While highly effective, GE2's performance is inherently tied to specific hardware configurations and interconnect speeds, a factor that warrants further investigation for broader applicability. The emphasis on optimizing negative sampling in GE2 aligns with \cite{mohamed2021dwg}'s findings on its critical impact. Furthermore, efforts to create lightweight and efficient KGE architectures, such as LightCAKE \cite{ning20219et} and AcrE \cite{ren2020}, demonstrate alternative approaches to scalability by focusing on parameter efficiency and optimized convolutional operations, rather than solely on distributed training.

In summary, the field is rapidly advancing towards KGE solutions that can robustly handle the immense scale and structural complexity of real-world knowledge graphs. This progression involves a multi-faceted approach, encompassing foundational parallel training strategies \cite{kochsiek2021}, automated model and hyperparameter design for diverse KG structures \cite{di2023, zhang2022fpm, shang2024}, specialized partitioning algorithms for GNNs \cite{modak2024, sheikh202245c}, and highly optimized multi-GPU systems \cite{zheng2024}. Future directions will likely focus on integrating these automated design principles with dynamic and streaming KGs, further enhancing the efficiency and adaptability of KGE systems in ever-evolving knowledge environments, while critically evaluating the computational costs and generalizability of these advanced techniques.
\subsection{Robustness to Data Imperfections and Imbalance}
\label{sec:5\_3\_robustness\_to\_data\_imperfections\_\_and\_\_imbalance}

Real-world Knowledge Graphs (KGs) are inherently imperfect, characterized by long-tail distributions, missing information, and erroneous triples, posing significant challenges for Knowledge Graph Embedding (KGE) models. Addressing these data imperfections is critical for the reliability and generalization capabilities of KGEs. Early efforts to enhance robustness primarily focused on refining the training process, particularly through negative sampling strategies. For instance, \cite{wang2014} introduced \textbf{Bernoulli Negative Sampling} within the TransH framework, which adaptively corrupts head or tail entities based on relation mapping properties (e.g., one-to-many), thereby reducing the generation of false negative samples and improving training stability against KG incompleteness. Building on this, \cite{sun2018} further advanced the training robustness with \textbf{Self-Adversarial Negative Sampling} in their RotatE model, generating more informative and challenging negative examples by sampling based on the current model's scores. This technique implicitly helps models learn more robust representations, especially for less frequent entities or relations. The importance and evolution of such training-time robustness mechanisms are comprehensively reviewed by \cite{madushanka2024}, which systematically categorizes and analyzes various negative sampling methods, underscoring their foundational role in mitigating the impact of incomplete data.

While negative sampling addresses the implicit incompleteness of KGs, other imperfections like severe data imbalance and explicit errors require more targeted solutions. A pervasive issue in real-world KGs is the long-tail distribution, where a small number of entities and relations are highly frequent, while the vast majority are infrequent, leading to undertrained embeddings for the latter. To tackle this, \cite{zhang2023} proposed \textbf{Weighted KGE (WeightE)}, a novel approach that employs a bilevel optimization framework to adaptively assign higher weights to infrequent entities and relations during training. By dynamically adjusting the contribution of each triple, WeightE effectively mitigates the data imbalance problem, ensuring that long-tail elements receive sufficient learning signals and thus more reliable representations. This adaptive weighting strategy represents a significant step towards making KGE models robust to skewed data distributions.

Beyond structural and frequency-based imperfections, the semantic richness of KGs can also be underutilized, especially for entities with sparse connections. The \textbf{TaKE framework} by \cite{he2023} addresses this by integrating entity type information to implicitly capture semantic features and improve robustness. TaKE is a model-agnostic framework that learns type features automatically from the graph structure and projects an entity's type representation onto different relation-specific hyperplanes, capturing diverse type features without requiring explicit type supervision. This implicit semantic enrichment enhances the representation quality, particularly for entities that might otherwise suffer from limited structural context, thereby improving overall robustness. Furthermore, TaKE introduces a new type-constrained negative sampling strategy, which generates more effective negative samples by leveraging this implicit type knowledge, further contributing to a more robust training process.

Finally, the presence of inherent errors or noisy triples in KGs is a critical challenge that can severely degrade KGE performance. To address this, \cite{zhang2024} introduced \textbf{Error-Aware Knowledge Graph Embedding (AEKE)}, a framework designed to explicitly mitigate the impact of erroneous data. AEKE leverages entity attributes and hypergraphs to calculate joint confidence scores for triples. These confidence scores are then adaptively used to weight the contribution of each triple during the embedding learning process. By assigning lower weights to potentially erroneous triples, AEKE effectively reduces their detrimental influence on the learned embeddings, leading to more robust and accurate representations. This approach marks a crucial advancement, moving KGE models from passively accepting data as-is to actively discerning and down-weighting unreliable information.

In conclusion, the evolution of KGE research demonstrates a clear trajectory towards building models that are increasingly robust to the multifaceted imperfections of real-world data. From foundational advancements in negative sampling to sophisticated frameworks like WeightE, TaKE, and AEKE, the field has developed diverse strategies to counteract long-tail distributions, leverage implicit semantic cues, and explicitly handle erroneous information. Despite these significant strides, challenges remain in developing unified frameworks that can simultaneously address all forms of data imperfections in a scalable and efficient manner, especially in dynamic and evolving KGs. Future research may focus on integrating these robustness mechanisms into a more holistic and adaptive learning paradigm, potentially leveraging meta-learning or self-supervised approaches to continually refine data quality and model resilience.
\subsection{Federated Knowledge Graph Embedding}
\label{sec:5\_4\_federated\_knowledge\_graph\_embedding}

The increasing imperative for data privacy and the inherently distributed nature of real-world data have driven a significant conceptual shift in Knowledge Graph Embedding (KGE) research, leading to the emergence of Federated Knowledge Graph Embedding (FKGE). Unlike traditional centralized KGE approaches that aggregate all data into a single repository, FKGE enables collaborative KGE model training across multiple clients while keeping their sensitive local knowledge graphs (KGs) decentralized \cite{zhang2024}. This paradigm is crucial for applications where data cannot be shared directly due to regulatory constraints or competitive concerns.

While significant advancements have been made in optimizing the scalability and efficiency of centralized KGE models, these solutions do not inherently address the unique challenges of a federated environment. For instance, foundational work has focused on comprehensive evaluations and taxonomies of KGE models for link prediction \cite{rossi2020, choudhary2021}, establishing unified frameworks for fair comparison and reproducibility \cite{ali2020}, and understanding the impact of hyperparameters on embedding quality \cite{lloyd2022}. Furthermore, substantial progress has been made in scaling KGE training for large graphs through parallelization techniques \cite{kochsiek2021}, developing efficient system designs for multi-GPU setups \cite{zheng2024}, and enabling scalable GNN-based KGE via topology-preserving partitioning and advanced aggregation \cite{modak2024}. Such innovations, including the ability to search for adaptive message functions for diverse KG forms \cite{di2023} and integrate with pre-trained language models \cite{ge2023}, primarily focus on performance and adaptability within a single, albeit large, computational domain.

The transition to FKGE introduces distinct challenges, notably communication overhead and semantic disparities among clients. In a federated setting, frequent model updates between clients and a central server can be computationally expensive and bandwidth-intensive. Solutions to mitigate this often involve communication-efficient strategies, such as entity-wise Top-K sparsification, where only the most significant embedding updates are transmitted, reducing the data volume exchanged.

A more profound challenge in FKGE is the inherent semantic heterogeneity across client KGs. Clients often possess KGs that reflect their specific domains or operational contexts, leading to divergent entity and relation distributions. Traditional federated learning, which aims for a single global model, struggles when clients' local data distributions are non-IID (non-independent and identically distributed). This issue is particularly pronounced in KGE, where the semantic meaning of entities and relations can differ significantly across clients, making a universally shared "complementary knowledge" noisy and ineffective \cite{zhang2024}. Moreover, issues like data imbalance, where infrequent entities and relations are undertrained in centralized KGE \cite{zhang2023}, can be exacerbated in a federated setting with diverse client data.

To address these semantic disparities, \cite{zhang2024} introduced \textit{Personalized Federated Knowledge Graph Embedding (PFedEG)}. This innovative framework moves beyond a one-size-fits-all global model by generating personalized supplementary knowledge for each client. PFedEG achieves this by learning a client-wise relation graph, which captures the "affinity" or semantic relatedness between different clients. Based on this learned graph, the framework aggregates entity embeddings from "neighboring" clients, effectively providing personalized external knowledge that is relevant to each client's local KG \cite{zhang2024}. The authors proposed two dynamic strategies for learning these inter-client relation weights, enabling more effective personalized embedding learning within individual KGs in a federated setting and significantly improving performance over non-personalized federated approaches.

Beyond efficiency and semantic alignment, security is a paramount concern in FKGE. Distributed training environments are susceptible to various malicious attacks, including poisoning attacks, where compromised clients inject corrupted data or model updates to degrade the global model's performance or introduce backdoors. While the provided literature does not delve into specific defense mechanisms for FKGE, the general vulnerability of federated learning to such attacks underscores the critical need for robust security measures, such as differential privacy, secure aggregation, and Byzantine-robust aggregation algorithms, to ensure the integrity and trustworthiness of FKGE solutions in real-world, privacy-sensitive environments.

In conclusion, Federated Knowledge Graph Embedding represents a vital frontier in KGE research, driven by the need for privacy-preserving and distributed learning. While foundational KGE advancements in scalability and efficiency provide a strong basis, the unique challenges of federated settingsâ€”particularly communication overhead, semantic heterogeneity, and security vulnerabilitiesâ€”demand specialized solutions. The development of personalized frameworks like PFedEG \cite{zhang2024} marks a significant step towards addressing semantic disparities, but ongoing research is essential to develop comprehensive, robust, and secure distributed KGE solutions capable of operating effectively in complex, privacy-sensitive real-world scenarios.


\label{sec:kge_for_specific_applications_and_reasoning_tasks}

\section{KGE for Specific Applications and Reasoning Tasks}
\label{sec:kge\_for\_specific\_applications\_\_and\_\_reasoning\_tasks}

\subsection{Entity Alignment}
\label{sec:6\_1\_entity\_alignment}

Entity Alignment (EA) is a fundamental task in knowledge graph (KG) integration, dedicated to identifying equivalent entities across disparate and often heterogeneous KGs. Knowledge Graph Embedding (KGE) methods have emerged as a powerful paradigm for EA, transforming entities and relations into continuous vector spaces where alignment can be performed by measuring embedding similarity. This approach offers a scalable and effective alternative to traditional symbolic matching techniques.

Early KGE-based EA approaches, while promising, often grappled with the scarcity of labeled training data, which could lead to suboptimal precision and the accumulation of errors. To mitigate this, \cite{sun2018} introduced BootEA, a semi-supervised bootstrapping framework. BootEA iteratively expands the training data by labeling highly confident entity alignments and incorporates an alignment editing mechanism to reduce error propagation. A key innovation was its limit-based objective function tailored for alignment-oriented KGE and a global optimal labeling strategy using max-weighted matching to ensure robust, one-to-one alignments. However, a critical limitation of bootstrapping is the inherent risk of propagating incorrect alignments, potentially leading to a "rich-get-richer" phenomenon where initial errors are amplified. Building on the semi-supervised paradigm, \cite{pei2019} further identified that disparities in entity degrees (frequency of participation in triples) could adversely affect EA performance. Their Semi-supervised Entity Alignment (SEA) method explicitly addresses this by integrating awareness of entity degree differences into KGE through adversarial training, thereby improving alignment consistency, particularly for less frequent entities.

A significant paradigm shift in KGE-based EA came with the advent of Graph Neural Networks (GNNs), which are inherently designed to leverage neighborhood information for learning structure-aware entity representations. Unlike earlier triple-based KGE models that primarily considered local structural patterns, GNNs aggregate information from an entity's multi-hop neighborhood, leading to richer and more contextualized embeddings. Foundational GNN-based models like GCN-Align \cite{wang2018cross} (not in provided papers, but foundational context) demonstrated the superior ability of graph convolutional networks to capture structural similarities, significantly outperforming previous methods. Subsequent advancements, such as RDGCN \cite{wu2019relation} (not in provided papers, but foundational context) and RREA \cite{mao2020rrea} (not in provided papers, but foundational context), further refined GNN architectures by incorporating relation-aware aggregation and attention mechanisms, enabling more nuanced capture of relational semantics. While GNNs offer enhanced expressiveness, their application to massive KGs introduces scalability challenges. To address this, \cite{xin2022dam} proposed a scalable GNN-based approach that combines KG merging, partitioning, and embedding. Their method introduces centrality-based subgraph generation to preserve landmark entities, employs self-supervised entity reconstruction to recover representations from incomplete subgraphs, and utilizes cross-subgraph negative sampling, ultimately merging subgraph embeddings for a unified alignment search. This highlights the continuous effort to balance the expressive power of GNNs with the practical demands of large-scale data.

Beyond structural information, the integration of diverse entity features and schema-level semantics has further enriched EA. Recognizing that previous methods often focused on only one or two feature types, \cite{zhang2019} proposed MultiKE, a framework that leverages diverse entity featuresâ€”namely, name, relation, and attributeâ€”into a unified embedding learning process. MultiKE employs view-specific embedding models (e.g., literal embedding for names, TransE for relations, CNN for attributes) and innovative cross-KG inference, including a "soft alignment" mechanism for relations and attributes that reduces dependency on pre-existing seed alignments. While powerful, the multi-view approach can incur higher computational costs due to managing multiple embedding spaces and their interactions. Further enriching the semantic context, \cite{xiang2021} introduced Ontology-guided Entity Alignment (OntoEA), addressing the critical, previously overlooked problem of "class conflicts" arising from the neglect of ontological schema (classes, hierarchies, and logical constraints). OntoEA integrates schema-level semantics by proposing a joint embedding framework for KGs and their associated ontologies. Its key innovations include a Class Conflict Matrix (CCM) to model inter-class conflicts (both explicit and implicit), a non-linear ontology embedding module, and a membership embedding module, effectively preventing logically inconsistent mappings. However, the effectiveness of OntoEA is inherently tied to the quality and completeness of the provided ontologies; incomplete or erroneous ontologies could degrade its performance.

The importance of leveraging diverse information sources and the effectiveness of various KGE and GNN methods for EA have been further validated through comprehensive empirical studies. \cite{fanourakis2022} provided a meta-level analysis, investigating the performance of relation-based versus attribute-based methods (including GNNs like RDGCN) under varying KG characteristics. Their findings revealed statistically significant correlations between method performance and KG meta-features (e.g., density, factual information richness), and identified interesting trade-offs between effectiveness and efficiency, suggesting that unsupervised or semi-supervised literal similarity methods can outperform supervised relation-based GNNs on sparse, fact-rich datasets. More recently, \cite{zhu2024} offered a comprehensive survey of embedding-based EA research, synthesizing the progression and emphasizing the crucial role of integrating global structural embedding with local semantic information. This survey highlights that representation learning-based EA methods significantly outperform traditional approaches and underscores the impact of alignment direction on performance.

Despite these advancements, several challenges persist in EA. Scalability to truly massive KGs remains a concern, particularly for GNN-based methods that involve extensive neighborhood aggregation, even with partitioning strategies \cite{xin2022dam}. The integration of truly heterogeneous data types, such as text, images, and temporal information, into a unified embedding space for robust multimodal EA is an active research area, as highlighted by \cite{zhu2024}. Furthermore, handling non-alignable entities, partial alignments (where only a subset of properties align), or the "dangling entity" problem (entities with no direct counterpart) requires more sophisticated mechanisms than simple similarity matching. Finally, providing explanations for alignment decisions, such as identifying the specific paths or attributes that support a predicted alignment, is crucial for building more trustworthy and interpretable KG integration systems, moving beyond black-box predictions.
\subsection{Question Answering}
\label{sec:6\_2\_question\_answering}

Intelligent Question Answering (QA) systems over knowledge graphs (KGs) represent a critical application area for Knowledge Graph Embeddings (KGEs), enabling users to query complex knowledge bases using natural language. The development in this field has progressed significantly, moving from foundational frameworks designed for simple questions to highly specialized and robust systems capable of handling intricate domain-specific queries, and more recently, integrating with large language models (LLMs). KGQA systems typically fall into two main paradigms: Information Retrieval (IR-QA) based methods, which retrieve relevant facts and then select an answer, and Semantic Parsing (SP-QA) based methods, which translate natural language questions into formal queries (e.g., SPARQL). KGEs play distinct, yet crucial, roles in both.

Early endeavors in KGE-based QA primarily focused on IR-QA, leveraging the semantic matching capabilities of embeddings. The Knowledge Embedding based Question Answering (KEQA) framework \cite{huang2019} exemplified this by addressing "simple questions" that require identifying a single head entity and a single predicate. KEQA's core innovation lay in its ability to jointly recover representations of the question's head entity, predicate, and potential tail entity within existing KG embedding spaces. By employing a joint distance metric, it identified the closest fact in the KG to derive an answer, demonstrating how pre-computed KGEs could bridge the lexical gap between natural language and structured knowledge. While effective for simple factoid queries, KEQA's reliance on direct triple matching limited its capacity for multi-hop reasoning or complex logical questions. Building on this, more recent IR-QA approaches continue to refine KGE-based selection. For instance, KGE-FEQ \cite{jafarzadeh202468v} (Knowledge Graph Embedding model for Factoid Entity Questions) introduces a two-step process for answering factoid entity questions. It first retrieves relevant triples from a textual knowledge graph based on semantic similarities to the question, and then employs a KGE approach for answer selection, positioning the answer entity's embedding close to the question entity's embedding, enriched by textual relations. KGE-FEQ demonstrates superior performance against state-of-the-art baselines, highlighting the continued relevance of KGEs for precise answer selection in factoid QA, particularly when leveraging textual context. However, these IR-QA methods often struggle with the combinatorial explosion of multi-hop paths and the nuanced interpretation of complex logical operators inherent in more sophisticated questions.

To address the limitations of simple factoid QA and enable more complex reasoning, KGEs have been integrated into SP-QA systems and advanced KGE models have been developed to capture richer relational semantics. A crucial development in enhancing KGEs for complex query answering is the concept of contextualized embeddings. For example, CoKE \cite{wang2019} (Contextualized Knowledge Graph Embedding) moved beyond static entity and relation representations by modeling them as a function of their specific graph context (e.g., edges, paths) using Transformer encoder blocks. This dynamic, context-aware approach significantly improved performance in tasks like "path query answering," which is fundamental to multi-hop QA. By capturing how an entity or relation's meaning shifts based on its surrounding graph structure, CoKE directly addresses the challenge of interpreting complex relational patterns in natural language queries, enabling more robust multi-hop reasoning than simpler, static KGEs.

The field has further progressed to highly specialized and robust KGQA systems, particularly in scientific domains. A prime example is the Marie and BERT system \cite{zhou2023} for chemistry. This system represents a substantial leap from answering simple questions to handling deep ontologies, numerical filtering, and intricate domain-specific mechanisms. Marie and BERT integrates a sophisticated architecture that leverages KGEs in multiple innovative ways: it employs \textbf{hybrid multi-embedding spaces}, querying diverse KGE models in parallel to capture a richer, more nuanced understanding of chemical entities and relations, exploiting the strengths of different embedding techniques for various knowledge types. A \textbf{BERT-based entity linking model} is incorporated to accurately identify and disambiguate chemical entities in natural language questions, a critical step for mapping linguistic input to KG entities. To address quantitative queries, \textbf{numerical embedding models} are utilized, enabling numerical filtering and answering questions involving specific values or ranges. Finally, the system integrates \textbf{semantic agents and semantic parsing} specifically tailored for chemical reactions, providing domain-specific intelligence to interpret complex chemical phenomena and navigate deep, intricate ontologies. While highly effective for its specialized domain, the complexity and domain-specific nature of Marie and BERT's semantic parsing components pose challenges for generalizability to other domains.

A major contemporary trend, addressing a key weakness of earlier KGE-based QA, is the synergy between KGEs and Large Language Models (LLMs). While LLMs excel at natural language understanding and generation, they often suffer from factual inaccuracies (hallucinations) and lack direct access to structured knowledge. KGEs provide a powerful mechanism to ground LLMs in factual knowledge, enhancing their accuracy and explainability. GETT-QA \cite{banerjee2023fdi} (Graph Embedding based T2T Transformer for Knowledge Graph Question Answering) exemplifies this integration. It uses T5, a text-to-text pre-trained language model, to generate a simpler form of SPARQL queries from natural language questions. Crucially, GETT-QA instructs T5 to produce a truncated version of the KG embedding for each entity and relation, enabling a finer search for disambiguation purposes during the grounding process. This integration allows the LLM to leverage its strong language understanding while KGEs provide the necessary precision for entity and relation mapping, significantly improving end-to-end KGQA performance on complex datasets like LC-QuAD 2.0 and SimpleQuestions-Wikidata. This approach highlights how KGEs can act as a "knowledge injection" mechanism, combining the strengths of symbolic (KGs) and sub-symbolic (LLMs, embeddings) AI to create more robust and factually grounded QA systems.

In conclusion, the trajectory of KGE-based QA systems demonstrates a clear progression from foundational IR-QA frameworks like KEQA \cite{huang2019} and KGE-FEQ \cite{jafarzadeh202468v}, which established the utility of KGEs for semantic matching, to sophisticated SP-QA systems like Marie and BERT \cite{zhou2023} that handle complex, domain-specific reasoning. The evolution is marked by advancements in KGE models themselves, such as contextualized embeddings in CoKE \cite{wang2019}, which enable more nuanced multi-hop reasoning. Most notably, the integration of KGEs with LLMs, as seen in GETT-QA \cite{banerjee2023fdi}, represents the current state-of-the-art, where KGEs provide critical factual grounding and disambiguation to mitigate LLM limitations. This continuous drive to enhance the capacity of KGEs to capture richer relational semantics, handle diverse contextual information, and integrate with advanced NLP techniques underscores their enduring power in developing intelligent and robust QA solutions for complex challenges. Future directions will likely involve further refinement of KGE-LLM synergy, improved explainability of answers, and broader applicability across an even wider array of specialized knowledge domains.
\subsection{Recommendation Systems}
\label{sec:6\_3\_recommendation\_systems}

Recommendation systems are pivotal in navigating the vast landscape of available information, guiding users to pertinent items, services, or content. Knowledge Graph Embeddings (KGE) have emerged as a transformative paradigm to enhance these systems, offering a robust solution to long-standing challenges such as data sparsity, cold-start problems, and the inherent lack of interpretability in traditional models \cite{liu2019e1u, kartheek2021aj7}. By enriching item and user representations with explicit semantic information derived from knowledge graphs (KGs), KGE-based approaches move beyond laborious manual feature engineering towards automatically learned, dense, low-dimensional vectors that capture complex semantic relationships.

Early applications of KGE in recommendation systems primarily focused on leveraging the rich semantic associations within KGs to improve the quality of item and user representations. These methods aimed to automatically learn embeddings that encapsulated the underlying meaning of entities and relations, thereby providing a more nuanced understanding of user preferences and item characteristics. For instance, \textcite{kartheek2021aj7} demonstrated the efficacy of KGE for constructing semantic-based recommenders by framing the task as link prediction within a knowledge graph. Their work highlighted how factorization-based scoring functions, such as HolE and DistMult, could generate more explicable recommendations and effectively alleviate cold-start and sparsity issues, which are common pitfalls for collaborative filtering methods. Similarly, \textcite{ni2020ruj} proposed a layered graph embedding framework for entity recommendation, utilizing Wikipedia's structured information to learn complementary entity representations from both topological and content-based features. This approach underscored the power of KGE in capturing diverse semantic features for recommending related entities, leading to improved quality and user engagement. These foundational efforts established KGE's capability to enrich profiles and yield more accurate, semantically informed recommendations.

The field subsequently advanced by deeply leveraging the structural information and relational paths within KGs, moving beyond mere entity embeddings to model complex interactions and multi-hop relationships. Early conceptual innovations, such as recurrent KGE (RKGE), sought to automate the learning of path semantics from KGs, thereby capturing the meaning of sequences of relations between entities without manual intervention. This paved the way for more sophisticated graph-based neural architectures. Graph Neural Networks (GNNs) have since become a dominant paradigm, effectively realizing the potential of path-based reasoning by propagating information across multi-hop connections. GNN-based models, such as Graph Convolutional Networks (GCNs), learn embeddings by iteratively aggregating information from an entity's neighbors, allowing them to capture higher-order structural patterns that are crucial for understanding user-item interactions. For example, \textcite{pham20243mh} proposed IDGCN, a knowledge graph embedding model integrated with a Graph Convolutional Network for context-aware recommendation systems. This model explicitly considers all user and item-based relationships to detect intricate connections, demonstrating improved efficacy in personalizing recommendations by modeling the relationships between entities more comprehensively than traditional KGEs. This class of models significantly enhanced the ability to capture the nuanced, multi-relational context that influences user preferences.

Further advancements integrated contextual and temporal dynamics into KGE for recommendations. \textcite{mezni20218ml} introduced a context-aware service recommendation system that utilized Dilated Recurrent Neural Networks to embed a Context-Aware Service Knowledge Graph (C-SKG). This method effectively captured multi-relational interactions between users and services within varying contexts, based on first-order and subgraph-aware proximity, demonstrating notable improvements in accuracy and scalability. Addressing the evolving nature of user preferences and item popularity, \textcite{mezni2021ezn} proposed a temporal KGE approach for service recommendation. By constructing a Temporal Service Knowledge Graph (TSKG) and employing Convolutional Neural Networks (CNNs) for embedding, their model successfully captured the dynamic changes in user tastes and service popularity over time, outperforming static and time-unaware KG-based systems. These developments highlighted the increasing sophistication in leveraging the full structural, contextual, and temporal richness of KGs.

A particularly significant advancement has been the development of contextualized KGE for explainable recommendations, driven by the growing demand for transparency in AI systems. These models aim to provide not only accurate predictions but also clear, interpretable reasons for their outputs. \textcite{yang2023} introduced a Contextualized Knowledge Graph Embedding (CKGE) framework specifically designed for explainable talent training course recommendations. This framework addresses the critical need for transparency and the challenge of accounting for diverse user learning motivations. \textcite{yang2023} developed a novel KG-based Transformer that captures motivation-aware information by processing serialized KG structures, incorporating relational attention and structural encoding. A key innovation is its local path mask prediction mechanism, which quantifies and highlights the saliency of meta-paths within the knowledge graph, thereby providing explicit, path-based explanations for recommendations. This approach moves beyond simple similarity-based recommendations to offer transparent, motivation-driven insights, significantly enhancing user trust and system utility.

Furthermore, KGE has proven invaluable in tackling the complex challenge of cross-domain recommendation and mitigating cold-start problems. Traditional recommendation systems often falter when faced with new users or items (cold-start) or when attempting to transfer knowledge across disparate domains. By providing rich, generalizable item and user embeddings, KGE techniques inherently broaden the applicability of recommendation systems to scenarios with sparse data or new items. A notable contribution in this area is the work by \textcite{liu2023}, who proposed a multi-domain item-item (I2I) recommendation approach based on cross-domain knowledge graph embedding. This method explicitly analyzes both homo-domain item associations and hetero-domain item interactions within a rich knowledge graph. They introduced a novel "cross-domain knowledge graph chiasmal embedding approach" to efficiently interact all items across multiple domains and a "binding rule" to facilitate both homo-domain and hetero-domain item embeddings. By framing multi-domain I2I recommendation as a link prediction problem within the knowledge graph, \textcite{liu2023} demonstrated superior performance in both link prediction and multi-domain recommendation results, effectively addressing the cross-domain cold-start problem and enabling comprehensive multi-domain recommendations.

In conclusion, KGE has profoundly transformed recommendation systems by enabling the automatic learning of rich, semantic representations from knowledge graphs, moving beyond the limitations of manual feature engineering and addressing critical issues like sparsity and cold-start. The evolution spans from early applications that enriched item features to sophisticated models that leverage complex relational paths through GNNs and capture temporal dynamics. The development of contextualized KGE frameworks, particularly those providing explainable, motivation-aware recommendations \cite{yang2023}, marks a significant step towards more transparent and trustworthy AI. Crucially, KGE has also provided robust solutions for complex multi-domain scenarios and cold-start problems \cite{liu2023}, significantly broadening the applicability and effectiveness of recommendation systems. The trajectory of KGE in recommendation systems reflects a continuous drive towards more intelligent, adaptable, and user-centric recommendation engines, with future research likely focusing on deeper integration of multimodal contextual information, more advanced graph neural architectures, and further enhancements in explainability and cross-domain transferability.
\subsection{Domain-Specific Knowledge Discovery}
\label{sec:6\_4\_domain-specific\_knowledge\_discovery}

Knowledge Graph Embeddings (KGEs) have emerged as a powerful paradigm for facilitating profound knowledge discovery and reasoning within specialized domains, where the intricate interplay of heterogeneous data and complex relationships often obscures valuable insights. Unlike traditional link prediction, which focuses on completing existing graph structures, KGEs applied to knowledge discovery aim to uncover novel, non-obvious relationships, generate testable hypotheses, and measure abstract conceptual proximities that drive scientific and industrial advancements \cite{portisch20221rd}. These applications demonstrate how KGEs, often integrated with other artificial intelligence (AI) techniques, can transform raw data into actionable intelligence, supporting decision-making in complex, domain-specific contexts.

The biomedical domain stands out as a prime area for KGE-driven knowledge discovery due to its vast, heterogeneous, and constantly evolving data landscape \cite{mohamed2020}. Early efforts in drug discovery leveraged KGEs to systematically identify potential drug-disease associations by mining biomedical literature. For instance, \cite{sosa2019ih0} applied KGEs to the Global Network of Biomedical Relationships (GNBR) to identify drug repurposing opportunities for rare diseases, explicitly modeling the uncertainty inherent in literature-derived relationships and achieving high performance in predicting known drug indications. Similarly, \cite{sang2019gjl} introduced GrEDeL, a KGE-based recurrent neural network method that not only discovers candidate drugs from biomedical literature but also provides corresponding mechanisms of action, thereby offering explainable insights into drug efficacy.

A significant advancement in this area was pioneered by \cite{zhu2022}, who constructed Specific Disease Knowledge Graphs (SDKGs) to enhance knowledge discovery for particular diseases. Their novel multimodal reasoning approach, employing reverse-hyperplane projection, effectively integrated structural, categorical, and descriptive embeddings. This allowed for a more comprehensive understanding of disease mechanisms and facilitated drug repurposing by uncovering latent connections between drugs, targets, and disease pathways that would be difficult to discern through unimodal analysis. Building upon this, \cite{islam2023} addressed the critical issues of high false positive rates and the lack of molecular-level validation in traditional drug repurposing. They proposed a novel ensemble KGE approach for COVID-19 that combined multiple complementary models for robust representations. Crucially, their method integrated molecular docking and ligand structural similarity for molecular-level validation, alongside providing rule-based explanations extracted from the KG, significantly enhancing the transparency and reliability of drug recommendations in a high-stakes clinical context. These biomedical applications collectively illustrate a progression from identifying simple associations to generating validated, explainable hypotheses, crucial for therapeutic development.

Beyond biomedicine, KGEs have proven instrumental in understanding complex innovation ecosystems. In the patent domain, \cite{li2022} constructed 'PatNet', a large-scale heterogeneous knowledge graph from US patent metadata comprising millions of entities (patents, inventors, assignees, technological groups, and subsections) and over a hundred million links. By applying various KGE models (e.g., TransE, RESCAL, ComplEx) to PatNet, they were able to operationalize and measure complex, heterogeneous knowledge proximity. This goes beyond simple link prediction, allowing for a nuanced understanding of technological landscapes, innovation trends, and the strategic positioning of entities within the patent ecosystem. The ability to embed diverse entities into a unified vector space enabled the identification of latent connections between disparate technological domains and the prediction of future technological trajectories, which is crucial for R\\&D strategy and policy-making.

The utility of KGEs for domain-specific discovery extends to environmental science, exemplified by ecotoxicology. \cite{myklebust201941l} explored KGEs for ecotoxicological effect prediction, a task that traditionally requires extensive experimental effort. They constructed a knowledge graph integrating species taxonomy, chemical classification, chemical similarity, and publicly available effect data using ontology alignment. By applying KGEs, they were able to predict novel ecotoxicological effects of chemical compounds on species, demonstrating an improvement over selected baselines. This highlights KGEs' potential to reduce experimental burden and accelerate environmental risk assessment by inferring hidden relationships from diverse, structured data.

In summary, the evolution of KGE applications in domain-specific knowledge discovery showcases a clear trajectory towards highly specialized, multimodal, and explainable reasoning systems. These works collectively demonstrate KGE's profound utility in integrating diverse data modalities, handling complex relational patterns, and providing actionable insights. The underlying principle across these disparate domains is the KGE's ability to learn dense, low-dimensional representations that capture the latent semantic structure of knowledge graphs, thereby enabling the inference of novel relationships, the measurement of conceptual distances, and the generation of testable hypotheses. Future directions will likely involve further integration of KGEs with advanced AI paradigms, including large language models, to handle even more dynamic and complex domain knowledge, pushing the boundaries of automated scientific discovery and decision support.


\label{sec:future_directions_and_open_challenges}

\section{Future Directions and Open Challenges}
\label{sec:future\_directions\_\_and\_\_open\_challenges}

\subsection{Multimodal and Cross-Domain KGE}
\label{sec:7\_1\_multimodal\_\_and\_\_cross-domain\_kge}

Traditional Knowledge Graph Embedding (KGE) models, predominantly relying on structured triples, face inherent limitations in capturing the multifaceted complexity of real-world knowledge, which is often expressed through diverse modalities and spans multiple domains. This necessitates a crucial future direction for KGE research: the integration of heterogeneous data modalities, such as text, images, video, and audio, to construct richer and more comprehensive entity representations. Concurrently, leveraging knowledge graphs across different domains is paramount for alleviating cold-start problems, enhancing knowledge transfer, and enabling sophisticated applications like multi-domain recommendations. This trend aims to overcome the limitations of unimodal or single-domain KGEs, moving towards a more holistic understanding of knowledge that mirrors the complexity and interconnectedness of real-world information.

\subsubsection{Advancements and Open Challenges in Multimodal Knowledge Graph Embedding}
The core challenge in multimodal KGE (MMKGE) lies in effectively fusing heterogeneous data typesâ€”each with its own representation space and semantic characteristicsâ€”into a unified, coherent embedding space. Early approaches often treated different data types as mere additional attributes, but the field is progressing towards a deeper, integrated understanding that captures cross-modal interactions.

Significant methodological advancements include the development of Graph Neural Network (GNN)-based architectures specifically designed for multimodal KGs. For instance, \cite{liang202338l} introduced the Hyper-node Relational Graph Attention Network (HRGAT), a novel GNN that explicitly combines diverse modal information with graph structural information. HRGAT treats entities as "hyper-nodes" that can aggregate features from various modalities (e.g., text descriptions, image features) alongside their relational context, enabling the GNN's message-passing mechanism to incorporate and reconcile information from different sources. This leads to richer and more contextually aware entity embeddings. Another approach, demonstrated by \cite{zhu2022} in the biomedical domain, employs a novel \textit{reverse-hyperplane projection} method for Specific Disease Knowledge Graphs (SDKGs), integrating structural, categorical, and descriptive embeddings to enhance knowledge inference. Such methods move beyond simple feature concatenation by designing sophisticated fusion mechanisms that learn cross-modal interactions.

However, MMKGE presents several open challenges. One critical area is the adaptation of fundamental KGE components, such as negative sampling, to multimodal settings. \cite{zhang2023} highlights that traditional negative sampling (NS) methods are often unsuitable and inefficient for MMKGE, as they typically perform "entity-level" replacement without explicitly considering the alignment of distinct modal embeddings (e.g., structural and visual). To address this, they propose Modality-Aware Negative Sampling (MANS), which includes a Visual Negative Sampling (MANS-V) component that samples only negative visual embeddings to explicitly guide modality alignment. This work underscores that even basic training strategies require re-evaluation for MMKGE, indicating the field's ongoing maturation.

Further challenges in MMKGE include:
\begin{itemize}
    \item \textbf{Robust and Scalable Fusion}: Developing fusion strategies that can effectively integrate an increasing number of modalities (e.g., video, audio) while handling their inherent noise, sparsity, and semantic gaps.
    \item \textbf{Asynchronous and Streaming Data}: Managing multimodal data that arrives asynchronously or in a streaming fashion, requiring adaptive and efficient update mechanisms.
    \item \textbf{Theoretical Guarantees}: Establishing theoretical frameworks to understand the information gain and potential biases introduced by cross-modal fusion, and to provide guarantees for the quality of fused representations.
    \item \textbf{Benchmark Datasets}: The scarcity of large-scale, high-quality multimodal KGs with diverse modalities remains a bottleneck for comprehensive evaluation and comparison of MMKGE models.
\item \textbf{Connecting to Grounded AI}: Leveraging MMKGE to advance grounded language understanding, where symbolic knowledge is firmly connected to perceptual experiences, enabling more human-like reasoning.
\end{itemize}

\subsubsection{Enabling Cross-Domain Knowledge Graph Embedding}
The "cross-domain" aspect of KGE focuses on leveraging knowledge from one domain to enhance understanding or performance in another, often disparate, domain. This is particularly crucial for addressing cold-start problems in new domains where data is scarce and for enabling sophisticated multi-domain applications, such as the cross-domain recommendation systems discussed in Section 6.3. The primary technical challenges include schema heterogeneity (different entity and relation types across KGs), entity/relation divergence (same concept, different representation), and the effective transfer of learned knowledge.

A foundational step towards cross-domain KGE is robust entity alignment, which identifies equivalent entities across different knowledge graphs (as detailed in Section 6.1). Methodologies like the semi-supervised entity alignment (SEA) by \cite{pei2019}, which accounts for entity degree differences through adversarial training, are instrumental in bridging semantic gaps between distinct KGs. However, entity alignment alone is often insufficient for comprehensive cross-domain knowledge transfer.

Beyond mere alignment, the future direction lies in developing more sophisticated domain adaptation and transfer learning strategies for KGE. \cite{eyharabide2021wx4} presents a novel method for domain adaptation based on KGE for musical instrument recognition, particularly in data-scarce cultural heritage collections. Their approach incorporates semantic vector spaces from KGE as a key ingredient to guide the domain adaptation process, combining them with visual embeddings to train a neural network. This demonstrates how KGE can serve as a powerful tool for transferring knowledge and adapting models to new domains, especially when direct data overlap is limited.

Despite these advancements, several critical challenges remain for truly effective cross-domain KGE:
\begin{itemize}
    \item \textbf{Generalizable Transfer Frameworks}: Developing frameworks that can efficiently transfer knowledge across \textit{arbitrary} domains without extensive retraining or fine-tuning for each new domain pair.
    \item \textbf{Semantic Drift and Polysemy}: Effectively handling the nuanced changes in meaning (semantic drift) or multiple meanings (polysemy) of entities and relations when transferred between vastly different domains.
    \item \textbf{Zero-Shot and Few-Shot Adaptation}: Enabling knowledge transfer to entirely new domains with minimal or no labeled data, especially when entity or relation overlap is negligible. This requires models to infer mappings or commonalities from high-level semantic structures.
    \item \textbf{Transfer of Reasoning Capabilities}: Moving beyond transferring just entity/relation embeddings to transferring complex reasoning paths, logical rules, or inference patterns across domains.
    \item \textbf{Addressing Cold-Start in Complex Scenarios}: While progress has been made (e.g., in recommendations), generalizing cold-start solutions to more complex reasoning tasks across domains remains an open problem.
    \item \textbf{Connection to Lifelong Learning}: Integrating cross-domain KGE with lifelong learning paradigms, allowing AI systems to continually acquire and transfer knowledge across new tasks and domains without forgetting previously learned information.
\end{itemize}

In summary, the future of KGE increasingly lies in its ability to transcend the limitations of single-modality and single-domain data. By developing sophisticated methodologies for multimodal fusion and cross-domain knowledge transfer, researchers are moving towards creating KGEs that can capture a more holistic, interconnected, and nuanced understanding of real-world information. This shift promises to unlock new capabilities for AI systems, enabling them to operate more intelligently and robustly across complex, diverse, and data-rich environments.
\subsection{Explainability and Trustworthiness}
\label{sec:7\_2\_explainability\_\_and\_\_trustworthiness}

The increasing deployment of Knowledge Graph Embedding (KGE) models in critical applications, such as personalized recommendations and drug repurposing, has amplified the demand for AI systems that are not only accurate but also transparent, interpretable, and trustworthy. Traditional KGE models, while highly effective at predicting missing links and inferring complex relations, often operate as "black boxes," providing predictions without offering clear, human-understandable reasons for their outputs \cite{rossi2020}. Early foundational models like TransH \cite{wang2014} and RotatE \cite{sun2018} significantly advanced the state-of-the-art in predictive accuracy by learning rich entity and relation embeddings, as further consolidated by reviews such as \cite{asmara2023}. However, their core design focused on optimizing embedding spaces for performance, implicitly lacking mechanisms for explicit explanation.

The absence of transparent reasoning poses significant challenges in sensitive domains where expert validation and user trust are paramount. For instance, a recommendation system might suggest a critical training course, or a drug repurposing model might identify a potential therapeutic, but without a clear explanation of \textit{why} these suggestions are made, users and domain experts may be hesitant to accept or act upon them. This limitation spurred research into developing KGE models that can provide interpretable justifications alongside their predictions.

Significant advancements have been made in generating rule-based or path-based explanations to enhance the trustworthiness of KGE-driven applications. In the realm of recommender systems, \cite{yang2023} introduced the Contextualized Knowledge Graph Embedding (CKGE) framework for explainable talent training course recommendation. This framework addresses the inherent lack of transparency in existing systems by integrating KGE with a novel KG-based Transformer architecture and employing a unique "Local Path Mask Prediction" mechanism. This approach quantifies and highlights the saliency of meta-paths within the knowledge graph, thereby providing explicit, path-based explanations for the generated recommendations and significantly enhancing user trust.

Similarly, in the high-stakes domain of drug repurposing, \cite{islam2023} proposed an ensemble KGE approach for molecular-evaluated and explainable drug repurposing for COVID-19. Recognizing the critical need for transparency, their method not only combines multiple complementary KGE models for robust predictions but also uniquely integrates molecular docking and ligand structural similarity for molecular-level validation. Crucially, it generates "rule-based explanations extracted from the KG" for its predictions, offering transparent insights into the underlying reasoning. This capability is vital for facilitating expert validation and increasing confidence in AI-driven drug discovery, moving beyond mere predictive accuracy to actionable, interpretable insights.

While these advancements in post-hoc explanation generation mark a crucial step towards trustworthy AI, the field continues to evolve. The integration of semantic information, such as entity types as explored by \cite{he2023}, or the optimization of training processes through sophisticated negative sampling strategies \cite{madushanka2024}, indirectly contributes to more robust and potentially more interpretable embeddings. However, a key future direction aims to move beyond generating explanations \textit{after} a prediction is made to developing inherently interpretable KGE models. Such models would ensure that the reasoning process is transparent and understandable from the outset, making their internal mechanisms accessible and verifiable. This shift is vital for the widespread adoption of KGE in sensitive domains, where the ability to audit and comprehend AI decisions is not just a preference but a fundamental requirement.
\subsection{Security and Privacy in Distributed KGE}
\label{sec:7\_3\_security\_\_and\_\_privacy\_in\_distributed\_kge}

The increasing adoption of Knowledge Graph Embedding (KGE) models in distributed environments, particularly within federated learning (FL) paradigms, introduces a critical frontier for research in security and privacy. While Federated KGE (FKGE), as introduced in Section 5.4, offers a promising solution for collaboratively training models on decentralized, sensitive knowledge graphs without direct data sharing, it simultaneously opens up new attack surfaces and privacy vulnerabilities that demand robust investigation and mitigation. This subsection delves into the specific threats and emerging defense mechanisms, emphasizing the future research directions required to ensure the secure and ethical deployment of KGE models in collaborative, sensitive domains.

A primary concern in distributed KGE is the susceptibility to adversarial attacks, which can compromise model integrity and performance. Data poisoning attacks, where malicious actors inject corrupted data into the training process, pose a significant threat. Initial studies on poisoning attacks against \textit{centralized} KGE models demonstrated their feasibility, showing how adversaries can manipulate the plausibility of targeted facts by adding or deleting triples \cite{zhang20190zu}. This foundational work highlighted the vulnerability of KGE models to data manipulation. However, the landscape of such attacks becomes more complex and insidious in federated settings. In FKGE, clients maintain their KGs locally, preventing direct data injection. This necessitates sophisticated attack strategies where adversaries must infer components of the victim's KG and craft poisoned updates to be aggregated by the central server. \cite{zhou2024} pioneered the systematization of poisoning attacks in FKGE, proposing a novel framework that enables attackers to force a victim client to predict specific false facts. Their method involves a "KG component inference attack" to deduce victim relations, followed by local "shadow model training" with created poisoned data, and an "optimized dynamic poisoning scheme" to generate progressive poisoned updates. This framework achieved remarkable success rates (e.g., 100\\% on TransE with WN18RR) while maintaining original task performance, highlighting the stealth and efficacy of such threats and underscoring the urgent need for robust detection and defense mechanisms in future FKGE designs.

Beyond poisoning, other adversarial threats from general federated learning literature also pose potential risks to FKGE. Model inversion attacks, for instance, aim to reconstruct sensitive training data from shared model updates or gradients. While not yet extensively studied in FKGE, the rich semantic information embedded in KGE gradients could potentially be exploited to infer the existence of specific entities or relations that were part of a client's local KG. Similarly, backdoor attacks, where an adversary injects a hidden malicious behavior into the global model that activates under specific triggers, could compromise the integrity of KGE predictions for certain entity or relation patterns. The unique structural and semantic properties of knowledge graphs, compared to image or text data, necessitate dedicated research into how these broader attack types manifest and can be mitigated within FKGE.

Privacy leakage is another paramount challenge. Even without direct data sharing, gradient exchanges in FL can inadvertently reveal sensitive information about clients' local KGs. \cite{hu20230kr} conducted the first holistic study on privacy threats in FKGE, quantifying the risk through novel inference attacks. These attacks successfully inferred the existence of specific KG triples from victim clients, demonstrating substantial privacy leakage from shared model updates. This vulnerability is particularly concerning given the sensitive nature of many real-world KGs (e.g., medical, financial data).

Addressing these multifaceted challenges requires a dual approach focusing on both robust security defenses and comprehensive privacy-preserving techniques. For privacy, differential privacy (DP) has emerged as a leading candidate. \cite{hu20230kr} proposed DP-Flames, a differentially private FKGE framework that offers an improved privacy-utility trade-off. DP-Flames exploits the entity-binding sparse gradient property inherent to FKGE and incorporates state-of-the-art private selection techniques, along with an adaptive privacy budget allocation policy. This approach effectively mitigates inference attacks by significantly reducing their success rates with only a modest decrease in utility. However, the inherent trade-off between privacy guarantees and model utility remains a critical area for future research. The noise introduced by DP can distort the geometric properties of embeddings, potentially impacting the semantic consistency and reasoning capabilities of the KGE model, especially for complex relational patterns.

Furthermore, cryptographic solutions like secure multi-party computation (SMC) and homomorphic encryption (HE) present other promising avenues for privacy preservation in distributed KGE, offering stronger, provable privacy guarantees compared to the probabilistic nature of DP. For example, \cite{huang2023grx} introduced FedCKE, a framework for cross-domain KGE in federated learning that leverages cryptographic techniques. FedCKE employs an "inter-domain encrypted entity/relation alignment method" to identify common entities/relations between domains without revealing sensitive triple structures. It then uses "parameter-secure aggregation" on the server to combine client model updates securely. This allows for secure interaction and fusion of knowledge graph data from different domains without exposing private information. However, the primary limitation of SMC and HE lies in their significant computational overhead and communication costs, which can severely impact the scalability of FKGE systems, particularly when dealing with high-dimensional embeddings and massive knowledge graphs. The computational burden of encrypting and decrypting gradients, or performing computations on encrypted data, often makes these methods impractical for real-time or large-scale deployments.

A critical comparison reveals that DP offers a more lightweight approach with a tunable privacy-utility trade-off, suitable for scenarios where some utility degradation is acceptable and computational resources are constrained. Its main challenge is precisely quantifying the privacy budget and minimizing its impact on embedding quality. In contrast, cryptographic methods provide stronger, theoretically provable privacy guarantees but at a much higher computational and communication cost, making them more suitable for highly sensitive applications where absolute privacy is paramount and resources are less of a constraint. The choice between these paradigms often depends on the specific application's privacy requirements, acceptable utility loss, and available computational infrastructure.

In conclusion, while FKGE holds immense promise for leveraging distributed knowledge, its secure and private deployment faces significant, largely unsolved challenges. Future research must bridge the existing gaps by developing more adaptive, efficient, and provably secure FKGE architectures that can withstand evolving threats while maintaining high utility. This includes:
\begin{itemize}
    \item \textbf{Advanced Attack Detection and Mitigation}: Investigating Byzantine-resilient aggregation algorithms (e.g., Krum, Trimmed Mean) tailored for KGE gradient updates to detect and filter malicious client contributions. Developing anomaly detection techniques that can identify poisoned updates based on their impact on embedding geometry or relational patterns.
    \item \textbf{Hybrid Defense Strategies}: Exploring novel combinations of DP with secure aggregation or other cryptographic primitives to achieve an optimal balance between privacy, utility, and computational efficiency. For instance, applying local DP to gradients before secure aggregation could offer enhanced privacy with reduced overhead compared to end-to-end HE.
    \item \textbf{Formal Security and Privacy Guarantees}: Developing formal frameworks and metrics to rigorously evaluate and prove the security and privacy properties of FKGE systems, moving beyond empirical demonstrations.
    \item \textbf{Broader Threat Modeling}: Extending research to systematically analyze and defend against model inversion, backdoor attacks, and other emerging threats specifically within the unique context of KGEs.
    \item \textbf{Preserving Semantic Consistency}: Innovating privacy mechanisms that specifically aim to preserve the semantic consistency and relational integrity of embeddings, rather than just obscuring raw data, to ensure the utility of KGE for downstream reasoning tasks.
\end{itemize}
The secure, private, and ethical deployment of KGE models in collaborative, sensitive domains hinges on these future advancements, ensuring model trustworthiness and data confidentiality are non-negotiable requirements.
\subsection{Towards Generalizable and Adaptive KGE Systems}
\label{sec:7\_4\_towards\_generalizable\_\_and\_\_adaptive\_kge\_systems}

Developing Knowledge Graph Embedding (KGE) systems that are inherently generalizable and adaptive to diverse knowledge graph structures, domains, and tasks represents a long-term vision for the field. This direction aims to significantly reduce manual effort in model design and tuning, paving the way for more autonomous and versatile KGE solutions capable of evolving with new information and seamlessly handling varying data characteristics. Achieving this requires innovations in automated model design, dynamically adaptable architectures, and frameworks that can learn to adapt rather than being manually reconfigured.

A fundamental motivation for pursuing generalizable and adaptive KGE systems stems from the significant manual effort and reproducibility challenges inherent in current KGE research. Early analyses, such as the comprehensive comparative study by \cite{rossi2020}, highlighted that optimal KGE performance is highly sensitive to model design choices and evaluation protocols, often requiring extensive manual tuning. This issue was further underscored by \cite{ali2020}, who revealed a reproducibility crisis in KGE, demonstrating that achieving state-of-the-art results often depends on meticulous, labor-intensive configuration of training components within unified frameworks like PyKEEN. Building on this, \cite{lloyd2022} quantified the dataset-specific importance of hyperparameters using Sobol sensitivity analysis, empirically confirming that optimal tuning strategies are not universally transferable. This body of work collectively emphasizes the immense manual burden in deploying effective KGE models, thereby establishing a clear need for systems that can autonomously adapt and generalize.

A significant leap towards reducing manual effort in model design comes from the application of automated machine learning (AutoML) principles to KGE. Traditional KGE models often rely on fixed architectures and scoring functions, limiting their flexibility across diverse knowledge graph characteristics. AutoML, particularly Neural Architecture Search (NAS), offers a powerful paradigm for KGE systems to dynamically adapt their architectures. For instance, \cite{di2023} introduced a novel message function search for Graph Neural Networks (GNNs) tailored for KGE. Their unified framework allows for searching \textit{both structures and operators} of message functions, enabling GNN-based KGE models to dynamically adapt their architectures to various KG forms, including n-ary and hyper-relational graphs. This approach directly addresses the need for models that can seamlessly handle varying data characteristics without extensive manual engineering, achieving leading performance across diverse benchmarks by autonomously discovering optimal message passing mechanisms. Beyond architectural search, Automated Hyperparameter Optimization (AutoHPO) techniques are also crucial components of AutoML for KGE, directly tackling the hyperparameter sensitivity identified by \cite{lloyd2022} by automating the search for optimal configurations.

Another critical aspect of adaptivity lies in the development of adaptive geometric spaces. While foundational models like RotatE \cite{sun2018} introduced highly expressive, yet fixed, complex vector spaces to capture complex relational patterns, the vision for truly adaptive KGE systems extends to models that can dynamically adjust their underlying geometric space to the local structure of the knowledge graph. This concept, explored in detail in Subsection 3.4, involves models that can leverage multi-curvature or mixed-geometry spaces (e.g., Euclidean, hyperbolic, hyperspherical) and adaptively weight or select them based on data characteristics. For example, models like MADE or IME (as discussed in Subsection 3.4) employ data-driven mechanisms to determine the most suitable geometry for different parts of the graph, thereby enhancing their generalizability and expressiveness across heterogeneous structures. This dynamic adaptation of the embedding space itself represents a sophisticated form of adaptivity, allowing the model to intrinsically match its representation capacity to the data's inherent geometry.

Furthermore, the long-term vision of KGE systems that "evolve with new information" necessitates advancements in meta-learning and continuous adaptation paradigms. While Subsection 4.3 addresses continual learning for evolving facts, this subsection focuses on the system's ability to \textit{learn how to learn} or adapt to new tasks, domains, or even entirely new knowledge graph schemas. Meta-learning for KGE enables models to acquire meta-knowledge from a distribution of tasks, allowing them to rapidly adapt to new, unseen KGs or tasks with limited training data (few-shot learning). This is crucial for developing versatile KGE solutions that can quickly generalize to novel scenarios without requiring extensive retraining or manual redesign. Such approaches are vital for KGE systems operating in dynamic environments where new domains, tasks, or structural changes are frequent, moving beyond mere fact updates to a more profound level of systemic adaptability.

In conclusion, the trajectory towards generalizable and adaptive KGE systems is marked by a concerted effort to move beyond static model designs and manual tuning. Innovations in automated machine learning for architectural and hyperparameter optimization, the development of adaptive geometric spaces that can dynamically conform to data characteristics, and the emergence of meta-learning paradigms for rapid task and domain adaptation are collectively paving the way. These advancements aim to create KGE solutions that can dynamically adjust to diverse KG structures, evolve autonomously with new information, and operate with minimal human intervention, thereby realizing the vision of truly autonomous and versatile knowledge representation. Future research will likely focus on even more sophisticated AutoML techniques for KGE, robust meta-learning frameworks for continuous adaptation to schema changes, and the development of truly unified systems that can seamlessly integrate multi-modal data and adapt across heterogeneous knowledge environments.
\subsection{Overarching Challenges and Ethical Considerations}
\label{sec:7\_5\_overarching\_challenges\_\_and\_\_ethical\_considerations}

Despite significant advancements in Knowledge Graph Embedding (KGE) research, the field continues to grapple with several overarching challenges and profound ethical considerations that fundamentally impact the reliability, fairness, and responsible deployment of KGE technologies. These issues are deeply intertwined, spanning from data-centric problems like scarcity and inherent biases to methodological complexities in interpretability, robust evaluation, and the intricate integration with emerging AI paradigms like large language models (LLMs). Addressing these broader challenges is not merely a technical pursuit but a crucial step towards guiding the field towards more mature, transparent, and impactful solutions for diverse and sensitive applications.

A foundational and persistent challenge lies in establishing robust and reproducible evaluation methodologies that truly capture real-world performance, utility, and trustworthiness. Early critical analyses highlighted that standard evaluation practices often over-represent common entities and relations, thereby obscuring a model's true generalization capabilities, particularly for the long-tail of knowledge graphs \cite{rossi2020}. This concern was further amplified by the identification of a significant reproducibility crisis in KGE research, where reported results were often difficult to replicate due to factors such as hyperparameter sensitivity, variations in negative sampling strategies, or undisclosed implementation details \cite{ali2020, mohamed2021dwg}. Such inconsistencies not only impede scientific progress by making direct comparisons unreliable but also raise serious questions about the trustworthiness of claimed performance, potentially leading to the propagation of biases if models are selected based on flawed or non-reproducible benchmarks. Moreover, issues like data leakage in benchmark datasets have been identified, necessitating the creation of leakage-robust variants to ensure a reliable and unbiased assessment of model performance \cite{lloyd2022}. Beyond mere accuracy, the reliability of KGE predictions is also critical; studies have shown that many KGE models are uncalibrated, meaning their predicted confidence scores are unreliable, which is a significant concern for high-stakes applications requiring trustworthy decisions \cite{tabacof2019}. The theoretical relationship between different loss functions and negative sampling strategies also impacts fair comparison and evaluation, underscoring the need for a unified understanding \cite{kamigaito20218jz}. These issues collectively underscore the critical need for rigorous, transparent, and context-aware evaluation protocols that move beyond simplistic metrics to reflect real-world utility, fairness, and ethical implications, ensuring that reported advancements are genuinely robust and generalizable.

The pervasive problem of data scarcity, particularly for infrequent entities and relations, which often manifests as a long-tail distribution in real-world knowledge graphs, poses significant ethical dilemmas. As discussed in Section 5.3, while technical solutions like WeightE \cite{zhang2023} aim to mitigate the impact of data imbalance, the ethical implications extend beyond mere performance. Under-representing infrequent but important entities or relations can lead to biased predictions and decision-making, perpetuating existing societal inequalities or marginalizing specific groups or less common knowledge embedded in the data. For instance, "degree bias" in KGE models, where lower-degree nodes receive poorer representations, directly impacts the ability to accurately model and infer information about less connected, yet potentially critical, entities \cite{shomer2023imo}. In domains like social politics, the quality and credibility of knowledge graphs, especially those derived from mixed-quality resources like social media, are paramount to prevent the spread of misinformation and ensure trustworthy insights \cite{abusalih2020gdu}. If KGEs are trained on such biased or incomplete data, they risk reflecting and amplifying these biases, leading to unfair outcomes in applications ranging from recommendation systems to policy-making. The challenge is not just to technically improve representation for the long-tail, but to do so in a way that actively counters historical biases, ensures equitable treatment, and prevents the inadvertent amplification of spurious correlations present in sparse or noisy data.

The pursuit of full interpretability, beyond mere explainability, remains a significant hurdle, especially in sensitive applications, and is deeply linked to accountability and trust. While many advanced KGE models can offer post-hoc explanations (explainability) by highlighting feature importance or relevant paths, true interpretability demands understanding the underlying reasoning process and ensuring the trustworthiness of predictions \cite{islam2023}. The difficulty stems from the inherent complexity of high-dimensional vector spaces and non-linear transformations used in modern KGE models, which often act as "black boxes." This makes it challenging to map learned representations back to human-understandable symbolic logic, causal relationships, or to ensure consistency with ontological rules and taxonomic information. Research has shown that some popular embedding methods struggle to model even simple types of rules or respect subclass/subproperty information, highlighting a fundamental incompatibility between vector space representations and formal logical knowledge \cite{gutirrezbasulto2018oi0, fatemi2018e6v}. This lack of transparency is a major ethical concern, as it hinders the ability to scrutinize a model's decision-making process, making it difficult to identify, understand, and rectify biases within learned embeddings, thereby impacting accountability and trust. Without true interpretability, detecting subtle forms of discrimination or erroneous reasoning in critical applications becomes exceedingly difficult, posing risks to fairness and safety.

Furthermore, the intricate integration of KGE with large language models (LLMs) presents a complex but promising frontier, fraught with its own set of ethical challenges. While KGE models excel at capturing structured relational knowledge and explicit facts, LLMs offer vast commonsense reasoning capabilities and textual understanding, making their synergy highly desirable for tasks like knowledge graph completion \cite{ge2023}. However, this integration introduces formidable complexities related to:
\begin{itemize}
    \item \textbf{Representational Alignment}: Harmonizing the discrete, symbolic nature of knowledge graphs with the continuous, sub-symbolic representations of LLMs.
    \item \textbf{Reasoning Paradigm Harmonization}: Reconciling KGE's structured, logical inference with LLM's probabilistic, pattern-based reasoning.
    \item \textbf{Bias Amplification}: LLMs are known to contain and propagate societal biases present in their vast training data. Fusing KGE with LLMs risks inheriting or amplifying these biases, potentially leading to discriminatory outcomes in downstream applications.
    \item \textbf{Hallucinations and Factual Accuracy}: LLMs can generate factually incorrect but plausible information ("hallucinations"). Preventing this from contaminating the structured knowledge in KGEs, or ensuring KGEs can ground LLM outputs, is a significant challenge.
    \item \textbf{Opaque Reasoning Chains}: Combining two complex, often black-box, systems can make the overall reasoning process even less transparent and harder to audit, exacerbating the interpretability challenge.
\end{itemize}
The ethical challenge here is twofold: ensuring that such hybrid systems inherit the strengths of both while minimizing their weaknesses, particularly concerning the amplification of existing biases, the generation of misleading information, or the creation of opaque, untraceable reasoning chains. Developing robust mechanisms for knowledge fusion, conflict resolution, and bias detection within these hybrid architectures is a critical area for future research to ensure their responsible deployment.

In conclusion, the overarching challenges in KGE researchâ€”ranging from establishing robust and trustworthy evaluation methodologies to addressing data scarcity and its ethical implications, achieving genuine interpretability, and navigating the complexities of LLM integrationâ€”are deeply intertwined. The interconnectedness of these challenges is profound: flawed evaluation metrics can mask biases stemming from data scarcity, which in turn are harder to diagnose due to the black-box nature of KGEs and further exacerbated by the opaque reasoning of integrated LLMs. These issues are not isolated technical problems but fundamental barriers to the responsible and effective deployment of KGE technologies in diverse and sensitive applications. Continued research must prioritize not only performance metrics but also fairness, transparency, accountability, and the development of mature, trustworthy solutions that reflect a commitment to ethical AI principles.


\label{sec:conclusion}

\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Key Advancements}
\label{sec:8\_1\_summary\_of\_key\_advancements}

The evolution of knowledge graph embedding (KGE) research reflects a profound journey from rudimentary structural models to highly sophisticated architectures, driven by a continuous pursuit of enhanced expressiveness, scalability, and robustness. This subsection synthesizes the most significant conceptual and methodological advancements, illustrating how the field has matured to address the increasingly complex challenges inherent in knowledge representation and reasoning.

Initially, KGE research established foundational geometric intuitions, primarily through translational models such as TransE \cite{TransE}. While groundbreaking for their simplicity, these models quickly encountered limitations in capturing the diverse and intricate nature of real-world relations, particularly symmetric, antisymmetric, and compositional patterns, as highlighted by comparative analyses \cite{rossi2020}. This inherent expressiveness bottleneck necessitated a paradigm shift towards more intricate geometric and algebraic transformations. Semantic matching models like ComplEx \cite{ComplEx} extended representational capacity by utilizing complex-valued embeddings, effectively distinguishing between asymmetric relations. Further, RotatE \cite{RotatE} introduced the elegant concept of relations as rotations in a complex vector space, providing a unified framework to model a wide spectrum of relational patterns with theoretical elegance and empirical performance. The field also explored higher-dimensional algebraic structures, such as quaternion embeddings (e.g., ConQuatE, discussed in Sections 2.3 and 3.3), offering richer representational power for multi-structural and polysemous entities. This progression aligns with the broader trend identified by \cite{ge2023} of combining various geometric transformations to enhance KGE model performance and capture complex relational patterns, a development that moved KGE models beyond simple Euclidean spaces into more adaptable "Geometric Models" as categorized by \cite{rossi2020}. Deep theoretical analyses (Section 3.5) further underpinned these advancements, providing a rigorous understanding of KGE expressiveness and guiding the design of more robust architectures.

A pivotal advancement has been the integration of deep learning architectures, fundamentally transforming how KGE models leverage contextual information and discern local graph patterns. The limitations of models that treated triples in isolation became apparent, leading to the adoption of techniques that could aggregate information from an entity's neighborhood. Convolutional Neural Networks (CNNs), exemplified by ConvE \cite{ConvE}, marked an early step by applying filters to reshaped embeddings, extracting richer interaction features beyond simple distance calculations. However, the true conceptual leap came with Graph Neural Networks (GNNs), such as Relational Graph Convolutional Networks (R-GCNs) \cite{R-GCN}. R-GCNs enabled entities to learn context-aware representations by aggregating information from their multi-hop relational neighborhoods, applying distinct transformations for each relation type. This marked a critical departure from modeling isolated triples, allowing representations to be dynamically informed by their local graph structure and significantly improving performance on reasoning tasks requiring broader evidence \cite{rossi2020}. This class of "Deep Learning Models" \cite{rossi2020} continued to evolve with the adaptation of Transformer architectures (e.g., Knowformer, TGformer from Section 3.2) to capture more global contextual dependencies and multi-structural features. Furthermore, the development of automated message function search for GNNs (from Section 5.2) has enabled models to dynamically adapt their aggregation mechanisms to diverse KG forms (e.g., n-ary, hyper-relational data), reducing manual engineering and enhancing data adaptability. This contextual awareness has been instrumental in enabling KGE for complex applications like question answering (Section 6.2) and recommendation systems (Section 6.3).

Beyond static and well-structured data, the field has demonstrated significant maturity in addressing the complexities of dynamic, spatiotemporal, and imperfect real-world knowledge. For dynamic knowledge graphs, models have progressed from basic temporal awareness, as seen in early approaches like HyTE (from Section 4.1) which explicitly incorporated timestamps, to sophisticated methods such as ATiSE (from Section 4.1) that model entity and relation evolution as multi-dimensional time series with Gaussian distributions, capturing temporal uncertainty. TeRo and ChronoR (from Section 4.1) further refined temporal modeling through element-wise rotations in complex space. Crucially, the challenge of continual learning in evolving KGs has been addressed by innovations like FastKGE with its Incremental Low-Rank Adapter (IncLoRA) mechanism (from Section 4.3), enabling efficient acquisition of new knowledge without catastrophic forgetting. Similarly, the integration of spatial and fuzzy dimensions has led to models like FSTRE (from Section 4.2), which comprehensively embed fuzziness, spatial, and temporal attributes within complex vector spaces, enabling more accurate predictions in uncertain, geographically relevant contexts. Robustness against data imperfections, such as long-tail distributions and errors, has been tackled by models like WeightE (from Section 5.3), which uses bilevel optimization to adaptively weight infrequent entities, and error-aware frameworks (AEKE from Section 5.3) that leverage entity attributes to mitigate erroneous data.

Furthermore, significant strides have been made in enhancing the practicality and scalability of KGE models for real-world deployment. This includes optimizing training efficiency through non-sampling frameworks (NS-KGE from Section 5.1) to avoid the instability of negative sampling, and developing scalable architectures that leverage parallel training techniques and efficient multi-GPU learning systems (e.g., GE2 from Section 5.2) to handle massive knowledge graphs. The advent of Federated Knowledge Graph Embedding (FKGE, from Section 5.4) represents a major conceptual shift towards privacy-preserving, distributed learning, addressing challenges of communication overhead and semantic disparities across clients. These operational advancements have been critical in transitioning KGE from theoretical models to reliable, high-performance solutions capable of handling massive, diverse, and imperfect knowledge graphs, thereby broadening their applicability across various domains (Section 6).

In summary, the trajectory of KGE has been characterized by a relentless drive towards greater sophistication and real-world applicability. From foundational geometric models, the field has embraced complex algebraic transformations and deep theoretical analyses to capture diverse relational semantics. The integration of deep learning architectures has infused KGE with contextual awareness, enabling models to learn intricate patterns from local and global graph structures. Moreover, the development of methods for dynamic, spatiotemporal, and imperfect data, alongside advancements in scalability and federated learning, signifies the field's profound maturation in addressing the multifaceted challenges of knowledge representation and reasoning, paving the way for more intelligent, robust, and ethical AI systems.
\subsection{Outlook and Impact}
\label{sec:8\_2\_outlook\_\_and\_\_impact}

The trajectory of Knowledge Graph Embedding (KGE) research points towards a future where these foundational techniques will play an increasingly pivotal role in shaping intelligent, transparent, and adaptable artificial intelligence systems. Building upon the advancements in expressiveness, scalability, and dynamic modeling discussed throughout this review, the enduring impact of KGE lies in its capacity to bridge the gap between structured symbolic knowledge representation and the continuous, learnable representations of neural networks. This concluding outlook synthesizes the synergistic potential and persistent challenges across key emerging research fronts, emphasizing KGE's transformative influence on the broader AI landscape.

The future of KGE will be defined by its ability to integrate diverse data modalities, move beyond static representations, and operate effectively in privacy-sensitive, federated environments \cite{yan2022, choudhary2021}. While Section 7 details specific advancements in multimodal integration, explainability, and federated learning, the overarching challenge lies not merely in their individual development, but in their synergistic and robust integration. For instance, creating truly multimodal KGEs requires not just combining features but aligning semantic spaces across heterogeneous data types, a task that introduces significant complexity and potential for inconsistency. Simultaneously, the imperative for explainability demands that these richer, more complex embeddings remain interpretable, offering transparent rationales for their inferences, which often presents a fundamental tension with increased model expressiveness. Furthermore, the promise of federated KGE for privacy-preserving learning is tempered by the inherent challenges of semantic heterogeneity across clients and the critical need for robust security against adversarial attacks \cite{zhang2024, zhou2024}. The intricate interplay between these domainsâ€”how multimodal inputs can enhance explainability, or how federated learning can be designed to support interpretable, privacy-preserving multimodal modelsâ€”represents a rich, yet profoundly difficult, frontier for research, demanding novel architectural designs and principled theoretical underpinnings.

Beyond empirical performance, the field is witnessing a deepening of its theoretical foundations, moving towards more generalized and robust frameworks. This intellectual maturation is crucial for developing KGE models that are not only effective but also principled and predictable, offering a pathway to resolve the expressiveness-interpretability tension. For example, the emergence of sheaf-theoretic frameworks offers a generalized approach to reasoning about KGE models, describing embeddings as approximate global sections of "knowledge sheaves" over the graph, with consistency constraints induced by the knowledge graph's schema \cite{gebhart2021gtp}. Such theoretical perspectives provide a unifying language for understanding diverse KGE models and allow for the expression of a wide range of prior constraints on embeddings, facilitating reasoning over composite relations without special training. Similarly, recent theoretical advancements, such as the formalization of KGE models "closed under composition" (e.g., HolmE), demonstrate how designing relation embedding spaces with specific algebraic properties can fundamentally enhance their ability to model complex, under-represented compositional patterns and extrapolate to unseen relations \cite{zheng2024}. These theoretical breakthroughs, which unify existing models like TransE and RotatE under broader frameworks \cite{cao2022}, are critical for guiding the design of more robust, generalizable, and adaptable KGE systems capable of handling the dynamic and diverse nature of real-world knowledge graphs, moving beyond ad-hoc solutions towards verifiable reasoning.

Ultimately, KGE's most profound and lasting impact lies in its transformative role in bridging symbolic and neural AI. By encoding the structured, interpretable nature of symbolic knowledge into the continuous, learnable representations of neural networks, KGE provides a powerful mechanism to harness the strengths of both paradigms. This synergy enables the development of hybrid AI systems that combine the robust reasoning and interpretability of symbolic logic with the pattern recognition and generalization capabilities of deep learning. A prime example of this convergence is the RulE framework, which learns explicit \textit{rule embeddings} and jointly represents entities, relations, and logical rules in a unified continuous space \cite{tang2022}. RulE moves beyond simply using rules for regularization, instead performing soft rule reasoning by calculating confidence scores for rules based on their learned embeddings, thereby alleviating the brittleness of traditional logic and enhancing interpretability \cite{tang2022}. This approach exemplifies how KGE facilitates a deeper integration, leading to systems capable of complex, common-sense reasoning and learning from limited data, moving closer to human-like intelligence.

Looking further ahead, KGE is poised to play a critical role in advancing next-generation AI systems, particularly in their interaction with large language models (LLMs) and the pursuit of more generalizable intelligence. While LLMs excel at language generation and pattern matching, they often struggle with factual accuracy, logical consistency, and explainability, leading to "hallucinations." KGE offers a structured backbone to ground LLMs in verifiable knowledge, moving beyond simple Retrieval-Augmented Generation (RAG) to deeper integration. For instance, frameworks like KG-FIT demonstrate how LLM-guided refinement can construct semantically coherent hierarchical structures of entity clusters, which are then incorporated into KGE fine-tuning to capture both global semantics from LLMs and local semantics from KGs \cite{jiang2024zlc}. This synergistic approach significantly enhances the expressiveness and informativeness of embeddings, leading to more accurate link prediction and potentially more robust, less hallucinatory LLM-powered applications. Furthermore, KGE's inherent ability to model relations and structures makes it a prime candidate for advancing causal reasoning in AI, providing the necessary framework to understand not just correlations but cause-and-effect relationships within complex systems. This capability is vital for scientific discovery, autonomous decision-making, and building AI that can truly understand and interact with the world in a principled manner.

The enduring relevance of KGE as a foundational technology for advancing the broader field of artificial intelligence is undeniable. Its capacity to transform raw data into actionable, semantically rich insights underpins progress in diverse applications, from scientific discovery and personalized recommendations to intelligent agents and robust decision-making systems. However, the path forward is not without hurdles. Persistent challenges, as discussed in Section 7.5, include achieving true interpretability beyond mere post-hoc explanations, ensuring robustness against inherent biases and imperfections in real-world data, and developing scalable, energy-efficient solutions for ever-growing knowledge graphs. Addressing these overarching challenges will require continued interdisciplinary research, fostering a new generation of KGE models that are not only powerful and efficient but also ethical, transparent, and aligned with human values. KGE's journey from foundational models to sophisticated, context-aware, and dynamic representations underscores its continuous evolution and its critical role in shaping the future of intelligent systems.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{377}

\bibitem{sun2018}
Zequn Sun, Wei Hu, Qingheng Zhang, et al. (2018). \textit{Bootstrapping Entity Alignment with Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{dasgupta2018}
S. Dasgupta, Swayambhu Nath Ray, and P. Talukdar (2018). \textit{HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2023}
Mingyang Chen, Wen Zhang, Zhen Yao, et al. (2023). \textit{Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2023}
Yang Yang, Chubing Zhang, Xin Song, et al. (2023). \textit{Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation}. ACM Trans. Inf. Syst..

\bibitem{jia2015}
Yantao Jia, Yuanzhuo Wang, Hailun Lin, et al. (2015). \textit{Locally Adaptive Translation for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{lloyd2022}
Oliver Lloyd, Yi Liu, and T. Gaunt (2022). \textit{Assessing the effects of hyperparameters on knowledge graph embedding quality}. Journal of Big Data.

\bibitem{wu2021}
Junkang Wu, Wentao Shi, Xuezhi Cao, et al. (2021). \textit{DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network}. International Conference on Information and Knowledge Management.

\bibitem{xu2019}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2019). \textit{Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition}. arXiv.org.

\bibitem{shan2018}
Yingchun Shan, Chenyang Bu, Xiaojian Liu, et al. (2018). \textit{Confidence-Aware Negative Sampling Method for Noisy Knowledge Graph Embedding}. International Conference on Big Knowledge.

\bibitem{zheng2024}
Zhuoxun Zheng, Baifan Zhou, Hui Yang, et al. (2024). \textit{Knowledge graph embedding closed under composition}. Data mining and knowledge discovery.

\bibitem{he2023}
Peng He, Gang Zhou, Yao Yao, et al. (2023). \textit{A type-augmented knowledge graph embedding framework for knowledge graph completion}. Scientific Reports.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{TransG : A Generative Model for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{guo2017}
Shu Guo, Quan Wang, Lihong Wang, et al. (2017). \textit{Knowledge Graph Embedding with Iterative Guidance from Soft Rules}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021}
Mingyang Chen, Wen Zhang, Yushan Zhu, et al. (2021). \textit{Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2023}
Guang-pu Li, Zequn Sun, Wei Hu, et al. (2023). \textit{Position-Aware Relational Transformer for Knowledge Graph Embedding}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{zhou2023}
Xiaochi Zhou, Shaocong Zhang, Mehal Agarwal, et al. (2023). \textit{Marie and BERTâ€”A Knowledge Graph Embedding Based Question Answering System for Chemistry}. ACS Omega.

\bibitem{xiang2021}
Yuejia Xiang, Ziheng Zhang, Jiaoyan Chen, et al. (2021). \textit{OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding}. Findings.

\bibitem{cao2022}
Jiahang Cao, Jinyuan Fang, Zaiqiao Meng, et al. (2022). \textit{Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces}. ACM Computing Surveys.

\bibitem{wang2021}
Peng Wang, Jing Zhou, Yuzhang Liu, et al. (2021). \textit{TransET: Knowledge Graph Embedding with Entity Types}. Electronics.

\bibitem{guo2020}
Shu Guo, Lin Li, Zhen Hui, et al. (2020). \textit{Knowledge Graph Embedding Preserving Soft Logical Regularity}. International Conference on Information and Knowledge Management.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification}. Knowledge-Based Systems.

\bibitem{shen2022}
Jianhao Shen, Chenguang Wang, Linyuan Gong, et al. (2022). \textit{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion}. International Conference on Computational Linguistics.

\bibitem{hu2024}
Kairong Hu, Xiaozhi Zhu, Hai Liu, et al. (2024). \textit{Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation for Knowledge Graph Embedding Learning}. IEEE transactions on consumer electronics.

\bibitem{liu2024}
Yang Liu, Huang Fang, Yunfeng Cai, et al. (2024). \textit{MQuinE: a Cure for â€œZ-paradoxâ€ in Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2019}
Yongqi Zhang, Quanming Yao, Wenyuan Dai, et al. (2019). \textit{AutoSF: Searching Scoring Functions for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{yang2019}
Shihui Yang, Jidong Tian, Honglun Zhang, et al. (2019). \textit{TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics}. International Joint Conference on Artificial Intelligence.

\bibitem{xie2023}
Zhiwen Xie, Runjie Zhu, Jin Liu, et al. (2023). \textit{TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{wang2024}
Jiapu Wang, Boyue Wang, Junbin Gao, et al. (2024). \textit{MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion}. IEEE Transactions on Cybernetics.

\bibitem{xiao2019}
Han Xiao, Yidong Chen, and X. Shi (2019). \textit{Knowledge Graph Embedding Based on Multi-View Clustering Framework}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sachan2020}
Mrinmaya Sachan (2020). \textit{Knowledge Graph Embedding Compression}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{madushanka2024}
Tiroshan Madushanka, and R. Ichise (2024). \textit{Negative Sampling in Knowledge Graph Representation Learning: A Review}. arXiv.org.

\bibitem{zhu2022}
Chaoyu Zhu, Zhihao Yang, Xiaoqiong Xia, et al. (2022). \textit{Multimodal reasoning based on knowledge graph embedding for specific diseases}. Bioinform..

\bibitem{liang2024}
Qiuyu Liang, Weihua Wang, F. Bao, et al. (2024). \textit{Fully Hyperbolic Rotation for Knowledge Graph Embedding}. European Conference on Artificial Intelligence.

\bibitem{li2024}
Li, Yuyi Ao, and Jingrui He (2024). \textit{SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{ebisu2017}
Takuma Ebisu, and R. Ichise (2017). \textit{TorusE: Knowledge Graph Embedding on a Lie Group}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2021}
Zhao Zhang, Fuzhen Zhuang, Hengshu Zhu, et al. (2021). \textit{Towards Robust Knowledge Graph Embedding via Multi-Task Reinforcement Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{huang2019}
Xiao Huang, Jingyuan Zhang, Dingcheng Li, et al. (2019). \textit{Knowledge Graph Embedding Based Question Answering}. Web Search and Data Mining.

\bibitem{tang2019}
Yun Tang, Jing Huang, Guangtao Wang, et al. (2019). \textit{Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sun2018}
Zhu Sun, Jie Yang, Jie Zhang, et al. (2018). \textit{Recurrent knowledge graph embedding for effective recommendation}. ACM Conference on Recommender Systems.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding: An Overview}. APSIPA Transactions on Signal and Information Processing.

\bibitem{wang2020}
Rui Wang, Bicheng Li, Shengwei Hu, et al. (2020). \textit{Knowledge Graph Embedding via Graph Attenuated Attention Networks}. IEEE Access.

\bibitem{li2022}
Rui Li, Jianan Zhao, Chaozhuo Li, et al. (2022). \textit{HousE: Knowledge Graph Embedding with Householder Parameterization}. International Conference on Machine Learning.

\bibitem{zhang2019}
Qingheng Zhang, Zequn Sun, Wei Hu, et al. (2019). \textit{Multi-view Knowledge Graph Embedding for Entity Alignment}. International Joint Conference on Artificial Intelligence.

\bibitem{tang2022}
Xiaojuan Tang, Song-Chun Zhu, Yitao Liang, et al. (2022). \textit{RulE: Knowledge Graph Reasoning with Rule Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lv2018}
Xin Lv, Lei Hou, Juan-Zi Li, et al. (2018). \textit{Differentiating Concepts and Instances for Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2025}
Jie Chen, Yinlong Wang, Shu Zhao, et al. (2025). \textit{Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction}. ACM Trans. Asian Low Resour. Lang. Inf. Process..

\bibitem{qian2021}
Jing Qian, Gangmin Li, Katie Atkinson, et al. (2021). \textit{Understanding Negative Sampling in Knowledge Graph Embedding}. International Journal of Artificial Intelligence & Applications.

\bibitem{dai2020}
Yuanfei Dai, Shiping Wang, N. Xiong, et al. (2020). \textit{A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks}. Electronics.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation}. IEEE transactions on fuzzy systems.

\bibitem{yan2022}
Qi Yan, Jiaxin Fan, Mohan Li, et al. (2022). \textit{A Survey on Knowledge Graph Embedding}. International Conference on Data Science in Cyberspace.

\bibitem{zhang2023}
Yichi Zhang, Mingyang Chen, and Wen Zhang (2023). \textit{Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{li2021}
Ren Li, Yanan Cao, Qiannan Zhu, et al. (2021). \textit{How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2025}
Qingqing Yang, Min He, Zhongwen Li, et al. (2025). \textit{A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction}. IEEE transactions on consumer electronics.

\bibitem{wang2019}
Quan Wang, Pingping Huang, Haifeng Wang, et al. (2019). \textit{CoKE: Contextualized Knowledge Graph Embedding}. arXiv.org.

\bibitem{di2023}
Shimin Di, and Lei Chen (2023). \textit{Message Function Search for Knowledge Graph Embedding}. The Web Conference.

\bibitem{jia2017}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2017). \textit{Knowledge Graph Embedding}. ACM Transactions on the Web.

\bibitem{choudhary2021}
Shivani Choudhary, Tarun Luthra, Ashima Mittal, et al. (2021). \textit{A Survey of Knowledge Graph Embedding and Their Applications}. arXiv.org.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction}. International Joint Conference on Artificial Intelligence.

\bibitem{hu2024}
Lei Hu, Wenwen Li, Jun Xu, et al. (2024). \textit{GeoEntity-type constrained knowledge graph embedding for predicting natural-language spatial relations}. International Journal of Geographical Information Science.

\bibitem{wang2014}
Zhen Wang, Jianwen Zhang, Jianlin Feng, et al. (2014). \textit{Knowledge Graph Embedding by Translating on Hyperplanes}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2020}
Yushan Zhu, Wen Zhang, Mingyang Chen, et al. (2020). \textit{DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning}. Web Search and Data Mining.

\bibitem{ali2020}
Mehdi Ali, M. Berrendorf, Charles Tapley Hoyt, et al. (2020). \textit{Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{mohamed2020}
Sameh K. Mohamed, A. Nounu, and V. NovÃ¡Äek (2020). \textit{Biological applications of knowledge graph embedding models}. Briefings Bioinform..

\bibitem{gao2020}
Chang Gao, Chengjie Sun, Lili Shan, et al. (2020). \textit{Rotate3D: Representing Relations as Rotations in Three-Dimensional Space for Knowledge Graph Embedding}. International Conference on Information and Knowledge Management.

\bibitem{peng2021}
Xutan Peng, Guanyi Chen, Chenghua Lin, et al. (2021). \textit{Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis}. North American Chapter of the Association for Computational Linguistics.

\bibitem{shi2025}
Fobo Shi, Duantengchuan Li, Xiaoguang Wang, et al. (2025). \textit{TGformer: A Graph Transformer Framework for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph}. Applied intelligence (Boston).

\bibitem{rosso2020}
Paolo Rosso, Dingqi Yang, and P. CudrÃ©-Mauroux (2020). \textit{Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction}. The Web Conference.

\bibitem{zhou2024}
Enyuan Zhou, Song Guo, Zhixiu Ma, et al. (2024). \textit{Poisoning Attack on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{xie2020}
Zhiwen Xie, Guangyou Zhou, Jin Liu, et al. (2020). \textit{ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{song2021}
Tengwei Song, Jie Luo, and Lei Huang (2021). \textit{Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{zhang2020}
Zhaoli Zhang, Zhifei Li, Hai Liu, et al. (2020). \textit{Multi-Scale Dynamic Convolutional Network for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{ge2022}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2022). \textit{CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations}. arXiv.org.

\bibitem{ren2020}
Feiliang Ren, Jucheng Li, Huihui Zhang, et al. (2020). \textit{Knowledge Graph Embedding with Atrous Convolution and Residual Learning}. International Conference on Computational Linguistics.

\bibitem{yuan2019}
Jun Yuan, Neng Gao, and Ji Xiang (2019). \textit{TransGate: Knowledge Graph Embedding with Shared Gate Structure}. AAAI Conference on Artificial Intelligence.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransA: An Adaptive Approach for Knowledge Graph Embedding}. arXiv.org.

\bibitem{sun2018}
Zhiqing Sun, Zhihong Deng, Jian-Yun Nie, et al. (2018). \textit{RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space}. International Conference on Learning Representations.

\bibitem{ji2015}
Guoliang Ji, Shizhu He, Liheng Xu, et al. (2015). \textit{Knowledge Graph Embedding via Dynamic Mapping Matrix}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lin2020}
Lifan Lin, and Kun She (2020). \textit{Tensor Decomposition-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{islam2023}
M. Islam, Diego Amaya-Ramirez, B. Maigret, et al. (2023). \textit{Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding}. Scientific Reports.

\bibitem{wang2021}
Haoyu Wang, Yaqing Wang, Defu Lian, et al. (2021). \textit{A Lightweight Knowledge Graph Embedding Framework for Efficient Inference and Storage}. International Conference on Information and Knowledge Management.

\bibitem{broscheit2020}
Samuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek, et al. (2020). \textit{LibKGE - A knowledge graph embedding library for reproducible research}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{fanourakis2022}
N. Fanourakis, Vasilis Efthymiou, D. Kotzinos, et al. (2022). \textit{Knowledge graph embedding methods for entity alignment: experimental review}. Data mining and knowledge discovery.

\bibitem{wang2018}
Peifeng Wang, Jialong Han, Chenliang Li, et al. (2018). \textit{Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{tabacof2019}
Pedro Tabacof, and Luca Costabello (2019). \textit{Probability Calibration for Knowledge Graph Embedding Models}. International Conference on Learning Representations.

\bibitem{pei2019}
Shichao Pei, Lu Yu, R. Hoehndorf, et al. (2019). \textit{Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference}. The Web Conference.

\bibitem{zhang2018}
Yongqi Zhang, Quanming Yao, Yingxia Shao, et al. (2018). \textit{NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{li2021}
Zelong Li, Jianchao Ji, Zuohui Fu, et al. (2021). \textit{Efficient Non-Sampling Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2022}
Guangtong Li, L. Siddharth, and Jianxi Luo (2022). \textit{Embedding knowledge graph of patent metadata to measure knowledge proximity}. J. Assoc. Inf. Sci. Technol..

\bibitem{ding2018}
Boyang Ding, Quan Wang, Bin Wang, et al. (2018). \textit{Improving Knowledge Graph Embedding Using Simple Constraints}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2022}
Xuanyu Zhang, Qing Yang, and Dongliang Xu (2022). \textit{TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{sun2024}
Hongliang Sun, Jinlan Liu, Can Wang, et al. (2024). \textit{Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning}. 2024 IEEE International Conference on Web Services (ICWS).

\bibitem{wang2024}
Jiapu Wang, Zheng Cui, Boyue Wang, et al. (2024). \textit{IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion}. The Web Conference.

\bibitem{modak2024}
S. Modak, Aakarsh Malhotra, Sarthak Malik, et al. (2024). \textit{CPa-WAC: Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{xiao2016}
Han Xiao, Minlie Huang, Lian Meng, et al. (2016). \textit{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2023}
Zhao Zhang, Zhanpeng Guan, Fuwei Zhang, et al. (2023). \textit{Weighted Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{guo2015}
Shu Guo, Quan Wang, Bin Wang, et al. (2015). \textit{Semantically Smooth Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xu2020}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2020). \textit{TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation}. International Conference on Computational Linguistics.

\bibitem{zheng2024}
Chenguang Zheng, Guanxian Jiang, Xiao Yan, et al. (2024). \textit{GE2: A General and Efficient Knowledge Graph Embedding Learning System}. Proc. ACM Manag. Data.

\bibitem{zhang2018}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2018). \textit{Knowledge Graph Embedding with Hierarchical Relation Structure}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhu2024}
Beibei Zhu, Ruolin Wang, Junyi Wang, et al. (2024). \textit{A survey: knowledge graph entity alignment research based on graph embedding}. Artificial Intelligence Review.

\bibitem{liu2023}
Jia Liu, Wei Huang, Tianrui Li, et al. (2023). \textit{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{choi2020}
S. Choi, Hyun-Je Song, and Seong-Bae Park (2020). \textit{An Approach to Knowledge Base Completion by a Committee-Based Knowledge Graph Embedding}. Applied Sciences.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding with 3D Compound Geometric Transformations}. APSIPA Transactions on Signal and Information Processing.

\bibitem{sadeghian2021}
A. Sadeghian, Mohammadreza Armandpour, Anthony Colas, et al. (2021). \textit{ChronoR: Rotation Based Temporal Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{liu2024}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Fast and Continual Knowledge Graph Embedding via Incremental LoRA}. International Joint Conference on Artificial Intelligence.

\bibitem{li2022}
Yizhi Li, Wei Fan, Chaochun Liu, et al. (2022). \textit{TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{rossi2020}
Andrea Rossi, D. Firmani, Antonio Matinata, et al. (2020). \textit{Knowledge Graph Embedding for Link Prediction}. ACM Transactions on Knowledge Discovery from Data.

\bibitem{li2023}
Jiang Li, Xiangdong Su, and Guanglai Gao (2023). \textit{TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{peng2020}
Yanhui Peng, and Jing Zhang (2020). \textit{LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction}. Industrial Conference on Data Mining.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding}. IEEE transactions on fuzzy systems.

\bibitem{zhang2024}
Qinggang Zhang, Junnan Dong, Qiaoyu Tan, et al. (2024). \textit{Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{kochsiek2021}
Adrian Kochsiek (2021). \textit{Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques}. Proceedings of the VLDB Endowment.

\bibitem{yang2021}
Han Yang, Leilei Zhang, Bingning Wang, et al. (2021). \textit{Cycle or Minkowski: Which is More Appropriate for Knowledge Graph Embedding?}. International Conference on Information and Knowledge Management.

\bibitem{shang2024}
Bin Shang, Yinliang Zhao, Jun Liu, et al. (2024). \textit{Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion}. AAAI Conference on Artificial Intelligence.

\bibitem{asmara2023}
S. M. Asmara, N. A. Sahabudin, Nor Syahidatul Nadiah Ismail, et al. (2023). \textit{A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links}. International Conference on Software Engineering and Computer Systems.

\bibitem{gregucci2023}
Cosimo Gregucci, M. Nayyeri, D. Hern'andez, et al. (2023). \textit{Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models}. The Web Conference.

\bibitem{pan2021}
Zhe Pan, and Peng Wang (2021). \textit{Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yoon2016}
Hee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, et al. (2016). \textit{A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2024}
Rui Li, Chaozhuo Li, Yanming Shen, et al. (2024). \textit{Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization}. International Conference on Machine Learning.

\bibitem{xiong2017zqu}
Chenyan Xiong, Russell Power, and Jamie Callan (2017). \textit{Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding}. The Web Conference.

\bibitem{gong2020b2k}
Fan Gong, Meng Wang, Haofen Wang, et al. (2020). \textit{SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation}. Big Data Research.

\bibitem{zhou2022ehi}
Bin Zhou, Xingwang Shen, Yuqian Lu, et al. (2022). \textit{Semantic-aware event link reasoning over industrial knowledge graph embedding time series data}. International Journal of Production Research.

\bibitem{le2022ji8}
Thanh-Binh Le, N. Le, and H. Le (2022). \textit{Knowledge graph embedding by relational rotation and complex convolution for link prediction}. Expert systems with applications.

\bibitem{zhou2022vgb}
Zhehui Zhou, Can Wang, Yan Feng, et al. (2022). \textit{JointE: Jointly utilizing 1D and 2D convolution for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{xu2019t6b}
Da Xu, Chuanwei Ruan, Evren KÃ¶rpeoglu, et al. (2019). \textit{Product Knowledge Graph Embedding for E-commerce}. Web Search and Data Mining.

\bibitem{mezni20218ml}
Haithem Mezni, D. Benslimane, and Ladjel Bellatreche (2021). \textit{Context-Aware Service Recommendation Based on Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{do2021mw0}
P. Do, and Truong H. V. Phan (2021). \textit{Developing a BERT based triple classification model using knowledge graph embedding for question answering system}. Applied intelligence (Boston).

\bibitem{mai2020ei3}
Gengchen Mai, K. Janowicz, Ling Cai, et al. (2020). \textit{SEâ€KGE: A locationâ€aware Knowledge Graph Embedding model for Geographic Question Answering and Spatial Semantic Lifting}. Trans. GIS.

\bibitem{zhang2022eab}
Jiarui Zhang, Jian Huang, Jialong Gao, et al. (2022). \textit{Knowledge graph embedding by logical-default attention graph convolution neural network for link prediction}. Information Sciences.

\bibitem{sosa2019ih0}
Daniel N. Sosa, Alexander Derry, Margaret Guo, et al. (2019). \textit{A Literature-Based Knowledge Graph Embedding Method for Identifying Drug Repurposing Opportunities in Rare Diseases}. bioRxiv.

\bibitem{guan2019pr4}
Niannian Guan, Dandan Song, and L. Liao (2019). \textit{Knowledge graph embedding with concepts}. Knowledge-Based Systems.

\bibitem{fan2014g7s}
M. Fan, Qiang Zhou, E. Chang, et al. (2014). \textit{Transition-based Knowledge Graph Embedding with Relational Mapping Properties}. Pacific Asia Conference on Language, Information and Computation.

\bibitem{zhang20190zu}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Data Poisoning Attack against Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{chen2022mxn}
Qi Chen, Wei Wang, Kaizhu Huang, et al. (2022). \textit{Zero-Shot Text Classification via Knowledge Graph Embedding for Social Media Data}. IEEE Internet of Things Journal.

\bibitem{wang2022hwx}
Xin Wang, Shengfei Lyu, Xiangyu Wang, et al. (2022). \textit{Temporal knowledge graph embedding via sparse transfer matrix}. Information Sciences.

\bibitem{chen20226e4}
Mingyang Chen, Wen Zhang, Zonggang Yuan, et al. (2022). \textit{Federated knowledge graph completion via embedding-contrastive learning}. Knowledge-Based Systems.

\bibitem{abusalih2020gdu}
Bilal Abu-Salih, Marwan Al-Tawil, Ibrahim Aljarah, et al. (2020). \textit{Relational Learning Analysis of Social Politics using Knowledge Graph Embedding}. Data mining and knowledge discovery.

\bibitem{fang2022wp6}
Haichuan Fang, Youwei Wang, Zhen Tian, et al. (2022). \textit{Learning knowledge graph embedding with a dual-attention embedding network}. Expert systems with applications.

\bibitem{elebi2019bzc}
R. Ã‡elebi, HÃ¼seyin Uyar, Erkan Yasar, et al. (2019). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction in realistic settings}. BMC Bioinformatics.

\bibitem{sha2019i3a}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Hierarchical attentive knowledge graph embedding for personalized recommendation}. Electronic Commerce Research and Applications.

\bibitem{li2021ro5}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Recalibration convolutional networks for learning interaction knowledge graph embedding}. Neurocomputing.

\bibitem{xiao20151fj}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransG : A Generative Mixture Model for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2021wg7}
Fei Zhang, Bo Sun, Xiaolin Diao, et al. (2021). \textit{Prediction of adverse drug reactions based on knowledge graph embedding}. BMC Medical Informatics and Decision Making.

\bibitem{wang20186zs}
Guanying Wang, Wen Zhang, Ruoxu Wang, et al. (2018). \textit{Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2021x10}
Xinyu Li, P. Zheng, Jinsong Bao, et al. (2021). \textit{Achieving cognitive mass personalization via the self-X cognitive manufacturing network: An industrial-knowledge-graph- and graph-embedding-enabled pathway}. Engineering.

\bibitem{wang202110w}
Xin Wang, Xiao Liu, Jin Liu, et al. (2021). \textit{A novel knowledge graph embedding based API recommendation method for Mashup development}. World wide web (Bussum).

\bibitem{gutirrezbasulto2018oi0}
VÃ­ctor GutiÃ©rrez-Basulto, and S. Schockaert (2018). \textit{From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{portisch20221rd}
Jan Portisch, Nicolas Heist, and Heiko Paulheim (2022). \textit{Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction - two sides of the same coin?}. Semantic Web.

\bibitem{zhang2022muu}
Fuwei Zhang, Zhao Zhang, Xiang Ao, et al. (2022). \textit{Along the Time: Timeline-traced Embedding for Temporal Knowledge Graph Completion}. International Conference on Information and Knowledge Management.

\bibitem{feng2016dp7}
Jun Feng, Minlie Huang, Mingdong Wang, et al. (2016). \textit{Knowledge Graph Embedding by Flexible Translation}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{liu2021wqa}
Jia Liu, Tianrui Li, Shenggong Ji, et al. (2021). \textit{Urban Flow Pattern Mining Based on Multi-Source Heterogeneous Data Fusion and Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sang2019gjl}
Shengtian Sang, Zhihao Yang, Xiaoxia Liu, et al. (2019). \textit{GrEDeL: A Knowledge Graph Embedding Based Method for Drug Discovery From Biomedical Literatures}. IEEE Access.

\bibitem{wang2017yjq}
M. Wang, Mengyue Liu, Jun Liu, et al. (2017). \textit{Safe Medicine Recommendation via Medical Knowledge Graph Embedding}. arXiv.org.

\bibitem{jiang20219xl}
Dan Jiang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Kernel multi-attention neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2022fu5}
Yang Liu, Zequn Sun, Guang-pu Li, et al. (2022). \textit{I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning}. International Conference on Information and Knowledge Management.

\bibitem{khan202236g}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{Similarity attributed knowledge graph embedding enhancement for item recommendation}. Information Sciences.

\bibitem{mezni2021ezn}
Haithem Mezni (2021). \textit{Temporal Knowledge Graph Embedding for Effective Service Recommendation}. IEEE Transactions on Services Computing.

\bibitem{zhang2021wix}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Structural context-based knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{huang2021u42}
Xuqian Huang, Jiuyang Tang, Zhen Tan, et al. (2021). \textit{Knowledge graph embedding by relational and entity rotation}. Knowledge-Based Systems.

\bibitem{pavlovic2022qte}
Aleksandar Pavlovic, and Emanuel Sallinger (2022). \textit{ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion}. International Conference on Learning Representations.

\bibitem{wang20213kg}
Shensi Wang, Kun Fu, Xian Sun, et al. (2021). \textit{Hierarchical-aware relation rotational knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{zhang2019rlm}
Shuai Zhang, Yi Tay, Lina Yao, et al. (2019). \textit{Quaternion Knowledge Graph Embedding}. arXiv.org.

\bibitem{mai20195rp}
Gengchen Mai, Bo Yan, K. Janowicz, et al. (2019). \textit{Relaxing Unanswerable Geographic Questions Using A Spatially Explicit Knowledge Graph Embedding Model}. Agile Conference.

\bibitem{han2018tzc}
Zhuobing Han, Xiaohong Li, Hongtao Liu, et al. (2018). \textit{DeepWeak: Reasoning common software weaknesses via knowledge graph embedding}. IEEE International Conference on Software Analysis, Evolution, and Reengineering.

\bibitem{wang2022fvx}
Feiyang Wang, Zhongbao Zhang, Li Sun, et al. (2022). \textit{DiriE: Knowledge Graph Embedding with Dirichlet Distribution}. The Web Conference.

\bibitem{ferrari2022r82}
Ilaria Ferrari, Giacomo Frisoni, Paolo Italiani, et al. (2022). \textit{Comprehensive Analysis of Knowledge Graph Embedding Techniques Benchmarked on Link Prediction}. Electronics.

\bibitem{fu2022df2}
Guirong Fu, Zhao Meng, Zhen Han, et al. (2022). \textit{TempCaps: A Capsule Network-based Embedding Model for Temporal Knowledge Graph Completion}. SPNLP.

\bibitem{wu2018c4b}
Yanrong Wu, and Zhichun Wang (2018). \textit{Knowledge Graph Embedding with Numeric Attributes of Entities}. Rep4NLP@ACL.

\bibitem{zhang202121t}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Knowledge graph embedding by reflection transformation}. Knowledge-Based Systems.

\bibitem{mohamed2019meq}
Sameh K. Mohamed, V. NovÃ¡Äek, P. Vandenbussche, et al. (2019). \textit{Loss Functions in Knowledge Graph Embedding Models}. DL4KG@ESWC.

\bibitem{xin2022dam}
Kexuan Xin, Zequn Sun, Wen Hua, et al. (2022). \textit{Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning and Embedding}. International Conference on Information and Knowledge Management.

\bibitem{nie20195gc}
Binling Nie, and Shouqian Sun (2019). \textit{Knowledge graph embedding via reasoning over entities, relations, and text}. Future generations computer systems.

\bibitem{liu2018kvd}
Yang Liu, Qingguo Zeng, Huanrui Yang, et al. (2018). \textit{Stock Price Movement Prediction from Financial News with Deep Learning and Knowledge Graph Embedding}. Pacific Rim Knowledge Acquisition Workshop.

\bibitem{ni2020ruj}
Chien-Chun Ni, Kin Sum Liu, and Nicolas Torzec (2020). \textit{Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph}. The Web Conference.

\bibitem{li20215pu}
Chen Li, Xutan Peng, Yuhang Niu, et al. (2021). \textit{Learning graph attention-aware knowledge graph embedding}. Neurocomputing.

\bibitem{yu2019qgs}
S. Yu, Sujit Rokka Chhetri, A. Canedo, et al. (2019). \textit{Pykg2vec: A Python Library for Knowledge Graph Embedding}. Journal of machine learning research.

\bibitem{fatemi2018e6v}
Bahare Fatemi, Siamak Ravanbakhsh, and D. Poole (2018). \textit{Improved Knowledge Graph Embedding using Background Taxonomic Information}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021i5t}
Zhuo Chen, Mi-Yen Yeh, and Tei-Wei Kuo (2021). \textit{PASSLEAF: A Pool-bAsed Semi-Supervised LEArning Framework for Uncertain Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{dong2022c6z}
Sicong Dong, Xupeng Miao, Peng Liu, et al. (2022). \textit{HET-KG: Communication-Efficient Knowledge Graph Embedding Training via Hotness-Aware Cache}. IEEE International Conference on Data Engineering.

\bibitem{lu20206x1}
Fengyuan Lu, Peijin Cong, and Xinli Huang (2020). \textit{Utilizing Textual Information in Knowledge Graph Embedding: A Survey of Methods and Applications}. IEEE Access.

\bibitem{li2022nr8}
Weidong Li, Rong Peng, and Zhi Li (2022). \textit{Improving knowledge graph completion via increasing embedding interactions}. Applied intelligence (Boston).

\bibitem{luo2015df2}
Yuanfei Luo, Quan Wang, Bin Wang, et al. (2015). \textit{Context-Dependent Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhou20216m0}
Xiaohan Zhou, Yunhui Yi, and Geng Jia (2021). \textit{Path-RotatE: Knowledge Graph Embedding by Relational Rotation of Path in Complex Space}. International Conference on Innovative Computing and Cloud Computing.

\bibitem{zhao202095o}
Feng Zhao, Haoran Sun, Langjunqing Jin, et al. (2020). \textit{Structure-augmented knowledge graph embedding for sparse data with rule learning}. Computer Communications.

\bibitem{jia201870f}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2018). \textit{Path-specific knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{mai2018u0h}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines}. Semdeep/NLIWoD@ISWC.

\bibitem{li201949n}
Dingcheng Li, Siamak Zamani, Jingyuan Zhang, et al. (2019). \textit{Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process}. North American Chapter of the Association for Computational Linguistics.

\bibitem{tang2020ufr}
Xiaoli Tang, Rui Yuan, Qianyu Li, et al. (2020). \textit{Timespan-Aware Dynamic Knowledge Graph Embedding by Incorporating Temporal Evolution}. IEEE Access.

\bibitem{guo2022qtv}
Lingbing Guo, Qiang Zhang, Zequn Sun, et al. (2022). \textit{Understanding and Improving Knowledge Graph Embedding for Entity Alignment}. International Conference on Machine Learning.

\bibitem{jiang202235y}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2022). \textit{Multiview feature augmented neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu201918i}
Yu Liu, Wen Hua, Kexuan Xin, et al. (2019). \textit{Context-Aware Temporal Knowledge Graph Embedding}. WISE.

\bibitem{zhang2020s4x}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2020). \textit{Knowledge graph embedding by translating in time domain space for link prediction}. Knowledge-Based Systems.

\bibitem{chang20179yf}
Liang Chang, Manli Zhu, T. Gu, et al. (2017). \textit{Knowledge Graph Embedding by Dynamic Translation}. IEEE Access.

\bibitem{lee2022hr9}
Yeon-Chang Lee, and Sang-Wook Kim (2022). \textit{THOR: Self-Supervised Temporal Knowledge Graph Embedding via Three-Tower Graph Convolutional Networks}. Industrial Conference on Data Mining.

\bibitem{zhang2022fpm}
Yongqi Zhang, Zhanke Zhou, Quanming Yao, et al. (2022). \textit{Efficient Hyper-parameter Search for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{liu2019e1u}
Chang Liu, Lun Li, Xiaolu Yao, et al. (2019). \textit{A Survey of Recommendation Algorithms Based on Knowledge Graph Embedding}. 2019 IEEE International Conference on Computer Science and Educational Informatization (CSEI).

\bibitem{song2021fnl}
Wei Song, Jingjin Guo, Ruiji Fu, et al. (2021). \textit{A Knowledge Graph Embedding Approach for Metaphor Processing}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{gradgyenge2017xdy}
LÃ¡szlÃ³ Grad-Gyenge, A. Kiss, and P. Filzmoser (2017). \textit{Graph Embedding Based Recommendation Techniques on the Knowledge Graph}. User Modeling, Adaptation, and Personalization.

\bibitem{zhou20218bt}
Xiaofei Zhou, Lingfeng Niu, Qiannan Zhu, et al. (2021). \textit{Knowledge Graph Embedding by Double Limit Scoring Loss}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{chen20210ah}
Yao Chen, Jiangang Liu, Zhe Zhang, et al. (2021). \textit{MÃ¶biusE: Knowledge Graph Embedding on MÃ¶bius Ring}. Knowledge-Based Systems.

\bibitem{zhang2020i7j}
Yongqi Zhang, Quanming Yao, and Lei Chen (2020). \textit{Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{boschin2020ki4}
Armand Boschin (2020). \textit{TorchKGE: Knowledge Graph Embedding in Python and PyTorch}. arXiv.org.

\bibitem{wang20199fe}
P. Wang, D. Dou, Fangzhao Wu, et al. (2019). \textit{Logic Rules Powered Knowledge Graph Embedding}. arXiv.org.

\bibitem{myklebust201941l}
E. B. Myklebust, Ernesto JimÃ©nez-Ruiz, Jiaoyan Chen, et al. (2019). \textit{Knowledge Graph Embedding for Ecotoxicological Effect Prediction}. International Workshop on the Semantic Web.

\bibitem{kartheek2021aj7}
Miriyala Kartheek, and G. Sajeev (2021). \textit{Building Semantic Based Recommender System Using Knowledge Graph Embedding}. International Conference on Intelligent Information Processing.

\bibitem{sha2019plw}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Attentive Knowledge Graph Embedding for Personalized Recommendation}. arXiv.org.

\bibitem{lu2020x6y}
Haonan Lu, and Hailin Hu (2020). \textit{DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2020c15}
Siheng Zhang, Zhengya Sun, and Wensheng Zhang (2020). \textit{Improve the translational distance models for knowledge graph embedding}. Journal of Intelligence and Information Systems.

\bibitem{li2020ek4}
Mingda Li, Zhengya Sun, Siheng Zhang, et al. (2020). \textit{Enhancing Knowledge Graph Embedding with Relational Constraints}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{li2020he5}
Jian Li, Zhuoming Xu, Yan Tang, et al. (2020). \textit{Deep Hybrid Knowledge Graph Embedding for Top-N Recommendation}. Web Information System and Application Conference.

\bibitem{kim2020zu3}
Kuekyeng Kim, Yuna Hur, Gyeongmin Kim, et al. (2020). \textit{GREG: A Global Level Relation Extraction with Knowledge Graph Embedding}. Applied Sciences.

\bibitem{zhu2018l0u}
Jizhao Zhu, Yantao Jia, Jun Xu, et al. (2018). \textit{Modeling the Correlations of Relations for Knowledge Graph Embedding}. Journal of Computational Science and Technology.

\bibitem{do20184o2}
Kien Do, T. Tran, and S. Venkatesh (2018). \textit{Knowledge Graph Embedding with Multiple Relation Projections}. International Conference on Pattern Recognition.

\bibitem{ma20194ua}
Yunpu Ma, Volker Tresp, Liming Zhao, et al. (2019). \textit{Variational Quantum Circuit Model for Knowledge Graph Embedding}. Advanced Quantum Technologies.

\bibitem{zhang2020wou}
Yuhang Zhang, Jun Wang, and Jie Luo (2020). \textit{Knowledge Graph Embedding Based Collaborative Filtering}. IEEE Access.

\bibitem{zhang2019hs5}
Wen Zhang, Shumin Deng, Han Wang, et al. (2019). \textit{XTransE: Explainable Knowledge Graph Embedding for Link Prediction with Lifestyles in e-Commerce}. Joint International Conference of Semantic Technology.

\bibitem{wang20198d2}
Zhihao Wang, and Xin Li (2019). \textit{Hybrid-TE: Hybrid Translation-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{tran20195x3}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective}. EDBT/ICDT Workshops.

\bibitem{xiong2018fof}
Shengwu Xiong, Weitao Huang, and P. Duan (2018). \textit{Knowledge Graph Embedding via Relation Paths and Dynamic Mapping Matrix}. ER Workshops.

\bibitem{radstok2021yup}
Wessel Radstok, M. Chekol, and M. SchÃ¤fer (2021). \textit{Are Knowledge Graph Embedding Models Biased, or Is it the Data That They Are Trained on?}. Wikidata@ISWC.

\bibitem{zhao2020o6z}
Ling Zhao, Hanhan Deng, L. Qiu, et al. (2020). \textit{Urban Multi-Source Spatio-Temporal Data Analysis Aware Knowledge Graph Embedding}. Symmetry.

\bibitem{zhang20182ey}
Maoyuan Zhang, Qi Wang, Wukui Xu, et al. (2018). \textit{Discriminative Path-Based Knowledge Graph Embedding for Precise Link Prediction}. European Conference on Information Retrieval.

\bibitem{jia20207dd}
Ningning Jia, Xiang Cheng, and Sen Su (2020). \textit{Improving Knowledge Graph Embedding Using Locally and Globally Attentive Relation Paths}. European Conference on Information Retrieval.

\bibitem{zhu2019ir6}
Qiannan Zhu, Xiaofei Zhou, P. Zhang, et al. (2019). \textit{A neural translating general hyperplane for knowledge graph embedding}. Journal of Computer Science.

\bibitem{wang2021dgy}
Shen Wang, Xiaokai Wei, C. D. Santos, et al. (2021). \textit{Knowledge Graph Representation via Hierarchical Hyperbolic Neural Graph Embedding}. 2021 IEEE International Conference on Big Data (Big Data).

\bibitem{ning20219et}
Zhiyuan Ning, Ziyue Qiao, Hao Dong, et al. (2021). \textit{LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding}. Pacific-Asia Conference on Knowledge Discovery and Data Mining.

\bibitem{sheikh20213qq}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2021). \textit{Knowledge Graph Embedding using Graph Convolutional Networks with Relation-Aware Attention}. arXiv.org.

\bibitem{rim2021s9a}
Wiem Ben Rim, Carolin (Haas) Lawrence, Kiril Gashteovski, et al. (2021). \textit{Behavioral Testing of Knowledge Graph Embedding Models for Link Prediction}. Conference on Automated Knowledge Base Construction.

\bibitem{zhang20179i2}
Chunhong Zhang, Miao Zhou, Xiao Han, et al. (2017). \textit{Knowledge Graph Embedding for Hyper-Relational Data}. Unpublished manuscript.

\bibitem{elebi20182bd}
R. Ã‡elebi, Erkan Yasar, HÃ¼seyin Uyar, et al. (2018). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction using Linked Open Data}. Workshop on Semantic Web Applications and Tools for Life Sciences.

\bibitem{garofalo20185g9}
Martina Garofalo, Maria Angela Pellegrino, Abdulrahman Altabba, et al. (2018). \textit{Leveraging Knowledge Graph Embedding Techniques for Industry 4.0 Use Cases}. arXiv.org.

\bibitem{wang201825m}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2018). \textit{Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network}. arXiv.org.

\bibitem{chung2021u2l}
Chanyoung Chung, and Joyce Jiyoung Whang (2021). \textit{Knowledge Graph Embedding via Metagraph Learning}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{tran2019j42}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space}. International Conference on Theory and Practice of Digital Libraries.

\bibitem{shi2017m2h}
Jun Shi, Huan Gao, G. Qi, et al. (2017). \textit{Knowledge Graph Embedding with Triple Context}. International Conference on Information and Knowledge Management.

\bibitem{zhang2017ixt}
Wen Zhang (2017). \textit{Knowledge Graph Embedding with Diversity of Structures}. The Web Conference.

\bibitem{zhu20196p1}
Ming-Yi Zhu, De-sheng Zhen, Ran Tao, et al. (2019). \textit{Top-N Collaborative Filtering Recommendation Algorithm Based on Knowledge Graph Embedding}. International Conference on Knowledge Management in Organizations.

\bibitem{kertkeidkachorn2019dkn}
Natthawut Kertkeidkachorn, Xin Liu, and R. Ichise (2019). \textit{GTransE: Generalizing Translation-Based Model on Uncertain Knowledge Graph Embedding}. JSAI.

\bibitem{zhu2019zqy}
Jia Zhu, Zetao Zheng, Min Yang, et al. (2019). \textit{A semi-supervised model for knowledge graph embedding}. Data mining and knowledge discovery.

\bibitem{zhang20193g2}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Towards Data Poisoning Attack against Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2019fcs}
Wenqiang Liu, Hongyun Cai, Xu Cheng, et al. (2019). \textit{Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{kanojia20171in}
Vibhor Kanojia, Hideyuki Maeda, Riku Togashi, et al. (2017). \textit{Enhancing Knowledge Graph Embedding with Probabilistic Negative Sampling}. The Web Conference.

\bibitem{gao2018di0}
Huan Gao, Jun Shi, G. Qi, et al. (2018). \textit{Triple Context-Based Knowledge Graph Embedding}. IEEE Access.

\bibitem{mai2018egi}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Support and Centrality: Learning Weights for Knowledge Graph Embedding Models}. International Conference Knowledge Engineering and Knowledge Management.

\bibitem{xiao2016bb9}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2016). \textit{Knowledge Semantic Representation: A Generative Model for Interpretable Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2024q3q}
Peifeng Liu, Lu Qian, Xingwei Zhao, et al. (2024). \textit{Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly}. IEEE Transactions on Industrial Informatics.

\bibitem{zhang2024cjl}
Jin-cheng Zhang, A. Zain, Kai Zhou, et al. (2024). \textit{A review of recommender systems based on knowledge graph embedding}. Expert systems with applications.

\bibitem{su2023v6e}
Xiao-Rui Su, Zhuhong You, Deshuang Huang, et al. (2023). \textit{Biomedical Knowledge Graph Embedding With Capsule Network for Multi-Label Drug-Drug Interaction Prediction}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhu2023bfj}
Xiangrong Zhu, Guang-pu Li, and Wei Hu (2023). \textit{Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning}. The Web Conference.

\bibitem{liu2024to0}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Towards Continual Knowledge Graph Embedding via Incremental Distillation}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2024vgj}
Wei Wang, Xiaoxuan Shen, Baolin Yi, et al. (2024). \textit{Knowledge-aware fine-grained attention networks with refined knowledge graph embedding for personalized recommendation}. Expert systems with applications.

\bibitem{li2024920}
Duantengchuan Li, Tao Xia, Jing Wang, et al. (2024). \textit{SDFormer: A shallow-to-deep feature interaction for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{lee202380l}
Jaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang (2023). \textit{InGram: Inductive Knowledge Graph Embedding via Relation Graphs}. International Conference on Machine Learning.

\bibitem{shokrzadeh2023twj}
Zeinab Shokrzadeh, M. Feizi-Derakhshi, M. Balafar, et al. (2023). \textit{Knowledge graph-based recommendation system enhanced by neural collaborative filtering and knowledge graph embedding}. Ain Shams Engineering Journal.

\bibitem{gao2023086}
Weibo Gao, Hao Wang, Qi Liu, et al. (2023). \textit{Leveraging Transferable Knowledge Concept Graph Embedding for Cold-Start Cognitive Diagnosis}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2024sgp}
Yufeng Li, Wenchao Zhao, Bo Dang, et al. (2024). \textit{Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning}. 2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE).

\bibitem{xue2023qi7}
Zengcan Xue, Zhao Zhang, Hai Liu, et al. (2023). \textit{Learning knowledge graph embedding with multi-granularity relational augmentation network}. Expert systems with applications.

\bibitem{duan2024d3f}
Pengbo Duan, Kuo Yang, Xin Su, et al. (2024). \textit{HTINet2: herbâ€“target prediction via knowledge graph embedding and residual-like graph neural network}. Briefings Bioinform..

\bibitem{chen20246rm}
Zhen Chen, Dalin Zhang, Shanshan Feng, et al. (2024). \textit{KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2022o32}
Jia Zhu, Changqin Huang, and P. D. Meo (2022). \textit{DFMKE: A dual fusion multi-modal knowledge graph embedding framework for entity alignment}. Information Fusion.

\bibitem{mitropoulou20235t0}
Katerina Mitropoulou, Panagiotis C. Kokkinos, P. Soumplis, et al. (2023). \textit{Anomaly Detection in Cloud Computing using Knowledge Graph Embedding and Machine Learning Mechanisms}. Journal of Grid Computing.

\bibitem{shomer2023imo}
Harry Shomer, Wei Jin, Wentao Wang, et al. (2023). \textit{Toward Degree Bias in Embedding-Based Knowledge Graph Completion}. The Web Conference.

\bibitem{wang202490m}
Mingjie Wang, Zijie Li, Jun Wang, et al. (2024). \textit{TracKGE: Transformer with Relation-pattern Adaptive Contrastive Learning for Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{li2024bl5}
Zhifei Li, Wei Huang, Xuchao Gong, et al. (2024). \textit{Decoupled semantic graph neural network for knowledge graph embedding}. Neurocomputing.

\bibitem{li2024y2a}
Mingqi Li, Wenming Ma, and Zihao Chu (2024). \textit{KGIE: Knowledge graph convolutional network for recommender system with interactive embedding}. Knowledge-Based Systems.

\bibitem{jia2023krv}
Yan Jia, Mengqi Lin, Yechen Wang, et al. (2023). \textit{Extrapolation over temporal knowledge graph via hyperbolic embedding}. CAAI Transactions on Intelligence Technology.

\bibitem{huang2023grx}
Wei Huang, Jia Liu, Tianrui Li, et al. (2023). \textit{FedCKE: Cross-Domain Knowledge Graph Embedding in Federated Learning}. IEEE Transactions on Big Data.

\bibitem{wang2023s70}
Ruoxin Wang, and C. F. Cheung (2023). \textit{Knowledge graph embedding learning system for defect diagnosis in additive manufacturing}. Computers in industry (Print).

\bibitem{hou20237gt}
Xiangning Hou, Ruizhe Ma, Li Yan, et al. (2023). \textit{T-GAE: A Timespan-aware Graph Attention-based Embedding Model for Temporal Knowledge Graph Completion}. Information Sciences.

\bibitem{jiang2023opm}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2023). \textit{Multisource hierarchical neural network for knowledge graph embedding}. Expert systems with applications.

\bibitem{lu2022bwo}
H. Lu, Hailin Hu, and Xiaodong Lin (2022). \textit{DensE: An enhanced non-commutative representation for knowledge graph embedding with adaptive semantic hierarchy}. Neurocomputing.

\bibitem{djeddi2023g71}
W. Djeddi, Khalil Hermi, S. Yahia, et al. (2023). \textit{Advancing drugâ€“target interaction prediction: a comprehensive graph-based approach integrating knowledge graph embedding and ProtBert pretraining}. BMC Bioinformatics.

\bibitem{zhang20243iw}
Yuchao Zhang, Xiangjie Kong, Zhehui Shen, et al. (2024). \textit{A survey on temporal knowledge graph embedding: Models and applications}. Knowledge-Based Systems.

\bibitem{le2023hjy}
Thanh-Binh Le, Huy Tran, and H. Le (2023). \textit{Knowledge graph embedding with the special orthogonal group in quaternion space for link prediction}. Knowledge-Based Systems.

\bibitem{yao2023y12}
Zhen Yao, Wen Zhang, Mingyang Chen, et al. (2023). \textit{Analogical Inference Enhanced Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{li2023y5q}
Zhipeng Li, Shanshan Feng, Jun Shi, et al. (2023). \textit{Future Event Prediction Based on Temporal Knowledge Graph Embedding}. Computer systems science and engineering.

\bibitem{yang2022j7z}
Shihan Yang, Weiya Zhang, R. Tang, et al. (2022). \textit{Approximate inferring with confidence predicting based on uncertain knowledge graph embedding}. Information Sciences.

\bibitem{banerjee2023fdi}
Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, et al. (2023). \textit{GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering}. Extended Semantic Web Conference.

\bibitem{hu20230kr}
Yuke Hu, Wei Liang, Ruofan Wu, et al. (2023). \textit{Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2023wgg}
Daiyi Li, Li Yan, Xiaowen Zhang, et al. (2023). \textit{EventKGE: Event knowledge graph embedding with event causal transfer}. Knowledge-Based Systems.

\bibitem{hao2022cl4}
Xinkun Hao, Qingfeng Chen, Haiming Pan, et al. (2022). \textit{Enhancing drugâ€“drug interaction prediction by three-way decision and knowledge graph embedding}. Granular Computing.

\bibitem{khan20222j1}
Nasrullah Khan, Z. Ma, Li Yan, et al. (2022). \textit{Hashing-based semantic relevance attributed knowledge graph embedding enhancement for deep probabilistic recommendation}. Applied intelligence (Boston).

\bibitem{le2022ybl}
Thanh-Binh Le, Ngoc Huynh, and Bac Le (2022). \textit{Knowledge graph embedding by projection and rotation on hyperplanes for link prediction}. Applied intelligence (Boston).

\bibitem{liang202338l}
Shuang Liang (2023). \textit{Knowledge Graph Embedding Based on Graph Neural Network}. IEEE International Conference on Data Engineering.

\bibitem{khan2022ipv}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{DCA-IoMT: Knowledge-Graph-Embedding-Enhanced Deep Collaborative Alert Recommendation Against COVID-19}. IEEE Transactions on Industrial Informatics.

\bibitem{he2022e37}
Peng He, Gang Zhou, Mengli Zhang, et al. (2022). \textit{Improving temporal knowledge graph embedding using tensor factorization}. Applied intelligence (Boston).

\bibitem{shen2022d5j}
Linshan Shen, Rongbo He, and Shaobin Huang (2022). \textit{Entity alignment with adaptive margin learning knowledge graph embedding}. Data & Knowledge Engineering.

\bibitem{di20210ib}
Shimin Di, Quanming Yao, Yongqi Zhang, et al. (2021). \textit{Efficient Relation-aware Scoring Function Search for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{niu2020uyy}
Guanglin Niu, Bo Li, Yongfei Zhang, et al. (2020). \textit{AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding}. Findings.

\bibitem{nie2023ejz}
H. Nie, Xiangguo Zhao, Xin Bi, et al. (2023). \textit{Correlation embedding learning with dynamic semantic enhanced sampling for knowledge graph completion}. World wide web (Bussum).

\bibitem{li2022du0}
Jiayi Li, and Yujiu Yang (2022). \textit{STaR: Knowledge Graph Embedding by Scaling, Translation and Rotation}. Autonomous Infrastructure, Management and Security.

\bibitem{daruna2022dmk}
A. Daruna, Devleena Das, and S. Chernova (2022). \textit{Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions}. IEEE/RJS International Conference on Intelligent RObots and Systems.

\bibitem{zhou20210ma}
Xing-Chun Zhou, Peng Wang, Qi Luo, et al. (2021). \textit{Multi-hop Knowledge Graph Reasoning Based on Hyperbolic Knowledge Graph Embedding and Reinforcement Learning}. IJCKG.

\bibitem{kun202384f}
Kong Wei Kun, Xin Liu, Teeradaj Racharak, et al. (2023). \textit{WeExt: A Framework of Extending Deterministic Knowledge Graph Embedding Models for Embedding Weighted Knowledge Graphs}. IEEE Access.

\bibitem{dong2022taz}
Yao Dong, Lei Wang, Ji Xiang, et al. (2022). \textit{RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space}. International Conference on Computational Linguistics.

\bibitem{kamigaito20218jz}
Hidetaka Kamigaito, and Katsuhiko Hayashi (2021). \textit{Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{krause2022th0}
Franziska Krause (2022). \textit{Dynamic Knowledge Graph Embeddings via Local Embedding Reconstructions}. Extended Semantic Web Conference.

\bibitem{zhang20213h6}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2021). \textit{Knowledge graph embedding with shared latent semantic units}. Neural Networks.

\bibitem{li2021tm6}
Guang-pu Li, Zequn Sun, Lei Qian, et al. (2021). \textit{Rule-based data augmentation for knowledge graph embedding}. AI Open.

\bibitem{wang2020au0}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2020). \textit{Enhancing knowledge graph embedding by composite neighbors for link prediction}. Computing.

\bibitem{wei20215a7}
Yuyang Wei, Wei Chen, Zhixu Li, et al. (2021). \textit{Incremental Update of Knowledge Graph Embedding by Rotating on Hyperplanes}. 2021 IEEE International Conference on Web Services (ICWS).

\bibitem{zhang2021rjh}
Yongqi Zhang, Quanming Yao, and Lei Chen (2021). \textit{Simple and automated negative sampling for knowledge graph embedding}. The VLDB journal.

\bibitem{sheikh202245c}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2022). \textit{Scaling knowledge graph embedding models for link prediction}. EuroMLSys@EuroSys.

\bibitem{ren2021muc}
Chao Ren, Le Zhang, Lintao Fang, et al. (2021). \textit{Ontological Concept Structure Aware Knowledge Transfer for Inductive Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{eyharabide2021wx4}
Victoria Eyharabide, I. E. I. Bekkouch, and Nicolae DragoÈ™ Constantin (2021). \textit{Knowledge Graph Embedding-Based Domain Adaptation for Musical Instrument Recognition}. De Computis.

\bibitem{hong2020hyg}
Y. Hong, Chenyang Bu, and Tingting Jiang (2020). \textit{Rule-enhanced Noisy Knowledge Graph Embedding via Low-quality Error Detection}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{huang2020sqc}
Yan Huang, Haili Sun, Xu Ke, et al. (2020). \textit{CoRelatE: Learning the correlation in multi-fold relations for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{kurokawa2021f4f}
M. Kurokawa (2021). \textit{Explainable Knowledge Reasoning Framework Using Multiple Knowledge Graph Embedding}. IJCKG.

\bibitem{mohamed2021dwg}
Sameh K. Mohamed, Emir MuÃ±oz, and V. NovÃ¡Äek (2021). \textit{On Training Knowledge Graph Embedding Models}. Inf..

\bibitem{gebhart2021gtp}
Thomas Gebhart, J. Hansen, and Paul Schrater (2021). \textit{Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding}. International Conference on Artificial Intelligence and Statistics.

\bibitem{deng2024643}
Weibin Deng, Yiteng Zhang, Hong Yu, et al. (2024). \textit{Knowledge graph embedding based on dynamic adaptive atrous convolution and attention mechanism for link prediction}. Information Processing & Management.

\bibitem{liu2024zr9}
Jin Liu, Hao Du, R. Guo, et al. (2024). \textit{MMGK: Multimodality Multiview Graph Representations and Knowledge Embedding for Mild Cognitive Impairment Diagnosis}. IEEE Transactions on Computational Social Systems.

\bibitem{zhang2024zmq}
Chengcheng Zhang, Tianyi Zang, and Tianyi Zhao (2024). \textit{KGE-UNIT: toward the unification of molecular interactions prediction based on knowledge graph and multi-task learning on drug discovery}. Briefings Bioinform..

\bibitem{he2024vks}
Mingsheng He, Lin Zhu, and Luyi Bai (2024). \textit{ConvTKG: A query-aware convolutional neural network-based embedding model for temporal knowledge graph completion}. Neurocomputing.

\bibitem{zhang2024fy0}
Dong Zhang, Zhe Rong, Chengyuan Xue, et al. (2024). \textit{SimRE: Simple contrastive learning with soft logical rule for knowledge graph embedding}. Information Sciences.

\bibitem{zhang2024ivc}
Dong Zhang, Wenlong Feng, Zonghang Wu, et al. (2024). \textit{CDRGN-SDE: Cross-Dimensional Recurrent Graph Network with neural Stochastic Differential Equation for temporal knowledge graph embedding}. Expert systems with applications.

\bibitem{jing2024nxw}
Yanzhen Jing, Guanghui Zhou, Chao Zhang, et al. (2024). \textit{XMKR: Explainable manufacturing knowledge recommendation for collaborative design with graph embedding learning}. Advanced Engineering Informatics.

\bibitem{jiang2024zlc}
Pengcheng Jiang, Lang Cao, Cao Xiao, et al. (2024). \textit{KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge}. Neural Information Processing Systems.

\bibitem{han2024u0t}
Zhulin Han, and Jian Wang (2024). \textit{Knowledge enhanced graph inference network based entity-relation extraction and knowledge graph construction for industrial domain}. Frontiers of Engineering Management.

\bibitem{quan2024o2a}
Huafeng Quan, Yiting Li, Dashuai Liu, et al. (2024). \textit{Protection of Guizhou Miao batik culture based on knowledge graph and deep learning}. Heritage Science.

\bibitem{liu2024tc2}
Bufan Liu, Chun-Hsien Chen, and Zuoxu Wang (2024). \textit{A multi-hierarchical aggregation-based graph convolutional network for industrial knowledge graph embedding towards cognitive intelligent manufacturing}. Journal of manufacturing systems.

\bibitem{hello2024hgf}
Nour Hello, P. Lorenzo, and E. Strinati (2024). \textit{Semantic Communication Enhanced by Knowledge Graph Representation Learning}. International Workshop on Signal Processing Advances in Wireless Communications.

\bibitem{li2024z0e}
Jinpeng Li, Hang Yu, Xiangfeng Luo, et al. (2024). \textit{COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion}. North American Chapter of the Association for Computational Linguistics.

\bibitem{yan2024joa}
Qun Yan, Juan Zhao, Linfu Xue, et al. (2024). \textit{Mineral Prospectivity Mapping Based on Spatial Feature Classification with Geological Map Knowledge Graph Embedding: Case Study of Gold Ore Prediction at Wulonggou, Qinghai Province (Western China)}. Natural Resources Research.

\bibitem{liu2024tn0}
Jhih-Chen Liu, Chiao-Ting Chen, Chi Lee, et al. (2024). \textit{Evolving Knowledge Graph Representation Learning with Multiple Attention Strategies for Citation Recommendation System}. ACM Transactions on Intelligent Systems and Technology.

\bibitem{wang20245dw}
Chuanghui Wang, Yunqing Yang, Jinshuai Song, et al. (2024). \textit{Research Progresses and Applications of Knowledge Graph Embedding Technique in Chemistry}. Journal of Chemical Information and Modeling.

\bibitem{long2024soi}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{KGDM: A Diffusion Model to Capture Multiple Relation Semantics for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2024ayq}
Qihui Zhou, Peiqi Yin, Xiao Yan, et al. (2024). \textit{Atom: An Efficient Query Serving System for Embedding-based Knowledge Graph Reasoning with Operator-level Batching}. Proc. ACM Manag. Data.

\bibitem{huang2024t19}
Chen Huang, Deshan Chen, Tengze Fan, et al. (2024). \textit{Incorporating environmental knowledge embedding and spatial-temporal graph attention networks for inland vessel traffic flow prediction}. Engineering applications of artificial intelligence.

\bibitem{lu2024fsd}
Ming Lu, Yancong Li, Jiangxiao Zhang, et al. (2024). \textit{Deep hyperbolic convolutional model for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2024yar}
Qi Liu, Qinghua Zhang, Fan Zhao, et al. (2024). \textit{Uncertain knowledge graph embedding: an effective method combining multi-relation and multi-path}. Frontiers Comput. Sci..

\bibitem{khan20242y2}
Nasrullah Khan, Zongmin Ma, Ruizhe Ma, et al. (2024). \textit{Continual knowledge graph embedding enhancement for joint interaction-based next click recommendation}. Knowledge-Based Systems.

\bibitem{xue2025ee8}
Zengcan Xue, Zhaoli Zhang, Hai Liu, et al. (2025). \textit{MHRN: A multi-perspective hierarchical relation network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{long20248vt}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{Fact Embedding through Diffusion Model for Knowledge Graph Completion}. The Web Conference.

\bibitem{huang20240su}
Chen Huang, Fei Yu, Zhiguo Wan, et al. (2024). \textit{Knowledge graph confidence-aware embedding for recommendation}. Neural Networks.

\bibitem{wang2024nej}
Yuzhuo Wang, Hongzhi Wang, Xianglong Liu, et al. (2024). \textit{GFedKG: GNN-based federated embedding model for knowledge graph completion}. Knowledge-Based Systems.

\bibitem{wang2024c8z}
Xinyan Wang, Kuo Yang, Ting Jia, et al. (2024). \textit{KDGene: knowledge graph completion for disease gene prediction using interactional tensor decomposition}. Briefings Bioinform..

\bibitem{liu2024x0k}
Yuhan Liu, Zelin Cao, Xing Gao, et al. (2024). \textit{Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport}. The Web Conference.

\bibitem{li2024uio}
Yongfang Li, and Chunhua Zhu (2024). \textit{TransE-MTP: A New Representation Learning Method for Knowledge Graph Embedding with Multi-Translation Principles and TransE}. Electronics.

\bibitem{zhang2024z78}
Qianjin Zhang, and Yandan Xu (2024). \textit{Knowledge graph embedding with inverse function representation for link prediction}. Engineering applications of artificial intelligence.

\bibitem{wang2024534}
Hao Wang, Dandan Song, Zhijing Wu, et al. (2024). \textit{A collaborative learning framework for knowledge graph embedding and reasoning}. Knowledge-Based Systems.

\bibitem{ni202438q}
Shengkun Ni, Xiangtai Kong, Yingying Zhang, et al. (2024). \textit{Identifying compound-protein interactions with knowledge graph embedding of perturbation transcriptomics}. Cell Genomics.

\bibitem{nie202499i}
Jixuan Nie, Xia Hou, Wenfeng Song, et al. (2024). \textit{Knowledge Graph Efficient Construction: Embedding Chain-of-Thought into LLMs}. VLDB Workshops.

\bibitem{wang2024d52}
Jingchao Wang, Weimin Li, Fangfang Liu, et al. (2024). \textit{ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion}. Expert systems with applications.

\bibitem{mao2024v2s}
Yuren Mao, Yu Hao, Xin Cao, et al. (2024). \textit{Dynamic Graph Embedding via Meta-Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{jafarzadeh202468v}
Parastoo Jafarzadeh, F. Ensan, Mahdiyar Ali Akbar Alavi, et al. (2024). \textit{A Knowledge Graph Embedding Model for Answering Factoid Entity Questions}. ACM Trans. Inf. Syst..

\bibitem{wang2024dea}
Yalin Wang, Yubin Peng, and Jingyu Guo (2024). \textit{Enhancing knowledge graph embedding with structure and semantic features}. Applied intelligence (Boston).

\bibitem{lu202436n}
Yuhuan Lu, Weijian Yu, Xin Jing, et al. (2024). \textit{HyperCL: A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{han2024gaq}
Yadan Han, Guangquan Lu, Shichao Zhang, et al. (2024). \textit{A Temporal Knowledge Graph Embedding Model Based on Variable Translation}. Tsinghua Science and Technology.

\bibitem{liu2024jz8}
Bingchen Liu, Shifu Hou, Weiyi Zhong, et al. (2024). \textit{Enhancing Temporal Knowledge Graph Alignment in News Domain With Box Embedding}. IEEE Transactions on Computational Social Systems.

\bibitem{he2024y6o}
Yunjie He, Daniel HernÃ¡ndez, M. Nayyeri, et al. (2024). \textit{Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning}. Unpublished manuscript.

\bibitem{fang20243a4}
Yan Fang, Xiaodong Liu, Wei Lu, et al. (2024). \textit{Knowledge graph completion with low-dimensional gated hierarchical hyperbolic embedding}. Knowledge-Based Systems.

\bibitem{zhang2024h9k}
Mingtao Zhang, Guoli Yang, Yi Liu, et al. (2024). \textit{Knowledge graph accuracy evaluation: an LLM-enhanced embedding approach}. International Journal of Data Science and Analysis.

\bibitem{li2024wyh}
Yicong Li, Yu Yang, Jiannong Cao, et al. (2024). \textit{Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach}. Knowledge Discovery and Data Mining.

\bibitem{dong2024ijo}
Dibo Dong, Shangwei Wang, Qiaoying Guo, et al. (2024). \textit{Short-Term Marine Wind Speed Forecasting Based on Dynamic Graph Embedding and Spatiotemporal Information}. Journal of Marine Science and Engineering.

\bibitem{wang20246c7}
Tao Wang, Bo Shen, Jinglin Zhang, et al. (2024). \textit{Knowledge Graph Embedding via Triplet Component Interactions}. Neural Processing Letters.

\bibitem{zhang2024yjo}
Pengfei Zhang, Xiaoxue Zhang, Yang Fang, et al. (2024). \textit{Knowledge Graph Embedding for Hierarchical Entities Based on Auto-Embedding Size}. Mathematics.

\bibitem{liang20247wv}
K. Liang, Yue Liu, Hao Li, et al. (2024). \textit{Clustering then Propagation: Select Better Anchors for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{liu2024t05}
Qi Liu, Yuanyuan Jin, Xuefei Cao, et al. (2024). \textit{An Entity Ontology-Based Knowledge Graph Embedding Approach to News Credibility Assessment}. IEEE Transactions on Computational Social Systems.

\bibitem{pham20243mh}
H. V. Pham, Trung Tuan Nguyen, Luu Minh Tuan, et al. (2024). \textit{IDGCN: A Proposed Knowledge Graph Embedding With Graph Convolution Network For Context-Aware Recommendation Systems}. Journal of Organizational Computing and Electronic Commerce.

\bibitem{li2024gar}
Yu Li, Zhu-Hong You, Shu-Min Wang, et al. (2024). \textit{Attention-Based Learning for Predicting Drug-Drug Interactions in Knowledge Graph Embedding Based on Multisource Fusion Information}. International Journal of Intelligent Systems.

\bibitem{li2024nje}
Nan Li, Zhihao Yang, Jian Wang, et al. (2024). \textit{Drugâ€“target interaction prediction using knowledge graph embedding}. iScience.

\bibitem{bao20249xp}
Liming Bao, Yan Wang, Xiaoyu Song, et al. (2024). \textit{HGCGE: hyperbolic graph convolutional networks-based knowledge graph embedding for link prediction}. Knowledge and Information Systems.

\bibitem{xu2024fto}
Guoshun Xu, Guozheng Rao, Li Zhang, et al. (2024). \textit{Entity-relation aggregation mechanism graph neural network for knowledge graph embedding}. Applied intelligence (Boston).

\bibitem{liang2024z0q}
Qiuyu Liang, Weihua Wang, Jie Yu, et al. (2024). \textit{Effective Knowledge Graph Embedding with Quaternion Convolutional Networks}. Natural Language Processing and Chinese Computing.

\bibitem{liu2024ixy}
Jie Liu, Lizheng Zu, Yunbin Yan, et al. (2024). \textit{Multi-Filter soft shrinkage network for knowledge graph embedding}. Expert systems with applications.

\bibitem{dong2025l9k}
Jie Dong, Cuiping Chen, Chi Zhang, et al. (2025). \textit{Knowledge Graph Embedding With Graph Convolutional Network and Bidirectional Gated Recurrent Unit for Fault Diagnosis of Industrial Processes}. IEEE Sensors Journal.

\bibitem{zhang2025ebv}
Sensen Zhang, Xun Liang, Simin Niu, et al. (2025). \textit{Integrating Large Language Models and MÃ¶bius Group Transformations for Temporal Knowledge Graph Embedding on the Riemann Sphere}. AAAI Conference on Artificial Intelligence.

\bibitem{liu20242zm}
Xinyue Liu, Jianan Zhang, Chi Ma, et al. (2024). \textit{Temporal Knowledge Graph Reasoning with Dynamic Hypergraph Embedding}. International Conference on Language Resources and Evaluation.

\bibitem{yang2024lwa}
Ruiyi Yang, Flora D. Salim, and Hao Xue (2024). \textit{SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding}. The Web Conference.

\bibitem{li20246qx}
Bo Li, Haowei Quan, Jiawei Wang, et al. (2024). \textit{Neural Library Recommendation by Embedding Project-Library Knowledge Graph}. IEEE Transactions on Software Engineering.

\bibitem{liu2024mji}
Xiaojian Liu, Xinwei Guo, and Wen Gu (2024). \textit{SecKG2vec: A novel security knowledge graph relational reasoning method based on semantic and structural fusion embedding}. Computers & security.

\bibitem{chen2024efo}
Bin Chen, Hongyi Li, Di Zhao, et al. (2024). \textit{Quality assessment of cyber threat intelligence knowledge graph based on adaptive joining of embedding model}. Complex &amp; Intelligent Systems.

\bibitem{chen2024uld}
Deng Chen, Weiwei Zhang, and Zuohua Ding (2024). \textit{Embedding dynamic graph attention mechanism into Clinical Knowledge Graph for enhanced diagnostic accuracy}. Expert systems with applications.

\bibitem{wang2017zm5}
Quan Wang, Zhendong Mao, Bin Wang, et al. (2017). \textit{Knowledge Graph Embedding: A Survey of Approaches and Applications}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{li2021qr0}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks}. IEEE Transactions on Neural Networks and Learning Systems.

\end{thebibliography}

\end{document}