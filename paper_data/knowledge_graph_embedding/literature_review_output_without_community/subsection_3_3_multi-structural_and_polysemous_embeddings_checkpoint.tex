\subsection*{Multi-Structural and Polysemous Embeddings}

Traditional Knowledge Graph Embedding (KGE) models, primarily designed for static, binary triples, often struggle to capture the full complexity of real-world knowledge. This complexity manifests in two key aspects: entities exhibiting diverse semantic characteristics based on context (polysemy), and knowledge existing in multi-structural forms beyond simple subject-predicate-object triples, such as hyper-relational or n-ary facts. This subsection delves into advanced embedding techniques that address these limitations, enabling KGE models to represent the multifaceted nature of knowledge more accurately and comprehensively.

Addressing entity polysemy, where an entity's meaning varies with context, has led to the development of dynamic and context-aware representations. Early approaches, such as \cite{AttnKGE}, introduced attention mechanisms to dynamically modulate entity embeddings based on the specific relation or local neighborhood. This allows an entity's representation to adapt to its surrounding context, offering a more flexible semantic interpretation. However, while effective, attention-based methods can sometimes incur significant computational overhead and may not explicitly disentangle distinct semantic facets, instead blending them into a single context-dependent vector.

Beyond attention, other context-aware KGE models have emerged to explicitly integrate broader structural information. \cite{luo2015df2} proposed a two-stage scheme that considers both local (triple-specific) and contextual (across triples) connectivity patterns, aiming for more accurate embeddings by leveraging implicit relationships. Expanding on this, \cite{gao2018di0} introduced a triple-context-based approach, where the context for each triple is composed of neighboring entities, their outgoing relations, and relation paths between target entities. This explicit integration of broader graph context enriches entity and relation embeddings, moving beyond isolated triple processing. More recently, \cite{ning20219et} presented LightCAKE, a lightweight framework for context-aware KGE that explicitly models graph context through an iterative aggregation strategy without introducing redundant trainable parameters. LightCAKE demonstrates that effective context integration can be achieved with a proper trade-off between graph context and model complexity, offering a more efficient alternative to some deep learning-heavy contextual models.

A significant advancement in capturing richer semantic patterns and distinct facets of an entity comes from the application of quaternion embeddings. Building upon the expressive algebraic structures of quaternion embeddings, which were introduced for general knowledge graph completion (e.g., ConQuatE \cite{ConQuatE}), subsequent research has adapted this framework to explicitly model polysemy. \cite{QuatPoly} specifically leverages the multi-dimensional components of quaternions to encode distinct semantic facets or meanings for an entity. Unlike traditional real or complex vectors, quaternions possess four components (one real, three imaginary) and non-commutative multiplication. QuatPoly utilizes these distinct components to represent different contextual meanings or aspects of an entity, allowing a single entity to maintain multiple, separable contextual representations within the quaternion space. For instance, each imaginary component could potentially encode a different 'view' or 'role' of an entity, providing a more structured and algebraically grounded way to disambiguate an entity's meaning across various contexts. This offers a potential advantage over simply having multiple independent embedding vectors or relying solely on attention mechanisms to blend a single embedding, as it intrinsically encodes multiple facets within a single, coherent mathematical structure.

Beyond polysemy, KGE models must also contend with the inherent multi-structural nature of real-world knowledge, which frequently extends beyond simple binary triples to include hyper-relational or n-ary facts. These facts involve a predicate and more than two arguments, or additional attributes qualifying a binary relation, which cannot be adequately captured by the (head, relation, tail) paradigm. The challenge lies in effectively embedding these variable-arity structures while maintaining semantic coherence and avoiding information loss.

One prominent approach to embedding hyper-relational facts is exemplified by HypE \cite{HypE}. This method transforms an n-ary fact into a central entity (representing the predicate) and its associated arguments, effectively converting the complex structure into a set of binary-like relationships that KGE models can process. HypE's flexibility in handling varying arities by projecting arguments onto a relation-specific hyperplane is notable. However, its reliance on a central predicate entity might oversimplify the intricate interactions between multiple arguments, potentially losing some of the nuanced semantics of the n-ary relationship.

An alternative perspective is offered by TransHR \cite{zhang20179i2}, which specifically addresses hyper-relational data by transforming the hyper-relations between a pair of entities into an individual vector that acts as a translation. Unlike HypE, which treats the predicate as a central entity, TransHR focuses on how the *hyper-relation itself* modifies the relationship between a head and tail entity, serving as a more nuanced translation vector. This approach aims to capture the specific influence of the hyper-relation on the entity pair, potentially offering better generalization for complex relational data compared to models that project entities into multiple embeddings or rely on a single central predicate representation. However, TransHR's emphasis on a head-tail pair within a hyper-relation might struggle with facts where the 'central' interaction is not easily reducible to a binary pair, or where the interplay between all arguments is equally significant. Other approaches to multi-structural facts include compositional models, which learn to compose the embeddings of arguments to form a representation of the entire n-ary fact, and graph neural network (GNN) based methods that can be generalized to operate on hypergraphs, directly modeling the complex connections.

In summary, the evolution towards multi-structural and polysemous embeddings represents a critical advancement in KGE, moving beyond simplistic representations to capture the nuanced and complex nature of real-world knowledge. Significant strides have been made in modeling context-dependent meanings through attention mechanisms, explicit context integration, and the algebraically rich properties of quaternions for polysemy. Concurrently, methods like HypE and TransHR have begun to tackle the challenge of embedding n-ary and hyper-relational facts. However, a key challenge remains in developing unified models that can seamlessly integrate these diverse aspects. Current models often specialize in either polysemy or multi-structural facts, but real-world knowledge graphs frequently exhibit both. Future research should focus on creating holistic frameworks that can simultaneously capture the contextual polysemy of entities and the complex, variable-arity nature of facts, potentially through adaptive geometric spaces or advanced GNN architectures, while also considering the increased computational overhead associated with these more expressive models.

\bibliographystyle{plainnat}
\bibliography{references}