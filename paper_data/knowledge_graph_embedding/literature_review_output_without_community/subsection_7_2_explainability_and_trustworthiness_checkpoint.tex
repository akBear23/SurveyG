\subsection{Explainability and Trustworthiness}

The increasing deployment of Knowledge Graph Embedding (KGE) models in critical applications, such as personalized recommendations and drug repurposing, has amplified the demand for AI systems that are not only accurate but also transparent, interpretable, and trustworthy. Traditional KGE models, while highly effective at predicting missing links and inferring complex relations, often operate as "black boxes," providing predictions without offering clear, human-understandable reasons for their outputs \cite{rossi2020}. Early foundational models like TransH \cite{wang2014} and RotatE \cite{sun2018} significantly advanced the state-of-the-art in predictive accuracy by learning rich entity and relation embeddings, as further consolidated by reviews such as \cite{asmara2023}. However, their core design focused on optimizing embedding spaces for performance, implicitly lacking mechanisms for explicit explanation.

The absence of transparent reasoning poses significant challenges in sensitive domains where expert validation and user trust are paramount. For instance, a recommendation system might suggest a critical training course, or a drug repurposing model might identify a potential therapeutic, but without a clear explanation of *why* these suggestions are made, users and domain experts may be hesitant to accept or act upon them. This limitation spurred research into developing KGE models that can provide interpretable justifications alongside their predictions.

Significant advancements have been made in generating rule-based or path-based explanations to enhance the trustworthiness of KGE-driven applications. In the realm of recommender systems, \cite{yang2023} introduced the Contextualized Knowledge Graph Embedding (CKGE) framework for explainable talent training course recommendation. This framework addresses the inherent lack of transparency in existing systems by integrating KGE with a novel KG-based Transformer architecture and employing a unique "Local Path Mask Prediction" mechanism. This approach quantifies and highlights the saliency of meta-paths within the knowledge graph, thereby providing explicit, path-based explanations for the generated recommendations and significantly enhancing user trust.

Similarly, in the high-stakes domain of drug repurposing, \cite{islam2023} proposed an ensemble KGE approach for molecular-evaluated and explainable drug repurposing for COVID-19. Recognizing the critical need for transparency, their method not only combines multiple complementary KGE models for robust predictions but also uniquely integrates molecular docking and ligand structural similarity for molecular-level validation. Crucially, it generates "rule-based explanations extracted from the KG" for its predictions, offering transparent insights into the underlying reasoning. This capability is vital for facilitating expert validation and increasing confidence in AI-driven drug discovery, moving beyond mere predictive accuracy to actionable, interpretable insights.

While these advancements in post-hoc explanation generation mark a crucial step towards trustworthy AI, the field continues to evolve. The integration of semantic information, such as entity types as explored by \cite{he2023}, or the optimization of training processes through sophisticated negative sampling strategies \cite{madushanka2024}, indirectly contributes to more robust and potentially more interpretable embeddings. However, a key future direction aims to move beyond generating explanations *after* a prediction is made to developing inherently interpretable KGE models. Such models would ensure that the reasoning process is transparent and understandable from the outset, making their internal mechanisms accessible and verifiable. This shift is vital for the widespread adoption of KGE in sensitive domains, where the ability to audit and comprehend AI decisions is not just a preference but a fundamental requirement.