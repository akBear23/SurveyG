\subsection{Translational Distance Models (e.g., TransE, TransH)}

The challenge of embedding knowledge graphs into continuous vector spaces to facilitate tasks like link prediction led to the development of translational distance models, which established a foundational geometric intuition for Knowledge Graph Embedding (KGE). These models represent entities and relations as vectors, where a relation is conceptualized as a translation operation. The earliest and most influential in this category is TransE, which posits that for a valid triplet $(h, r, t)$, the embedding of the head entity $h$ plus the embedding of the relation $r$ should approximately equal the embedding of the tail entity $t$ ($h + r \approx t$). While elegant and efficient, TransE faced significant limitations in modeling complex relation types, such as one-to-many, many-to-one, and many-to-many relations, due to its assumption of a single, fixed vector representation for each entity regardless of the specific relation it participates in.

To address these shortcomings, \textcite{wang2014} introduced TransH (Translation on Hyperplanes), a direct and influential improvement over TransE. TransH models each relation $r$ not merely as a translation vector, but as a combination of a relation-specific hyperplane (defined by its normal vector $w_r$) and a translation vector $d_r$ that lies within this hyperplane. For a given triplet $(h, r, t)$, the entity embeddings $h$ and $t$ are first projected onto the relation-specific hyperplane $w_r$. This projection mechanism allows entities to have distributed, relation-specific representations, thereby effectively handling complex relation mapping properties that TransE struggled with, without significantly increasing model complexity. Furthermore, \textcite{wang2014} also introduced Bernoulli negative sampling, an improved strategy for generating negative examples based on relation mapping properties, which significantly enhanced training effectiveness by reducing false negatives.

The geometric intuition established by TransH, particularly the concept of relation-specific projections, has had a lasting impact on subsequent KGE research. For instance, the TaKE framework proposed by \textcite{he2023} re-purposes the "hyperplane" concept, applying it to entity *type* representations rather than directly to entities. In TaKE, relation-specific hyperplanes are used to project an entity's type representation, allowing the model to capture diverse type features relevant to different relational contexts and demonstrating a deeper integration of semantic information built upon the foundational geometric ideas. Similarly, the importance of sophisticated negative sampling, first enhanced by TransH's Bernoulli sampling \cite{wang2014}, has evolved into a critical area of dedicated research, as comprehensively reviewed by \textcite{madushanka2024}, highlighting the enduring impact of TransH's training innovations.

The foundational status of TransE and TransH is consistently underscored by their inclusion in comprehensive literature reviews. Surveys such as \textcite{asmara2023} provide a focused comparative analysis of TransE, TransH, and TransR, consolidating knowledge for researchers and practitioners. Broader overviews like that by \textcite{ge2023} classify TransE and TransH as pivotal distance-based models, recognizing their role in establishing the geometric paradigm for KGE and identifying overarching trends in the field, such as the combination of various geometric transformations.

In conclusion, translational distance models, spearheaded by TransE and its direct successor TransH, laid the essential groundwork for KGE by introducing a powerful geometric intuition. TransH, in particular, demonstrated how to overcome the limitations of simpler translation models by introducing relation-specific hyperplanes, thereby paving the way for more expressive and robust models. Their contributions, both in terms of geometric transformations and critical training mechanisms like negative sampling, continue to influence the development of sophisticated KGE approaches that leverage diverse contextual and semantic information.