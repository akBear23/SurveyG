\subsection*{Advanced Models for Complex Relational Patterns (e.g., LineaRE, TranS)}

The accurate representation of diverse and complex relational patterns, such as symmetry, antisymmetry, inversion, composition, and various mapping properties (e.g., one-to-many, many-to-one, many-to-many), presents a significant challenge for Knowledge Graph Embedding (KGE) models. Simpler foundational models, often relying on basic translational or bilinear operations, frequently struggle to comprehensively capture these intricate logical structures. This necessitates the development of advanced architectures and scoring functions specifically engineered to model the nuanced logic of knowledge graph relations. This subsection examines key advancements in KGE models that innovate in their design to precisely capture these complex relational patterns.

A notable advancement in achieving comprehensive relational modeling with remarkable simplicity is **LineaRE** \cite{peng2020}. Unlike models that increase architectural complexity, LineaRE demonstrated that a simple linear regression-based approach could mathematically prove its ability to model all four fundamental connectivity patterns (symmetry, antisymmetry, inversion, and composition) and all four mapping properties (one-to-one, one-to-many, many-to-one, and many-to-many) simultaneously. This work was significant as it provided a powerful, scalable, and mathematically grounded alternative that challenged the prevailing notion that greater expressivity inherently required more complex non-linear transformations or higher-dimensional spaces. Its linear nature offered computational efficiency while providing robust theoretical guarantees for capturing a wide spectrum of relational semantics.

Further architectural innovations emerged to handle more nuanced relational complexities, particularly the context-dependent nature of relations. **TranS** \cite{zhang2022} advanced transition-based KGE models by introducing synthetic relation representations. Instead of relying on static, predefined relation vectors, TranS dynamically generates relation representations that adapt to the specific head and tail entities involved in a triple. This dynamic generation allows the model to effectively handle scenarios where multiple distinct relationships or complex interactions exist between the same entity pair, thereby capturing a richer and more context-dependent relational logic than static embeddings. This approach addresses the polysemy of relations, where the meaning or manifestation of a relation can vary depending on the entities it connects.

Beyond these, other models have explored sophisticated geometric transformations and algebraic structures to enhance pattern capture. **STaR** (Scaling, Translation, and Rotation) \cite{li2022du0} proposes a bilinear KGE model that integrates scaling, translation, and rotation operations. This combination is designed to concurrently model all six common relation patterns (including non-commutativity) and effectively handle complex relations like one-to-N, N-to-1, and N-to-N. STaR introduces a translation matrix as an equivalent for direct incorporation into the bilinear framework, demonstrating how a unified geometric approach can achieve comprehensive expressiveness for diverse relational types.

Addressing a specific, yet crucial, complex pattern, **Rot-Pro** \cite{song2021} focuses on modeling *transitivity*, a pattern often overlooked or poorly handled by prior KGE models. While models like RotatE excelled at symmetry, antisymmetry, inversion, and composition, they struggled with transitivity without forcing entities in a transitive chain to have identical embeddings, thereby limiting expressiveness. Rot-Pro innovates by combining relational rotation (for other patterns) with *projection* for transitivity within a complex vector space. It theoretically proves that transitive relations can be modeled using idempotent transformations (projections), allowing entities in a transitive chain to have distinct embeddings but share the same *projected vector*. This unified framework is capable of inferring all five major patterns: symmetry, antisymmetry, inversion, composition, and transitivity, a significant leap in comprehensive pattern support.

Further generalizing geometric approaches, **CompoundE** \cite{ge2022} proposes a novel KGE model that leverages a cascade of three fundamental geometric operations: translation, rotation, and scaling. By applying these operations in a relation-specific, compound manner, CompoundE is formally cast within the framework of the affine group. This mathematical foundation demonstrates its enhanced capability to model a richer set of complex relation types, including symmetric/antisymmetric, inversion, transitive, commutative/non-commutative, and sub-relations. CompoundE's design flexibility, allowing various permutations and subsets of these operations, enables it to adapt to different dataset characteristics and achieve state-of-the-art performance with fewer parameters, highlighting the power of combining elementary geometric transformations.

Another innovative approach is **ExpressivE** \cite{pavlovic2022qte}, which introduces a spatio-functional embedding. This model embeds pairs of entities as points and relations as hyper-parallelograms in a virtual triple space. This unique geometric design allows ExpressivE to jointly capture a rich set of inference patterns, including composition and hierarchy, while also providing an intuitive and consistent geometric interpretation of these patterns through the spatial relationships of the hyper-parallelograms. This model showcases how novel spatial representations can inherently encode complex relational logic and offer greater interpretability.

In conclusion, the evolution of KGE models for complex relational patterns demonstrates a continuous drive towards greater expressivity, theoretical rigor, and architectural sophistication. From the elegant linear approach of LineaRE \cite{peng2020} and the dynamic synthetic relations of TranS \cite{zhang2022}, to the unified geometric transformations of STaR \cite{li2022du0} and CompoundE \cite{ge2022}, the projection-based transitivity modeling of Rot-Pro \cite{song2021}, and the spatio-functional embeddings of ExpressivE \cite{pavlovic2022qte}, these advancements collectively highlight a profound shift. Researchers are moving beyond simple structural representations to capture the intricate, context-dependent, and multi-faceted nature of knowledge graph relations. Future directions in this area will likely focus on developing even more adaptive and context-aware relation representations, potentially integrating external knowledge or meta-learning techniques to infer and model novel or implicit relational logic within knowledge graphs, further enhancing their reasoning capabilities.