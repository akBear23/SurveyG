\subsection{Security and Privacy in Distributed KGE}

The increasing adoption of Knowledge Graph Embedding (KGE) models in distributed environments, particularly within federated learning (FL) paradigms, introduces a critical frontier for research in security and privacy. While Federated KGE (FKGE), as introduced in Section 5.4, offers a promising solution for collaboratively training models on decentralized, sensitive knowledge graphs without direct data sharing, it simultaneously opens up new attack surfaces and privacy vulnerabilities that demand robust investigation and mitigation. This subsection delves into the specific threats and emerging defense mechanisms, emphasizing the future research directions required to ensure the secure and ethical deployment of KGE models in collaborative, sensitive domains.

A primary concern in distributed KGE is the susceptibility to adversarial attacks, which can compromise model integrity and performance. Data poisoning attacks, where malicious actors inject corrupted data into the training process, pose a significant threat. Initial studies on poisoning attacks against *centralized* KGE models demonstrated their feasibility, showing how adversaries can manipulate the plausibility of targeted facts by adding or deleting triples \cite{zhang20190zu}. This foundational work highlighted the vulnerability of KGE models to data manipulation. However, the landscape of such attacks becomes more complex and insidious in federated settings. In FKGE, clients maintain their KGs locally, preventing direct data injection. This necessitates sophisticated attack strategies where adversaries must infer components of the victim's KG and craft poisoned updates to be aggregated by the central server. \cite{zhou2024} pioneered the systematization of poisoning attacks in FKGE, proposing a novel framework that enables attackers to force a victim client to predict specific false facts. Their method involves a "KG component inference attack" to deduce victim relations, followed by local "shadow model training" with created poisoned data, and an "optimized dynamic poisoning scheme" to generate progressive poisoned updates. This framework achieved remarkable success rates (e.g., 100\% on TransE with WN18RR) while maintaining original task performance, highlighting the stealth and efficacy of such threats and underscoring the urgent need for robust detection and defense mechanisms in future FKGE designs.

Beyond poisoning, other adversarial threats from general federated learning literature also pose potential risks to FKGE. Model inversion attacks, for instance, aim to reconstruct sensitive training data from shared model updates or gradients. While not yet extensively studied in FKGE, the rich semantic information embedded in KGE gradients could potentially be exploited to infer the existence of specific entities or relations that were part of a client's local KG. Similarly, backdoor attacks, where an adversary injects a hidden malicious behavior into the global model that activates under specific triggers, could compromise the integrity of KGE predictions for certain entity or relation patterns. The unique structural and semantic properties of knowledge graphs, compared to image or text data, necessitate dedicated research into how these broader attack types manifest and can be mitigated within FKGE.

Privacy leakage is another paramount challenge. Even without direct data sharing, gradient exchanges in FL can inadvertently reveal sensitive information about clients' local KGs. \cite{hu20230kr} conducted the first holistic study on privacy threats in FKGE, quantifying the risk through novel inference attacks. These attacks successfully inferred the existence of specific KG triples from victim clients, demonstrating substantial privacy leakage from shared model updates. This vulnerability is particularly concerning given the sensitive nature of many real-world KGs (e.g., medical, financial data).

Addressing these multifaceted challenges requires a dual approach focusing on both robust security defenses and comprehensive privacy-preserving techniques. For privacy, differential privacy (DP) has emerged as a leading candidate. \cite{hu20230kr} proposed DP-Flames, a differentially private FKGE framework that offers an improved privacy-utility trade-off. DP-Flames exploits the entity-binding sparse gradient property inherent to FKGE and incorporates state-of-the-art private selection techniques, along with an adaptive privacy budget allocation policy. This approach effectively mitigates inference attacks by significantly reducing their success rates with only a modest decrease in utility. However, the inherent trade-off between privacy guarantees and model utility remains a critical area for future research. The noise introduced by DP can distort the geometric properties of embeddings, potentially impacting the semantic consistency and reasoning capabilities of the KGE model, especially for complex relational patterns.

Furthermore, cryptographic solutions like secure multi-party computation (SMC) and homomorphic encryption (HE) present other promising avenues for privacy preservation in distributed KGE, offering stronger, provable privacy guarantees compared to the probabilistic nature of DP. For example, \cite{huang2023grx} introduced FedCKE, a framework for cross-domain KGE in federated learning that leverages cryptographic techniques. FedCKE employs an "inter-domain encrypted entity/relation alignment method" to identify common entities/relations between domains without revealing sensitive triple structures. It then uses "parameter-secure aggregation" on the server to combine client model updates securely. This allows for secure interaction and fusion of knowledge graph data from different domains without exposing private information. However, the primary limitation of SMC and HE lies in their significant computational overhead and communication costs, which can severely impact the scalability of FKGE systems, particularly when dealing with high-dimensional embeddings and massive knowledge graphs. The computational burden of encrypting and decrypting gradients, or performing computations on encrypted data, often makes these methods impractical for real-time or large-scale deployments.

A critical comparison reveals that DP offers a more lightweight approach with a tunable privacy-utility trade-off, suitable for scenarios where some utility degradation is acceptable and computational resources are constrained. Its main challenge is precisely quantifying the privacy budget and minimizing its impact on embedding quality. In contrast, cryptographic methods provide stronger, theoretically provable privacy guarantees but at a much higher computational and communication cost, making them more suitable for highly sensitive applications where absolute privacy is paramount and resources are less of a constraint. The choice between these paradigms often depends on the specific application's privacy requirements, acceptable utility loss, and available computational infrastructure.

In conclusion, while FKGE holds immense promise for leveraging distributed knowledge, its secure and private deployment faces significant, largely unsolved challenges. Future research must bridge the existing gaps by developing more adaptive, efficient, and provably secure FKGE architectures that can withstand evolving threats while maintaining high utility. This includes:
\begin{itemize}
    \item **Advanced Attack Detection and Mitigation**: Investigating Byzantine-resilient aggregation algorithms (e.g., Krum, Trimmed Mean) tailored for KGE gradient updates to detect and filter malicious client contributions. Developing anomaly detection techniques that can identify poisoned updates based on their impact on embedding geometry or relational patterns.
    \item **Hybrid Defense Strategies**: Exploring novel combinations of DP with secure aggregation or other cryptographic primitives to achieve an optimal balance between privacy, utility, and computational efficiency. For instance, applying local DP to gradients before secure aggregation could offer enhanced privacy with reduced overhead compared to end-to-end HE.
    \item **Formal Security and Privacy Guarantees**: Developing formal frameworks and metrics to rigorously evaluate and prove the security and privacy properties of FKGE systems, moving beyond empirical demonstrations.
    \item **Broader Threat Modeling**: Extending research to systematically analyze and defend against model inversion, backdoor attacks, and other emerging threats specifically within the unique context of KGEs.
    \item **Preserving Semantic Consistency**: Innovating privacy mechanisms that specifically aim to preserve the semantic consistency and relational integrity of embeddings, rather than just obscuring raw data, to ensure the utility of KGE for downstream reasoning tasks.
\end{itemize}
The secure, private, and ethical deployment of KGE models in collaborative, sensitive domains hinges on these future advancements, ensuring model trustworthiness and data confidentiality are non-negotiable requirements.