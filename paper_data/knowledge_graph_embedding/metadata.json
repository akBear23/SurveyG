{
    "82ca372cdb6d4a13004486e6fccf54faf8191315.pdf": {
        "title": "Temporal Smoothness Regularisers for Neural Link Predictors",
        "authors": [
            "Manuel Dileo",
            "Pasquale Minervini",
            "Matteo Zignani",
            "S. Gaito"
        ],
        "published_date": "2023",
        "journal": "arXiv.org",
        "abstract": "Most algorithms for representation learning and link prediction on relational data are designed for static data. However, the data to which they are applied typically evolves over time, including online social networks or interactions between users and items in recommender systems. This is also the case for graph-structured knowledge bases -- knowledge graphs -- which contain facts that are valid only for specific points in time. In such contexts, it becomes crucial to correctly identify missing links at a precise time point, i.e. the temporal prediction link task. Recently, Lacroix et al. and Sadeghian et al. proposed a solution to the problem of link prediction for knowledge graphs under temporal constraints inspired by the canonical decomposition of 4-order tensors, where they regularise the representations of time steps by enforcing temporal smoothing, i.e. by learning similar transformation for adjacent timestamps. However, the impact of the choice of temporal regularisation terms is still poorly understood. In this work, we systematically analyse several choices of temporal smoothing regularisers using linear functions and recurrent architectures. In our experiments, we show that by carefully selecting the temporal smoothing regulariser and regularisation weight, a simple method like TNTComplEx can produce significantly more accurate results than state-of-the-art methods on three widely used temporal link prediction datasets. Furthermore, we evaluate the impact of a wide range of temporal smoothing regularisers on two state-of-the-art temporal link prediction models. Our work shows that simple tensor factorisation models can produce new state-of-the-art results using newly proposed temporal regularisers, highlighting a promising avenue for future research.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/82ca372cdb6d4a13004486e6fccf54faf8191315.pdf"
    },
    "015f4afb73bdfdc90d3f4a8881006397c1a6f7f5.pdf": {
        "title": "Knowledge Graph Reasoning Model Based on Negative Sampling Optimization",
        "authors": [
            "Yi Yuan",
            "Yongxin Feng",
            "Yuntao Zhao"
        ],
        "published_date": "2024",
        "journal": "2024 2nd International Conference on Signal Processing and Intelligent Computing (SPIC)",
        "abstract": "In the process of training a network security knowledge graph reasoning model, a large number of negative samples are required to enhance the model's generalization ability and accuracy. Considering that traditional negative sampling methods fail to generate high-quality negative samples, which in turn affects the accuracy of the network security knowledge graph reasoning model, based on Knowledge graph embedding by translating on hyperplanes (TransH), we propose an improved negative sampling TransH model (INS-TransH). This model constrains the negative samples based on the syntactic rules between entities and relations, thereby further enhancing the quality of generated negative samples. It also applies a self-adversarial negative sampling method that calculates the scores of negative samples to filter out low-quality samples. We construct a network security knowledge graph using Common Vulnerabilities and Exposures (CWE) database, Common Weakness Enumeration (CVE) database, and Common Attack Pattern Enumeration and Classification (CAPEC) database, and use this knowledge graph as the dataset for the INS-TransH model. Experimental results demonstrate that the INS-TransH model exhibits improved knowledge reasoning capability, with enhancements in Hits at 10 (Hits@10) and Mean Reciprocal Rank (MRR) metrics.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/015f4afb73bdfdc90d3f4a8881006397c1a6f7f5.pdf"
    },
    "2ca8aef0f3accdc93803d8231dad6ba573193e9d.pdf": {
        "title": "RatE: Relation-Adaptive Translating Embedding for Knowledge Graph Completion",
        "authors": [
            "Hao Huang",
            "Guodong Long",
            "Tao Shen",
            "Jing Jiang",
            "Chengqi Zhang"
        ],
        "published_date": "2020",
        "journal": "International Conference on Computational Linguistics",
        "abstract": "Many graph embedding approaches have been proposed for knowledge graph completion via link prediction. Among those, translating embedding approaches enjoy the advantages of light-weight structure, high efficiency and great interpretability. Especially when extended to complex vector space, they show the capability in handling various relation patterns including symmetry, antisymmetry, inversion and composition. However, previous translating embedding approaches defined in complex vector space suffer from two main issues: 1) representing and modeling capacities of the model are limited by the translation function with rigorous multiplication of two complex numbers; and 2) embedding ambiguity caused by one-to-many relations is not explicitly alleviated. In this paper, we propose a relation-adaptive translation function built upon a novel weighted product in complex space, where the weights are learnable, relation-specific and independent to embedding size. The translation function only requires eight more scalar parameters each relation, but improves expressive power and alleviates embedding ambiguity problem. Based on the function, we then present our Relation-adaptive translating Embedding (RatE) approach to score each graph triple. Moreover, a novel negative sampling method is proposed to utilize both prior knowledge and self-adversarial learning for effective optimization. Experiments verify RatE achieves state-of-the-art performance on four link prediction benchmarks.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/2ca8aef0f3accdc93803d8231dad6ba573193e9d.pdf"
    },
    "4ed72065b6a97de06049e4e3c55c6bcb6e060fae.pdf": {
        "title": "Temporal Knowledge Graph Completion with Time-sensitive Relations in Hypercomplex Space",
        "authors": [
            "Lianshang Cai",
            "Xin Mao",
            "Zhihong Wang",
            "Shangqing Zhao",
            "Yuhao Zhou",
            "Changxu Wu",
            "Man Lan"
        ],
        "published_date": "2024",
        "journal": "arXiv.org",
        "abstract": "Temporal knowledge graph completion (TKGC) aims to fill in missing facts within a given temporal knowledge graph at a specific time. Existing methods, operating in real or complex spaces, have demonstrated promising performance in this task. This paper advances beyond conventional approaches by introducing more expressive quaternion representations for TKGC within hypercomplex space. Unlike existing quaternion-based methods, our study focuses on capturing time-sensitive relations rather than time-aware entities. Specifically, we model time-sensitive relations through time-aware rotation and periodic time translation, effectively capturing complex temporal variability. Furthermore, we theoretically demonstrate our method's capability to model symmetric, asymmetric, inverse, compositional, and evolutionary relation patterns. Comprehensive experiments on public datasets validate that our proposed approach achieves state-of-the-art performance in the field of TKGC.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/4ed72065b6a97de06049e4e3c55c6bcb6e060fae.pdf"
    },
    "b527d027563f4a96d3686a681017c625f49d45ad.pdf": {
        "title": "Modeling Unseen Entities from a Semantic Evidence View in Temporal Knowledge Graphs",
        "authors": [
            "Chen Guo",
            "Tianxiu Wang",
            "Yang Lin",
            "Hao Chen",
            "Haiyang Yu",
            "Chengwei Zhu",
            "Jing Qiu"
        ],
        "published_date": "2022",
        "journal": "International Conference on Data Science in Cyberspace",
        "abstract": "Temporal Knowledge Graphs (TKGs), which model dynamic events along the timeline, have attracted much attention in recent years. For temporal knowledge graphs suffer from incompleteness, quite a lot of researches are devoted to TKG reasoning, attempting to predict missing temporal facts from known events. Depending on different tasks of TKG reasoning, existing work can be divided into two categories: interpolation and extrapolation. Compared with interpolation, extrapolation faces more challenges and difficulties, which is intended to forecast facts in the future. It\u2019s worth noting that dealing with unseen entities effectively is one of important challenges, while only a few of prior work focuses on handling unseen data explicitly. To this end, we model unseen entities from semantic perspectives in this work, combining with temporal-path-based reinforcement learning, which guarantees the interpretability of reasoning. Through extensive experiments on standard TKG tasks, we verify that our model performs well on link prediction task at future timestamps, demonstrating better extrapolation capabilities.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/b527d027563f4a96d3686a681017c625f49d45ad.pdf"
    },
    "8214ea85abaacefa7db27014cdef7e603ebe8f76.pdf": {
        "title": "ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding",
        "authors": [
            "Lijing Zhu",
            "Qizhen Lan",
            "Qing Tian",
            "Wenbo Sun",
            "Li Yang",
            "Lu Xia",
            "Yixin Xie",
            "Xi Xiao",
            "Tiehang Duan",
            "Cui Tao",
            "Shuteng Niu"
        ],
        "published_date": "2025",
        "journal": "arXiv.org",
        "abstract": "Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge while preserving past information. However, existing methods struggle with efficiency and scalability due to two key limitations: (1) suboptimal knowledge preservation between snapshots caused by manually designed node/relation importance scores that ignore graph dependencies relevant to the downstream task, and (2) computationally expensive graph traversal for node/relation importance calculation, leading to slow training and high memory overhead. To address these limitations, we introduce ETT-CKGE (Efficient, Task-driven, Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE method that leverages efficient task-driven tokens for efficient and effective knowledge transfer between snapshots. Our method introduces a set of learnable tokens that directly capture task-relevant signals, eliminating the need for explicit node scoring or traversal. These tokens serve as consistent and reusable guidance across snapshots, enabling efficient token-masked embedding alignment between snapshots. Importantly, knowledge transfer is achieved through simple matrix operations, significantly reducing training time and memory usage. Extensive experiments across six benchmark datasets demonstrate that ETT-CKGE consistently achieves superior or competitive predictive performance, while substantially improving training efficiency and scalability compared to state-of-the-art CKGE methods. The code is available at: https://github.com/lijingzhu1/ETT-CKGE/tree/main",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/8214ea85abaacefa7db27014cdef7e603ebe8f76.pdf"
    },
    "2a3f862199883ceff5e3c74126f0c80770653e05.pdf": {
        "title": "Knowledge Graph Embedding by Translating on Hyperplanes",
        "authors": [
            "Zhen Wang",
            "Jianwen Zhang",
            "Jianlin Feng",
            "Zheng Chen"
        ],
        "published_date": "2014",
        "journal": "AAAI Conference on Artificial Intelligence",
        "abstract": "\n \n We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.\n \n",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "The main contribution of this paper is the introduction of **TransH**, a novel knowledge graph embedding model designed to effectively handle complex relation mapping properties (such as reflexive, one-to-many, many-to-one, and many-to-many) while maintaining the computational efficiency characteristic of simpler models like TransE.\n\n**Problem Addressed:**\nThe paper addresses the challenge of embedding large-scale knowledge graphs into continuous vector spaces. While TransE, a recently proposed method, offers efficiency and good predictive performance, it struggles to adequately model complex mapping properties inherent in real-world relations. More sophisticated models can preserve these properties but often sacrifice efficiency. The paper aims to find a good trade-off between model capacity (ability to capture complex relation types) and computational efficiency. Additionally, it tackles the practical problem of constructing negative examples during training to reduce false negative labels in incomplete knowledge graphs.\n\n**Method Used:**\nTransH models a relation not just as a translation vector, but as a **hyperplane** in the embedding space, along with a **translation operation performed on that hyperplane**. For a given triplet (head entity, relation, tail entity), both the head and tail entities are first projected onto the relation-specific hyperplane. The translation operation then occurs between these projected entities. This mechanism allows TransH to differentiate between entities involved in complex mappings (e.g., multiple tails for one head) without significantly increasing model complexity. Furthermore, the paper proposes a simple trick for constructing negative examples by leveraging the one-to-many/many-to-one mapping properties of relations, which helps reduce the likelihood of false negative labeling during training.\n\n**Key Findings:**\nExtensive experiments on benchmark datasets like WordNet and Freebase, across tasks such as link prediction, triplet classification, and fact extraction, demonstrate that TransH delivers significant improvements in predictive accuracy compared to TransE. Crucially, it achieves this enhanced performance with comparable scalability and efficiency, effectively balancing model capacity with computational cost. The model successfully preserves complex relation mapping properties, overcoming a key limitation of TransE.\n\n**Key Math Equations (Conceptual):**\nWhile explicit equations are not provided in the abstract, the core mathematical concept of TransH can be described as follows:\n1.  Each relation `r` is represented by two vectors: a normal vector `w_r` (defining the hyperplane) and a translation vector `d_r` (which lies on the hyperplane, i.e., `d_r \u22a5 w_r`).\n2.  For a given triplet `(h, r, t)`, the entity embeddings `h` and `t` are first projected onto the hyperplane defined by `w_r`. The projection of an entity vector `v` onto this hyperplane can be conceptually represented as `v_\u22a5 = v - (v^T w_r)w_r` (assuming `w_r` is a unit vector).\n3.  The scoring function for a valid triplet aims to minimize the distance between the translated projected head and the projected tail:\n    `f(h, r, t) = || (h - (h^T w_r)w_r) + d_r - (t - (t^T w_r)w_r) ||`\n    where `||.||` denotes a distance metric (e.g., L1 or L2 norm).\n\n**New Direction: Yes**\nReasoning: This paper proposes a new *method path* within the existing research direction of translational knowledge graph embedding models. While TransE established the translation-based paradigm, TransH introduces a fundamentally different and more sophisticated way to model relations by incorporating relation-specific hyperplanes. This conceptual shift allows it to address complex mapping properties that TransE could not, thereby significantly enhancing the capacity of translational models and opening up new avenues for their improvement. It's not just an incremental tweak but a significant methodological innovation within its class.\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/2a3f862199883ceff5e3c74126f0c80770653e05.pdf"
    },
    "493c72a2beb9ed2ffa0184b01d1a2dd6f30b2384.pdf": {
        "title": "SIE-GNN: A Link Prediction Algorithm Based on Semantic-Aware Graph Neural Networks",
        "authors": [
            "Guo Jiusi",
            "Gao Jing",
            "Congjiao Xie"
        ],
        "published_date": "2024",
        "journal": "International Conference on Innovative Computing and Cloud Computing",
        "abstract": "In the field of knowledge graph link prediction, models based on Graph Neural Networks (GNNs) handle multi-faceted and diverse relationships, effectively capturing complex relationships and patterns between entities. However, these models face limitations in addressing complex interactions between entities. A novel link prediction model, SIE-GNN, is proposed to address this issue. This model combines Semantic Evidence-Aware Graph Neural Networks (SE-GNN) with the InteractE model that integrates a triple-attention mechanism. Evaluation results on public datasets indicate that the SIE-GNN model shows improvements in performance metrics compared to the SE-GNN model.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/493c72a2beb9ed2ffa0184b01d1a2dd6f30b2384.pdf"
    },
    "a2057cb5600179d0af947b4be9380dcc2ee386d8.pdf": {
        "title": "Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities",
        "authors": [
            "Wei Lu",
            "Rachel K. Luu",
            "Markus J. Buehler"
        ],
        "published_date": "2024",
        "journal": "npj Computational Materials",
        "abstract": "The advancement of Large Language Models (LLMs) for domain applications in fields such as materials science and engineering depends on the development of fine-tuning strategies that adapt models for specialized, technical capabilities. In this work, we explore the effects of Continued Pretraining (CPT), Supervised Fine-Tuning (SFT), and various preference-based optimization approaches, including Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO), on fine-tuned LLM performance. Our analysis shows how these strategies influence model outcomes and reveals that the merging of multiple fine-tuned models can lead to the emergence of capabilities that surpass the individual contributions of the parent models. We find that model merging leads to new functionalities that neither parent model could achieve alone, leading to improved performance in domain-specific assessments. Experiments with different model architectures are presented, including Llama 3.1 8B and Mistral 7B models, where similar behaviors are observed. Exploring whether the results hold also for much smaller models, we use a tiny LLM with 1.7 billion parameters and show that very small LLMs do not necessarily feature emergent capabilities under model merging, suggesting that model scaling may be a key component. In open-ended yet consistent chat conversations between a human and AI models, our assessment reveals detailed insights into how different model variants perform and show that the smallest model achieves a high intelligence score across key criteria including reasoning depth, creativity, clarity, and quantitative precision. Other experiments include the development of image generation prompts based on disparate biological material design concepts, to create new microstructures, architectural concepts, and urban design based on biological materials-inspired construction principles.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/a2057cb5600179d0af947b4be9380dcc2ee386d8.pdf"
    },
    "990334cf76845e2da64d3baa10b0a671e433d4b6.pdf": {
        "title": "TorusE: Knowledge Graph Embedding on a Lie Group",
        "authors": [
            "Takuma Ebisu",
            "R. Ichise"
        ],
        "published_date": "2017",
        "journal": "AAAI Conference on Artificial Intelligence",
        "abstract": "\n \n Knowledge graphs are useful for many artificial intelligence (AI) tasks. However, knowledge graphs often have missing facts. To populate the graphs, knowledge graph embedding models have been developed. Knowledge graph embedding models map entities and relations in a knowledge graph to a vector space and predict unknown triples by scoring candidate triples. TransE is the first translation-based method and it is well known because of its simplicity and efficiency for knowledge graph completion. It employs the principle that the differences between entity embeddings represent their relations. The principle seems very simple, but it can effectively capture the rules of a knowledge graph. However, TransE has a problem with its regularization. TransE forces entity embeddings to be on a sphere in the embedding vector space. This regularization warps the embeddings and makes it difficult for them to fulfill the abovementioned principle. The regularization also affects adversely the accuracies of the link predictions. On the other hand, regularization is important because entity embeddings diverge by negative sampling without it. This paper proposes a novel embedding model, TorusE, to solve the regularization problem. The principle of TransE can be defined on any Lie group. A torus, which is one of the compact Lie groups, can be chosen for the embedding space to avoid regularization. To the best of our knowledge, TorusE is the first model that embeds objects on other than a real or complex vector space, and this paper is the first to formally discuss the problem of regularization of TransE. Our approach outperforms other state-of-the-art approaches such as TransE, DistMult and ComplEx on a standard link prediction task. We show that TorusE is scalable to large-size knowledge graphs and is faster than the original TransE.\n \n",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Main Contribution Summary:**\n\nThis paper introduces **TorusE**, a novel knowledge graph embedding model designed to address the regularization problems inherent in translation-based models like TransE.\n\n*   **Problem Addressed:** Traditional knowledge graph embedding models, particularly TransE, suffer from issues related to regularization. TransE forces entity embeddings onto a sphere in a real vector space, which warps the embeddings, making it difficult for them to accurately represent the `head + relation \u2248 tail` principle and negatively impacting link prediction accuracy. While regularization is crucial to prevent embedding divergence, its current implementation in TransE is problematic.\n\n*   **Method Used:** TorusE proposes to embed entities and relations not in a real or complex vector space, but on a **Lie group**, specifically a **torus**. The core idea is that the translation principle of TransE can be naturally defined on any Lie group. By choosing a compact Lie group like a torus, the model intrinsically avoids the need for explicit regularization. The compactness of the torus prevents embeddings from diverging, thus solving the regularization problem without warping the embedding space. This is the first model to embed objects on a space other than a real or complex vector space.\n\n*   **Key Findings:**\n    *   TorusE significantly outperforms state-of-the-art knowledge graph embedding models such as TransE, DistMult, and ComplEx on standard link prediction tasks.\n    *   The model demonstrates strong scalability to large-sized knowledge graphs.\n    *   TorusE is faster than the original TransE model.\n\n*   **Key Math Equations (Implied):**\n    *   **TransE's Translation Principle (General Form):** `h + r \u2248 t`, where `h, r, t` are embeddings of head entity, relation, and tail entity, respectively.\n    *   **TransE's Scoring Function (e.g., L1/L2 norm):** `score(h, r, t) = ||h + r - t||_p` (where `p` is 1 or 2).\n    *   **TransE's Regularization:** `||e||_2 = 1` for entity embeddings `e`.\n    *   **TorusE's Adapted Principle:** The translation principle is applied directly on the torus. This implies a scoring function using a distance metric on the torus, e.g., `score(h, r, t) = d_T(h \\oplus r, t)`, where `d_T` is a suitable distance metric on the torus and `\\oplus` is the group operation on the torus. The explicit regularization `||e||_2 = 1` is *avoided* due to the inherent compactness of the torus space.\n\n---\n\n**New Direction: Yes**\n\n**Reasoning:** This paper proposes a new research direction by fundamentally changing the underlying mathematical space for knowledge graph embeddings. Prior work primarily focused on real or complex vector spaces. TorusE's innovation lies in moving to a **Lie group (a torus)**. This is explicitly stated as \"the first model that embeds objects on other than a real or complex vector space.\" This shift opens up a completely new avenue for research, encouraging the exploration of other geometric spaces and their properties (e.g., hyperbolic spaces, other Lie groups) for knowledge representation, rather than just refining existing methods within Euclidean or complex domains. It's a conceptual leap in how embeddings are structured and regularized.\n\n---\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/990334cf76845e2da64d3baa10b0a671e433d4b6.pdf"
    },
    "aa01f43e88184f13a0b9f8d5e8337951f1c2fef1.pdf": {
        "title": "Croppable Knowledge Graph Embedding",
        "authors": [
            "Yushan Zhu",
            "Wen Zhang",
            "Zhiqiang Liu",
            "Mingyang Chen",
            "Lei Liang",
            "Hua-zeng Chen"
        ],
        "published_date": "2024",
        "journal": "Annual Meeting of the Association for Computational Linguistics",
        "abstract": "Knowledge Graph Embedding (KGE) is a common approach for Knowledge Graphs (KGs) in AI tasks. Embedding dimensions depend on application scenarios. Requiring a new dimension means training a new KGE model from scratch, increasing cost and limiting efficiency and flexibility. In this work, we propose a novel KGE training framework MED. It allows one training to obtain a croppable KGE model for multiple scenarios with different dimensional needs. Sub-models of required dimensions can be directly cropped and used without extra training. In MED, we propose a mutual learning mechanism to improve the low-dimensional sub-models and make high-dimensional sub-models retain the low-dimensional sub-models' capacity, an evolutionary improvement mechanism to promote the high-dimensional sub-models to master the triple that the low-dimensional sub-models can not, and a dynamic loss weight to adaptively balance the multiple losses. Experiments on 4 KGE models across 4 standard KG completion datasets, 3 real-world scenarios using a large-scale KG, and extending MED to the BERT language model demonstrate its effectiveness, high efficiency, and flexible extensibility.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/aa01f43e88184f13a0b9f8d5e8337951f1c2fef1.pdf"
    },
    "7237d45b0b0cfd9e3c2b3441723caa655d27acb1.pdf": {
        "title": "SIT: Selective Incremental Training for Dynamic Knowledge Graph Embedding",
        "authors": [
            "Zhifeng Jia",
            "Hanmo Liu",
            "Haoyang Li",
            "Lei Chen"
        ],
        "published_date": "2025",
        "journal": "IEEE International Conference on Data Engineering",
        "abstract": "In recent years, dynamic knowledge graph embedding (DKGE) has been widely studied to deal with large-scale dynamic knowledge graphs (DKG). The core idea is to encode dynamic information within DKGs into embedding vectors and decode them for various downstream tasks on the DKGs. Plenty of contributions have been made to this field. Full retraining DKG models additionally encode temporal information for higher performance, while neighboring retraining models view time data as dynamic changes in graph topology for better efficiency. However, existing approaches within these categories suffer from either effectiveness-insufficient or efficiency-insufficient issues. Recent contributions in graph area propose solutions to selectively retrain the models by choosing training data following certain criteria, but the majority of selective retraining models are designed for homogeneous graphs. The heterogeneous graph information and large graph sizes make it improper to transfer methods across scenarios. In this paper, we propose an efficient selective incremental training framework for DKGE, namely SIT. Given a restriction on training data size, we select a set of important triples instead of all triples in the DKG to improve training efficiency. In detail, we design a novel importance criteria considering DKGE model parameters, historical embedding and graph topology. Extensive experiments on open-source datasets demonstrate the effectiveness and efficiency of the SIT framework against different DKGE models.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/7237d45b0b0cfd9e3c2b3441723caa655d27acb1.pdf"
    },
    "e1034cc4f5676fff43b7bfa94f9cad24755ac4e5.pdf": {
        "title": "Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding",
        "authors": [
            "Linyu Li",
            "Zhi Jin",
            "Yuanpeng He",
            "Dongming Jin",
            "Yichi Zhang",
            "Haoran Duan",
            "Nyima Tash"
        ],
        "published_date": "2025",
        "journal": "arXiv.org",
        "abstract": "Since knowledge graphs (KG) will continue to evolve in real scenarios, traditional KGE models are only suitable for static knowledge graphs. Therefore, continual knowledge graph embedding (CKGE) has attracted the attention of researchers. Currently, a key challenge facing CKGE is that the model is prone to\"catastrophic forgetting\", resulting in the loss of previously learned knowledge. In order to effectively alleviate this problem, we propose a new CKGE model BAKE. First, we note that the Bayesian posterior update principle provides a natural continual learning strategy that is insensitive to data order and can theoretically effectively resist the forgetting of previous knowledge during data evolution. Different from the existing CKGE method, BAKE regards each batch of new data as a Bayesian update of the model prior. Under this framework, as long as the posterior distribution of the model is maintained, the model can better preserve the knowledge of early snapshots even after evolving through multiple time snapshots. Secondly, we propose a continual clustering method for CKGE, which further directly combats knowledge forgetting by constraining the evolution difference (or change amplitude) between new and old knowledge between different snapshots. We conduct extensive experiments on BAKE on multiple datasets, and the results show that BAKE significantly outperforms existing baseline models.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/e1034cc4f5676fff43b7bfa94f9cad24755ac4e5.pdf"
    },
    "67d8d17d06fa49760dfdd4112207b39fa52c243e.pdf": {
        "title": "Modeling Multi-mapping Relations for Precise Cross-lingual Entity Alignment",
        "authors": [
            "Xiaofei Shi",
            "Yanghua Xiao"
        ],
        "published_date": "2019",
        "journal": "Conference on Empirical Methods in Natural Language Processing",
        "abstract": "Entity alignment aims to find entities in different knowledge graphs (KGs) that refer to the same real-world object. An effective solution for cross-lingual entity alignment is crucial for many cross-lingual AI and NLP applications. Recently many embedding-based approaches were proposed for cross-lingual entity alignment. However, almost all of them are based on TransE or its variants, which have been demonstrated by many studies to be unsuitable for encoding multi-mapping relations such as 1-N, N-1 and N-N relations, thus these methods obtain low alignment precision. To solve this issue, we propose a new embedding-based framework. Through defining dot product-based functions over embeddings, our model can better capture the semantics of both 1-1 and multi-mapping relations. We calibrate embeddings of different KGs via a small set of pre-aligned seeds. We also propose a weighted negative sampling strategy to generate valuable negative samples during training and we regard prediction as a bidirectional problem in the end. Experimental results (especially with the metric Hits@1) on real-world multilingual datasets show that our approach significantly outperforms many other embedding-based approaches with state-of-the-art performance.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/67d8d17d06fa49760dfdd4112207b39fa52c243e.pdf"
    },
    "10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf": {
        "title": "Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding",
        "authors": [
            "Mingyang Chen",
            "Wen Zhang",
            "Zhen Yao",
            "Yushan Zhu",
            "Yang Gao",
            "Jeff Z. Pan",
            "Hua-zeng Chen"
        ],
        "published_date": "2023",
        "journal": "AAAI Conference on Artificial Intelligence",
        "abstract": "We propose an entity-agnostic representation learning method for handling the problem of inefficient parameter storage costs brought by embedding knowledge graphs. Conventional knowledge graph embedding methods map elements in a knowledge graph, including entities and relations, into continuous vector spaces by assigning them one or multiple specific embeddings (i.e., vector representations). Thus the number of embedding parameters increases linearly as the growth of knowledge graphs. In our proposed model, Entity-Agnostic Representation Learning (EARL), we only learn the embeddings for a small set of entities and refer to them as reserved entities. To obtain the embeddings for the full set of entities, we encode their distinguishable information from their connected relations, k-nearest reserved entities, and multi-hop neighbors. We learn universal and entity-agnostic encoders for transforming distinguishable information into entity embeddings. This approach allows our proposed EARL to have a static, efficient, and lower parameter count than conventional knowledge graph embedding methods. Experimental results show that EARL uses fewer parameters and performs better on link prediction tasks than baselines, reflecting its parameter efficiency.",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Problem Addressed:**\nThe paper addresses the problem of inefficient parameter storage costs in conventional Knowledge Graph Embedding (KGE) methods. Traditional KGE approaches assign specific vector representations (embeddings) to every entity and relation, causing the number of embedding parameters to increase linearly with the growth of the knowledge graph, leading to high storage and computational costs for large-scale KGs.\n\n**Method Used:**\nThe paper proposes an Entity-Agnostic Representation Learning (EARL) method. EARL tackles the parameter efficiency problem by not learning specific embeddings for all entities. Instead, it:\n1.  Learns and stores embeddings for only a small, pre-defined subset of entities, referred to as \"reserved entities.\"\n2.  For all other entities (including non-reserved ones), their embeddings are dynamically generated. This is achieved by encoding their \"distinguishable information,\" which is derived from their connected relations, their k-nearest reserved entities, and their multi-hop neighbors within the graph.\n3.  Universal and entity-agnostic encoders are employed to transform this distinguishable contextual information into the final entity embeddings.\nThis design allows EARL to maintain a static, efficient, and significantly lower parameter count, independent of the total number of entities in the knowledge graph.\n\n**Key Findings:**\nExperimental results demonstrate that EARL achieves superior parameter efficiency, utilizing fewer parameters than conventional baseline KGE methods. Crucially, despite this significant reduction in parameters, EARL performs better on standard link prediction tasks, indicating that its entity-agnostic representation learning effectively captures the necessary semantic information while being more resource-efficient.\n\n**Key Math Equations:**\nThe provided content does not explicitly include any mathematical equations.\n\n**New Direction: Yes**\n**Reasoning:** This paper proposes a new research direction and method path by introducing a paradigm shift in how entity embeddings are learned for parameter efficiency in Knowledge Graph Embedding. Instead of the conventional approach of directly learning and storing a unique embedding for every entity, EARL introduces the novel concept of \"reserved entities\" and \"entity-agnostic encoders.\" This allows for the dynamic generation of entity embeddings from contextual information (relations, neighbors) rather than direct storage, leading to a static and significantly reduced parameter count irrespective of the knowledge graph's size. This shift from direct parameterization to a generative, context-dependent parameterization for entities is a novel and impactful approach to addressing the scalability and parameter efficiency challenges in KGEs.\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf"
    },
    "55d5e73d64537eb52a54ef9d4be0b6fc26dbd267.pdf": {
        "title": "Scaling Knowledge Graph Embedding with Parallel TransE and Graph Partitioning",
        "authors": [
            "Khouloud Ammar",
            "Wissem Inoubli",
            "Sami Zghal",
            "E. Nguifo"
        ],
        "published_date": "2024",
        "journal": "ACS/IEEE International Conference on Computer Systems and Applications",
        "abstract": "Knowledge graph embedding has emerged as a fundamental technique to represent entities and relationships in knowledge graphs within low-dimensional vector spaces. Among these methods, translation-based approaches stand out by treating relations as translations from head entities to tail entities, achieving state-of-the-art results. However, the training process of these methods can be prohibitively time-consuming, especially for large knowledge graphs, posing significant challenges in practical applications. As knowledge graphs grow in size and complexity, surpassing the capacities of existing systems, there is an urgent need for scalable solutions in knowledge representation learning. These graphs, comprising millions of nodes and billions of edges, serve as powerful data structures for representing and understanding complex networks of knowledge. Translation-based models, particularly TransE, have been instrumental in encoding structured information about entities and their relationships in low-dimensional embedding spaces. However, the current implementation of TransE is constrained to single-node machines, limiting its scalability and applicability. To address these limitations, this paper proposes leveraging parallel computing technique, such as parallel TransE with graph partitioning, to enhance scalability and efficiency in knowledge graph embedding.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/55d5e73d64537eb52a54ef9d4be0b6fc26dbd267.pdf"
    },
    "01e33f3b5826f6ca848e31acdb2ac1871b362211.pdf": {
        "title": "GNN-FTuckER: A novel link prediction model for identifying suitable populations for tea varieties",
        "authors": [
            "Jun Li",
            "Bing Yang",
            "Jiaxin Liu",
            "Xu Wang",
            "Zhongyuan Wu",
            "Qiang Huang",
            "Peng He"
        ],
        "published_date": "2025",
        "journal": "PLoS ONE",
        "abstract": "Current research on tea primarily focuses on foundational studies of phenotypic characteristics, with insufficient exploration of the relationship between tea varieties and suitable populations. To address this issue, this paper proposes a link prediction model based on Graph Neural Networks (GNN) and tensor decomposition, named GNN-FTuckER, designed to predict the \u201ctea suitability\u201d relationships within the tea knowledge graph. This model integrates the SE-GNN structural encoder with an improved TuckER model decoder. The SE-GNN encoder enhances the modeling capability of the global graph structure by explicitly modeling relations, entities, and triples, thereby obtaining embedding vectors through aggregation, updating, and iterative operations. The improved TuckER model enhances the capture of complex semantics between entities and relations by introducing nonlinear activation functions. To support our research, we constructed a tea dataset, TeaPle. In comparative experiments, GNN-FTuckER achieved superior performance on both public datasets (WN18RR, FB15k-237) and the TeaPle dataset. Ablation studies indicate that the model improved H@10 by 4.3% on the WN18RR dataset and by 1.5% on the FB15k-237 dataset, with a 1.3% increase in MRR. In the TeaPle dataset, H@3 improved by 4.7% and H@10 increased by 3.1%. This research provides significant insights for further exploring the potential of tea varieties and evaluating the health benefits of tea consumption.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/01e33f3b5826f6ca848e31acdb2ac1871b362211.pdf"
    },
    "90fd972d81f4c8d031028f1616269a176651e356.pdf": {
        "title": "RETIA: Relation-Entity Twin-Interact Aggregation for Temporal Knowledge Graph Extrapolation",
        "authors": [
            "Kangzheng Liu",
            "Feng Zhao",
            "Guandong Xu",
            "Xianzhi Wang",
            "Hai Jin"
        ],
        "published_date": "2023",
        "journal": "IEEE International Conference on Data Engineering",
        "abstract": "Temporal knowledge graph (TKG) extrapolation aims to predict future unknown events (facts) based on historical information, and has attracted considerable attention due to its great practical significance. Accurate representations (embeddings) of entities and relations form the basis of TKG extrapolation. Recent work has been devoted to improving the rationality of entity representations. However, on the one hand, ignoring relation modeling results in incomplete relation representations; therefore, some approaches aggregate only immediately adjacent entities of relations, but this can lead to the \"message islands\" problem of relation modeling. On the other hand, ignoring the association constraints between relations and entities can make the embeddings of both relations and entities prone to overfitting. To address the abovementioned challenges, we propose an advanced method, namely, RETIA. For the former issue, we generate twin hyperrelation subgraphs for each historical subgraph and then aggregate both the adjacent entities and relations in the hyperrelation subgraphs through a graph convolutional network (GCN). About the latter concern, we propose a twin-interact module (TIM), which provides communication channels for relation aggregation and entity aggregation during the evolution of the historical sequence. Experiments conducted on five public datasets show that RETIA has made great improvements across several evaluation metrics. Our released code is available at https://github.com/CGCL-codes/RETIA.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/90fd972d81f4c8d031028f1616269a176651e356.pdf"
    },
    "aad0ec661d87a83e4c3bdc50b977779d8c349fdd.pdf": {
        "title": "Node-based Knowledge Graph Contrastive Learning for Medical Relationship Prediction",
        "authors": [
            "Zhiguang Fan",
            "Yuedong Yang",
            "Mingyuan Xu",
            "Hongming Chen"
        ],
        "published_date": "2023",
        "journal": "arXiv.org",
        "abstract": "The embedding of Biomedical Knowledge Graphs (BKGs) generates robust representations, valuable for a variety of artificial intelligence applications, including predicting drug combinations and reasoning disease-drug relationships. Meanwhile, contrastive learning (CL) is widely employed to enhance the distinctiveness of these representations. However, constructing suitable contrastive pairs for CL, especially within Knowledge Graphs (KGs), has been challenging. In this paper, we proposed a novel node-based contrastive learning method for knowledge graph embedding, NC-KGE. NC-KGE enhances knowledge extraction in embeddings and speeds up training convergence by constructing appropriate contrastive node pairs on KGs. This scheme can be easily integrated with other knowledge graph embedding (KGE) methods. For downstream task such as biochemical relationship prediction, we have incorporated a relation-aware attention mechanism into NC-KGE, focusing on the semantic relationships and node interactions. Extensive experiments show that NC-KGE performs competitively with state-of-the-art models on public datasets like FB15k-237 and WN18RR. Particularly in biomedical relationship prediction tasks, NC-KGE outperforms all baselines on datasets such as PharmKG8k-28, DRKG17k-21, and BioKG72k-14, especially in predicting drug combination relationships. We release our code at https://github.com/zhi520/NC-KGE.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/aad0ec661d87a83e4c3bdc50b977779d8c349fdd.pdf"
    },
    "bc1c5c0cd480fdb573b9e21fb699f45e8e72b62a.pdf": {
        "title": "HTransE: Hybrid Translation-based Embedding for Knowledge Graphs",
        "authors": [
            "A. Shah",
            "Bonaventure C. Molokwu",
            "Ziad Kobti"
        ],
        "published_date": "2022",
        "journal": "2022 IEEE International Conference on Knowledge Graph (ICKG)",
        "abstract": "Basically, a Knowledge Graph (KG) is a graph variant that represents data via triplets comprising a head, a tail, and a relation. Realistically, most KGs are compiled either manually or semi-automatically, and this usually results in a significant loss of vital information with respect to the KG. Thus, this problem of incompleteness is common to virtually all KGs; and it is formally defined as Knowledge Graph Completion (KGC) problem. In this paper, we have explored learning the representations of a KGs with regard to its entities and relations for the purpose of any predicting missing link(s). In that regard, this paper proposes a hybrid variant, composed of TransE and SimplE models, for solving KGC problems. On one hand, the TransE model depicts a relation as the translation from the source entity (head) to the target entity (tail) within an embedding space. In TransE, the head and tail entities are derived from the same embedding-generation class, which results in a low prediction score. Also, the TransE model is not able to capture symmetric relationships as well as one-to-many relationships. On the other hand, the SimplE model is based on Canonical Polyadic (CP) decomposition. SimplE enhances CP via the addition of the inverse relation, while the head entity and tail entity are derived from different embedding-generation classes which are interdependent. Hence, we employed the principle of inverse-relation embedding (from the SimplE model) onto the native TransE model so as to yield a new hybrid resultant: HTransE. Therefore, HTransE boasts of efficiency as well as improved prediction scores. Efficiently, HTransE converges much quicker in comparison to TransE. In other words, HTransE converges at approximately $n/2$ iterations where $n$ denotes the iterations required to fully train TransE. Our results outperform the native TransE approach with a significant difference. Also, HTransE outperforms several state-of-the-art models on different datasets.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/bc1c5c0cd480fdb573b9e21fb699f45e8e72b62a.pdf"
    },
    "5eed6005cd44ecbf17c7a154d2c4e40dc75692d4.pdf": {
        "title": "A Novel Asymmetric Embedding Model for Knowledge Graph Completion",
        "authors": [
            "Zhiqiang Geng",
            "Zhongkun Li",
            "Yongming Han"
        ],
        "published_date": "2018",
        "journal": "International Conference on Pattern Recognition",
        "abstract": "Modeling knowledge graph completion by encoding each entity and relation into a continuous tensor space becomes very hot. Meanwhile, many models including TransE, TransH, TransR, CTransR, TransD, TranSpare, TransDR, STransE, DT, FT and OrbitE are proposed for knowledge graph completion. However, all these previous works take less attention to the asymmetrical and the imbalance of many relations (some relations link a subject and many objects, and other relations link many subjects and many objects). Therefore, this paper proposes a novel asymmetrical embedding model(AEM) for knowledge graph completion. Because of the different properties of the head and tail entities in the triplets of the same relationship, every head entity vector and every tail entity vector are weighted by the corresponding head relation vector and the corresponding tail relation vector, respectively. And then new entity vector representations are obtained and the new entity vectors in the same triple are similar. Because the AEM weights each dimension of the entity vectors, it can accurately represent the latent attributes of entities and relationships. Moreover, the number of parameters of the AEM is so small that it is easier to train. Finally, compared with previous embedding models, the AEM obtains a better link prediction performance through two benchmark datasets FB15K and WN18.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/5eed6005cd44ecbf17c7a154d2c4e40dc75692d4.pdf"
    },
    "f1833b793c9c7f72af775e59495e8afae945ca6b.pdf": {
        "title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding",
        "authors": [
            "Yifei Li",
            "Lingling Zhang",
            "Hang Yan",
            "Tianzhe Zhao",
            "Zihan Ma",
            "Muye Huang",
            "Jun Liu"
        ],
        "published_date": "2025",
        "journal": "Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2",
        "abstract": "Traditional knowledge graph (KG) embedding methods aim to represent entities and relations in a low-dimensional space, primarily focusing on static graphs. However, real-world KGs are dynamically evolving with the constant addition of entities, relations and facts. To address such dynamic nature of KGs, several continual knowledge graph embedding (CKGE) methods have been developed to efficiently update KG embeddings to accommodate new facts while maintaining learned knowledge. As KGs grow at different rates and scales in real-world scenarios, existing CKGE methods often fail to consider the varying scales of updates and lack systematic evaluation throughout the entire update process. In this paper, we propose SAGE, a scale-aware gradual evolution framework for CKGE. Specifically, SAGE firstly determine the embedding dimensions based on the update scales and expand the embedding space accordingly. The Dynamic Distillation mechanism is further employed to balance the preservation of learned knowledge and the incorporation of new facts. We conduct extensive experiments on seven benchmarks, and the results show that SAGE consistently outperforms existing baselines, with a notable improvement of 1.38% in MRR, 1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with methods using fixed embedding dimensions show that SAGE achieves optimal performance on every snapshot, demonstrating the importance of adaptive embedding dimensions in CKGE. The codes of SAGE are publicly available at: https://github.com/lyfxjtu/Dynamic-Embedding.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/f1833b793c9c7f72af775e59495e8afae945ca6b.pdf"
    },
    "35bf12df551fa4851495585005e666ae53957672.pdf": {
        "title": "VISION: a modular AI assistant for natural human-instrument interaction at scientific user facilities",
        "authors": [
            "Shray Mathur",
            "Noah van der Vleuten",
            "Kevin Yager",
            "Esther Tsai"
        ],
        "published_date": "2024",
        "journal": "Machine Learning: Science and Technology",
        "abstract": "Scientific user facilities, such as synchrotron beamlines, are equipped with a wide array of hardware and software tools that require a codebase for human-computer-interaction. This often necessitates developers to be involved to establish connection between users/researchers and the complex instrumentation. The advent of generative AI presents an opportunity to bridge this knowledge gap, enabling seamless communication and efficient experimental workflows. Here we present a modular architecture for the Virtual Scientific Companion by assembling multiple AI-enabled cognitive blocks that each scaffolds large language models (LLMs) for a specialized task. With VISION, we performed LLM-based operation on the beamline workstation with low latency and demonstrated the first voice-controlled experiment at an x-ray scattering beamline. The modular and scalable architecture allows for easy adaptation to new instruments and capabilities. Development on natural language-based scientific experimentation is a building block for an impending future where a science exocortex\u2014a synthetic extension to the cognition of scientists\u2014may radically transform scientific practice and discovery.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/35bf12df551fa4851495585005e666ae53957672.pdf"
    },
    "bffca4e6ec2876542bbb1191528be122e987b919.pdf": {
        "title": "How Does a Generative Large Language Model Perform on Domain-Specific Information Extraction?\u2500A Comparison between GPT-4 and a Rule-Based Method on Band Gap Extraction",
        "authors": [
            "Xin Wang",
            "Liangliang Huang",
            "Shuozhi Xu",
            "Kun Lu"
        ],
        "published_date": "2024",
        "journal": "Journal of Chemical Information and Modeling",
        "abstract": "The advent of generative Large Language Models (LLMs) has greatly impacted the field of Natural Language Processing. However, it is inconclusive how generative LLMs perform on domain-specific information extraction tasks. This study compares the performance of GPT-4 and a rule-based information extraction method based on ChemDataExtractor on band gap information extraction, a task that has important implications for the materials science domain. No training data is required for either method, which is desirable because there is a lack of training data in the materials science domain compared with a variety of material information that is of interest. Manual evaluation on 415 randomly selected articles showed that the GPT-4 model achieved a higher level of accuracy in extracting materials' band gap information than the rule-based method (Correctness 87.95% vs 51.08%, Partial correctness 11.33% vs 36.87%, incorrectness 0.72% vs 12.05%). Further analysis of the errors reveals the strengths and weaknesses of the GPT-4 model compared to the rule-based method. The GPT-4 model shows stronger performance in interdependency resolution and complicated material name recognition, while it also has weaknesses in hallucination, identifying band gap values, and identifying band gap types. Revised prompt based on the error analysis leads to improved accuracy for GPT-4. To the best of our knowledge, this study is the first to compare the GPT-4 model and ChemDataExtractor for the band gap extraction task. This study provides evidence to support using generative LLMs for domain-specific information extraction tasks.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/bffca4e6ec2876542bbb1191528be122e987b919.pdf"
    },
    "ee6da7e7c6785f9aa7c610884ae3294f39958d1a.pdf": {
        "title": "Fuzzy Logic based Logical Query Answering on Knowledge Graph",
        "authors": [
            "X. Chen",
            "Ziniu Hu",
            "Yizhou Sun"
        ],
        "published_date": "2021",
        "journal": "AAAI Conference on Artificial Intelligence",
        "abstract": "Answering complex First-Order Logical (FOL) queries on large-scale incomplete knowledge graphs (KGs) is an important yet challenging task. Recent advances embed logical queries and KG entities in the same space and conduct query answering via dense similarity search. However, most logical operators designed in previous studies do not satisfy the axiomatic system of classical logic, limiting their performance. Moreover, these logical operators are parameterized and thus require many complex FOL queries as training data, which are often arduous to collect or even inaccessible in most real-world KGs. We thus present FuzzQE, a fuzzy logic based logical query embedding framework for answering FOL queries over KGs. FuzzQE follows fuzzy logic to define logical operators in a principled and learning-free manner, where only entity and relation embeddings require learning. FuzzQE can further benefit from labeled complex logical queries for training. Extensive experiments on two benchmark datasets demonstrate that FuzzQE provides significantly better performance in answering FOL queries compared to state-of-the-art methods. In addition, FuzzQE trained with only KG link prediction can achieve comparable performance to those trained with extra complex query data.",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Summary:**\n\n**Problem Addressed:**\nThe paper addresses the challenging task of answering complex First-Order Logical (FOL) queries on large-scale, incomplete knowledge graphs (KGs). It highlights two key limitations of existing embedding-based methods: (1) their logical operators often fail to satisfy the axiomatic system of classical logic, hindering performance, and (2) these parameterized operators necessitate large amounts of complex FOL query training data, which is frequently difficult or impossible to acquire in real-world scenarios.\n\n**Method Used:**\nThe paper introduces **FuzzQE**, a fuzzy logic based logical query embedding framework. FuzzQE's core approach involves defining logical operators (e.g., conjunction, disjunction, negation) in a principled and learning-free manner, directly following the mathematical framework of fuzzy logic. This ensures axiomatic consistency for the operators. Unlike previous methods, only the entity and relation embeddings within the KG require learning, significantly reducing the reliance on complex query training data. While the operators are learning-free, FuzzQE can optionally benefit from labeled complex logical queries for further performance enhancement.\n\n**Key Findings:**\n*   FuzzQE demonstrates significantly superior performance in answering FOL queries compared to state-of-the-art methods across two benchmark datasets.\n*   A crucial finding is that FuzzQE, even when trained solely with KG link prediction (i.e., without any complex query data), achieves performance comparable to methods that rely on extensive complex query training data. This underscores its data efficiency and robustness.\n\n**Key Math Equations (if applicable):**\nWhile specific equations are not provided in the content, the core mathematical contribution lies in the *principled definition of logical operators based on fuzzy logic*. This involves leveraging established fuzzy logic operations such as t-norms (for conjunction), t-conorms (for disjunction), and fuzzy implication/negation functions, which provide a mathematically grounded framework for handling degrees of truth for logical operations.\n\n---\n\n**New Direction: Yes**\n\n**Reasoning:**\nThis paper proposes a new method path by introducing a fundamentally different approach to defining logical operators within the query embedding paradigm for KG query answering. Prior methods typically relied on parameterized, data-driven operators that often lacked axiomatic consistency and demanded extensive training data. FuzzQE, in contrast, offers a *principled, learning-free approach* by grounding these operators in fuzzy logic. This shift from purely empirical, data-dependent operator learning to an axiomatically-sound, fuzzy-logic-based definition represents a significant methodological departure. It addresses a core limitation of previous work (lack of axiomatic consistency and data hunger) with a novel theoretical and practical solution, thereby opening a new avenue for designing more robust and data-efficient logical query answering systems.\n\n---\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/ee6da7e7c6785f9aa7c610884ae3294f39958d1a.pdf"
    },
    "f29bbf38cda51564bec05f9a13be4c81c5c7539d.pdf": {
        "title": "On the Knowledge Graph Completion Using Translation Based Embedding: The Loss Is as Important as the Score",
        "authors": [
            "M. Nayyeri",
            "Chengjin Xu",
            "Yadollah Yaghoobzadeh",
            "H. S. Yazdi",
            "Jens Lehmann"
        ],
        "published_date": "2019",
        "journal": "arXiv.org",
        "abstract": "Knowledge graphs (KGs) represent world's facts in structured forms. KG completion exploits the existing facts in a KG to discover new ones. Translation-based embedding model (TransE) is a prominent formulation to do KG completion. Despite the efficiency of TransE in memory and time, it suffers from several limitations in encoding relation patterns such as many-to-many relation patterns, symmetric, reflexive etc. To tackle this problem, most of the attempts have circled around the revision of the score function of TransE i.e., proposing a more complicated score function such as Trans(A, D, G, H, R, etc) to mitigate the limitations. In this paper, we tackle this problem from a different perspective. We pose theoretical investigations of the main limitations of TransE in the light of loss function rather than the score function. To the best of our knowledge, this has not been investigated so far comprehensively. We show that by a proper selection of the loss function for training the TransE model, the main limitations of the model are mitigated. This is explained by setting upper-bound for the scores of positive samples, showing the region of truth (i.e., the region that a triple is considered positive by the model). Our theoretical proofs with experimental results fill the gap between the capability of translation-based class of embedding models and the loss function. The theories emphasize the importance of the selection of the loss functions for training the models. Our experimental evaluations on different loss functions used for training the models justify our theoretical proofs and confirm the importance of the loss functions on the performance.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/f29bbf38cda51564bec05f9a13be4c81c5c7539d.pdf"
    },
    "edab323e53a441bdd0ae262316f9a6550dcb4996.pdf": {
        "title": "Revisiting Text and Knowledge Graph Joint Embeddings: The Amount of Shared Information Matters!",
        "authors": [
            "Paolo Rosso",
            "Dingqi Yang",
            "P. Cudr\u00e9-Mauroux"
        ],
        "published_date": "2019",
        "journal": "2019 IEEE International Conference on Big Data (Big Data)",
        "abstract": "Jointly learning embeddings from text and a Knowledge Graph benefits both word and entity/relation embeddings by taking advantage of both large-scale unstructured content (text) and high-quality structured data (the Knowledge Graph). Current techniques leverage anchors to associate entities in the Knowledge Graph to corresponding words in the text corpus; these anchors are then used to generate additional learning samples during the embedding learning process. However, we show in this paper that such techniques yield suboptimal results, as they fail to control the amount of shared information between the two data sources during the joint learning process. Moreover, the additional learning samples often incur significant computational overhead. Aiming at releasing the power of such joint embeddings, we propose JOINER, a new joint text and Knowledge Graph embedding method using regularization. JOINER not only preserves co-occurrence between words in a text corpus and relations between entities in a Knowledge Graph, it also provides the flexibility to control the amount of information shared between the two data sources via regularization. Our method does not generate additional learning samples, which makes it computationally efficient. Our extensive empirical evaluation on real datasets shows the superiority of JOINER across different evaluation tasks, including analogical reasoning, link prediction, and relation extraction. Compared to state-of-the-art techniques generating additional learning samples from a set of anchors, our method yields better results (with up to 4.3% absolute improvement) and significantly less computational overhead (76% less learning time overhead).",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/edab323e53a441bdd0ae262316f9a6550dcb4996.pdf"
    },
    "a467d2c79ff319aaa5361e8b1fc1e4ddb6305048.pdf": {
        "title": "Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning",
        "authors": [
            "Markus J. Buehler"
        ],
        "published_date": "2024",
        "journal": "Machine Learning: Science and Technology",
        "abstract": "Leveraging generative Artificial Intelligence (AI), we have transformed a dataset comprising 1000 scientific papers focused on biological materials into a comprehensive ontological knowledge graph. Through an in-depth structural analysis of this graph, we have calculated node degrees, identified communities along with their connectivities, and evaluated clustering coefficients and betweenness centrality of pivotal nodes, uncovering fascinating knowledge architectures. We find that the graph has an inherently scale-free nature, shows a high level of connectedness, and can be used as a rich source for downstream graph reasoning by taking advantage of transitive and isomorphic properties to reveal insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, propose never-before-seen material designs, and predict material behaviors. Using a large language embedding model we compute deep node representations and use combinatorial node similarity ranking to develop a path sampling strategy that allows us to link dissimilar concepts that have previously not been related. One comparison revealed detailed structural parallels between biological materials and Beethoven\u2019s 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. In another example, the algorithm proposed an innovative hierarchical mycelium-based composite based on integrating path sampling with principles extracted from Kandinsky\u2019s \u2018Composition VII\u2019 painting. The resulting material integrates an innovative set of concepts that include a balance of chaos and order, adjustable porosity, mechanical strength, and complex patterned chemical functionalization. We uncover other isomorphisms across science, technology and art, revealing a nuanced ontology of immanence that reveal a context-dependent heterarchical interplay of constituents. Because our method transcends established disciplinary boundaries through diverse data modalities (graphs, images, text, numerical data, etc), graph-based generative AI achieves a far higher degree of novelty, explorative capacity, and technical detail, than conventional approaches and establishes a widely useful framework for innovation by revealing hidden connections.",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Summary for Research Survey:**\n\n**Problem Addressed:**\nThe paper addresses the challenge of accelerating scientific discovery by transcending traditional disciplinary boundaries, identifying gaps in existing knowledge, and generating novel insights, particularly in areas like material design and behavior prediction. It aims to overcome the limitations of conventional approaches by achieving a higher degree of novelty, explorative capacity, and technical detail in uncovering unprecedented interdisciplinary relationships.\n\n**Method Used:**\nThe authors propose a multi-faceted methodology centered on generative AI and graph-based reasoning:\n1.  **Generative Knowledge Extraction:** Generative AI is employed to transform a large dataset (1000 scientific papers on biological materials) into a comprehensive ontological knowledge graph.\n2.  **Graph-based Representation and Analysis:** The constructed knowledge graph is subjected to in-depth structural analysis, including calculation of node degrees, identification of communities and their connectivities, and evaluation of clustering coefficients and betweenness centrality for pivotal nodes.\n3.  **Deep Node Representation:** A large language embedding model is used to compute deep, contextualized representations for the nodes within the graph.\n4.  **Path Sampling Strategy:** Leveraging combinatorial node similarity ranking based on these embeddings, a path sampling strategy is developed. This strategy is crucial for linking dissimilar concepts that have not been previously related, facilitating the discovery of hidden connections.\n5.  **Multimodal Intelligent Graph Reasoning:** The framework utilizes transitive and isomorphic properties for downstream graph reasoning, integrating diverse data modalities (graphs, images, text, numerical data). Isomorphic mapping is specifically highlighted for finding structural parallels across disparate domains (e.g., science, music, art).\n6.  **Application to Novel Design:** The method integrates path sampling with principles extracted from artistic works (e.g., Kandinsky\u2019s \u2018Composition VII\u2019) to propose innovative material designs.\n\n**Key Findings:**\n*   The constructed knowledge graph exhibits an inherently scale-free nature and a high level of connectedness, proving to be a rich source for downstream graph reasoning.\n*   The method successfully uncovers unprecedented interdisciplinary relationships, identifies knowledge gaps, proposes novel material designs, and predicts material behaviors.\n*   It revealed detailed structural parallels between biological materials and Beethoven\u2019s 9th Symphony through isomorphic mapping, demonstrating shared patterns of complexity.\n*   The algorithm proposed an innovative hierarchical mycelium-based composite by integrating path sampling with principles from Kandinsky\u2019s \u2018Composition VII\u2019, resulting in a material with a unique balance of chaos and order, adjustable porosity, mechanical strength, and complex patterned chemical functionalization.\n*   The research uncovered other isomorphisms across science, technology, and art, revealing a nuanced ontology of immanence and a context-dependent heterarchical interplay of constituents.\n*   The proposed framework achieves a significantly higher degree of novelty, explorative capacity, and technical detail compared to conventional approaches, establishing a widely useful framework for innovation.\n\n**Key Math Equations (if applicable):**\nWhile no explicit mathematical equations are presented, the methodology implicitly relies on concepts from graph theory (e.g., node degrees, clustering coefficients, betweenness centrality), linear algebra for vector space embeddings and similarity metrics (e.g., cosine similarity for combinatorial node similarity ranking), and abstract algebra for isomorphic mapping (finding structure-preserving transformations between complex systems).\n\n**New Direction: Yes**\n\n**Reasoning:**\nThis paper proposes a new research direction and method path due to its highly innovative integration and application of advanced AI techniques. The key elements that signify a new direction include:\n1.  **Generative AI for Large-Scale Ontological Knowledge Graph Construction:** The use of generative AI to automatically extract and structure knowledge from a vast corpus of scientific literature into an ontological graph represents a significant step towards autonomous knowledge base creation for scientific discovery.\n2.  **Multimodal Cross-Domain Isomorphic Reasoning:** The most novel aspect is the application of graph reasoning, particularly through path sampling and isomorphic mapping, to link \"dissimilar concepts\" across vastly different modalities and domains (e.g., biological materials, music, abstract art). This goes beyond typical knowledge graph applications by generating *tangible scientific outputs* like novel material designs from such diverse inspirations.\n3.  **Integration of Artistic Principles for Scientific Design:** The explicit and systematic use of principles derived from art (e.g., Kandinsky's 'Composition VII') to inform and inspire the design of new scientific materials is a highly unconventional and potentially transformative approach, opening entirely new avenues for interdisciplinary innovation and creative problem-solving in science.\n4.  **Emphasis on \"Unprecedented\" and \"Never-Before-Seen\":** The paper's explicit goal of discovering truly novel connections and designs, rather than merely optimizing existing ones, positions it as a pioneering effort in explorative AI for scientific discovery, pushing the boundaries of what AI can contribute to fundamental research.\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/a467d2c79ff319aaa5361e8b1fc1e4ddb6305048.pdf"
    },
    "6037fc9874d34869df3d044a99f7bc574113aef9.pdf": {
        "title": "Generalized Translation-Based Embedding of Knowledge Graph",
        "authors": [
            "Takuma Ebisu",
            "R. Ichise"
        ],
        "published_date": "2020",
        "journal": "IEEE Transactions on Knowledge and Data Engineering",
        "abstract": "Knowledge graphs are useful for many AI tasks but often have missing facts. To populate the graphs, knowledge graph embedding models have been developed. TransE is one of such models and the first translation-based method. TransE is well known because the principle of TransE can effectively capture the rules of a knowledge graph although it seems very simple. However, TransE has problems with its regularization and an unchangeable ratio of negative sampling. In this paper, we generalize TransE to solve these problems by proposing knowledge graph embedding on a Lie group (KGLG) and the Weighted Negative Part (WNP) method for the objective function of translation-based models. KGLG is the novel translation-based method which embeds entities and relations of a knowledge graph on any Lie group. It allows us not to employ regularization during training of the model if we choose a compact lie group for the embedding space. The WNP method is for changing the ratio of negative sampling, which enhances translation-based models. Our approach outperforms other state-of-the-art approaches such as TransE, DistMult, and ComplEx on a standard link prediction task. We show that TorusE, KGLG on a torus, is scalable to large-size knowledge graphs and faster than the original TransE.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/6037fc9874d34869df3d044a99f7bc574113aef9.pdf"
    },
    "8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf": {
        "title": "Knowledge graph embedding by logical-default attention graph convolution neural network for link prediction",
        "authors": [
            "Jiarui Zhang",
            "Jian Huang",
            "Jialong Gao",
            "Runhai Han",
            "Cong Zhou"
        ],
        "published_date": "2022",
        "journal": "Information Sciences",
        "abstract": "",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Summary of Main Contribution:**\n\nThis paper addresses the problem of link prediction in knowledge graphs (KGs), which involves inferring missing relationships between entities. It proposes a novel method called \"logical-default attention graph convolution neural network\" (LDAGCN) for generating knowledge graph embeddings (KGEs). The core idea is to leverage graph convolutional networks (GCNs) to aggregate structural information from the KG, enhanced by a specialized \"logical-default attention\" mechanism. This attention mechanism likely aims to dynamically weigh the importance of neighboring entities and relations, potentially incorporating logical rules or default assumptions to improve the quality and interpretability of the learned embeddings. These high-quality embeddings are then used to predict missing links.\n\n**Key Findings (Inferred from Title, as Content is Missing):**\nWhile specific findings cannot be detailed without the paper's content, it is expected that the LDAGCN model would demonstrate superior performance on standard link prediction benchmarks (e.g., FB15k-237, WN18RR) compared to existing state-of-the-art KGE models. The paper would likely highlight improvements in metrics such as Mean Reciprocal Rank (MRR) and Hits@N, attributing these gains to the effective integration of logical-default attention within the GCN framework for learning more expressive and accurate knowledge graph embeddings.\n\n**Key Math Equations (Inferred from Title, as Content is Missing):**\nThe paper would likely present several key mathematical formulations, including:\n1.  **Graph Convolutional Layer:** A variant of the GCN propagation rule, adapted for KGs, which aggregates features from neighboring nodes:\n    $H^{(l+1)} = \\sigma(\\sum_{r \\in \\mathcal{R}} \\tilde{A}_r H^{(l)} W_r^{(l)} + b^{(l)})$\n    where $H^{(l)}$ is the entity embedding matrix at layer $l$, $\\tilde{A}_r$ is an adjacency matrix for relation type $r$, $W_r^{(l)}$ are learnable weight matrices, and $\\sigma$ is an activation function.\n2.  **Logical-Default Attention Mechanism:** The core novelty would be the specific formulation of the attention weights $\\alpha_{ij}$ for aggregating neighbor information. This would likely involve a function that computes attention scores based on entity features, relation types, and potentially incorporates logical predicates or default rules:\n    $\\alpha_{ij} = \\text{Attention}(h_i, h_j, r_{ij}, \\text{logical_context})$\n    These attention weights would then be used to weight the aggregated features in the GCN.\n3.  **Scoring Function for Link Prediction:** A function to score the plausibility of a triple $(h, r, t)$ based on their embeddings:\n    $s(h, r, t) = f(e_h, e_r, e_t)$ (e.g., distance-based, dot product, or a neural network).\n4.  **Loss Function:** A training objective, such as a margin-based ranking loss or negative sampling loss, to optimize the embeddings:\n    $\\mathcal{L} = \\sum_{(h,r,t) \\in \\mathcal{T}} \\sum_{(h',r',t') \\in \\mathcal{T}'} \\max(0, \\gamma + s(h,r,t) - s(h',r',t'))$\n    where $\\mathcal{T}$ are positive triples, $\\mathcal{T}'$ are negative triples, and $\\gamma$ is a margin.\n\n---\n\n**New Direction: Yes**\n\n**Reasoning:**\nWhile the use of Graph Convolutional Networks (GCNs) and attention mechanisms for Knowledge Graph Embedding (KGE) and link prediction is an established research area, the specific term \"logical-default attention\" suggests a novel approach to integrating symbolic or rule-based reasoning within a neural attention mechanism. Standard attention mechanisms primarily learn statistical correlations. The inclusion of \"logical-default\" implies an attempt to:\n1.  **Incorporate explicit logical principles:** Guiding the attention mechanism to prioritize certain paths or relationships based on logical rules (e.g., transitivity, symmetry, inverse relations).\n2.  **Handle missing or uncertain information with defaults:** Providing a structured way for the model to make informed guesses or fallbacks when explicit information is absent, similar to default reasoning in AI.\n\nIf this \"logical-default attention\" mechanism significantly departs from purely data-driven attention by embedding a form of structured reasoning or rule-based inference directly into the attention weights, it represents a new method path. It pushes the boundary of hybrid AI, attempting to combine the strengths of neural networks (representation learning) with symbolic reasoning (logic and defaults) in a novel architectural component for KGE. This could open new avenues for developing more robust, interpretable, and reasoning-capable KGE models.\n\n---\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf"
    },
    "961ce12c6d3aeb57b9146860ace228f0eb91703a.pdf": {
        "title": "LifeGPT: Topology-Agnostic Generative Pretrained Transformer Model for Cellular Automata",
        "authors": [
            "Jaime A. Berkovich",
            "Markus J. Buehler"
        ],
        "published_date": "2024",
        "journal": "npj Artificial Intelligence",
        "abstract": "Conway's Game of Life (Life), a well known algorithm within the broader class of cellular automata (CA), exhibits complex emergent dynamics, with extreme sensitivity to initial conditions. Modeling and predicting such intricate behavior without explicit knowledge of the system's underlying topology presents a significant challenge, motivating the development of algorithms that can generalize across various grid configurations and boundary conditions. We develop a decoder-only generative pretrained transformer (GPT) model to solve this problem, showing that our model can simulate Life on a toroidal grid with no prior knowledge on the size of the grid, or its periodic boundary conditions (LifeGPT). LifeGPT is topology-agnostic with respect to its training data and our results show that a GPT model is capable of capturing the deterministic rules of a Turing-complete system with near-perfect accuracy, given sufficiently diverse training data. We also introduce the idea of an `autoregressive autoregressor' to recursively implement Life using LifeGPT. Our results pave the path towards true universal computation within a large language model framework, synthesizing of mathematical analysis with natural language processing, and probing AI systems for situational awareness about the evolution of such algorithms without ever having to compute them. Similar GPTs could potentially solve inverse problems in multicellular self-assembly by extracting CA-compatible rulesets from real-world biological systems to create new predictive models, which would have significant consequences for the fields of bioinspired materials, tissue engineering, and architected materials design.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/961ce12c6d3aeb57b9146860ace228f0eb91703a.pdf"
    },
    "1de548c37feb944b81a400a80f92a56e98b46424.pdf": {
        "title": "RotateQVS: Representing Temporal Information as Rotations in Quaternion Vector Space for Temporal Knowledge Graph Completion",
        "authors": [
            "Kai Chen",
            "Ye Wang",
            "Yitong Li",
            "Aiping Li"
        ],
        "published_date": "2022",
        "journal": "Annual Meeting of the Association for Computational Linguistics",
        "abstract": "Temporal factors are tied to the growth of facts in realistic applications, such as the progress of diseases and the development of political situation, therefore, research on Temporal Knowledge Graph (TKG) attracks much attention. In TKG, relation patterns inherent with temporality are required to be studied for representation learning and reasoning across temporal facts. However, existing methods can hardly model temporal relation patterns, nor can capture the intrinsic connections between relations when evolving over time, lacking of interpretability. In this paper, we propose a novel temporal modeling method which represents temporal entities as Rotations in Quaternion Vector Space (RotateQVS) and relations as complex vectors in Hamilton\u2019s quaternion space. We demonstrate our method can model key patterns of relations in TKG, such as symmetry, asymmetry, inverse, and can capture time-evolved relations by theory. And empirically, we show that our method can boost the performance of link prediction tasks over four temporal knowledge graph benchmarks.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/1de548c37feb944b81a400a80f92a56e98b46424.pdf"
    },
    "8a678bfe9a8be5978d9ee9e60318d7b378839b06.pdf": {
        "title": "Combination of Translation and Rotation in Dual Quaternion Space for Temporal Knowledge Graph Completion",
        "authors": [
            "Ruiguo Yu",
            "Tao Liu",
            "Jian Yu",
            "Wenbin Zhang",
            "Yue Zhao",
            "Ming Yang",
            "Mankun Zhao",
            "Jiujiang Guo"
        ],
        "published_date": "2023",
        "journal": "IEEE International Joint Conference on Neural Network",
        "abstract": "Compared with static knowledge graphs (KGs) temporal KGs record the dynamic relations between entities over time, therefore, research on temporal Knowledge Graph Completion (KGC) attracts much attention. Temporal KGs exhibit complex temporal relation patterns, such as multiple relations. However, existing methods can hardly model all the relation patterns and apply to the temporal KGs. In this paper, we propose a novel temporal KGC method that Combining Translation and Rotation (ComTR) in Dual Quaternion Space for temporal KGC. Specifically, we use dual-quaternion-based multiplication to model timestamps and relations as the combination of translation and rotation operations. We analyze the relation patterns of temporal KGs in detail and demonstrate that our method can model all the relation patterns in temporal KGs. Empirically, we show that ComTR can achieve the state-of-the-art performances over four temporal KGC benchmarks datasets.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/8a678bfe9a8be5978d9ee9e60318d7b378839b06.pdf"
    },
    "741d039aba804db2e2600fc7be7a1b8e303aec49.pdf": {
        "title": "AtomAgents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence",
        "authors": [
            "Alireza Ghafarollahi",
            "Markus J. Buehler"
        ],
        "published_date": "2024",
        "journal": "arXiv.org",
        "abstract": "The design of alloys is a multi-scale problem that requires a holistic approach that involves retrieving relevant knowledge, applying advanced computational methods, conducting experimental validations, and analyzing the results, a process that is typically reserved for human experts. Machine learning (ML) can help accelerate this process, for instance, through the use of deep surrogate models that connect structural features to material properties, or vice versa. However, existing data-driven models often target specific material objectives, offering limited flexibility to integrate out-of-domain knowledge and cannot adapt to new, unforeseen challenges. Here, we overcome these limitations by leveraging the distinct capabilities of multiple AI agents that collaborate autonomously within a dynamic environment to solve complex materials design tasks. The proposed physics-aware generative AI platform, AtomAgents, synergizes the intelligence of large language models (LLM) the dynamic collaboration among AI agents with expertise in various domains, including knowledge retrieval, multi-modal data integration, physics-based simulations, and comprehensive results analysis across modalities that includes numerical data and images of physical simulation results. The concerted effort of the multi-agent system allows for addressing complex materials design problems, as demonstrated by examples that include autonomously designing metallic alloys with enhanced properties compared to their pure counterparts. Our results enable accurate prediction of key characteristics across alloys and highlight the crucial role of solid solution alloying to steer the development of advanced metallic alloys. Our framework enhances the efficiency of complex multi-objective design tasks and opens new avenues in fields such as biomedical materials engineering, renewable energy, and environmental sustainability.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/741d039aba804db2e2600fc7be7a1b8e303aec49.pdf"
    },
    "d3c2121cb18b13b051a314686c1bcbedf888c7f2.pdf": {
        "title": "PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking",
        "authors": [
            "Markus J. Buehler"
        ],
        "published_date": "2024",
        "journal": "npj Artificial Intelligence",
        "abstract": "PRefLexOR (Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning) combines preference optimization with concepts from Reinforcement Learning to enable models to self-teach through iterative reasoning improvements. We propose a recursive learning approach that engages the model in multi-step reasoning, revisiting, and refining intermediate steps before producing a final output in training and inference phases. Through multiple training stages, the model first learns to align its reasoning with accurate decision paths by optimizing the log odds between preferred and non-preferred responses. During this process, PRefLexOR builds a dynamic knowledge graph by generating questions from random text chunks and retrieval-augmentation to contextualize relevant details from the entire training corpus. In the second stage, preference optimization enhances model performance by using rejection sampling to fine-tune reasoning quality by continually producing in-situ training data while masking the reasoning steps. Recursive optimization within a thinking token framework introduces iterative feedback loops, where the model refines reasoning, achieving deeper coherence, consistency, and adaptability. Implemented in small language models with only 3 billion parameters, we should that even tiny models can iteratively teach themselves to reason with greater depth and reflectivity. Our implementation is straightforward and can be incorporated into any existing pretrained LLM. We focus our examples on applications in biological materials science and demonstrate the method in a variety of case studies that range from in-domain to cross-domain applications. Using reasoning strategies that include thinking and reflection modalities we build a multi-agent recursive self-improving inference approach to successively improve responses via repeated sampling in inference time.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/d3c2121cb18b13b051a314686c1bcbedf888c7f2.pdf"
    },
    "3936283c320b3b3fc948121c2a9fb1e790425a07.pdf": {
        "title": "Meta-Learning Based Knowledge Extrapolation for Temporal Knowledge Graph",
        "authors": [
            "Zhongwu Chen",
            "Chengjin Xu",
            "Fenglong Su",
            "Zhen Huang",
            "Yong Dou"
        ],
        "published_date": "2023",
        "journal": "The Web Conference",
        "abstract": "In the last few years, the solution to Knowledge Graph (KG) completion via learning embeddings of entities and relations has attracted a surge of interest. Temporal KGs(TKGs) extend traditional Knowledge Graphs (KGs) by associating static triples with timestamps forming quadruples. Different from KGs and TKGs in the transductive setting, constantly emerging entities and relations in incomplete TKGs create demand to predict missing facts with unseen components, which is the extrapolation setting. Traditional temporal knowledge graph embedding (TKGE) methods are limited in the extrapolation setting since they are trained within a fixed set of components. In this paper, we propose a Meta-Learning based Temporal Knowledge Graph Extrapolation (MTKGE) model, which is trained on link prediction tasks sampled from the existing TKGs and tested in the emerging TKGs with unseen entities and relations. Specifically, we meta-train a GNN framework that captures relative position patterns and temporal sequence patterns between relations. The learned embeddings of patterns can be transferred to embed unseen components. Experimental results on two different TKG extrapolation datasets show that MTKGE consistently outperforms both the existing state-of-the-art models for knowledge graph extrapolation and specifically adapted KGE and TKGE baselines.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/3936283c320b3b3fc948121c2a9fb1e790425a07.pdf"
    },
    "20c1f84151e2b824e217a2de4396b0a038f1af5f.pdf": {
        "title": "MlpE: Knowledge Graph Embedding with Multilayer Perceptron Networks",
        "authors": [
            "Qing Xu",
            "Kaijun Ren",
            "Xiaoli Ren",
            "Shuibing Long",
            "Xiaoyong Li"
        ],
        "published_date": "2022",
        "journal": "2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta)",
        "abstract": "Knowledge graph embedding (KGE) is an efficient method to predict missing links in knowledge graphs. Most KGE models based on convolutional neural networks have been designed for improving the ability of capturing interaction. Although these models work well, they suffered from the limited receptive field of the convolution kernel, which lead to the lack of ability to capture long-distance interactions. In this paper, we firstly illustrate the interactions between entities and relations and discuss its effect in KGE models by experiments, and then propose MlpE, which is a fully connected network with only three layers. MlpE aims to capture long-distance interactions to improve the performance of link prediction. Extensive experimental evaluations on four typical datasets WN18RR, FB15k-237, DB100k and YAGO3-10 have shown the superority of MlpE, especially in some cases MlpE can achieve the better performance with less parameters than the state-of-the-art convolution-based KGE model.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/20c1f84151e2b824e217a2de4396b0a038f1af5f.pdf"
    },
    "ffd395e779f9f4a626dc0ea3eecffec1c5f57ac0.pdf": {
        "title": "Temporal relevance for representing learning over temporal knowledge graphs",
        "authors": [
            "Bowen Song",
            "K. Amouzouvi",
            "Chengjin Xu",
            "Maocai Wang",
            "Jens Lehmann",
            "S. Vahdati"
        ],
        "published_date": "2024",
        "journal": "Semantic Web",
        "abstract": "Representation learning for link prediction is one of the leading approaches to deal with incompleteness problem of real world knowledge graphs. Such methods are often called knowledge graph embedding models which represent entities and relationships in knowledge graphs in continuous vector spaces. By doing this, semantic relationships and patterns can be captured in the form of compact vectors. In temporal knowledge graphs, the connection of temporal and relational information is crucial for representing facts accurately. Relations provide the semantic context for facts, while timestamps indicate the temporal validity of facts. The importance of time is different for the semantics of different facts. Some relations in some temporal facts are time-insensitive, while others are highly time-dependent. However, existing embedding models often overlook the time sensitivity of different facts in temporal knowledge graphs. These models tend to focus on effectively representing connection between individual components of quadruples, consequently capturing only a fraction of the overall knowledge. Ignoring importance of temporal properties reduces the ability of temporal knowledge graph embedding models in accurately capturing these characteristics. To address these challenges, we propose a novel embedding model based on temporal relevance, which can effectively capture the time sensitivity of semantics and better represent facts. This model operates within a complex space with real and imaginary parts to effectively embed temporal knowledge graphs. Specifically, the real part of the final embedding of our proposed model captures semantic characteristic with temporal sensitivity by learning the relational information and temporal information through transformation and attention mechanism. Simultaneously, the imaginary part of the embeddings learns the connections between different elements in the fact without predefined weights. Our approach is evaluated through extensive experiments on the link prediction task, where it majorly outperforms state-of-the-art models. The proposed model also demonstrates remarkable effectiveness in capturing the complexities of temporal knowledge graphs.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/ffd395e779f9f4a626dc0ea3eecffec1c5f57ac0.pdf"
    },
    "679f709e2736b0970429a2972f0aea48664bdbc3.pdf": {
        "title": "A Survey on Temporal Knowledge Graph: Representation Learning and Applications",
        "authors": [
            "Lianshang Cai",
            "Xin Mao",
            "Yuhao Zhou",
            "Zhaoguang Long",
            "Changxu Wu",
            "Man Lan"
        ],
        "published_date": "2024",
        "journal": "arXiv.org",
        "abstract": "Knowledge graphs have garnered significant research attention and are widely used to enhance downstream applications. However, most current studies mainly focus on static knowledge graphs, whose facts do not change with time, and disregard their dynamic evolution over time. As a result, temporal knowledge graphs have attracted more attention because a large amount of structured knowledge exists only within a specific period. Knowledge graph representation learning aims to learn low-dimensional vector embeddings for entities and relations in a knowledge graph. The representation learning of temporal knowledge graphs incorporates time information into the standard knowledge graph framework and can model the dynamics of entities and relations over time. In this paper, we conduct a comprehensive survey of temporal knowledge graph representation learning and its applications. We begin with an introduction to the definitions, datasets, and evaluation metrics for temporal knowledge graph representation learning. Next, we propose a taxonomy based on the core technologies of temporal knowledge graph representation learning methods, and provide an in-depth analysis of different methods in each category. Finally, we present various downstream applications related to the temporal knowledge graphs. In the end, we conclude the paper and have an outlook on the future research directions in this area.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/679f709e2736b0970429a2972f0aea48664bdbc3.pdf"
    },
    "0732eecfb26c93a839ecbe9314a247d6a89f1fd0.pdf": {
        "title": "Rapid and Automated Alloy Design with Graph Neural Network-Powered LLM-Driven Multi-Agent Systems",
        "authors": [
            "Alireza Ghafarollahi",
            "Markus J. Buehler"
        ],
        "published_date": "2024",
        "journal": "arXiv.org",
        "abstract": "A multi-agent AI model is used to automate the discovery of new metallic alloys, integrating multimodal data and external knowledge including insights from physics via atomistic simulations. Our multi-agent system features three key components: (a) a suite of LLMs responsible for tasks such as reasoning and planning, (b) a group of AI agents with distinct roles and expertise that dynamically collaborate, and (c) a newly developed graph neural network (GNN) model for rapid retrieval of key physical properties. A set of LLM-driven AI agents collaborate to automate the exploration of the vast design space of MPEAs, guided by predictions from the GNN. We focus on the NbMoTa family of body-centered cubic (bcc) alloys, modeled using an ML-based interatomic potential, and target two key properties: the Peierls barrier and solute/screw dislocation interaction energy. Our GNN model accurately predicts these atomic-scale properties, providing a faster alternative to costly brute-force calculations and reducing the computational burden on multi-agent systems for physics retrieval. This AI system revolutionizes materials discovery by reducing reliance on human expertise and overcoming the limitations of direct all-atom simulations. By synergizing the predictive power of GNNs with the dynamic collaboration of LLM-based agents, the system autonomously navigates vast alloy design spaces, identifying trends in atomic-scale material properties and predicting macro-scale mechanical strength, as demonstrated by several computational experiments. This approach accelerates the discovery of advanced alloys and holds promise for broader applications in other complex systems, marking a significant step forward in automated materials design.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/0732eecfb26c93a839ecbe9314a247d6a89f1fd0.pdf"
    },
    "7826c7f186f2f0b33f45b511098d4ffb14f815fd.pdf": {
        "title": "Autonomous Inorganic Materials Discovery via Multi-Agent Physics-Aware Scientific Reasoning",
        "authors": [
            "Alireza Ghafarollahi",
            "Markus J. Buehler"
        ],
        "published_date": "2025",
        "journal": "",
        "abstract": "Conventional machine learning approaches accelerate inorganic materials design via accurate property prediction and targeted material generation, yet they operate as single-shot models limited by the latent knowledge baked into their training data. A central challenge lies in creating an intelligent system capable of autonomously executing the full inorganic materials discovery cycle, from ideation and planning to experimentation and iterative refinement. We introduce SparksMatter, a multi-agent AI model for automated inorganic materials design that addresses user queries by generating ideas, designing and executing experimental workflows, continuously evaluating and refining results, and ultimately proposing candidate materials that meet the target objectives. SparksMatter also critiques and improves its own responses, identifies research gaps and limitations, and suggests rigorous follow-up validation steps, including DFT calculations and experimental synthesis and characterization, embedded in a well-structured final report. The model's performance is evaluated across case studies in thermoelectrics, semiconductors, and perovskite oxides materials design. The results demonstrate the capacity of SparksMatter to generate novel stable inorganic structures that target the user's needs. Benchmarking against frontier models reveals that SparksMatter consistently achieves higher scores in relevance, novelty, and scientific rigor, with a significant improvement in novelty across multiple real-world design tasks as assessed by a blinded evaluator. These results demonstrate SparksMatter's unique capacity to generate chemically valid, physically meaningful, and creative inorganic materials hypotheses beyond existing materials knowledge.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/7826c7f186f2f0b33f45b511098d4ffb14f815fd.pdf"
    },
    "aabc477b93f2b1c1de457d4a3ba4e0a8b196f1f7.pdf": {
        "title": "A Continual Knowledge Graph Embedding Method Based on Local-Global Distillation",
        "authors": [
            "Xiangjun Shi",
            "Chong Mu",
            "Ling Tian",
            "Bin Yan",
            "Weidong Xiao",
            "Jingxuan Wang"
        ],
        "published_date": "2025",
        "journal": "2025 8th International Symposium on Big Data and Applied Statistics (ISBDAS)",
        "abstract": "Knowledge Graph Embedding (KGE) technology, which maps entities and relations into low-dimensional vector spaces, has been widely applied in tasks such as question answering, semantic search, and link prediction. However, traditional KGE models often assume that knowledge graphs are static and cannot easily adapt to the dynamic nature of knowledge graphs that frequently update in real-world. This paper proposes a novel Continual Knowledge Graph Embedding (CKGE) method, which introduces a hierarchical backpropagation mechanism and a local-global distillation module to fully leverage the topological structure features of the graph and effectively mitigate the catastrophic forgetting problem. Through experiments on four benchmark datasets, the performance advantages of the proposed method are verified. Compared to existing methods, this method performs better in terms of dynamic adaptability of knowledge graphs and retention of old knowledge, demonstrating its efficiency and effectiveness in continual learning of knowledge graphs.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/aabc477b93f2b1c1de457d4a3ba4e0a8b196f1f7.pdf"
    },
    "c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf": {
        "title": "Future Event Prediction Based on Temporal Knowledge Graph Embedding",
        "authors": [
            "Zhipeng Li",
            "Shanshan Feng",
            "Jun Shi",
            "Yang Zhou",
            "Yong Liao",
            "Yangzhao Yang",
            "Yangyang Li",
            "Nenghai Yu",
            "Xun Shao"
        ],
        "published_date": "2023",
        "journal": "Computer systems science and engineering",
        "abstract": "Accurate prediction of future events brings great benefits and reduces losses for society in many domains, such as civil unrest, pandemics, and crimes. Knowledge graph is a general language for describing and modeling complex systems. Different types of events continually occur, which are often related to historical and concurrent events. In this paper, we formalize the future event prediction as a temporal knowledge graph reasoning problem. Most existing studies either conduct reasoning on static knowledge graphs or assume knowledges graphs of all timestamps are available during the training process. As a result, they cannot effectively reason over temporal knowledge graphs and predict events happening in the future. To address this problem, some recent works learn to infer future events based on historical event-based temporal knowledge graphs. However, these methods do not comprehensively consider the latent patterns and influences behind historical events and concurrent events simultaneously. This paper proposes a new graph representation learning model, namely Recurrent Event Graph ATtention Network (RE-GAT), based on a novel historical and concurrent events attention-aware mechanism by modeling the event knowledge graph sequence recurrently. More specifically, our RE-GAT uses an attention-based historical events embedding module to encode past events, and employs an attentionbased concurrent events embedding module to model the associations of events at the same timestamp. A translation-based decoder module and a learning objective are developed to optimize the embeddings of entities and relations. We evaluate our proposed method on four benchmark datasets. Extensive experimental results demonstrate the superiority of our RE-GAT model comparing to various baselines, which proves that our method can more accurately predict what events are going to happen.",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Problem Addressed:**\nThe paper addresses the problem of accurately predicting future events (e.g., civil unrest, pandemics, crimes), formalizing it as a temporal knowledge graph (TKG) reasoning problem. It highlights limitations of existing methods, which either reason on static knowledge graphs, assume the availability of all timestamps during training, or fail to comprehensively consider the latent patterns and influences of *both historical and concurrent events simultaneously* when inferring future events from historical event-based TKGs.\n\n**Method Used:**\nThe paper proposes a new graph representation learning model called **Recurrent Event Graph ATtention Network (RE-GAT)**. This model is designed with a novel historical and concurrent events attention-aware mechanism, which recurrently models the sequence of event knowledge graphs. Specifically, RE-GAT comprises:\n1.  An **attention-based historical events embedding module** to encode patterns from past events.\n2.  An **attention-based concurrent events embedding module** to model the associations and influences of events occurring at the same timestamp.\n3.  A **translation-based decoder module** and a learning objective to optimize the embeddings of entities and relations, facilitating future event prediction.\n\n**Key Findings:**\nExtensive experimental results on four benchmark datasets demonstrate the superiority of the proposed RE-GAT model compared to various baseline methods. The findings indicate that RE-GAT can more accurately predict what events are going to happen, proving its effectiveness in temporal knowledge graph reasoning for future event prediction.\n\n**Key Math Equations:**\nThe paper describes the use of \"attention-based historical events embedding module,\" \"attention-based concurrent events embedding module,\" a \"translation-based decoder module,\" and a \"learning objective.\" While specific mathematical equations are not provided in the abstract, these components imply the application of established techniques such as graph attention networks (e.g., GAT), recurrent neural networks (e.g., GRU/LSTM for recurrent modeling), translation-based embedding models (e.g., TransE, RotatE, or their variants) for knowledge graph completion, and a suitable loss function (e.g., margin-based ranking loss) for optimizing the embeddings.\n\n**New Direction: Yes**\nThe paper proposes a new method path by introducing RE-GAT, which features a *novel historical and concurrent events attention-aware mechanism*. While temporal knowledge graph embedding and attention mechanisms are existing fields, the specific combination and the dedicated attention mechanism for simultaneously and comprehensively modeling *both* historical temporal dependencies and concurrent intra-timestamp relationships within a recurrent framework for future event prediction represent a distinct methodological advancement. This addresses a specific gap identified in prior work, thereby offering a new and more effective approach to the problem.\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf"
    },
    "f0e836ef3cb6e74114b195d95ec2b8754598bdbe.pdf": {
        "title": "Combination of Unified Embedding Model and Observed Features for Knowledge Graph Completion",
        "authors": [
            "Takuma Ebisu",
            "R. Ichise"
        ],
        "published_date": "2019",
        "journal": "arXiv.org",
        "abstract": "Knowledge graphs are useful for many artificial intelligence tasks but often have missing data. Hence, a method for completing knowledge graphs is required. Existing approaches include embedding models, the Path Ranking Algorithm, and rule evaluation models. However, these approaches have limitations. For example, all the information is mixed and difficult to interpret in embedding models, and traditional rule evaluation models are basically slow. In this paper, we provide an integrated view of various approaches and combine them to compensate for their limitations. We first unify state-of-the-art embedding models, such as ComplEx and TorusE, reinterpreting them as a variant of translation-based models. Then, we show that these models utilize paths for link prediction and propose a method for evaluating rules based on this idea. Finally, we combine an embedding model and observed feature models to predict missing triples. This is possible because all of these models utilize paths. We also conduct experiments, including link prediction tasks, with standard datasets to evaluate our method and framework. The experiments show that our method can evaluate rules faster than traditional methods and that our framework outperforms state-of-the-art models in terms of link prediction.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/f0e836ef3cb6e74114b195d95ec2b8754598bdbe.pdf"
    },
    "9dadb0136f8965a84c9210aeeab3f1f5fd484acb.pdf": {
        "title": "Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation",
        "authors": [
            "Guiquan Sun",
            "Xikun Zhang",
            "Jingchao Ni",
            "Dongjin Song"
        ],
        "published_date": "2025",
        "journal": "arXiv.org",
        "abstract": "Machine learning on heterogeneous graphs has experienced rapid advancement in recent years, driven by the inherently heterogeneous nature of real-world data. However, existing studies typically assume the graphs to be static, while real-world graphs are continuously expanding. This dynamic nature requires models to adapt to new data while preserving existing knowledge. To this end, this work addresses the challenge of continual learning on heterogeneous graphs by introducing the Meta-learning based Knowledge Distillation framework (MKD), designed to mitigate catastrophic forgetting in evolving heterogeneous graph structures. MKD combines rapid task adaptation through meta-learning on limited samples with knowledge distillation to achieve an optimal balance between incorporating new information and maintaining existing knowledge. To improve the efficiency and effectiveness of sample selection, MKD incorporates a novel sampling strategy that selects a small number of target-type nodes based on node diversity and maintains fixed-size buffers for other types. The strategy retrieves first-order neighbors along metapaths and selects important neighbors based on their structural relevance, enabling the sampled subgraphs to retain key topological and semantic information. In addition, MKD introduces a semantic-level distillation module that aligns the attention distributions over different metapaths between teacher and student models, encouraging semantic consistency beyond the logit level. Comprehensive evaluations across three benchmark datasets validate MKD's effectiveness in handling continual learning scenarios on expanding heterogeneous graphs.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/9dadb0136f8965a84c9210aeeab3f1f5fd484acb.pdf"
    },
    "31c2fab492866e3de3397773161f557f1540b47e.pdf": {
        "title": "Unified Contextualized Knowledge Embedding Method for Static and Temporal Knowledge Graph",
        "authors": [
            "Yifu Gao",
            "Linbo Qiao",
            "Zhen Huang",
            "Zhigang Kan",
            "Yongquan He",
            "Dongsheng Li"
        ],
        "published_date": "2025",
        "journal": "IEEE Transactions on Audio, Speech, and Language Processing",
        "abstract": "Recent years, there is a growing interest in knowledge graph embedding (KGE), which maps symbolic entities and relations into low-dimensional vector space to effectively represent structured data from the knowledge graph. In addition, the concept of temporal knowledge graph is proposed to document dynamically changing facts in the real world. Existing works attempt to incorporate temporal information into static KGE methods to accomplish temporal knowledge representations. However, existing static or temporal KGE approaches focus on the single query fact and ignore the query-relevant contextual information in the graph structure. This paper moves beyond the traditional way of scoring facts in distinct vector space and proposes a unified framework with pre-trained language models (PLM) to learn dynamic contextualized static/temporal knowledge graph embeddings, called CoS/TKGE. Given the query-specific subgraph, our model transforms it into an input sequence and uses the PLM to obtain the contextualized knowledge representations, which is flexible adaptive to the input graph contexts. We reformulate the link prediction task as a mask prediction problem to fine-tune the pre-trained language model. And the contrastive learning technique is employed to align dynamic contextual embeddings with static global embeddings. Experimental results on three widely used static and temporal KG datasets show the superiority of our model.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/31c2fab492866e3de3397773161f557f1540b47e.pdf"
    },
    "9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf": {
        "title": "A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links",
        "authors": [
            "S. M. Asmara",
            "N. A. Sahabudin",
            "Nor Syahidatul Nadiah Ismail",
            "I. A. Sabri"
        ],
        "published_date": "2023",
        "journal": "International Conference on Software Engineering and Computer Systems",
        "abstract": "Knowledge representation and reasoning require knowledge graph embedding as it is crucial in the area. It involves mapping entities and relationships from a knowledge graph into vectors of lower dimensions that are continuous in nature. This encoding enables machine learning algorithms to effectively reason and make predictions on graph-structured data. This review article offers an overview and critical analysis specifically about the methods of knowledge graph embedding which are TransE, TransH, and TransR. The key concepts, methodologies, strengths, and limitations of these methods, along with examining their applications and experiments conducted by existing researchers have been studied. The motivation to conduct this study is to review the well-known and most applied knowledge embedding methods and compare the features of those methods so that a comprehensive resource for researchers and practitioners interested in delving into knowledge graph embedding techniques is delivered.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf"
    },
    "3721a8bc7aed12e42ed446bee791d3117f32c101.pdf": {
        "title": "Graph Pattern Entity Ranking Model for Knowledge Graph Completion",
        "authors": [
            "Takuma Ebisu",
            "R. Ichise"
        ],
        "published_date": "2019",
        "journal": "North American Chapter of the Association for Computational Linguistics",
        "abstract": "Knowledge graphs have been developed rapidly in recent years and shown their usefulness for many artificial intelligence tasks. However, knowledge graphs often have lots of missing facts. To solve this problem, many knowledge graph embedding models to populate knowledge graphs have been developed and have shown outstanding performance these days. However, knowledge graph embedding models are so called-black box. Hence, we actually does not know how information of a knowledge graph is processed and the models are hard to interpret. In this paper, we utilize graph patterns in a knowledge graph to overcome such problems. Our proposed model, graph pattern entity ranking Model (GRank), constructs an entity ranking system for each graph pattern and evaluate them using a measure for a ranking system. By doing so, we can find helpful graph patterns for predicting facts. Then we conduct the link prediction tasks on standard data sets to evaluate GRank. We show our approach outperforms other state-of-the-art approaches such as ComplEx and TorusE on standard metrics such as HITS@n and MRR. Moreover, This model is easily interpretable because output facts are described by graph patterns.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/3721a8bc7aed12e42ed446bee791d3117f32c101.pdf"
    },
    "f55b8375c275a7aab270bc2ad4f089298031621c.pdf": {
        "title": "Knowledge Graph Representation via Similarity-Based Embedding",
        "authors": [
            "Zhen Tan",
            "Xiang Zhao",
            "Yang Fang",
            "Bin Ge",
            "W. Xiao"
        ],
        "published_date": "2018",
        "journal": "Scientific Programming",
        "abstract": "Knowledge graph, a typical multi-relational structure, includes large-scale facts of the world, yet it is still far away from completeness. Knowledge graph embedding, as a representation method, constructs a low-dimensional and continuous space to describe the latent semantic information and predict the missing facts. Among various solutions, almost all embedding models have high time and memory-space complexities and, hence, are difficult to apply to large-scale knowledge graphs. Some other embedding models, such as TransE and DistMult, although with lower complexity, ignore inherent features and only use correlations between different entities to represent the features of each entity. To overcome these shortcomings, we present a novel low-complexity embedding model, namely, SimE-ER, to calculate the similarity of entities in independent and associated spaces. In SimE-ER, each entity (relation) is described as two parts. The entity (relation) features in independent space are represented by the features entity (relation) intrinsically owns and, in associated space, the entity (relation) features are expressed by the entity (relation) features they connect. And the similarity between the embeddings of the same entities in different representation spaces is high. In experiments, we evaluate our model with two typical tasks: entity prediction and relation prediction. Compared with the state-of-the-art models, our experimental results demonstrate that SimE-ER outperforms existing competitors and has low time and memory-space complexities.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/f55b8375c275a7aab270bc2ad4f089298031621c.pdf"
    },
    "c1b22d1383ec17452d6f9da67d427a832f165b1c.pdf": {
        "title": "Unlearning of Knowledge Graph Embedding via Preference Optimization",
        "authors": [
            "Jiajun Liu",
            "Wenjun Ke",
            "Peng Wang",
            "Yao He",
            "Ziyu Shang",
            "Guozheng Li",
            "Zijie Xu",
            "Ke Ji"
        ],
        "published_date": "2025",
        "journal": "arXiv.org",
        "abstract": "Existing knowledge graphs (KGs) inevitably contain outdated or erroneous knowledge that needs to be removed from knowledge graph embedding (KGE) models. To address this challenge, knowledge unlearning can be applied to eliminate specific information while preserving the integrity of the remaining knowledge in KGs. Existing unlearning methods can generally be categorized into exact unlearning and approximate unlearning. However, exact unlearning requires high training costs while approximate unlearning faces two issues when applied to KGs due to the inherent connectivity of triples: (1) It fails to fully remove targeted information, as forgetting triples can still be inferred from remaining ones. (2) It focuses on local data for specific removal, which weakens the remaining knowledge in the forgetting boundary. To address these issues, we propose GraphDPO, a novel approximate unlearning framework based on direct preference optimization (DPO). Firstly, to effectively remove forgetting triples, we reframe unlearning as a preference optimization problem, where the model is trained by DPO to prefer reconstructed alternatives over the original forgetting triples. This formulation penalizes reliance on forgettable knowledge, mitigating incomplete forgetting caused by KG connectivity. Moreover, we introduce an out-boundary sampling strategy to construct preference pairs with minimal semantic overlap, weakening the connection between forgetting and retained knowledge. Secondly, to preserve boundary knowledge, we introduce a boundary recall mechanism that replays and distills relevant information both within and across time steps. We construct eight unlearning datasets across four popular KGs with varying unlearning rates. Experiments show that GraphDPO outperforms state-of-the-art baselines by up to 10.1% in MRR_Avg and 14.0% in MRR_F1.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/c1b22d1383ec17452d6f9da67d427a832f165b1c.pdf"
    },
    "d797e29368ef0e35e2f172ca795ce78d7d1aaf7f.pdf": {
        "title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models",
        "authors": [
            "Sajratul Y. Rubaiat",
            "Hasan M. Jamil"
        ],
        "published_date": "2025",
        "journal": "arXiv.org",
        "abstract": "The exponential growth of scientific literature challenges researchers extracting and synthesizing knowledge. Traditional search engines return many sources without direct, detailed answers, while general-purpose LLMs may offer concise responses that lack depth or omit current information. LLMs with search capabilities are also limited by context window, yielding short, incomplete answers. This paper introduces WISE (Workflow for Intelligent Scientific Knowledge Extraction), a system addressing these limits by using a structured workflow to extract, refine, and rank query-specific knowledge. WISE uses an LLM-powered, tree-based architecture to refine data, focusing on query-aligned, context-aware, and non-redundant information. Dynamic scoring and ranking prioritize unique contributions from each source, and adaptive stopping criteria minimize processing overhead. WISE delivers detailed, organized answers by systematically exploring and synthesizing knowledge from diverse sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces processed text by over 80% while achieving significantly higher recall over baselines like search engines and other LLM-based approaches. ROUGE and BLEU metrics reveal WISE's output is more unique than other systems, and a novel level-based metric shows it provides more in-depth information. We also explore how the WISE workflow can be adapted for diverse domains like drug discovery, material science, and social science, enabling efficient knowledge extraction and synthesis from unstructured scientific papers and web sources.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/d797e29368ef0e35e2f172ca795ce78d7d1aaf7f.pdf"
    },
    "7d1eab892ce6be5f847113e07c8f961d97f9b051.pdf": {
        "title": "Cephalo: Multi\u2010Modal Vision\u2010Language Models for Bio\u2010Inspired Materials Analysis and Design",
        "authors": [
            "Markus J. Buehler"
        ],
        "published_date": "2024",
        "journal": "Advanced Functional Materials",
        "abstract": "Cephalo is presented as a series of multimodal vision large language models (V\u2010LLMs) designed for materials science applications, integrating visual and linguistic data for enhanced understanding. A key innovation of Cephalo is its advanced dataset generation method. Cephalo is trained on integrated image and text data from thousands of scientific papers and science\u2010focused Wikipedia data demonstrates it can interpret complex visual scenes, generate precise language descriptions, and answer queries about images effectively. The combination of a vision encoder with an autoregressive transformer supports multimodal natural language understanding, which can be coupled with other generative methods to create an image\u2010to\u2010text\u2010to\u20103D pipeline. To develop more capable models from smaller ones, both mixture\u2010of\u2010expert methods and model merging are reported. The models are examined in diverse use cases that incorporate biological materials, fracture and engineering analysis, protein biophysics, and bio\u2010inspired design based on insect behavior. Generative applications include bio\u2010inspired designs, including pollen\u2010inspired architected materials, as well as the synthesis of bio\u2010inspired material microstructures from a photograph of a solar eclipse. Additional model fine\u2010tuning with a series of molecular dynamics results demonstrate Cephalo's enhanced capabilities to accurately predict statistical features of stress and atomic energy distributions, as well as crack dynamics and damage in materials.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/7d1eab892ce6be5f847113e07c8f961d97f9b051.pdf"
    },
    "4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf": {
        "title": "Knowledge Graph Embedding via Triplet Component Interactions",
        "authors": [
            "Tao Wang",
            "Bo Shen",
            "Jinglin Zhang",
            "Yu Zhong"
        ],
        "published_date": "2024",
        "journal": "Neural Processing Letters",
        "abstract": "In knowledge graph embedding, multidimensional representations of entities and relations are learned in vector space. Although distance-based graph embedding methods have shown promise in link prediction, they neglect context information among the triplet components, i.e., the head_entity, relation, and tail_entity, limiting their ability to describe multivariate relation patterns and mapping properties. Such context information denotes the entity structural association inside the same triplet and implies the correlation between entities that are not directly connected. In this work, we propose a novel knowledge graph embedding model that explicitly considers context information in graph embedding via triplet component interactions (TCIE). To build connections between components and incorporate contextual information, entities and relations are represented as vectors comprised of two specialized parts, enabling comprehensive interaction. By simultaneously interacting with one-hop related head and tail entities, TCIE strengthens the connections between distant entities and enables contextual information to be transmitted across the knowledge graph. Mathematical proofs and experiments are performed to analyse the modelling ability of TCIE in knowledge graph embedding. TCIE shows a strong capacity for modelling four relation patterns (i.e., symmetry, antisymmetry, inverse, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many). The experimental evaluation of ogbl-wikikg2, ogbl-biokg, FB15k, and FB15k-237 shows that TCIE achieves state-of-the-art results in link prediction.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf"
    },
    "71b97553fcd9367b5feb1b317af140625a75b8f7.pdf": {
        "title": "CrsKGC-RSAN: Conflict Resolution Strategy for Knowledge Graph Completion",
        "authors": [
            "Jie Chen",
            "Xin Zhang",
            "Wuyang Zhang",
            "Siyu Tan",
            "Shu Zhao"
        ],
        "published_date": "2023",
        "journal": "2023 IEEE International Conference on Data Mining Workshops (ICDMW)",
        "abstract": "Knowledge Graph Completion (KGC) is a crucial task aimed at enhancing the completeness and accuracy of knowledge graphs (KGs) by filling in missing information. However, existing knowledge graphs face two significant challenges: 1) the negative sampling issues, where false negative samples contain potential true facts, while easy negative samples limit the generalization capacity of KGC models; and 2) the entity conflict issues, many candidate entities exhibit similarities in their explicit structural features, making it difficult for models to make accurate decisions. To address the above challenges, we propose a novel method, CrsKGC-RSAN. For the negative sampling issues, we propose a Relative Self Adversarial Negative Sampling (RSAN) module. RSAN allows for a smaller margin between positive and negative samples compared to traditional self-adversarial sampling, while assigning higher training weights to hard negative samples to address false negative and esay negative sample issues. For the entity conflict issues, we present CrsKGC module, which, in addition to learning explicit structural features of KGs, mines implicit structural features, enhancing the comprehensiveness of feature learning and effectively resolving entity conflicts. Extensive experiments on the FB15K-237 and WN18RR datasets demonstrate that our method achieves state-of-the-art results and exhibits compatibility.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/71b97553fcd9367b5feb1b317af140625a75b8f7.pdf"
    },
    "16fb5cf5666ad4727f5132e24846d58b5285ab1a.pdf": {
        "title": "Knowledge Graph Embedding via Adaptive Negative Subsampling",
        "authors": [
            "Dong Zhu",
            "Haonan Tan",
            "Le Wang",
            "YuJie Feng",
            "Yao Lin",
            "Zhaoquan Gu"
        ],
        "published_date": "2023",
        "journal": "International Conference on Data Science in Cyberspace",
        "abstract": "Knowledge graph embedding is an important approach for addressing the task of knowledge graph completion, which has received extensive attention and research in recent years. However, some existing methods, such as non-Euclidean space embedding models, suffer from high computational complexity and time costs, as well as high embedding dimensions, without significant improvements over classical models. In this study, we apply adaptive negative sampling to the classical knowledge graph embedding model transE. We have performed optimization on the loss function while preserving the fundamental principles of embedding. As a result, our model is referred to as ANStransE. We also experimentally investigate the impact of different dimensions and evaluate our approach on three commonly used datasets (WN18RR, FB15k-237, and YAGO3-10). The experimental results demonstrate a significant performance improvement achieved by our method, enabling a simple model to achieve results close to the state-of-the-art. Through adaptive negative sampling, we are able to effectively utilize negative samples for model training, thereby enhancing the accuracy and efficiency of knowledge graph representation learning.11The relevant code has been made publicly available at https://github.com/xiaoluoElon/ANStransE.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/16fb5cf5666ad4727f5132e24846d58b5285ab1a.pdf"
    },
    "354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf": {
        "title": "Analogical Inference Enhanced Knowledge Graph Embedding",
        "authors": [
            "Zhen Yao",
            "Wen Zhang",
            "Mingyang Chen",
            "Yufen Huang",
            "Yezhou Yang",
            "Hua-zeng Chen"
        ],
        "published_date": "2023",
        "journal": "AAAI Conference on Artificial Intelligence",
        "abstract": "Knowledge graph embedding (KGE), which maps entities and relations in a knowledge graph into continuous vector spaces, has achieved great success in predicting missing links in knowledge graphs. However, knowledge graphs often contain incomplete triples that are difficult to inductively infer by KGEs. To address this challenge, we resort to analogical inference and propose a novel and general self-supervised framework AnKGE to enhance KGE models with analogical inference capability. We propose an analogical object retriever that retrieves appropriate analogical objects from entity-level, relation-level, and triple-level. And in AnKGE, we train an analogy function for each level of analogical inference with the original element embedding from a well-trained KGE model as input, which outputs the analogical object embedding. In order to combine inductive inference capability from the original KGE model and analogical inference capability enhanced by AnKGE, we interpolate the analogy score with the base model score and introduce the adaptive weights in the score function for prediction. Through extensive experiments on FB15k-237 and WN18RR datasets, we show that AnKGE achieves competitive results on link prediction task and well performs analogical inference.",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "This paper addresses the challenge that Knowledge Graph Embedding (KGE) models, despite their success in link prediction, often struggle with inductively inferring incomplete triples within knowledge graphs.\n\nTo solve this, the paper proposes **AnKGE**, a novel and general self-supervised framework that enhances KGE models with analogical inference capabilities. The method involves:\n1.  An **analogical object retriever** that identifies appropriate analogical objects at entity-level, relation-level, and triple-level.\n2.  Training an **analogy function** for each level of analogical inference. These functions take original element embeddings from a pre-trained KGE model as input and output analogical object embeddings.\n3.  Combining the inductive inference capability of the original KGE model with the enhanced analogical inference. This is achieved by **interpolating the analogy score with the base model score** using adaptive weights in the final score function for prediction.\n\nKey findings indicate that AnKGE achieves competitive results on the link prediction task and effectively performs analogical inference, as demonstrated through extensive experiments on the FB15k-237 and WN18RR datasets.\n\n**Key Math Equations:**\nWhile no explicit equations are provided in the abstract, the core mathematical contribution lies in the score function for prediction, which combines the base KGE model's score with the analogy score using adaptive weights. Conceptually, this can be represented as:\n$S_{final}(h, r, t) = w \\cdot S_{analogy}(h, r, t) + (1-w) \\cdot S_{base}(h, r, t)$\nwhere $S_{final}$ is the final prediction score, $S_{analogy}$ is the score derived from analogical inference, $S_{base}$ is the score from the original KGE model, and $w$ is an adaptively learned weight (or set of weights) that balances the contribution of the two inference mechanisms.\n\nNew Direction: Yes\nThis paper proposes a new method path by introducing a novel self-supervised framework (AnKGE) that integrates analogical inference into KGEs. While KGEs and analogical reasoning are existing concepts, their specific combination in a multi-level retrieval and adaptive scoring framework for addressing the inductive inference limitation of KGEs is presented as a novel approach. It offers a distinct way to enhance the reasoning capabilities of KGE models beyond their typical inductive inference.\n\nType: Methodology",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf"
    },
    "ead2db9e4290b9cfb1185e859af8d34bc32f300e.pdf": {
        "title": "Beyond Benchmarks: Assessing Knowledge Graph Completion Methods on Non-Benchmark Employee Data",
        "authors": [
            "Muhammad Habiburahman",
            "K. Wiharja",
            "Muhammad Fikriansyah"
        ],
        "published_date": "2024",
        "journal": "2024 International Conference on Data Science and Its Applications (ICoDSA)",
        "abstract": "This research delves into the application of Knowledge Graph Completion (KGC) methods-Translating Embeddings (TransE), Complex Embeddings (ComplEx), and Knowledge Graph Embedding by Translating on Hyperplanes (TransH)-within the unique context of Human Resources (HR) knowledge graphs. The selected methods were chosen due to their classical and commonly employed nature in prior research. The study follows a systematic research flow encompassing data extraction, preprocessing, RDF mapping, and model training, aiming to comprehensively evaluate these models on a non-benchmark employee dataset from Telkomsel. The findings reveal distinct strengths and weaknesses of each model. TransE displays consistent accuracy in placing entities correctly, while ComplEx showcases notable adaptability to diverse HR relationships, contributing to its efficacy in capturing intricate connections, and TransH provides nuanced insights with a unique semantic relationship perspective, offering semantic depth in HR domains. Our research acknowledges that the current performance metrics (MR, MRR, and Hits@K) of TransE, ComplEx, and TransH in adapting to HR knowledge graphs are below those reported in previous studies using benchmark datasets (FB15K and WN18RR), emphasizing the need for transparent acknowledgment and paving the way for future improvements to achieve higher accuracy. In conclusion, this research pioneers evaluating KGC methods on non-benchmark HR datasets, providing practical insights for HR information system management. It lays a foundation for applying Knowledge Graph Completion methods, bridging the gap between theoretical knowledge and practical applications in HR knowledge graphs.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/ead2db9e4290b9cfb1185e859af8d34bc32f300e.pdf"
    },
    "34950447fcc855f4ee042c8a88842cf334c393bf.pdf": {
        "title": "NSE-KGC: A knowledge graph completion method based on neighborhood semantic evidence",
        "authors": [
            "Yana Lv",
            "Haomiao Bao"
        ],
        "published_date": "2024",
        "journal": "2024 5th International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)",
        "abstract": "The objective of knowledge graph completion (KGC) is to enhance our understanding of known facts and infer missing relationships within the graph. Previous text-based methods have advanced the field by incorporating effective contrastive learning techniques and improving learning efficiency through the construction of three distinct types of negative samples. Despite these advancements, these methods do not leverage the structural information inherent in the knowledge graph. In this paper, we introduce NSE-KGC, a novel model for knowledge graph completion that capitalizes on random walk neighbor semantic evidence aggregation. NSE-KGC explicitly models neighbor patterns identified through random walks at each layer and integrates these patterns through multi-layer aggregation. Extensive experiments conducted on two benchmark datasets, FB15k-237 and WN18RR, demonstrate that NSE-KGC achieves state-of-the-art performance in knowledge graph completion and offers superior entity extrapolation capabilities.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/34950447fcc855f4ee042c8a88842cf334c393bf.pdf"
    },
    "fe2034614d8327b867c08c8b047946d96f102e23.pdf": {
        "title": "N-MlpE: Optimizing Multilayer Perceptron Network-based Knowledge Graph Embedding Model with Neighborhood Information",
        "authors": [
            "Qing Xu",
            "Xiaoli Ren",
            "Kaijun Ren",
            "Jiarun Lin",
            "Xiaoyong Li"
        ],
        "published_date": "2023",
        "journal": "International Conference on Parallel and Distributed Systems",
        "abstract": "As an effective knowledge organizing and modeling technique, knowledge graph has become a key topic in graph research, but the practical application of KG is limited by its incompleteness. In recent years, many knowledge graph embedding(KGE) methods for knowledge graph completion(KGC) based on graph neural networks(GNN) have been proposed. However, most GNN-based KGC models are still suffer from the encoder-decoder structure of low efficiency in aggregating neighborhood information and the difficulty of model training. This paper present an optimized model that incorporates Neighborhood information into knowledge inference, to improve the performance of KGC models based on multilayer perceptron network(MLP), which is named N-MlpE. We generate an input sequence that includes the query triplet and its neighbor entities and relationships, and then feed it to an adaptive filter module to remove useless neighbors for the inference to improve the accuracy of the inference, and reduce the computational complexity of training the model. The filtered sequence is then fed into a weight calculation module and a feature extraction module simultaneously, the former is designed based on selfattention to model the relevant rule inference, which enhances the interpretability of KGE models, and the latter is based on MLP and used to capture the long-distance interactions between triplets, which can significantly improve the accuracy of inference. Extensive experiments are conducted on two standard KG datasets WN18RR and FB15k237 to verify the effectiveness of N-MlpE, the results show that the accuracy of N-MlpE model outperforms most GNN-based models.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/fe2034614d8327b867c08c8b047946d96f102e23.pdf"
    },
    "777e1fc47501b40b275c8896573400eb733c08e9.pdf": {
        "title": "Double-Branch Multi-Attention based Graph Neural Network for Knowledge Graph Completion",
        "authors": [
            "Hongcai Xu",
            "J. Bao",
            "Wenbo Liu"
        ],
        "published_date": "2023",
        "journal": "Annual Meeting of the Association for Computational Linguistics",
        "abstract": "Graph neural networks (GNNs), which effectively use topological structures in the knowledge graphs (KG) to embed entities and relations in low-dimensional spaces, have shown great power in knowledge graph completion (KGC). KG has abundant global and local structural information, however, many GNN-based KGC models cannot capture these two types of information about the graph structure by designing complex aggregation schemes, and are not designed well to learn representations of seen entities with sparse neighborhoods in isolated subgraphs. In this paper, we find that a simple attention-based method can outperform a general GNN-based approach for KGC. We then propose a double-branch multi-attention based graph neural network (MA-GNN) to learn more expressive entity representations which contain rich global-local structural information. Specifically, we first explore the graph attention network-based local aggregator to learn entity representations. Furthermore, we propose a snowball local attention mechanism by leveraging the semantic similarity between two-hop neighbors to enrich the entity embedding. Finally, we use Transformer-based self-attention to learn long-range dependence between entities to obtain richer representations with the global graph structure and entity features. Experimental results on five benchmark datasets show that MA-GNN achieves significant improvements over strong baselines for inductive KGC.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/777e1fc47501b40b275c8896573400eb733c08e9.pdf"
    },
    "ebc63750fbd2f96223edb9e01f289a6701290980.pdf": {
        "title": "A Generative Adaptive Replay Continual Learning Model for Temporal Knowledge Graph Reasoning",
        "authors": [
            "Zhiyu Zhang",
            "Wei Chen",
            "Youfang Lin",
            "Huaiyu Wan"
        ],
        "published_date": "2025",
        "journal": "Annual Meeting of the Association for Computational Linguistics",
        "abstract": "Recent Continual Learning (CL)-based Temporal Knowledge Graph Reasoning (TKGR) methods focus on significantly reducing computational cost and mitigating catastrophic forgetting caused by fine-tuning models with new data. However, existing CL-based TKGR methods still face two key limitations: (1) They usually one-sidedly reorganize individual historical facts, while overlooking the historical context essential for accurately understanding the historical semantics of these facts; (2) They preserve historical knowledge by simply replaying historical facts, while ignoring the potential conflicts between historical and emerging facts. In this paper, we propose a Deep Generative Adaptive Replay (DGAR) method, which can generate and adaptively replay historical entity distribution representations from the whole historical context. To address the first challenge, historical context prompts as sampling units are built to preserve the whole historical context information. To overcome the second challenge, a pre-trained diffusion model is adopted to generate the historical distribution. During the generation process, the common features between the historical and current distributions are enhanced under the guidance of the TKGR model. In addition, a layer-by-layer adaptive replay mechanism is designed to effectively integrate historical and current distributions. Experimental results demonstrate that DGAR significantly outperforms baselines in reasoning and mitigating forgetting.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/ebc63750fbd2f96223edb9e01f289a6701290980.pdf"
    },
    "75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf": {
        "title": "TransE-MTP: A New Representation Learning Method for Knowledge Graph Embedding with Multi-Translation Principles and TransE",
        "authors": [
            "Yongfang Li",
            "Chunhua Zhu"
        ],
        "published_date": "2024",
        "journal": "Electronics",
        "abstract": "The purpose of representation learning is to encode the entities and relations in a knowledge graph as low-dimensional and real-valued vectors through machine learning technology. Traditional representation learning methods like TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of a graph\u2019s entities, are effective for learning the embeddings of knowledge bases, but struggle to effectively model complex relations like one-to-many, many-to-one, and many-to-many. To overcome the above issues, we introduce a new method for knowledge representation, reasoning, and completion based on multi-translation principles and TransE (TransE-MTP). By defining multiple translation principles (MTPs) for different relation types, such as one-to-one and complex relations like one-to-many, many-to-one, and many-to-many, and combining MTPs with a typical translating-based model for modeling multi-relational data (TransE), the proposed method, TransE-MTP, ensures that multiple optimization objectives can be targeted and optimized during training on complex relations, thereby providing superior prediction performance. We implement a prototype of TransE-MTP to demonstrate its effectiveness at link prediction and triplet classification on two prominent knowledge graph datasets: Freebase and Wordnet. Our experimental results show that the proposed method enhanced the performance of both TransE and knowledge graph embedding by translating on hyperplanes (TransH), which confirms its effectiveness and competitiveness.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf"
    },
    "4e52607397a96fb2104a99c570c9cec29c9ca519.pdf": {
        "title": "ChronoR: Rotation Based Temporal Knowledge Graph Embedding",
        "authors": [
            "A. Sadeghian",
            "Mohammadreza Armandpour",
            "Anthony Colas",
            "D. Wang"
        ],
        "published_date": "2021",
        "journal": "AAAI Conference on Artificial Intelligence",
        "abstract": "Despite the importance and abundance of temporal knowledge graphs, most of the current research has been focused on reasoning on static graphs. In this paper, we study the challenging problem of inference over temporal knowledge graphs. In particular, the task of temporal link prediction. In general, this is a difficult task due to data non-stationarity, data heterogeneity, and its complex temporal dependencies. \nWe propose Chronological Rotation embedding (ChronoR), a novel model for learning representations for entities, relations, and time. Learning dense representations is frequently used as an efficient and versatile method to perform reasoning on knowledge graphs. The proposed model learns a k-dimensional rotation transformation parametrized by relation and time, such that after each fact's head entity is transformed using the rotation, it falls near its corresponding tail entity. By using high dimensional rotation as its transformation operator, ChronoR captures rich interaction between the temporal and multi-relational characteristics of a Temporal Knowledge Graph. Experimentally, we show that ChronoR is able to outperform many of the state-of-the-art methods on the benchmark datasets for temporal knowledge graph link prediction.",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Summary of Main Contribution:**\n\nThis paper addresses the challenging problem of inference over temporal knowledge graphs (TKGs), specifically focusing on the task of temporal link prediction. This task is complicated by data non-stationarity, heterogeneity, and complex temporal dependencies inherent in TKGs, which most existing research on static graphs fails to capture.\n\nThe paper proposes **ChronoR (Chronological Rotation embedding)**, a novel model for learning dense representations for entities, relations, and time. The core method involves learning a *k-dimensional rotation transformation* that is uniquely parametrized by both the relation and the time of a fact. The objective is that after a head entity's embedding is transformed by this specific rotation, it should fall near its corresponding tail entity's embedding. By utilizing high-dimensional rotation as its transformation operator, ChronoR is designed to effectively capture the intricate interactions between the temporal and multi-relational characteristics present in TKGs.\n\n**Key Findings:**\nExperimentally, ChronoR demonstrates superior performance, outperforming many state-of-the-art methods on benchmark datasets for temporal knowledge graph link prediction.\n\n**Key Math Equations:**\nThe provided content describes the conceptual basis of the method\u2014a \"k-dimensional rotation transformation parametrized by relation and time\" that maps a transformed head entity near its tail entity. However, specific mathematical equations for this rotation (e.g., the exact matrix formulation, the parametrization function, or the loss function used for training) are not explicitly provided in this excerpt.\n\n---\n\n**New Direction: Yes**\n**Reasoning:** This paper proposes a \"novel model\" called ChronoR, which introduces a new \"k-dimensional rotation transformation parametrized by relation and time\" as its core mechanism for learning temporal knowledge graph embeddings. While the overarching problem of temporal link prediction is an existing research area, the specific *method* of using relation- and time-parametrized high-dimensional rotations for this task is presented as novel. This constitutes a new methodological path within the field of temporal knowledge graph embedding.\n\n---\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/4e52607397a96fb2104a99c570c9cec29c9ca519.pdf"
    },
    "73b6e71b3c647ac2d30f4f856b5dd2a81ffa3cb8.pdf": {
        "title": "TransFTTA: A Knowledge Representation Learning Model Based on Flexible Translation and Temporal Attributes",
        "authors": [
            "Boyu Wan",
            "Yingtao Niu",
            "Changxing Chen"
        ],
        "published_date": "2023",
        "journal": "2023 4th International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",
        "abstract": "Knowledge graph is a structured semantic knowledge base that has gained widespread application across diverse domains. Nevertheless, traditional models for knowledge graph representation learning, such as TransE and TransH, exhibit constraints in terms of representation capacity and accuracy when it comes to tasks like link prediction and triple classification. To overcome these issues and further enhance the performance of knowledge graph representation models, this study introduces a knowledge representation learning model based on dynamic translation and temporal attributes. The experimental findings substantiate that our proposed model exhibits a significant improvement in performance compared to the TransE and TransH models.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/73b6e71b3c647ac2d30f4f856b5dd2a81ffa3cb8.pdf"
    },
    "eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf": {
        "title": "Fast and Continual Knowledge Graph Embedding via Incremental LoRA",
        "authors": [
            "Jiajun Liu",
            "Wenjun Ke",
            "Peng Wang",
            "Jiahao Wang",
            "Jinhua Gao",
            "Ziyu Shang",
            "Guozheng Li",
            "Zijie Xu",
            "Ke Ji",
            "Yining Li"
        ],
        "published_date": "2024",
        "journal": "International Joint Conference on Artificial Intelligence",
        "abstract": "Continual Knowledge Graph Embedding (CKGE) aims to efficiently learn new knowledge and simultaneously preserve old knowledge. Dominant approaches primarily focus on alleviating catastrophic forgetting of old knowledge but neglect efficient learning for the emergence of new knowledge. However, in real-world scenarios, knowledge graphs (KGs) are continuously growing, which brings a significant challenge to fine-tuning KGE models efficiently. To address this issue, we propose a fast CKGE framework (FastKGE), incorporating an incremental low-rank adapter (IncLoRA) mechanism to efficiently acquire new knowledge while preserving old knowledge. Specifically, to mitigate catastrophic forgetting, FastKGE isolates and allocates new knowledge to specific layers based on the fine-grained influence between old and new KGs. Subsequently, to accelerate fine-tuning, FastKGE devises an efficient IncLoRA mechanism, which embeds the specific layers into incremental low-rank adapters with fewer training parameters. Moreover, IncLoRA introduces adaptive rank allocation, which makes the LoRA aware of the importance of entities and adjusts its rank scale adaptively. We conduct experiments on four public datasets and two new datasets with a larger initial scale. Experimental results demonstrate that FastKGE can reduce training time by 34%-49% while still achieving competitive link prediction performance against state-of-the-art models on four public datasets (average MRR score of 21.0% vs. 21.1%). Meanwhile, on two newly constructed datasets, FastKGE saves 51%-68% training time and improves link prediction performance by 1.5%.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf"
    },
    "f42d060fb530a11daecd90695211c01a5c264f8d.pdf": {
        "title": "Towards Continual Knowledge Graph Embedding via Incremental Distillation",
        "authors": [
            "Jiajun Liu",
            "Wenjun Ke",
            "Peng Wang",
            "Ziyu Shang",
            "Jinhua Gao",
            "Guozheng Li",
            "Ke Ji",
            "Yanhe Liu"
        ],
        "published_date": "2024",
        "journal": "AAAI Conference on Artificial Intelligence",
        "abstract": "Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges. To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge. However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods. On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs. On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively. In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs. First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features. Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation. Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge. Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines. Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score. More exploratory experiments validate the effectiveness of IncDE in proficiently learning new knowledge while preserving old knowledge across all time steps.",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Summary of Main Contribution:**\n\nThis paper addresses the challenge of Continual Knowledge Graph Embedding (CKGE), where the goal is to efficiently learn new knowledge in a knowledge graph (KG) while effectively preserving previously learned knowledge, without the high cost of retraining on the entire graph. The authors identify a critical gap in existing CKGE methods: their failure to fully leverage the explicit graph structure of KGs. Specifically, current methods often learn new triples in a random order, disrupting the inner structure, and preserve old triples with equal priority, which is ineffective against catastrophic forgetting.\n\nTo solve this, the paper proposes **Incremental Distillation (IncDE)**, a competitive method that explicitly incorporates the KG's graph structure. IncDE comprises three key components:\n1.  **Hierarchical Strategy:** To optimize the learning order, new triples are ranked and grouped into layers for layer-by-layer learning, utilizing both inter- and intra-hierarchical orders based on graph structure features.\n2.  **Novel Incremental Distillation Mechanism:** This mechanism facilitates the seamless transfer of entity representations from the previous learning layer to the next, significantly promoting the preservation of old knowledge.\n3.  **Two-stage Training Paradigm:** This paradigm is adopted to prevent the over-corruption of old knowledge that might occur due to under-trained new knowledge.\n\n**Key Findings:**\nExperimental results demonstrate that IncDE outperforms state-of-the-art baselines. Notably, the incremental distillation mechanism alone contributes to significant improvements, ranging from 0.2% to 6.5% in the Mean Reciprocal Rank (MRR) score. Further experiments validate IncDE's effectiveness in proficiently learning new knowledge while consistently preserving old knowledge across all time steps.\n\n**Key Math Equations:**\nThe provided content does not include specific mathematical equations. The paper describes algorithmic strategies and mechanisms.\n\n---\n\n**New Direction: Yes**\n\n**Reasoning:** While the overarching task of Continual Knowledge Graph Embedding (CKGE) is an established research area, this paper proposes a novel methodological path by explicitly addressing and leveraging the *explicit graph structure* of KGs, which it argues has been \"heavily ignored by existing CKGE methods.\" The introduction of a hierarchical strategy for learning order based on graph structure and a novel incremental distillation mechanism tailored to this structural learning represents a significant departure from prior approaches. It's not just an incremental improvement on an existing technique but a new paradigm for how to approach catastrophic forgetting in KGE by integrating structural awareness into the learning and preservation process.\n\n---\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/f42d060fb530a11daecd90695211c01a5c264f8d.pdf"
    },
    "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf": {
        "title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View",
        "authors": [
            "Ren Li",
            "Yanan Cao",
            "Qiannan Zhu",
            "Guanqun Bi",
            "Fang Fang",
            "Yi Liu",
            "Qian Li"
        ],
        "published_date": "2021",
        "journal": "AAAI Conference on Artificial Intelligence",
        "abstract": "Knowledge Graph Embedding (KGE) aims to learn representations for entities and relations. Most KGE models have gained great success, especially on extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a trained model can still correctly predict t from (h, r, ?), or h from (?, r, t), such extrapolation ability is impressive. However, most existing KGE works focus on the design of delicate triple modeling function, which mainly tells us how to measure the plausibility of observed triples, but offers limited explanation of why the methods can extrapolate to unseen data, and what are the important factors to help KGE extrapolate. Therefore in this work, we attempt to study the KGE extrapolation of two problems: 1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with better extrapolation ability? \nFor the problem 1, we first discuss the impact factors for extrapolation and from relation, entity and triple level respectively, propose three Semantic Evidences (SEs), which can be observed from train set and provide important semantic information for extrapolation. Then we verify the effectiveness of SEs through extensive experiments on several typical KGE methods.\nFor the problem 2, to make better use of the three levels of SE, we propose a novel GNN-based KGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In SE-GNN, each level of SE is modeled explicitly by the corresponding neighbor pattern, and merged sufficiently by the multi-layer aggregation, which contributes to obtaining more extrapolative knowledge representation. \nFinally, through extensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task and performs a better extrapolation ability. Our code is available at https://github.com/renli1024/SE-GNN.",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Main Contribution:**\nThis paper addresses the critical gap in understanding *why* Knowledge Graph Embedding (KGE) models extrapolate to unseen data, rather than just *how* they measure triple plausibility. It tackles two main problems: 1) How KGE extrapolates to unseen data, and 2) How to design KGE models with better extrapolation ability.\n\n**Problem Addressed:**\nExisting KGE models demonstrate impressive extrapolation capabilities (predicting missing entities in unseen triples), but the underlying mechanisms and contributing factors for this ability are not well understood. Most research focuses on designing sophisticated triple modeling functions, leaving the \"why\" of extrapolation largely unexplained.\n\n**Method Used to Solve It:**\n1.  **For understanding extrapolation:** The paper proposes three \"Semantic Evidences (SEs)\" at the relation, entity, and triple levels. These SEs are observable patterns within the training data that provide crucial semantic information for extrapolation. The effectiveness of these SEs is then verified through experiments on various typical KGE methods.\n2.  **For designing better extrapolating models:** A novel Graph Neural Network (GNN)-based KGE model, named Semantic Evidence aware Graph Neural Network (SE-GNN), is introduced. SE-GNN explicitly models each level of SE using corresponding neighbor patterns and integrates them through multi-layer aggregation to learn more extrapolative knowledge representations.\n\n**Key Findings:**\n*   The proposed Semantic Evidences (SEs) are identified as important factors contributing to KGE's extrapolation ability.\n*   The SE-GNN model achieves state-of-the-art performance on Knowledge Graph Completion tasks on datasets like FB15k-237 and WN18RR.\n*   SE-GNN demonstrates superior extrapolation ability compared to existing methods.\n\n**Key Math Equations:**\nThe provided content does not include specific mathematical equations. The methods are described conceptually and architecturally (e.g., GNN-based modeling, multi-layer aggregation).\n\n**New Direction: Yes**\n**Reasoning:** This paper proposes a new research direction by shifting the focus from merely achieving high performance in KGE to *understanding the underlying mechanisms of extrapolation*. By introducing the concept of \"Semantic Evidences\" and then designing a model (SE-GNN) *specifically* to leverage these evidences, it offers a novel methodological path for KGE model design. It moves beyond just improving scoring functions to explicitly incorporating factors that explain and enhance extrapolation, thereby opening up new avenues for research into interpretable and robust KGE.\n\n**Type: Methodology**",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf"
    },
    "1a7372170ed6dca35b76889238ad366e23f1faa6.pdf": {
        "title": "Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion",
        "authors": [
            "Rui Ying",
            "Mengting Hu",
            "Jianfeng Wu",
            "Yalan Xie",
            "Xiaoyi Liu",
            "Zhunheng Wang",
            "Ming Jiang",
            "Hang Gao",
            "Linlin Zhang",
            "Renhong Cheng"
        ],
        "published_date": "2024",
        "journal": "Annual Meeting of the Association for Computational Linguistics",
        "abstract": "Temporal knowledge graph completion aims to infer the missing facts in temporal knowledge graphs. Current approaches usually embed factual knowledge into continuous vector space and apply geometric operations to learn potential patterns in temporal knowledge graphs. However, these methods only adopt a single operation, which may have limitations in capturing the complex temporal dynamics present in temporal knowledge graphs. Therefore, we propose a simple but effective method, i.e. TCompoundE, which is specially designed with two geometric operations, including time-specific and relation-specific operations. We provide mathematical proofs to demonstrate the ability of TCompoundE to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing temporal knowledge graph embedding models. Our code is available at https://github.com/nk-ruiying/TCompoundE.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/1a7372170ed6dca35b76889238ad366e23f1faa6.pdf"
    },
    "f0e29bcea02510d992bd4b2ac4f2b63a8ea37705.pdf": {
        "title": "Time-dependent Entity Embedding is not All You Need: A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework",
        "authors": [
            "Zhen Han",
            "Gengyuan Zhang",
            "Yunpu Ma",
            "Volker Tresp"
        ],
        "published_date": "2021",
        "journal": "Conference on Empirical Methods in Natural Language Processing",
        "abstract": "Various temporal knowledge graph (KG) completion models have been proposed in the recent literature. The models usually contain two parts, a temporal embedding layer and a score function derived from existing static KG modeling approaches. Since the approaches differ along several dimensions, including different score functions and training strategies, the individual contributions of different temporal embedding techniques to model performance are not always clear. In this work, we systematically study six temporal embedding approaches and empirically quantify their performance across a wide range of configurations with about 3000 experiments and 13159 GPU hours. We classify the temporal embeddings into two classes: (1) timestamp embeddings and (2) time-dependent entity embeddings. Despite the common belief that the latter is more expressive, an extensive experimental study shows that timestamp embeddings can achieve on-par or even better performance with significantly fewer parameters. Moreover, we find that when trained appropriately, the relative performance differences between various temporal embeddings often shrink and sometimes even reverse when compared to prior results. For example, TTransE (CITATION), one of the first temporal KG models, can outperform more recent architectures on ICEWS datasets. To foster further research, we provide the first unified open-source framework for temporal KG completion models with full composability, where temporal embeddings, score functions, loss functions, regularizers, and the explicit modeling of reciprocal relations can be combined arbitrarily.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/f0e29bcea02510d992bd4b2ac4f2b63a8ea37705.pdf"
    },
    "5a66e25c2e2eddac28dc648c8cba1a626338aa37.pdf": {
        "title": "ROTAN: A Rotation-based Temporal Attention Network for Time-Specific Next POI Recommendation",
        "authors": [
            "Shanshan Feng",
            "Feiyu Meng",
            "Lisi Chen",
            "Shuo Shang",
            "Y. Ong"
        ],
        "published_date": "2024",
        "journal": "Knowledge Discovery and Data Mining",
        "abstract": "The next Point-of-interest recommendation has attracted extensive research interest recently, which predicts users' subsequent movements. The main challenge is how to effectively capture users' personalized sequential transitions in check-in trajectory, and various methods have been developed. However, most existing studies ignore the temporal information when conducting the next POI recommendation. To fill this gap, we investigate a time-specific next POI recommendation task, which additionally incorporates the target time information. We propose a brand new Time2Rotation technique to capture the temporal information. Different from conventional methods, we represent timeslots as rotation vectors and then perform the rotation operations. Based on the Time2Rotation technique, we propose a novel rotation-based temporal attention network, namely ROTAN, for the time-specific next POI recommendation task. The ROTAN begins by building a collaborative POI transition graph, capturing the asymmetric temporal influence in sequential transitions. After that, it incorporates temporal information into the modeling of individual check-in trajectories, extracting separate representations for user preference and POI influence to reflect their distinct temporal patterns. Lastly, the target time is integrated to generate recommendations. Extensive experiments are conducted on three real-world datasets, which demonstrates the advantages of the proposed Time2Rotation technique and ROTAN recommendation model.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/5a66e25c2e2eddac28dc648c8cba1a626338aa37.pdf"
    },
    "8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf": {
        "title": "KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding",
        "authors": [
            "Zhen Chen",
            "Dalin Zhang",
            "Shanshan Feng",
            "Kaixuan Chen",
            "Lisi Chen",
            "Peng Han",
            "Shuo Shang"
        ],
        "published_date": "2024",
        "journal": "AAAI Conference on Artificial Intelligence",
        "abstract": "Trajectory similarity computation serves as a fundamental functionality of various spatial information applications. Although existing deep learning similarity computation methods offer better efficiency and accuracy than non-learning solutions, they are still immature in trajectory embedding and suffer from poor generality and heavy preprocessing for training. Targeting these limitations, we propose a novel framework named KGTS based on knowledge graph grid embedding, prompt trajectory embedding, and unsupervised contrastive learning for improved trajectory similarity computation. Specifically, we first embed map grids with a GRot embedding method to vigorously grasp the neighbouring relations of grids. Then, a prompt trajectory embedding network incorporates the resulting grid embedding and extracts trajectory structure and point order information. It is trained by unsupervised contrastive learning, which not only alleviates the heavy preprocessing burden but also provides exceptional generality with creatively designed strategies for positive sample generation. The prompt trajectory embedding adopts a customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding. Extensive experiments on two real-world trajectory datasets demonstrate the superior performance of KGTS over state-of-the-art methods.",
        "keywords": [],
        "paper_type": "Methodology",
        "summary": "**Summary for Research Survey:**\n\n**Problem Addressed:**\nThe paper addresses the limitations of existing deep learning methods for trajectory similarity computation. These limitations include immaturity in trajectory embedding, poor generality across different datasets or scenarios, and the heavy preprocessing burden required for training.\n\n**Method Used:**\nThe paper proposes a novel framework named KGTS (Knowledge Graph Trajectory Similarity) to overcome these issues. KGTS integrates three main components:\n1.  **Knowledge Graph Grid Embedding:** It first embeds map grids using a GRot embedding method to effectively capture the neighboring relations between grids.\n2.  **Prompt Trajectory Embedding:** A prompt trajectory embedding network then incorporates these grid embeddings to extract trajectory structure and point order information. A customized \"prompt paradigm\" is employed to mitigate the representational gap between the grid embeddings and the trajectory embeddings.\n3.  **Unsupervised Contrastive Learning:** The prompt trajectory embedding network is trained using unsupervised contrastive learning. This approach reduces the need for extensive preprocessing and enhances generality, featuring creatively designed strategies for positive sample generation.\n\n**Key Findings:**\n*   KGTS demonstrates superior performance over state-of-the-art methods on two real-world trajectory datasets.\n*   The unsupervised contrastive learning approach effectively alleviates the heavy preprocessing burden commonly associated with deep learning models for trajectory similarity.\n*   The framework achieves exceptional generality, indicating its robust applicability across diverse trajectory datasets and scenarios.\n\n**Key Math Equations (if applicable):**\nWhile the abstract does not explicitly list mathematical equations, the described methods inherently rely on them. The \"GRot embedding method\" would involve specific mathematical formulations for graph/grid embedding. The \"prompt trajectory embedding network\" would utilize neural network architectures with associated activation functions, weight matrices, and loss functions. \"Unsupervised contrastive learning\" is built upon specific loss functions (e.g., InfoNCE loss) and similarity metrics (e.g., cosine similarity) for optimizing embedding spaces. The \"creatively designed strategies for positive sample generation\" would also involve algorithmic or mathematical rules for constructing these pairs.\n\n**New Direction: Yes**\n**Type: Methodology**\n\n**Reasoning for 'New Direction: Yes':**\nThis paper proposes a new method path by innovatively combining and adapting several advanced techniques for trajectory similarity computation. While knowledge graphs, prompt learning, and contrastive learning exist individually, their specific integration within the KGTS framework for this problem is novel. The use of a \"customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding\" and the \"creatively designed strategies for positive sample generation\" within an unsupervised contrastive learning setup are specific methodological innovations. This is not merely an incremental improvement on an existing architecture but a novel architectural and training strategy that addresses fundamental limitations of prior approaches (generality, preprocessing burden) in a distinct way.",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf"
    },
    "4cc64d9426731495a3dac43186f5206ab29a7822.pdf": {
        "title": "Universal Knowledge Graph Embedding Framework Based on High-Quality Negative Sampling and Weighting",
        "authors": [
            "Pengfei Zhang",
            "Huang Peng",
            "Yang Fang",
            "Zongqiang Yang",
            "Yanli Hu",
            "Zhen Tan",
            "Weidong Xiao"
        ],
        "published_date": "2024",
        "journal": "Mathematics",
        "abstract": "The traditional model training approach based on negative sampling randomly samples a portion of negative samples for training, which can easily overlook important negative samples and adversely affect the training of knowledge graph embedding models. Some researchers have explored non-sampling model training frameworks that use all unobserved triples as negative samples to improve model training performance. However, both training methods inevitably introduce false negative samples and easy-to-separate negative samples that are far from the model\u2019s decision boundary, and they do not consider the adverse effects of long-tail entities and relations during training, thus limiting the improvement of model training performance. To address this issue, we propose a universal knowledge graph embedding framework based on high-quality negative sampling and weighting, called HNSW-KGE. First, we conduct pre-training based on the NS-KGE non-sampling training framework to quickly obtain an initial set of relatively high-quality embedding vector representations for all entities and relations. Second, we design a candidate negative sample set construction strategy that samples a certain number of negative samples that are neither false negatives nor easy-to-separate negatives for all positive triples, based on the embedding vectors obtained from pre-training. This ensures the provision of high-quality negative samples for model training. Finally, we apply weighting to the loss function based on the frequency of the entities and relations appearing in the triples to mitigate the adverse effects of long-tail entities and relations on model training. Experiments conducted on benchmark datasets FB15K237 and WN18RR using various knowledge graph embedding models demonstrate that our proposed framework HNSW-KGE, based on high-quality negative sampling and weighting, achieves better training performance and exhibits versatility, making it applicable to various types of knowledge embedding models.",
        "keywords": [],
        "paper_type": "",
        "summary": "",
        "file_path": "/media/aiserver/New Volume/HDD_linux/bear/SurveyX/paper_data/knowledge_graph_embedding/core_papers/4cc64d9426731495a3dac43186f5206ab29a7822.pdf"
    }
}