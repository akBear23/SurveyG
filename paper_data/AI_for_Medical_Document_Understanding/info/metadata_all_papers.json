{
    "9b90291103892b9f9665c11461d7bc9ea40ea9ec.pdf": {
        "title": "MONAI: An open-source framework for deep learning in healthcare",
        "authors": [
            "M. Cardoso",
            "Wenqi Li",
            "Richard Brown",
            "Nic Ma",
            "E. Kerfoot",
            "Yiheng Wang",
            "Benjamin Murrey",
            "A. Myronenko",
            "Can Zhao",
            "Dong Yang",
            "V. Nath",
            "Yufan He",
            "Ziyue Xu",
            "Ali Hatamizadeh",
            "Wenjie Zhu",
            "Yun Liu",
            "Mingxin Zheng",
            "Yucheng Tang",
            "Isaac Yang",
            "Michael Zephyr",
            "Behrooz Hashemian",
            "Sachidanand Alle",
            "Mohammad Zalbagi Darestani",
            "C. Budd",
            "M. Modat",
            "Tom Kamiel Magda Vercauteren",
            "Guotai Wang",
            "Yiwen Li",
            "Yipeng Hu",
            "Yunguan Fu",
            "Benjamin L. Gorman",
            "Hans J. Johnson",
            "Brad W. Genereaux",
            "B. S. Erdal",
            "Vikash Gupta",
            "A. Diaz-Pinto",
            "Andre Dourson",
            "L. Maier-Hein",
            "P. Jaeger",
            "M. Baumgartner",
            "Jayashree Kalpathy-Cramer",
            "Mona G. Flores",
            "J. Kirby",
            "L. Cooper",
            "H. Roth",
            "Daguang Xu",
            "David Bericat",
            "R. Floca",
            "S. K. Zhou",
            "Haris Shuaib",
            "K. Farahani",
            "K. Maier-Hein",
            "S. Aylward",
            "Prerna Dogra",
            "S. Ourselin",
            "Andrew Feng"
        ],
        "published_date": "2022",
        "abstract": "Artificial Intelligence (AI) is having a tremendous impact across most areas of science. Applications of AI in healthcare have the potential to improve our ability to detect, diagnose, prognose, and intervene on human disease. For AI models to be used clinically, they need to be made safe, reproducible and robust, and the underlying software framework must be aware of the particularities (e.g. geometry, physiology, physics) of medical data being processed. This work introduces MONAI, a freely available, community-supported, and consortium-led PyTorch-based framework for deep learning in healthcare. MONAI extends PyTorch to support medical data, with a particular focus on imaging, and provide purpose-specific AI model architectures, transformations and utilities that streamline the development and deployment of medical AI models. MONAI follows best practices for software-development, providing an easy-to-use, robust, well-documented, and well-tested software framework. MONAI preserves the simple, additive, and compositional approach of its underlying PyTorch libraries. MONAI is being used by and receiving contributions from research, clinical and industrial teams from around the world, who are pursuing applications spanning nearly every aspect of healthcare.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/9b90291103892b9f9665c11461d7bc9ea40ea9ec.pdf",
        "venue": "arXiv.org",
        "citationCount": 611,
        "score": 203.66666666666666,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### MONAI: An open-source framework for deep learning in healthcare \\cite{cardoso20221om}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: General-purpose deep learning frameworks (e.g., PyTorch, TensorFlow) lack domain-specific functionalities required for medical data, which has unique characteristics (e.g., complex formats, rich metadata, high dimensionality, physiological basis) \\cite{cardoso20221om}. Concurrently, existing healthcare-specific frameworks are fragmented, leading to diluted development efforts, reduced code quality, and slowed research progress \\cite{cardoso20221om}.\n    *   **Importance and Challenge**: Developing AI models for healthcare requires specialized processing, architectures, and augmentation methods that account for the particularities of medical data. For clinical use, these models must be safe, reproducible, and robust, necessitating a foundational software framework that understands these domain-specific requirements \\cite{cardoso20221om}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **General-purpose frameworks**: TensorFlow, PyTorch, Keras, JAX, Apache MXNet \\cite{cardoso20221om}.\n        *   **Healthcare-specific frameworks**: NiftyNet, DLTK, DeepNeuro (academic); NVIDIA Clara, Microsoft Project InnerEye (industry) \\cite{cardoso20221om}.\n    *   **Limitations of Previous Solutions**:\n        *   General-purpose frameworks necessitate significant custom development and testing for medical data, increasing R&D time and risk \\cite{cardoso20221om}.\n        *   Healthcare-specific frameworks have emerged in a disjointed manner, resulting in a fragmented software landscape that hinders collaboration and efficiency \\cite{cardoso20221om}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: MONAI Core is a freely available, community-supported, and consortium-led PyTorch-based framework designed for deep learning in healthcare, with a primary focus on imaging, video, and structured data \\cite{cardoso20221om}. It extends PyTorch by providing domain-optimized foundational capabilities for developing healthcare AI model training workflows \\cite{cardoso20221om}.\n    *   **Novelty and Differentiation**:\n        *   **PyTorch-Native Design**: Adheres to PyTorch's guidelines, offering an \"opt-in and incremental\" approach that allows seamless integration of MONAI components into existing PyTorch projects with minimal learning curve \\cite{cardoso20221om}.\n        *   **Domain-Specific Components**: Provides purpose-specific AI model architectures, transformations, and utilities tailored for medical data, addressing the unique requirements of medical image I/O, preprocessing, and augmentation \\cite{cardoso20221om}.\n        *   **Compositional and Standardized Pipelines**: Emphasizes robust, low-level data processing components that can be efficiently composed into complex and flexible pipelines, reducing effort for reproducing algorithmic research baselines \\cite{cardoso20221om}.\n        *   **Open-Source and Collaborative**: Licensed under Apache-2.0, fostering broad adoption and contributions from research, clinical, and industrial teams, aiming to unify the fragmented healthcare AI software field \\cite{cardoso20221om}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   **Comprehensive Medical Image Transforms**: Offers a wide array of medical image-specific transformations for I/O (`LoadImage`), spatial manipulation (`Spacing`, `Orientation`, `Affine`, `Rand3DElastic`), intensity normalization (`NormalizeIntensity`, `RandGaussianNoise`), and crop/pad operations \\cite{cardoso20221om}.\n        *   **Physics-Specific Transforms**: Includes transformations grounded in the physics of medical image acquisition, such as `RandKSpaceSpikeNoise` for MR imaging, enabling augmentation in relevant domains \\cite{cardoso20221om}.\n        *   **Invertible Transforms**: Provides mechanisms to invert previously applied spatial transforms, crucial for applications like test-time augmentation (TTA), augmentation-consistency based domain adaptation, and preserving original image geometry during inference \\cite{cardoso20221om}.\n        *   **Flexible Transform Application**: Supports both array-based and dictionary-based transforms, allowing for simple single-data processing or complex pipelines with paired data \\cite{cardoso20221om}.\n    *   **System Design or Architectural Innovations**:\n        *   **Modular Core Architecture**: Organized into distinct modules for data handling (`monai.data`), loss functions (`monai.losses`), network definitions (`monai.networks`), transforms (`monai.transforms`), metrics (`monai.metrics`), optimizers (`monai.optimizers`), and workflow engines/handlers (`monai.engines`, `monai.handlers`) \\cite{cardoso20221om}.\n        *   **PyTorch Ecosystem Integration**: Seamlessly extends and integrates with core PyTorch ecosystem tools, such as Ignite, ensuring two-way compatibility and leveraging existing quality-of-life frameworks \\cite{cardoso20221om}.\n\n5.  **Experimental Validation**\n    *   The provided paper content primarily describes the MONAI framework, its design principles, and its components. It states that MONAI is \"being used by and receiving contributions from research, clinical and industrial teams\" and provides \"examples of how MONAI Core... is being applied to solve a variety of healthcare challenges\" through other Project MONAI components (e.g., MONAI Label, MONAI Deploy) \\cite{cardoso20221om}.\n    *   However, this specific paper **does not present empirical experimental results or benchmarks** comparing MONAI's performance (e.g., speed, accuracy, resource usage) against other frameworks or models built without MONAI's specific features.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: MONAI Core focuses on foundational capabilities for deep learning model research and development, particularly for imaging, video, and structured data \\cite{cardoso20221om}. It does not encompass all aspects of the AI lifecycle; other Project MONAI components address areas like AI-assisted labeling (MONAI Label), clinical deployment (MONAI Deploy), and federated learning (MONAI FL) \\cite{cardoso20221om}.\n    *   **Scope of Applicability**: Primarily applicable to the development and training of AI models for healthcare, with a strong emphasis on medical imaging and related structured data \\cite{cardoso20221om}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: MONAI significantly advances the technical state-of-the-art by providing a standardized, robust, and community-driven open-source framework specifically tailored for the unique challenges of medical AI development \\cite{cardoso20221om}. Its PyTorch-native, modular, and extensible design, coupled with domain-specific components, streamlines and accelerates the R&D process for healthcare AI \\cite{cardoso20221om}.\n    *   **Potential Impact**: It has the potential to accelerate, simplify, and reduce the risks associated with healthcare AI model development and subsequent clinical deployment \\cite{cardoso20221om}. By unifying fragmented efforts and fostering collaboration, MONAI aims to standardize best practices and improve the safety, reproducibility, and robustness of AI in healthcare \\cite{cardoso20221om}.",
        "keywords": [
            "MONAI",
            "deep learning in healthcare",
            "medical imaging",
            "PyTorch-based framework",
            "domain-specific functionalities",
            "medical image transformations",
            "invertible transforms",
            "open-source framework",
            "standardized AI pipelines",
            "reproducibility and robustness",
            "fragmented healthcare AI",
            "AI model development",
            "clinical deployment"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"this work **introduces monai**, a freely available... pytorch-based **framework** for deep learning in healthcare.\"\n*   it details what monai \"extends pytorch to support\" and what it \"provide[s]\" (purpose-specific ai model architectures, transformations, and utilities).\n*   the introduction discusses the need for robust \"software frameworks and tools\" to develop ai models in healthcare, setting the stage for the proposed solution (monai).\n\nthese points strongly indicate that the paper is presenting a new system or tool, along with the methods and components it offers. this aligns with the definition of a **technical** paper.\n\n**classification: technical**"
    },
    "69d49a06f09cf934310ccbf3bb2a360fa719272d.pdf": {
        "title": "Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans",
        "authors": [
            "M. Roberts",
            "D. Driggs",
            "Matthew Thorpe",
            "J. Gilbey",
            "Michael Yeung",
            "S. Ursprung",
            "Angelica I. Avil\u00e9s-Rivero",
            "Christian Etmann",
            "C. McCague",
            "L. Beer",
            "J. Weir-McCall",
            "Z. Teng",
            "E. Gkrania-Klotsas",
            "Alessandro Anna Emily Emmanuel Georg Ghassem Guang Helmut Jac Ruggiero Korhonen Jefferson Ako Langs Gozaliasl Ya",
            "A. Ruggiero",
            "A. Korhonen",
            "E. Jefferson",
            "E. Ako",
            "G. Langs",
            "G. Gozaliasl",
            "Guang Yang",
            "H. Prosch",
            "J. Preller",
            "Jan Stanczuk",
            "Jingjing Tang",
            "J. Hofmanninger",
            "J. Babar",
            "L. E. Sanchez",
            "M. Thillai",
            "Paula Martin Gonzalez",
            "P. Teare",
            "Xiaoxiang Zhu",
            "Mishal N. Patel",
            "Conor Cafolla",
            "Hojjat Azadbakht",
            "Joseph Jacob",
            "Josh Lowe",
            "Kang Zhang",
            "Kyle Bradley",
            "Marcel Wassin",
            "Markus Holzer",
            "Kangyu Ji",
            "Maria Delgado Ortet",
            "T. Ai",
            "N. Walton",
            "P. Li\u00f2",
            "S. Stranks",
            "Tolou Shadbahr",
            "Weizhe Lin",
            "Y. Zha",
            "Zhangming Niu",
            "J. H. Rudd",
            "E. Sala",
            "C. Sch\u00f6nlieb"
        ],
        "published_date": "2020",
        "abstract": "Machine learning methods offer great promise for fast and accurate detection and prognostication of coronavirus disease 2019 (COVID-19) from standard-of-care chest radiographs (CXR) and chest computed tomography (CT) images. Many articles have been published in 2020 describing new machine learning-based models for both of these tasks, but it is unclear which are of potential clinical utility. In this systematic review, we consider all published papers and preprints, for the period from 1 January 2020 to 3 October 2020, which describe new machine learning models for the diagnosis or prognosis of COVID-19 from CXR or CT images. All manuscripts uploaded to bioRxiv, medRxiv and arXiv along with all entries in EMBASE and MEDLINE in this timeframe are considered. Our search identified 2,212 studies, of which 415 were included after initial screening and, after quality screening, 62 studies were included in this systematic review. Our review finds that none of the models identified are of potential clinical use due to methodological flaws and/or underlying biases. This is a major weakness, given the urgency with which validated COVID-19 models are needed. To address this, we give many recommendations which, if followed, will solve these issues and lead to higher-quality model development and well-documented manuscripts. Many machine learning-based approaches have been developed for the prognosis and diagnosis of COVID-19 from medical images and this Analysis identifies over 2,200 relevant published papers and preprints in this area. After initial screening, 62 studies are analysed and the authors find they all have methodological flaws standing in the way of clinical utility. The authors have several recommendations to address these issues.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/69d49a06f09cf934310ccbf3bb2a360fa719272d.pdf",
        "venue": "Nature Machine Intelligence",
        "citationCount": 807,
        "score": 161.4,
        "summary": "Here is a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the proliferation of machine learning (ML) models for COVID-19 detection and prognostication using chest radiographs (CXR) and computed tomography (CT) scans, highlighting the lack of clarity regarding their clinical utility and methodological soundness.\n    *   **Importance & Challenge**: The urgent global need for accurate and validated COVID-19 diagnostic and prognostic tools during the pandemic necessitates a critical evaluation of rapidly developed ML solutions. Many published models suffer from methodological flaws and biases, hindering their potential for effective clinical deployment and potentially leading to misleading results.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a systematic review that builds upon earlier broad analyses of predictive models for COVID-19, but distinguishes itself by focusing specifically on ML models using imaging data.\n    *   **Limitations of Previous Solutions (i.e., the reviewed papers)**: The paper identifies that existing ML models for COVID-19 imaging are limited by: (i) bias in small datasets; (ii) variability in large, internationally-sourced datasets; (iii) poor integration of multi-stream data; (iv) the inherent difficulty of prognostication; and (v) insufficient collaboration between clinicians and data analysts. Critically, it finds that none of the reviewed models are of potential clinical use due to pervasive methodological flaws and biases.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper employs a systematic review methodology, searching multiple databases (EMBASE, MEDLINE, bioRxiv, medRxiv, arXiv) for published papers and preprints (January 1 to October 3, 2020) describing new ML models for COVID-19 diagnosis or prognosis from CXR or CT images.\n    *   **Novelty/Difference (of the review itself)**: This review is novel in its rigorous *quality screening stage* (using CLAIM and RQS checklists) to ensure only studies with sufficiently documented methodologies are included for detailed analysis, going beyond mere risk of bias assessment. It specifically focuses on identifying *systematic methodological flaws* in imaging-based ML models for COVID-19 and provides detailed recommendations informed by both clinicians and algorithm developers to address these issues.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (of the review)**: The primary contribution is the development and application of a comprehensive quality screening and risk of bias assessment framework to systematically evaluate a large body of rapidly emerging ML research in a critical medical context.\n    *   **Theoretical Insights or Analysis (from the review's findings)**: The paper's key finding is that *none* of the 61 rigorously reviewed ML models for COVID-19 diagnosis or prognosis from CXR/CT images met the threshold for clinical utility due to widespread methodological deficiencies. It provides a detailed categorization of these recurring issues, including insufficient documentation of model selection, pre-processing, training details, lack of external validation, poor robustness analysis, and the use of small or imbalanced datasets.\n\n*   **5. Experimental Validation (of the review's findings)**\n    *   **Experiments Conducted**: The \"experiment\" is the systematic review process itself. An initial search yielded 2,212 studies, which were progressively filtered through abstract screening (415 relevant), full-text screening (319), and a stringent quality review (61 included for detailed analysis). These 61 papers were then analyzed for methodological rigor, risk of bias (using PROBAST guidance), dataset characteristics, and reported performance.\n    *   **Key Performance Metrics and Comparison Results (as reported by this review)**:\n        *   **Quality Screening Failures**: 215/254 deep learning papers and 44/68 traditional ML papers were excluded due to quality issues. Common failures included insufficient documentation of model selection (61%), image pre-processing (58%), and training details (49%).\n        *   **Risk of Bias**: 54/61 papers exhibited a high risk of bias in at least one domain (participants, predictors, outcomes, or analysis), with others having unclear risks.\n        *   **Validation Practices**: 47/61 papers relied solely on internal validation; only 12 used truly external test datasets.\n        *   **Dataset Issues**: Many datasets were small (19/32 diagnosis papers used <2,000 datapoints for development) and often imbalanced. Only 4/32 diagnosis papers had both large and balanced testing datasets.\n        *   **Code Availability**: Only 13/61 papers made their code publicly available.\n        *   **Reported Performance (from reviewed papers)**: While reviewed papers reported high accuracies (e.g., 0.88-0.99 for CXR diagnosis, 0.76-0.98 for CT diagnosis) and AUCs (0.70-1.00 for CT diagnosis), the review explicitly cautions against direct comparison due to the identified methodological inconsistencies \\cite{roberts2020wnr}.\n\n*   **6. Limitations & Scope (of the review itself)**\n    *   **Technical Limitations/Assumptions**: The review is limited to papers published or preprinted up to October 3, 2020. The assessment of bias and quality is dependent on the information provided in the manuscripts, which was often found to be inadequate. \"Unclear\" risk of bias categories reflect the inherent difficulty in assessing undocumented methodologies.\n    *   **Scope of Applicability**: The findings and recommendations are specifically tailored to machine learning models for COVID-19 diagnosis and prognostication using chest radiographs and CT scans.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the understanding of the *quality and reliability* of early ML research in a critical medical domain. It provides a crucial shift from merely reporting high performance metrics to rigorously scrutinizing the underlying methodology, reproducibility, and clinical relevance of ML models.\n    *   **Potential Impact on Future Research**: The paper offers a vital set of detailed recommendations for researchers, algorithm developers, authors, and reviewers to enhance the rigor, reproducibility, and clinical utility of future ML models in medical imaging, particularly in rapid response scenarios. It underscores the critical need for improved data curation, robust validation strategies, transparent reporting of methods, and interdisciplinary collaboration to ensure that ML advancements translate into meaningful clinical impact \\cite{roberts2020wnr}.",
        "keywords": [
            "Machine learning models",
            "COVID-19 diagnosis and prognostication",
            "Medical imaging (CXR",
            "CT)",
            "Systematic review methodology",
            "Methodological flaws and biases",
            "Clinical utility",
            "Quality screening",
            "Risk of bias assessment",
            "External validation",
            "Insufficient documentation",
            "Imbalanced datasets",
            "Recommendations for medical imaging ML"
        ],
        "paper_type": "based on the abstract and introduction, this paper clearly fits the **survey** type.\n\nhere's why:\n\n*   **abstract explicitly states:** \"in this systematic review, we search embase via ovid, medline via pubmed, biorxiv, medrxiv and arxiv for published papers and preprints...\"\n*   **abstract details:** the process of identifying 2,212 studies, screening them, and including 61 studies in \"this systematic review.\"\n*   **abstract's findings:** are derived from the analysis of these existing models (\"our review finds that none of the models identified are of potential clinical use...\").\n*   **introduction's context:** sets the stage by mentioning the rush to develop ai models, which is the body of literature the paper then systematically reviews.\n\nthe paper comprehensively reviews existing literature, identifies flaws, and provides recommendations based on that review, which are all hallmarks of a survey paper."
    },
    "47f7e6326f7ee042966130f673743abbb99b8a7f.pdf": {
        "title": "AI in Medical Imaging Informatics: Current Challenges and Future Directions",
        "authors": [
            "A. Panayides",
            "A. Amini",
            "N. Filipovic",
            "Ashish Sharma",
            "S. Tsaftaris",
            "A. Young",
            "D. Foran",
            "N. Do",
            "S. Golemati",
            "T. Kur\u00e7",
            "Kun Huang",
            "K. Nikita",
            "B. Veasey",
            "M. Zervakis",
            "J. Saltz",
            "C. Pattichis"
        ],
        "published_date": "2020",
        "abstract": "This paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/47f7e6326f7ee042966130f673743abbb99b8a7f.pdf",
        "venue": "IEEE journal of biomedical and health informatics",
        "citationCount": 390,
        "score": 78.0,
        "summary": "This paper reviews state-of-the-art research solutions across the spectrum of medical imaging informatics, discusses clinical translation, and provides future directions for advancing clinical practice. More specifically, it summarizes advances in medical imaging acquisition technologies for different modalities, highlighting the necessity for efficient medical data management strategies in the context of AI in big healthcare data analytics. It then provides a synopsis of contemporary and emerging algorithmic methods for disease classification and organ/ tissue segmentation, focusing on AI and deep learning architectures that have already become the de facto approach. The clinical benefits of in-silico modelling advances linked with evolving 3D reconstruction and visualization applications are further documented. Concluding, integrative analytics approaches driven by associate research branches highlighted in this study promise to revolutionize imaging informatics as known today across the healthcare continuum for both radiology and digital pathology applications. The latter, is projected to enable informed, more accurate diagnosis, timely prognosis, and effective treatment planning, underpinning precision medicine.",
        "keywords": []
    },
    "e4599e4561888b1406a521dec5ba37275e83e727.pdf": {
        "title": "Machine learning based approaches for detecting COVID-19 using clinical text data",
        "authors": [
            "A. Khanday",
            "Syed Tanzeel Rabani",
            "Q. Khan",
            "N. Rouf",
            "Masarat Mohi Ud Din"
        ],
        "published_date": "2020",
        "abstract": "Technology advancements have a rapid effect on every field of life, be it medical field or any other field. Artificial intelligence has shown the promising results in health care through its decision making by analysing the data. COVID-19 has affected more than 100 countries in a matter of no time. People all over the world are vulnerable to its consequences in future. It is imperative to develop a control system that will detect the coronavirus. One of the solution to control the current havoc can be the diagnosis of disease with the help of various AI tools. In this paper, we classified textual clinical reports into four classes by using classical and ensemble machine learning algorithms. Feature engineering was performed using techniques like Term frequency/inverse document frequency (TF/IDF), Bag of words (BOW) and report length. These features were supplied to traditional and ensemble machine learning classifiers. Logistic regression and Multinomial Na\u00efve Bayes showed better results than other ML algorithms by having 96.2% testing accuracy. In future recurrent neural network can be used for better accuracy.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/e4599e4561888b1406a521dec5ba37275e83e727.pdf",
        "venue": "International journal of information technology",
        "citationCount": 271,
        "score": 54.2,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n**1. Research Problem & Motivation**\n*   **Problem:** The paper addresses the critical need for rapid and accurate detection of COVID-19 by classifying textual clinical reports to differentiate it from other similar respiratory illnesses like SARS and ARDS \\cite{khanday2020j59}.\n*   **Importance & Challenge:**\n    *   COVID-19's rapid global spread necessitated effective control systems for early detection \\cite{khanday2020j59}.\n    *   Traditional diagnostic methods faced limitations due to resource scarcity and the sheer volume of potential cases \\cite{khanday2020j59}.\n    *   Machine learning offers a promising avenue for diagnosis from clinical data, but less work had focused on textual data compared to image-based approaches at the time \\cite{khanday2020j59}.\n\n**2. Related Work & Positioning**\n*   **Relation to existing approaches:**\n    *   Acknowledges the established use of NLP and ML in text analytics, sentiment analysis, and disease diagnosis (e.g., epilepsy, diabetes) \\cite{khanday2020j59}.\n    *   Mentions contemporary ML/DL efforts for COVID-19 diagnosis, primarily using chest radiography images (e.g., COVID-Net \\cite{wang2020covid}) and prognostic prediction from patient data \\cite{yan2020machine,jiang2020machine}.\n*   **Limitations of previous solutions:**\n    *   The paper highlights a gap in research, stating that \"less work is being done on diagnosis and predicting using text\" for COVID-19 \\cite{khanday2020j59}. This positions their work as a contribution to textual data analysis for diagnosis.\n    *   Existing prognostic models often relied on small datasets from specific hospitals, potentially limiting generalizability \\cite{khanday2020j59}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:**\n    *   A supervised machine learning framework is proposed to classify textual clinical reports into four categories: COVID, ARDS, SARS, and Both (COVID, ARDS) \\cite{khanday2020j59}.\n    *   **Data Collection:** Utilized an open-source GitHub repository containing 212 patient records with clinical notes and findings \\cite{khanday2020j59}.\n    *   **Preprocessing:** Involved cleaning text by removing unnecessary elements (punctuation, stopwords, symbols, URLs, links) and lemmatization \\cite{khanday2020j59}.\n    *   **Feature Engineering:** Extracted 40 relevant features using Term Frequency-Inverse Document Frequency (TF/IDF), Bag of Words (BOW), and report length, including unigrams and bigrams \\cite{khanday2020j59}.\n    *   **Classification:** Evaluated a suite of classical ML algorithms (Logistic Regression, Multinomial Naive Bayes, SVM, Decision Tree) and ensemble methods (Bagging, AdaBoost, Random Forest, Stochastic Gradient Boosting) \\cite{khanday2020j59}.\n*   **Novelty/Difference:**\n    *   The primary innovation is the systematic application and comparative evaluation of a broad range of classical and ensemble machine learning algorithms for *multi-class textual classification* of clinical reports to distinguish COVID-19 from other respiratory illnesses \\cite{khanday2020j59}. This addresses a less explored area compared to image-based diagnostics.\n\n**4. Key Technical Contributions**\n*   **Novel algorithms, methods, or techniques:**\n    *   Development and evaluation of a machine learning pipeline for multi-class classification of COVID-19, SARS, ARDS, and co-occurrence (Both COVID, ARDS) from unstructured clinical text \\cite{khanday2020j59}.\n    *   Demonstrated the effectiveness of specific feature engineering techniques (TF/IDF, BOW, report length, unigrams, bigrams) for extracting diagnostic signals from clinical notes \\cite{khanday2020j59}.\n    *   Provided a comparative analysis of eight distinct ML and ensemble algorithms, identifying top-performing models for this specific textual diagnostic task \\cite{khanday2020j59}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:**\n    *   Classification experiments were performed on a dataset of 212 English clinical reports, labeled into the four target categories \\cite{khanday2020j59}.\n    *   The preprocessed data, with 40 extracted features, was used to train and test the selected machine learning algorithms \\cite{khanday2020j59}.\n*   **Key Performance Metrics and Comparison Results:**\n    *   The primary metric reported was testing accuracy \\cite{khanday2020j59}.\n    *   Logistic Regression and Multinomial Naive Bayes achieved the highest performance among all tested algorithms \\cite{khanday2020j59}.\n    *   Both Logistic Regression and Multinomial Naive Bayes yielded a **96.2% testing accuracy** \\cite{khanday2020j59}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:**\n    *   The dataset size of 212 patient records is relatively small, which might affect the generalizability of the models to larger and more diverse populations \\cite{khanday2020j59}.\n    *   The paper acknowledges the need for \"a huge amount of data\" for robust machine learning classification \\cite{khanday2020j59}.\n    *   It suggests that Recurrent Neural Networks (RNNs) could potentially achieve \"better accuracy\" in future work, implying current models might not be fully optimized \\cite{khanday2020j59}.\n*   **Scope of Applicability:**\n    *   The models are specifically designed for classifying clinical text reports related to respiratory illnesses \\cite{khanday2020j59}.\n    *   The current implementation is limited to English-language clinical notes \\cite{khanday2020j59}.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:**\n    *   Demonstrates the high efficacy of classical and ensemble machine learning for multi-class classification of COVID-19 from unstructured clinical text, achieving 96.2% accuracy with Logistic Regression and MNB \\cite{khanday2020j59}.\n    *   Provides a valuable baseline and empirical evidence for the utility of NLP in rapid, potentially cost-effective, and early diagnosis of COVID-19 and similar diseases from textual data \\cite{khanday2020j59}.\n*   **Potential Impact on Future Research:**\n    *   Encourages further exploration of textual data for disease diagnosis, especially in the context of emerging pandemics where clinical notes are often the first available detailed patient information \\cite{khanday2020j59}.\n    *   Suggests future work could involve more advanced deep learning architectures like RNNs to potentially enhance accuracy and handle more complex linguistic patterns \\cite{khanday2020j59}.\n    *   Contributes to the development of AI-driven decision support systems in healthcare, particularly for diagnostic assistance in resource-constrained settings \\cite{khanday2020j59}.",
        "keywords": [
            "COVID-19 diagnosis",
            "textual clinical reports",
            "machine learning",
            "Natural Language Processing (NLP)",
            "multi-class classification",
            "respiratory illnesses",
            "feature engineering",
            "Logistic Regression",
            "Multinomial Naive Bayes",
            "96.2% accuracy",
            "early detection",
            "unstructured clinical text",
            "comparative evaluation"
        ],
        "paper_type": "the paper describes a specific methodology for classifying clinical text reports to detect covid-19 using machine learning algorithms. it details the techniques used (feature engineering, specific classifiers) and presents the performance results (accuracy). this aligns with presenting a new system or method and its evaluation.\n\ntherefore, the classification is: **technical**"
    },
    "f106ef1bad05ed38011cbd711d7c397080023b86.pdf": {
        "title": "Beware explanations from AI in health care",
        "authors": [
            "Boris Babic",
            "S. Gerke",
            "T. Evgeniou",
            "I. Cohen"
        ],
        "published_date": "2021",
        "abstract": "The benefits of explainable artificial intelligence are not what they appear Artificial intelligence and machine learning (AI/ML) algorithms are increasingly developed in health care for diagnosis and treatment of a variety of medical conditions (1). However, despite the technical prowess of such systems, their adoption has been challenging, and whether and how much they will actually improve health care remains to be seen. A central reason for this is that the effectiveness of AI/ML-based medical devices depends largely on the behavioral characteristics of its users, who, for example, are often vulnerable to well-documented biases or algorithmic aversion (2). Many stakeholders increasingly identify the so-called black-box nature of predictive algorithms as the core source of users' skepticism, lack of trust, and slow uptake (3, 4). As a result, lawmakers have been moving in the direction of requiring the availability of explanations for black-box algorithmic decisions (5). Indeed, a near-consensus is emerging in favor of explainable AI/ML among academics, governments, and civil society groups. Many are drawn to this approach to harness the accuracy benefits of noninterpretable AI/ML such as deep learning or neural nets while also supporting transparency, trust, and adoption. We argue that this consensus, at least as applied to health care, both overstates the benefits and undercounts the drawbacks of requiring black-box algorithms to be explainable.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/f106ef1bad05ed38011cbd711d7c397080023b86.pdf",
        "venue": "Science",
        "citationCount": 165,
        "score": 41.25,
        "summary": "The benefits of explainable artificial intelligence are not what they appear Artificial intelligence and machine learning (AI/ML) algorithms are increasingly developed in health care for diagnosis and treatment of a variety of medical conditions (1). However, despite the technical prowess of such systems, their adoption has been challenging, and whether and how much they will actually improve health care remains to be seen. A central reason for this is that the effectiveness of AI/ML-based medical devices depends largely on the behavioral characteristics of its users, who, for example, are often vulnerable to well-documented biases or algorithmic aversion (2). Many stakeholders increasingly identify the so-called black-box nature of predictive algorithms as the core source of users' skepticism, lack of trust, and slow uptake (3, 4). As a result, lawmakers have been moving in the direction of requiring the availability of explanations for black-box algorithmic decisions (5). Indeed, a near-consensus is emerging in favor of explainable AI/ML among academics, governments, and civil society groups. Many are drawn to this approach to harness the accuracy benefits of noninterpretable AI/ML such as deep learning or neural nets while also supporting transparency, trust, and adoption. We argue that this consensus, at least as applied to health care, both overstates the benefits and undercounts the drawbacks of requiring black-box algorithms to be explainable.",
        "keywords": []
    },
    "1bfc69cd9a06be740ea6a0f421e0852a90856220.pdf": {
        "title": "Experimental evidence of effective human\u2013AI collaboration in medical decision-making",
        "authors": [
            "C. Reverberi",
            "T. Rigon",
            "A. Solari",
            "C. Hassan",
            "P. Cherubini",
            "Giulio Halim Sebastian Sabela M\u00e1rio Agn\u00e8s Gl\u00f2ria Fern\u00e1nde Antonelli Awadie Bernhofer Carballal Dinis-Ribeiro",
            "G. Antonelli",
            "H. Awadie",
            "Sebastian Bernhofer",
            "S. Carballal",
            "M. Dinis-Ribeiro",
            "Agn\u00e8s Fern\u00e1ndez-Clotett",
            "G. F. Esparrach",
            "I. Gralnek",
            "Yuta Higasa",
            "Takuro Hirabayashi",
            "Tatsuki Hirai",
            "Mineo Iwatate",
            "Miki Kawano",
            "Markus Mader",
            "A. Maieron",
            "Sebastian Mattes",
            "Tastuya Nakai",
            "I. Ord\u00e1s",
            "R. Ortig\u00e3o",
            "Oswaldo Ortiz Z\u00fa\u00f1iga",
            "M. Pellis\u00e9",
            "C. Pinto",
            "F. Riedl",
            "Ariadna S\u00e1nchez",
            "E. Steiner",
            "Yukari Tanaka",
            "Andrea Cherubini"
        ],
        "published_date": "2022",
        "abstract": "Artificial Intelligence (ai) systems are precious support for decision-making, with many applications also in the medical domain. The interaction between mds and ai enjoys a renewed interest following the increased possibilities of deep learning devices. However, we still have limited evidence-based knowledge of the context, design, and psychological mechanisms that craft an optimal human\u2013ai collaboration. In this multicentric study, 21 endoscopists reviewed 504 videos of lesions prospectively acquired from real colonoscopies. They were asked to provide an optical diagnosis with and without the assistance of an ai support system. Endoscopists were influenced by ai (OR=3.05\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textsc {or}=3.05$$\\end{document}), but not erratically: they followed the ai advice more when it was correct (OR=3.48\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textsc {or}=3.48$$\\end{document}) than incorrect (OR=1.85\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\textsc {or}=1.85$$\\end{document}). Endoscopists achieved this outcome through a weighted integration of their and the ai opinions, considering the case-by-case estimations of the two reliabilities. This Bayesian-like rational behavior allowed the human\u2013ai hybrid team to outperform both agents taken alone. We discuss the features of the human\u2013ai interaction that determined this favorable outcome.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/1bfc69cd9a06be740ea6a0f421e0852a90856220.pdf",
        "venue": "Scientific Reports",
        "citationCount": 114,
        "score": 38.0,
        "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: The paper addresses the critical lack of evidence-based knowledge regarding the optimal context, design, and psychological mechanisms that facilitate effective human-AI collaboration in medical decision-making. Existing challenges include over-reliance, under-reliance, and the opacity of AI's judgment reliability.\n    *   **Importance/Challenge**: While AI systems offer significant support in medicine, achieving a \"hybrid intelligence\" that outperforms either humans or AI alone requires a deep understanding of how medical professionals integrate AI advice. This is crucial for maximizing diagnostic accuracy and avoiding pitfalls in high-stakes clinical settings.\n\n*   **Related Work & Positioning**\n    *   This work distinguishes itself by investigating the *inner dynamics of AI-supported medical doctors' belief revision*, a topic largely unexplored by previous experimental studies.\n    *   Prior research primarily focused on evaluating the standalone performance of AI-based medical devices or measuring the overall improvement in MD diagnostic accuracy *when supported* by AI.\n    *   Limitations of previous approaches include not delving into the psychological processes of belief revision, the perceived reliability of AI advice, or the specific mechanisms by which MDs integrate AI opinions.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The study models endoscopists' diagnostic updates as a Bayesian-like belief-revision process, where AI advice is treated as an additional piece of information. It employs a rigorous statistical framework based on logistic regression with mixed effects to quantify various aspects of human-AI interaction.\n    *   **Novelty**:\n        *   **Within-Subject Design**: Utilizes a unique within-subject experimental design, comparing the same endoscopists' decisions with and without real-time AI support on a prospectively acquired dataset of 504 colonoscopy videos \\cite{reverberi2022av0}.\n        *   **Novel Statistical Model**: Developed a new, rigorous statistical model that transparently separates \"efficacy\" (the MD aligning with correct AI advice) and \"safety\" (the MD maintaining their own belief when AI is incorrect). This model provides a nuanced assessment of over- and under-reliance \\cite{reverberi2022av0}.\n        *   **Psychological Process Exploration**: Explored the psychological processes underlying effective hybrid teams, even with non-transparent AI, by collecting novel parameters such as the MDs' interpretation of AI output and their *perceived reliability of each AI advice*, which were previously overlooked \\cite{reverberi2022av0}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of a Bayesian-like belief-revision model and associated logistic regression framework to quantitatively analyze the dynamics of human-AI diagnostic decision-making, specifically distinguishing between the positive (efficacy) and negative (safety) impacts of AI advice \\cite{reverberi2022av0}.\n    *   **System Design/Architectural Innovations**: The experimental setup involved integrating a real-time AI CADx system (GI Genius v3.0) into a controlled observational study, allowing for the dynamic presentation of AI advice and the collection of detailed human responses, including perceived AI confidence \\cite{reverberi2022av0}.\n    *   **Theoretical Insights/Analysis**: Provides empirical evidence for a \"Bayesian-like rational behavior\" in human-AI collaboration, where humans selectively integrate AI opinions by weighting their own and the AI's perceived reliabilities on a case-by-case basis \\cite{reverberi2022av0}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: A multicentric study involving 21 endoscopists (10 experts, 11 non-experts) who reviewed 504 video clips of colorectal lesions. Diagnoses were made in two sessions: one without AI assistance (AI only for lesion detection) and one with dynamic AI optical diagnosis \\cite{reverberi2022av0}.\n    *   **Key Performance Metrics**: Four odds ratios were calculated using logistic regression with mixed effects:\n        *   **AI Influence (\u03c9I)**: Measured the convergence of endoscopists' diagnoses with AI's.\n        *   **AI Effect on Diagnostic Accuracy (\u03c9A)**: Measured overall diagnostic accuracy improvement with AI.\n        *   **Effectiveness (\u03c9E)**: Measured accuracy improvement when AI's advice was correct.\n        *   **Safety (\u03c9S)**: Measured the ability to maintain diagnostic performance when AI's advice was incorrect \\cite{reverberi2022av0}.\n    *   **Comparison Results**:\n        *   Endoscopists were significantly influenced by AI (\u03c9I = 3.05) \\cite{reverberi2022av0}.\n        *   AI assistance led to an overall improvement in diagnostic performance (implied by \u03c9A > 1, with the hybrid team outperforming individual agents) \\cite{reverberi2022av0}.\n        *   Crucially, endoscopists demonstrated selective trust: they followed AI advice significantly more when it was correct (\u03c9E = 3.48) than when it was incorrect (\u03c9S = 1.85), indicating a rational integration process \\cite{reverberi2022av0}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The study focuses on the human-AI interaction with a specific, non-transparent AI system (GI Genius v3.0). While the AI's internal motives were not conveyed, the study successfully investigated how humans interact with such systems.\n    *   **Scope of Applicability**: The findings are directly applicable to optical diagnosis of colorectal lesions. However, the demonstrated mechanisms of Bayesian-like rational integration and selective trust in AI advice have broader implications for human-AI collaboration in other medical domains and critical decision-making contexts.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing robust empirical evidence and a novel statistical framework for understanding the mechanisms of effective human-AI collaboration in medical diagnosis \\cite{reverberi2022av0}.\n    *   It demonstrates that medical professionals can engage in a sophisticated, \"Bayesian-like rational behavior,\" selectively integrating AI advice based on perceived reliability, which leads to superior \"hybrid intelligence\" outcomes \\cite{reverberi2022av0}.\n    *   **Potential Impact**: The insights gained are crucial for designing future human-AI interfaces, developing training protocols that foster optimal collaboration, and mitigating risks associated with over- or under-reliance on AI, thereby enhancing diagnostic accuracy and efficiency across various critical applications.",
        "keywords": [
            "Human-AI collaboration",
            "Medical decision-making",
            "Bayesian-like belief-revision",
            "Hybrid intelligence",
            "Selective trust",
            "Over-reliance",
            "Under-reliance",
            "Novel statistical model",
            "Efficacy and safety quantification",
            "Within-subject experimental design",
            "Perceived reliability of AI advice",
            "Optical diagnosis of colorectal lesions",
            "Bayesian-like rational behavior"
        ],
        "paper_type": "based on the provided content:\n\nthe \"abstract\" section is actually a list of references, so it doesn't provide information about the paper's content.\n\nhowever, the **introduction** clearly describes:\n*   **findings with statistical evidence:** \"endoscopists were influenced by ai ( or=3.05 ), but not erratically: they followed the ai advice more when it was correct ( or=3.48 ) than incorrect ( or=1.85 ).\"\n*   **observed behavior and outcomes:** \"endoscopists achieved this outcome through a weighted integration of their and the ai opinions...\", \"this bayesian\u2011like rational behavior allowed the human\u2013ai hybrid team to outperform both agents taken alone.\"\n*   **discussion of features determining the outcome:** \"we discuss the features of the human\u2013ai interaction that determined this favorable outcome.\"\n\nthe title \"experimental evidence of effective human\u2013ai collaboration in medical decision-making\" further reinforces the nature of the paper.\n\nthese elements align perfectly with the criteria for an **empirical** paper:\n*   **abstract mentions:** \"study\", \"experiment\", \"data\", \"statistical\", \"findings\" (implied by the introduction's content and title)\n*   **introduction discusses:** \"research questions\", \"methodology\", \"participants\" (implied by the description of endoscopists' behavior and statistical results)\n\ntherefore, this paper is an **empirical** study.\n\n**classification:** empirical"
    },
    "f079d2c49646735ffe09e4117b075f4e900f4420.pdf": {
        "title": "Developing, implementing and governing artificial intelligence in medicine: a step-by-step approach to prevent an artificial intelligence winter",
        "authors": [
            "Davy van de Sande",
            "M. V. van Genderen",
            "J. M. Smit",
            "Joost Huiskens",
            "J. J. Visser",
            "Robert E. R. Veen",
            "E. van Unen",
            "Oliver Hilgers Ba",
            "D. Gommers",
            "J. Bommel"
        ],
        "published_date": "2022",
        "abstract": "Objective Although the role of artificial intelligence (AI) in medicine is increasingly studied, most patients do not benefit because the majority of AI models remain in the testing and prototyping environment. The development and implementation trajectory of clinical AI models are complex and a structured overview is missing. We therefore propose a step-by-step overview to enhance clinicians\u2019 understanding and to promote quality of medical AI research. Methods We summarised key elements (such as current guidelines, challenges, regulatory documents and good practices) that are needed to develop and safely implement AI in medicine. Conclusion This overview complements other frameworks in a way that it is accessible to stakeholders without prior AI knowledge and as such provides a step-by-step approach incorporating all the key elements and current guidelines that are essential for implementation, and can thereby help to move AI from bytes to bedside.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/f079d2c49646735ffe09e4117b075f4e900f4420.pdf",
        "venue": "BMJ Health & Care Informatics",
        "citationCount": 79,
        "score": 26.333333333333332,
        "summary": "Objective Although the role of artificial intelligence (AI) in medicine is increasingly studied, most patients do not benefit because the majority of AI models remain in the testing and prototyping environment. The development and implementation trajectory of clinical AI models are complex and a structured overview is missing. We therefore propose a step-by-step overview to enhance clinicians\u2019 understanding and to promote quality of medical AI research. Methods We summarised key elements (such as current guidelines, challenges, regulatory documents and good practices) that are needed to develop and safely implement AI in medicine. Conclusion This overview complements other frameworks in a way that it is accessible to stakeholders without prior AI knowledge and as such provides a step-by-step approach incorporating all the key elements and current guidelines that are essential for implementation, and can thereby help to move AI from bytes to bedside.",
        "keywords": []
    },
    "216fd02c57bce4f280134c2bf166447d5ad1e97e.pdf": {
        "title": "Guidelines for Artificial Intelligence in Medicine: Literature Review and Content Analysis of Frameworks",
        "authors": [
            "Norah L. Crossnohere",
            "Mohamed I. Elsaid",
            "J. Paskett",
            "S. Bose-Brill",
            "John F. P. Bridges"
        ],
        "published_date": "2022",
        "abstract": "Background Artificial intelligence (AI) is rapidly expanding in medicine despite a lack of consensus on its application and evaluation. Objective We sought to identify current frameworks guiding the application and evaluation of AI for predictive analytics in medicine and to describe the content of these frameworks. We also assessed what stages along the AI translational spectrum (ie, AI development, reporting, evaluation, implementation, and surveillance) the content of each framework has been discussed. Methods We performed a literature review of frameworks regarding the oversight of AI in medicine. The search included key topics such as \u201cartificial intelligence,\u201d \u201cmachine learning,\u201d \u201cguidance as topic,\u201d and \u201ctranslational science,\u201d and spanned the time period 2014-2022. Documents were included if they provided generalizable guidance regarding the use or evaluation of AI in medicine. Included frameworks are summarized descriptively and were subjected to content analysis. A novel evaluation matrix was developed and applied to appraise the frameworks\u2019 coverage of content areas across translational stages. Results Fourteen frameworks are featured in the review, including six frameworks that provide descriptive guidance and eight that provide reporting checklists for medical applications of AI. Content analysis revealed five considerations related to the oversight of AI in medicine across frameworks: transparency, reproducibility, ethics, effectiveness, and engagement. All frameworks include discussions regarding transparency, reproducibility, ethics, and effectiveness, while only half of the frameworks discuss engagement. The evaluation matrix revealed that frameworks were most likely to report AI considerations for the translational stage of development and were least likely to report considerations for the translational stage of surveillance. Conclusions Existing frameworks for the application and evaluation of AI in medicine notably offer less input on the role of engagement in oversight and regarding the translational stage of surveillance. Identifying and optimizing strategies for engagement are essential to ensure that AI can meaningfully benefit patients and other end users.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/216fd02c57bce4f280134c2bf166447d5ad1e97e.pdf",
        "venue": "Journal of Medical Internet Research",
        "citationCount": 75,
        "score": 25.0,
        "summary": "Background Artificial intelligence (AI) is rapidly expanding in medicine despite a lack of consensus on its application and evaluation. Objective We sought to identify current frameworks guiding the application and evaluation of AI for predictive analytics in medicine and to describe the content of these frameworks. We also assessed what stages along the AI translational spectrum (ie, AI development, reporting, evaluation, implementation, and surveillance) the content of each framework has been discussed. Methods We performed a literature review of frameworks regarding the oversight of AI in medicine. The search included key topics such as \u201cartificial intelligence,\u201d \u201cmachine learning,\u201d \u201cguidance as topic,\u201d and \u201ctranslational science,\u201d and spanned the time period 2014-2022. Documents were included if they provided generalizable guidance regarding the use or evaluation of AI in medicine. Included frameworks are summarized descriptively and were subjected to content analysis. A novel evaluation matrix was developed and applied to appraise the frameworks\u2019 coverage of content areas across translational stages. Results Fourteen frameworks are featured in the review, including six frameworks that provide descriptive guidance and eight that provide reporting checklists for medical applications of AI. Content analysis revealed five considerations related to the oversight of AI in medicine across frameworks: transparency, reproducibility, ethics, effectiveness, and engagement. All frameworks include discussions regarding transparency, reproducibility, ethics, and effectiveness, while only half of the frameworks discuss engagement. The evaluation matrix revealed that frameworks were most likely to report AI considerations for the translational stage of development and were least likely to report considerations for the translational stage of surveillance. Conclusions Existing frameworks for the application and evaluation of AI in medicine notably offer less input on the role of engagement in oversight and regarding the translational stage of surveillance. Identifying and optimizing strategies for engagement are essential to ensure that AI can meaningfully benefit patients and other end users.",
        "keywords": []
    },
    "9d53177352bd6019f42ec1a7b32feff353b7bd3f.pdf": {
        "title": "Acupuncture for Chronic Severe Functional Constipation",
        "authors": [
            "Zhishun Liu",
            "Shiyan Yan",
            "Jiani Wu",
            "Liyun He",
            "Ning Li",
            "Guirong Dong",
            "J. Fang",
            "W. Fu",
            "Lixin Fu",
            "Jianhua Sun",
            "Linpeng Wang",
            "Shun Wang",
            "Jun Yang",
            "Hongxing Zhang",
            "Jianbin Zhang",
            "Jiping Zhao",
            "W. Zhou",
            "Zhongyu Zhou",
            "Yanke Ai",
            "Kehua Zhou",
            "Jia Liu",
            "Huanfang Xu",
            "Yuying Cai",
            "Baoyan Liu"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/9d53177352bd6019f42ec1a7b32feff353b7bd3f.pdf",
        "venue": "Annals of Internal Medicine",
        "citationCount": 225,
        "score": 25.0,
        "summary": "",
        "keywords": []
    },
    "3b0dbd77778d22e332a9618b321b425cec96a0b5.pdf": {
        "title": "Utilization of Self-Diagnosis Health Chatbots in Real-World Settings: Case Study",
        "authors": [
            "Xiang-hong Fan",
            "Daren Chao",
            "Zhan Zhang",
            "Dakuo Wang",
            "Xiaohua Li",
            "Feng Tian"
        ],
        "published_date": "2020",
        "abstract": "Background Artificial intelligence (AI)-driven chatbots are increasingly being used in health care, but most chatbots are designed for a specific population and evaluated in controlled settings. There is little research documenting how health consumers (eg, patients and caregivers) use chatbots for self-diagnosis purposes in real-world scenarios. Objective The aim of this research was to understand how health chatbots are used in a real-world context, what issues and barriers exist in their usage, and how the user experience of this novel technology can be improved. Methods We employed a data-driven approach to analyze the system log of a widely deployed self-diagnosis chatbot in China. Our data set consisted of 47,684 consultation sessions initiated by 16,519 users over 6 months. The log data included a variety of information, including users\u2019 nonidentifiable demographic information, consultation details, diagnostic reports, and user feedback. We conducted both statistical analysis and content analysis on this heterogeneous data set. Results The chatbot users spanned all age groups, including middle-aged and older adults. Users consulted the chatbot on a wide range of medical conditions, including those that often entail considerable privacy and social stigma issues. Furthermore, we distilled 2 prominent issues in the use of the chatbot: (1) a considerable number of users dropped out in the middle of their consultation sessions, and (2) some users pretended to have health concerns and used the chatbot for nontherapeutic purposes. Finally, we identified a set of user concerns regarding the use of the chatbot, including insufficient actionable information and perceived inaccurate diagnostic suggestions. Conclusions Although health chatbots are considered to be convenient tools for enhancing patient-centered care, there are issues and barriers impeding the optimal use of this novel technology. Designers and developers should employ user-centered approaches to address the issues and user concerns to achieve the best uptake and utilization. We conclude the paper by discussing several design implications, including making the chatbots more informative, easy-to-use, and trustworthy, as well as improving the onboarding experience to enhance user engagement.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/3b0dbd77778d22e332a9618b321b425cec96a0b5.pdf",
        "venue": "Journal of Medical Internet Research",
        "citationCount": 111,
        "score": 22.200000000000003,
        "summary": "Background Artificial intelligence (AI)-driven chatbots are increasingly being used in health care, but most chatbots are designed for a specific population and evaluated in controlled settings. There is little research documenting how health consumers (eg, patients and caregivers) use chatbots for self-diagnosis purposes in real-world scenarios. Objective The aim of this research was to understand how health chatbots are used in a real-world context, what issues and barriers exist in their usage, and how the user experience of this novel technology can be improved. Methods We employed a data-driven approach to analyze the system log of a widely deployed self-diagnosis chatbot in China. Our data set consisted of 47,684 consultation sessions initiated by 16,519 users over 6 months. The log data included a variety of information, including users\u2019 nonidentifiable demographic information, consultation details, diagnostic reports, and user feedback. We conducted both statistical analysis and content analysis on this heterogeneous data set. Results The chatbot users spanned all age groups, including middle-aged and older adults. Users consulted the chatbot on a wide range of medical conditions, including those that often entail considerable privacy and social stigma issues. Furthermore, we distilled 2 prominent issues in the use of the chatbot: (1) a considerable number of users dropped out in the middle of their consultation sessions, and (2) some users pretended to have health concerns and used the chatbot for nontherapeutic purposes. Finally, we identified a set of user concerns regarding the use of the chatbot, including insufficient actionable information and perceived inaccurate diagnostic suggestions. Conclusions Although health chatbots are considered to be convenient tools for enhancing patient-centered care, there are issues and barriers impeding the optimal use of this novel technology. Designers and developers should employ user-centered approaches to address the issues and user concerns to achieve the best uptake and utilization. We conclude the paper by discussing several design implications, including making the chatbots more informative, easy-to-use, and trustworthy, as well as improving the onboarding experience to enhance user engagement.",
        "keywords": []
    },
    "06773aeb019c078fcda6a20e7bd1afc27aaf07dc.pdf": {
        "title": "COVID-19 Mortality Among American Indian and Alaska Native Persons \u2014 14 States, January\u2013June 2020",
        "authors": [
            "Jessica Arrazola",
            "Matthew M Masiello",
            "Sujata Joshi",
            "Adrian E Dominguez",
            "Amy J. Poel",
            "C. Wilkie",
            "J. Bressler",
            "J. Mclaughlin",
            "J. Kraszewski",
            "K. Komatsu",
            "Xandy Peterson Pompa",
            "Megan Jespersen",
            "Gillian Richardson",
            "N. Lehnertz",
            "Pamela Lemaster",
            "Britney Rust",
            "Alison Keyser Metobo",
            "B. Doman",
            "David Casey",
            "Jessica Kumar",
            "Alyssa L Rowell",
            "T. Miller",
            "Mike Mannell",
            "Ozair H Naqvi",
            "A. Wendelboe",
            "R. Leman",
            "J. Clayton",
            "Bree Barbeau",
            "Samantha K Rice",
            "V. Warren-Mears",
            "Abigail Echo-Hawk",
            "Andria Apostolou",
            "M. Landen"
        ],
        "published_date": "2020",
        "abstract": "American Indian/Alaska Native (AI/AN) persons experienced disproportionate mortality during the 2009 influenza A(H1N1) pandemic (1,2). Concerns of a similar trend during the coronavirus disease 2019 (COVID-19) pandemic led to the formation of a workgroup* to assess the prevalence of COVID-19 deaths in the AI/AN population. As of December 2, 2020, CDC has reported 2,689 COVID-19-associated deaths among non-Hispanic AI/AN persons in the United States.\u2020 A recent analysis found that the cumulative incidence of laboratory-confirmed COVID-19 cases among AI/AN persons was 3.5 times that among White persons (3). Among 14 participating states, the age-adjusted AI/AN COVID-19 mortality rate (55.8 deaths per 100,000; 95% confidence interval [CI] =\u00a052.5-59.3) was 1.8 (95% CI =\u00a01.7-2.0) times that among White persons (30.3 deaths per 100,000; 95% CI =\u00a029.9-30.7). Although COVID-19 mortality rates increased with age among both AI/AN and White persons, the disparity was largest among those aged 20-49 years. Among persons aged 20-29 years, 30-39 years, and 40-49 years, the COVID-19 mortality rates among AI/AN were 10.5, 11.6, and 8.2 times, respectively, those among White persons. Evidence that AI/AN communities might be at increased risk for COVID-19 illness and death demonstrates the importance of documenting and understanding the reasons for these disparities while developing collaborative approaches with federal, state, municipal, and tribal agencies to minimize the impact of COVID-19 on AI/AN communities. Together, public health partners can plan for medical countermeasures and prevention activities for AI/AN communities.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/06773aeb019c078fcda6a20e7bd1afc27aaf07dc.pdf",
        "venue": "MMWR. Morbidity and mortality weekly report",
        "citationCount": 109,
        "score": 21.8,
        "summary": "Here's a focused summary of the technical/research paper for a literature review:\n\n---\n\n### Analysis of COVID-19 Mortality Among American Indian and Alaska Native Persons \\cite{arrazola2020ad1}\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** To quantify the specific technical problem of disproportionate COVID-19 mortality rates among American Indian and Alaska Native (AI/AN) persons compared to non-Hispanic White persons in the early phase of the pandemic (January\u2013June 2020).\n    *   **Importance & Challenge:** This problem is critical due to historical evidence of AI/AN communities experiencing higher mortality during previous pandemics (e.g., 2009 H1N1) and existing health inequities. Quantifying this disparity is essential for informing targeted public health interventions, resource allocation (e.g., vaccine distribution), and addressing underlying social determinants of health. A key challenge is ensuring accurate and comprehensive data collection across diverse state and tribal jurisdictions.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The study builds upon prior research indicating disproportionate mortality among AI/AN persons during the 2009 influenza A(H1N1) pandemic \\cite{arrazola2020ad1}(1,2) and a recent analysis showing higher COVID-19 case incidence among AI/AN persons \\cite{arrazola2020ad1}(3).\n    *   **Limitations of Previous Solutions:** While not explicitly detailing limitations of prior *methodologies*, this work addresses a gap by providing specific, quantified COVID-19 *mortality* rates and age-specific disparities, which previous studies on incidence or other pandemics did not fully cover for COVID-19.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm:**\n        *   **Study Design:** A retrospective observational study utilizing confirmed COVID-19\u2013associated death data from 14 participating states (representing ~46.5% of the U.S. AI/AN population) for January 1\u2013June 30, 2020.\n        *   **Data Collection:** A multi-agency workgroup (including state health departments, Tribal Epidemiology Centers, Indian Health Service, and CDC) collected data from case investigations, death certificates, and laboratory reports.\n        *   **Population Definition:** AI/AN persons were defined as AI/AN alone or in any racial/ethnic combination (including non-Hispanic and Hispanic). The comparator group was specifically defined as non-Hispanic White persons only, chosen to avoid confounding with other racial/ethnic groups also experiencing disparities.\n        *   **Statistical Analysis:** Age-adjusted and age-specific COVID-19 mortality rates were calculated using 2019 postcensal population estimates as denominators and age-adjusted to the 2000 U.S. standard population. 95% Confidence Intervals (CIs) were calculated using the Byar approximation to the Poisson distribution, and rate ratios were used for comparisons. Data for persons aged <20 years were suppressed due to small numbers (<10) to protect identifiability.\n    *   **Novelty/Difference:**\n        *   **Collaborative Multi-Jurisdictional Approach:** The formation and execution of a workgroup involving federal, state, and tribal public health entities for rapid, coordinated data collection and analysis is a significant methodological aspect, aiming for more culturally responsive and accurate data.\n        *   **Focused Comparator Group:** The deliberate choice of non-Hispanic White persons as the sole comparator group allowed for a clear and focused assessment of the AI/AN disparity.\n        *   **Emphasis on Age-Specific Disparities:** The study's detailed age-group analysis revealed a critical and novel finding regarding the disproportionately high mortality among younger AI/AN adults (20\u201349 years), which was not widely understood at the time.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   Development and implementation of a rapid, multi-jurisdictional epidemiological surveillance framework for assessing racial/ethnic disparities in pandemic mortality, integrating state and tribal public health data sources.\n        *   Standardized methodology for calculating age-adjusted and age-specific mortality rates and rate ratios across diverse state health departments.\n    *   **Theoretical Insights or Analysis:**\n        *   Quantified the overall age-adjusted COVID-19 mortality rate for AI/AN persons as 1.8 times higher than for non-Hispanic White persons \\cite{arrazola2020ad1}.\n        *   Uncovered a striking age-specific disparity: AI/AN persons aged 20\u201349 years experienced COVID-19 mortality rates 8.2 to 11.6 times higher than their White counterparts \\cite{arrazola2020ad1}. This highlights a unique vulnerability in younger AI/AN populations compared to general COVID-19 mortality patterns.\n        *   Demonstrated that AI/AN persons who died from COVID-19 were significantly younger overall than White persons who died \\cite{arrazola2020ad1}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The study analyzed 1,134 COVID-19 deaths among AI/AN persons and 18,815 deaths among White persons across 14 states during the specified period \\cite{arrazola2020ad1}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Overall Age-Adjusted Mortality Rate:** AI/AN: 55.8 deaths per 100,000; White: 30.3 deaths per 100,000.\n        *   **Rate Ratio (AI/AN:White):** 1.8 (95% CI = 1.7\u20132.0) \\cite{arrazola2020ad1}.\n        *   **Sex Disparity:** Men in both groups had higher mortality rates, with the AI/AN:White rate ratio remaining 1.8 for both sexes \\cite{arrazola2020ad1}.\n        *   **Age-Specific Disparity (Rate Ratios AI/AN:White):**\n            *   20\u201329 years: 10.5 (95% CI = 6.3\u201317.6) \\cite{arrazola2020ad1}.\n            *   30\u201339 years: 11.6 (95% CI = 8.5\u201315.8) \\cite{arrazola2020ad1}.\n            *   40\u201349 years: 8.2 (95% CI = 6.5\u201310.5) \\cite{arrazola2020ad1}.\n            *   The disparity decreased with increasing age, with the rate ratio for persons aged \u226580 years being 0.9 \\cite{arrazola2020ad1}.\n        *   **Age Distribution of Deaths:** 35.1% of AI/AN COVID-19 deaths occurred in persons aged <60 years, compared to 6.3% of White deaths \\cite{arrazola2020ad1}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions:**\n        *   **Limited Comparator Groups:** Only compared AI/AN with non-Hispanic White persons, not other racial/ethnic groups \\cite{arrazola2020ad1}.\n        *   **Potential Underreporting:** COVID-19 deaths likely underreported due to testing limitations and reluctance, especially early in the pandemic \\cite{arrazola2020ad1}.\n        *   **Race/Ethnicity Misclassification:** Potential undercounting of AI/AN deaths due to misclassification in data systems \\cite{arrazola2020ad1}.\n        *   **Incomplete Underlying Health Data:** Inconsistent data collection precluded analysis of underlying health conditions as a contributing factor \\cite{arrazola2020ad1}.\n        *   **No Clustering Accounted For:** The analytic methods did not account for potential clustering of deaths by state \\cite{arrazola2020ad1}.\n    *   **Scope of Applicability:** The findings are based on data from 14 selected states and may not be generalizable to the entire U.S. AI/AN population \\cite{arrazola2020ad1}. The data covers only the first half of 2020.\n\n7.  **Technical Significance**\n    *   **Advance State-of-the-Art:** This paper provides crucial early epidemiological evidence, rigorously quantifying the severe and disproportionate impact of COVID-19 mortality on AI/AN communities, particularly highlighting the unexpected vulnerability of younger AI/AN adults \\cite{arrazola2020ad1}. It demonstrates a successful model for rapid, collaborative public health surveillance involving tribal partners during a crisis.\n    *   **Potential Impact on Future Research:** The findings directly informed public health policy, including recommendations for COVID-19 vaccine allocation strategies by the Advisory Committee on Immunization Practices (ACIP) to prioritize AI/AN populations \\cite{arrazola2020ad1}(6,7). It underscores the urgent need for improved, consistent, and complete collection of race/ethnicity and underlying health data in public health surveillance. It also calls for further research into the complex interplay of socioeconomic factors, healthcare access, and underlying health conditions contributing to these disparities, and emphasizes the importance of culturally sensitive, tribally-led public health interventions.",
        "keywords": [
            "COVID-19 mortality",
            "American Indian and Alaska Native (AI/AN)",
            "health disparities",
            "retrospective observational study",
            "multi-jurisdictional epidemiological surveillance",
            "age-adjusted mortality rates",
            "age-specific mortality rates",
            "rate ratios",
            "disproportionate mortality",
            "younger AI/AN adult mortality",
            "public health interventions",
            "vaccine allocation strategies",
            "social determinants of health",
            "race/ethnicity data collection"
        ],
        "paper_type": "**empirical**\n\n**reasoning:**\n\n*   **title:** \"covid-19 mortality among american indian and alaska native persons \u2014 14 states, january\u2013june 2020\" clearly indicates a data-driven study focused on specific populations and a defined time period.\n*   **venue:** mmwr (morbidity and mortality weekly report) is a publication known for presenting public health surveillance data, epidemiological studies, and reports based on observed data and statistical analysis.\n*   **introduction:**\n    *   it states the purpose: \"assess the prevalence of covid-19 deaths in the ai/an population.\" this is a direct research question.\n    *   it immediately presents quantitative findings with statistical measures: \"cumulative incidence... was 3.5 times that among white persons,\" \"age-adjusted ai/an covid-19 mortality rate (55.8 deaths per 100,000; 95% confidence interval [ci] = 52.5\u201359.3) was 1.8 (95% ci = 1.7\u20132.0) times that among white persons.\"\n    *   it discusses specific data points and comparisons across age groups, all indicative of a study analyzing collected data.\n\nthese elements strongly align with the criteria for an **empirical** paper, which focuses on data-driven studies with statistical analysis and findings."
    },
    "1e3e86fe20ab0fc072855ce1e405560d0bdca7b8.pdf": {
        "title": "Predicting Mortality Risk in Patients with COVID-19 Using Artificial Intelligence to Help Medical Decision-Making",
        "authors": [
            "M. Pourhomayoun",
            "Mahdi Shakibi"
        ],
        "published_date": "2020",
        "abstract": "In the wake of COVID-19 disease, caused by the SARS-CoV-2 virus, we designed and developed a predictive model based on Artificial Intelligence (AI) and Machine Learning algorithms to determine the health risk and predict the mortality risk of patients with COVID-19. In this study, we used documented data of 117,000 patients world-wide with laboratory-confirmed COVID-19. This study proposes an AI model to help hospitals and medical facilities decide who needs to get attention first, who has higher priority to be hospitalized, triage patients when the system is overwhelmed by overcrowding, and eliminate delays in providing the necessary care. The results demonstrate 93% overall accuracy in predicting the mortality rate. We used several machine learning algorithms including Support Vector Machine (SVM), Artificial Neural Networks, Random Forest, Decision Tree, Logistic Regression, and K-Nearest Neighbor (KNN) to predict the mortality rate in patients with COVID-19. In this study, the most alarming symptoms and features were also identified. Finally, we used a separate dataset of COVID-19 patients to evaluate our developed model accuracy, and used confusion matrix to make an in-depth analysis of our classifiers and calculate the sensitivity and specificity of our model.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/1e3e86fe20ab0fc072855ce1e405560d0bdca7b8.pdf",
        "venue": "medRxiv",
        "citationCount": 94,
        "score": 18.8,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n---\n\n### Focused Summary for Literature Review: Predicting Mortality Risk in Patients with COVID-19 Using Artificial Intelligence\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical need for accurately predicting the mortality risk in patients diagnosed with COVID-19 \\cite{pourhomayoun2020fzd}.\n    *   **Importance and Challenge**: This problem is crucial for enabling effective medical decision-making, prioritizing patient care, triaging patients when healthcare systems are overwhelmed by overcrowding, and eliminating delays in providing necessary care during a pandemic. The rapid spread and severe outcomes of COVID-19 made such a predictive tool highly urgent and challenging to develop quickly with reliable accuracy \\cite{pourhomayoun2020fzd}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself within the broader application of Artificial Intelligence (AI) and Machine Learning (ML) in medicine, acknowledging that AI has proven effective in predicting medical conditions and adverse events \\cite{pourhomayoun2020fzd}.\n    *   **Limitations of Previous Solutions**: While not explicitly detailing limitations of prior specific COVID-19 mortality prediction models (given the early stage of the pandemic at the time of publication), the paper implicitly highlights the urgent need for a robust, data-driven solution to manage the unprecedented scale and impact of the COVID-19 outbreak on healthcare systems \\cite{pourhomayoun2020fzd}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a data-driven predictive analytics algorithm based on AI and ML to determine health risk and predict mortality in COVID-19 patients. This involves a multi-stage pipeline: data preprocessing, feature engineering and selection, and comparative evaluation of various ML algorithms \\cite{pourhomayoun2020fzd}.\n    *   **Data Preprocessing**: Utilized a large dataset of over 117,000 laboratory-confirmed COVID-19 patients. Steps included removing useless/redundant data elements, employing data imputation techniques for missing values, and creating a balanced dataset (equal observations for recovered and deceased patients) to mitigate bias \\cite{pourhomayoun2020fzd}.\n    *   **Feature Selection**: Initially extracted 112 features from symptoms, medical notes, demographic, and physiological data. Through consultation with a medical team and application of univariate/multivariate filter and wrapper methods, the feature set was reduced to 42 most informative features, including age, sex, comorbidities (e.g., diabetes, cardiovascular disease), and various symptoms (e.g., cough, dyspnea, fatigue, pneumonia) \\cite{pourhomayoun2020fzd}.\n    *   **Predictive Modeling**: Evaluated six different machine learning algorithms: Support Vector Machine (SVM), Artificial Neural Networks (ANN), Random Forest, Decision Tree, Logistic Regression, and K-Nearest Neighbor (KNN). The ANN model achieved the best performance, configured with two hidden layers (10 and 3 neurons), sigmoid activation, and a stochastic gradient optimizer \\cite{pourhomayoun2020fzd}.\n    *   **Novelty**: The innovation lies in the comprehensive and systematic application of a robust ML pipeline to a large, diverse, real-world COVID-19 patient dataset during the early phase of the pandemic, focusing on identifying key risk factors and developing a highly accurate predictive model for immediate clinical utility \\cite{pourhomayoun2020fzd}.\n\n4.  **Key Technical Contributions**\n    *   **Robust Data Handling Pipeline**: Developed a complete pipeline for processing large-scale, real-world COVID-19 patient data, including effective strategies for data cleaning, missing value imputation, and class imbalance handling \\cite{pourhomayoun2020fzd}.\n    *   **Medically-Informed Feature Selection**: Implemented an advanced feature selection methodology combining statistical filters and wrappers, guided by medical expertise, to identify a concise yet highly predictive set of 42 features from a broader initial pool \\cite{pourhomayoun2020fzd}.\n    *   **Comparative ML Performance Analysis**: Conducted a thorough comparative analysis of six prominent machine learning algorithms for COVID-19 mortality prediction, demonstrating the superior performance of Artificial Neural Networks \\cite{pourhomayoun2020fzd}.\n    *   **High-Accuracy Predictive Model**: Delivered a highly accurate (93.75% overall accuracy) AI model capable of predicting mortality risk, offering a valuable tool for clinical decision support and resource management during a public health crisis \\cite{pourhomayoun2020fzd}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The models were trained and tested on a dataset of over 117,000 laboratory-confirmed COVID-19 patients from 76 countries. A balanced dataset was created for training and testing, ensuring no overlap between the two sets \\cite{pourhomayoun2020fzd}.\n    *   **Evaluation Methodology**: A 10-fold random cross-validation (with no overlap, no replacement) was used to evaluate the developed models \\cite{pourhomayoun2020fzd}.\n    *   **Key Performance Metrics**: Overall Accuracy, Receiver Operating Characteristic (ROC) curves, Area Under Curve (AUC), and Confusion Matrix (for sensitivity and specificity) were used to assess model performance \\cite{pourhomayoun2020fzd}.\n    *   **Comparison Results**:\n        *   Artificial Neural Network achieved the highest overall accuracy of 93.75% \\cite{pourhomayoun2020fzd}.\n        *   Other algorithms performed as follows: Random Forest (91.88%), SVM (90.63%), Decision Tree (90.63%), Logistic Regression (90.00%), and KNN (83.12%) \\cite{pourhomayoun2020fzd}.\n        *   The Neural Network also demonstrated the best performance in terms of ROC curve and AUC \\cite{pourhomayoun2020fzd}.\n        *   Age and chronic diseases (comorbidities) were identified as the most highly correlated features with patient mortality risk \\cite{pourhomayoun2020fzd}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While the model achieves high accuracy, the interpretability of the best-performing Neural Network model, beyond feature importance, is not deeply explored, which can be a challenge in clinical contexts requiring transparent decision rationales \\cite{pourhomayoun2020fzd}.\n    *   **Data Specificity**: The model's performance is validated on a specific dataset from the early phase of the COVID-19 pandemic. Its generalizability to future variants, different populations, or evolving treatment protocols would require continuous re-evaluation and retraining \\cite{pourhomayoun2020fzd}.\n    *   **Scope of Applicability**: The system is primarily designed for predicting mortality risk in COVID-19 patients to aid in triage and resource allocation. While the methodology is generalizable, direct application to other diseases would necessitate new data and model training \\cite{pourhomayoun2020fzd}.\n\n7.  **Technical Significance**\n    *   **Advancement in Medical AI**: This work significantly advances the technical state-of-the-art by demonstrating the successful deployment of a highly accurate and robust AI-driven predictive model for a critical, real-time medical challenge during a global pandemic \\cite{pourhomayoun2020fzd}.\n    *   **Immediate Clinical Impact**: The developed system offers a practical and reliable tool for hospitals and medical facilities to make informed decisions, prioritize patients, and optimize resource allocation, potentially leading to reduced mortality rates and improved patient outcomes during health crises \\cite{pourhomayoun2020fzd}.\n    *   **Methodological Blueprint**: The comprehensive methodology, encompassing data preprocessing, expert-guided feature selection, and comparative ML evaluation, provides a valuable blueprint for developing similar predictive analytics solutions for other infectious diseases or medical conditions \\cite{pourhomayoun2020fzd}.\n    *   **Future Research Potential**: The study lays a foundation for future research in AI-assisted pandemic response, encouraging expansion to other diseases and continuous refinement of models with evolving medical data and understanding \\cite{pourhomayoun2020fzd}.",
        "keywords": [
            "COVID-19 mortality risk prediction",
            "Artificial Intelligence (AI)",
            "Machine Learning (ML)",
            "Predictive analytics algorithm",
            "Data preprocessing",
            "Feature engineering and selection",
            "Artificial Neural Networks (ANN)",
            "Comparative ML performance analysis",
            "High-accuracy predictive model",
            "Clinical decision support",
            "Age and comorbidities",
            "Robust data handling pipeline",
            "Public health crisis management"
        ],
        "paper_type": "the paper describes the design and development of a predictive ai model for covid-19 mortality risk. while it proposes a new model (a technical aspect), the abstract heavily emphasizes the data-driven nature of the work and its statistical analysis.\n\nhere's a breakdown of keywords from the abstract and introduction that align with the classification criteria:\n\n*   **technical indicators:** \"designed and developed a predictive model based on artificial intelligence (ai) and machine learning algorithms\", \"proposes an ai model\", \"used several machine learning algorithms including support vector machine (svm), artificial neural networks, random forest, decision tree, logistic regression, and k-nearest neighbor (knn)\".\n*   **empirical indicators:** \"in this study, we used documented data of 117,000 patients world-wide\", \"the results demonstrate 93% overall accuracy in predicting the mortality rate\", \"in this study, the most alarming symptoms and features were also identified\", \"used a separate dataset of covid-19 patients to evaluate our developed model accuracy\", \"used confusion matrix to make an in-depth analysis of our classifiers and calculate the sensitivity and specificity of our model\".\n\nwhile the paper involves developing a technical solution, the abstract's primary focus is on the *study* conducted, the *data* used, the *results* obtained (accuracy, identified features), and the *statistical evaluation* of the model's performance using specific metrics. the repeated use of \"in this study\" and the detailed description of data and evaluation strongly point to a data-driven approach with statistical analysis.\n\ntherefore, the paper is best classified as **empirical**."
    },
    "9381bd3acdbda98aa1a9b549c729e2601e16cd41.pdf": {
        "title": "Overview of Automatic Clinical Coding: Annotations, Guidelines, and Solutions for non-English Clinical Cases at CodiEsp Track of CLEF eHealth 2020",
        "authors": [
            "Antonio Miranda-Escalada",
            "Aitor Gonzalez-Agirre",
            "Jordi Armengol-Estap\u00e9",
            "Martin Krallinger"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/9381bd3acdbda98aa1a9b549c729e2601e16cd41.pdf",
        "venue": "Conference and Labs of the Evaluation Forum",
        "citationCount": 84,
        "score": 16.8,
        "summary": "",
        "keywords": []
    },
    "6ab6e6f62323132e299fc6717ad0f5ca000414d5.pdf": {
        "title": "Natural language processing in clinical neuroscience and psychiatry: A review",
        "authors": [
            "C. Crema",
            "Giuseppe Attardi",
            "Daniele Sartiano",
            "A. Redolfi"
        ],
        "published_date": "2022",
        "abstract": "Natural language processing (NLP) is rapidly becoming an important topic in the medical community. The ability to automatically analyze any type of medical document could be the key factor to fully exploit the data it contains. Cutting-edge artificial intelligence (AI) architectures, particularly machine learning and deep learning, have begun to be applied to this topic and have yielded promising results. We conducted a literature search for 1,024 papers that used NLP technology in neuroscience and psychiatry from 2010 to early 2022. After a selection process, 115 papers were evaluated. Each publication was classified into one of three categories: information extraction, classification, and data inference. Automated understanding of clinical reports in electronic health records has the potential to improve healthcare delivery. Overall, the performance of NLP applications is high, with an average F1-score and AUC above 85%. We also derived a composite measure in the form of Z-scores to better compare the performance of NLP models and their different classes as a whole. No statistical differences were found in the unbiased comparison. Strong asymmetry between English and non-English models, difficulty in obtaining high-quality annotated data, and train biases causing low generalizability are the main limitations. This review suggests that NLP could be an effective tool to help clinicians gain insights from medical reports, clinical research forms, and more, making NLP an effective tool to improve the quality of healthcare services.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/6ab6e6f62323132e299fc6717ad0f5ca000414d5.pdf",
        "venue": "Frontiers in Psychiatry",
        "citationCount": 47,
        "score": 15.666666666666666,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the technical problem of automatically analyzing and extracting insights from unstructured medical documents in clinical neuroscience and psychiatry \\cite{crema2022kii}.\n    *   This problem is important because a vast amount of medical text produced daily in hospitals and clinics remains unstructured and under-exploited. Leveraging this data through Natural Language Processing (NLP) has the potential to improve healthcare delivery, facilitate earlier diagnoses, better identify candidates for medical procedures, and assist physicians in decision-making \\cite{crema2022kii}.\n\n*   **Related Work & Positioning**\n    *   This work is a systematic review that synthesizes existing NLP applications in neuroscience and psychiatry \\cite{crema2022kii}.\n    *   It positions itself by highlighting limitations of previous reviews: some are too narrowly focused, others are outdated and do not cover recent breakthroughs like attention mechanisms, Transformers, and BERT-models, and some exclusively focus on deep learning, omitting traditional machine learning approaches \\cite{crema2022kii}.\n    *   The paper aims to provide a broad, comprehensive, and up-to-date overview of the current NLP landscape, encompassing both traditional machine learning and modern deep learning techniques, specifically tailored for clinicians in neuroscience and psychiatry \\cite{crema2022kii}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach of the paper itself is a systematic literature review. It involved a search for 1,024 papers using NLP in neuroscience and psychiatry from 2010 to early 2022, with 115 ultimately evaluated \\cite{crema2022kii}.\n    *   The innovation lies in its comprehensive scope and analytical framework:\n        *   It classifies evaluated publications into three major NLP categories: information extraction, classification, and data inference \\cite{crema2022kii}.\n        *   It provides a detailed overview of NLP architectures, tracing the evolution from pre-neural network (NN) statistical models to modern deep learning (DL) architectures, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and particularly Transformer-based models (BERT, BioBERT, Clinical BERT, Umls-BERT, GPT, T5) \\cite{crema2022kii}.\n        *   It introduces a composite Z-score measure to enable a more unbiased comparison of performance across different NLP models and classes \\cite{crema2022kii}.\n\n*   **Key Technical Contributions**\n    *   **Systematic Categorization**: Provides a structured classification of NLP applications in clinical neuroscience and psychiatry into information extraction, classification, and data inference tasks \\cite{crema2022kii}.\n    *   **Performance Synthesis**: Aggregates and reports the overall performance of NLP applications, noting an average F1-score and AUC above 85% \\cite{crema2022kii}.\n    *   **Comparative Metric**: Proposes and applies a composite Z-score measure for unbiased comparison of NLP model performance across different classes, finding no statistical differences in the unbiased comparison \\cite{crema2022kii}.\n    *   **Architectural Overview**: Details the significant advancements in NLP architectures, emphasizing the shift towards deep learning, particularly the impact of Transformer models and their domain-specific adaptations (e.g., BioBERT, Clinical BERT, Umls-BERT) for improved performance in biomedical text mining tasks \\cite{crema2022kii}.\n    *   **Transfer Learning Emphasis**: Highlights the role of transfer learning and fine-tuning in enabling the specialization of pre-trained models for specific clinical tasks with reduced labeled data requirements \\cite{crema2022kii}.\n\n*   **Experimental Validation**\n    *   The review's findings are validated through a systematic analysis of 115 selected papers. It quantitatively reports that NLP applications in the field achieve high performance, with an average F1-score and AUC exceeding 85% \\cite{crema2022kii}.\n    *   The paper's comparative analysis, using the derived Z-scores, indicates that there were \"no statistical differences found in the unbiased comparison\" across different NLP model classes \\cite{crema2022kii}.\n    *   The reviewed literature demonstrates that domain-specific BERT-based models (BioBERT, Clinical BERT, Umls-BERT) consistently outperform generic BERT and previous state-of-the-art models in biomedical Named Entity Recognition (NER), relation extraction, and Question Answering (QA) tasks, validating the utility of domain-specific contextual embeddings \\cite{crema2022kii}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations Identified**: The review highlights several critical limitations in the current state of NLP in clinical neuroscience and psychiatry:\n        *   A strong asymmetry exists between English and non-English models, indicating a lack of resources and development for other languages \\cite{crema2022kii}.\n        *   Difficulty in obtaining high-quality annotated data, which is essential for training and fine-tuning robust NLP models \\cite{crema2022kii}.\n        *   Train biases in existing models often lead to low generalizability, meaning models may not perform well when applied to new, diverse clinical datasets or settings \\cite{crema2022kii}.\n    *   **Scope of Applicability**: The review's scope is specifically focused on NLP applications within *clinical neuroscience and psychiatry*, analyzing how these technologies can be applied to clinical documents in these specialized medical fields \\cite{crema2022kii}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: The paper significantly advances the technical state-of-the-art by providing a comprehensive synthesis of the rapid evolution and high performance of NLP, particularly deep learning and Transformer architectures, in clinical neuroscience and psychiatry \\cite{crema2022kii}. It confirms NLP's capability to achieve high accuracy in complex medical text analysis tasks \\cite{crema2022kii}.\n    *   **Potential Impact on Future Research**: This review serves as a crucial resource for future research by:\n        *   Identifying successful NLP methodologies and architectures applicable to clinical data, guiding researchers toward effective approaches \\cite{crema2022kii}.\n        *   Clearly outlining key technical challenges (data scarcity, language bias, generalizability) that require focused attention in future development, thereby shaping research agendas \\cite{crema2022kii}.\n        *   Bridging the gap between advanced NLP techniques and their practical utility in clinical settings, encouraging further interdisciplinary collaboration between AI researchers and medical professionals \\cite{crema2022kii}.",
        "keywords": [
            "Natural Language Processing (NLP)",
            "Clinical neuroscience and psychiatry",
            "Systematic literature review",
            "Unstructured medical documents",
            "Deep learning architectures",
            "Transformer models (BERT-based)",
            "Information extraction",
            "classification",
            "data inference",
            "Composite Z-score measure",
            "Transfer learning",
            "Domain-specific contextual embeddings",
            "High NLP performance (F1-score",
            "AUC)",
            "Data scarcity"
        ],
        "paper_type": "based on the provided abstract and introduction, this paper is a **survey**.\n\nhere's why:\n\n*   **title:** \"natural language processing in clinical neuroscience and psychiatry: a **review**\" - the word \"review\" is a direct indicator.\n*   **abstract/content:**\n    *   it describes and summarizes various existing nlp models (biobert, clinical bert, umls-bert) and tools (nltk, spacy, hugging face, ctakes, bio-yodie, metamap, medcat).\n    *   it explicitly states, \"table 1 gives a summary of the nlp tools and resources that were utilized in the included studies.\"\n    *   the \"methods\" section details a systematic literature search process (\"a search was conducted to identify all potentially relevant publications...\", \"the acl anthology, pubmed, embase, and psycinfo repositories were queried...\", \"prisma chart (figure 2)\"). this is characteristic of a comprehensive literature review.\n    *   the \"results\" section categorizes and discusses findings from the *analyzed papers* (65 papers identified through the search) into three topics: information extraction, classification, and outcome prediction, reporting performance metrics *from those papers*.\n    *   the \"discussion\" section explicitly states, \"**our review** shows that nlp is used for many different purposes...\"\n*   **classification criteria match:** it perfectly aligns with the \"survey\" criteria: \"reviews existing literature comprehensively,\" and the abstract/introduction discuss \"literature organization\" and \"classification schemes\" (for tools and research areas)."
    },
    "a63d98ba89060ded321bfe482f69c7edd33953dd.pdf": {
        "title": "Explainable machine learning practices: opening another black box for reliable medical AI",
        "authors": [
            "E. Ratti",
            "Mark Graves"
        ],
        "published_date": "2022",
        "abstract": "In the past few years, machine learning (ML) tools have been implemented with success in the medical context. However, several practitioners have raised concerns about the lack of transparency\u2014at the algorithmic level\u2014of many of these tools; and solutions from the field of explainable AI (XAI) have been seen as a way to open the \u2018black box\u2019 and make the tools more trustworthy. Recently, Alex London has argued that in the medical context we do not need machine learning tools to be interpretable at the algorithmic level to make them trustworthy, as long as they meet some strict empirical desiderata. In this paper, we analyse and develop London\u2019s position. In particular, we make two claims. First, we claim that London\u2019s solution to the problem of trust can potentially address another problem, which is how to evaluate the reliability of ML tools in medicine for regulatory purposes. Second, we claim that to deal with this problem, we need to develop London\u2019s views by shifting the focus from the opacity of algorithmic details to the opacity of the way in which ML tools are trained and built. We claim that to regulate AI tools and evaluate their reliability, agencies need an explanation of how ML tools have been built, which requires documenting and justifying the technical choices that practitioners have made in designing such tools. This is because different algorithmic designs may lead to different outcomes, and to the realization of different purposes. However, given that technical choices underlying algorithmic design are shaped by value-laden considerations, opening the black box of the design process means also making transparent and\u00a0motivating (technical and ethical) values and preferences behind such choices. Using tools from philosophy of technology and philosophy of science, we elaborate a framework showing how an explanation of the training processes of ML tools in medicine should look like.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/a63d98ba89060ded321bfe482f69c7edd33953dd.pdf",
        "venue": "AI and Ethics",
        "citationCount": 44,
        "score": 14.666666666666666,
        "summary": "Here's a focused summary of the technical paper \\cite{ratti20224zg} for a literature review, emphasizing technical innovations and empirical validation:\n\n---\n\n### Focused Summary for Literature Review: Explainable Machine Learning Practices for Reliable Medical AI \\cite{ratti20224zg}\n\nThis paper analyzes the challenges of ensuring reliability and trustworthiness in Machine Learning (ML) tools for medical applications, particularly from a regulatory perspective. It proposes a novel conceptual framework for \"explainable machine learning practices\" that shifts the focus from algorithmic interpretability to the transparency and justification of the ML tool's design and training process.\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** The paper addresses the lack of transparency in ML tools used in medicine, which hinders their trustworthiness and makes their evaluation for regulatory purposes challenging. It highlights that while Explainable AI (XAI) often focuses on algorithmic opacity, a more fundamental problem lies in the opacity of the ML tool's design and training process.\n    *   **Importance and challenge:** The increasing deployment of ML in high-stakes medical contexts necessitates robust mechanisms to ensure reliability, safety, and effectiveness. Traditional regulatory approaches struggle with \"unlocked\" (adaptive) ML algorithms that can change behavior post-deployment. The challenge is to develop a framework that allows regulatory bodies to assess not just empirical performance, but also the underlying technical choices and values that shape an ML tool's function and purpose.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** The work builds upon and critiques Alex London's argument that empirical validation, rather than algorithmic interpretability, is sufficient for trusting ML in medicine. It also positions itself against traditional XAI, which primarily aims to explain algorithmic outputs.\n    *   **Limitations of previous solutions:**\n        *   **London's empirical validation:** While important, it doesn't fully address *how* regulatory bodies should evaluate the design process or *who* is responsible for ensuring reliability beyond individual practitioners.\n        *   **Traditional XAI:** Its focus on explaining *what* an algorithm does (algorithmic opacity) overlooks the critical need to understand *how* an ML tool was built and *why* specific technical choices were made (design/training opacity) for regulatory oversight.\n        *   **Existing regulatory guidelines (e.g., FDA's GMLP):** These often lack specific details on how to document and justify the technical decisions and underlying values throughout the ML development lifecycle.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method/algorithm:** The paper does not propose a new technical algorithm or ML method. Instead, its core innovation is a *conceptual framework* for achieving \"explainable machine learning practices.\" This framework emphasizes the systematic documentation and justification of technical choices made during the design and training of ML tools. It draws on philosophy of technology and science to interpret and elaborate on regulatory desiderata.\n    *   **Novelty/Difference:** The novelty lies in shifting the focus of \"explainability\" from the internal workings of an algorithm to the transparency of the *entire development process*. This involves:\n        1.  **Documenting technical decisions:** Recording choices made from problem selection, data management, feature extraction, model training, evaluation, to deployment and update procedures.\n        2.  **Motivating technical choices:** Providing explicit reasons and justifications for these decisions, including the cognitive and noncognitive (value-laden) considerations that influenced them. This demonstrates how the design facilitates the intended purpose and ensures reliability.\n\n4.  **Key Technical Contributions**\n    *   **Novel methods/techniques:**\n        *   A conceptual framework that redefines \"explainability\" for regulatory purposes, focusing on the *process* of ML tool development rather than just algorithmic output.\n        *   The identification and articulation of \"design opacity\" (opacity of training/building process) as a distinct and critical problem for medical AI regulation, separate from \"algorithmic opacity.\"\n        *   A structured approach to interpreting and expanding upon regulatory guidelines (like FDA's Good Machine Learning Practices and Algorithm-Change Protocols) by requiring explicit documentation and justification of technical design choices, including their underlying values.\n    *   **Theoretical insights/analysis:** The paper provides a philosophical grounding for viewing ML tools as \"devices\" (requiring scrutiny of their construction) rather than \"pharmaceuticals\" (primarily requiring empirical efficacy). It integrates philosophical concepts to build a robust argument for why and how the design process of ML tools should be explained for regulatory reliability.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** The paper *does not* present any experimental validation of its proposed framework or any ML models.\n    *   **Key performance metrics and comparison results:** Not applicable. The paper discusses the *importance* of empirical validation for ML tools (as emphasized by London and FDA guidelines) but does not perform or report on such validation itself. Its contribution is a framework for *how to explain the design process* that *leads* to reliable tools, which would then be subject to empirical validation by others.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations/assumptions:** The proposed framework is conceptual; its practical implementation details for diverse ML development workflows are not fully elaborated. It assumes that technical choices and their underlying value-laden considerations can be adequately documented and justified, which may be complex in practice.\n    *   **Scope of applicability:** The framework is primarily intended for the regulation and evaluation of ML tools in high-stakes domains like medicine, where reliability, safety, and effectiveness are critical. It aims to inform regulatory bodies and ML practitioners involved in developing such systems.\n\n7.  **Technical Significance**\n    *   **Advancement of state-of-the-art:** The paper significantly advances the discourse on ML explainability and regulation by proposing a novel, process-centric view of \"explainable practices.\" This shifts the focus from merely understanding *what* an algorithm does to understanding *how and why* an ML system was built, providing a more comprehensive basis for regulatory oversight than solely relying on post-hoc algorithmic explanations or empirical performance metrics.\n    *   **Potential impact on future research:** This work could influence the development of more detailed and actionable \"Good Machine Learning Practices\" by regulatory bodies, emphasizing the need for systematic documentation and ethical justification of technical design choices. It may also stimulate research into practical methodologies and tools for implementing such transparency throughout the ML lifecycle, fostering the development of more trustworthy and responsible AI systems in critical applications.",
        "keywords": [
            "Explainable Machine Learning Practices",
            "Medical AI Regulation",
            "Conceptual Framework",
            "Design Opacity",
            "Process Transparency",
            "Justification of Technical Choices",
            "ML Tool Reliability",
            "ML Tool Trustworthiness",
            "Algorithmic Opacity",
            "Good Machine Learning Practices (GMLP)",
            "Systematic Documentation",
            "High-stakes Medical Applications",
            "Philosophical Grounding"
        ],
        "paper_type": "**position**\n\n**reasoning:**\n\nthe abstract and introduction strongly indicate a **position** paper. here's why:\n\n*   **analyzes and develops a viewpoint:** the paper explicitly states, \"we analyse and develop london\u2019s position.\" it engages with an existing argument (london's) about the interpretability of ml tools in medicine.\n*   **makes specific claims/arguments:** the abstract highlights, \"we make two claims.\" these claims are the core arguments the paper puts forward, building upon or reframing london's ideas.\n*   **argues for a future direction/approach:** the paper argues that to regulate ai tools and evaluate their reliability, agencies \"need an explanation of how ml tools have been built,\" emphasizing the transparency of the design process and underlying values. it elaborates a \"framework showing how an explanation... should look like,\" which is a prescriptive stance.\n*   **discusses current problems and proposed solutions/directions:** it identifies the problem of opacity and lack of trust in medical ai and then proposes a shift in focus and a new approach to address it.\n*   **keywords and venue:** the keywords like \"black box,\" \"medical ai,\" \"reliable ai,\" \"values,\" \"trustworthiness\" and the venue \"ai and ethics\" further suggest a paper that engages with conceptual, ethical, and policy-oriented arguments rather than purely technical, empirical, or theoretical ones in a mathematical sense."
    },
    "253b7d3ebd12adb21ad4ad7a79b2d9084edc0565.pdf": {
        "title": "Overview of DisTEMIST at BioASQ: Automatic detection and normalization of diseases from clinical texts: results, methods, evaluation and multilingual resources",
        "authors": [
            "Antonio Miranda-Escalada",
            "Luis Gasco",
            "Salvador Lima-L\u00f3pez",
            "Eul\u00e0lia Farr\u00e9-Maduell",
            "D. Estrada",
            "A. Nentidis",
            "Anastasia Krithara",
            "Georgios Katsimpras",
            "G. Paliouras",
            "Martin Krallinger"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/253b7d3ebd12adb21ad4ad7a79b2d9084edc0565.pdf",
        "venue": "Conference and Labs of the Evaluation Forum",
        "citationCount": 44,
        "score": 14.666666666666666,
        "summary": "",
        "keywords": []
    },
    "982c80d77d9ee37abfc357b5929dcec8c059e7a7.pdf": {
        "title": "Process Knowledge-Infused AI: Toward User-Level Explainability, Interpretability, and Safety",
        "authors": [
            "Amit P. Sheth",
            "Manas Gaur",
            "Kaushik Roy",
            "Revathy Venkataraman",
            "Vedant Khandelwal",
            "Amit P. Sheth"
        ],
        "published_date": "2022",
        "abstract": "AI has seen wide adoption for automating tasks in several domains. However, AI's use in high-value, sensitive, or safety-critical applications such as self-management for personalized health or personalized nutrition has been challenging. These require that the AI system follows guidelines or well-defined processes set by experts, community, or standards. We characterize these as process knowledge (PK). For example, to diagnose the severity of depression, the AI system should incorporate PK that is part of the clinical decision-making process, such as the Patient Health Questionnaire (PHQ-9). Likewise, a nutritionist's knowledge and dietary guidelines are needed to create food plans for diabetic patients. Furthermore, the BlackBox nature of purely data-reliant statistical AI systems falls short in providing user-understandable explanations, such as what a clinician would need to ensure and document compliance with medical guidelines before relying on a recommendation. Using the examples of mental health and cooking recipes for diabetic patients, we show why, what, and how to incorporate PK along with domain knowledge in machine learning. We discuss methods for infusing PK and present performance evaluation metrics. Support for safety and user-level explainability of the PK-infused learning improves confidence and trust in the AI system.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/982c80d77d9ee37abfc357b5929dcec8c059e7a7.pdf",
        "venue": "IEEE Internet Computing",
        "citationCount": 41,
        "score": 13.666666666666666,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: Process Knowledge-Infused AI \\cite{sheth2022243}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Current AI systems, particularly deep learning models and Large Language Models (LLMs), lack user-level explainability, interpretability, and safety, making them unsuitable for high-value, sensitive, or safety-critical applications (e.g., personalized health, allergy-aware food recommendations, mental health diagnosis) \\cite{sheth2022243}. They often fail to adhere to expert guidelines or well-defined processes.\n    *   **Importance & Challenge**: This problem is critical because trust and confidence in AI are paramount in sensitive domains. The \"black-box\" nature of many AI systems prevents users from understanding decisions, and their \"single-shot\" classification/generation capabilities struggle with multi-step, orchestrated responses required by real-world processes. Furthermore, current AI systems can exhibit unsafe behaviors or \"hallucinate\" responses, posing significant risks in applications like mental healthcare \\cite{sheth2022243}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself within \"Knowledge-infused Learning (KiL),\" a class of Neuro-Symbolic AI techniques. It aims to improve upon data-centric statistical learning by integrating various forms of knowledge into deep neural networks \\cite{sheth2022243}.\n    *   **Limitations of Previous Solutions**:\n        *   **Benchmarking datasets**: Current Natural Language Understanding (NLU) benchmarks for LLMs are insufficient for achieving user-level explainability, safety, and uncertainty handling \\cite{sheth2022243}.\n        *   **Data-centric statistical learning**: Relies solely on data, leading to reduced performance, safety issues, and lack of interpretability in complex, guideline-driven tasks \\cite{sheth2022243}.\n        *   **Black-box AI**: Fails to provide user-understandable explanations, hindering trust and adoption in critical applications \\cite{sheth2022243}.\n        *   **Other knowledge forms**: While knowledge graphs are structured, they are not inherently ordered. Semantic lexicons add context but cannot enforce conceptual flow. Ontologies can provide stricter control but process knowledge offers a unique, explicit ordering of steps \\cite{sheth2022243}.\n        *   **Deep generative language models**: Prone to hallucination, incoherent, irrelevant, and factually incorrect responses, and unsafe risk behaviors, especially in conversational settings \\cite{sheth2022243}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes \"Process Knowledge-Infused AI,\" which explicitly integrates \"Process Knowledge\" \u2013 an ordered set of steps or information mapping to evidence-based guidelines \u2013 into AI algorithms \\cite{sheth2022243}. This infusion is part of Knowledge-infused Learning (KiL), a neuro-symbolic approach.\n    *   **Novelty/Difference**:\n        *   **Explicit Guideline Adherence**: Unlike implicit learning from data, this approach explicitly controls model learning to recover and adhere to expert guidelines or processes \\cite{sheth2022243}.\n        *   **Deterministic Nature**: Enforces a deterministic nature in AI decision-making, crucial for user-level explanations, uncertainty handling, and safety \\cite{sheth2022243}.\n        *   **Process Knowledge as Constraints**: Process knowledge is modeled as meta-information to capture sequential context, or as explicit constraints (e.g., Textual Entailment Constraints, Rules with Tags and Ranks) to guide question generation or recommendation processes \\cite{sheth2022243}.\n        *   **User-level Explainability**: Enables explanations by querying, traversing, and mapping high-importance features to concepts in knowledge graphs (e.g., SNOMED-CT) and by following the explicit steps of the infused process knowledge \\cite{sheth2022243}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Concept of Process Knowledge**: Defines \"Process Knowledge\" as an ordered set of information reflecting evidence-based guidelines or expert conceptual understanding, distinct from other knowledge forms like knowledge graphs or ontologies \\cite{sheth2022243}.\n    *   **Framework for Process Knowledge Infusion**: Introduces a conceptual framework for infusing process knowledge into statistical AI systems, particularly for Natural Language Generation (NLG) and classification tasks \\cite{sheth2022243}.\n    *   **Modeling Process Knowledge**: Demonstrates various representations of process knowledge, such as flowcharts (e.g., C-SSRS for suicide risk), flattened structures (e.g., GAD-7 for anxiety), and rule-based systems (e.g., \"Tag and Rank\" rules for conversational flow) \\cite{sheth2022243}.\n    *   **Process Knowledge as Constraints for NLG**: Proposes using process knowledge to define explicit constraints (e.g., Textual Entailment Constraints, conditional probability rules with Tags and Ranks) to control and make safe the question generation process in conversational AI, addressing issues like hallucination and unsafe responses \\cite{sheth2022243}.\n    *   **User-level Explainability and Safety Mechanisms**: Outlines how process knowledge infusion directly contributes to user-level explanations (by showing the path taken through guidelines) and ensures safety (by preventing medically incorrect or harmful generations) \\cite{sheth2022243}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted (Conceptual/Demonstrative)**: The paper primarily presents conceptual validation through two demanding use cases, illustrating *how* process knowledge would be applied and its benefits, rather than reporting quantitative experimental results from a fully implemented system \\cite{sheth2022243}.\n    *   **Use Cases**:\n        *   **Mental Health Triaging**: Demonstrated using clinical guidelines like the Columbia Suicide Severity Rating Scale (C-SSRS) (Figure 1, Figure 4) and GAD-7/PHQ-9 for anxiety/depression assessment. This shows how an AI agent can map user input to a sequence of questions, track cues, and ask appropriate follow-up questions, ensuring medically correct and safe interactions \\cite{sheth2022243}.\n        *   **Food Recipe Recommendations**: Illustrated for diabetes management and general healthy eating. Process knowledge from Dietary Guidelines for Americans or specific diabetic dietary guidelines is used to recommend appropriate meals, generate explanations, and avoid unsafe recommendations (e.g., high CHO from added sugars) \\cite{sheth2022243}. It also considers adverse effects of cooking actions (e.g., deep-frying potatoes producing trans-fats) \\cite{sheth2022243}.\n    *   **Key Performance Metrics (Implied)**: While not explicitly detailed, the paper implies that performance would be evaluated based on metrics related to:\n        *   Adherence to guidelines/processes.\n        *   Safety of generated responses/recommendations.\n        *   User-level explainability and interpretability.\n        *   Reduction in hallucination and unsafe behaviors \\cite{sheth2022243}.\n    *   **Comparison Results (Conceptual)**: The paper conceptually compares the behavior of a process knowledge-infused AI with typical deep statistical language models, highlighting how the former avoids unsafe generations and provides structured, explainable outcomes that the latter cannot \\cite{sheth2022243}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations (of current AI addressed by this work)**: The paper highlights the inherent limitations of current data-centric AI, including its black-box nature, inability to follow multi-step processes, proneness to hallucination, and generation of unsafe or inappropriate content \\cite{sheth2022243}.\n    *   **Assumptions (of the proposed approach)**: Assumes the availability of well-defined process knowledge (expert guidelines, clinical questionnaires) that can be formalized and infused into AI systems \\cite{sheth2022243}.\n    *   **Scope of Applicability**: Primarily targets high-value, sensitive, and safety-critical domains where adherence to expert guidelines, user trust, and explicit explanations are essential. Examples include mental health, personalized nutrition, and other healthcare applications \\cite{sheth2022243}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: Significantly advances the technical state-of-the-art in neuro-symbolic AI by introducing \"Process Knowledge\" as a distinct and critical form of knowledge for AI systems \\cite{sheth2022243}. It moves beyond purely data-driven or general knowledge graph approaches to explicitly incorporate procedural expertise.\n    *   **Addresses Critical AI Challenges**: Provides a foundational approach to tackle long-standing challenges in AI regarding safety, user-level explainability, and interpretability, especially in sensitive domains where current AI falls short \\cite{sheth2022243}.\n    *   **Enables Trustworthy AI**: By enforcing adherence to expert guidelines and providing transparent decision-making paths, it lays the groundwork for developing more trustworthy and reliable AI systems \\cite{sheth2022243}.\n    *   **Potential Impact on Future Research**: Opens new avenues for research in:\n        *   Developing novel neuro-symbolic algorithms that explicitly learn and recover expert guidelines \\cite{sheth2022243}.\n        *   Creating specialized datasets integrated with process knowledge \\cite{sheth2022243}.\n        *   Designing AI systems that are inherently context-sensitive, adaptable, and transferable across tasks within a domain \\cite{sheth2022243}.\n        *   Improving the safety and ethical deployment of AI in critical applications.",
        "keywords": [
            "Process Knowledge-Infused AI",
            "Process Knowledge",
            "Neuro-Symbolic AI",
            "Knowledge-infused Learning",
            "User-level Explainability",
            "AI Safety",
            "Explicit Guideline Adherence",
            "Hallucination Reduction",
            "Safety-critical applications",
            "Mental Health Triaging",
            "Natural Language Generation (NLG)",
            "Textual Entailment Constraints",
            "Trustworthy AI"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **presents new methods, algorithms, or systems:** the abstract explicitly states that \"process knowledge infusion develops a new and complementary set of methods, datasets, and evaluation methods.\" the introduction further discusses how this approach would \"yield a new class of neuro-symbolic algorithms\" and describes \"methods for infusing process knowledge into statistical ai systems.\" it details \"process knowledge as constraints\" with specific rules and augmentation of probability functions.\n*   **discusses technical problem, proposed solution:** the paper identifies problems with current ai systems (e.g., hallucination in nlg, lack of user-level explainability, safety concerns) and proposes \"process knowledge-infused ai\" as a solution, detailing *how* this infusion works and its benefits.\n*   **mentions \"propose,\" \"develop,\" \"present,\" \"algorithm,\" \"method\":** these keywords are directly or indirectly present throughout the text, indicating the development and presentation of a new technical approach."
    },
    "43561ac2f7e9612d3d48951f0d2aa290829adf3e.pdf": {
        "title": "Human-centered AI: ensuring human control while increasing automation",
        "authors": [
            "B. Shneiderman"
        ],
        "published_date": "2022",
        "abstract": "A new synthesis is emerging that integrates Artificial Intelligence (AI) technologies with Human-Computer Interaction to produce Human-Centered AI (HCAI). Advocates of this new synthesis seek to amplify, augment, and enhance human abilities, so as to empower people, build their self-efficacy, support creativity, recognize responsibility, and promote social connections. Researchers, developers, business leaders, policy makers and others are expanding the technology-centered scope of AI to include HCAI ways of thinking. This expansion from an algorithm-focused view to embrace a human-centered perspective, can shape the future of technology so as to better serve human needs. Educators, designers, software engineers, product managers, evaluators, and government agency staffers can build on AI-driven technologies to design products and services that make life better for the users. These human-centered products and services will enable people to better care for each other, build sustainable communities, and restore the environment. The passionate advocates of HCAI are devoted to furthering human values, rights, justice, and dignity, by building reliable, safe, and trustworthy systems. Early hypertext systems required user assigned links for text files, giving full control to users, while providing readers with an understandable and predictable design. However, innovators quickly realized that there were many strategies to improve hypertext designs by giving users spatial presentations of the related documents, recommendations for links, ways to collaborate, and interactive animated graphical presentations. Other features supported history-keeping, note-taking, and audio for all users, but especially for users with visual disabilities. Over time improved hypertext systems incorporated machine learning and other artificial intelligence techniques that provided automation of features, but sometimes produced unexpected and incomprehensible results. Current strategies are to give users more control by providing previews of potential traversals, reminders, alerts, and suggestions that guide human reflection about their goals and methods. Atzenbeck et al. suggest that hypertext is a method of inquiry, opening the door to creativity support tools that accelerate exploration and discovery, amplified by the Al-infused supertools of Human-Centered AI [1]. A medical hypertext scenario could enable a physician to provide a patient history, lab tests, and current symptoms as a starting point. The hypertext system could respond with a set of possible diagnoses, which could be selected by the physician, leading to a refined analysis, links to recent clinical trial results, suggestions of consulting specialists, and recommendations for leading treatment centers. The physician could share the analysis with teammates or specialists to get feedback. The physician's exploration records could be saved to the patient's history, so that the treatment plan could be formulated based on reliable resources and then refined by discussions with patients, who would be given links to patient-centered descriptions of the diagnosis and treatment plan. The physician is responsible for what happens, but this scenario provides a strong history for retrospective analyzes of the choices that were made and the outcomes. If human-centered AI design scenarios like this one are oriented to amplifying, augmenting, empowering and enhancing human performance, then the chance of successful outcomes will increase. The passionate advocates of HCAI are devoted to furthering human values, rights, justice, and dignity, by building reliable, safe, and trustworthy systems. The talk will include examples, references to further work, and discussion time for questions. These ideas are drawn from Ben Shneiderman's new book Human-Centered AI [6]. Further information at: https://hcil.umd.edu/human-centered-ai",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/43561ac2f7e9612d3d48951f0d2aa290829adf3e.pdf",
        "venue": "HUMAN@HT",
        "citationCount": 40,
        "score": 13.333333333333332,
        "summary": "A new synthesis is emerging that integrates Artificial Intelligence (AI) technologies with Human-Computer Interaction to produce Human-Centered AI (HCAI). Advocates of this new synthesis seek to amplify, augment, and enhance human abilities, so as to empower people, build their self-efficacy, support creativity, recognize responsibility, and promote social connections. Researchers, developers, business leaders, policy makers and others are expanding the technology-centered scope of AI to include HCAI ways of thinking. This expansion from an algorithm-focused view to embrace a human-centered perspective, can shape the future of technology so as to better serve human needs. Educators, designers, software engineers, product managers, evaluators, and government agency staffers can build on AI-driven technologies to design products and services that make life better for the users. These human-centered products and services will enable people to better care for each other, build sustainable communities, and restore the environment. The passionate advocates of HCAI are devoted to furthering human values, rights, justice, and dignity, by building reliable, safe, and trustworthy systems. Early hypertext systems required user assigned links for text files, giving full control to users, while providing readers with an understandable and predictable design. However, innovators quickly realized that there were many strategies to improve hypertext designs by giving users spatial presentations of the related documents, recommendations for links, ways to collaborate, and interactive animated graphical presentations. Other features supported history-keeping, note-taking, and audio for all users, but especially for users with visual disabilities. Over time improved hypertext systems incorporated machine learning and other artificial intelligence techniques that provided automation of features, but sometimes produced unexpected and incomprehensible results. Current strategies are to give users more control by providing previews of potential traversals, reminders, alerts, and suggestions that guide human reflection about their goals and methods. Atzenbeck et al. suggest that hypertext is a method of inquiry, opening the door to creativity support tools that accelerate exploration and discovery, amplified by the Al-infused supertools of Human-Centered AI [1]. A medical hypertext scenario could enable a physician to provide a patient history, lab tests, and current symptoms as a starting point. The hypertext system could respond with a set of possible diagnoses, which could be selected by the physician, leading to a refined analysis, links to recent clinical trial results, suggestions of consulting specialists, and recommendations for leading treatment centers. The physician could share the analysis with teammates or specialists to get feedback. The physician's exploration records could be saved to the patient's history, so that the treatment plan could be formulated based on reliable resources and then refined by discussions with patients, who would be given links to patient-centered descriptions of the diagnosis and treatment plan. The physician is responsible for what happens, but this scenario provides a strong history for retrospective analyzes of the choices that were made and the outcomes. If human-centered AI design scenarios like this one are oriented to amplifying, augmenting, empowering and enhancing human performance, then the chance of successful outcomes will increase. The passionate advocates of HCAI are devoted to furthering human values, rights, justice, and dignity, by building reliable, safe, and trustworthy systems. The talk will include examples, references to further work, and discussion time for questions. These ideas are drawn from Ben Shneiderman's new book Human-Centered AI [6]. Further information at: https://hcil.umd.edu/human-centered-ai",
        "keywords": []
    },
    "b9492da3d4c0d80ff83b2b661d774b0f20b9da11.pdf": {
        "title": "Will the EU Medical Device Regulation help to improve the safety and performance of medical AI devices?",
        "authors": [
            "E. Niemiec"
        ],
        "published_date": "2022",
        "abstract": "Concerns have been raised over the quality of evidence on the performance of medical artificial intelligence devices, including devices that are already on the market in the USA and Europe. Recently, the Medical Device Regulation, which aims to set high standards of safety and quality, has become applicable in the European Union. The aim of this article is to discuss whether, and how, the Medical Device Regulation will help improve the safety and performance of medical artificial intelligence devices entering the market. The Medical Device Regulation introduces new rules for risk classification of the devices, which will result in more devices subjected to a higher degree of scrutiny before entering the market; more stringent requirements on clinical evaluation, including the requirement for appraisal of clinical data; new requirements for post-market surveillance, which may help spot early on any new, unexpected side effects and risks of the devices; and requirements for notified bodies, including for expertise of the personnel and consideration of relevant best practice documents. The guidance of the Medical Device Coordination Group on clinical evaluation of medical device software and the MEDDEV2.7 guideline on clinical evaluation also attend to some of the problems identified in studies on medical artificial intelligence devices. The Medical Device Regulation will likely help improve the safety and performance of the medical artificial intelligence devices on the European market. The impact of the Regulation, however, is also dependent on its adequate enforcement by the European Union member states.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/b9492da3d4c0d80ff83b2b661d774b0f20b9da11.pdf",
        "venue": "Digital Health",
        "citationCount": 37,
        "score": 12.333333333333332,
        "summary": "Concerns have been raised over the quality of evidence on the performance of medical artificial intelligence devices, including devices that are already on the market in the USA and Europe. Recently, the Medical Device Regulation, which aims to set high standards of safety and quality, has become applicable in the European Union. The aim of this article is to discuss whether, and how, the Medical Device Regulation will help improve the safety and performance of medical artificial intelligence devices entering the market. The Medical Device Regulation introduces new rules for risk classification of the devices, which will result in more devices subjected to a higher degree of scrutiny before entering the market; more stringent requirements on clinical evaluation, including the requirement for appraisal of clinical data; new requirements for post-market surveillance, which may help spot early on any new, unexpected side effects and risks of the devices; and requirements for notified bodies, including for expertise of the personnel and consideration of relevant best practice documents. The guidance of the Medical Device Coordination Group on clinical evaluation of medical device software and the MEDDEV2.7 guideline on clinical evaluation also attend to some of the problems identified in studies on medical artificial intelligence devices. The Medical Device Regulation will likely help improve the safety and performance of the medical artificial intelligence devices on the European market. The impact of the Regulation, however, is also dependent on its adequate enforcement by the European Union member states.",
        "keywords": []
    },
    "4c2983fed4f4efde7d4fe2ba53319010ef2c51bb.pdf": {
        "title": "A dataset of simulated patient-physician medical interviews with a focus on respiratory cases",
        "authors": [
            "Faiha Fareez",
            "Tishya Parikh",
            "Christopher Wavell",
            "Saba Shahab",
            "Meghan Chevalier",
            "Scott Good",
            "Isabella De Blasi",
            "Rafik Rhouma",
            "Christopher McMahon",
            "Jean-Paul Lam",
            "T. Lo",
            "Christopher W. Smith"
        ],
        "published_date": "2022",
        "abstract": "Artificial Intelligence (AI) is playing a major role in medical education, diagnosis, and outbreak detection through Natural Language Processing (NLP), machine learning models and deep learning tools. However, in order to train AI to facilitate these medical fields, well-documented and accurate medical conversations are needed. The dataset presented covers a series of medical conversations in the format of Objective Structured Clinical Examinations (OSCE), with a focus on respiratory cases in audio format and corresponding text documents. These cases were simulated, recorded, transcribed, and manually corrected with the underlying aim of providing a comprehensive set of medical conversation data to the academic and industry community. Potential applications include speech recognition detection for speech-to-text errors, training NLP models to extract symptoms, detecting diseases, or for educational purposes, including training an avatar to converse with healthcare professional students as a standardized patient during clinical examinations. The application opportunities for the presented dataset are vast, given that this calibre of data is difficult to access and costly to develop. Measurement(s) conversations Technology Type(s) audio recording and transcription Factor Type(s) N/A Sample Characteristic - Organism simulated medical exams Sample Characteristic - Environment simulation Sample Characteristic - Location simulation Measurement(s) conversations Technology Type(s) audio recording and transcription Factor Type(s) N/A Sample Characteristic - Organism simulated medical exams Sample Characteristic - Environment simulation Sample Characteristic - Location simulation",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/4c2983fed4f4efde7d4fe2ba53319010ef2c51bb.pdf",
        "venue": "Scientific Data",
        "citationCount": 35,
        "score": 11.666666666666666,
        "summary": "Here's a focused summary of the paper \\cite{fareez202251c} for a literature review, emphasizing technical innovations and empirical validation:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The lack of robust, accurately documented, and accessible medical conversation data (specifically audio files with corresponding processed and transcribed text documents) for training Artificial Intelligence (AI) and Natural Language Processing (NLP) models in medicine.\n    *   **Importance and Challenge**: AI/NLP is increasingly vital for medical education, diagnosis, and outbreak detection. However, existing medical conversation data is often confidential, costly to develop, prone to speech-to-text errors, lacks clear speaker identification, or is only available in text format without the nuances of live conversation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Previous Speech Recognition (SR) software studies showed high error rates (7.4\u201365%) in medical contexts \\cite{fareez202251c}.\n        *   Controlled environment recordings still suffered from conversational disfluencies (false starts, pauses, repetitions) \\cite{fareez202251c}.\n        *   SR trained on medical dictations had higher error rates due to grammatical differences and lack of punctuation compared to conversations \\cite{fareez202251c}.\n        *   Existing large-scale datasets like MedDialog are often text-only, lack a structured approach (like OSCEs), and are derived from online forums rather than live interactions \\cite{fareez202251c}.\n        *   The Bristol Archive Project offers video-recorded consultations but requires ethics approval for access and has generalizability limitations due to its specific patient population \\cite{fareez202251c}.\n    *   **Limitations of Previous Solutions**: High SR error rates, challenges with conversational speech (disfluencies, unclear speaker transitions), lack of structured data, text-only formats, and significant barriers to access due to confidentiality, regulations, and cost \\cite{fareez202251c}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: Creation of a comprehensive dataset of 272 simulated patient-physician medical interviews, primarily focusing on respiratory cases, conducted in the Objective Structured Clinical Examination (OSCE) format. The dataset includes both high-quality audio recordings and meticulously manually corrected text transcripts.\n    *   **Novelty/Difference**:\n        *   **Structured Data Generation**: Utilizes the OSCE format, known for its objectivity, consistency, and ability to handle unpredictable patient behavior, to simulate realistic yet structured medical conversations \\cite{fareez202251c}.\n        *   **Dual-Modality Data**: Provides both high-quality audio (MP3) and corresponding text transcripts, enabling research in both speech processing and NLP \\cite{fareez202251c}.\n        *   **Rigorous Manual Correction**: Transcripts undergo extensive manual correction for speech-to-text errors (spelling, grammar, punctuation), inclusion of missed key information (e.g., \"sexual\" health inquiries), and explicit speaker identification (\"D\" for doctor, \"P\" for patient) \\cite{fareez202251c}.\n        *   **Controlled Recording Environment**: Efforts were made to minimize disfluencies and ensure high-quality audio, addressing a known challenge in conversational speech data \\cite{fareez202251c}.\n        *   **Domain-Specific Focus**: Prioritizes respiratory cases, chosen based on prevalence and mortality, including contemporary conditions like COVID-19, making it highly relevant for specific medical AI applications \\cite{fareez202251c}.\n        *   **Public Accessibility**: The dataset is made publicly available, directly addressing the major roadblock of accessing high-calibre medical conversation data \\cite{fareez202251c}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Dataset Creation Methodology**: A multi-stage process involving structured OSCE simulations, audio cleaning (trimming extraneous information), automated transcription, meticulous manual correction by medical professionals, and a two-person quality control review \\cite{fareez202251c}.\n    *   **High-Fidelity Data Resource**: Delivers a unique, high-quality dataset of medical conversations that overcomes common issues of speech-to-text errors, disfluencies, and lack of speaker identification, making it exceptionally suitable for training robust NLP and speech recognition models \\cite{fareez202251c}.\n    *   **Standardized Content**: The OSCE format ensures that physicians follow a consistent approach to history-taking (symptoms, onset, location, severity, etc.), providing a structured basis for information extraction \\cite{fareez202251c}.\n\n5.  **Experimental Validation**\n    *   **Dataset Quality Assurance**: The paper details the rigorous process used to ensure the quality and accuracy of the dataset itself, rather than validating an external model.\n        *   **OSCE Format Validation**: The use of OSCEs inherently facilitated objectivity, consistency, and organization in the simulated medical conversations \\cite{fareez202251c}.\n        *   **Multi-Stage Manual Correction & Review**: Transcripts were initially corrected for speech-to-text errors, grammar, punctuation, and content gaps. A subsequent quality control step involved a team of two exhaustively reviewing all audio files while simultaneously editing the transcripts to ensure perfect alignment and accuracy \\cite{fareez202251c}.\n        *   **Physician Blinding**: The acting physicians were blinded to the final diagnosis during simulations to ensure realistic questioning and prevent leading questions, mimicking real clinical settings \\cite{fareez202251c}.\n        *   **Structured History Taking**: Physicians were instructed to follow a standard history-taking format, ensuring comprehensive and consistent data collection across cases \\cite{fareez202251c}.\n\n6.  **Limitations & Scope**\n    *   **Limited Non-Respiratory Cases**: The dataset has a small number of cases for musculoskeletal, cardiac, dermatological, and gastrointestinal illnesses, limiting its applicability for training models in these specific domains \\cite{fareez202251c}.\n    *   **Absence of Assessment Pressure**: The simulated OSCEs lacked the pressure of formal assessment, which might affect the realism of dialogue for NLP models specifically designed for evaluative contexts \\cite{fareez202251c}.\n    *   **Voice-Age Mismatch**: Medical students/residents (in their twenties) portrayed patients of various ages, potentially leading to voice-age mismatches that could impact speech recognition accuracy for specific demographic voice profiles \\cite{fareez202251c}.\n    *   **Simulated Nature**: The data is simulated, not derived from real patient encounters, which might limit its generalizability to highly nuanced or emotionally charged real-world clinical interactions.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This dataset significantly advances the technical state-of-the-art by providing a high-quality, structured, and publicly accessible resource that directly addresses a critical data bottleneck for AI/NLP development in medicine \\cite{fareez202251c}.\n    *   **Potential Impact on Future Research**:\n        *   **Improved Speech Recognition**: Enables the development and testing of more accurate speech recognition tools for medical conversations, including detection and correction of speech-to-text errors \\cite{fareez202251c}.\n        *   **Advanced NLP Models**: Facilitates training NLP models for tasks such as Named-Entity Recognition (NER), symptom extraction, disease detection, and building sophisticated educational models (e.g., AI avatars for OSCEs) \\cite{fareez202251c}.\n        *   **End-to-End Systems**: Supports the creation of comprehensive AI systems from initial symptom extraction to final disease classification \\cite{fareez202251c}.\n        *   **Reduced Development Costs**: By providing a free, high-quality dataset, it lowers the barrier to entry for researchers and institutions, accelerating innovation in medical AI \\cite{fareez202251c}.",
        "keywords": [
            "Medical conversation data",
            "AI/NLP models",
            "Speech Recognition",
            "Objective Structured Clinical Examination (OSCE)",
            "simulated patient-physician interviews",
            "dual-modality dataset",
            "manual transcript correction",
            "speaker identification",
            "respiratory cases",
            "publicly accessible dataset",
            "high-fidelity data resource",
            "data bottleneck",
            "symptom extraction"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the title \"a dataset of simulated patient-physician medical interviews...\" and the venue \"scientific data\" strongly indicate that the paper's primary purpose is to present a new dataset.\n*   the abstract mentions \"the dataset presented cover...\" and the introduction states \"providing a comprehensive set of medical conversation data\" and \"a team... created this dataset. the medical interviews were recorded...\".\n*   this paper describes the methodology for creating and documenting a new data resource. while it doesn't present \"findings\" from analyzing the data, the systematic creation and documentation of data is a core component of empirical research, providing the foundation for future data-driven studies.\n\nthis aligns best with the **empirical** classification, as it describes a \"data-driven study\" in the sense of creating and documenting a significant data resource.\n\n**classification: empirical**"
    },
    "1adb84ff58a2ed6d7a5345820489997edef66033.pdf": {
        "title": "What Does This Acronym Mean? Introducing a New Dataset for Acronym Identification and Disambiguation",
        "authors": [
            "Amir Pouran Ben Veyseh",
            "Franck Dernoncourt",
            "Quan Hung Tran",
            "Thien Huu Nguyen"
        ],
        "published_date": "2020",
        "abstract": "Acronyms are the short forms of phrases that facilitate conveying lengthy sentences in documents and serve as one of the mainstays of writing. Due to their importance, identifying acronyms and corresponding phrases (i.e., acronym identification (AI)) and finding the correct meaning of each acronym (i.e., acronym disambiguation (AD)) are crucial for text understanding. Despite the recent progress on this task, there are some limitations in the existing datasets which hinder further improvement. More specifically, limited size of manually annotated AI datasets or noises in the automatically created acronym identification datasets obstruct designing advanced high-performing acronym identification models. Moreover, the existing datasets are mostly limited to the medical domain and ignore other domains. In order to address these two limitations, we first create a manually annotated large AI dataset for scientific domain. This dataset contains 17,506 sentences which is substantially larger than previous scientific AI datasets. Next, we prepare an AD dataset for scientific domain with 62,441 samples which is significantly larger than previous scientific AD dataset. Our experiments show that the existing state-of-the-art models fall far behind human-level performance on both datasets proposed by this work. In addition, we propose a new deep learning model which utilizes the syntactical structure of the sentence to expand an ambiguous acronym in a sentence. The proposed model outperforms the state-of-the-art models on the new AD dataset, providing a strong baseline for future research on this dataset.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/1adb84ff58a2ed6d7a5345820489997edef66033.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 58,
        "score": 11.600000000000001,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: The paper addresses two crucial sub-tasks in text understanding: Acronym Identification (AI) \u2013 finding acronyms and their corresponding long forms, and Acronym Disambiguation (AD) \u2013 determining the correct meaning of an ambiguous acronym in context \\cite{veyseh2020cxv}.\n*   **Importance and Challenge**: Acronyms are prevalent, especially in technical documents, and their correct processing is vital for downstream NLP applications like question answering and document retrieval. The problem is challenging due to:\n    *   **Limited and Noisy Datasets**: Existing AI datasets are either small, manually annotated, or large but automatically generated with significant noise, hindering the development of advanced models \\cite{veyseh2020cxv}.\n    *   **Domain Specificity**: Most existing datasets are limited to the medical domain, neglecting the unique challenges and prevalence of acronyms in other scientific domains \\cite{veyseh2020cxv}.\n    *   **Contextual Ambiguity**: Acronyms often have multiple meanings, requiring robust contextual understanding for disambiguation, especially when long-range dependencies are involved \\cite{veyseh2020cxv}.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**: Previous work includes rule-based methods for AI, feature-based models (SVM, Naive Bayes) for AD, and more recent deep learning methods \\cite{veyseh2020cxv}.\n*   **Limitations of Previous Solutions**:\n    *   **Dataset Quality and Size**: Existing AI datasets are either too small for deep learning or suffer from noise due to heuristic generation, failing to capture diverse acronym forms \\cite{veyseh2020cxv}.\n    *   **Domain Bias**: A significant portion of existing datasets is confined to the medical domain, limiting generalizability to other scientific fields \\cite{veyseh2020cxv}.\n    *   **Context Encoding in AD Models**: Traditional feature-based AD models struggle with effective context representation, while current deep learning models (LSTMs, language models) often fail to capture long-range dependencies crucial for disambiguation due to vanishing gradient issues \\cite{veyseh2020cxv}.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The paper proposes a novel deep learning model for Acronym Disambiguation (GAD) that explicitly leverages the syntactic structure of sentences using Graph Convolutional Neural Networks (GCNs) \\cite{veyseh2020cxv}.\n    *   **Sentence Encoder**: Uses a Bidirectional Long Short-Term Memory (BiLSTM) network to generate sequential word representations, augmented with POS tag embeddings \\cite{veyseh2020cxv}.\n    *   **Context Encoder (Innovation)**: Employs a GCN to process the dependency tree of the sentence. This component augments word representations with structure-aware contextual information, addressing the limitation of BiLSTMs in capturing long-range dependencies \\cite{veyseh2020cxv}.\n    *   **Prediction**: Concatenates the BiLSTM and GCN representations of the acronym, along with max-pooled sentence representations from both encoders, and feeds them into a two-layer feed-forward classifier to predict the correct long form \\cite{veyseh2020cxv}.\n*   **Novelty/Difference**: The primary innovation lies in the integration of GCNs to explicitly model syntactic dependency structures for acronym disambiguation. This is a novel approach for AD, as prior deep learning models for this task did not exploit dependency trees to enrich contextual information, especially for long-range dependencies \\cite{veyseh2020cxv}.\n\n### 4. Key Technical Contributions\n\n*   **Novel Algorithms/Methods**:\n    *   Introduction of a graph-based acronym disambiguation (GAD) model that utilizes Graph Convolutional Neural Networks (GCNs) to incorporate syntactic dependency structures for improved contextual understanding \\cite{veyseh2020cxv}.\n*   **System Design/Architectural Innovations**:\n    *   A multi-component deep learning architecture for AD combining sequential (BiLSTM) and structural (GCN) context encoders to create a comprehensive representation for ambiguous acronyms \\cite{veyseh2020cxv}.\n*   **Novel Resources**:\n    *   Release of SciAI, the first publicly available and largest manually annotated acronym identification dataset in the scientific domain (17,506 sentences) \\cite{veyseh2020cxv}.\n    *   Release of SciAD, the largest acronym disambiguation dataset in the scientific domain (62,441 samples), curated from humanly verified ambiguous acronym dictionaries \\cite{veyseh2020cxv}.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**:\n    *   Extensive evaluation of state-of-the-art AI and AD models on the newly proposed SciAI and SciAD datasets \\cite{veyseh2020cxv}.\n    *   Comparison of the proposed GAD model against existing state-of-the-art AD models on SciAD, as well as on the UAD Wikipedia dataset and a newly created SciUAD dataset (unsupervisedly extracted from the scientific corpus) \\cite{veyseh2020cxv}.\n*   **Key Performance Metrics and Comparison Results**:\n    *   The experiments demonstrate that existing state-of-the-art models for both AI and AD fall significantly short of human-level performance on the new SciAI and SciAD datasets \\cite{veyseh2020cxv}.\n    *   The proposed GAD model consistently outperforms existing state-of-the-art AD models on the new SciAD dataset, establishing a strong baseline for future research \\cite{veyseh2020cxv}.\n    *   Comparison of SciAD with UAD and SciUAD highlights SciAD's larger number of ambiguous acronyms and higher ambiguity level, indicating its effectiveness in capturing real-world ambiguity \\cite{veyseh2020cxv}.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**:\n    *   The AD dataset creation relies on a \"one-sense-per-discourse\" assumption, meaning an acronym's meaning does not change within a single paper \\cite{veyseh2020cxv}.\n    *   The GAD model's performance is dependent on the quality of dependency parses, which can be noisy \\cite{veyseh2020cxv}.\n*   **Scope of Applicability**:\n    *   The datasets and the proposed model are primarily focused on the **scientific domain**, addressing a gap in existing resources \\cite{veyseh2020cxv}.\n    *   The GAD model is specifically designed for acronym disambiguation, though the underlying GCN approach for long-range dependencies could be applicable to other NLP tasks \\cite{veyseh2020cxv}.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**:\n    *   The paper significantly advances the state-of-the-art by providing large-scale, high-quality, human-annotated datasets (SciAI and SciAD) for acronym identification and disambiguation in the scientific domain, which were previously lacking \\cite{veyseh2020cxv}.\n    *   It introduces a novel deep learning architecture (GAD) that effectively leverages syntactic structures via GCNs to improve acronym disambiguation, outperforming existing models and setting a new baseline \\cite{veyseh2020cxv}.\n*   **Potential Impact on Future Research**:\n    *   The released datasets are crucial resources for training and evaluating more advanced deep learning models for AI and AD, especially in scientific contexts \\cite{veyseh2020cxv}.\n    *   The GAD model's success in integrating syntactic information suggests a promising direction for future research in NLP tasks requiring robust contextual understanding and handling of long-range dependencies \\cite{veyseh2020cxv}.\n    *   The identified gap between human and model performance on the new datasets highlights the need for further innovation in this area \\cite{veyseh2020cxv}.",
        "keywords": [
            "Acronym Identification (AI)",
            "Acronym Disambiguation (AD)",
            "Graph Convolutional Neural Networks (GCNs)",
            "syntactic dependency structures",
            "long-range dependencies",
            "graph-based Acronym Disambiguation (GAD) model",
            "SciAI dataset",
            "SciAD dataset",
            "scientific domain",
            "deep learning architecture",
            "contextual understanding",
            "Bidirectional LSTMs (BiLSTM)",
            "state-of-the-art advancement",
            "high-quality human-annotated datasets"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **new datasets (system/resource):** the abstract explicitly states: \"we \ufb01rst create a manually annotated large ai dataset for scienti\ufb01c domain\" and \"next, we prepare an ad dataset for scienti\ufb01c domain\". creating new, large, manually annotated datasets to address limitations in existing ones is a significant technical contribution, providing new resources for the field. the title itself emphasizes \"introducing a new dataset\".\n2.  **new model (method/algorithm):** the abstract also states: \"in addition, we propose a new deep learning model which utilizes the syntactical structure of the sentence to expand an ambiguous acronym in a sentence.\" this is a clear presentation of a new method or algorithm.\n3.  **empirical evaluation (validation):** while the paper does conduct experiments (\"our experiments show...\", \"the proposed model outperforms...\"), these experiments serve to evaluate the newly created datasets and the newly proposed model. the empirical analysis is a crucial part of validating the technical contributions, but the primary focus is on presenting these new contributions.\n\nthe paper fits the \"technical\" criteria: \"presents new methods, algorithms, or systems.\" the new datasets can be considered new \"systems\" or \"resources,\" and the deep learning model is a new \"method.\""
    },
    "c8f6ca89c0b0e99028d38d73f149cf6d180dbafb.pdf": {
        "title": "Conceptualising Artificial Intelligence as a Digital Healthcare Innovation: An Introductory Review",
        "authors": [
            "Anmol Arora"
        ],
        "published_date": "2020",
        "abstract": "Abstract Artificial intelligence (AI) is widely recognised as a transformative innovation and is already proving capable of outperforming human clinicians in the diagnosis of specific medical conditions, especially in image analysis within dermatology and radiology. These abilities are enhanced by the capacity of AI systems to learn from patient records, genomic information and real-time patient data. Uses of AI range from integrating with robotics to creating training material for clinicians. Whilst AI research is mounting, less attention has been paid to the practical implications on healthcare services and potential barriers to implementation. AI is recognised as a \u201cSoftware as a Medical Device (SaMD)\u201d and is increasingly becoming a topic of interest for regulators. Unless the introduction of AI is carefully considered and gradual, there are risks of automation bias, overdependence and long-term staffing problems. This is in addition to already well-documented generic risks associated with AI, such as data privacy, algorithmic biases and corrigibility. AI is able to potentiate innovations which preceded it, using Internet of Things, digitisation of patient records and genetic data as data sources. These synergies are important in both realising the potential of AI and utilising the potential of the data. As machine learning systems begin to cross-examine an array of databases, we must ensure that clinicians retain autonomy over the diagnostic process and understand the algorithmic processes generating diagnoses. This review uses established management literature to explore artificial intelligence as a digital healthcare innovation and highlight potential risks and opportunities.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c8f6ca89c0b0e99028d38d73f149cf6d180dbafb.pdf",
        "venue": "Medical devices",
        "citationCount": 54,
        "score": 10.8,
        "summary": "Abstract Artificial intelligence (AI) is widely recognised as a transformative innovation and is already proving capable of outperforming human clinicians in the diagnosis of specific medical conditions, especially in image analysis within dermatology and radiology. These abilities are enhanced by the capacity of AI systems to learn from patient records, genomic information and real-time patient data. Uses of AI range from integrating with robotics to creating training material for clinicians. Whilst AI research is mounting, less attention has been paid to the practical implications on healthcare services and potential barriers to implementation. AI is recognised as a \u201cSoftware as a Medical Device (SaMD)\u201d and is increasingly becoming a topic of interest for regulators. Unless the introduction of AI is carefully considered and gradual, there are risks of automation bias, overdependence and long-term staffing problems. This is in addition to already well-documented generic risks associated with AI, such as data privacy, algorithmic biases and corrigibility. AI is able to potentiate innovations which preceded it, using Internet of Things, digitisation of patient records and genetic data as data sources. These synergies are important in both realising the potential of AI and utilising the potential of the data. As machine learning systems begin to cross-examine an array of databases, we must ensure that clinicians retain autonomy over the diagnostic process and understand the algorithmic processes generating diagnoses. This review uses established management literature to explore artificial intelligence as a digital healthcare innovation and highlight potential risks and opportunities.",
        "keywords": []
    },
    "4ed6906e5f0d41ab1ee17bcf225653b61765ec10.pdf": {
        "title": "Artificial intelligence in medical imaging practice in Africa: a qualitative content analysis study of radiographers\u2019 perspectives",
        "authors": [
            "W. Antwi",
            "T. Akudjedu",
            "B. Botwe"
        ],
        "published_date": "2021",
        "abstract": "Purpose Studies have documented the clinical potentials of artificial intelligence (AI) in medical imaging practice to improving patient care. This study aimed to qualitatively explore the perception of radiographers relating to the integration of AI in medical imaging practice in Africa. Methods The study employed a qualitative design using an open-ended online instrument administered between March and August 2020. Participants consisted of radiographers working within Africa during the time of the study. Data obtained were analysed using qualitative content analysis. Six themes of concerns were generated: expectant tool; career insecurity; cost of new technology, equipment preservation and data insecurity; service delivery quality; need for expanding AI awareness. Results A total of 475 valid responses were obtained. Participants demonstrated a positive outlook about AI in relation to clinical quality improvement, competent diagnosis, radiation dose reduction and improvement in research. They however expressed concerns relating to the implementation of this technology, including job security and loss of core professional radiographer skills and roles. In addition, concerns regarding AI equipment maintenance, lack of awareness about AI and education and training opportunities were evident. Conclusion Awareness of the importance of AI in medical imaging practice was acknowledged; however, concerns relating to job security, data protection must be given critical attention for successful implementation of these advanced technologies in medical imaging in Africa. Inclusion of AI modules in the training of future radiographers is highly recommended.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/4ed6906e5f0d41ab1ee17bcf225653b61765ec10.pdf",
        "venue": "Insights into Imaging",
        "citationCount": 43,
        "score": 10.75,
        "summary": "Here is a focused summary of the paper for a literature review, emphasizing its methodological approach and empirical findings regarding perceptions, rather than technical AI innovations.\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** This paper does not address a specific *technical* problem within AI development itself. Instead, it addresses the *socio-technical challenge* of understanding radiographers' perceptions regarding the integration of Artificial Intelligence (AI) into medical imaging practice in Africa \\cite{antwi2021evl}.\n    *   **Importance and Challenge:** AI holds significant potential for improving patient care and efficiency in medical imaging globally. However, successful implementation, especially in low-resource settings like Africa, critically depends on the acceptance and readiness of the end-users (radiographers). Previous studies on this topic were predominantly quantitative, lacking the depth required to fully understand these complex perceptions and concerns \\cite{antwi2021evl}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon existing studies that have explored radiographers' perspectives on AI integration in medical imaging \\cite{antwi2021evl}.\n    *   **Limitations of Previous Solutions:** The paper highlights that prior research largely employed quantitative methodologies, which provided limited depth into the nuanced perceptions of healthcare professionals regarding AI \\cite{antwi2021evl}. This study positions itself as a qualitative exploration to fill this methodological gap, particularly in the under-researched African context.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm:** The core method is a *qualitative content analysis* of open-ended responses from an online survey \\cite{antwi2021evl}. This is a research methodology for analyzing textual data, not a technical AI algorithm or system.\n    *   **Novelty/Difference:** The innovation lies in the *qualitative design* applied to explore the perceptions of radiographers in Africa, providing in-depth insights into their expectations and concerns about AI. This approach offers a richer understanding than previous quantitative studies, focusing on human factors rather than technological advancements \\cite{antwi2021evl}.\n\n*   **Key Technical Contributions**\n    *   This paper does *not* present novel AI algorithms, system designs, or theoretical insights related to AI *technology*. Its primary contribution is the *identification and qualitative analysis of key themes* representing radiographers' perceptions of AI in Africa \\cite{antwi2021evl}. These themes include:\n        *   AI as an expectant tool for clinical quality improvement.\n        *   Concerns about career insecurity and job displacement.\n        *   Issues related to the cost of new technology.\n        *   Challenges in equipment preservation and data insecurity.\n        *   Potential for improved service delivery quality.\n        *   The urgent need for expanding AI awareness, education, and training \\cite{antwi2021evl}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The study conducted an exploratory cross-sectional online survey using an open-ended instrument administered to radiographers working across Africa between March and August 2020 \\cite{antwi2021evl}.\n    *   **Key Performance Metrics and Comparison Results:** The \"validation\" is qualitative. A total of 475 valid qualitative responses were obtained and analyzed using qualitative content analysis \\cite{antwi2021evl}. The results demonstrated a dual perspective: participants generally held a positive outlook on AI's potential for clinical quality improvement, competent diagnosis, radiation dose reduction, and research enhancement. However, they also expressed significant concerns regarding job security, potential loss of core professional skills, equipment maintenance, data security, and a lack of AI awareness and training opportunities \\cite{antwi2021evl}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The study's qualitative nature means its findings, while rich in depth, may not be broadly generalizable across all African contexts or to other healthcare professions. It relies on self-reported perceptions, which can be subjective \\cite{antwi2021evl}. The online survey format during the COVID-19 pandemic might have influenced participation and responses.\n    *   **Scope of Applicability:** The findings are specifically applicable to understanding the perspectives of radiographers in Africa regarding AI integration in medical imaging \\cite{antwi2021evl}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** This paper advances the understanding of the *human and organizational factors* critical for the successful adoption and integration of AI technologies in healthcare, particularly in diverse and often resource-constrained environments like Africa \\cite{antwi2021evl}. It highlights that technological advancement alone is insufficient without addressing end-user concerns and readiness.\n    *   **Potential Impact on Future Research:** The findings provide crucial insights for policymakers, educators, and technology developers to create targeted strategies for AI implementation in medical imaging in Africa. This includes developing relevant training programs, addressing ethical concerns like job security and data protection, and fostering greater AI awareness to ensure a smooth and beneficial transition to AI-integrated practices \\cite{antwi2021evl}.",
        "keywords": [
            "AI integration in medical imaging",
            "Radiographers' perceptions",
            "Qualitative content analysis",
            "Socio-technical challenge",
            "Africa",
            "low-resource settings",
            "Job security concerns",
            "AI awareness and training",
            "Clinical quality improvement",
            "Human and organizational factors",
            "Data security",
            "Qualitative exploration",
            "Exploratory cross-sectional online survey"
        ],
        "paper_type": "the paper is an **empirical** study.\n\nhere's why:\n\n*   **abstract mentions:** \"this study aimed to qualitatively explore...\", \"the study employed a qualitative design...\", \"participants consisted of radiographers...\", \"data obtained were analysed...\", \"results: a total of 475 valid responses were obtained. participants demonstrated...\", \"findings\" (implied by results and conclusions).\n*   **introduction (first part) discusses:** \"results: a total of 475 valid responses were obtained...\", \"conclusion: awareness... was acknowledged; however, concerns... must be given critical attention...\", \"keywords: ...online surveys, qualitative study\".\n\nthese elements clearly indicate a data-driven study with a defined methodology, participants, data collection, analysis, and findings, which aligns perfectly with the criteria for an empirical paper."
    },
    "55283517de38f4d5870e2f6f4ef42122d2229e5a.pdf": {
        "title": "COVID-19 disease diagnosis with light-weight CNN using modified MFCC and enhanced GFCC from human respiratory sounds",
        "authors": [
            "Lella Kranthi Kumar",
            "P. Alphonse"
        ],
        "published_date": "2022",
        "abstract": "In the last 2 years, medical researchers and clinical scientists have paid close attention to the problem of respiratory sound classification to classify COVID-19 disease symptoms. In the physical world, very few AI-based (Artificial Intelligence) techniques are often used to detect COVID-19/SARS-CoV-2 respiratory disease symptoms from the human respiratory system-generated acoustic sounds such as acoustic voice sound, breathing (inhale and exhale) sounds, and cough sound. We propose a light-weight Convolutional Neural Network (CNN) with Modified-Mel-frequency Cepstral Coefficient (M-MFCC) using different depths and kernel sizes to classify COVID-19 and other respiratory sound disease symptoms such as Asthma, Pertussis, and Bronchitis. The proposed network outperforms conventional feature extraction models and existing Deep Learning (DL) models for COVID-19/SARS-CoV-2 classification accuracy in the range of 4\u201310%. The model\u2019s performance is compared with the COVID-19 crowdsourced benchmark dataset and gives a competitive performance. We applied different receptive fields and depths in the proposed model to get different contextual information that should aid in classification. And our experiments suggested 1 \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\times $$\\end{document}\u00d7 12 receptive fields and a depth of 5-Layer for the light-weight CNN to extract and identify the features from respiratory sound data. The model is also trained and tested with different modalities of data to showcase its effectiveness in classification.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/55283517de38f4d5870e2f6f4ef42122d2229e5a.pdf",
        "venue": "The European Physical Journal Special Topics",
        "citationCount": 32,
        "score": 10.666666666666666,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical technical problem of accurately and efficiently classifying COVID-19 disease symptoms, alongside other respiratory conditions like Asthma, Pertussis, and Bronchitis, using human respiratory acoustic sounds (voice, breathing, cough) \\cite{kumar2022ski}.\n    *   This problem is important due to the global impact of the COVID-19 pandemic, the high death toll, and the urgent need for inexpensive, quick, and flexible screening technologies, especially given the difficulties in data collection and contact tracing \\cite{kumar2022ski}. It is challenging because existing AI-based techniques are often limited in their application to real-world, unregulated crowdsourced acoustic data for multi-disease diagnosis \\cite{kumar2022ski}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches include various AI/DL/ML frameworks using SVM, VGGNet, RNNs, and 1D CNNs, often combined with feature extraction techniques like MFCC, for binary COVID-19 detection or analysis of cough/breath sounds \\cite{kumar2022ski}. Some works focused on mobile applications for data collection and initial screening \\cite{kumar2022ski}.\n    *   Limitations of previous solutions include a focus on a small number of basic features, infrequent discussion of newly emerging respiratory sound features, and a lack of robust, appropriate prediction methods for classifying SARS-CoV-2 symptoms from diverse, unregulated crowdsourced data \\cite{kumar2022ski}.\n    *   This work positions itself by focusing on \"deep sound extracting features\" and covering a \"broader range of respiratory sound features\" through novel modified techniques, aiming for more effective performance on crowdsourced COVID-19 sound data \\cite{kumar2022ski}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is a light-weight Convolutional Neural Network (CNN) designed for classifying COVID-19 and other respiratory diseases \\cite{kumar2022ski}.\n    *   The innovation lies in its feature extraction methods:\n        *   **Modi\ufb01ed-Mel-frequency Cepstral Coe\ufb03cient (M-MFCC)**: An enhanced version of the conventional MFCC for extracting in-depth features \\cite{kumar2022ski}.\n        *   **Enhanced-Gamma-tone Frequency Cepstral Coe\ufb03cients (EGFCC)**: A novel technique based on Gammatone filter banks, designed to capture transient respiratory audio sound features \\cite{kumar2022ski}.\n    *   The light-weight CNN architecture is optimized by exploring different depths and kernel sizes (receptive fields), with experiments suggesting optimal performance with `1x12` receptive fields and a 5-layer depth for extracting and identifying features from respiratory sound data \\cite{kumar2022ski}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of M-MFCC and EGFCC as advanced, specialized feature extraction techniques for comprehensive analysis of human respiratory sounds \\cite{kumar2022ski}.\n    *   **System Design/Architectural Innovations**: Development of a light-weight CNN model specifically tailored for respiratory sound classification, with empirically determined optimal receptive fields and network depth for efficiency and accuracy \\cite{kumar2022ski}.\n    *   **Enhanced Feature Representation**: The combination of M-MFCC and EGFCC allows for the extraction of both in-depth and transient acoustic features, which are crucial for distinguishing between various respiratory conditions \\cite{kumar2022ski}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The proposed light-weight CNN model, utilizing M-MFCC and EGFCC features, was trained and tested on a crowdsourced dataset \\cite{kumar2022ski}. Its performance was compared against conventional feature extraction models and existing Deep Learning (DL) models \\cite{kumar2022ski}. The model was also evaluated across different data modalities (breath, cough, voice) and with varying CNN architectural parameters (receptive fields, depth) \\cite{kumar2022ski}.\n    *   **Dataset**: A crowdsourced \"respiratory COVID-19 sounds\" dataset from Cambridge University, collected via Android and web applications, comprising approximately 70,000 samples from diverse users, including ~325 COVID-19 positive cases \\cite{kumar2022ski}.\n    *   **Key Performance Metrics & Results**: The proposed network demonstrated superior performance, outperforming conventional feature extraction models and existing DL models for COVID-19/SARS-CoV-2 classification accuracy by 4\u201310% \\cite{kumar2022ski}. It achieved competitive performance on the benchmark dataset and was effective in classifying not only COVID-19 but also Asthma, Pertussis, and Bronchitis \\cite{kumar2022ski}. The optimal CNN configuration was identified as having `1x12` receptive fields and a 5-layer depth \\cite{kumar2022ski}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The dataset collected via the Android app contained redundant information, which the authors note needs to be eliminated in future work to improve performance \\cite{kumar2022ski}.\n    *   **Scope of Applicability**: The model is primarily applicable for the early, inexpensive, and rapid screening of COVID-19 and other specified respiratory diseases using acoustic sounds, particularly from crowdsourced and potentially unregulated data sources \\cite{kumar2022ski}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art in acoustic-based medical diagnostics by introducing novel, specialized feature extraction techniques (M-MFCC and EGFCC) and integrating them with an optimized light-weight CNN \\cite{kumar2022ski}. The demonstrated 4-10% accuracy improvement over existing DL models for COVID-19 classification is a notable technical leap \\cite{kumar2022ski}.\n    *   The potential impact is substantial, offering a robust, low-cost, and scalable solution for widespread respiratory health monitoring and early disease identification, particularly relevant for managing pandemics and improving public health screening through integration into common devices \\cite{kumar2022ski}. It also paves the way for future research in advanced acoustic feature engineering and efficient deep learning architectures for medical applications \\cite{kumar2022ski}.",
        "keywords": [
            "COVID-19 classification",
            "respiratory acoustic sounds",
            "light-weight Convolutional Neural Network (CNN)",
            "Modified-Mel-frequency Cepstral Coefficient (M-MFCC)",
            "Enhanced-Gammatone Frequency Cepstral Coefficients (EGFCC)",
            "feature extraction",
            "multi-disease diagnosis",
            "crowdsourced acoustic data",
            "deep learning",
            "acoustic-based medical diagnostics",
            "transient respiratory audio features",
            "optimized CNN architecture",
            "inexpensive screening",
            "accuracy improvement"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"we propose a light-weight convolutional neural network (cnn) with modi\ufb01ed-mel-frequency cepstral coe\ufb03cient (m-mfcc)...\"** this explicitly states the development and presentation of a new method/system.\n2.  the abstract details the components of the proposed network (different depths, kernel sizes) and its performance comparison against existing models.\n3.  the introduction sets up a technical problem (need for inexpensive, quick, flexible covid-19 screening) that the proposed solution aims to address.\n4.  while the paper includes empirical elements (experiments, comparison with benchmark dataset, performance metrics), these serve to validate the effectiveness of the *proposed new method*. the primary focus is on presenting the novel cnn architecture and feature extraction technique.\n\nthis aligns perfectly with the criteria for a **technical** paper.\n\n**classification: technical**"
    },
    "c0fbbaf1c6126846ae5eca37f5129aba395c5ef1.pdf": {
        "title": "Artificial Intelligence Algorithm for Screening Heart Failure with Reduced Ejection Fraction Using Electrocardiography.",
        "authors": [
            "Jinwoo Cho",
            "ByeongTak Lee",
            "J. Kwon",
            "Yeha Lee",
            "Hyunho Park",
            "B. Oh",
            "K. Jeon",
            "Jinsik Park",
            "Kyung-Hee Kim"
        ],
        "published_date": "2020",
        "abstract": "Although heart failure with reduced ejection fraction (HFrEF) is a common clinical syndrome and can be modified by the administration of appropriate medical therapy, there is no adequate tool available to perform reliable, economical, early-stage screening. To meet this need, we developed an interpretable artificial intelligence (AI) algorithm for HFrEF screening using electrocardiography (ECG) and validated its performance. This retrospective cohort study included two hospitals. An AI algorithm based on a convolutional neural network was developed using 39,371 ECG results from 17,127 patients. The internal validation included 3,470 ECGs from 2,908 patients. Furthermore, we conducted external validation using 4,362 ECGs from 4,176 patients from another hospital to verify the applicability of the algorithm across different centers. The end-point was to detect HFrEF, defined as an ejection fraction <40%. We also visualized the regions in 12 lead ECG that affected HFrEF detection in the AI algorithm and compared this to the previously documented literature. During the internal and external validation, the areas under the curves of the AI algorithm using a 12 lead ECG for detecting HFrEF were 0.913 (95% confidence interval, 0.902-0.925) and 0.961 (0.951-0.971), respectively, and the areas under the curves of the AI algorithm using a single-lead ECG were 0.874 (0.859-0.890) and 0.929 (0.911-0.946), respectively. The deep learning-based AI algorithm performed HFrEF detection well using not only a 12 lead but also a single-lead ECG. These results suggest that HFrEF can be screened not only using a 12 lead ECG, as is typical of a conventional ECG machine, but also with a single-lead ECG performed by a wearable device employing the AI algorithm, thereby preventing irreversible disease progression and mortality.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c0fbbaf1c6126846ae5eca37f5129aba395c5ef1.pdf",
        "venue": "ASAIO journal (1992)",
        "citationCount": 53,
        "score": 10.600000000000001,
        "summary": "Although heart failure with reduced ejection fraction (HFrEF) is a common clinical syndrome and can be modified by the administration of appropriate medical therapy, there is no adequate tool available to perform reliable, economical, early-stage screening. To meet this need, we developed an interpretable artificial intelligence (AI) algorithm for HFrEF screening using electrocardiography (ECG) and validated its performance. This retrospective cohort study included two hospitals. An AI algorithm based on a convolutional neural network was developed using 39,371 ECG results from 17,127 patients. The internal validation included 3,470 ECGs from 2,908 patients. Furthermore, we conducted external validation using 4,362 ECGs from 4,176 patients from another hospital to verify the applicability of the algorithm across different centers. The end-point was to detect HFrEF, defined as an ejection fraction <40%. We also visualized the regions in 12 lead ECG that affected HFrEF detection in the AI algorithm and compared this to the previously documented literature. During the internal and external validation, the areas under the curves of the AI algorithm using a 12 lead ECG for detecting HFrEF were 0.913 (95% confidence interval, 0.902-0.925) and 0.961 (0.951-0.971), respectively, and the areas under the curves of the AI algorithm using a single-lead ECG were 0.874 (0.859-0.890) and 0.929 (0.911-0.946), respectively. The deep learning-based AI algorithm performed HFrEF detection well using not only a 12 lead but also a single-lead ECG. These results suggest that HFrEF can be screened not only using a 12 lead ECG, as is typical of a conventional ECG machine, but also with a single-lead ECG performed by a wearable device employing the AI algorithm, thereby preventing irreversible disease progression and mortality.",
        "keywords": []
    },
    "473c62a48d1c2b7c8e41589d29668faf0458ebb3.pdf": {
        "title": "Explicability of artificial intelligence in radiology: Is a fifth bioethical principle conceptually necessary?",
        "authors": [
            "F. Ursin",
            "C. Timmermann",
            "F. Steger"
        ],
        "published_date": "2021",
        "abstract": "Recent years have witnessed intensive efforts to specify which requirements ethical artificial intelligence (AI) must meet. General guidelines for ethical AI consider a varying number of principles important. A frequent novel element in these guidelines, that we have bundled together under the term explicability, aims to reduce the black-box character of machine learning algorithms. The centrality of this element invites reflection on the conceptual relation between explicability and the four bioethical principles. This is important because the application of general ethical frameworks to clinical decision-making entails conceptual questions: Is explicability a free-standing principle? Is it already covered by the well-established four bioethical principles? Or is it an independent value that needs to be recognized as such in medical practice? We discuss these questions in a conceptual-ethical analysis, which builds upon the findings of an empirical document analysis. On the example of the medical specialty of radiology, we analyze the position of radiological associations on the ethical use of medical AI. We address three questions: Are there references to explicability or a similar concept? What are the reasons for such inclusion? Which ethical concepts are referred to?",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/473c62a48d1c2b7c8e41589d29668faf0458ebb3.pdf",
        "venue": "Bioethics",
        "citationCount": 39,
        "score": 9.75,
        "summary": "Recent years have witnessed intensive efforts to specify which requirements ethical artificial intelligence (AI) must meet. General guidelines for ethical AI consider a varying number of principles important. A frequent novel element in these guidelines, that we have bundled together under the term explicability, aims to reduce the black-box character of machine learning algorithms. The centrality of this element invites reflection on the conceptual relation between explicability and the four bioethical principles. This is important because the application of general ethical frameworks to clinical decision-making entails conceptual questions: Is explicability a free-standing principle? Is it already covered by the well-established four bioethical principles? Or is it an independent value that needs to be recognized as such in medical practice? We discuss these questions in a conceptual-ethical analysis, which builds upon the findings of an empirical document analysis. On the example of the medical specialty of radiology, we analyze the position of radiological associations on the ethical use of medical AI. We address three questions: Are there references to explicability or a similar concept? What are the reasons for such inclusion? Which ethical concepts are referred to?",
        "keywords": []
    },
    "dd686a025ecaa43350b67afeea3ad2c4f32527fc.pdf": {
        "title": "The Future Ethics of Artificial Intelligence in Medicine: Making Sense of Collaborative Models",
        "authors": [
            "Torbj\u00f8rn Gundersen",
            "Kristine B\u00e6r\u00f8e"
        ],
        "published_date": "2022",
        "abstract": "This article examines the role of medical doctors, AI designers, and other stakeholders in making applied AI and machine learning ethically acceptable on the general premises of shared decision-making in medicine. Recent policy documents such as the EU strategy on trustworthy AI and the research literature have often suggested that AI could be made ethically acceptable by increased collaboration between developers and other stakeholders. The article articulates and examines four central alternative models of how AI can be designed and applied in patient care, which we call the ordinary evidence model, the ethical design model, the collaborative model, and the public deliberation model. We argue that the collaborative model is the most promising for covering most AI technology, while the public deliberation model is called for when the technology is recognized as fundamentally transforming the conditions for ethical shared decision-making.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/dd686a025ecaa43350b67afeea3ad2c4f32527fc.pdf",
        "venue": "Science and Engineering Ethics",
        "citationCount": 28,
        "score": 9.333333333333332,
        "summary": "This article examines the role of medical doctors, AI designers, and other stakeholders in making applied AI and machine learning ethically acceptable on the general premises of shared decision-making in medicine. Recent policy documents such as the EU strategy on trustworthy AI and the research literature have often suggested that AI could be made ethically acceptable by increased collaboration between developers and other stakeholders. The article articulates and examines four central alternative models of how AI can be designed and applied in patient care, which we call the ordinary evidence model, the ethical design model, the collaborative model, and the public deliberation model. We argue that the collaborative model is the most promising for covering most AI technology, while the public deliberation model is called for when the technology is recognized as fundamentally transforming the conditions for ethical shared decision-making.",
        "keywords": []
    },
    "9993b3d0c2442318af77dd4b3604255d8bf7a8b0.pdf": {
        "title": "Burnout within UK surgical specialties: a systematic review.",
        "authors": [
            "B. Balendran",
            "MF Bath",
            "AI Awopetu",
            "SM Kreckler"
        ],
        "published_date": "2021",
        "abstract": "INTRODUCTION\nBurnout is of growing concern within the surgical workforce, having been shown to result in reduced job satisfaction, decreased patient satisfaction and higher rates of medical errors. Determining the extent of burnout and identifying its risk factors within UK surgical practice is essential to ensure appropriate interventions can be implemented to improve mental wellbeing.\n\n\nMATERIALS\nA systematic search of PubMed, Medline, Embase, PsychINFO and Cochrane databases was performed, following PRISMA guidelines. Studies published between January 2000 and October 2019 that reported prevalence data or risk factors on burnout for surgeons working within the UK and/or the Republic of Ireland were included.\n\n\nFINDINGS\nTen papers met the inclusion criteria. The overall prevalence of burnout amongst surgeons in the UK was 32.0% (IQR 28.9-41.0%), with surgical trainees having the highest prevalence (59.0%) of burnout documented for any subgroup. The most common risk factors identified for burnout were younger surgeon age and lower clinical grade. Being married or living with a partner was found to be protective.\n\n\nCONCLUSIONS\nBurnout is highly prevalent in UK surgical specialties, mostly amongst surgical trainees. Targeted pre-emptive interventions based upon relevant risk factors for burnout should be prioritised, at both individual and institutional levels.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/9993b3d0c2442318af77dd4b3604255d8bf7a8b0.pdf",
        "venue": "Annals of the Royal College of Surgeons of England",
        "citationCount": 35,
        "score": 8.75,
        "summary": "INTRODUCTION\nBurnout is of growing concern within the surgical workforce, having been shown to result in reduced job satisfaction, decreased patient satisfaction and higher rates of medical errors. Determining the extent of burnout and identifying its risk factors within UK surgical practice is essential to ensure appropriate interventions can be implemented to improve mental wellbeing.\n\n\nMATERIALS\nA systematic search of PubMed, Medline, Embase, PsychINFO and Cochrane databases was performed, following PRISMA guidelines. Studies published between January 2000 and October 2019 that reported prevalence data or risk factors on burnout for surgeons working within the UK and/or the Republic of Ireland were included.\n\n\nFINDINGS\nTen papers met the inclusion criteria. The overall prevalence of burnout amongst surgeons in the UK was 32.0% (IQR 28.9-41.0%), with surgical trainees having the highest prevalence (59.0%) of burnout documented for any subgroup. The most common risk factors identified for burnout were younger surgeon age and lower clinical grade. Being married or living with a partner was found to be protective.\n\n\nCONCLUSIONS\nBurnout is highly prevalent in UK surgical specialties, mostly amongst surgical trainees. Targeted pre-emptive interventions based upon relevant risk factors for burnout should be prioritised, at both individual and institutional levels.",
        "keywords": []
    },
    "c726eced731a308d76b52b61cbec074b5323f496.pdf": {
        "title": "Surveillance for Violent Deaths \u2014\u2028National Violent Death Reporting System, 18 States, 2014",
        "authors": [
            "Katherine A. Fowler",
            "Shane P. D. Jack",
            "Bridget H. Lyons",
            "C. Betz",
            "Emiko Petrosky"
        ],
        "published_date": "2018",
        "abstract": "Problem/Condition In 2014, approximately 59,000 persons died in the United States as a result of violence-related injuries. This report summarizes data from CDC\u2019s National Violent Death Reporting System (NVDRS) regarding violent deaths from 18 U.S. states for 2014. Results are reported by sex, age group, race/ethnicity, marital status, location of injury, method of injury, circumstances of injury, and other selected characteristics. Reporting Period Covered 2014. Description of System NVDRS collects data from participating states regarding violent deaths. Data are obtained from death certificates, coroner/medical examiner reports, law enforcement reports, and secondary sources (e.g., child fatality review team data, supplemental homicide reports, hospital data, and crime laboratory data). This report includes data from 18 states that collected statewide data for 2014 (Alaska, Colorado, Georgia, Kentucky, Maryland, Massachusetts, Michigan, New Jersey, New Mexico, North Carolina, Ohio, Oklahoma, Oregon, Rhode Island, South Carolina, Utah, Virginia, and Wisconsin). NVDRS collates documents for each death and links deaths that are related (e.g., multiple homicides, a homicide followed by a suicide, or multiple suicides) into a single incident. Results For 2014, a total of 22,098 fatal incidents involving 22,618 deaths were captured by NVDRS in the 18 states included in this report. The majority of deaths were suicides (65.6%), followed by homicides (22.5%), deaths of undetermined intent (10.0%), deaths involving legal intervention (1.3%) (i.e., deaths caused by law enforcement and other persons with legal authority to use deadly force, excluding legal executions), and unintentional firearm deaths (<1%). The term \u201clegal intervention\u201d is a classification incorporated into the International Classification of Diseases, Tenth Revision (ICD-10) and does not denote the lawfulness or legality of the circumstances surrounding a death caused by law enforcement. Suicides occurred at higher rates among males, non-Hispanic American Indian/Alaska Natives (AI/AN), non-Hispanic whites, persons aged 45\u201354 years, and males aged \u226575 years. Suicides were preceded primarily by a mental health, intimate partner, substance abuse, or physical health problem or a crisis during the previous or upcoming 2 weeks. Homicide rates were higher among males and persons aged <1 year and 15\u201344 years; rates were highest among non-Hispanic black and AI/AN males. Homicides primarily were precipitated by arguments and interpersonal conflicts, occurrence in conjunction with another crime, or related to intimate partner violence (particularly for females). When the relationship between a homicide victim and a suspected perpetrator was known, it was most often either an acquaintance/friend or an intimate partner. Legal intervention death rates were highest among males and persons aged 20\u201344 years; rates were highest among non-Hispanic black males and Hispanic males. Precipitating factors for the majority of legal intervention deaths were alleged criminal activity in progress, the victim reportedly using a weapon in the incident, a mental health or substance abuse problem, an argument or conflict, or a recent crisis. Deaths of undetermined intent occurred more frequently among males, particularly non-Hispanic black and AI/AN males, and persons aged 30\u201354 years. Substance abuse, mental health problems, physical health problems, and a recent crisis were the most common circumstances preceding deaths of undetermined intent. Unintentional firearm deaths were more frequent among males, non-Hispanic whites, and persons aged 10\u201324 years; these deaths most often occurred while the shooter was playing with a firearm and were most often precipitated by a person unintentionally pulling the trigger or mistakenly thinking the firearm was unloaded. Interpretation This report provides a detailed summary of data from NVDRS for 2014. The results indicate that violent deaths resulting from self-inflicted or interpersonal violence disproportionately affected persons aged <65 years, males, and certain minority populations. The primary precipitating factors for homicides and suicides were intimate partner problems, interpersonal conflicts, mental health and substance abuse problems, and recent crises. Public Health Action NVDRS data are used to monitor the occurrence of violence-related fatal injuries and assist public health authorities in the development, implementation, and evaluation of programs and policies to reduce and prevent violent deaths. For example, North Carolina VDRS data were used to improve case ascertainment of pregnancy-associated suicides, Wisconsin VDRS data were used to develop the statewide suicide prevention strategy, and Colorado VDRS data were used to develop programs and prevention strategies for suicide among veterans. The continued development and expansion of NVDRS to include all U.S. states, territories, and the District of Columbia are essential to public health efforts to reduce the impact of violence.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c726eced731a308d76b52b61cbec074b5323f496.pdf",
        "venue": "Morbidity and mortality weekly report. Surveillance summaries",
        "citationCount": 59,
        "score": 8.428571428571429,
        "summary": "Problem/Condition In 2014, approximately 59,000 persons died in the United States as a result of violence-related injuries. This report summarizes data from CDC\u2019s National Violent Death Reporting System (NVDRS) regarding violent deaths from 18 U.S. states for 2014. Results are reported by sex, age group, race/ethnicity, marital status, location of injury, method of injury, circumstances of injury, and other selected characteristics. Reporting Period Covered 2014. Description of System NVDRS collects data from participating states regarding violent deaths. Data are obtained from death certificates, coroner/medical examiner reports, law enforcement reports, and secondary sources (e.g., child fatality review team data, supplemental homicide reports, hospital data, and crime laboratory data). This report includes data from 18 states that collected statewide data for 2014 (Alaska, Colorado, Georgia, Kentucky, Maryland, Massachusetts, Michigan, New Jersey, New Mexico, North Carolina, Ohio, Oklahoma, Oregon, Rhode Island, South Carolina, Utah, Virginia, and Wisconsin). NVDRS collates documents for each death and links deaths that are related (e.g., multiple homicides, a homicide followed by a suicide, or multiple suicides) into a single incident. Results For 2014, a total of 22,098 fatal incidents involving 22,618 deaths were captured by NVDRS in the 18 states included in this report. The majority of deaths were suicides (65.6%), followed by homicides (22.5%), deaths of undetermined intent (10.0%), deaths involving legal intervention (1.3%) (i.e., deaths caused by law enforcement and other persons with legal authority to use deadly force, excluding legal executions), and unintentional firearm deaths (<1%). The term \u201clegal intervention\u201d is a classification incorporated into the International Classification of Diseases, Tenth Revision (ICD-10) and does not denote the lawfulness or legality of the circumstances surrounding a death caused by law enforcement. Suicides occurred at higher rates among males, non-Hispanic American Indian/Alaska Natives (AI/AN), non-Hispanic whites, persons aged 45\u201354 years, and males aged \u226575 years. Suicides were preceded primarily by a mental health, intimate partner, substance abuse, or physical health problem or a crisis during the previous or upcoming 2 weeks. Homicide rates were higher among males and persons aged <1 year and 15\u201344 years; rates were highest among non-Hispanic black and AI/AN males. Homicides primarily were precipitated by arguments and interpersonal conflicts, occurrence in conjunction with another crime, or related to intimate partner violence (particularly for females). When the relationship between a homicide victim and a suspected perpetrator was known, it was most often either an acquaintance/friend or an intimate partner. Legal intervention death rates were highest among males and persons aged 20\u201344 years; rates were highest among non-Hispanic black males and Hispanic males. Precipitating factors for the majority of legal intervention deaths were alleged criminal activity in progress, the victim reportedly using a weapon in the incident, a mental health or substance abuse problem, an argument or conflict, or a recent crisis. Deaths of undetermined intent occurred more frequently among males, particularly non-Hispanic black and AI/AN males, and persons aged 30\u201354 years. Substance abuse, mental health problems, physical health problems, and a recent crisis were the most common circumstances preceding deaths of undetermined intent. Unintentional firearm deaths were more frequent among males, non-Hispanic whites, and persons aged 10\u201324 years; these deaths most often occurred while the shooter was playing with a firearm and were most often precipitated by a person unintentionally pulling the trigger or mistakenly thinking the firearm was unloaded. Interpretation This report provides a detailed summary of data from NVDRS for 2014. The results indicate that violent deaths resulting from self-inflicted or interpersonal violence disproportionately affected persons aged <65 years, males, and certain minority populations. The primary precipitating factors for homicides and suicides were intimate partner problems, interpersonal conflicts, mental health and substance abuse problems, and recent crises. Public Health Action NVDRS data are used to monitor the occurrence of violence-related fatal injuries and assist public health authorities in the development, implementation, and evaluation of programs and policies to reduce and prevent violent deaths. For example, North Carolina VDRS data were used to improve case ascertainment of pregnancy-associated suicides, Wisconsin VDRS data were used to develop the statewide suicide prevention strategy, and Colorado VDRS data were used to develop programs and prevention strategies for suicide among veterans. The continued development and expansion of NVDRS to include all U.S. states, territories, and the District of Columbia are essential to public health efforts to reduce the impact of violence.",
        "keywords": []
    },
    "68d210d739fd92e3b2854a093cb0fc9f4a622bd5.pdf": {
        "title": "Prevalence and risk factors for vascular calcification in Chinese patients receiving dialysis: baseline results from a prospective cohort study",
        "authors": [
            "Zhi-Hong Liu",
            "Xue-Qing Yu",
            "Jun-Wei Yang",
            "Aili Jiang",
            "Bi-cheng Liu",
            "C. Xing",
            "Ji-zhuang Lou",
            "Mei Wang",
            "Hong Cheng",
            "Jun Liu",
            "Jun-zhou Fu",
            "Ai-hua Zhang",
            "Miao-jia Zhang",
            "Qiao-ling Zhou",
            "Chen Yu",
            "Rong Wang",
            "Li Wang",
            "Yuqing Chen",
            "Tian-Jun Guan",
            "Ai Peng",
            "N. Chen",
            "C. Hao",
            "Xu-yang Cheng"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/68d210d739fd92e3b2854a093cb0fc9f4a622bd5.pdf",
        "venue": "Current Medical Research and Opinion",
        "citationCount": 55,
        "score": 7.857142857142857,
        "summary": "",
        "keywords": []
    },
    "7aabd2566ddc3980c07bbd3652ac903f20153a12.pdf": {
        "title": "Legal concerns in health-related artificial intelligence: a scoping review protocol",
        "authors": [
            "Michael Da Silva",
            "T. Horsley",
            "Devin Singh",
            "Emily Ann Da Silva",
            "Valentina Ly",
            "Bryan Thomas",
            "Ryan C. Daniel",
            "Karni Chagal-Feferkorn",
            "Samantha Iantomasi",
            "Kelli White",
            "Arianne Kent",
            "C. Flood"
        ],
        "published_date": "2022",
        "abstract": "Background Medical innovations offer tremendous hope. Yet, similar innovations in governance (law, policy, ethics) are likely necessary if society is to realize medical innovations\u2019 fruits and avoid their pitfalls. As innovations in artificial intelligence (AI) advance at a rapid pace, scholars across multiple disciplines are articulating concerns in health-related AI that likely require legal responses to ensure the requisite balance. These scholarly perspectives may provide critical insights into the most pressing challenges that will help shape and advance future regulatory reforms. Yet, to the best of our knowledge, there is no comprehensive summary of the literature examining legal concerns in relation to health-related AI. We thus aim to summarize and map the literature examining legal concerns in health-related AI using a scoping review approach. Methods The scoping review framework developed by (J Soc Res Methodol 8:19-32, 2005) and extended by (Implement Sci 5:69, 2010) and the Preferred Reporting Items for Systematic Reviews and Meta-Analysis extension for scoping reviews (PRISMA-ScR) guided our protocol development. In close consultation with trained librarians, we will develop a highly sensitive search for MEDLINE\u00ae\u00a0(OVID) and adapt it for multiple databases designed to comprehensively capture texts in law, medicine, nursing, pharmacy, other healthcare professions (e.g., dentistry, nutrition), public health, computer science, and engineering. English- and French-language records will be included if they examine health-related AI, describe or prioritize a legal concern in health-related AI or propose a solution thereto, and were published in 2012 or later. Eligibility assessment will be conducted independently and in duplicate at all review stages. Coded data will be analyzed along themes and stratified across discipline-specific literatures. Discussion This first-of-its-kind scoping review will summarize available literature examining, documenting, or prioritizing legal concerns in health-related AI to advance law and policy reform(s). The review may also reveal discipline-specific concerns, priorities, and proposed solutions to the concerns. It will thereby identify priority areas that should be the focus of future reforms and regulatory options available to stakeholders in reform processes. Trial registration This protocol was submitted to the Open Science Foundation registration database. See https://osf.io/zav7w .",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/7aabd2566ddc3980c07bbd3652ac903f20153a12.pdf",
        "venue": "Systematic Reviews",
        "citationCount": 23,
        "score": 7.666666666666666,
        "summary": "This paper, \"Legal concerns in health-related artificial intelligence: a scoping review protocol\" by Da Silva et al. \\cite{silva2022rp2}, outlines the methodology for a comprehensive scoping review rather than presenting novel technical solutions or empirical results in AI development. Therefore, the analysis below focuses on the *protocol's design* and its contribution to *literature synthesis* on a technical-legal intersection.\n\nHere's a focused summary for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the lack of a comprehensive, systematic overview of *legal concerns* arising from the rapid advancement and deployment of health-related Artificial Intelligence (AI). While medical AI offers significant promise, its integration into healthcare settings introduces complex legal challenges.\n    *   **Importance and Challenge**: This problem is crucial because appropriate governance (law, policy, ethics) is necessary to realize AI's benefits while mitigating risks like diagnostic errors, algorithmic bias, privacy breaches, and liability attribution. The challenge lies in the multidisciplinary nature of these concerns, spanning law, medicine, computer science, and engineering, and the need to understand how existing legal frameworks apply (or fail to apply) to novel AI applications.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The authors acknowledge existing reports and scholarly works that touch upon legal implications or ethical concerns in health-related AI.\n    *   **Limitations of Previous Solutions**:\n        *   No systematic overview: A preliminary search revealed no current or underway *systematic reviews or scoping reviews* specifically on *legal concerns* in health-related AI.\n        *   Distinction from ethical reviews: While ethical issues often overlap with legal ones, the paper argues that legal concerns and responses are distinct and require separate analysis, particularly regarding democratic legitimacy, precedent, and specific legal requirements (e.g., informed consent laws vs. ethical principles of consent).\n        *   Disciplinary silos: Existing literature often comes from specific disciplines, lacking a comprehensive, cross-disciplinary mapping of legal concerns and their prioritization.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (for a review protocol)**: The core method is a rigorous *scoping review protocol* guided by established frameworks (Arksey and O\u2019Malley, Levac et al., PRISMA-ScR). This involves a systematic, multi-stage process:\n        1.  Identifying research questions (primary: what is known about legal concerns; secondary: prioritization, disciplinary differences).\n        2.  Developing a highly sensitive, peer-reviewed search strategy for multiple multidisciplinary databases (MEDLINE\u00ae, EMBASE, HeinOnline, Web of Science, Scopus, IEEE Xplore).\n        3.  Establishing clear eligibility criteria for study selection (English/French, published 2012 onwards, examining health-related AI, describing/prioritizing legal concerns or proposing solutions).\n        4.  Independent and duplicate eligibility assessment at all stages.\n        5.  Planned data charting, collation, summarization, and reporting of results, including thematic analysis and stratification by discipline.\n        6.  Stakeholder consultation.\n    *   **Novelty/Difference**: The novelty lies in being the *first-of-its-kind* comprehensive scoping review specifically focused on *legal concerns* in health-related AI, explicitly distinguishing them from ethical concerns. It also innovates by aiming to map these concerns *across multiple disciplines* (law, medicine, nursing, pharmacy, public health, computer science, engineering) and identify how different disciplines prioritize them. The inclusion of a broad range of databases, including legal and engineering ones, is a key aspect of this multidisciplinary approach.\n\n*   **Key Technical Contributions (of the protocol design)**\n    *   **Novel Methodology Design**: Development of a robust, multidisciplinary search strategy (detailed in Table 1) designed to capture legal concerns from diverse fields, peer-reviewed using the PRESS checklist.\n    *   **Clear Scope and Eligibility**: Precisely defined inclusion/exclusion criteria to focus on *legal concerns* and their proposed solutions in health-related AI, published since 2012 to capture recent developments (e.g., deep learning).\n    *   **Framework for Cross-Disciplinary Analysis**: The protocol establishes a systematic approach to identify, categorize, and compare legal concerns and their prioritization across different academic and professional disciplines.\n\n*   **Experimental Validation (Planned Validation Steps for the Review Process)**\n    *   As this is a protocol, no experimental results are presented. However, the protocol outlines rigorous validation steps for the *review process itself*:\n        *   **Search Strategy Validation**: The MEDLINE\u00ae search strategy was pilot-tested and peer-reviewed by an independent information specialist using the Peer Review of Electronic Search Strategies (PRESS) checklist.\n        *   **Eligibility Assessment Validation**: Eligibility assessment will be conducted *independently and in duplicate* at all review stages to minimize bias and ensure consistency.\n        *   **Team Expertise**: The research team includes expert clinicians, AI innovators, legal researchers, ethics experts, and a member with scoping review expertise to ensure diverse perspectives and methodological rigor.\n\n*   **Limitations & Scope**\n    *   **Temporal Scope**: The review is limited to publications from 2012 onwards, chosen to capture the rise of modern AI (e.g., deep learning). This means earlier foundational legal discussions might be excluded.\n    *   **Language Scope**: Limited to English- and French-language records.\n    *   **Publication Type**: Excludes abstracts and conference proceedings, focusing on published articles and book chapters, and American law journals.\n    *   **Focus on Legal Concerns**: While acknowledging overlap, the review explicitly distinguishes and prioritizes legal concerns over purely ethical ones, which might lead to a narrower interpretation of some issues.\n    *   **Grey Literature**: While some grey literature will be considered (from a previous task force bibliography and crowdsourcing), it is not the primary focus, and initial examinations suggest it may be less suitable for mapping concerns *across disciplines*.\n\n*   **Technical Significance**\n    *   **Advancing State-of-the-Art (in literature synthesis)**: This protocol represents a significant step towards systematically mapping the complex legal landscape of health-related AI, providing a foundational resource for policymakers, legal scholars, AI developers, and healthcare professionals.\n    *   **Potential Impact on Future Research**: The completed review will:\n        *   Identify priority areas for future legal and policy reforms.\n        *   Reveal discipline-specific concerns and proposed solutions, fostering interdisciplinary dialogue.\n        *   Highlight research gaps in the existing literature, guiding future scholarly inquiry into specific legal challenges of AI.\n        *   Inform the development of regulatory options for stakeholders involved in AI governance.",
        "keywords": [
            "Health-related Artificial Intelligence (AI)",
            "legal concerns",
            "scoping review protocol",
            "multidisciplinary approach",
            "AI governance",
            "systematic literature synthesis",
            "algorithmic bias",
            "privacy breaches",
            "liability attribution",
            "search strategy design",
            "policy reforms",
            "research gaps",
            "distinction from ethical concerns"
        ],
        "paper_type": "the paper explicitly states its purpose is to conduct a \"scoping review\" to \"summarize and map the literature examining legal concerns in health-related ai.\" it details the methodology for this review, including search strategies, eligibility criteria, and how data will be collated and reported.\n\nthis aligns directly with the criteria for a **survey** paper:\n*   **abstract mentions:** \"scoping review approach\", \"comprehensive summary of the literature\", \"summarize and map the literature\".\n*   **introduction discusses:** \"no systematic overview of the legal concerns\", \"systematically map legal concerns\", \"examine the extent, range, and nature of research activity across the disciplines; to summarize and disseminate research findings to relevant stakeholders; and to identify research gaps in the existing literature.\" it also outlines the stages of the scoping review methodology.\n\ntherefore, the paper is a **survey**."
    },
    "7c813d1ffa86b9937c04e68507c6ab1b69d8484d.pdf": {
        "title": "Defining the concepts of a smart nursing home and its potential technology utilities that integrate medical services and are acceptable to stakeholders: a scoping review",
        "authors": [
            "Yuanyuan Zhao",
            "F. Rokhani",
            "S. Sazlina",
            "N. Devaraj",
            "Jing Su",
            "Boon-How Chew"
        ],
        "published_date": "2021",
        "abstract": "Background and objectives Smart technology in nursing home settings has the potential to elevate an operation that manages more significant number of older residents.\u00a0However, the concepts, definitions, and types of smart technology, integrated medical services, and stakeholders\u2019 acceptability of smart nursing homes are less clear. This scoping review aims to define a smart nursing home and examine the qualitative evidence on technological feasibility, integration of medical services, and acceptability of the stakeholders. Methods Comprehensive searches were conducted on stakeholders\u2019 websites (Phase 1) and 11 electronic databases (Phase 2), for existing concepts of smart nursing home, on what and how technologies and medical services were implemented in nursing home settings, and acceptability assessment by the stakeholders. The publication year was inclusive from January 1999 to September 2021. The language was limited to English and Chinese. Included articles must report nursing home settings related to older adults\u2009\u2265\u200960\u00a0years old with or without medical demands but not bed-bound. Technology Readiness Levels were used to measure the readiness of new technologies and system designs. The analysis was guided by the Framework Method and the smart technology adoption behaviours of elder consumers theoretical model. The results were reported according to the PRISMA-ScR. Results A\u00a0total of\u00a0177 literature\u00a0(13 website documents and 164 journal articles) were selected. Smart nursing homes are technology-assisted nursing homes that allow the life enjoyment of their residents. They used IoT, computing technologies, cloud computing, big data and AI, information management systems, and digital health to integrate medical services in monitoring abnormal events,\u00a0assisting daily living, conducting teleconsultation, managing health information,\u00a0and improving the interaction between providers and residents. Fifty-five percent of the new technologies were ready for use in nursing homes (levels 6\u20137), and the remaining were proven the technical feasibility (levels 1\u20135). Healthcare professionals with higher education, better tech-savviness, fewer years at work, and older adults with more severe illnesses were more acceptable to smart technologies. Conclusions Smart nursing homes with integrated medical services have great potential to improve the quality of care and ensure older residents\u2019 quality of life.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/7c813d1ffa86b9937c04e68507c6ab1b69d8484d.pdf",
        "venue": "BMC Geriatrics",
        "citationCount": 30,
        "score": 7.5,
        "summary": "Background and objectives Smart technology in nursing home settings has the potential to elevate an operation that manages more significant number of older residents.\u00a0However, the concepts, definitions, and types of smart technology, integrated medical services, and stakeholders\u2019 acceptability of smart nursing homes are less clear. This scoping review aims to define a smart nursing home and examine the qualitative evidence on technological feasibility, integration of medical services, and acceptability of the stakeholders. Methods Comprehensive searches were conducted on stakeholders\u2019 websites (Phase 1) and 11 electronic databases (Phase 2), for existing concepts of smart nursing home, on what and how technologies and medical services were implemented in nursing home settings, and acceptability assessment by the stakeholders. The publication year was inclusive from January 1999 to September 2021. The language was limited to English and Chinese. Included articles must report nursing home settings related to older adults\u2009\u2265\u200960\u00a0years old with or without medical demands but not bed-bound. Technology Readiness Levels were used to measure the readiness of new technologies and system designs. The analysis was guided by the Framework Method and the smart technology adoption behaviours of elder consumers theoretical model. The results were reported according to the PRISMA-ScR. Results A\u00a0total of\u00a0177 literature\u00a0(13 website documents and 164 journal articles) were selected. Smart nursing homes are technology-assisted nursing homes that allow the life enjoyment of their residents. They used IoT, computing technologies, cloud computing, big data and AI, information management systems, and digital health to integrate medical services in monitoring abnormal events,\u00a0assisting daily living, conducting teleconsultation, managing health information,\u00a0and improving the interaction between providers and residents. Fifty-five percent of the new technologies were ready for use in nursing homes (levels 6\u20137), and the remaining were proven the technical feasibility (levels 1\u20135). Healthcare professionals with higher education, better tech-savviness, fewer years at work, and older adults with more severe illnesses were more acceptable to smart technologies. Conclusions Smart nursing homes with integrated medical services have great potential to improve the quality of care and ensure older residents\u2019 quality of life.",
        "keywords": []
    },
    "0c3ceda44c277b83a48eda5c12b1d8fbdc44056b.pdf": {
        "title": "Artificial Intelligence (AI) in Breast Imaging: A Scientometric Umbrella Review",
        "authors": [
            "X. Tan",
            "W. L. Cheor",
            "Li Li Lim",
            "Khairul Shakir Ab Rahman",
            "I. Bakrin"
        ],
        "published_date": "2022",
        "abstract": "Artificial intelligence (AI), a rousing advancement disrupting a wide spectrum of applications with remarkable betterment, has continued to gain momentum over the past decades. Within breast imaging, AI, especially machine learning and deep learning, honed with unlimited cross-data/case referencing, has found great utility encompassing four facets: screening and detection, diagnosis, disease monitoring, and data management as a whole. Over the years, breast cancer has been the apex of the cancer cumulative risk ranking for women across the six continents, existing in variegated forms and offering a complicated context in medical decisions. Realizing the ever-increasing demand for quality healthcare, contemporary AI has been envisioned to make great strides in clinical data management and perception, with the capability to detect indeterminate significance, predict prognostication, and correlate available data into a meaningful clinical endpoint. Here, the authors captured the review works over the past decades, focusing on AI in breast imaging, and systematized the included works into one usable document, which is termed an umbrella review. The present study aims to provide a panoramic view of how AI is poised to enhance breast imaging procedures. Evidence-based scientometric analysis was performed in accordance with the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guideline, resulting in 71 included review works. This study aims to synthesize, collate, and correlate the included review works, thereby identifying the patterns, trends, quality, and types of the included works, captured by the structured search strategy. The present study is intended to serve as a \u201cone-stop center\u201d synthesis and provide a holistic bird\u2019s eye view to readers, ranging from newcomers to existing researchers and relevant stakeholders, on the topic of interest.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/0c3ceda44c277b83a48eda5c12b1d8fbdc44056b.pdf",
        "venue": "Diagnostics",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "Artificial intelligence (AI), a rousing advancement disrupting a wide spectrum of applications with remarkable betterment, has continued to gain momentum over the past decades. Within breast imaging, AI, especially machine learning and deep learning, honed with unlimited cross-data/case referencing, has found great utility encompassing four facets: screening and detection, diagnosis, disease monitoring, and data management as a whole. Over the years, breast cancer has been the apex of the cancer cumulative risk ranking for women across the six continents, existing in variegated forms and offering a complicated context in medical decisions. Realizing the ever-increasing demand for quality healthcare, contemporary AI has been envisioned to make great strides in clinical data management and perception, with the capability to detect indeterminate significance, predict prognostication, and correlate available data into a meaningful clinical endpoint. Here, the authors captured the review works over the past decades, focusing on AI in breast imaging, and systematized the included works into one usable document, which is termed an umbrella review. The present study aims to provide a panoramic view of how AI is poised to enhance breast imaging procedures. Evidence-based scientometric analysis was performed in accordance with the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guideline, resulting in 71 included review works. This study aims to synthesize, collate, and correlate the included review works, thereby identifying the patterns, trends, quality, and types of the included works, captured by the structured search strategy. The present study is intended to serve as a \u201cone-stop center\u201d synthesis and provide a holistic bird\u2019s eye view to readers, ranging from newcomers to existing researchers and relevant stakeholders, on the topic of interest.",
        "keywords": []
    },
    "3d8d80cf9acf0d6ca7e5e5e95756dd639054829d.pdf": {
        "title": "An Analysis of Body Language of Patients Using Artificial Intelligence",
        "authors": [
            "Rawad Abdulghafor",
            "Abdelrahman M Abdelmohsen",
            "S. Turaev",
            "Mohammed A. H. Ali",
            "Sharyar Wani"
        ],
        "published_date": "2022",
        "abstract": "In recent decades, epidemic and pandemic illnesses have grown prevalent and are a regular source of concern throughout the world. The extent to which the globe has been affected by the COVID-19 epidemic is well documented. Smart technology is now widely used in medical applications, with the automated detection of status and feelings becoming a significant study area. As a result, a variety of studies have begun to focus on the automated detection of symptoms in individuals infected with a pandemic or epidemic disease by studying their body language. The recognition and interpretation of arm and leg motions, facial recognition, and body postures is still a developing field, and there is a dearth of comprehensive studies that might aid in illness diagnosis utilizing artificial intelligence techniques and technologies. This literature review is a meta review of past papers that utilized AI for body language classification through full-body tracking or facial expressions detection for various tasks such as fall detection and COVID-19 detection, it looks at different methods proposed by each paper, their significance and their results.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/3d8d80cf9acf0d6ca7e5e5e95756dd639054829d.pdf",
        "venue": "Healthcare",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "In recent decades, epidemic and pandemic illnesses have grown prevalent and are a regular source of concern throughout the world. The extent to which the globe has been affected by the COVID-19 epidemic is well documented. Smart technology is now widely used in medical applications, with the automated detection of status and feelings becoming a significant study area. As a result, a variety of studies have begun to focus on the automated detection of symptoms in individuals infected with a pandemic or epidemic disease by studying their body language. The recognition and interpretation of arm and leg motions, facial recognition, and body postures is still a developing field, and there is a dearth of comprehensive studies that might aid in illness diagnosis utilizing artificial intelligence techniques and technologies. This literature review is a meta review of past papers that utilized AI for body language classification through full-body tracking or facial expressions detection for various tasks such as fall detection and COVID-19 detection, it looks at different methods proposed by each paper, their significance and their results.",
        "keywords": []
    },
    "346c603c39f71d8ff19e87728a3a276c927edada.pdf": {
        "title": "Artificial Intelligence Advances in the World of Cardiovascular Imaging",
        "authors": [
            "Bhakti Patel",
            "A. Makaryus"
        ],
        "published_date": "2022",
        "abstract": "The tremendous advances in digital information and communication technology have entered everything from our daily lives to the most intricate aspects of medical and surgical care. These advances are seen in electronic and mobile health and allow many new applications to further improve and make the diagnoses of patient diseases and conditions more precise. In the area of digital radiology with respect to diagnostics, the use of advanced imaging tools and techniques is now at the center of evaluation and treatment. Digital acquisition and analysis are central to diagnostic capabilities, especially in the field of cardiovascular imaging. Furthermore, the introduction of artificial intelligence (AI) into the world of digital cardiovascular imaging greatly broadens the capabilities of the field both with respect to advancement as well as with respect to complete and accurate diagnosis of cardiovascular conditions. The application of AI in recognition, diagnostics, protocol automation, and quality control for the analysis of cardiovascular imaging modalities such as echocardiography, nuclear cardiac imaging, cardiovascular computed tomography, cardiovascular magnetic resonance imaging, and other imaging, is a major advance that is improving rapidly and continuously. We document the innovations in the field of cardiovascular imaging that have been brought about by the acceptance and implementation of AI in relation to healthcare professionals and patients in the cardiovascular field.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/346c603c39f71d8ff19e87728a3a276c927edada.pdf",
        "venue": "Healthcare",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "The tremendous advances in digital information and communication technology have entered everything from our daily lives to the most intricate aspects of medical and surgical care. These advances are seen in electronic and mobile health and allow many new applications to further improve and make the diagnoses of patient diseases and conditions more precise. In the area of digital radiology with respect to diagnostics, the use of advanced imaging tools and techniques is now at the center of evaluation and treatment. Digital acquisition and analysis are central to diagnostic capabilities, especially in the field of cardiovascular imaging. Furthermore, the introduction of artificial intelligence (AI) into the world of digital cardiovascular imaging greatly broadens the capabilities of the field both with respect to advancement as well as with respect to complete and accurate diagnosis of cardiovascular conditions. The application of AI in recognition, diagnostics, protocol automation, and quality control for the analysis of cardiovascular imaging modalities such as echocardiography, nuclear cardiac imaging, cardiovascular computed tomography, cardiovascular magnetic resonance imaging, and other imaging, is a major advance that is improving rapidly and continuously. We document the innovations in the field of cardiovascular imaging that have been brought about by the acceptance and implementation of AI in relation to healthcare professionals and patients in the cardiovascular field.",
        "keywords": []
    },
    "1a19bd0444ace3458fbf2b5ea3ddcb269c5ea2b2.pdf": {
        "title": "Risk and Protective Factors Related to the Wellness of American Indian and Alaska Native Youth: A Systematic Review",
        "authors": [
            "Catherine E. Burnette",
            "Charles R. Figley"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/1a19bd0444ace3458fbf2b5ea3ddcb269c5ea2b2.pdf",
        "venue": "",
        "citationCount": 61,
        "score": 6.777777777777778,
        "summary": "",
        "keywords": []
    },
    "b5501ca7d40be9a87cff3486ea2f14c77a9d2244.pdf": {
        "title": "Appositeness of Optimized and Reliable Machine Learning for Healthcare: A Survey",
        "authors": [
            "S. Swain",
            "Bharat Bhushan",
            "Gaurav Dhiman",
            "Wattana Viriyasitavat"
        ],
        "published_date": "2022",
        "abstract": "Machine Learning (ML) has been categorized as a branch of Artificial Intelligence (AI) under the Computer Science domain wherein programmable machines imitate human learning behavior with the help of statistical methods and data. The Healthcare industry is one of the largest and busiest sectors in the world, functioning with an extensive amount of manual moderation at every stage. Most of the clinical documents concerning patient care are hand-written by experts, selective reports are machine-generated. This process elevates the chances of misdiagnosis thereby, imposing a risk to a patient's life. Recent technological adoptions for automating manual operations have witnessed extensive use of ML in its applications. The paper surveys the applicability of ML approaches in automating medical systems. The paper discusses most of the optimized statistical ML frameworks that encourage better service delivery in clinical aspects. The universal adoption of various Deep Learning (DL) and ML techniques as the underlying systems for a variety of wellness applications, is delineated by challenges and elevated by myriads of security. This work tries to recognize a variety of vulnerabilities occurring in medical procurement, admitting the concerns over its predictive performance from a privacy point of view. Finally providing possible risk delimiting facts and directions for active challenges in the future.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/b5501ca7d40be9a87cff3486ea2f14c77a9d2244.pdf",
        "venue": "Archives of Computational Methods in Engineering",
        "citationCount": 20,
        "score": 6.666666666666666,
        "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the significant burden of clinical documentation on healthcare professionals, which consumes a substantial portion of their working time (e.g., 10.8-11.5 hours/week, or 28% of a clinician's week) \\cite{swain2022az2}. This administrative load reduces direct patient interaction time (less than 13% of a doctor's day) and contributes to clinician de-motivation and burnout \\cite{swain2022az2}.\n    *   **Importance & Challenge:** Accurate, timely, and comprehensive documentation is critical for patient safety, informed decision-making, and regulatory compliance in a complex healthcare environment \\cite{swain2022az2}. The sheer volume of documentation (e.g., millions of reports and letters annually) makes traditional methods like typing, handwriting, or outsourced dictation inefficient, slow, and prone to backlogs, hindering the flow of vital clinical information \\cite{swain2022az2}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work positions speech recognition (SR) as a direct and superior alternative to conventional clinical documentation methods, including manual typing, handwriting, and traditional digital dictation that relies on secretarial transcription \\cite{swain2022az2}.\n    *   **Limitations of Previous Solutions:** Previous methods are highlighted as time-consuming, contributing to clinician workload and burnout, and leading to significant delays in documentation turnaround times (e.g., outpatient letters taking weeks or 12 days) \\cite{swain2022az2}. These inefficiencies impede timely access to critical patient information and create administrative backlogs.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper advocates for the application of mature, high-accuracy speech recognition technology, specifically \"Dragon Medical,\" for direct dictation into Electronic Patient Records (EPRs) and other clinical information systems \\cite{swain2022az2}.\n    *   **Novelty/Difference:** The innovation lies not in developing a new SR algorithm, but in the *effective application and integration* of this advanced, domain-specific SR technology within diverse and complex clinical workflows in UK healthcare settings \\cite{swain2022az2}. Key technical features enabling this include:\n        *   High accuracy (stated as 99%) coupled with a comprehensive medical dictionary \\cite{swain2022az2}.\n        *   Significantly faster input speed (3x faster than typing) \\cite{swain2022az2}.\n        *   User-centric design: no initial speech profile training required, a single voice profile usable across different devices and locations, and support for macros to streamline documentation \\cite{swain2022az2}.\n        *   Flexibility: compatibility with various input devices, including smartphone microphones, and rapid installation on clinical workstations \\cite{swain2022az2}.\n\n*   **Key Technical Contributions**\n    *   **System Design/Architectural Innovations:** The paper demonstrates successful integration strategies for SR within existing healthcare IT infrastructures, such as EPRs (e.g., Cerner Millennium PowerNote) and Laboratory Information Management Systems (LIMS) \\cite{swain2022az2}. This includes the development of voice commands to navigate complex forms and support for structured coding standards like SNOMED within pathology reporting workflows \\cite{swain2022az2}.\n    *   **Practical Implementation Insights:** It provides empirical evidence and practical blueprints for deploying a mature SR system at scale, showcasing its adaptability to address specific documentation challenges across various clinical specialties and its capacity to optimize complex clinical workflows \\cite{swain2022az2}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The paper presents multiple real-world case studies from various NHS trusts in the UK, demonstrating the impact of Dragon Medical across different clinical departments \\cite{swain2022az2}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **A&E Department (South Tees Hospitals):** Achieved up to 40% time savings (approx. 3.5 minutes per patient) in document creation compared to typing, equating to 389 days saved or nearly 2 Whole Time Equivalents (WTEs) annually. 98% of users reported time savings, 86% noted faster information availability, and 88% observed quality improvements \\cite{swain2022az2}.\n        *   **Outpatient Study (Acute University NHS Hospital):** Reduced outpatient letter turnaround times from 12 days to an average of 3 days (with some achieving 24-48 hours). This led to a 77% reduction in outsourced transcription costs and avoided the recruitment of a secretarial role, saving \u00a318,333 \\cite{swain2022az2}.\n        *   **Community Study (Cambridge Community Services NHS Trust):** Decreased letter turnaround times from weeks to 2-5 days, resulting in increased patient throughput, reduced waiting times, and elimination of backlogs \\cite{swain2022az2}.\n        *   **Pathology Study (University Hospitals Plymouth NHS Trust):** Successfully eliminated a backlog of 600 reports and achieved turnaround times that exceeded national targets \\cite{swain2022az2}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper, originating from a vendor, does not delve into specific technical limitations of the Dragon Medical system itself, assuming its stated 99% accuracy and robustness for clinical use \\cite{swain2022az2}.\n    *   **Scope of Applicability:** The scope is focused on the practical application and benefits of speech recognition within UK healthcare settings, covering a range of specialties and documentation types. While the problem motivation includes Australian statistics, the solutions and empirical evidence are primarily derived from UK NHS contexts \\cite{swain2022az2}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** This work significantly advances the understanding of how mature, high-accuracy speech recognition technology can be effectively deployed to fundamentally transform clinical documentation processes. It provides compelling empirical evidence that SR can move healthcare beyond inefficient traditional methods, leading to more efficient, accurate, and timely record-keeping \\cite{swain2022az2}.\n    *   **Potential Impact on Future Research:** The paper highlights the critical role of domain-specific, user-friendly SR systems. It suggests future research could explore further optimization of SR integration with diverse and evolving EPR/LIMS systems, investigate its broader impact on data quality for advanced analytics and AI applications, and conduct large-scale, long-term studies to quantify its effects on clinician well-being, patient outcomes, and overall healthcare system efficiency \\cite{swain2022az2}.",
        "keywords": [
            "Speech Recognition (SR)",
            "Clinical Documentation",
            "Electronic Patient Records (EPRs)",
            "Healthcare Workflow Optimization",
            "System Integration Strategies",
            "Reduced Documentation Turnaround Times",
            "Clinician Burden Reduction",
            "Real-world Case Studies",
            "Dragon Medical",
            "Domain-specific Medical Dictionary",
            "Time and Cost Savings",
            "Scalable SR Deployment",
            "SNOMED Coding"
        ],
        "paper_type": "the provided \"abstract\" and \"introduction\" content is actually an agenda and introductory slides from a presentation titled \"the role of speech recognition in clinical documentation \u2013a uk perspective,\" not the abstract and introduction for the paper \"appositeness of optimized and reliable machine learning for healthcare: a survey.\"\n\nhowever, following the instruction to classify based on the *provided content*:\n\nthe agenda (presented as the abstract/introduction) explicitly lists:\n*   \"uk case studies: the power of clinical engagement\"\n*   \"benefits of speech recognition in the epr\"\n\nthe introduction further discusses:\n*   \"the complexity of healthcare\"\n*   \"the clinical documentation challenge\"\n*   provides statistics on healthcare activity, setting a \"specific context\" and \"real-world scenario.\"\n\nthese elements strongly align with the criteria for a **case_study**:\n*   **abstract mentions:** \"case study\", \"application\", \"practice\", \"experience\" (explicitly \"uk case studies\").\n*   **introduction discusses:** \"specific context, real-world scenario\" (uk healthcare, clinical documentation challenge, speech recognition application).\n\ntherefore, based on the provided content, the paper type is:\n\n**case_study**"
    },
    "9000711dcaf9cd851722fde9bea0bd9101a692f1.pdf": {
        "title": "Clinical Natural Language Processing with Deep Learning",
        "authors": [
            "Sadid A. Hasan",
            "Oladimeji Farri"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/9000711dcaf9cd851722fde9bea0bd9101a692f1.pdf",
        "venue": "Data Science for Healthcare",
        "citationCount": 40,
        "score": 6.666666666666666,
        "summary": "Here's a focused summary of the provided technical paper excerpt for a literature review:\n\n---\n\n**Analysis of \"Clinical Natural Language Processing with Deep Learning\" \\cite{hasan2019zc3}**\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the challenge of processing and understanding the vast and growing volume of unstructured clinical free text documents, such as those found in Electronic Health Record (EHR) systems \\cite{hasan2019zc3}.\n    *   **Why important and challenging**: This problem is crucial for extracting valuable knowledge to optimize clinical care and improve patient outcomes. It is challenging due to the unique characteristics of clinical text, including widespread use of acronyms and nonstandard jargon, inconsistent document structures, and the critical requirement for rigorous de-identification and anonymization to ensure patient privacy \\cite{hasan2019zc3}.\n\n2.  **Related Work & Positioning**\n    *   **How this work relates to existing approaches**: The paper positions deep learning as a superior alternative to traditional machine learning (ML) techniques for various NLP tasks, including those in the clinical domain \\cite{hasan2019zc3}.\n    *   **Limitations of previous solutions**: Traditional ML approaches are noted for requiring labor-intensive feature engineering for data representation \\cite{hasan2019zc3}. Furthermore, linear models like Support Vector Machines (SVMs) or logistic regression are less effective for the nonlinear classification problems inherent in language processing compared to deep learning's nonlinear neural network models \\cite{hasan2019zc3}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method or algorithm**: The paper introduces deep learning as the core technical method, emphasizing its ability to utilize multi-layered neural network architectures to automatically learn hierarchical representations of data \\cite{hasan2019zc3}. This contrasts with traditional ML's reliance on manual feature engineering.\n    *   **What makes this approach novel or different**: Deep learning's novelty lies in its capacity to automatically learn multiple levels of increasingly abstract representations, its effectiveness for nonlinear classification problems with hierarchical inputs (like language), and its adaptability through transfer learning \\cite{hasan2019zc3}. The paper *will* survey existing deep learning algorithms applied to clinical NLP and describe specific applications developed at Philips Research.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**: While the provided excerpt does not detail specific novel algorithms, it highlights deep learning's fundamental advantage in automatically learning hierarchical data representations \\cite{hasan2019zc3}. The chapter *promises* to describe deep learning-driven clinical NLP applications such as diagnostic inferencing, biomedical article retrieval, clinical paraphrase generation, adverse drug event (ADE) detection, and medical image caption generation \\cite{hasan2019zc3}.\n    *   **Theoretical insights or analysis**: The paper underscores the theoretical insight that deep learning models excel at solving nonlinear classification problems with naturally hierarchical inputs, making them particularly well-suited for NLP \\cite{hasan2019zc3}.\n\n5.  **Experimental Validation**\n    *   **What experiments were conducted**: The provided excerpt *states* that the chapter will include a detailed description of various deep learning-driven clinical NLP applications developed at Philips Research, implying that experimental validation for these applications will be discussed later in the full paper \\cite{hasan2019zc3}.\n    *   **Key performance metrics and comparison results**: No specific experimental results or performance metrics are presented in this introductory section \\cite{hasan2019zc3}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The paper acknowledges that NLP is generally considered an \"AI-complete problem\" due to the complexities of representing, learning, and using various forms of knowledge (linguistic, situational, world, visual) \\cite{hasan2019zc3}. It also implicitly assumes the availability of sufficient (potentially unlabeled) data and computational resources (GPUs) for deep learning to be effective \\cite{hasan2019zc3}.\n    *   **Scope of applicability**: The work focuses specifically on clinical NLP, aiming to foster innovation for applications such as clinical decision support, patient cohort identification, population health management, pharmacovigilance, personalized medicine, and clinical text summarization \\cite{hasan2019zc3}. The chapter itself serves as an overview and literature survey.\n\n7.  **Technical Significance**\n    *   **How this advances the technical state-of-the-art**: The paper advocates for deep learning as a significant advancement over traditional ML for clinical NLP, particularly in its ability to handle the unique complexities of clinical text and automatically learn features \\cite{hasan2019zc3}. This reduces the need for labor-intensive feature engineering, a bottleneck in previous approaches.\n    *   **Potential impact on future research**: By demonstrating the superior performance and capabilities of deep learning in clinical NLP, the work encourages further research and innovation in developing practical clinical applications, ultimately aiming to optimize clinical care and improve patient outcomes across the healthcare continuum \\cite{hasan2019zc3}.",
        "keywords": [
            "Clinical Natural Language Processing (NLP)",
            "Deep Learning",
            "Electronic Health Records (EHR)",
            "Unstructured Clinical Text",
            "Automatic Feature Learning",
            "Nonlinear Classification",
            "Traditional Machine Learning Limitations",
            "Patient Privacy",
            "Clinical Decision Support",
            "Adverse Drug Event (ADE) Detection",
            "Pharmacovigilance",
            "Transfer Learning",
            "Optimizing Clinical Care"
        ],
        "paper_type": "based on the abstract and introduction provided, this paper is best classified as a **survey**.\n\nhere's why:\n\n*   **abstract's purpose:** the abstract explicitly states, \"in this section, we will introduce how deep learning techniques can be applied to solve nlp problems in general. first, we will provide a brief descripti\". this indicates the paper's goal is to explain and describe existing applications and techniques, which is characteristic of a survey or review. it discusses the \"recent surge in deep learning\" and its reasons, and how it \"achieved promising results,\" which are elements of describing the state-of-the-art.\n*   **introduction's context:** the introduction sets the stage by discussing the importance of nlp, particularly in the clinical domain (\"novel clinical nlp solutions\"), and the superior performance of deep learning techniques for various nlp tasks. this provides the background for a comprehensive overview of how deep learning is utilized in this specific area.\n*   **lack of other criteria:**\n    *   it does not propose new methods or algorithms (**technical**).\n    *   it does not present mathematical proofs or formal models (**theoretical**).\n    *   it does not describe experiments, data collection, or statistical analysis conducted by the authors (**empirical**).\n    *   while it focuses on a specific application *domain* (clinical nlp), it appears to be a broader overview of techniques within that domain rather than a detailed analysis of one or a few specific instances (**case_study**).\n    *   it does not argue for a particular viewpoint or future direction (**position**).\n    *   there are no indicators of it being a brief communication or work-in-progress (**short**).\n\nthe paper aims to educate the reader on the application of deep learning in clinical nlp, which is the primary function of a survey paper."
    },
    "1fc228642851ba6c2cb6cc7f8050d2a016145862.pdf": {
        "title": "Applying Artificial Intelligence to Address the Knowledge Gaps in Cancer Care.",
        "authors": [
            "G. Simon",
            "C. Dinardo",
            "Koichi Takahashi",
            "T. Cascone",
            "Cynthia A. Powers",
            "Rick J Stevens",
            "Joshua Allen",
            "M. Antonoff",
            "D. Gomez",
            "P. Keane",
            "Fernando Jose Suarez Saiz",
            "Q. Nguyen",
            "Emily B. Roarty",
            "S. Pierce",
            "Jianjun Zhang",
            "Emily Hardeman Barnhill",
            "Kate Lakhani",
            "K. Shaw",
            "Brett Smith",
            "S. Swisher",
            "Rob High",
            "P. Futreal",
            "J. Heymach",
            "L. Chin"
        ],
        "published_date": "2018",
        "abstract": "BACKGROUND\nRapid advances in science challenge the timely adoption of evidence-based care in community settings. To bridge the gap between what is possible and what is practiced, we researched approaches to developing an artificial intelligence (AI) application that can provide real-time patient-specific decision support.\n\n\nMATERIALS AND METHODS\nThe Oncology Expert Advisor (OEA) was designed to simulate peer-to-peer consultation with three core functions: patient history summarization, treatment options recommendation, and management advisory. Machine-learning algorithms were trained to construct a dynamic summary of patients cancer history and to suggest approved therapy or investigative trial options. All patient data used were retrospectively accrued. Ground truth was established for approximately 1,000 unique patients. The full Medline database of more than 23 million published abstracts was used as the literature corpus.\n\n\nRESULTS\nOEA's accuracies of searching disparate sources within electronic medical records to extract complex clinical concepts from unstructured text documents varied, with F1 scores of 90%-96% for non-time-dependent concepts (e.g., diagnosis) and F1 scores of 63%-65% for time-dependent concepts (e.g., therapy history timeline). Based on constructed patient profiles, OEA suggests approved therapy options linked to supporting evidence (99.9% recall; 88% precision), and screens for eligible clinical trials on ClinicalTrials.gov (97.9% recall; 96.9% precision).\n\n\nCONCLUSION\nOur results demonstrated technical feasibility of an AI-powered application to construct longitudinal patient profiles in context and to suggest evidence-based treatment and trial options. Our experience highlighted the necessity of collaboration across clinical and AI domains, and the requirement of clinical expertise throughout the process, from design to training to testing.\n\n\nIMPLICATIONS FOR PRACTICE\nArtificial intelligence (AI)-powered digital advisors such as the Oncology Expert Advisor have the potential to augment the capacity and update the knowledge base of practicing oncologists. By constructing dynamic patient profiles from disparate data sources and organizing and vetting vast literature for relevance to a specific patient, such AI applications could empower oncologists to consider all therapy options based on the latest scientific evidence for their patients, and help them spend less time on information \"hunting and gathering\" and more time with the patients. However, realization of this will require not only AI technology maturation but also active participation and leadership by clincial experts.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/1fc228642851ba6c2cb6cc7f8050d2a016145862.pdf",
        "venue": "The Oncologist",
        "citationCount": 46,
        "score": 6.571428571428571,
        "summary": "BACKGROUND\nRapid advances in science challenge the timely adoption of evidence-based care in community settings. To bridge the gap between what is possible and what is practiced, we researched approaches to developing an artificial intelligence (AI) application that can provide real-time patient-specific decision support.\n\n\nMATERIALS AND METHODS\nThe Oncology Expert Advisor (OEA) was designed to simulate peer-to-peer consultation with three core functions: patient history summarization, treatment options recommendation, and management advisory. Machine-learning algorithms were trained to construct a dynamic summary of patients cancer history and to suggest approved therapy or investigative trial options. All patient data used were retrospectively accrued. Ground truth was established for approximately 1,000 unique patients. The full Medline database of more than 23 million published abstracts was used as the literature corpus.\n\n\nRESULTS\nOEA's accuracies of searching disparate sources within electronic medical records to extract complex clinical concepts from unstructured text documents varied, with F1 scores of 90%-96% for non-time-dependent concepts (e.g., diagnosis) and F1 scores of 63%-65% for time-dependent concepts (e.g., therapy history timeline). Based on constructed patient profiles, OEA suggests approved therapy options linked to supporting evidence (99.9% recall; 88% precision), and screens for eligible clinical trials on ClinicalTrials.gov (97.9% recall; 96.9% precision).\n\n\nCONCLUSION\nOur results demonstrated technical feasibility of an AI-powered application to construct longitudinal patient profiles in context and to suggest evidence-based treatment and trial options. Our experience highlighted the necessity of collaboration across clinical and AI domains, and the requirement of clinical expertise throughout the process, from design to training to testing.\n\n\nIMPLICATIONS FOR PRACTICE\nArtificial intelligence (AI)-powered digital advisors such as the Oncology Expert Advisor have the potential to augment the capacity and update the knowledge base of practicing oncologists. By constructing dynamic patient profiles from disparate data sources and organizing and vetting vast literature for relevance to a specific patient, such AI applications could empower oncologists to consider all therapy options based on the latest scientific evidence for their patients, and help them spend less time on information \"hunting and gathering\" and more time with the patients. However, realization of this will require not only AI technology maturation but also active participation and leadership by clincial experts.",
        "keywords": []
    },
    "04354a01838734e4cb97dd35bc3019e22f6ed1f5.pdf": {
        "title": "A Literature Review on Ethics for AI in Biomedical Research and Biobanking",
        "authors": [
            "Michael Kargl",
            "M. Plass",
            "Heimo M\u00fcller"
        ],
        "published_date": "2022",
        "abstract": "Summary Background : Artificial Intelligence (AI) is becoming more and more important especially in datacentric fields, such as biomedical research and biobanking. However, AI does not only offer advantages and promising benefits, but brings about also ethical risks and perils. In recent years, there has been growing interest in AI ethics, as reflected by a huge number of (scientific) literature dealing with the topic of AI ethics. The main objectives of this review are: (1) to provide an overview about important (upcoming) AI ethics regulations and international recommendations as well as available AI ethics tools and frameworks relevant to biomedical research, (2) to identify what AI ethics can learn from findings in ethics of traditional biomedical research - in particular looking at ethics in the domain of biobanking, and (3) to provide an overview about the main research questions in the field of AI ethics in biomedical research. Methods : We adopted a modified thematic review approach focused on understanding AI ethics aspects relevant to biomedical research. For this review, four scientific literature databases at the cross-section of medical, technical, and ethics science literature were queried: PubMed, BMC Medical Ethics, IEEE Xplore, and Google Scholar. In addition, a grey literature search was conducted to identify current trends in legislation and standardization. Results : More than 2,500 potentially relevant publications were retrieved through the initial search and 57 documents were included in the final review. The review found many documents describing high-level principles of AI ethics, and some publications describing approaches for making AI ethics more actionable and bridging the principles-to-practice gap. Also, some ongoing regulatory and standardization initiatives related to AI ethics were identified. It was found that ethical aspects of AI implementation in biobanks are often like those in biomedical research, for example with regards to handling big data or tackling informed consent. The review revealed current \u2018hot\u2019 topics in AI ethics related to biomedical research. Furthermore, several published tools and methods aiming to support practical implementation of AI ethics, as well as tools and frameworks specifically addressing complete and transparent reporting of biomedical studies involving AI are described in the review results. Conclusions : The review results provide a practically useful overview of research strands as well as regulations, guidelines, and tools regarding AI ethics in biomedical research. Furthermore, the review results show the need for an ethical-mindful and balanced approach to AI in biomedical research, and specifically reveal the need for AI ethics research focused on understanding and resolving practical problems arising from the use of AI in science and society.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/04354a01838734e4cb97dd35bc3019e22f6ed1f5.pdf",
        "venue": "Yearbook of Medical Informatics",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "Summary Background : Artificial Intelligence (AI) is becoming more and more important especially in datacentric fields, such as biomedical research and biobanking. However, AI does not only offer advantages and promising benefits, but brings about also ethical risks and perils. In recent years, there has been growing interest in AI ethics, as reflected by a huge number of (scientific) literature dealing with the topic of AI ethics. The main objectives of this review are: (1) to provide an overview about important (upcoming) AI ethics regulations and international recommendations as well as available AI ethics tools and frameworks relevant to biomedical research, (2) to identify what AI ethics can learn from findings in ethics of traditional biomedical research - in particular looking at ethics in the domain of biobanking, and (3) to provide an overview about the main research questions in the field of AI ethics in biomedical research. Methods : We adopted a modified thematic review approach focused on understanding AI ethics aspects relevant to biomedical research. For this review, four scientific literature databases at the cross-section of medical, technical, and ethics science literature were queried: PubMed, BMC Medical Ethics, IEEE Xplore, and Google Scholar. In addition, a grey literature search was conducted to identify current trends in legislation and standardization. Results : More than 2,500 potentially relevant publications were retrieved through the initial search and 57 documents were included in the final review. The review found many documents describing high-level principles of AI ethics, and some publications describing approaches for making AI ethics more actionable and bridging the principles-to-practice gap. Also, some ongoing regulatory and standardization initiatives related to AI ethics were identified. It was found that ethical aspects of AI implementation in biobanks are often like those in biomedical research, for example with regards to handling big data or tackling informed consent. The review revealed current \u2018hot\u2019 topics in AI ethics related to biomedical research. Furthermore, several published tools and methods aiming to support practical implementation of AI ethics, as well as tools and frameworks specifically addressing complete and transparent reporting of biomedical studies involving AI are described in the review results. Conclusions : The review results provide a practically useful overview of research strands as well as regulations, guidelines, and tools regarding AI ethics in biomedical research. Furthermore, the review results show the need for an ethical-mindful and balanced approach to AI in biomedical research, and specifically reveal the need for AI ethics research focused on understanding and resolving practical problems arising from the use of AI in science and society.",
        "keywords": []
    },
    "3a8547a9293d65d8a7bf204c1966be4615110dcf.pdf": {
        "title": "Interfacing With the Electronic Health Record (EHR): A Comparative Review of Modes of Documentation",
        "authors": [
            "John P. Avendano",
            "Daniel O Gallagher",
            "Joseph D Hawes",
            "J. Boyle",
            "L. Glasser",
            "Jomar N A Aryee",
            "Brian M. Katt"
        ],
        "published_date": "2022",
        "abstract": "Electronic health records (EHRs) have provided physicians with a systematic framework for collecting patient data, organizing notes from the healthcare team, and managing the daily workflow in the modern era of healthcare. Despite these advantages, EHRs have proven to be problematic for clinicians. The burdensome regulations requiring increased documentation with the EHR paradigm have led to inefficiencies from data-entry requirements forcing physicians to spend an inordinate amount of time on it, affecting the time available for direct patient care as well as leading to professional burnout. As a result, new modalities such as speech recognition, medical scribes, pre-made EHR templates, and digital scribes [a form of artificial intelligence (AI) based on ambient speech recognition] are increasingly being used to reduce charting time and increase the time available for patient care. The purpose of our review is to provide an up-to-date review of the literature on these modalities including their benefits and shortcomings, to help physicians and other medical professionals choose the best methods to document their patient-care encounters efficiently and effectively.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/3a8547a9293d65d8a7bf204c1966be4615110dcf.pdf",
        "venue": "Cureus",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "Electronic health records (EHRs) have provided physicians with a systematic framework for collecting patient data, organizing notes from the healthcare team, and managing the daily workflow in the modern era of healthcare. Despite these advantages, EHRs have proven to be problematic for clinicians. The burdensome regulations requiring increased documentation with the EHR paradigm have led to inefficiencies from data-entry requirements forcing physicians to spend an inordinate amount of time on it, affecting the time available for direct patient care as well as leading to professional burnout. As a result, new modalities such as speech recognition, medical scribes, pre-made EHR templates, and digital scribes [a form of artificial intelligence (AI) based on ambient speech recognition] are increasingly being used to reduce charting time and increase the time available for patient care. The purpose of our review is to provide an up-to-date review of the literature on these modalities including their benefits and shortcomings, to help physicians and other medical professionals choose the best methods to document their patient-care encounters efficiently and effectively.",
        "keywords": []
    },
    "0140588666895742e1e6be2c8e710112d0c16e5d.pdf": {
        "title": "Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs",
        "authors": [
            "Jun Chen",
            "Xiaoya Dai",
            "Quan Yuan",
            "Chao Lu",
            "Hai-ting Huang"
        ],
        "published_date": "2020",
        "abstract": "The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system. The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare. The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/0140588666895742e1e6be2c8e710112d0c16e5d.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 31,
        "score": 6.2,
        "summary": "The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability. In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system. The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare. The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.",
        "keywords": []
    },
    "bc182bf3daf3c03d246fa86b1ad008dc9fd0fe23.pdf": {
        "title": "Medicare program; hospital inpatient prospective payment systems for acute care hospitals and the long-term care hospital prospective payment system and fiscal year 2015 rates; quality reporting requirements for specific providers; reasonable compensation equivalents for physician services in exclud",
        "authors": [
            "Hhs Centers for Medicare Medicare Services"
        ],
        "published_date": "2014",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/bc182bf3daf3c03d246fa86b1ad008dc9fd0fe23.pdf",
        "venue": "Federal register",
        "citationCount": 67,
        "score": 6.090909090909091,
        "summary": "",
        "keywords": []
    },
    "a3260f8cdc050ffa8e76aa4843d16d70efddaa65.pdf": {
        "title": "Methodology for Conducting Post-Marketing Surveillance of Software as a Medical Device Based on Artificial Intelligence Technologies",
        "authors": [
            "V. Zinchenko",
            "K. Arzamasov",
            "S. Chetverikov",
            "A. Maltsev",
            "V. Novik",
            "E. Akhmad",
            "D. Sharova",
            "A. Andreychenko",
            "A. Vladzymyrskyy",
            "S. Morozov"
        ],
        "published_date": "2022",
        "abstract": "The aim of the study was to develop a methodology for conducting post-registration clinical monitoring of software as a medical device based on artificial intelligence technologies (SaMD-AI). Materials and Methods The methodology of post-registration clinical monitoring is based on the requirements of regulatory legal acts issued by the Board of the Eurasian Economic Commission. To comply with these requirements, the monitoring involves submission of the review of adverse events reports, the review of developers\u2019 routine reports on the safety and efficiency of SaMD-AI, and the assessment of the system for collecting and analyzing developers\u2019 post-registration data on the safety and efficiency of medical devices. The methodology was developed with regard to the recommendations of the International Medical Device Regulators Forum and the documents issued by the Food and Drug Administration (USA). Field-testing of this methodology was carried out using SaMD-AI designed for diagnostic imaging. Results The post-registration monitoring of SaMD-AI consists of three key stages: collecting user feedback, technical monitoring and clinical validation. Technical monitoring involves routine evaluation of SaMD-AI output data quality to detect and remove flaws in a timely manner, and to secure the product stability. Major outcomes include an ordered list of technical flaws in SaMD-AI and their classification using evidence from diagnostic imaging studies. The application of this methodology resulted in a gradual reduction in the number of studies with flaws due to timely improvements in artificial intelligence algorithms: the number of flaws decreased to 5% in various aspects during subsequent testing. Clinical validation confirmed that SaMD-AI is capable of producing clinically meaningful outputs related to its intended use within the functionality determined by the developer. The testing procedure and the baseline testing framework were established during the field testing. Conclusion The developed methodology will ensure the safety and efficiency of SaMD-AI taking into account its specifics as intangible medical devices. The methodology presented in this paper can be used by SaMD-AI developers to plan and carry out the post-registration clinical monitoring.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/a3260f8cdc050ffa8e76aa4843d16d70efddaa65.pdf",
        "venue": "Sovremennye tekhnologii v meditsine",
        "citationCount": 18,
        "score": 6.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   The paper addresses the critical technical problem of developing a specific methodology for post-marketing surveillance (PSM) of Software as a Medical Device based on Artificial Intelligence technologies (SaMD-AI) \\cite{zinchenko2022swk}.\n    *   This problem is important and challenging because existing generic medical device (MD) monitoring requirements fail to account for the unique characteristics of SaMD-AI, such as its lack of data interpretability, potential for bias when applied to different populations, and the dynamic nature of deep neural networks with continuous learning capabilities \\cite{zinchenko2022swk}.\n    *   SaMD-AI is often classified as a high-risk (Class III) MD, necessitating rigorous and tailored annual PSM to ensure its safety and efficiency throughout its product lifecycle in routine clinical practice \\cite{zinchenko2022swk}.\n\n2.  **Related Work & Positioning**\n    *   The work acknowledges existing generic PSM methodologies for MDs, which include validation, verification, and user feedback mechanisms \\cite{zinchenko2022swk}. It also references established practices for SaMD-AI developers, such as Good Machine Learning Practice and change management plans (e.g., GOST R IEC 62304\u20142013, FDA's Predetermined Change Control Plan) \\cite{zinchenko2022swk}.\n    *   The primary limitation of previous solutions is that published PSM documents are generic and \"fail to embrace the unique features of SaMD-AI,\" lacking a specialized approach to monitor both the effectiveness and safety of AI-driven software in real-world clinical settings \\cite{zinchenko2022swk}.\n\n3.  **Technical Approach & Innovation**\n    *   The core technical approach is a three-stage methodology for SaMD-AI post-marketing surveillance:\n        1.  **Feedback:** Standard collection and analysis of adverse event reports and user feedback \\cite{zinchenko2022swk}.\n        2.  **Technical Monitoring:** Routine evaluation of SaMD-AI output data quality to detect and eliminate technical flaws and assess product stability. This involves using a pseudo-random sample of studies (25% \"no pathology,\" 75% \"contains pathology\"), expert review for flaws, and classification of flaws by type and criticality \\cite{zinchenko2022swk}.\n        3.  **Clinical Validation:** Confirmation of SaMD-AI's ability to produce clinically meaningful outputs by testing on a verified data set. This includes defining performance parameters (sensitivity, specificity, accuracy), determining an optimal threshold for pathology detection based on the clinical task (e.g., maxNPV for screening, Youden index for balance), and rigorous data set construction \\cite{zinchenko2022swk}.\n    *   This approach is novel because it specifically tailors PSM to the \"intangible\" and \"black-box\" nature of SaMD-AI, providing a structured framework that integrates regulatory requirements (Eurasian Economic Commission) with international recommendations (IMDRF, FDA) \\cite{zinchenko2022swk}. It introduces a detailed technical monitoring stage with a quantitative flaw classification system and a systematic clinical validation process, including context-dependent threshold selection for AI outputs \\cite{zinchenko2022swk}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A multi-stage methodology explicitly separating \"Technical Monitoring\" and \"Clinical Validation\" for SaMD-AI, addressing its unique operational characteristics \\cite{zinchenko2022swk}.\n        *   A detailed classification system for technical flaws specific to SaMD-AI in medical imaging, categorized by type (e.g., distorted images, analysis errors, timeout) and criticality \\cite{zinchenko2022swk}.\n        *   A method for determining the optimal threshold value for SaMD-AI's probabilistic outputs (\u03c1) based on the specific clinical task (e.g., maximizing Negative Predictive Value for screening, Youden index for balanced performance, Positive Predictive Value for specific detection) \\cite{zinchenko2022swk}.\n        *   A framework for building sufficiently large and verified data sets for clinical validation, emphasizing proportional sampling and expert review \\cite{zinchenko2022swk}.\n    *   **System Design/Architectural Innovations:** The methodology itself represents a structured system for ongoing oversight, designed to ensure the safety and efficiency of SaMD-AI throughout its lifecycle \\cite{zinchenko2022swk}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Field-testing of the methodology was performed using SaMD-AI for diagnostic imaging within an \"Experiment on the use of innovative technologies in the field of computer vision\" in Moscow \\cite{zinchenko2022swk}.\n        *   **Technical Monitoring:** Over 550 SaMD-AI tests were conducted across various modalities (60% CT, 28% X-ray/photofluorography, 12% mammography). Medical experts reviewed pseudo-random samples for technical flaws \\cite{zinchenko2022swk}.\n        *   **Clinical Validation:** The procedure and baseline testing framework for clinical validation were established during field testing, involving the evaluation of performance parameters on verified data sets \\cite{zinchenko2022swk}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Technical Monitoring:** The methodology led to a \"gradual reduction in the number of studies with flaws\" due to timely algorithm improvements, with the number of flaws decreasing to 5% in various aspects during subsequent testing \\cite{zinchenko2022swk}. Initially, the average flaw rate was 13% (SD 4.2%), with \"No interpretation of findings\" (28%) and \"Labelling is outside a target organ\" (26%) being prevalent critical flaws \\cite{zinchenko2022swk}.\n        *   **Clinical Validation:** Confirmed that SaMD-AI is \"capable of producing clinically meaningful outputs related to its intended use,\" with performance parameters (sensitivity, specificity, accuracy) calculated and compared against developer claims \\cite{zinchenko2022swk}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The \"Feedback\" stage, being common to all MDs, was acknowledged but considered beyond the scope of this specific publication \\cite{zinchenko2022swk}. The methodology assumes the SaMD-AI developer will create a PSM plan during development \\cite{zinchenko2022swk}.\n    *   **Scope of Applicability:** The methodology was primarily tested and illustrated with SaMD-AI used in diagnostic imaging \\cite{zinchenko2022swk}. While developed within the Eurasian Economic Commission's regulatory framework, it incorporates international recommendations (IMDRF, FDA), suggesting broader applicability for SaMD-AI developers globally \\cite{zinchenko2022swk}.\n\n7.  **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing a much-needed, structured, and specific methodology for the post-marketing surveillance of SaMD-AI, addressing the unique challenges posed by AI in medical applications where generic MD monitoring falls short \\cite{zinchenko2022swk}.\n    *   It offers practical tools, such as a detailed flaw classification system and context-dependent threshold selection for AI outputs, which are crucial for ensuring the ongoing safety and efficiency of these complex systems \\cite{zinchenko2022swk}.\n    *   The methodology has the potential to impact future research by serving as a foundational framework for regulatory bodies and developers, fostering greater user confidence in SaMD-AI, and encouraging further development of automated and efficient monitoring techniques for AI in healthcare \\cite{zinchenko2022swk}.",
        "keywords": [
            "SaMD-AI post-marketing surveillance",
            "three-stage methodology",
            "technical monitoring",
            "clinical validation",
            "AI interpretability and bias",
            "flaw classification system",
            "optimal threshold determination",
            "diagnostic imaging",
            "SaMD-AI performance parameters",
            "ongoing safety and efficiency",
            "regulatory framework for AI medical devices"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **title:** \"methodology for conducting post-marketing surveillance of software as a medical device based on artificial intelligence technologies\" directly indicates the presentation of a \"methodology,\" which aligns with the \"technical\" classification's mention of \"new methods, algorithms, or systems.\"\n2.  **introduction:**\n    *   it clearly outlines a \"technical problem\": the need for robust post-marketing surveillance (psm) for high-risk software as a medical device based on artificial intelligence (samd-ai), highlighting unique challenges like lack of data interpretability, bias, and continuous learning.\n    *   it implies a \"proposed solution\" by stating the necessity of this psm and that the paper will detail the \"methodology for conducting\" it. the goal is to \"increase user confidence in samd-ai, and ensure safety,\" which is a practical, technical objective.\n3.  **keywords:** while the abstract is truncated, the prominent \"methodology\" in the title is a strong indicator for \"technical\" papers, which often \"propose,\" \"develop,\" or \"present\" a new method or system.\n\nthe content does not suggest a comprehensive review (survey), mathematical proofs (theoretical), data collection and analysis (empirical), a specific application deep dive (case_study), a primary argument for a viewpoint (position), or a brief communication (short)."
    },
    "376de00d07ed1a9a8a842dc3d192a9131cf36001.pdf": {
        "title": "Facial asymmetry index in normal young adults.",
        "authors": [
            "C. S. Huang",
            "X. Q. Liu",
            "Y. R. Chen"
        ],
        "published_date": "2013",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/376de00d07ed1a9a8a842dc3d192a9131cf36001.pdf",
        "venue": "Orthodontics & craniofacial research",
        "citationCount": 69,
        "score": 5.75,
        "summary": "",
        "keywords": []
    },
    "d3236f0962d5ae482f17e26778e8a0996e4b911c.pdf": {
        "title": "Automated Decision Support Technologies and the Legal Profession",
        "authors": [
            "Daniel N Kluttz",
            "D. Mulligan"
        ],
        "published_date": "2019",
        "abstract": "A quiet revolution is afoot in the field of law. Technical systems employing algorithms are shaping and displacing professional decision making, and they are disrupting and restructuring relationships between law firms, lawyers, and clients. Decision-support systems marketed to legal professionals to support e-discovery \u2014 generally referred to as \u201ctechnology-assisted review\u201d (TAR) \u2014 increasingly rely on \u201cpredictive coding,\u201d machine-learning techniques to classify and predict which of the voluminous electronic documents subject to litigation should be withheld or produced to the opposing side. These systems and the companies offering them are reshaping relationships between lawyers and clients, introducing new kinds of professionals into legal practice, altering the discovery process, and shaping how lawyers construct knowledge about their cases and professional obligations. In the midst of these shifting relationships \u2014 and the ways in which these systems are shaping the construction and presentation of knowledge \u2014 lawyers are grappling with their professional obligations, ethical duties, and what it means for the future of legal practice. \n \nThrough in-depth, semi-structured interviews of experts in this space \u2014 the technology company representatives who develop and sell such systems to law firms and the legal professionals who decide whether and how to use them in practice \u2014 we shed light on the organizational structures, professional rules and norms, and technical system properties that are shaping and being reshaped by predictive coding systems. Our findings show that AI-supported decision systems such as these are reconfiguring professional work practices. In particular, they highlight concerns about potential loss of professional agency and skill, limited understanding and thereby both over- and under-reliance on decision-support systems, and confusion about responsibility and accountability as new kinds of technical professionals and technologies are brought into legal practice. The introduction of predictive coding systems and the new professional and organizational arrangements they are ushering into legal practice compound general concerns over the opacity of technical systems with specific concerns about encroachments on the construction of expert knowledge, liability frameworks, and the potential (mis-)alignment of machine reasoning with professional logics and ethics. \n \nBased on our findings, we conclude that predictive coding tools \u2014 and likely other algorithmic systems lawyers use to construct knowledge and reason about legal practice \u2014 challenge the current model for evaluating whether and how tools are appropriate for legal practice. As tools become both more complex, and more consequential, it is unreasonable to rely solely on legal professionals \u2014 judges, law firms, and lawyers \u2014 to determine which technologies are appropriate for use. The legal professionals we interviewed report relying on the evaluation and judgement of a range of new technical experts within law firms and, increasingly, third-party vendors and their technical experts. This system for choosing technical systems upon which lawyers rely to make professional decisions \u2014 e.g., whether documents are responsive, whether the standard of proportionality has been met \u2014 is no longer sufficient. As the tools of medicine are reviewed by appropriate experts before they are put out for consideration and adoption by medical professionals, we argue that the legal profession must develop new processes for determining which algorithmic tools are fit to support lawyers\u2019 decision making. Relatedly, because predictive coding systems are used to produce lawyers\u2019 professional judgment, we argue they must be designed for contestability \u2014 providing greater transparency, interaction, and configurability around embedded choices to ensure decisions about how to embed core professional judgments, such as relevance and proportionality remain salient and demand engagement from lawyers, not just their technical experts.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/d3236f0962d5ae482f17e26778e8a0996e4b911c.pdf",
        "venue": "Social Science Research Network",
        "citationCount": 33,
        "score": 5.5,
        "summary": "A quiet revolution is afoot in the field of law. Technical systems employing algorithms are shaping and displacing professional decision making, and they are disrupting and restructuring relationships between law firms, lawyers, and clients. Decision-support systems marketed to legal professionals to support e-discovery \u2014 generally referred to as \u201ctechnology-assisted review\u201d (TAR) \u2014 increasingly rely on \u201cpredictive coding,\u201d machine-learning techniques to classify and predict which of the voluminous electronic documents subject to litigation should be withheld or produced to the opposing side. These systems and the companies offering them are reshaping relationships between lawyers and clients, introducing new kinds of professionals into legal practice, altering the discovery process, and shaping how lawyers construct knowledge about their cases and professional obligations. In the midst of these shifting relationships \u2014 and the ways in which these systems are shaping the construction and presentation of knowledge \u2014 lawyers are grappling with their professional obligations, ethical duties, and what it means for the future of legal practice. \n \nThrough in-depth, semi-structured interviews of experts in this space \u2014 the technology company representatives who develop and sell such systems to law firms and the legal professionals who decide whether and how to use them in practice \u2014 we shed light on the organizational structures, professional rules and norms, and technical system properties that are shaping and being reshaped by predictive coding systems. Our findings show that AI-supported decision systems such as these are reconfiguring professional work practices. In particular, they highlight concerns about potential loss of professional agency and skill, limited understanding and thereby both over- and under-reliance on decision-support systems, and confusion about responsibility and accountability as new kinds of technical professionals and technologies are brought into legal practice. The introduction of predictive coding systems and the new professional and organizational arrangements they are ushering into legal practice compound general concerns over the opacity of technical systems with specific concerns about encroachments on the construction of expert knowledge, liability frameworks, and the potential (mis-)alignment of machine reasoning with professional logics and ethics. \n \nBased on our findings, we conclude that predictive coding tools \u2014 and likely other algorithmic systems lawyers use to construct knowledge and reason about legal practice \u2014 challenge the current model for evaluating whether and how tools are appropriate for legal practice. As tools become both more complex, and more consequential, it is unreasonable to rely solely on legal professionals \u2014 judges, law firms, and lawyers \u2014 to determine which technologies are appropriate for use. The legal professionals we interviewed report relying on the evaluation and judgement of a range of new technical experts within law firms and, increasingly, third-party vendors and their technical experts. This system for choosing technical systems upon which lawyers rely to make professional decisions \u2014 e.g., whether documents are responsive, whether the standard of proportionality has been met \u2014 is no longer sufficient. As the tools of medicine are reviewed by appropriate experts before they are put out for consideration and adoption by medical professionals, we argue that the legal profession must develop new processes for determining which algorithmic tools are fit to support lawyers\u2019 decision making. Relatedly, because predictive coding systems are used to produce lawyers\u2019 professional judgment, we argue they must be designed for contestability \u2014 providing greater transparency, interaction, and configurability around embedded choices to ensure decisions about how to embed core professional judgments, such as relevance and proportionality remain salient and demand engagement from lawyers, not just their technical experts.",
        "keywords": []
    },
    "c4e1fa0f84ca75e6b6ec5eeb4411d8fad89de728.pdf": {
        "title": "Novel artificial intelligence algorithm: an accurate and independent measure of spinopelvic parameters.",
        "authors": [
            "Lindsay D. Orosz",
            "Fenil R. Bhatt",
            "E. Jazini",
            "M. Dreischarf",
            "P. Grover",
            "Julia N. Grigorian",
            "Rita T Roy",
            "T. Schuler",
            "Christopher R. Good",
            "C. Haines"
        ],
        "published_date": "2022",
        "abstract": "OBJECTIVE\nThe analysis of sagittal alignment by measuring spinopelvic parameters has been widely adopted among spine surgeons globally, and sagittal imbalance is a well-documented cause of poor quality of life. These measurements are time-consuming but necessary to make, which creates a growing need for an automated analysis tool that measures spinopelvic parameters with speed, precision, and reproducibility without relying on user input. This study introduces and evaluates an algorithm based on artificial intelligence (AI) that fully automatically measures spinopelvic parameters.\n\n\nMETHODS\nTwo hundred lateral lumbar radiographs (pre- and postoperative images from 100 patients undergoing lumbar fusion) were retrospectively analyzed by board-certified spine surgeons who digitally measured lumbar lordosis, pelvic incidence, pelvic tilt, and sacral slope. The novel AI algorithm was also used to measure the same parameters. To evaluate the agreement between human and AI-automated measurements, the mean error (95% CI, SD) was calculated and interrater reliability was assessed using the 2-way random single-measure intraclass correlation coefficient (ICC). ICC values larger than 0.75 were considered excellent.\n\n\nRESULTS\nThe AI algorithm determined all parameters in 98% of preoperative and in 95% of postoperative images with excellent ICC values (preoperative range 0.85-0.92, postoperative range 0.81-0.87). The mean errors were smallest for pelvic incidence both pre- and postoperatively (preoperatively -0.5\u00b0 [95% CI -1.5\u00b0 to 0.6\u00b0] and postoperatively 0.0\u00b0 [95% CI -1.1\u00b0 to 1.2\u00b0]) and largest preoperatively for sacral slope (-2.2\u00b0 [95% CI -3.0\u00b0 to -1.5\u00b0]) and postoperatively for lumbar lordosis (3.8\u00b0 [95% CI 2.5\u00b0 to 5.0\u00b0]).\n\n\nCONCLUSIONS\nAdvancements in AI translate to the arena of medical imaging analysis. This method of measuring spinopelvic parameters on spine radiographs has excellent reliability comparable to expert human raters. This application allows users to accurately obtain critical spinopelvic measurements automatically, which can be applied to clinical practice. This solution can assist physicians by saving time in routine work and by avoiding error-prone manual measurements.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c4e1fa0f84ca75e6b6ec5eeb4411d8fad89de728.pdf",
        "venue": "Journal of Neurosurgery : Spine",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "OBJECTIVE\nThe analysis of sagittal alignment by measuring spinopelvic parameters has been widely adopted among spine surgeons globally, and sagittal imbalance is a well-documented cause of poor quality of life. These measurements are time-consuming but necessary to make, which creates a growing need for an automated analysis tool that measures spinopelvic parameters with speed, precision, and reproducibility without relying on user input. This study introduces and evaluates an algorithm based on artificial intelligence (AI) that fully automatically measures spinopelvic parameters.\n\n\nMETHODS\nTwo hundred lateral lumbar radiographs (pre- and postoperative images from 100 patients undergoing lumbar fusion) were retrospectively analyzed by board-certified spine surgeons who digitally measured lumbar lordosis, pelvic incidence, pelvic tilt, and sacral slope. The novel AI algorithm was also used to measure the same parameters. To evaluate the agreement between human and AI-automated measurements, the mean error (95% CI, SD) was calculated and interrater reliability was assessed using the 2-way random single-measure intraclass correlation coefficient (ICC). ICC values larger than 0.75 were considered excellent.\n\n\nRESULTS\nThe AI algorithm determined all parameters in 98% of preoperative and in 95% of postoperative images with excellent ICC values (preoperative range 0.85-0.92, postoperative range 0.81-0.87). The mean errors were smallest for pelvic incidence both pre- and postoperatively (preoperatively -0.5\u00b0 [95% CI -1.5\u00b0 to 0.6\u00b0] and postoperatively 0.0\u00b0 [95% CI -1.1\u00b0 to 1.2\u00b0]) and largest preoperatively for sacral slope (-2.2\u00b0 [95% CI -3.0\u00b0 to -1.5\u00b0]) and postoperatively for lumbar lordosis (3.8\u00b0 [95% CI 2.5\u00b0 to 5.0\u00b0]).\n\n\nCONCLUSIONS\nAdvancements in AI translate to the arena of medical imaging analysis. This method of measuring spinopelvic parameters on spine radiographs has excellent reliability comparable to expert human raters. This application allows users to accurately obtain critical spinopelvic measurements automatically, which can be applied to clinical practice. This solution can assist physicians by saving time in routine work and by avoiding error-prone manual measurements.",
        "keywords": []
    },
    "aeb5a82400a2ea187d74d60d53cb000a06f8e881.pdf": {
        "title": "Use of Artificial Intelligence for Medical Literature Search: Randomized Controlled Trial Using the Hackathon Format",
        "authors": [
            "D. Schoeb",
            "R. Suarez-Ibarrola",
            "S. Hein",
            "F. F. Dressler",
            "F. Adams",
            "D. Schlager",
            "A. Miernik"
        ],
        "published_date": "2020",
        "abstract": "Background Mapping out the research landscape around a project is often time consuming and difficult. Objective This study evaluates a commercial artificial intelligence (AI) search engine (IRIS.AI) for its applicability in an automated literature search on a specific medical topic. Methods To evaluate the AI search engine in a standardized manner, the concept of a science hackathon was applied. Three groups of researchers were tasked with performing a literature search on a clearly defined scientific project. All participants had a high level of expertise for this specific field of research. Two groups were given access to the AI search engine IRIS.AI. All groups were given the same amount of time for their search and were instructed to document their results. Search results were summarized and ranked according to a predetermined scoring system. Results The final scoring awarded 49 and 39 points out of 60 to AI groups 1 and 2, respectively, and the control group received 46 points. A total of 20 scientific studies with high relevance were identified, and 5 highly relevant studies (\u201cspot on\u201d) were reported by each group. Conclusions AI technology is a promising approach to facilitate literature searches and the management of medical libraries. In this study, however, the application of AI technology lead to a more focused literature search without a significant improvement in the number of results.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/aeb5a82400a2ea187d74d60d53cb000a06f8e881.pdf",
        "venue": "Interactive Journal of Medical Research",
        "citationCount": 26,
        "score": 5.2,
        "summary": "This document is an introductory welcome note to an event, not a traditional research paper detailing specific technical innovations or experimental results. It describes the purpose of the \"IRIS.AI Scithon\u2122Event\" which aims to use an AI tool for literature review on Virtual and Augmented Reality (VR/AR) in surgical education. Therefore, the analysis below focuses on the *event's objectives* and the *described capabilities* of the IRIS.AI tool, rather than reporting novel findings from the paper itself.\n\nHere's a focused summary for literature review based on the provided text:\n\n1.  **Research Problem & Motivation** \\cite{schoeb202027r}\n    *   **Specific technical problem**: The core problem addressed is the inefficiency and difficulty of conducting comprehensive literature research, particularly in rapidly evolving fields like Virtual and Augmented Reality (VR/AR) in surgical education. Human researchers struggle to screen, understand, and contextualize the \"sheer size of published data.\"\n    *   **Why important and challenging**: Surgical training methods have seen marginal change despite technological advances. VR/AR holds high potential for improving practical surgical education, but its routine implementation requires extensive research to identify promising approaches and unanswered questions. This necessary literature research is time-consuming and frustrating duemaking it a significant bottleneck.\n\n2.  **Related Work & Positioning** \\cite{schoeb202027r}\n    *   **Relation to existing approaches**: The text implicitly positions the IRIS.AI science assistant as an advanced tool to overcome the limitations of traditional, manual literature review methods. It acknowledges that \"a number of studies and projects have been performed on this topic\" (VR/AR in surgery), indicating existing research efforts that could benefit from more efficient review.\n    *   **Limitations of previous solutions**: The primary limitation highlighted is the human brain's capacity when \"screening vast amounts of data,\" which is described as \"nearly impossible to read and understand much less to be put in context.\"\n\n3.  **Technical Approach & Innovation** \\cite{schoeb202027r}\n    *   **Core technical method**: The event leverages the IRIS.AI science assistant, an \"artificial intelligence technology, which can read scientific papers and to some extent understand their content.\" This AI is designed to assist researchers in navigating and comprehending large datasets.\n    *   **Novelty/Difference**: The novelty lies in using AI to \"overcome the limitations of a human brain\" in literature screening, enabling researchers to \"find scientific data faster and more efficiently.\" The event itself is an innovative approach to evaluate this AI technology in a real-world medical research context.\n\n4.  **Key Technical Contributions** \\cite{schoeb202027r}\n    *   **Novel algorithms, methods, or techniques**: The paper *describes* the IRIS.AI science assistant as an existing AI technology for reading and understanding scientific papers, rather than presenting its internal algorithms. The *event's contribution* is the planned evaluation of this AI technology for medical research and the mapping of the research landscape regarding VR/AR in surgery.\n    *   **System design or architectural innovations**: Not detailed in this introductory text.\n    *   **Theoretical insights or analysis**: Not provided in this introductory text.\n\n5.  **Experimental Validation** \\cite{schoeb202027r}\n    *   **Experiments conducted**: The paper *does not report* experimental validation. Instead, it describes the \"Scithon\u2122Event\" as an initiative to \"evaluate the IRIS.AI technology for the usage in medical research\" and to \"map out the research landscape regarding virtual and augmented reality in surgery.\" This event is designed to *perform* the validation, not to present its results.\n    *   **Key performance metrics and comparison results**: No results are presented in this introductory text.\n\n6.  **Limitations & Scope** \\cite{schoeb202027r}\n    *   **Technical limitations or assumptions**: The text notes that the IRIS.AI science assistant can \"to some extent understand their content,\" implying potential limitations in its comprehension capabilities, which the event aims to evaluate.\n    *   **Scope of applicability**: The IRIS.AI technology is presented for general scientific literature screening. The specific application focus for this event is \"virtual and augmented reality in surgery,\" particularly for improving medical education.\n\n7.  **Technical Significance** \\cite{schoeb202027r}\n    *   **Advance the technical state-of-the-art**: The *concept* of using AI to significantly accelerate and improve the efficiency of literature review for complex medical topics represents a potential advancement in research methodology. If successful, the IRIS.AI technology could streamline the initial phases of research projects.\n    *   **Potential impact on future research**: By enabling faster and more efficient identification of promising approaches and unanswered questions, such AI tools could accelerate the development and implementation of cutting-edge technologies like VR/AR in medical education and practice. It could also foster more comprehensive and less biased literature reviews.",
        "keywords": [
            "AI-powered literature review",
            "Virtual and Augmented Reality (VR/AR)",
            "surgical education",
            "IRIS.AI science assistant",
            "research methodology evaluation",
            "scientific data screening",
            "overcoming human limitations",
            "medical research",
            "research landscape mapping",
            "efficiency in research",
            "rapidly evolving fields",
            "comprehensive literature research"
        ],
        "paper_type": "the paper should be classified as **empirical**.\n\nhere's why:\n\n1.  **title:** the title \"use of artificial intelligence for medical literature search: **randomized controlled trial** using the hackathon format\" is the strongest indicator. a \"randomized controlled trial\" (rct) is a specific and rigorous type of **empirical study** designed to test an intervention or hypothesis through data collection and statistical analysis.\n2.  **introduction:** the introduction states the purpose \"to **evaluate** the iris.ai technology for the usage in medical research.\" evaluation, especially through a controlled trial, is a hallmark of empirical research.\n3.  **criteria match:** the \"empirical\" criteria mention \"study\", \"experiment\", \"data\", \"statistical\", \"findings\", \"research questions\", \"methodology\", \"participants\". an rct inherently involves these elements, even if the provided abstract/introduction is more of an event description than a typical research summary."
    },
    "845b4bfa329180b290b3d858dccc1fa38e7e0622.pdf": {
        "title": "Infection-related ventilator-associated complications in ICU patients colonised with extended-spectrum \u03b2-lactamase-producing Enterobacteriaceae",
        "authors": [
            "F. Barbier",
            "S. Bailly",
            "C. Schwebel",
            "L. Papazian",
            "\u00c9. Azoulay",
            "H. Kallel",
            "S. Siami",
            "L. Argaud",
            "G. Marcotte",
            "B. Misset",
            "J. Reignier",
            "M. Darmon",
            "J. Zahar",
            "D. Goldgran\u2011Toledano",
            "E. Montmollin",
            "B. Souweine",
            "B. Mourvillier",
            "J. Timsit",
            "F. T. F. Group"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/845b4bfa329180b290b3d858dccc1fa38e7e0622.pdf",
        "venue": "Intensive Care Medicine",
        "citationCount": 36,
        "score": 5.142857142857142,
        "summary": "",
        "keywords": []
    },
    "fd5f1554ae48478ff0bdeb5fb7e99a6f8d0a74da.pdf": {
        "title": "How do providers of artificial intelligence (AI) solutions propose and legitimize the values of their solutions for supporting diagnostic radiology workflow? A technography study in 2021",
        "authors": [
            "M. R. Mehrizi",
            "Simon H. Gerritsen",
            "Wouter M. de Klerk",
            "Chantal Houtschild",
            "Silke M. H. Dinnessen",
            "Luna Zhao",
            "Rik van Sommeren",
            "Abby Zerfu"
        ],
        "published_date": "2022",
        "abstract": "Objectives How do providers of artificial intelligence (AI) solutions propose and legitimize the values of their solutions for supporting diagnostic radiology workflow? Methods We systematically analyze 393 AI applications developed for supporting diagnostic radiology workflow. We collected qualitative and quantitative data by analyzing around 1250 pages of documents retrieved from companies\u2019 websites and legal documents. Five investigators read and interpreted collected data, extracted the features and functionalities of the AI applications, and finally entered them into an excel file for identifying the patterns. Results Over the last 2 years, we see an increase in the number of AI applications (43%) and number of companies offering them (34%), as well as their average age (45%). Companies claim various value propositions related to increasing the \u201cefficiency\u201d of radiology work (18%)\u2014e.g., via reducing the time and cost of performing tasks and reducing the work pressure\u2014and \u201cquality\u201d of offering medical services (31%)\u2014e.g., via enhancing the quality of clinical decisions and enhancing the quality of patient care, or both of them (28%). To legitimize and support their value propositions, the companies use multiple strategies simultaneously, particularly by seeking legal approvals (72%), promoting their partnership with medical and academic institutions (75%), highlighting the expertise of their teams (56%), and showcasing examples of implementing their solutions in practice (53%). Conclusions Although providers of AI applications claim a wide range of value propositions, they often provide limited evidence to show how their solutions deliver such systematic values in clinical practice. Key Points \u2022 AI applications in radiology continue to grow in number and diversity. \u2022 Companies offering AI applications claim various value propositions and use multiple ways to legitimize these propositions. \u2022 Systematic scientific evidence showing the actual effectiveness of AI applications in clinical context is limited.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/fd5f1554ae48478ff0bdeb5fb7e99a6f8d0a74da.pdf",
        "venue": "European Radiology",
        "citationCount": 15,
        "score": 5.0,
        "summary": "Objectives How do providers of artificial intelligence (AI) solutions propose and legitimize the values of their solutions for supporting diagnostic radiology workflow? Methods We systematically analyze 393 AI applications developed for supporting diagnostic radiology workflow. We collected qualitative and quantitative data by analyzing around 1250 pages of documents retrieved from companies\u2019 websites and legal documents. Five investigators read and interpreted collected data, extracted the features and functionalities of the AI applications, and finally entered them into an excel file for identifying the patterns. Results Over the last 2 years, we see an increase in the number of AI applications (43%) and number of companies offering them (34%), as well as their average age (45%). Companies claim various value propositions related to increasing the \u201cefficiency\u201d of radiology work (18%)\u2014e.g., via reducing the time and cost of performing tasks and reducing the work pressure\u2014and \u201cquality\u201d of offering medical services (31%)\u2014e.g., via enhancing the quality of clinical decisions and enhancing the quality of patient care, or both of them (28%). To legitimize and support their value propositions, the companies use multiple strategies simultaneously, particularly by seeking legal approvals (72%), promoting their partnership with medical and academic institutions (75%), highlighting the expertise of their teams (56%), and showcasing examples of implementing their solutions in practice (53%). Conclusions Although providers of AI applications claim a wide range of value propositions, they often provide limited evidence to show how their solutions deliver such systematic values in clinical practice. Key Points \u2022 AI applications in radiology continue to grow in number and diversity. \u2022 Companies offering AI applications claim various value propositions and use multiple ways to legitimize these propositions. \u2022 Systematic scientific evidence showing the actual effectiveness of AI applications in clinical context is limited.",
        "keywords": []
    },
    "22299ef4fd0d02935917a0a3c99677dbb07b31a3.pdf": {
        "title": "Expert-level aspiration and penetration detection during flexible endoscopic evaluation of swallowing with artificial intelligence-assisted diagnosis",
        "authors": [
            "Weihao Weng",
            "Mitsuyoshi Imaizumi",
            "S. Murono",
            "Xin Zhu"
        ],
        "published_date": "2022",
        "abstract": "Flexible endoscopic evaluation of swallowing (FEES) is considered the gold standard in diagnosing oropharyngeal dysphagia. Recent advances in deep learning have led to a resurgence of artificial intelligence-assisted computer-aided diagnosis (AI-assisted CAD) for a variety of applications. AI-assisted CAD would be a remarkable benefit in providing medical services to populations with inadequate access to dysphagia experts, especially in aging societies. This paper presents an AI-assisted CAD named FEES-CAD for aspiration and penetration detection on video recording during FEES. FEES-CAD segments the input FEES video and classifies penetration, aspiration, residue in the vallecula, and residue in the hypopharynx based on the segmented FEES video. We collected and annotated FEES videos from 199 patients to train the network and tested the performance of FEES-CAD using FEES videos from other 40 patients. These patients consecutively underwent FEES between December 2016 and August 2019 at Fukushima Medical University Hospital. FEES videos were deidentified, randomized, and rated by FEES-CAD and laryngologists with over 15 years of experience in performing FEES. FEES-CAD achieved an average Dice similarity coefficient of 98.6\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\%$$\\end{document}%. FEES-CAD achieved expert-level accuracy performance on penetration (92.5\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\%$$\\end{document}%), aspiration (92.5\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\%$$\\end{document}%), residue in the vallecula (100\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\%$$\\end{document}%), and residue in the hypopharynx (87.5\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\%$$\\end{document}%) classification tasks. To the best of our knowledge, FEES-CAD is the first CNN-based system that achieves expert-level performance in detecting aspiration and penetration.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/22299ef4fd0d02935917a0a3c99677dbb07b31a3.pdf",
        "venue": "Scientific Reports",
        "citationCount": 15,
        "score": 5.0,
        "summary": "Here is a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the critical need for accurate and expert-level detection of aspiration and penetration during Flexible Endoscopic Evaluation of Swallowing (FEES) \\cite{weng2022sm6}. It also aims to classify residue in the vallecula and hypopharynx.\n    *   **Importance & Challenge:** Oropharyngeal dysphagia, common in stroke and neurological patients, leads to severe complications like aspiration pneumonia, dehydration, and malnutrition \\cite{weng2022sm6}. Aspiration (material below vocal folds) and penetration (material into laryngeal vestibule) are the most dangerous complications. FEES is a gold standard, but accurate detection is challenging for inexperienced clinicians due to the rapid swallowing motion (e.g., 0.3s \"white-out\") and significant inter-examiner variability \\cite{weng2022sm6}. There is also a growing need for dysphagia expertise in aging societies with inadequate access to specialists \\cite{weng2022sm6}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Previous AI-assisted methods for medical diagnosis include traditional machine learning, image-based AI, and Convolutional Neural Networks (CNNs) for various medical imaging tasks (e.g., COVID-19, breast lesions) \\cite{weng2022sm6}. Attention mechanisms and transfer learning (from large non-medical datasets like ImageNet or domain adaptation) have also been explored \\cite{weng2022sm6}.\n    *   **Limitations of Previous Solutions:**\n        *   Traditional supervised learning models (e.g., SVMs) have not achieved satisfactory performance \\cite{weng2022sm6}.\n        *   CNNs often lack long-range pixel-pixel dependencies, and self-attention mechanisms can introduce extra parameters, making models prone to overfitting, which is detrimental for high-accuracy medical tasks \\cite{weng2022sm6}.\n        *   High false positives in segmentation can lead to unreliable clinical diagnoses \\cite{weng2022sm6}.\n        *   CNN performance heavily relies on massive annotated data, which is scarce in medical domains \\cite{weng2022sm6}.\n        *   Domain-adapted CNNs and self-supervised/semi-supervised methods still face challenges in collecting suitable large-scale labeled or unlabeled medical datasets and ensuring generalizability due to data heterogeneity and interobserver variability \\cite{weng2022sm6}.\n        *   Crucially, existing CNN models and training strategies have rarely been applied specifically to the complex task of detecting aspiration and penetration from FEES videos \\cite{weng2022sm6}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes an AI-assisted Computer-Aided Diagnosis (CAD) system named FEES-CAD \\cite{weng2022sm6}. Its primary function is to segment objects of interest in FEES video frames and then classify aspiration, penetration, and residue.\n    *   **Algorithm/Architecture:** FEES-CAD employs a customized UNet architecture \\cite{weng2022sm6}.\n        *   It features four contracting paths, four expansive paths, and a bottleneck \\cite{weng2022sm6}.\n        *   Each path and bottleneck consists of two 3x3 convolutions, followed by batch normalization and a Gaussian Error Linear Unit (GELU) activation \\cite{weng2022sm6}.\n        *   Contracting paths use 2x2 max-pooling, while expansive paths use 2x2 unpooling and concatenate feature maps from corresponding contracting paths \\cite{weng2022sm6}.\n        *   The output layer uses a softmax-activated 1x1 convolution for multi-class segmentation prediction \\cite{weng2022sm6}.\n        *   The network is designed for a five-class segmentation problem: vocal fold, subglottis, laryngeal vestibule, test bolus, and \"white-out\" (indicating swallowing motion) \\cite{weng2022sm6}.\n    *   **Novelty:** FEES-CAD is presented as the first CNN-based system to achieve expert-level performance in detecting aspiration and penetration from FEES videos \\cite{weng2022sm6}. Its customized UNet architecture is tailored to handle the specific challenges of FEES, such as rapid \"white-out\" events and the need for precise anatomical segmentation. The system is trained from scratch on a dedicated, expert-annotated FEES video dataset, rather than relying heavily on pre-training from non-medical images or generic domain adaptation \\cite{weng2022sm6}.\n\n4.  **Key Technical Contributions**\n    *   **Novel System Design:** Introduction of FEES-CAD, an AI-assisted CAD system specifically designed for real-time aspiration and penetration detection during FEES \\cite{weng2022sm6}.\n    *   **Customized UNet Architecture:** Development of a specialized UNet variant optimized for the unique characteristics of FEES video segmentation, including the identification of critical anatomical landmarks (vocal fold, subglottis, laryngeal vestibule) and transient events like \"white-out\" \\cite{weng2022sm6}.\n    *   **Expert-Level Performance:** Achieving expert-level accuracy in both segmentation (Dice similarity coefficient) and classification tasks (aspiration, penetration, residue) \\cite{weng2022sm6}.\n    *   **Dedicated Dataset:** Creation and utilization of a substantial, expert-annotated dataset of FEES videos (25,630 images from 199 videos) from a clinical setting, which is crucial for training robust medical AI models \\cite{weng2022sm6}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The FEES-CAD network was trained and validated on a dataset of FEES videos and then tested on an independent set of videos \\cite{weng2022sm6}. The performance was compared against expert laryngologists.\n    *   **Dataset:** 25,630 expert-annotated images from 199 FEES videos were used for training and validation, with an additional 40 FEES videos used for testing \\cite{weng2022sm6}. These videos were collected from consecutive patients at Fukushima Medical University Hospital between 2016 and 2019 \\cite{weng2022sm6}. Ground truth annotations were established by a panel of laryngologists and dysphagia experts with over 20 years of experience \\cite{weng2022sm6}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Segmentation:** FEES-CAD achieved an average Dice similarity coefficient of **98.6%** \\cite{weng2022sm6}.\n        *   **Classification (Accuracy):** FEES-CAD demonstrated expert-level accuracy:\n            *   Penetration: **92.5%** \\cite{weng2022sm6}.\n            *   Aspiration: **92.5%** \\cite{weng2022sm6}.\n            *   Residue in the vallecula: **100%** \\cite{weng2022sm6}.\n            *   Residue in the hypopharynx: **87.5%** \\cite{weng2022sm6}.\n        *   The system's performance was comparable to or exceeded that of expert laryngologists with over 15 years of experience \\cite{weng2022sm6}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper primarily discusses general challenges in medical image segmentation (e.g., overfitting with self-attention, high false positives) that FEES-CAD aims to overcome. It does not explicitly list specific technical limitations of the FEES-CAD system itself. The training relies on a carefully annotated dataset, which is acknowledged as expensive and time-consuming \\cite{weng2022sm6}.\n    *   **Scope of Applicability:** The system is specifically designed for the analysis of FEES video recordings to detect aspiration, penetration, and residue \\cite{weng2022sm6}. The dataset used for training and testing originated from a single medical institution, which might imply potential limitations in generalizability to different acquisition protocols or patient populations from other hospitals \\cite{weng2022sm6}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** FEES-CAD represents a significant advancement as the first CNN-based system to achieve expert-level performance in the complex and clinically critical task of detecting aspiration and penetration from FEES videos \\cite{weng2022sm6}.\n    *   **Potential Impact on Future Research:**\n        *   **Clinical Practice:** It offers a remarkable benefit for providing AI-assisted diagnosis, particularly for beginner and intermediate clinicians, reducing diagnostic variability and improving patient care \\cite{weng2022sm6}. It can help address the shortage of dysphagia experts, especially in aging societies \\cite{weng2022sm6}.\n        *   **Dysphagia Management:** By enabling more accurate and consistent detection of aspiration and penetration, FEES-CAD can significantly contribute to the tertiary prevention and treatment of aspiration pneumonia and other complications in dysphagia patients, thereby improving survival rates for stroke survivors \\cite{weng2022sm6}.\n        *   **AI in Medicine:** This work demonstrates the feasibility and high performance of deep learning for real-time, critical diagnostic tasks in endoscopy, paving the way for similar AI applications in other specialized medical evaluations \\cite{weng2022sm6}.",
        "keywords": [
            "Flexible Endoscopic Evaluation of Swallowing (FEES)",
            "Aspiration",
            "Penetration",
            "Oropharyngeal dysphagia",
            "Residue detection",
            "AI-assisted Computer-Aided Diagnosis (CAD) system",
            "Customized UNet architecture",
            "Convolutional Neural Networks (CNNs)",
            "Image segmentation",
            "Multi-class classification",
            "Expert-level performance",
            "Dedicated expert-annotated FEES video dataset",
            "Dice similarity coefficient",
            "\"White-out\" events"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the paper describes the development and evaluation of a new system called \"fees-cad\" which is a \"cnn-based system\" for \"ai-assisted diagnosis.\"\n*   it explicitly states what the system does (\"classifies penetration, aspiration, residue...\"), how it was trained (\"collected and annotated fees videos from 199 patients to train the network\"), and how its performance was tested (\"tested the performance of fees-cad using fees videos from other 40 patients\").\n*   it reports specific performance metrics (\"achieved an average dice similarity coefficient of 98.6 %\", \"achieved expert-level accuracy performance\").\n*   it claims novelty for this \"cnn-based system.\"\n\nthese elements strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. while it includes empirical evaluation, the core contribution is the proposed ai-assisted diagnostic system.\n\n**classification: technical**"
    },
    "f3ba29e22330147909b05fa9be628a1adcb8cd50.pdf": {
        "title": "Adrenal Incidentaloma: Prevalence and Referral Patterns From Routine Practice in a Large UK University Teaching Hospital",
        "authors": [
            "F. Hanna",
            "S. Hancock",
            "Cherian George",
            "A. Clark",
            "J. Sim",
            "B. Issa",
            "Gillian Powner",
            "J. Waldron",
            "C. Duff",
            "S. Lea",
            "A. Golash",
            "M. Sathiavageeswaran",
            "A. Heald",
            "A. Fryer"
        ],
        "published_date": "2021",
        "abstract": "Abstract Context Adrenal incidentalomas (AIs) are increasingly being identified during unrelated imaging. Unlike AI clinical management, data on referral patterns in routine practice are lacking. Objective This work aimed to identify factors associated with AI referral. Methods We linked data from imaging reports and outpatient bookings from a large UK teaching hospital. We examined (i) AI prevalence and (ii) pattern of referral to endocrinology, stratified by age, imaging modality, scan anatomical site, requesting clinical specialty, and temporal trends. Using key radiology phrases to identify scans reporting potential AI, we identified 4097 individuals from 479\u2005945 scan reports (2015-2019). Main outcome measures included prevalence of AI and referral rates. Results Overall, AI lesions were identified in 1.2% of scans. They were more prevalent in abdomen computed tomography and magnetic resonance imaging scans (3.0% and 0.6%, respectively). Scans performed increased 7.7% year-on-year from 2015 to 2019, with a more pronounced increase in the number with AI lesions (14.7% per year). Only 394 of 4097 patients (9.6%) had a documented endocrinology referral code within 90 days, with medical (11.8%) more likely to refer than surgical (7.2%) specialties (P\u2005<\u2005.001). Despite prevalence increasing with age, older patients were less likely to be referred (P\u2005<\u2005.001). Conclusion While overall AI prevalence appeared low, scan numbers are large and rising; the number with identified AI are increasing still further. The poor AI referral rates, even in centers such as ours where dedicated AI multidisciplinary team meetings and digital management systems are used, highlights the need for new streamlined, clinically effective systems and processes to appropriately manage the AI workload.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/f3ba29e22330147909b05fa9be628a1adcb8cd50.pdf",
        "venue": "Journal of the Endocrine Society",
        "citationCount": 20,
        "score": 5.0,
        "summary": "Abstract Context Adrenal incidentalomas (AIs) are increasingly being identified during unrelated imaging. Unlike AI clinical management, data on referral patterns in routine practice are lacking. Objective This work aimed to identify factors associated with AI referral. Methods We linked data from imaging reports and outpatient bookings from a large UK teaching hospital. We examined (i) AI prevalence and (ii) pattern of referral to endocrinology, stratified by age, imaging modality, scan anatomical site, requesting clinical specialty, and temporal trends. Using key radiology phrases to identify scans reporting potential AI, we identified 4097 individuals from 479\u2005945 scan reports (2015-2019). Main outcome measures included prevalence of AI and referral rates. Results Overall, AI lesions were identified in 1.2% of scans. They were more prevalent in abdomen computed tomography and magnetic resonance imaging scans (3.0% and 0.6%, respectively). Scans performed increased 7.7% year-on-year from 2015 to 2019, with a more pronounced increase in the number with AI lesions (14.7% per year). Only 394 of 4097 patients (9.6%) had a documented endocrinology referral code within 90 days, with medical (11.8%) more likely to refer than surgical (7.2%) specialties (P\u2005<\u2005.001). Despite prevalence increasing with age, older patients were less likely to be referred (P\u2005<\u2005.001). Conclusion While overall AI prevalence appeared low, scan numbers are large and rising; the number with identified AI are increasing still further. The poor AI referral rates, even in centers such as ours where dedicated AI multidisciplinary team meetings and digital management systems are used, highlights the need for new streamlined, clinically effective systems and processes to appropriately manage the AI workload.",
        "keywords": []
    },
    "c10004c6f4962f956c83b026c5189708032fc653.pdf": {
        "title": "SimpleDet: A Simple and Versatile Distributed Framework for Object Detection and Instance Recognition",
        "authors": [
            "Yuntao Chen",
            "Chenxia Han",
            "Yanghao Li",
            "Zehao Huang",
            "Yi Jiang",
            "Naiyan Wang",
            "Zhaoxiang Zhang"
        ],
        "published_date": "2019",
        "abstract": "Object detection and instance recognition play a central role in many AI applications like autonomous driving, video surveillance and medical image analysis. However, training object detection models on large scale datasets remains computationally expensive and time consuming. This paper presents an efficient and open source object detection framework called SimpleDet which enables the training of state-of-the-art detection models on consumer grade hardware at large scale. SimpleDet supports up-to-date detection models with best practice. SimpleDet also supports distributed training with near linear scaling out of box. Codes, examples and documents of SimpleDet can be found at this https URL .",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c10004c6f4962f956c83b026c5189708032fc653.pdf",
        "venue": "Journal of machine learning research",
        "citationCount": 30,
        "score": 5.0,
        "summary": "Object detection and instance recognition play a central role in many AI applications like autonomous driving, video surveillance and medical image analysis. However, training object detection models on large scale datasets remains computationally expensive and time consuming. This paper presents an efficient and open source object detection framework called SimpleDet which enables the training of state-of-the-art detection models on consumer grade hardware at large scale. SimpleDet supports up-to-date detection models with best practice. SimpleDet also supports distributed training with near linear scaling out of box. Codes, examples and documents of SimpleDet can be found at this https URL .",
        "keywords": []
    },
    "eaaf39d71d28545c80e3cda752e72cb5dc7b7b90.pdf": {
        "title": "The Future of Law Firms (and Lawyers) in the Age of Artificial Intelligence",
        "authors": [
            "A. Davis"
        ],
        "published_date": "2020",
        "abstract": "This article explores the future for lawyers and law firms in the light of the changes that Artificial Intelligence (\u201cAI\u201d) is already bringing to the universe of legal services. Part I briefly describes some of the ways AI is already in use in ordinary life \u2013 from facial recognition, through medical diagnosis to translation services. Part II describes how AI is transforming what it means to provide legal services in six primary areas: litigation review; expertise automation; legal research; contract analytics; contract and litigation document generation; and predictive analytics. Part III explores who are the providers of these AI driven legal services \u2013 often non-lawyer legal service providers \u2013 and how these providers are replacing at least some of what clients have traditionally sought from lawyers. Part III also discusses the implications of all these changes both for the future role of lawyers individually, and in particular what services will clients still need lawyers to perform: judgment, empathy, creativity and adaptability. In turn, this Part examines what will these changes mean for the size, shape, composition and economic model of law firms, as well as the implications of these changes for legal education and lawyer training. Part IV identifies the principal legal, ethical, regulatory and risk management issues raised by the use of AI in the provision of legal services. Finally, in Part V the article considers who will be the likely providers of AI based services other than law firms: legal publishers, major accounting firms and venture capital funded businesses.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/eaaf39d71d28545c80e3cda752e72cb5dc7b7b90.pdf",
        "venue": "",
        "citationCount": 24,
        "score": 4.800000000000001,
        "summary": "This article explores the future for lawyers and law firms in the light of the changes that Artificial Intelligence (\u201cAI\u201d) is already bringing to the universe of legal services. Part I briefly describes some of the ways AI is already in use in ordinary life \u2013 from facial recognition, through medical diagnosis to translation services. Part II describes how AI is transforming what it means to provide legal services in six primary areas: litigation review; expertise automation; legal research; contract analytics; contract and litigation document generation; and predictive analytics. Part III explores who are the providers of these AI driven legal services \u2013 often non-lawyer legal service providers \u2013 and how these providers are replacing at least some of what clients have traditionally sought from lawyers. Part III also discusses the implications of all these changes both for the future role of lawyers individually, and in particular what services will clients still need lawyers to perform: judgment, empathy, creativity and adaptability. In turn, this Part examines what will these changes mean for the size, shape, composition and economic model of law firms, as well as the implications of these changes for legal education and lawyer training. Part IV identifies the principal legal, ethical, regulatory and risk management issues raised by the use of AI in the provision of legal services. Finally, in Part V the article considers who will be the likely providers of AI based services other than law firms: legal publishers, major accounting firms and venture capital funded businesses.",
        "keywords": []
    },
    "abf3e27f04960692433d85a79a21b1332d4cdc9d.pdf": {
        "title": "The promise and perils of AI in medicine",
        "authors": [
            "Robert W. Sparrow",
            "Joshua Hatherley"
        ],
        "published_date": "2019",
        "abstract": "LANGUAGE NOTE | Document text in English; abstract also in Chinese.\u4eba\u5de5\u667a\u80fd\uff08\uff21\uff29\uff09\u5c07\u5982\u4f55\u4fc3\u9032\u4eba\u985e\u7684\u91ab\u7642\u4fdd\u5065\uff1f\u5982\u679c\u6211\u5011\u64d4\u5fc3\u4eba\u5de5\u667a\u80fd\u4ecb\u5165\u91ab\u7642\u7684\u98a8\u96aa\uff0c\u6211\u5011\u53c8\u61c9\u8a72\u95dc\u6ce8\u4ec0\u9ebd\u5462\uff1f\u672c\u6587\u8a66\u5716\u6982\u8ff0\u6b64\u985e\u554f\u984c\uff0c\u4e26\u5c0d\u4eba\u5de5\u667a\u80fd\u4ecb\u5165\u91ab\u7642\u7684\u98a8\u96aa\u8207\u5e0c\u671b\u4f5c\u4e00\u500b\u521d\u6b65\u8a55\u50f9\u3002\u4eba\u5de5\u667a\u80fd\u4f5c\u70ba\u4e00\u7a2e\u7814\u7a76\u5de5\u5177\u548c\u8a3a\u65b7\u5de5\u5177\u5177\u6709\u5de8\u5927\u7684\u6f5b\u529b\uff0c\u7279\u5225\u662f\u5728\u57fa\u56e0\u7d44\u5b78\u548c\u516c\u5171\u885b\u751f\u9818\u57df\u4e2d\u3002\u4eba\u5de5\u667a\u80fd\u5728\u91ab\u7642\u4e2d\u7684\u5ee3\u6cdb\u4f7f\u7528\u53ef\u80fd\u9084\u6703\u5c0d\u91ab\u7642\u7cfb\u7d71\u7684\u7d44\u7e54\u65b9\u5f0f\u548c\u5546\u696d\u5be6\u8e10\u7522\u751f\u6df1\u523b\u7684\u5f71\u97ff\uff0c\u800c\u9019\u4e9b\u5f71\u97ff\u7684\u65b9\u5f0f\u8207\u7a0b\u5ea6\u9084\u6c92\u6709\u88ab\u5145\u5206\u8a8d\u8b58\u5230\u3002\u5728\u4eba\u5de5\u667a\u80fd\u91ab\u5b78\u7684\u71b1\u60c5\u64c1\u8b77\u8005\u770b\u4f86\uff0c\u61c9\u7528\u4eba\u5de5\u667a\u80fd\u53ef\u4ee5\u5e6b\u52a9\u91ab\u751f\u96c6\u4e2d\u7cbe\u529b\u5728\u5c0d\u4ed6\u5011\u548c\u75c5\u4eba\u800c\u8a00\u771f\u6b63\u91cd\u8981\u7684\u554f\u984c\u4e0a\u3002\u7136\u800c\uff0c\u672c\u6587\u5c07\u8ad6\u8b49\u9019\u4e9b\u6a02\u89c0\u7684\u5224\u65b7\u662f\u57fa\u65bc\u5c0d\u73fe\u4ee3\u91ab\u7642\u74b0\u5883\u4e0b\u6a5f\u69cb\u548c\u7d93\u6fdf\u904b\u884c\u898f\u5247\u7684\u4e00\u4e9b\u4e0d\u5408\u60c5\u7406\u7684\u5047\u8a2d\u4e4b\u4e0a\u3002\u672c\u6587\u5c07\u805a\u7126\u65bc\u5982\u4e0b\u4e00 \u4e9b\u91cd\u8981\u8b70\u984c\uff1a\u5927\u8cc7\u6599\u4e2d\u7684\u96b1\u79c1\u3001\u76e3\u7ba1\u548c\u504f\u898b\uff0c\u904e\u5206\u4fe1\u4efb\u6a5f\u5668\u7684\u98a8\u96aa\uff0c\u900f\u660e\u5ea6\u554f\u984c\uff0c\u91ab\u7642\u5c08\u696d\u4eba\u58eb\u7684\u201c\u53bb\u6280\u80fd\u5316\u201d\u554f\u984c\uff0c\u4eba\u5de5\u667a\u80fd\u91cd\u5851\u91ab\u7642\u4fdd\u5065\u7684\u65b9\u5f0f\uff0c\u4ee5\u53ca\u4eba\u5de5\u667a\u80fd\u5c0d\u91ab\u7642\u4fdd\u5065\u4e2d\u6b0a\u529b\u5206\u914d\u7684\u5f71\u97ff\u3002\u5176\u4e2d\u6709\u5169\u500b\u95dc\u9375\u7684\u554f\u984c\u5c24\u5176\u503c\u5f97\u54f2\u5b78\u5bb6\u548c\u751f\u547d\u502b\u7406\u5b78\u5bb6\u7684\u9032\u4e00\u6b65\u95dc\u6ce8\u3002\u7b2c\u4e00\uff0c\u7576\u91ab\u751f\u4e0d\u50c5\u9700\u8981\u8655\u7406\u4eba\u800c\u4e14\u9700\u8981\u8655\u7406\u8cc7\u6599\u7684\u6642\u5019\uff0c\u91ab\u7642\u5be6\u8e10\u6703\u5448\u73fe\u51fa\u4ec0\u9ebd\u6a23\u7684\u5f62\u614b\uff1f\u7b2c\u4e8c\uff0c\u5728\u91ab\u7642\u6c7a\u7b56\u6b0a\u8861\u4e2d\uff0c\u6211\u5011\u61c9\u8a72\u7ed9\u4e88\u4f86\u81ea\u6a5f\u5668\u7684\u610f\u898b\u4ee5\u591a\u5927\u7684\u6b0a\u91cd\uff1fWhat does Artificial Intelligence (AI) have to contribute to health care? And what should we be looking out for if we are worried about its risks? In this paper we offer a survey, and initial evaluation, of hopes and fears about the applications of artificial intelligence in medicine. AI clearly has enormous potential as a research tool, in genomics and public health especially, as well as a diagnostic aid. It\u2019s also highly likely to impact on the organisational and business practices of healthcare systems in ways that are perhaps under-appreciated. Enthusiasts for AI have held out the prospect that it will free physicians up to spend more time attending to what really matters to them and their patients. We will argue that this claim depends upon implausible assumptions about the institutional and economic imperatives operating in contemporary healthcare settings. We will also highlight important concerns about privacy, surveillance, and bias in big data, as well as the risks of over trust in machines, the challenges of transparency, the deskilling of healthcare practitioners, the way AI reframes healthcare, and the implications of AI for the distribution of power in healthcare institutions. We will suggest that two questions, in particular, are deserving of further attention from philosophers and bioethicists. What does care look like when one is dealing with data as much as people? And, what weight should we give to the advice of machines in our own deliberations about medical decisions?DOWNLOAD HISTORY | This article has been downloaded 119 times in Digital Commons before migrating into this platform.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/abf3e27f04960692433d85a79a21b1332d4cdc9d.pdf",
        "venue": "International Journal of Chinese & Comparative Philosophy of Medicine",
        "citationCount": 28,
        "score": 4.666666666666666,
        "summary": "Here is a focused summary of the paper by \\cite{sparrow2019147} for a literature review, emphasizing technical aspects and empirical validation where applicable, and noting the paper's primary focus.\n\n---\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper does not address a specific technical problem with AI itself. Instead, it provides a comprehensive survey and initial evaluation of the *hopes and fears* surrounding the application of Artificial Intelligence (AI) in medicine.\n    *   **Importance and Challenge:** The problem is important because AI holds enormous potential to transform healthcare (e.g., as a research tool, diagnostic aid, and in administration), but also introduces significant ethical, social, and practical challenges that need careful consideration to maximize benefits and mitigate risks. The challenge lies in understanding the multifaceted impact of AI beyond its technical capabilities, particularly in real-world healthcare settings.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions itself as a critical review and ethical analysis of existing and emerging AI applications in medicine. It references various technical advancements (e.g., deep learning in medical imaging, IBM Watson, Alphafold) and the claims made by proponents of AI in healthcare.\n    *   **Limitations of Previous Solutions:** The paper highlights limitations not of AI *solutions* themselves, but of the *unquestioning enthusiasm* surrounding them. It critiques the assumptions that AI will inherently \"rehumanize\" medicine or reduce healthcare costs, arguing these depend on implausible assumptions about institutional and economic imperatives. It also points out methodological limitations in studies comparing AI to human clinicians in diagnostic tasks \\cite{Liu et al. 2019}, noting that many promising results lack clinical validation.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm:** The paper *surveys* various core AI techniques relevant to medicine, including:\n        *   **Machine Learning (ML) and Deep Learning (DL):** Used for pattern identification in large datasets, particularly in medical imaging for diagnosis (e.g., diabetic retinopathy, skin/breast cancers \\cite{Gulshan et al. 2016, Esteva et al. 2017, Golden 2017}).\n        *   **Natural Language Processing (NLP):** Applied in systems like IBM Watson for trawling medical literature to recommend treatments \\cite{Somashekhar et al. 2017} and potentially for automating clinical note-taking.\n        *   **Algorithmic Approaches:** Utilized in genomics (genetic sequencing, genome-wide association studies), drug discovery (predicting efficacious molecules, e.g., DeepMind's Alphafold \\cite{AlQuraishi 2019}), and data mining Electronic Health Records (EHRs) for phenotypes and biomarkers \\cite{Chen et al. 2017}.\n    *   **Novelty/Difference:** The paper's novelty lies not in presenting a new technical approach, but in its *philosophical and ethical evaluation* of these existing and emerging AI technologies within the medical context. It critically examines the broader implications of AI deployment, moving beyond purely technical performance metrics.\n\n*   **Key Technical Contributions**\n    *   This paper does not present novel algorithms, methods, system designs, or theoretical insights from the authors' own technical research. Its contribution is primarily in the domain of *philosophy, bioethics, and critical analysis* of AI's societal impact in medicine. It synthesizes existing technical applications and critically evaluates their ethical, social, and economic consequences.\n\n*   **Experimental Validation**\n    *   The paper does not present any original experimental validation or empirical results from the authors' research. It references existing studies and their findings (e.g., AI outperforming humans in specific diagnostic tasks), but also critically notes the methodological limitations and lack of clinical validation in many of these studies \\cite{Liu et al. 2019}. The paper's validation comes from its logical arguments and ethical reasoning, rather than empirical data.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper highlights several limitations and assumptions regarding AI deployment:\n        *   AI's reliance on good data, understanding of causal relations, and experimental design, noting that AI isn't \"magic\" and human error in interpretation can lead it astray.\n        *   The \"black box\" nature of some AI systems, raising concerns about transparency and procedural justice.\n        *   The potential for AI to increase, rather than decrease, data demands on physicians due to monitoring requirements and liability concerns.\n    *   **Scope of Applicability:** The paper's analysis applies broadly to the integration of AI across various domains of medicine: research (genomics, drug discovery), diagnosis (medical imaging, clinical data), and administration (scheduling, billing, insurance, managed care). Its ethical and philosophical arguments are relevant to policymakers, healthcare professionals, AI developers, and patients.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** While not advancing the technical state-of-the-art in AI algorithms, \\cite{sparrow2019147} significantly advances the *critical discourse* surrounding AI in medicine. It moves beyond optimistic projections to systematically identify and analyze the \"perils\" alongside the \"promise.\"\n    *   **Potential Impact on Future Research:** The paper's primary impact is on future research in bioethics, medical philosophy, and health policy. It calls for further attention to crucial questions such as: \"What does care look like when one is dealing with data as much as people?\" and \"What weight should we give to the advice of machines in our own deliberations about medical decisions?\" It encourages a more nuanced and cautious approach to AI integration, emphasizing the need for \"clever design of AI and a concerted campaign by the medical profession to resist the economic and institutional imperatives\" that might otherwise undermine patient and physician well-being.",
        "keywords": [
            "Artificial Intelligence in medicine",
            "Ethical and social challenges",
            "Philosophical evaluation of AI",
            "Critical discourse",
            "Machine Learning/Deep Learning",
            "Natural Language Processing",
            "Medical imaging diagnosis",
            "Drug discovery and genomics",
            "Clinical validation limitations",
            "Black box AI transparency",
            "Healthcare transformation",
            "Economic imperatives",
            "Patient and physician well-being"
        ],
        "paper_type": "the paper should be classified as **position**.\n\nhere's why:\n\n1.  **explicit argumentation:** the abstract uses strong argumentative language: \"we will **argue** that this claim depends upon implausible assumptions...\", \"we will also **highlight** important concerns...\", \"we will **suggest** that two questions... are deserving of further attention...\". this directly aligns with the \"position\" criteria: \"argues for viewpoint or future direction.\"\n\n2.  **focus on problems and directions:** the paper identifies \"perils\" (privacy, surveillance, bias, over-trust, transparency, deskilling, power distribution) and proposes specific areas for \"further attention from philosophers and bioethicists.\" this matches the \"position\" criteria: \"current problems, proposed direction.\"\n\n3.  **\"survey\" as a means to an end:** while the abstract states \"we offer a survey, and initial evaluation, of hopes and fears,\" this survey appears to be the foundation upon which the paper builds its critical arguments and proposed future directions. it's not a comprehensive literature review focused on organizing existing knowledge (as a typical \"survey\" paper would emphasize with classification schemes or exhaustive literature organization), but rather an evaluation used to establish a particular viewpoint.\n\n4.  **venue alignment:** the \"international journal of chinese & comparative philosophy of medicine\" is a philosophical journal, which frequently publishes papers that present arguments, ethical analyses, and conceptual positions on topics, rather than purely technical, empirical, or comprehensive literature surveys in the traditional sense."
    },
    "2e7e9a40e852d83bca34ff4cb44fda727b281b84.pdf": {
        "title": "A novel endoimaging system for endoscopic 3D reconstruction in bladder cancer patients",
        "authors": [
            "R. Suarez-Ibarrola",
            "M. Kriegmair",
            "F. Waldbillig",
            "B. Gr\u00fcne",
            "Misgana Negassi",
            "Ujwala Parupalli",
            "Annette Schmitt",
            "A. Reiterer",
            "Christoph M\u00fcller",
            "A. Scheurer",
            "S. Baur",
            "K. Klein",
            "J. Fallert",
            "L. M\u00fcndermann",
            "Jenshika Yoganathan",
            "Marco Probst",
            "P. Ihle",
            "Neven Bobic",
            "T. Schumm",
            "H. Rehn",
            "A. Betke",
            "Michael Graurock",
            "M. Forrer",
            "C. Gratzke",
            "A. Miernik",
            "S. Hein"
        ],
        "published_date": "2020",
        "abstract": "Abstract Introduction The methods employed to document cystoscopic findings in bladder cancer patients lack accuracy and are subject to observer variability. We propose a novel endoimaging system and an online documentation platform to provide post-procedural 3D bladder reconstructions for improved diagnosis, management and follow-up. Material and methods The RaVeNNA4pi consortium is comprised of five industrial partners, two university hospitals and two technical institutes. These are grouped into hardware, software and clinical partners according to their professional expertise. The envisaged endoimaging system consists of an innovative cystoscope that generates 3D bladder reconstructions allowing users to remotely access a cloud-based centralized database to visualize individualized 3D bladder models from previous cystoscopies archived in DICOM format. Results Preliminary investigations successfully tracked the endoscope's rotational and translational movements. The structure-from-motion pipeline was tested in a bladder phantom and satisfactorily demonstrated 3D reconstructions of the processing sequence. AI-based semantic image segmentation achieved a 0.67 dice-score-coefficient over all classes. An online-platform allows physicians and patients to digitally visualize endoscopic findings by navigating a 3D bladder model. Conclusions Our work demonstrates the current developments of a novel endoimaging system equipped with the potential to generate 3D bladder reconstructions from cystoscopy videos and AI-assisted automated detection of bladder tumors.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/2e7e9a40e852d83bca34ff4cb44fda727b281b84.pdf",
        "venue": "MITAT. Minimally invasive therapy & allied technologies",
        "citationCount": 23,
        "score": 4.6000000000000005,
        "summary": "Abstract Introduction The methods employed to document cystoscopic findings in bladder cancer patients lack accuracy and are subject to observer variability. We propose a novel endoimaging system and an online documentation platform to provide post-procedural 3D bladder reconstructions for improved diagnosis, management and follow-up. Material and methods The RaVeNNA4pi consortium is comprised of five industrial partners, two university hospitals and two technical institutes. These are grouped into hardware, software and clinical partners according to their professional expertise. The envisaged endoimaging system consists of an innovative cystoscope that generates 3D bladder reconstructions allowing users to remotely access a cloud-based centralized database to visualize individualized 3D bladder models from previous cystoscopies archived in DICOM format. Results Preliminary investigations successfully tracked the endoscope's rotational and translational movements. The structure-from-motion pipeline was tested in a bladder phantom and satisfactorily demonstrated 3D reconstructions of the processing sequence. AI-based semantic image segmentation achieved a 0.67 dice-score-coefficient over all classes. An online-platform allows physicians and patients to digitally visualize endoscopic findings by navigating a 3D bladder model. Conclusions Our work demonstrates the current developments of a novel endoimaging system equipped with the potential to generate 3D bladder reconstructions from cystoscopy videos and AI-assisted automated detection of bladder tumors.",
        "keywords": []
    },
    "c9040f88c7f1e99c261e4329296d51a70c953eb2.pdf": {
        "title": "GENERATOR Breast DataMart\u2014The Novel Breast Cancer Data Discovery System for Research and Monitoring: Preliminary Results and Future Perspectives",
        "authors": [
            "F. Marazzi",
            "L. Tagliaferri",
            "V. Masiello",
            "F. Moschella",
            "G. Colloca",
            "B. Corvari",
            "A. Sanchez",
            "N. Capocchiano",
            "R. Pastorino",
            "C. Iacomini",
            "J. Lenkowicz",
            "C. Masciocchi",
            "S. Patarnello",
            "G. Franceschini",
            "M. Gambacorta",
            "R. Masetti",
            "V. Valentini"
        ],
        "published_date": "2021",
        "abstract": "Background: Artificial Intelligence (AI) is increasingly used for process management in daily life. In the medical field AI is becoming part of computerized systems to manage information and encourage the generation of evidence. Here we present the development of the application of AI to IT systems present in the hospital, for the creation of a DataMart for the management of clinical and research processes in the field of breast cancer. Materials and methods: A multidisciplinary team of radiation oncologists, epidemiologists, medical oncologists, breast surgeons, data scientists, and data management experts worked together to identify relevant data and sources located inside the hospital system. Combinations of open-source data science packages and industry solutions were used to design the target framework. To validate the DataMart directly on real-life cases, the working team defined tumoral pathology and clinical purposes of proof of concepts (PoCs). Results: Data were classified into \u201cNot organized, not \u2018ontologized\u2019 data\u201d, \u201cOrganized, not \u2018ontologized\u2019 data\u201d, and \u201cOrganized and \u2018ontologized\u2019 data\u201d. Archives of real-world data (RWD) identified were platform based on ontology, hospital data warehouse, PDF documents, and electronic reports. Data extraction was performed by direct connection with structured data or text-mining technology. Two PoCs were performed, by which waiting time interval for radiotherapy and performance index of breast unit were tested and resulted available. Conclusions: GENERATOR Breast DataMart was created for supporting breast cancer pathways of care. An AI-based process automatically extracts data from different sources and uses them for generating trend studies and clinical evidence. Further studies and more proof of concepts are needed to exploit all the potentials of this system.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c9040f88c7f1e99c261e4329296d51a70c953eb2.pdf",
        "venue": "Journal of Personalized Medicine",
        "citationCount": 18,
        "score": 4.5,
        "summary": "Background: Artificial Intelligence (AI) is increasingly used for process management in daily life. In the medical field AI is becoming part of computerized systems to manage information and encourage the generation of evidence. Here we present the development of the application of AI to IT systems present in the hospital, for the creation of a DataMart for the management of clinical and research processes in the field of breast cancer. Materials and methods: A multidisciplinary team of radiation oncologists, epidemiologists, medical oncologists, breast surgeons, data scientists, and data management experts worked together to identify relevant data and sources located inside the hospital system. Combinations of open-source data science packages and industry solutions were used to design the target framework. To validate the DataMart directly on real-life cases, the working team defined tumoral pathology and clinical purposes of proof of concepts (PoCs). Results: Data were classified into \u201cNot organized, not \u2018ontologized\u2019 data\u201d, \u201cOrganized, not \u2018ontologized\u2019 data\u201d, and \u201cOrganized and \u2018ontologized\u2019 data\u201d. Archives of real-world data (RWD) identified were platform based on ontology, hospital data warehouse, PDF documents, and electronic reports. Data extraction was performed by direct connection with structured data or text-mining technology. Two PoCs were performed, by which waiting time interval for radiotherapy and performance index of breast unit were tested and resulted available. Conclusions: GENERATOR Breast DataMart was created for supporting breast cancer pathways of care. An AI-based process automatically extracts data from different sources and uses them for generating trend studies and clinical evidence. Further studies and more proof of concepts are needed to exploit all the potentials of this system.",
        "keywords": []
    },
    "ad2d689b1f0901930d7c3408e322c9aac360f4ea.pdf": {
        "title": "Economic and utilization outcomes of medication management at a large Medicaid plan with disease management pharmacists using a novel artificial intelligence platform from 2018 to 2019: a retrospective observational study using regression methods",
        "authors": [],
        "published_date": "2021",
        "abstract": "BACKGROUND: Medication therapy management (MTM) and comprehensive medication management (CMM) have been practiced by clinical pharmacists as a predominantly manual activity with interventions documented in a record-keeping system. Program evaluations, largely based on estimations of projected savings and utilization reductions, have not accurately predicted actual claims and utilization changes, leading many to doubt the efficacy of medication management. OBJECTIVE: To assess the impact on actual medical claims of a novel artificial intelligence (AI) platform that identifies members and provides decision support to clinicians in performing telephonic interventions similar to MTM and CMM with high-risk Medicaid members. METHODS: This retrospective observational study used mixed-effects regression models that flexibly account for general trends in cost, as measured by actual claims, to identify the amount of savings and associated impact. To study the economics, total cost of care (TCoC), defined as all medication costs plus all noncapitated medical costs, was evaluated. Utilization was evaluated through the number of emergency department (ED) visits, hospital admissions, bed days, and readmissions. The study included 2,150 predominantly middle-aged (aged 40-64 years) Medicaid members with an average of 10 medications for chronic conditions among an average of 25 total medications. The analysis considered cost and utilization data from August 2017 through April 2019. Interventions occurred between January 2018 and February 2019. RESULTS: Statistically significant correlations were found between receiving interventions and decreased costs and utilization. The economic study found a 19.3% reduction in the TCoC (P < 0.001) that, applied to a preintervention monthly cost of $2,872, yielded a savings of $554 per member per month (PMPM). Medication costs showed a 17.4% reduction (P < 0.001), which, when applied to preintervention cost of $1,110, yielded a savings of $192 PMPM. The utilization study found a 15.1% reduction in ED visits (P = 0.002), a 9.4% reduction in hospital admissions (P = 0.008), and a 10.2% reduction in bed days (P = 0.01). Return on investment is 12.4:1 based on TCoC savings and program costs. CONCLUSIONS: This study evaluated the CMM-Wrap program, which used an advanced AI platform integrated with health plan data, clinical pharmacists trained in disease management, telephonic patient engagement, and closed-loop provider coordination. The results correlate cost and utilization savings with the program. The TCoC savings of $554 PMPM translates to approximately $1.2M a month and more than $14M annually for the 2,150 members in the study. We believe Medicaid and Medicare payment of AI enhanced telephonic CMM services would substantially decrease government health care expenditures, whereas improving health program expansion to Medicaid members with similar risks could save the Health Plan $109M annually. For instance, we estimate that California\u2019s Medicaid (Medi-Cal) program could save more than $1B annually by applying the program\u2019s observed impact to a similar high-risk cohort (about 1.6%) of Medi-Cal members. Additionally, benefits will accrue to nonmanaged health plans based on the savings themselves.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/ad2d689b1f0901930d7c3408e322c9aac360f4ea.pdf",
        "venue": "Journal of Managed Care & Specialty Pharmacy",
        "citationCount": 17,
        "score": 4.25,
        "summary": "BACKGROUND: Medication therapy management (MTM) and comprehensive medication management (CMM) have been practiced by clinical pharmacists as a predominantly manual activity with interventions documented in a record-keeping system. Program evaluations, largely based on estimations of projected savings and utilization reductions, have not accurately predicted actual claims and utilization changes, leading many to doubt the efficacy of medication management. OBJECTIVE: To assess the impact on actual medical claims of a novel artificial intelligence (AI) platform that identifies members and provides decision support to clinicians in performing telephonic interventions similar to MTM and CMM with high-risk Medicaid members. METHODS: This retrospective observational study used mixed-effects regression models that flexibly account for general trends in cost, as measured by actual claims, to identify the amount of savings and associated impact. To study the economics, total cost of care (TCoC), defined as all medication costs plus all noncapitated medical costs, was evaluated. Utilization was evaluated through the number of emergency department (ED) visits, hospital admissions, bed days, and readmissions. The study included 2,150 predominantly middle-aged (aged 40-64 years) Medicaid members with an average of 10 medications for chronic conditions among an average of 25 total medications. The analysis considered cost and utilization data from August 2017 through April 2019. Interventions occurred between January 2018 and February 2019. RESULTS: Statistically significant correlations were found between receiving interventions and decreased costs and utilization. The economic study found a 19.3% reduction in the TCoC (P < 0.001) that, applied to a preintervention monthly cost of $2,872, yielded a savings of $554 per member per month (PMPM). Medication costs showed a 17.4% reduction (P < 0.001), which, when applied to preintervention cost of $1,110, yielded a savings of $192 PMPM. The utilization study found a 15.1% reduction in ED visits (P = 0.002), a 9.4% reduction in hospital admissions (P = 0.008), and a 10.2% reduction in bed days (P = 0.01). Return on investment is 12.4:1 based on TCoC savings and program costs. CONCLUSIONS: This study evaluated the CMM-Wrap program, which used an advanced AI platform integrated with health plan data, clinical pharmacists trained in disease management, telephonic patient engagement, and closed-loop provider coordination. The results correlate cost and utilization savings with the program. The TCoC savings of $554 PMPM translates to approximately $1.2M a month and more than $14M annually for the 2,150 members in the study. We believe Medicaid and Medicare payment of AI enhanced telephonic CMM services would substantially decrease government health care expenditures, whereas improving health program expansion to Medicaid members with similar risks could save the Health Plan $109M annually. For instance, we estimate that California\u2019s Medicaid (Medi-Cal) program could save more than $1B annually by applying the program\u2019s observed impact to a similar high-risk cohort (about 1.6%) of Medi-Cal members. Additionally, benefits will accrue to nonmanaged health plans based on the savings themselves.",
        "keywords": []
    },
    "8da5fb56b6ade82c5eb621f858a4c8aa3f60f4ef.pdf": {
        "title": "Breaking down the silos of artificial intelligence in surgery: glossary of terms",
        "authors": [
            "A. Moglia",
            "K. Georgiou",
            "L. Morelli",
            "K. Toutouzas",
            "R. Satava",
            "A. Cuschieri"
        ],
        "published_date": "2022",
        "abstract": "The literature on artificial intelligence (AI) in surgery has advanced rapidly during the past few years. However, the published studies on AI are mostly reported by computer scientists using their own jargon which is unfamiliar to surgeons. A literature search was conducted in using PubMed following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) statement. The primary outcome of this review is to provide a glossary with definitions of the commonly used AI terms in surgery to improve their understanding by surgeons. One hundred ninety-five studies were included in this review, and 38 AI terms related to surgery were retrieved. Convolutional neural networks were the most frequently culled term by the search, accounting for 74 studies on AI in surgery, followed by classification task (n\u2009=\u200962), artificial neural networks (n\u2009=\u200953), and regression (n\u2009=\u200949). Then, the most frequent expressions were supervised learning (reported in 24 articles), support vector machine (SVM) in 21, and logistic regression in 16. The rest of the 38 terms was seldom mentioned. The proposed glossary can be used by several stakeholders. First and foremost, by residents and attending consultant surgeons, both having to understand the fundamentals of AI when reading such articles. Secondly, junior researchers at the start of their career in Surgical Data Science and thirdly experts working in the regulatory sections of companies involved in the AI Business Software as a Medical Device (SaMD) preparing documents for submission to the Food and Drug Administration (FDA) or other agencies for approval.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/8da5fb56b6ade82c5eb621f858a4c8aa3f60f4ef.pdf",
        "venue": "Surgical Endoscopy",
        "citationCount": 12,
        "score": 4.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The rapid advancement of Artificial Intelligence (AI) in surgery has led to a growing body of literature, but this literature is often reported by computer scientists using specialized jargon unfamiliar to surgeons \\cite{moglia2022vm0}. This creates a communication barrier.\n    *   **Importance and Challenge**: This problem is critical because AI is expected to significantly impact all stages of surgery (pre-, intra-, post-operative), and medical devices based on AI are entering the market. Surgeons' lack of understanding of AI terms leads to AI systems being perceived as \"black boxes,\" hindering adoption, effective use, and multidisciplinary collaboration between surgeons, engineers, and computer scientists \\cite{moglia2022vm0}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a systematic review that synthesizes and categorizes existing AI terminology used in surgical literature. It builds upon the growing body of AI applications in medicine and surgery.\n    *   **Limitations of Previous Solutions**: Previous reviews on AI in surgery often mention only a minority of the relevant AI terms or do not provide comprehensive explanations, leaving a gap in accessible definitions for the surgical community \\cite{moglia2022vm0}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper employs a systematic literature search methodology (PubMed, following PRISMA guidelines) to identify commonly used AI terms in surgical literature (specifically laparoscopy and robot-assisted surgery). It then compiles a comprehensive glossary with definitions for these terms.\n    *   **Novelty/Difference**: The innovation lies in the *systematic identification and definition* of 38 frequently used AI terms specifically tailored for the surgical context, based on their actual occurrence in recent surgical literature. This approach provides a data-driven, focused glossary designed to bridge the knowledge gap for surgeons and related stakeholders \\cite{moglia2022vm0}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**: While not introducing new AI algorithms, the paper's contribution is a novel *methodology for identifying and curating* a domain-specific technical glossary through a systematic review of scientific literature.\n    *   **System Design or Architectural Innovations**: Not applicable, as this is a review and glossary.\n    *   **Theoretical Insights or Analysis**: The primary contribution is the *curated glossary* itself, which serves as a foundational resource. Additionally, the *quantitative analysis of term frequency* (e.g., identifying CNNs, classification, ANNs, and regression as the most frequently used terms) provides insight into the current focus of AI research in surgery \\cite{moglia2022vm0}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: A systematic literature search was performed on PubMed in September 2021, applying specific filters (last 5 years, humans, English language, journal articles, abstracts). Two independent reviewers screened titles and abstracts.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   195 studies were included in the final analysis.\n        *   38 distinct AI terms related to surgery were retrieved and defined \\cite{moglia2022vm0}.\n        *   Frequency analysis showed Convolutional Neural Networks (CNNs) as the most frequently culled term (74 studies), followed by classification (62), Artificial Neural Networks (ANNs) (53), and regression (49) \\cite{moglia2022vm0}.\n        *   Comparison with 10 other surgery-related AI reviews demonstrated that this work provides a significantly more comprehensive list of defined terms; for instance, the most comprehensive prior review mentioned only 21 out of the 38 terms (55%), while others mentioned far fewer \\cite{moglia2022vm0}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: The search was limited to PubMed, English language, and articles published within the last 5 years, potentially excluding relevant older or non-English literature. The three fundamental terms (AI, ML, DL) were intentionally excluded from the 38 terms identified \\cite{moglia2022vm0}.\n    *   **Scope of Applicability**: The glossary is primarily intended for residents and attending consultant surgeons, junior researchers in Surgical Data Science, and experts in regulatory sections of companies developing AI Software as a Medical Device (SaMD) \\cite{moglia2022vm0}.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This paper significantly advances the *understanding and accessibility* of AI in surgery by systematically addressing the technical jargon barrier. It does not introduce new AI algorithms but provides a critical tool for interdisciplinary communication and education.\n    *   **Potential Impact on Future Research**: The glossary will facilitate better comprehension of AI research by surgeons, fostering more effective multidisciplinary collaboration. It can accelerate the integration of AI into clinical practice, support the training of future surgical data scientists, and aid in the regulatory approval processes for AI-based medical devices \\cite{moglia2022vm0}.",
        "keywords": [
            "AI in surgery",
            "AI terminology",
            "communication barrier",
            "systematic review",
            "curated glossary",
            "multidisciplinary collaboration",
            "Convolutional Neural Networks (CNNs)",
            "frequency analysis",
            "Surgical Data Science",
            "Software as a Medical Device (SaMD)",
            "knowledge gap",
            "PRISMA guidelines"
        ],
        "paper_type": "based on the abstract and introduction, this paper clearly fits the **survey** type.\n\nhere's why:\n\n*   **abstract mentions:** \"a literature search was conducted... following the preferred reporting items for systematic reviews and meta-analyses (prisma) statement. the primary outcome of this review is to provide a glossary with definitions of the commonly used ai terms in surgery...\" and \"one hundred ninety-five studies were included in this review...\"\n*   **introduction discusses:** \"the primary outcome of this review is to provide definitions of the commonly used ai terms in surgery to simplify their understanding by surgeons.\" and \"the secondary outcome is to provide, in a supplement, a detailed list of surgical articles in which ai terminology is used.\"\n*   **methods section:** details a systematic literature search and analysis process, which is characteristic of a comprehensive review.\n*   **results section:** presents findings from the literature review, such as the frequency of terms and the glossary itself.\n*   **discussion/conclusion:** reiterates the goal of \"review[ing] and attempt[ing] to categorize the relevant terms as well as provide a glossary for surgeons.\"\n\nthe paper's core contribution is to synthesize and organize existing knowledge (ai terms in surgical literature) into a comprehensive glossary, which is a hallmark of a survey or review paper."
    },
    "f58197c2b69754c006019d7272bce08f3710ac7d.pdf": {
        "title": "Get Protected! Recommendations for Staff in IR",
        "authors": [
            "G. Bartal",
            "E. Va\u00f1\u00f3",
            "G. Paulo"
        ],
        "published_date": "2021",
        "abstract": "Evaluation and registration of patient and staff doses are mandatory under the current European legislation, and the occupational dose limits recommended by the ICRP have been adopted by most of the countries in the world. Relevant documents and guidelines published by international organisations and interventional radiology societies are referred. Any potential reduction of patient and staff doses should be compatible with the clinical outcomes of the procedures. The review summarises the most common protective measures and the needed quality control for them, the criteria to select the appropriate protection devices, and how to avoid unnecessary occupational radiation exposures. Moreover, the current and future advancements in personnel radiation protection using medical simulation with virtual and augmented reality, robotics, and artificial intelligence (AI) are commented. A section on the personnel radiation protection in the era of COVID-19 is introduced, showing the expanding role of the interventional radiology during the pandemic. The review is completed with a summary of the main factors to be considered in the selection of the appropriate radiation protection tools and practical advices to improve the protection of the staff.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/f58197c2b69754c006019d7272bce08f3710ac7d.pdf",
        "venue": "Cardiovascular and Interventional Radiology",
        "citationCount": 15,
        "score": 3.75,
        "summary": "Evaluation and registration of patient and staff doses are mandatory under the current European legislation, and the occupational dose limits recommended by the ICRP have been adopted by most of the countries in the world. Relevant documents and guidelines published by international organisations and interventional radiology societies are referred. Any potential reduction of patient and staff doses should be compatible with the clinical outcomes of the procedures. The review summarises the most common protective measures and the needed quality control for them, the criteria to select the appropriate protection devices, and how to avoid unnecessary occupational radiation exposures. Moreover, the current and future advancements in personnel radiation protection using medical simulation with virtual and augmented reality, robotics, and artificial intelligence (AI) are commented. A section on the personnel radiation protection in the era of COVID-19 is introduced, showing the expanding role of the interventional radiology during the pandemic. The review is completed with a summary of the main factors to be considered in the selection of the appropriate radiation protection tools and practical advices to improve the protection of the staff.",
        "keywords": []
    },
    "8916ed77f946a4592d00abf5d1ed5b46777e0803.pdf": {
        "title": "Artificial Intelligence Research During COVID-19 Pandemic: Contributed to Future Education",
        "authors": [
            "Nurhasan Nurhasan",
            "B. Prahani*",
            "Nadi Suprapto",
            "Muchamad Arif Al Ardha"
        ],
        "published_date": "2022",
        "abstract": "Besides being able to cause death, it turns out that the COVID-19 pandemic has caused problems in various sectors, including education, health, and Social Life. In addition, the COVID-19 pandemic also has an impact on the field of research, including the field of Artificial Intelligence (AI) which has become an interesting issue in the era of the Industrial Revolution 4.0. Based on Scopus data (search by article title, , and keywords), it shows that there are 381,691 documents. Results of Bibliometric through VOSViewer found some parameters or interrelationships among variables to capture the trend and novelty of researching on AI During COVID-19 Pandemic, such as researching on AI During COVID-19 Pandemic and technology, detection, patient, diagnosis, performance, radiologist, feature, education, and image. Implication research in general evidence, (1) AI researchers have bright career opportunities in the present and the future. This is evident from the condition of the central role of AI in all lines of human life, (2) The trend of AI research is also classified as having a very large contribution to have entered various fields during the COVID-19 pandemic. (3) Including the prevention of COVID-19 transmission with AI, various tools and media have emerged in the medical world that provides many benefits. (4) AI needs more exploration in education field research. In the last five years, AI has made a major contribution advancing the field of education. \u00a9 2022 Eskisehir Osmangazi University. All rights reserved.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/8916ed77f946a4592d00abf5d1ed5b46777e0803.pdf",
        "venue": "International Journal of Instruction",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "Besides being able to cause death, it turns out that the COVID-19 pandemic has caused problems in various sectors, including education, health, and Social Life. In addition, the COVID-19 pandemic also has an impact on the field of research, including the field of Artificial Intelligence (AI) which has become an interesting issue in the era of the Industrial Revolution 4.0. Based on Scopus data (search by article title, , and keywords), it shows that there are 381,691 documents. Results of Bibliometric through VOSViewer found some parameters or interrelationships among variables to capture the trend and novelty of researching on AI During COVID-19 Pandemic, such as researching on AI During COVID-19 Pandemic and technology, detection, patient, diagnosis, performance, radiologist, feature, education, and image. Implication research in general evidence, (1) AI researchers have bright career opportunities in the present and the future. This is evident from the condition of the central role of AI in all lines of human life, (2) The trend of AI research is also classified as having a very large contribution to have entered various fields during the COVID-19 pandemic. (3) Including the prevention of COVID-19 transmission with AI, various tools and media have emerged in the medical world that provides many benefits. (4) AI needs more exploration in education field research. In the last five years, AI has made a major contribution advancing the field of education. \u00a9 2022 Eskisehir Osmangazi University. All rights reserved.",
        "keywords": []
    },
    "96579c44f66812d47e835e1264cde4211c9f0bd4.pdf": {
        "title": "Common strategic research agenda for radiation protection in medicine",
        "authors": [
            "European Association of Nuclear Medicine",
            "European Federation of Organizations for Medical Physics",
            "European Federation of Radiographer Societies",
            "European Society of Radiology",
            "European Society for Radiotherapy and Oncology"
        ],
        "published_date": "2017",
        "abstract": "AbstractReflecting the change in funding strategies for European research projects, and the goal to jointly improve medical radiation protection through sustainable research efforts, five medical societies involved in the application of ionising radiation (European Association of Nuclear Medicine, EANM; European Federation of Organizations for Medical Physics. EFOMP; European Federation of Radiographer Societies, EFRS; European Society of Radiology, ESR; European Society for Radiotherapy and Oncology, ESTRO) have identified research areas of common interest and developed this first edition of the Common Strategic Research Agenda (SRA) for medical radiation protection.The research topics considered necessary and most urgent for effective medical care and efficient in terms of radiation protection are summarised in five main themes:1.Measurement and quantification in the field of medical applications of ionising radiation2.Normal tissue reactions, radiation-induced morbidity and long-term health problems3.Optimisation of radiation exposure and harmonisation of practices4.Justification of the use of ionising radiation in medical practice5.Infrastructures for quality assurance\n The SRA is a living document; thus comments and suggestions by all stakeholders in medical radiation protection are welcome and will be dealt with by the European Alliance for Medical Radiation Protection Research (EURAMED) established by the above-mentioned societies.Main messages\u2022 Overcome the fragmentation of medical radiation protection research in Europe\n \u2022 Identify research areas of joint interest in the field of medical radiation protection\n \u2022 Improve the use of ionising radiation in medicine\n \u2022 Collect stakeholder feedback and seek consensus\n \u2022 Emphasise importance of clinical translation and evaluation of research results",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/96579c44f66812d47e835e1264cde4211c9f0bd4.pdf",
        "venue": "Insights into Imaging",
        "citationCount": 29,
        "score": 3.625,
        "summary": "Here's a focused summary of the provided technical paper for a literature review, adhering to your citation and formatting requirements:\n\n---\n\n### Analysis of \"Radiological protection education and training for healthcare staff and students\" \\cite{medicine2017s6a}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the widespread deficiency in radiological protection (RP) education and training among healthcare staff and students, despite a continuous increase in the number and complexity of diagnostic and interventional medical procedures utilizing ionizing radiation \\cite{medicine2017s6a}. This deficiency leads to suboptimal dose management for both patients and medical professionals.\n    *   **Importance and Challenge:** This problem is critical because these procedures can result in high patient doses, potentially inducing deterministic health effects (e.g., skin injuries) and increasing the probability of stochastic effects (e.g., cancer) \\cite{medicine2017s6a}. Medical professionals conducting these procedures can also approach occupational dose limits. The challenge lies in establishing effective, relevant, practical, and time-efficient methods for delivering comprehensive RP education and training to a diverse range of healthcare personnel \\cite{medicine2017s6a}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon and significantly expands the basic recommendations for RP education and training previously outlined in ICRP Publications 103 and 105 \\cite{medicine2017s6a}.\n    *   **Limitations of Previous Solutions:** Previous ICRP recommendations were foundational but lacked specific, detailed guidance on the *delivery* and *content* of RP education and training for the various categories of medical staff and healthcare professionals involved in diagnostic and interventional procedures \\cite{medicine2017s6a}. This paper aims to fill that gap with more actionable guidance.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm:** The paper does not present a novel algorithm or system design in the traditional sense. Instead, its core technical approach is to provide a comprehensive *framework and detailed guidance* for the structured development and implementation of RP education and training programs \\cite{medicine2017s6a}. This includes:\n        *   Clearly defining \"education\" (imparting theoretical knowledge on radiation health effects, principles, legislation) and \"training\" (providing instruction and practical skills for specific ionizing radiation modalities like CT or fluoroscopy) \\cite{medicine2017s6a}.\n        *   Categorizing healthcare professionals based on their involvement with radiation and recommending tailored RP education and training content for each group (e.g., prescribers, medical students, radiologists, radiographers, medical physicists, interventional cardiologists) \\cite{medicine2017s6a}.\n        *   Outlining priority topics, specific educational objectives, and suggested methodologies for training delivery, including integration into academic curricula, postgraduate studies, and continuing medical education \\cite{medicine2017s6a}.\n        *   Proposing a system for the accreditation of organizations providing RP training and the certification of individuals who successfully complete it \\cite{medicine2017s6a}.\n    *   **Novelty/Difference:** The innovation lies in its comprehensive, structured, and practical guidance for *implementing* RP education and training, moving beyond general principles to address specific content, delivery methods, and a robust quality assurance framework (accreditation and certification) for a broad spectrum of healthcare professionals \\cite{medicine2017s6a}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   A formalized distinction between \"education\" and \"training\" in RP, emphasizing both foundational knowledge and practical, modality-specific skills \\cite{medicine2017s6a}.\n        *   Detailed, categorized recommendations for RP training content tailored to different medical specialties and roles, including specific examples for Nuclear Medicine, Interventional Radiology, and Paediatric Radiology (Annexes A and B) \\cite{medicine2017s6a}.\n    *   **System Design or Architectural Innovations:**\n        *   Proposed framework for the accreditation of training organizations and the certification of individuals, establishing a standardized approach to ensure competence in RP \\cite{medicine2017s6a}.\n        *   Defined roles and responsibilities for various stakeholders (e.g., regulators, universities, professional societies, industry) in the coordination, development, and financing of RP education and training programs \\cite{medicine2017s6a}.\n    *   **Theoretical Insights or Analysis:**\n        *   Comprehensive analysis of potential health effects from radiation exposure (deterministic, stochastic, and effects on embryo/fetus) as the fundamental basis for RP education, linking these effects directly to the necessity of dose management \\cite{medicine2017s6a}.\n\n5.  **Experimental Validation**\n    *   This paper is a guidance document and does not present experimental validation in the traditional scientific sense (e.g., controlled experiments, data collection, statistical analysis) \\cite{medicine2017s6a}.\n    *   Its recommendations are based on the collective expert judgment and consensus of the International Commission on Radiological Protection (ICRP) Committee 3, drawing upon existing scientific knowledge, observed deficiencies in RP practices, and the evolving landscape of medical imaging and interventional procedures \\cite{medicine2017s6a}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions:** The successful implementation of the recommendations relies on the availability of adequate resources (e.g., qualified medical physicists, dedicated training institutions, regulatory oversight) and the commitment of healthcare professionals and institutions to prioritize RP education \\cite{medicine2017s6a}.\n    *   **Scope of Applicability:** The guidance primarily focuses on RP in diagnostic (radiography, fluoroscopy, nuclear medicine) and interventional (fluoroscopically guided) medical procedures \\cite{medicine2017s6a}. It explicitly excludes radiation therapy modalities, except for aspects of nuclear medicine therapy \\cite{medicine2017s6a}. The recommendations are intended for global application to regulators, health authorities, professional bodies, industry, and academic institutions.\n\n7.  **Technical Significance**\n    *   **Advancement of the Technical State-of-the-Art:** This paper significantly advances the technical state-of-the-art in radiological protection by providing a comprehensive, actionable, and structured framework for education and training \\cite{medicine2017s6a}. It moves beyond general principles to offer detailed guidance on curriculum content, delivery methods, and a robust quality assurance system (accreditation and certification), which is crucial for practical implementation.\n    *   **Potential Impact on Future Research:**\n        *   It serves as a foundational document for the development of national and international curricula for RP education and training, promoting standardization and best practices \\cite{medicine2017s6a}.\n        *   It highlights the need for further research into the effectiveness of different pedagogical approaches for RP, particularly in developing practical skills for dose optimization in complex interventional procedures \\cite{medicine2017s6a}.\n        *   It could stimulate studies evaluating the impact of accredited training and certified competence on actual patient and occupational dose reduction, and on the incidence of radiation-induced effects.\n\n---",
        "keywords": [
            "Radiological protection education",
            "healthcare staff training",
            "ionizing radiation",
            "dose management",
            "diagnostic and interventional procedures",
            "patient and occupational safety",
            "accreditation and certification",
            "comprehensive guidance framework",
            "tailored training content",
            "radiation health effects",
            "medical physicists",
            "suboptimal dose management"
        ],
        "paper_type": "based on the provided content:\n\n*   **title:** \"common strategic research agenda for radiation protection in medicine\"\n*   **introduction:** the introduction highlights \"the need for a greater awareness of radiological protection,\" discusses \"education and training in rp,\" and outlines \"the knowledge that rp education and training should provide.\" it also mentions \"recommendations in publications 103 and 105\" and discusses \"consequences of failure to deliver training in rp\" and \"categories of medical and healthcare professionals requiring education and training.\" the recurring theme is identifying current problems (lack of awareness, insufficient training) and outlining what *should* be done or what is *required*. the word \"agenda\" in the title further reinforces a forward-looking, prescriptive approach.\n\nthis aligns best with the **position** paper criteria:\n*   **abstract mentions:** \"argue\", \"position\", \"vision\", \"future\", \"should\" (the introduction uses \"should provide\" and discusses \"need\" and \"requiring\").\n*   **introduction discusses:** \"current problems\" (lack of awareness, consequences of failure) and \"proposed direction\" (strategic research agenda, what training should provide).\n\nthe paper is arguing for a specific viewpoint and outlining a future direction or agenda for radiation protection in medicine.\n\n**classification: position**"
    },
    "b2deb192850ed9cc8d58aade49e0eda7ffa256fd.pdf": {
        "title": "Bibliometric Analysis of Articles on Accounting and Covid-19 during the Pandemic",
        "authors": [
            "I. Firmansyah",
            "Aam Slamet Rusydiana"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/b2deb192850ed9cc8d58aade49e0eda7ffa256fd.pdf",
        "venue": "",
        "citationCount": 14,
        "score": 3.5,
        "summary": "",
        "keywords": []
    },
    "227eb63a63d588f56d6f4d3d5e410a61381a804b.pdf": {
        "title": "BeCaked: An Explainable Artificial Intelligence Model for COVID-19 Forecasting",
        "authors": [
            "Duc Quang Nguyen",
            "N. Q. Vo",
            "T. Nguyen",
            "Khuong Nguyen-An",
            "Q. N. Nguyen",
            "D. N. Tran",
            "T. Quan"
        ],
        "published_date": "2021",
        "abstract": "From the end of 2019, one of the most serious and largest spread pandemics occurred in Wuhan (China) named Coronavirus (COVID-19). As reported by the World Health Organization, there are currently more than 100 million infectious cases with an average mortality rate of about five percent all over the world. To avoid serious consequences on people\u2019s lives and the economy, policies and actions need to be suitably made in time. To do that, the authorities need to know the future trend in the development process of this pandemic. This is the reason why forecasting models play an important role in controlling the pandemic situation. However, the behavior of this pandemic is extremely complicated and difficult to be analyzed, so that an effective model is not only considered on accurate forecasting results but also the explainable capability for human experts to take action pro-actively. With the recent advancement of Artificial Intelligence (AI) techniques, the emerging Deep Learning (DL) models have been proving highly effective when forecasting this pandemic future from the huge historical data. However, the main weakness of DL models is lacking the explanation capabilities. To overcome this limitation, we introduce a novel combination of the Susceptible-Infectious-Recovered-Deceased (SIRD) compartmental model and Variational Autoencoder (VAE) neural network known as BeCaked. With pandemic data provided by the Johns Hopkins University Center for Systems Science and Engineering, our model achieves 0.98 R2\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$R^2$$\\end{document} and 0.012 MAPE at world level with 31-step forecast and up to 0.99 R2\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$R^2$$\\end{document} and 0.0026 MAPE at country level with 15-step forecast on predicting daily infectious cases. Not only enjoying high accuracy, but BeCaked also offers useful justifications for its results based on the parameters of the SIRD model. Therefore, BeCaked can be used as a reference for authorities or medical experts to make on time right decisions.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/227eb63a63d588f56d6f4d3d5e410a61381a804b.pdf",
        "venue": "Scientific Reports",
        "citationCount": 14,
        "score": 3.5,
        "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### BeCaked: An Explainable Artificial Intelligence Model for COVID\u201119 Forecasting \\cite{nguyen2021mev}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of accurately forecasting COVID-19 infectious cases while simultaneously providing explainable insights for human experts to make timely and effective policy decisions.\n    *   **Importance & Challenge:** Accurate forecasting is crucial for pandemic control and economic stability. However, the complex behavior of pandemics makes effective modeling difficult. Existing Deep Learning (DL) models offer high accuracy but operate as \"black boxes,\" lacking explainability. Traditional mathematical models (e.g., SIRD) are explainable but suffer from subjective and difficult-to-estimate parameters, leading to lower accuracy. Bridging this gap between accuracy and explainability is a significant challenge.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Mathematical Models (e.g., SIR, SIRD, SEIRD, ARIMA):** These models provide explainable parameters but rely on transition rates that are difficult and subjectively estimated by human experts, leading to lower predictive performance \\cite{nguyen2021mev}.\n        *   **Machine Learning Models (e.g., LSTM-based, Variational-LSTM Autoencoder, NARNN):** DL models, particularly Recurrent Neural Networks (RNN) and Long Short Term Memory (LSTM), have shown high accuracy in forecasting time-series data like pandemic spread.\n    *   **Limitations of Previous Solutions:**\n        *   **Mathematical Models:** Parameters are often biasedly determined and lack \"sensitivity\" to real-world data, resulting in suboptimal performance \\cite{nguyen2021mev}.\n        *   **Machine Learning Models (especially DL):** They are typically \"black-box\" models, meaning their predictions lack internal justification or insights into *why* a particular forecast is made. This prevents medical experts from understanding root causes and formulating proactive action plans, despite high accuracy \\cite{nguyen2021mev}. Some SOTA DL models also require extensive, detailed input data (e.g., contact tracing, testing numbers) which may not be available in all regions.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces BeCaked, a novel semi-supervised Explainable AI model that combines the Susceptible-Infectious-Recovered-Deceased (SIRD) compartmental model with a Variational Autoencoder (VAE) neural network, enhanced by LSTM layers.\n    *   **Novelty/Difference:**\n        *   **Hybrid Architecture:** BeCaked integrates the strengths of both mathematical (SIRD) and DL (LSTM, VAE) models. LSTM layers are used to extract \"sequence\" features from historical time-series data.\n        *   **Explainable Encoding:** The key innovation is a *modified Autoencoder architecture* that is explicitly enforced to encode the processed sequence data from the LSTM layers directly into the parameters (\u03b2, \u03b3, \u03bc) of the SIRD model. This forces the DL component to learn representations that are directly interpretable within the context of an established epidemiological model.\n        *   **End-to-End Trainable & Semi-Supervised:** The entire architecture is end-to-end trainable and leverages semi-supervised learning principles (inherent to Autoencoders), reducing the need for extensively labeled data.\n        *   **Justification for Forecasts:** Unlike traditional black-box DL models, BeCaked provides justifications for its forecasting results based on the learned SIRD parameters, offering actionable insights to experts.\n\n4.  **Key Technical Contributions**\n    *   **Novel Hybrid Architecture:** Development of BeCaked, a unique combination of LSTM, a modified Variational Autoencoder, and the SIRD epidemiological model, designed for both high accuracy and explainability in pandemic forecasting.\n    *   **Explainable AI Mechanism:** Introduction of a method to make DL-based forecasting explainable by explicitly mapping latent representations to interpretable epidemiological parameters (SIRD's \u03b2, \u03b3, \u03bc) through a constrained Autoencoder.\n    *   **Semi-Supervised Learning for Explainability:** Demonstrating how a semi-supervised approach (VAE) can be adapted to provide meaningful, human-understandable insights from complex time-series data.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The BeCaked model was evaluated on real-world COVID-19 pandemic data provided by the Johns Hopkins University Center for Systems Science and Engineering.\n    *   **Key Performance Metrics & Results:**\n        *   **Metrics:** R-squared (R\u00b2) for goodness of fit and Mean Absolute Percentage Error (MAPE) for prediction accuracy.\n        *   **World Level:** Achieved 0.98 R\u00b2 and 0.012 MAPE for a 31-step forecast of daily infectious cases.\n        *   **Country Level:** Achieved up to 0.99 R\u00b2 and 0.0026 MAPE for a 15-step forecast of daily infectious cases.\n        *   **Comparison:** The paper states that BeCaked enjoys high accuracy compared to some state-of-the-art (SOTA) models, while also providing analytical insights into the pandemic's dynamics through SIRD parameters.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The provided text does not explicitly list technical limitations of BeCaked itself. However, it implicitly assumes the SIRD model's underlying epidemiological dynamics are sufficient for explainability. The complexity of pandemic behavior is acknowledged as a general challenge.\n    *   **Scope of Applicability:** The model is specifically designed for forecasting infectious disease spread, particularly COVID-19, using historical time-series data. Its explainability is tied to the SIRD model parameters, making it suitable for scenarios where these epidemiological insights are valuable for decision-making.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** BeCaked significantly advances the technical state-of-the-art by effectively bridging the gap between the high predictive accuracy of Deep Learning models and the crucial explainability offered by traditional mathematical epidemiological models. It provides a concrete method for making complex DL forecasts transparent and interpretable.\n    *   **Potential Impact:** The model's ability to provide both accurate forecasts and actionable justifications (via SIRD parameters) can empower authorities and medical experts to make more informed, timely, and proactive decisions regarding public health policies (e.g., lockdown measures, resource allocation). It fosters trust in AI systems for critical applications like pandemic management.",
        "keywords": [
            "BeCaked",
            "Explainable AI (XAI)",
            "COVID-19 forecasting",
            "Hybrid AI architecture",
            "SIRD compartmental model",
            "Variational Autoencoder (VAE)",
            "LSTM neural networks",
            "Bridging accuracy and explainability",
            "Epidemiological parameters",
            "Semi-supervised learning",
            "Actionable insights",
            "Pandemic control",
            "Constrained Autoencoder",
            "High predictive accuracy"
        ],
        "paper_type": "based on the abstract and introduction, this paper clearly fits the **technical** classification.\n\nhere's why:\n\n*   **presents new methods, algorithms, or systems:** the paper explicitly states its purpose is to introduce \"becaked: an explainable artificial intelligence model for covid-19 forecasting.\" it details the architecture of this model, including its encoder (lstm layers, fc layers), decoder (approximating sird model with euler method), parameter encoding, and training process (mse loss, adam optimizer). the entire \"the becaked model\" section is dedicated to describing this novel system.\n*   **abstract mentions: \"propose\", \"develop\", \"present\", \"algorithm\", \"method\":** the text uses phrases like \"the becaked model can be regarded as a variation...\", \"we present the equations...\", \"our proposed method\", \"our proposed solution\". it describes the \"operational mechanism of becaked\" and its \"implementation of proposed model.\"\n*   **introduction discusses: technical problem, proposed solution:** the paper addresses the problem of covid-19 forecasting and explainability, and then presents \"becaked\" as its solution, detailing its technical components and how it works.\n\nwhile the paper also contains a significant **empirical** component (performance evaluation, data preparation, comparison with other models using r2 and mape metrics), the core contribution and primary focus, as described in the abstract and introduction, is the development and presentation of the novel \"becaked\" model. the empirical evaluation serves to validate this technical contribution.\n\ntherefore, the most appropriate classification is **technical**."
    },
    "cc1eb116753ba3b14e047b2396ef5ea36d06150b.pdf": {
        "title": "Addressing disparities in the health of American Indian and Alaska Native people: the importance of improved public health data.",
        "authors": [
            "U. Bauer",
            "M. Plescia"
        ],
        "published_date": "2014",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/cc1eb116753ba3b14e047b2396ef5ea36d06150b.pdf",
        "venue": "American Journal of Public Health",
        "citationCount": 38,
        "score": 3.4545454545454546,
        "summary": "",
        "keywords": []
    },
    "7dee85dea47eb0521522f4bb14ece1b0da6d8914.pdf": {
        "title": "Evaluation of the clinical characteristics of suspected or confirmed cases of COVID-19 during home care with isolation: A new retrospective analysis based on O2O",
        "authors": [
            "Hui Xu",
            "Sufang Huang",
            "Shangkun Liu",
            "J. Deng",
            "B. Jiao",
            "L. Ai",
            "Yaru Xiao",
            "Li Yan",
            "Shusheng Li"
        ],
        "published_date": "2020",
        "abstract": "Summary Background The recent outbreak of the novel coronavirus in December 2019 (COVID-19) has activated top-level response nationwide. We developed a new treatment model based on the online-to-offline (O2O) model for the home isolated patients, because in the early stages the medical staff were insufficient to cope with so many patients. Methods In this single-centered, retrospective study, we enrolled 48 confirmed/suspected COVID-19 patients who underwent home isolation in Wuhan between January 6 and January 31, 2020. By WeChat and online document editing all patients were treated with medical observation scale. The clinical indications such as Fever, Muscle soreness, Dyspnea and Lack of strength were collected with this system led by medical staff in management, medicine, nursing, rehabilitation and psychology. Findings The mean(SD) age of 48 patients was 39.08(13.88) years, 35(72.9%) were women. Compared with non-hospitalized patients, inpatients were older(\u22658805;70years, 2.4% vs 33.3%, P<0.04). All inpatients had fever, 50% inpatients had coughs and showed infiltration in both lungs at the time of diagnosis. 33.3% inpatients exhibited negative changes in their CT results at initial diagnosis. The body temperature of non-hospitalized patients with mild symptoms returned to normal by day 4-5. While dyspnea peaked on day 6 for non-hospitalized patients with mild symptoms, it persisted in hospitalized patients and exacerbated over time. The lack of strength and muscle soreness were both back to normal by day 4 for non-hospitalized patients. Interpretation Monitoring the trends of symptoms is more important for identifying severe cases. Excessive laboratory data and physical examination are not necessary for the evaluation of patients with mild symptoms. The system we developed is the first to convert the subjective symptoms of patients into objective scores. This type of O2O, subjective-to-objective strategy may be used in regions with similar highly infectious diseases to minimize the possibility of infection among medical staff.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/7dee85dea47eb0521522f4bb14ece1b0da6d8914.pdf",
        "venue": "medRxiv",
        "citationCount": 17,
        "score": 3.4000000000000004,
        "summary": "Here is a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical challenge of managing a large influx of COVID-19 patients during the early stages of the pandemic in Wuhan, where medical resources and staff were severely insufficient, leading to high risks of cross-infection and many patients being isolated at home without adequate medical guidance \\cite{xu2020p6a}.\n    *   This problem was important and challenging due to the rapid spread and high infectivity of COVID-19, necessitating effective patient monitoring and care while simultaneously protecting healthcare workers and conserving scarce hospital capacity \\cite{xu2020p6a}.\n\n*   **Related Work & Positioning**\n    *   At the time of the study, there was a significant gap in the literature regarding the clinical characteristics and outcomes of COVID-19 patients undergoing home isolation \\cite{xu2020p6a}.\n    *   Existing medical observation methods were primarily hospital-centric and lacked a structured approach for remote monitoring of home-quarantined patients, especially one integrated with an Online-to-Offline (O2O) model \\cite{xu2020p6a}. This work positions itself as a novel solution to bridge this gap by providing a practical, remote management system.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is an O2O model-based remote patient management system for home-isolated COVID-19 patients \\cite{xu2020p6a}. This system leverages common digital communication tools like WeChat and online document editing to facilitate continuous and interactive monitoring.\n    *   A key innovation is the development of a \"medical observation scale\" (Quarantine Management Assessment Form) that allows patients to self-report subjective symptoms (e.g., fever, dyspnea, lack of strength) which are then converted into objective, quantifiable scores \\cite{xu2020p6a}. This is highlighted as the \"first to convert the subjective symptoms of patients into objective scores\" in this context \\cite{xu2020p6a}.\n    *   The approach is novel in its integration of a multidisciplinary medical team (management, medicine, nursing, rehabilitation, psychology) providing guidance based on these remotely collected, quantified subjective symptom changes, thereby enabling effective care while minimizing physical contact and resource strain \\cite{xu2020p6a}.\n\n*   **Key Technical Contributions**\n    *   **System Design/Architectural Innovation**: Design and implementation of an O2O-based remote patient management system for infectious diseases, utilizing readily available digital platforms (WeChat, online documents) for continuous, interactive monitoring of home-isolated patients \\cite{xu2020p6a}.\n    *   **Novel Method/Technique**: Creation of a structured, expert-validated \"medical observation scale\" that systematically translates subjective patient-reported symptoms into objective, graded scores. This scale underwent two rounds of expert consultation, demonstrating its scientific soundness and reliability \\cite{xu2020p6a}.\n    *   **Clinical Insights from Technical System**: The system enabled the identification of crucial symptom trend patterns that differentiate mild from severe cases (e.g., persistent dyspnea and lack of strength indicating deterioration), demonstrating the utility of continuous remote monitoring for early identification of critical patients \\cite{xu2020p6a}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: A single-centered, retrospective study was performed on 48 confirmed/suspected COVID-19 patients undergoing home isolation in Wuhan between January 6 and January 31, 2020 \\cite{xu2020p6a}. The developed O2O system and medical observation scale were used to collect daily clinical symptom data.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   The system successfully collected and analyzed symptom data, revealing that hospitalized patients were significantly older (\u226570 years: 2.4% non-hospitalized vs. 33.3% hospitalized, P<0.04) and presented with more severe initial symptoms (all had fever, 50% coughs, 50% bilateral lung infiltration) \\cite{xu2020p6a}.\n        *   Crucially, the system demonstrated its ability to differentiate symptom trends between mild and severe cases:\n            *   Non-hospitalized patients with mild symptoms showed body temperature returning to normal by day 4-5, dyspnea peaking on day 6 then improving, and lack of strength/muscle soreness resolving by day 4 \\cite{xu2020p6a}.\n            *   In contrast, hospitalized patients exhibited sustained elevated body temperature, dyspnea that persisted and exacerbated over time, and lack of strength that never alleviated and continued to worsen \\cite{xu2020p6a}.\n        *   The findings empirically validated that monitoring these symptom trends via the O2O system was more effective for identifying severe cases than relying on initial assessments or extensive laboratory data for mild patients \\cite{xu2020p6a}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The system relies on patient self-reporting, which assumes patient access to and proficiency with smartphones/computers, as well as their ability to accurately interpret and report their subjective symptoms according to the scale \\cite{xu2020p6a}.\n    *   **Study Limitations**: The study was a single-center, retrospective analysis with a relatively small sample size (48 patients), which may limit the generalizability of the findings \\cite{xu2020p6a}. Exclusion criteria (e.g., age <18 or >75, inability to cooperate) further narrow the scope of direct applicability \\cite{xu2020p6a}.\n    *   **Scope of Applicability**: The O2O, subjective-to-objective strategy is primarily applicable in regions with insufficient medical resources and high prevalence of infectious diseases, particularly for managing mild to moderate cases suitable for home isolation \\cite{xu2020p6a}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing an early, practical, and validated O2O model for remote patient monitoring during a pandemic \\cite{xu2020p6a}. It introduces a novel, expert-validated method for quantifying subjective symptoms, enabling continuous and interactive patient management outside traditional hospital settings \\cite{xu2020p6a}. This approach effectively reduces the burden on overwhelmed healthcare systems and minimizes cross-infection risks for both patients and medical staff \\cite{xu2020p6a}.\n    *   The O2O, subjective-to-objective strategy offers a valuable blueprint for future research and implementation in managing widespread infectious diseases, especially in resource-constrained environments \\cite{xu2020p6a}. It underscores the importance of continuous symptom trend monitoring for early detection of patient deterioration, paving the way for integrating more objective home-based measurements and expanding such systems to larger, more diverse populations.",
        "keywords": [
            "O2O model-based remote patient management",
            "COVID-19 home isolation",
            "medical observation scale",
            "subjective symptoms to objective scores",
            "continuous remote monitoring",
            "symptom trend patterns",
            "early identification of critical patients",
            "resource-constrained healthcare",
            "multidisciplinary medical team",
            "empirical validation",
            "reducing healthcare burden",
            "infectious disease management",
            "digital communication tools"
        ],
        "paper_type": "based on the title, abstract, and introduction:\n\n*   **title:** \"evaluation of the clinical characteristics of suspected or confirmed cases of covid-19 during home care with isolation: a new retrospective analysis based on o2o\"\n    *   \"evaluation of the clinical characteristics\" points to studying data.\n    *   \"retrospective analysis\" explicitly states a data-driven approach looking back at past events.\n    *   \"cases\" (plural) implies a dataset.\n*   **introduction:**\n    *   describes the context of the covid-19 outbreak and the problem of overwhelmed medical resources.\n    *   states that a \"new treatment method based on the online-to-offline (o2o) business model\" was developed for patients under quarantine.\n    *   the overall goal, as indicated by the title, is to *evaluate* the clinical characteristics of these patients, which involves collecting and analyzing data.\n\nthis aligns perfectly with the criteria for an **empirical** paper:\n*   abstract mentions: \"study\" (implied by \"evaluation\"), \"data\" (implied by \"clinical characteristics\" and \"cases\"), \"statistical\" (implied by \"analysis\"), \"findings\" (expected from an evaluation).\n*   introduction discusses: \"research questions\" (what are the clinical characteristics?), \"methodology\" (retrospective analysis based on o2o), \"participants\" (suspected or confirmed covid-19 cases).\n\ntherefore, the paper is an **empirical** study."
    },
    "4c400a0ff72487fc8dbc3abc858db14ad67789d4.pdf": {
        "title": "Stress Exposure and Physical, Mental, and Behavioral Health among American Indian Adults with Type 2 Diabetes",
        "authors": [
            "Melissa L. Walls",
            "Kelley J. Sittner",
            "B. Aronson",
            "Angie K. Forsberg",
            "L. Whitbeck",
            "M. al\u2019Absi"
        ],
        "published_date": "2017",
        "abstract": "American Indian (AI) communities experience disproportionate exposure to stressors and health inequities including type 2 diabetes. Yet, we know little about the role of psychosocial stressors for AI diabetes-related health outcomes. We investigated associations between a range of stressors and psychological, behavioral, and physical health for AIs with diabetes. This community-based participatory research with 5 AI tribes includes 192 AI adult type 2 diabetes patients recruited from clinical records at tribal clinics. Data are from computer-assisted interviews and medical charts. We found consistent bivariate relationships between chronic to discrete stressors and mental and behavioral health outcomes; several remained even after accounting for participant age, gender, and income. Fewer stressors were linked to physical health. We also document a dose\u2013response relationship between stress accumulation and worse health. Findings underscore the importance of considering a broad range of stressors for comprehensive assessment of stress burden and diabetes. Policies and practices aimed at reducing stress exposure and promoting tools for stress management may be mechanisms for optimal health for AI diabetes patients.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/4c400a0ff72487fc8dbc3abc858db14ad67789d4.pdf",
        "venue": "International Journal of Environmental Research and Public Health",
        "citationCount": 27,
        "score": 3.375,
        "summary": "American Indian (AI) communities experience disproportionate exposure to stressors and health inequities including type 2 diabetes. Yet, we know little about the role of psychosocial stressors for AI diabetes-related health outcomes. We investigated associations between a range of stressors and psychological, behavioral, and physical health for AIs with diabetes. This community-based participatory research with 5 AI tribes includes 192 AI adult type 2 diabetes patients recruited from clinical records at tribal clinics. Data are from computer-assisted interviews and medical charts. We found consistent bivariate relationships between chronic to discrete stressors and mental and behavioral health outcomes; several remained even after accounting for participant age, gender, and income. Fewer stressors were linked to physical health. We also document a dose\u2013response relationship between stress accumulation and worse health. Findings underscore the importance of considering a broad range of stressors for comprehensive assessment of stress burden and diabetes. Policies and practices aimed at reducing stress exposure and promoting tools for stress management may be mechanisms for optimal health for AI diabetes patients.",
        "keywords": []
    },
    "063e16c467465a5399c5a30f4ca55a337f0dd502.pdf": {
        "title": "Integrating artificial intelligence and natural language processing for computer-assisted reporting and report understanding in nuclear cardiology",
        "authors": [
            "Ernest V. Garcia"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/063e16c467465a5399c5a30f4ca55a337f0dd502.pdf",
        "venue": "Journal of Nuclear Cardiology",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "",
        "keywords": []
    },
    "df81cd4f8e10f95a5c7ff5efea6a0933b66db5b3.pdf": {
        "title": "Conversational Artificial Intelligence for Spinal Pain Questionnaire: Validation and User Satisfaction",
        "authors": [
            "K. Nam",
            "Da Young Kim",
            "D. H. Kim",
            "Jung Hwan Lee",
            "Jae Il Lee",
            "Mi Jeong Kim",
            "Joo Young Park",
            "Jae Hyun Hwang",
            "S. Yun",
            "B. Choi",
            "Min Gyu Kim",
            "I. Han"
        ],
        "published_date": "2022",
        "abstract": "Objective The purpose of our study is to develop a spoken dialogue system (SDS) for pain questionnaire in patients with spinal disease. We evaluate user satisfaction and validated the performance accuracy of the SDS in medical staff and patients. Methods The SDS was developed to investigate pain and related psychological issues in patients with spinal diseases based on the pain questionnaire protocol. We recognized patients\u2019 various answers, summarized important information, and documented them. User satisfaction and performance accuracy were evaluated in 30 potential users of SDS, including doctors, nurses, and patients and statistically analyzed. Results The overall satisfaction score of 30 patients was 5.5 \u00b1 1.4 out of 7 points. Satisfaction scores were 5.3 \u00b1 0.8 for doctors, 6.0 \u00b1 0.6 for nurses, and 5.3 \u00b1 0.5 for patients. In terms of performance accuracy, the number of repetitions of the same question was 13, 16, and 33 (13.5%, 16.8%, and 34.7%) for doctors, nurses, and patients, respectively. The number of errors in the summarized comment by the SDS was 5, 0, and 11 (5.2%, 0.0%, and 11.6 %), respectively. The number of summarization omissions was 7, 5, and 7 (7.3%, 5.3%, and 7.4%), respectively. Conclusion This is the first study in which voice-based conversational artificial intelligence (AI) was developed for a spinal pain questionnaire and validated by medical staff and patients. The conversational AI showed favorable results in terms of user satisfaction and performance accuracy. Conversational AI can be useful for the diagnosis and remote monitoring of various patients as well as for pain questionnaires in the future.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/df81cd4f8e10f95a5c7ff5efea6a0933b66db5b3.pdf",
        "venue": "Neurospine",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Objective The purpose of our study is to develop a spoken dialogue system (SDS) for pain questionnaire in patients with spinal disease. We evaluate user satisfaction and validated the performance accuracy of the SDS in medical staff and patients. Methods The SDS was developed to investigate pain and related psychological issues in patients with spinal diseases based on the pain questionnaire protocol. We recognized patients\u2019 various answers, summarized important information, and documented them. User satisfaction and performance accuracy were evaluated in 30 potential users of SDS, including doctors, nurses, and patients and statistically analyzed. Results The overall satisfaction score of 30 patients was 5.5 \u00b1 1.4 out of 7 points. Satisfaction scores were 5.3 \u00b1 0.8 for doctors, 6.0 \u00b1 0.6 for nurses, and 5.3 \u00b1 0.5 for patients. In terms of performance accuracy, the number of repetitions of the same question was 13, 16, and 33 (13.5%, 16.8%, and 34.7%) for doctors, nurses, and patients, respectively. The number of errors in the summarized comment by the SDS was 5, 0, and 11 (5.2%, 0.0%, and 11.6 %), respectively. The number of summarization omissions was 7, 5, and 7 (7.3%, 5.3%, and 7.4%), respectively. Conclusion This is the first study in which voice-based conversational artificial intelligence (AI) was developed for a spinal pain questionnaire and validated by medical staff and patients. The conversational AI showed favorable results in terms of user satisfaction and performance accuracy. Conversational AI can be useful for the diagnosis and remote monitoring of various patients as well as for pain questionnaires in the future.",
        "keywords": []
    },
    "ba160e3734af4b7aa7dc950192ffd5a50e9168b8.pdf": {
        "title": "Amelioration of diabetic nephropathy in db/db mice treated with tibetan medicine formula Siwei Jianghuang Decoction Powder extract",
        "authors": [
            "Xianrong Lai",
            "D. Tong",
            "Xiaopeng Ai",
            "Jiasi Wu",
            "Yu Luo",
            "F. Zuo",
            "Zhicheng Wei",
            "Yanqiao Li",
            "Wanyi Huang",
            "Wenqian Wang",
            "Qing Jiang",
            "Xianli Meng",
            "Yong Zeng",
            "Ping Wang"
        ],
        "published_date": "2018",
        "abstract": "Siwei Jianghuang Decoction Powder (SWJH) documented originally in the Four Medical Tantras-Blue Glaze exhibited beneficial effects on diabetic nephropathy (DN) via combined synergistically action of multiple formula components including Curcumae longae Rhizoma, Berberidis dictyophyllae Cortex, Phyllanthi Fructus and Tribuli Fructus. This study investigated the effects of SWJH on DN in db/db mice and possible underlying mechanisms. The ten weeks old db/db mice treated with SWJH by intra-gastric administration once a day for 8 weeks. After 8 weeks, body weight, water and food intake of mice were recorded. The level of fasting blood glucose (FBG) was measured. Serum creatinine (Scr), blood urea nitrogen (BUN), urine microalbumin (UMAlb), serum uric acid (UA) and urinary albumin excretion (UAE) were detected. An enzyme-linked immunosorbent assay was performed to test serum vascular endothelial growth factor (VEGF) and transforming growth factor-\u03b21 (TGF-\u03b21). Real-time PCR and Western blot analysis were used to test mRNA and protein expression of hypoxia inducible factor-1\u03b1 (HIF-1\u03b1), VEGF and TGF-\u03b21 in kidney tissue. SWJH treatment significantly reduced the levels of FBG, Scr, BUN, UMAlb, UA and UAE and retarded renal fibrosis. SWJH treatment further significantly reduced serum TGF-\u03b21 level and downregulated the expression of HIF-1\u03b1, VEGF and TGF-\u03b21 at both mRNA and protein levels. Principal component analysis and partial least squares regression and hierarchical cluster analysis demonstrated that SWJH treatment significantly ameliorated renal damage in DN mice. These consequences suggested that SWJH formulations were effective in the treatment of DN through regulating the HIF-1\u03b1, VEGF and TGF-\u03b21 overexpression.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/ba160e3734af4b7aa7dc950192ffd5a50e9168b8.pdf",
        "venue": "Scientific Reports",
        "citationCount": 23,
        "score": 3.2857142857142856,
        "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n*   **Research Problem & Motivation**\n    *   This paper addresses the critical technical problem of developing novel and effective kidney protection interventions for Diabetic Nephropathy (DN) \\cite{lai20188tu}.\n    *   DN is a major complication of diabetes mellitus (DM), leading to end-stage renal disease (ESRD) and kidney failure. The problem is challenging because persistent hyperglycemia alone does not halt DN progression, and multiple factors, including increased vascular permeability and endothelial dysfunction, are involved in its pathogenesis \\cite{lai20188tu}. Existing drug candidates have not fully clarified the progression towards ESRD, highlighting a need for new therapeutic strategies with fewer side effects \\cite{lai20188tu}.\n\n*   **Related Work & Positioning**\n    *   The work positions itself within the context of traditional Chinese medicine (TCM), specifically a Tibetan herbal formula called Siwei Jianghuang Decoction Powder (SWJH), which has empirical use for kidney diseases \\cite{lai20188tu}.\n    *   Previous research indicated that individual components of SWJH (e.g., Curcumae Longae Rhizoma, Berberidis dictyophyllae Cortex) have beneficial effects on DN-related markers and pathways (e.g., reducing TGF-\u03b21, inhibiting HIF-1\u03b1 and VEGF) \\cite{lai20188tu}. Preliminary studies by the authors also showed SWJH's effect on renal injury in STZ-induced diabetic rats \\cite{lai20188tu}.\n    *   Limitations of previous solutions include the unclear progression of DN despite research efforts and the need for multi-target drugs with fewer side effects, which TCM formulas like SWJH, with their synergistic multi-component action, might offer \\cite{lai20188tu}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method involves administering an extract of the SWJH formula (comprising Curcumae Longae Rhizoma, Berberidis dictyophyllae Cortex, Phyllanthi Fructus, and Tribuli Fructus) to db/db mice, a well-established model for type 2 diabetes and DN \\cite{lai20188tu}.\n    *   The innovation lies in systematically investigating the *molecular mechanisms* by which this traditional multi-component formula ameliorates DN, specifically focusing on the modulation of the Hypoxia-Inducible Factor-1\u03b1 (HIF-1\u03b1), Vascular Endothelial Growth Factor (VEGF), and Transforming Growth Factor-\u03b21 (TGF-\u03b21) signaling pathways \\cite{lai20188tu}. The study also employs advanced multivariate statistical analyses (Principal Component Analysis, Partial Least Squares Regression, Hierarchical Cluster Analysis) to interpret the complex biological data \\cite{lai20188tu}.\n\n*   **Key Technical Contributions**\n    *   **Novel Mechanism Elucidation**: The paper provides empirical evidence that SWJH ameliorates DN by significantly downregulating the overexpression of HIF-1\u03b1, VEGF, and TGF-\u03b21 at both mRNA and protein levels in kidney tissue \\cite{lai20188tu}. This identifies specific molecular targets for the traditional formula.\n    *   **Multi-component Validation**: It technically validates the synergistic action of a multi-component traditional herbal formula in a complex disease model, moving beyond single-compound studies \\cite{lai20188tu}.\n    *   **Comprehensive Analytical Approach**: The study integrates biochemical assays, histopathological examination (H&E, PASM, GBM thickness measurement), immunohistochemistry, real-time PCR, and Western blot analysis to provide a robust, multi-level assessment of renal damage and molecular changes \\cite{lai20188tu}.\n    *   **Advanced Data Analysis**: The application of multivariate statistical methods (PCA, PLS, HCA) to biological data from a traditional medicine intervention is a technical contribution, allowing for a more comprehensive understanding of the treatment's overall effect on multiple parameters \\cite{lai20188tu}.\n\n*   **Experimental Validation**\n    *   **Animal Model**: Ten-week-old male db/db mice (type 2 diabetic model) were treated with low, middle, and high doses of SWJH extract for 8 weeks, alongside diabetic control, normal control (db/m), metformin, and berberine groups \\cite{lai20188tu}.\n    *   **Key Performance Metrics**:\n        *   **Physiological/Biochemical**: Body weight, kidney weight, fasting blood glucose (FBG), blood urea nitrogen (BUN), urine albumin excretion (UAE), urine microalbumin (UMAlb), serum creatinine (Scr), serum uric acid (UA), serum TGF-\u03b21, and VEGF levels were measured \\cite{lai20188tu}.\n        *   **Histopathological**: Renal tissue sections were analyzed using H&E and Periodic Acid-Silver Methenamine (PASM) staining to assess renal tubule epithelial cell edema, renal capsule stenosis, epithelial cell shedding, fatty degeneration, and glomerular basement membrane (GBM) thickness \\cite{lai20188tu}.\n        *   **Molecular**: mRNA expression of HIF-1\u03b1, VEGF, and TGF-\u03b21 in kidney tissue was quantified by real-time PCR. Protein expression of these factors was assessed by Western blot and immunohistochemistry \\cite{lai20188tu}.\n    *   **Comparison Results**: SWJH treatment significantly reduced FBG, Scr, BUN, UMAlb, UA, and UAE levels, ameliorated renal histopathological changes (e.g., reduced GBM thickness), and significantly decreased serum TGF-\u03b21. Crucially, SWJH downregulated the mRNA and protein expression of HIF-1\u03b1, VEGF, and TGF-\u03b21 in kidney tissue, demonstrating its molecular impact \\cite{lai20188tu}. Multivariate analyses (PCA, PLS, HCA) further confirmed the significant amelioration of renal damage by SWJH \\cite{lai20188tu}.\n\n*   **Limitations & Scope**\n    *   **Animal Model**: The study was conducted in db/db mice, an animal model of type 2 diabetes. While relevant, findings need to be validated in other models and, ultimately, in human clinical trials \\cite{lai20188tu}.\n    *   **Specific Pathways**: The focus was primarily on the HIF-1\u03b1/VEGF/TGF-\u03b21 pathways. While critical, DN pathogenesis is multi-factorial, and other contributing mechanisms might exist that were not explored \\cite{lai20188tu}.\n    *   **Extract vs. Individual Compounds**: The study used a crude extract of the formula. While beneficial for understanding the traditional medicine, it doesn't fully elucidate the contribution of each individual component or their specific interactions within the complex mixture \\cite{lai20188tu}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing robust, multi-level scientific validation for a traditional Tibetan medicine formula (SWJH) in treating DN \\cite{lai20188tu}.\n    *   It moves beyond empirical observation to identify specific molecular targets (HIF-1\u03b1, VEGF, TGF-\u03b21) and pathways, offering a mechanistic understanding of SWJH's therapeutic effects \\cite{lai20188tu}.\n    *   The comprehensive experimental design, integrating biochemical, histological, and molecular analyses with advanced multivariate statistics, sets a precedent for rigorously evaluating complex natural product interventions \\cite{lai20188tu}.\n    *   The potential impact on future research includes guiding the development of multi-target therapeutic strategies for DN, potentially leading to novel drug candidates derived from natural products, and encouraging further mechanistic studies into traditional medicines using modern scientific tools \\cite{lai20188tu}.",
        "keywords": [
            "Diabetic Nephropathy (DN)",
            "kidney protection interventions",
            "Siwei Jianghuang Decoction Powder (SWJH)",
            "Tibetan herbal formula",
            "HIF-1\u03b1/VEGF/TGF-\u03b21 pathways",
            "molecular mechanisms elucidation",
            "db/db mice model",
            "multivariate statistical analysis",
            "renal histopathological amelioration",
            "glomerular basement membrane thickness",
            "multi-component validation",
            "end-stage renal disease (ESRD)",
            "downregulation of gene/protein expression"
        ],
        "paper_type": "this paper is **empirical**.\n\nhere's why:\n\n*   **abstract:** the title \"amelioration of diabetic nephropathy in db/db mice treated with tibetan medicine formula...\" immediately suggests an experimental study. the abstract mentions \"this study investigated the e...\" (implying investigation/experiment) and discusses the \"beneficial effects\" of a formula, which would be determined through data.\n*   **introduction:** this section explicitly details the methodology and findings:\n    *   \"water and food intake of mice were recorded.\"\n    *   \"the level of fasting blood glucose (fbg) was measured.\"\n    *   \"serum creatinine (scr), blood urea nitrogen (bun), urine microalbumin (umalb), serum uric acid (ua) and urinary albumin excretion (uae) were detected.\"\n    *   \"an enzyme-linked immunosorbent assay was performed...\"\n    *   \"real-time pcr and western blot analysis were used...\"\n    *   \"swjh treatment significantly reduced the levels of fbg, scr, bun, umalb, ua and uae and retarded renal fibrosis.\" (these are clear findings based on data).\n    *   \"principal component analysis and partial least squares regression and hierarchical cluster analysis demonstrated that swjh treatment significantly ameliorated renal damage in dn mice.\" (explicit mention of statistical analysis and findings).\n\nthese elements are all characteristic of an empirical study that collects and analyzes data from experiments to answer a research question."
    },
    "a830cda28038c474c0477e1991eb54d1c9c124d3.pdf": {
        "title": "Software as a Medical Device: Regulating AI in Healthcare via Responsible AI",
        "authors": [
            "M. Ahmad",
            "Steve Overman",
            "Christine Allen",
            "Vikas Kumar",
            "Ankur Teredesai",
            "C. Eckert"
        ],
        "published_date": "2021",
        "abstract": "With the increased adoption of AI in healthcare, there is a growing recognition and demand to regulate AI in healthcare to avoid potential harm and unfair bias against vulnerable populations. Around a hundred governmental bodies and commissions as well as leaders in the tech sector have proposed principles to create responsible AI systems. However, most of these proposals are short on specifics which has led to charges of ethics washing. In this tutorial we offer a guide to help navigate through complex governmental regulations and explain the various constituent practical elements of a responsible AI system in healthcare in the light of proposed regulations. Additionally, we breakdown and emphasize that the recommendations from regulatory bodies like FDA or the EU are necessary but not sufficient elements of creating a responsible AI system. We elucidate how regulations and guidelines often focus on epistemic concerns to the detriment of practical concerns e.g., requirement for fairness without explicating what fairness constitutes for a use case. FDA's Software as a medical device document and EU's GDPR among other AI governance documents talk about the need for implementing sufficiently good machine learning practices. In this tutorial we elucidate what that would mean from a practical perspective for real world use cases in healthcare throughout the machine learning cycle i.e., Data Management, Data Specification, Feature Engineering, Model Evaluation, Model Specification, Model Explainability, Model Fairness, Reproducibility, checks for data leakage and model leakage. We note that conceptualizing responsible AI as a process rather than an end goal accords well with how AI systems are used in practice. We also discuss how a domain centric stakeholder perspective translates into balancing requirements for multiple competing optimization criteria.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/a830cda28038c474c0477e1991eb54d1c9c124d3.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 13,
        "score": 3.25,
        "summary": "With the increased adoption of AI in healthcare, there is a growing recognition and demand to regulate AI in healthcare to avoid potential harm and unfair bias against vulnerable populations. Around a hundred governmental bodies and commissions as well as leaders in the tech sector have proposed principles to create responsible AI systems. However, most of these proposals are short on specifics which has led to charges of ethics washing. In this tutorial we offer a guide to help navigate through complex governmental regulations and explain the various constituent practical elements of a responsible AI system in healthcare in the light of proposed regulations. Additionally, we breakdown and emphasize that the recommendations from regulatory bodies like FDA or the EU are necessary but not sufficient elements of creating a responsible AI system. We elucidate how regulations and guidelines often focus on epistemic concerns to the detriment of practical concerns e.g., requirement for fairness without explicating what fairness constitutes for a use case. FDA's Software as a medical device document and EU's GDPR among other AI governance documents talk about the need for implementing sufficiently good machine learning practices. In this tutorial we elucidate what that would mean from a practical perspective for real world use cases in healthcare throughout the machine learning cycle i.e., Data Management, Data Specification, Feature Engineering, Model Evaluation, Model Specification, Model Explainability, Model Fairness, Reproducibility, checks for data leakage and model leakage. We note that conceptualizing responsible AI as a process rather than an end goal accords well with how AI systems are used in practice. We also discuss how a domain centric stakeholder perspective translates into balancing requirements for multiple competing optimization criteria.",
        "keywords": []
    },
    "d107cdc61d4d8a886da87f3133e793892c0ce31b.pdf": {
        "title": "Reducing racial bias in AI models for clinical use requires a top-down intervention",
        "authors": [
            "Supriya Kapur"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/d107cdc61d4d8a886da87f3133e793892c0ce31b.pdf",
        "venue": "Nature Machine Intelligence",
        "citationCount": 13,
        "score": 3.25,
        "summary": "",
        "keywords": []
    },
    "b86bae2d2a30fbe8a338021c98118967c3719c98.pdf": {
        "title": "Education and Communication in an Interprofessional Antimicrobial Stewardship Program",
        "authors": [
            "P. Foral",
            "Jennifer Anthone",
            "C. Destache",
            "R. Vivekanandan",
            "L. Preheim",
            "G. Gorby",
            "J. Horne",
            "Leo A. Dobronski",
            "J. Syed",
            "C. Mindru",
            "Mir A Ali",
            "Karim F. Ali",
            "Kari A Neemann",
            "M. Bittner"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/b86bae2d2a30fbe8a338021c98118967c3719c98.pdf",
        "venue": "The Journal of the American Osteopathic Association",
        "citationCount": 28,
        "score": 3.1111111111111107,
        "summary": "",
        "keywords": []
    },
    "0c79de253b4f7cac489665d5ecafe70375af57b4.pdf": {
        "title": "Assessing Vitamins, Minerals and Supplements Marketed to Children in Canada",
        "authors": [
            "Charlene Elliott"
        ],
        "published_date": "2019",
        "abstract": "Given the growth of supplements specifically designed for children in Canada, this study examines the nutrient levels of these products, and evaluates them in light of the US Health and Medical Division (HMD)\u2014formerly the Institute of Medicine\u2014and Health Canada\u2019s recommendations. Content analysis was used to document the nutrient levels of child-targeted vitamins, minerals and fish oils/omega-3s (n = 80) in Calgary, Alberta, Canada. Products were assessed according to HMD and Health Canada dosage recommendations for children, and the percentage of Estimate Average Requirements (EAR), Adequate Intakes (AI), and Tolerable Upper Intakes Level (UL) calculated. Median EAR/AI/UL percentages and quartiles were calculated for each nutrient, and estimates for the adequate intake recommendations plotted with box plots. Sixty five percent of the products assessed were multivitamins; the median dose was higher than AI recommendations for vitamins A, B6, B12, and C, as well as thiamin, riboflavin, pantothenic acid, and biotin. Substantial variation in vitamin, mineral, or fish oil dosage was found between similar supplements\u2014with nutrients such as vitamin B12 ranging from 83% to 5557% of AI. Such findings matter because the very existence of these products suggests that children should be taking them, yet more research is needed on their potential (adverse) effects over both the short and long term. The substantial variation in dosages between products also raises questions about the (perhaps unnecessary) fortification of our children, as well as the expectations that parents know\u2014or are even aware of\u2014appropriate nutrient levels for their kids.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/0c79de253b4f7cac489665d5ecafe70375af57b4.pdf",
        "venue": "International Journal of Environmental Research and Public Health",
        "citationCount": 18,
        "score": 3.0,
        "summary": "Given the growth of supplements specifically designed for children in Canada, this study examines the nutrient levels of these products, and evaluates them in light of the US Health and Medical Division (HMD)\u2014formerly the Institute of Medicine\u2014and Health Canada\u2019s recommendations. Content analysis was used to document the nutrient levels of child-targeted vitamins, minerals and fish oils/omega-3s (n = 80) in Calgary, Alberta, Canada. Products were assessed according to HMD and Health Canada dosage recommendations for children, and the percentage of Estimate Average Requirements (EAR), Adequate Intakes (AI), and Tolerable Upper Intakes Level (UL) calculated. Median EAR/AI/UL percentages and quartiles were calculated for each nutrient, and estimates for the adequate intake recommendations plotted with box plots. Sixty five percent of the products assessed were multivitamins; the median dose was higher than AI recommendations for vitamins A, B6, B12, and C, as well as thiamin, riboflavin, pantothenic acid, and biotin. Substantial variation in vitamin, mineral, or fish oil dosage was found between similar supplements\u2014with nutrients such as vitamin B12 ranging from 83% to 5557% of AI. Such findings matter because the very existence of these products suggests that children should be taking them, yet more research is needed on their potential (adverse) effects over both the short and long term. The substantial variation in dosages between products also raises questions about the (perhaps unnecessary) fortification of our children, as well as the expectations that parents know\u2014or are even aware of\u2014appropriate nutrient levels for their kids.",
        "keywords": []
    },
    "c5938c5c82ffd15a58bc363335464e31af045d48.pdf": {
        "title": "An appendix to the 2012 IOF\u2013ECTS guidelines for the management of glucocorticoid-induced osteoporosis",
        "authors": [
            "S. Lekamwasam",
            "J. Adachi",
            "D. Agnusdei",
            "J. Bilezikian",
            "S. Boonen",
            "F. Borgstr\u00f6m",
            "C. Cooper",
            "A. D. P\u00e9rez",
            "R. Eastell",
            "L. Hofbauer",
            "J. Kanis",
            "B. Langdahl",
            "O. Lesnyak",
            "R. Lorenc",
            "E. McCloskey",
            "O. Messina",
            "N. Napoli",
            "B. Obermayer-Pietsch",
            "S. Ralston",
            "P. Sambrook",
            "S. Silverman",
            "M. Sosa",
            "J. Stepan",
            "G. Suppan",
            "D. A. Wahl",
            "J. Compston",
            "for the Epidos group"
        ],
        "published_date": "2012",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c5938c5c82ffd15a58bc363335464e31af045d48.pdf",
        "venue": "Archives of Osteoporosis",
        "citationCount": 38,
        "score": 2.9230769230769234,
        "summary": "",
        "keywords": []
    },
    "9f1804fb51570686108d2d94badb3baf84f8052d.pdf": {
        "title": "Promise and Perils of Big Data and Artificial Intelligence in Clinical Medicine and Biomedical Research.",
        "authors": [
            "F. Rodriguez",
            "David Scheinker",
            "R. Harrington"
        ],
        "published_date": "2018",
        "abstract": "The use of big data and AI to help guide clinical decisions is a central aspect of precision medicine initiatives. Yet, buzz words like big data and AI mystify many clinicians and biomedical researchers. Their widespread use in other industries and initial clinical applications can serve as a guide to a clearer understanding of what they are and are not, their promise, and their potential peril. Many common terms mean different things in different contexts. AI typically refers to a machine with human capabilities; machine learning (ML) may refer either to a set of computational and statistical tools for identifying relationships in data or to the use of such tools to make predictions based on data; deep neural networks are a particular type of ML whose success at tasks, such as image recognition, has led to them being referred to as AI or deep learning. Developing most AI, ML, and deep neural network tools requires access to big data\u2014another concept with multiple meanings. For data scientists, it implies using more data than one computer can handle with significant attendant analytical and computational challenges and opportunities; for clinicians and biomedical researchers, it refers to complex datasets with numerous structured and unstructured data fields, such as those typically found in electronic health records. Figure 1 illustrates the relationship between AI, ML, deep neural networks, and big data. Reinforcement learning is a notable exception to the use of big data to train AI. It is an approach to building AI tools based only on feedback. For example, DeepMind program AlphaGo Zero became the most powerful Go program in the world solely by playing against itself. Thus far, reinforcement learning in health care has been developed using historical data representing decisions and feedback. If (when) AI starts to make and test clinical decisions, algorithms will have the capacity to learn on their own. The widespread use of AI by companies such as Amazon and Google offers lessons for health care. Such industries have applied AI to enable smart electronic assistants, facial recognition software, and self-driving cars; the use and misuse of such applications have raised concerns about safety, fairness, and privacy. For example, data are increasingly being used to predict consumer behavior and preferences. Do I want the computers at a department store to know that I am pregnant before my family does? Will I be charged more for plane tickets if I buy them using an iPhone? There are numerous proof-of-concept examples of ML in health care. It has been applied to clinical risk prediction and to learning from the large volume of data generated by electronic health records and other large datasets. Notable recent examples include the classification of a picture of a nevus as malignant or benign, of a retinal fundus image to predict the risk of cardiovascular disease, and of using histopathology specimens to predict prognosis in lung cancer. In everyday clinical cardiology, ML has been used for interpreting automated ECG, determine left ventricular ejection fractions from echocardiography, and scoring coronary artery calcium scans. A form of AI known as computer vision is being developed to prevent clinical errors and improve patient safety. Although relatively few of these methods have been implemented in routine clinical practice, and none on a large scale, they demonstrate the promise of AI in clinical medicine and biomedical research. AI can be used to automate and simplify tasks too onerous or time-consuming for a single person or team to perform. What if we could use voice recognition software as a medical scribe to allow more doctoring and less documenting in patient exam rooms? What if we could aggregate a personalized cohort to ask simple, clinically relevant questions with a few clicks of our mouse? Research teams at Stanford University have developed a variety of approaches to leveraging data found in the electronic health record to help clinicians make decisions based on a patient\u2019s unique characteristics. One such approach allows a physician considering a decision to call for an informatics consult. This triggers an ML algorithm that identifies patients similar to the one being considered and presents their treatments and outcomes. Similar tools could be used to quickly identify patients who meet criteria for entry into randomized clinical trials, dramatically cutting enrollment and recruitment costs. In translational research, ML can efficiently identify candidate molecules for drug development. ML can better risk stratify populations underrepresented in our available evidence base and identify patterns and relationships not captured by traditional statistical methods. The opinions expressed in this article are not necessarily those of the editors or of the American Heart Association. From the Division of Cardiovascular Medicine, Cardiovascular Institute (F.R., R.A.H.), Department of Medicine (F.R., R.A.H.), and Department of Management Science and Engineering (D.S.), Stanford University, CA. Correspondence to Fatima Rodriguez, MD, MPH, Division of Cardiovascular Medicine, Stanford University School of Medicine, 870 Quarry Rd, Falk CVRC, Stanford, CA 94305\u20135406. Email frodrigu@ stanford.edu Viewpoints",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/9f1804fb51570686108d2d94badb3baf84f8052d.pdf",
        "venue": "Circulation Research",
        "citationCount": 20,
        "score": 2.8571428571428568,
        "summary": "The use of big data and AI to help guide clinical decisions is a central aspect of precision medicine initiatives. Yet, buzz words like big data and AI mystify many clinicians and biomedical researchers. Their widespread use in other industries and initial clinical applications can serve as a guide to a clearer understanding of what they are and are not, their promise, and their potential peril. Many common terms mean different things in different contexts. AI typically refers to a machine with human capabilities; machine learning (ML) may refer either to a set of computational and statistical tools for identifying relationships in data or to the use of such tools to make predictions based on data; deep neural networks are a particular type of ML whose success at tasks, such as image recognition, has led to them being referred to as AI or deep learning. Developing most AI, ML, and deep neural network tools requires access to big data\u2014another concept with multiple meanings. For data scientists, it implies using more data than one computer can handle with significant attendant analytical and computational challenges and opportunities; for clinicians and biomedical researchers, it refers to complex datasets with numerous structured and unstructured data fields, such as those typically found in electronic health records. Figure 1 illustrates the relationship between AI, ML, deep neural networks, and big data. Reinforcement learning is a notable exception to the use of big data to train AI. It is an approach to building AI tools based only on feedback. For example, DeepMind program AlphaGo Zero became the most powerful Go program in the world solely by playing against itself. Thus far, reinforcement learning in health care has been developed using historical data representing decisions and feedback. If (when) AI starts to make and test clinical decisions, algorithms will have the capacity to learn on their own. The widespread use of AI by companies such as Amazon and Google offers lessons for health care. Such industries have applied AI to enable smart electronic assistants, facial recognition software, and self-driving cars; the use and misuse of such applications have raised concerns about safety, fairness, and privacy. For example, data are increasingly being used to predict consumer behavior and preferences. Do I want the computers at a department store to know that I am pregnant before my family does? Will I be charged more for plane tickets if I buy them using an iPhone? There are numerous proof-of-concept examples of ML in health care. It has been applied to clinical risk prediction and to learning from the large volume of data generated by electronic health records and other large datasets. Notable recent examples include the classification of a picture of a nevus as malignant or benign, of a retinal fundus image to predict the risk of cardiovascular disease, and of using histopathology specimens to predict prognosis in lung cancer. In everyday clinical cardiology, ML has been used for interpreting automated ECG, determine left ventricular ejection fractions from echocardiography, and scoring coronary artery calcium scans. A form of AI known as computer vision is being developed to prevent clinical errors and improve patient safety. Although relatively few of these methods have been implemented in routine clinical practice, and none on a large scale, they demonstrate the promise of AI in clinical medicine and biomedical research. AI can be used to automate and simplify tasks too onerous or time-consuming for a single person or team to perform. What if we could use voice recognition software as a medical scribe to allow more doctoring and less documenting in patient exam rooms? What if we could aggregate a personalized cohort to ask simple, clinically relevant questions with a few clicks of our mouse? Research teams at Stanford University have developed a variety of approaches to leveraging data found in the electronic health record to help clinicians make decisions based on a patient\u2019s unique characteristics. One such approach allows a physician considering a decision to call for an informatics consult. This triggers an ML algorithm that identifies patients similar to the one being considered and presents their treatments and outcomes. Similar tools could be used to quickly identify patients who meet criteria for entry into randomized clinical trials, dramatically cutting enrollment and recruitment costs. In translational research, ML can efficiently identify candidate molecules for drug development. ML can better risk stratify populations underrepresented in our available evidence base and identify patterns and relationships not captured by traditional statistical methods. The opinions expressed in this article are not necessarily those of the editors or of the American Heart Association. From the Division of Cardiovascular Medicine, Cardiovascular Institute (F.R., R.A.H.), Department of Medicine (F.R., R.A.H.), and Department of Management Science and Engineering (D.S.), Stanford University, CA. Correspondence to Fatima Rodriguez, MD, MPH, Division of Cardiovascular Medicine, Stanford University School of Medicine, 870 Quarry Rd, Falk CVRC, Stanford, CA 94305\u20135406. Email frodrigu@ stanford.edu Viewpoints",
        "keywords": []
    },
    "c29584c03839d1d8f15a8d97a4f5c4a6be211a58.pdf": {
        "title": "FDA Regulation of Predictive Clinical Decision-Support Tools: What Does It Mean for Hospitals?",
        "authors": [
            "G. Weissman"
        ],
        "published_date": "2020",
        "abstract": "Recent experiences in the transportation industry highlight the importance of getting right the regulation of decision-support systems in high-stakes environments. Two tragic plane crashes resulted in 346 deaths and were deemed, in part, to be related to a cockpit alert system that overwhelmed pilots with multiple notifications.1 Similarly, a driverless car struck and killed a pedestrian in the street, in part because the car was not programmed to look for humans outside of a crosswalk.2 These two bellwether events offer poignant lessons for the healthcare industry in which human lives also depend on decision-support systems. Clinical decision-support (CDS) systems are computerized applications, often embedded in an electronic health record (EHR), that provide information to clinicians to inform care. Although CDS systems have been used for many years,3 they have never been subjected to any enforcement of formal testing requirements. However, a draft guidance document released in 2019 from the Food and Drug Administration (FDA) outlined new directions for the regulation of CDS systems.4 Although the FDA has thus far focused regulatory efforts on predictive systems developed by private manufacturers,5,6 this new document provides examples of software that would require regulation for CDS systems that hospitals are already using. Thus, this new guidance raises critical questions\u2014will hospitals themselves be evaluated like private manufacturers, be exempted from federal regulation, or require their own specialized regulation? The FDA has not yet clarified its approach to hospitals or hospital-developed CDS systems, which leaves open numerous possibilities in a rapidly evolving regulatory environment. Although the FDA has officially regulated CDS systems under section 201(h) of the Federal Food, Drug, and Cosmetic Act (1938), only recently has the FDA begun to sketch the shape of its regulatory efforts. This trend to actually regulate CDS systems began with the 21st Century Cures Act (2016) that amended the definition of software systems that qualify as medical devices and outlined criteria under which a system may be exempt from FDA oversight. For example, regulation would not apply to systems that support \u201cpopulation health\u201d or a \u201chealthy lifestyle\u201d or to ones that qualify as \u201celectronic patient records\u201d as long as they do not \u201cinterpret or analyze\u201d data within them.7 Following the rapid proliferation of many machine learning and other predictive technologies with medical applications, the FDA began the voluntary Digital Health Software Precertification (Pre-Cert) Program in 2017. Through this program, the FDA selected nine companies from more than 100 applicants and certified them across five domains of excellence. Notably, the Pre-Cert Program currently allows for certification of software manufacturers themselves and does not approve or test actual software devices directly. This regulatory pathway will eventually allow manufacturers to apply under a modified premarket review process for individual software as a medical device (SaMD) that use artificial intelligence (AI) and machine learning. In the meantime, however, many hospitals have developed and deployed their own predictive CDS systems that cross the boundaries into the FDA\u2019s purview and, indeed, do \u201cinterpret or analyze\u201d data for real-time EHR alerts, population health management, and other applications. Regulatory oversight for hospitals could provide quality or safety standards where currently there are none. However, such regulations could also interfere with existing local care practices, hinder rapid development of new CDS systems, and may be perceived as interfering in hospital operations. With the current enthusiasm for AI-based technologies and the concurrent lack of evidence to suggest their effectiveness in practice, regulation could also prompt necessary scrutiny of potential harms of CDS systems, an area with even less evidence. At the same time, CDS developers\u2014private or hospital based\u2014may be able to avoid regulation for some devices with well-placed disclaimers about the intended use of the CDS, one of the FDA criteria for determining the degree of oversight. If the FDA were to regulate hospitals or hospital-developed CDS systems, there are several unanswered questions to consider so that such regulations have their intended impact. First, does the FDA intend to regulate hospitals and hospital-developed software at all? The framework for determining whether a CDS system will be regulated depends on the severity of the clinical scenario, the ability to independently evaluate the model output, and the intended user (Table). Notably, many types of CDS systems that would require regulation under this framework are already commonplace. For example, the FDA intends to regulate software that \u201cidentifies patients who may exhibit signs of opioid addiction,\u201d a scenario similar to prediction models already developed at academic hospitals.8 The FDA also plans to regulate a software device Corresponding Author: Gary E Weissman, MD, MSHP; Email: gary.weissman@ pennmedicine.upenn.edu; Telephone: 215-746-2887; Twitter: @garyweissman.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c29584c03839d1d8f15a8d97a4f5c4a6be211a58.pdf",
        "venue": "Journal of Hospital Medicine",
        "citationCount": 14,
        "score": 2.8000000000000003,
        "summary": "Recent experiences in the transportation industry highlight the importance of getting right the regulation of decision-support systems in high-stakes environments. Two tragic plane crashes resulted in 346 deaths and were deemed, in part, to be related to a cockpit alert system that overwhelmed pilots with multiple notifications.1 Similarly, a driverless car struck and killed a pedestrian in the street, in part because the car was not programmed to look for humans outside of a crosswalk.2 These two bellwether events offer poignant lessons for the healthcare industry in which human lives also depend on decision-support systems. Clinical decision-support (CDS) systems are computerized applications, often embedded in an electronic health record (EHR), that provide information to clinicians to inform care. Although CDS systems have been used for many years,3 they have never been subjected to any enforcement of formal testing requirements. However, a draft guidance document released in 2019 from the Food and Drug Administration (FDA) outlined new directions for the regulation of CDS systems.4 Although the FDA has thus far focused regulatory efforts on predictive systems developed by private manufacturers,5,6 this new document provides examples of software that would require regulation for CDS systems that hospitals are already using. Thus, this new guidance raises critical questions\u2014will hospitals themselves be evaluated like private manufacturers, be exempted from federal regulation, or require their own specialized regulation? The FDA has not yet clarified its approach to hospitals or hospital-developed CDS systems, which leaves open numerous possibilities in a rapidly evolving regulatory environment. Although the FDA has officially regulated CDS systems under section 201(h) of the Federal Food, Drug, and Cosmetic Act (1938), only recently has the FDA begun to sketch the shape of its regulatory efforts. This trend to actually regulate CDS systems began with the 21st Century Cures Act (2016) that amended the definition of software systems that qualify as medical devices and outlined criteria under which a system may be exempt from FDA oversight. For example, regulation would not apply to systems that support \u201cpopulation health\u201d or a \u201chealthy lifestyle\u201d or to ones that qualify as \u201celectronic patient records\u201d as long as they do not \u201cinterpret or analyze\u201d data within them.7 Following the rapid proliferation of many machine learning and other predictive technologies with medical applications, the FDA began the voluntary Digital Health Software Precertification (Pre-Cert) Program in 2017. Through this program, the FDA selected nine companies from more than 100 applicants and certified them across five domains of excellence. Notably, the Pre-Cert Program currently allows for certification of software manufacturers themselves and does not approve or test actual software devices directly. This regulatory pathway will eventually allow manufacturers to apply under a modified premarket review process for individual software as a medical device (SaMD) that use artificial intelligence (AI) and machine learning. In the meantime, however, many hospitals have developed and deployed their own predictive CDS systems that cross the boundaries into the FDA\u2019s purview and, indeed, do \u201cinterpret or analyze\u201d data for real-time EHR alerts, population health management, and other applications. Regulatory oversight for hospitals could provide quality or safety standards where currently there are none. However, such regulations could also interfere with existing local care practices, hinder rapid development of new CDS systems, and may be perceived as interfering in hospital operations. With the current enthusiasm for AI-based technologies and the concurrent lack of evidence to suggest their effectiveness in practice, regulation could also prompt necessary scrutiny of potential harms of CDS systems, an area with even less evidence. At the same time, CDS developers\u2014private or hospital based\u2014may be able to avoid regulation for some devices with well-placed disclaimers about the intended use of the CDS, one of the FDA criteria for determining the degree of oversight. If the FDA were to regulate hospitals or hospital-developed CDS systems, there are several unanswered questions to consider so that such regulations have their intended impact. First, does the FDA intend to regulate hospitals and hospital-developed software at all? The framework for determining whether a CDS system will be regulated depends on the severity of the clinical scenario, the ability to independently evaluate the model output, and the intended user (Table). Notably, many types of CDS systems that would require regulation under this framework are already commonplace. For example, the FDA intends to regulate software that \u201cidentifies patients who may exhibit signs of opioid addiction,\u201d a scenario similar to prediction models already developed at academic hospitals.8 The FDA also plans to regulate a software device Corresponding Author: Gary E Weissman, MD, MSHP; Email: gary.weissman@ pennmedicine.upenn.edu; Telephone: 215-746-2887; Twitter: @garyweissman.",
        "keywords": []
    },
    "f4e5526632a3934702e352d1b357deb86a62d347.pdf": {
        "title": "What Have Hospital Social Workers Been Prepared for COVID-19 from SARS, MERS, and H1N1?",
        "authors": [
            "J. Cheung"
        ],
        "published_date": "2020",
        "abstract": "s, only 14 articles (see Table 1) were deemed relevant. Half of these were conceptual articles, and the other half were research articles. Six were qualitative studies; only one quantitative research article on this topic was identified, which was a survey of 55 nongovernmental organizations in Hong Kong that initiated actions and responses to the SARS crisis (Wong & Leung, 2008). Their study reflected that the loss of confidence in the ruling elite and a substantially low level of trust toward the Hong Kong government hindered the efficiency of relief work. Nonetheless, maintaining such a dialectic relationship between civil society and the state might at the same time pose an expectation for enhancement of the government\u2019s function through adversarial endeavors. Academics in social work are unable to provide an immediahte and substantial response to capture lessons learned from frontline practitioners\u2019 involvement. Although five articles were published two years after the outbreak of SARS, they were all conceptual articles published in the same journal and included in the same special issue. The first study on social work intervention in response to SARS was not published until 2007, that is, four years after the initial outbreak. Another four research articles on SARS were published between 2007 and 2010. After that, one article on H1N1 was published in 2012 and one on MERS in 2016. Only a very small number of studies were conducted years after the major outbreaks. Evidence-informed practice standards could not be established in a timely manner; moreover, frontline practitioners were unable to locate important and useful resources during their ongoing fight against disease. Unlike studies in medical disciplines, which are published rapidly during the outbreak period on a daily basis, the first study of the medical social work practice of combating SARS in Singapore was published in the aftermath of the outbreak. Other studies were also mainly conducted retrospectively. doi: 10.1093/hsw/hlaa017 VC 2020 National Association of Social Workers 1 Ta b le 1 : A rt ic le s o n S o ci a lW o rk R e sp o n se to R e sp ir a to ry D is e a se O u tb re a k s (2 0 0 0 \u20132 0 2 0 ) A u th o rs Y e a r D is e a se C o u n tr y /R e g io n A rt ic le Ty p e N Jo u rn a l 1 . R ez ae i, F ., M ar ac y , M . R ., Y ar m o h am m ad ia n , M . H ., & K ey v an ar a, M . 2 0 2 0 N S Ir an R es ea rc h 1 7 A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 2 . P ar k , H . J. , & L ee , B . J. 2 0 1 6 M E R S K o re a R es ea rc h 2 2 S oc ia l W or k in P u bl ic H ea lt h (Q 4 ) 3 . S iu , J. Y .M . 2 0 1 2 H 1 N 1 H o n g K o n g R es ea rc h 4 0 H ea lt h an d S oc ia l C ar e in th e C om m u n it y (Q 1 ) 4 . K o ll er , D ., N ic h o la s, D ., G ea ri n g , R ., & K al fa , O . 2 0 1 0 S A R S C an ad a R es ea rc h 2 1 H ea lt h an d S oc ia l C ar e in th e C om m u n it y (Q 1 ) 5 . W o n g , H ., & L eu n g , T .T .F . 2 0 0 8 S A R S H o n g K o n g R es ea rc h 5 5 A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 6 . R o so ff , P . M . 2 0 0 8 In fl u en za N S C o n ce p tu al N A S oc ia l W or k in H ea lt h C ar e (Q 2 ) 7 . G ea ri n g , R . E ., S ai n i, M ., & M cN ei ll , T . 2 0 0 7 S A R S C an ad a R es ea rc h 1 9 H ea lt h & S oc ia l W or k (Q 2 ) 8 . R o w la n d s, A . 2 0 0 7 S A R S S in g ap o re R es ea rc h 2 8 S oc ia l W or k in H ea lt h C ar e (Q 2 ) 9 . S ei p el , M . 2 0 0 5 N S N S C o n ce p tu al N A In te rn at io n al S oc ia l W or k (Q 4 ) 1 0 . T io n g , T . N . 2 0 0 4 S A R S S in g ap o re C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 1 1 . Y u en -T sa n g , A .W .K ., & T si en -W o n g , T .B .K . 2 0 0 4 S A R S H o n g K o n g C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 1 2 . S ze , Y . H ., & T in g , W . F . 2 0 0 4 S A R S H o n g K o n g C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 1 3 . H u i, J. M .C ., & T su i, M . S . 2 0 0 4 S A R S H o n g K o n g C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 1 4 . C h an , C . C ., C h an , K .H .W ., & C h o w , C . B . 2 0 0 4 S A R S H o n g K o n g C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) N o te s: N S 1\u20444 n o t sp ec ifi ed ;N A 1\u20444 n o t ap p lic ab le ;M ER S 1\u20444 M id d le Ea st re sp ir at o ry sy n d ro m e; SA R S 1\u20444 se ve re ac u te re sp ir at o ry sy n d ro m e. Jo u rn al im p ac t fa ct o r q u ar ti le re fe rs to th e q u o ti en t o f a jo u rn al \u2019s ra n k in ca te g o ry .Q 1 d en o te s th e to p 25 p er ce n t o f th e jo u rn al im p ac t fa ct o r d is tr ib u ti o n ,Q 2 is b et w ee n to p 25 p er ce n t an d to p 50 p er ce n t, Q 3 is b et w ee n to p 50 p er ce n t an d to p 75 p er ce n t, an d Q 4 d en o te s th e b o tt o m 25 p er ce n t. SARS, MERS, and H1N1 have spread widely in developing countries. Nonetheless, the majority of articles (11 out of 14, 78.6 percent) were about the context of developed regions, including Canada, Hong Kong, Korea, and Singapore. Developing and underdeveloped regions were underrepresented in the studies, with only one article (7.1 percent) in Iran. No study was found related to highly and densely populated countries such as China or India, where respiratory diseases were most likely to be transmitted through human-to-human interaction. It remains questionable whether important lessons learned from developed places could be applied to disadvantaged zones. Scholars examined experiences of social workers practicing in a hospital environment during SARS and aimed to formulate essential themes and structures of social work practices for better preparedness to meet similar crises in the future (Gearing, Saini, & McNeill, 2007; Rowlands, 2007). Perspectives and recommendations of young participants who were hospitalized during SARS in a large pediatric hospital had also been covered (Koller, Nicholas, Gearing, & Kalfa, 2010). Social work scholars advocated the inclusion of children\u2019s omitted voices in health care decision making, policy planning, and the development of guidelines for future pandemics. More recent studies explored the social and psychoemotional difficulties of foreign residents in South Korea during the MERS outbreak in the community from a social work perspective (Park & Lee, 2016) as well as the functional role of community-based health organizations as gatekeepers of communities in Iran in times of biohazards (Rezaei, Maracy, Yarmohammadian, & Keyvanara, 2020). The role of social work in pandemics has long been significant throughout the years but has not been well documented in the literature. It can be traced back to as early as 1918 with the pivotal and central role hospital social workers played during the great worldwide influenza pandemic. But only sketchy and cryptic reference was provided to social workers during a pandemic by authoritative public health institutes or organizations around the globe (Rosoff, 2008). Social workers can perform an essential role in crisis management while keeping in mind that the basic human rights of every person must be safeguarded. The World Health Organization published guidelines titled Infection Prevention and Control of Epidemicand Pandemic-Prone Acute Respiratory Infections in Health Care in 2014 but placed social workers under the umbrella term \u2018\u2018healthcare worker\u201d with many other professionals. Professional bodies such as the National Association of Social Workers could and should take the lead in directing social workers to participate in prevention and control at the time of severe outbreaks. In our current battle against COVID-19, the Chinese Association of Social Workers, which helps coordinate practitioners in the forefront of the fighting, published The Social Worker Support Manual on the Prevention and Control of Pneumonia Caused by the Coronavirus Infection and proposed basic working principles and methods to social workers. Although the original version is in Chinese, it serves as an excellent foundation for further development of a more established international protocol. Sudden onset of novel respiratory diseases in the community created an immediate necessity for conducting studies in adapting to the new epidemic environment. Only traditional crisis management approaches and theories have now been applied to medical social work, which help little when facing emerging risks in the course of combating novel viruses and challenges. Theoretical frameworks of social work intervention in relation to these unprecedented types of emergency, such as SARS, have not been consciously developed (Rowlands, 2007). Social work departments at institutions should not only play their role in the ivory tower with laboratory studies. Practitioners and academics need to work in synchrony with each other at the time of infectious disease control. CONCLUSION In the last 20 years, high-impact social work journals remain quite detached from the outbreaks. Only two articles were accepted in Q1 journals (that is, the top 25 percent of the journal impact factor distribution). There is seemingly a lack of interest among academics and editors of the top journals on these issues. Only one journal had developed a special issue; however, it mainly included conceptual articles. Ample research has helped inform medical and nursing practice in dealing with tiny but deadly viruses and has been published and well cited in some of the most prestige academic journals in the world. High-impact social work journals should also take a much more proactive stance to map out a new research agenda in a timely manner. Only then can state-of-the-art and CHEUNG / What Have Hospital Social Workers Been Prep",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/f4e5526632a3934702e352d1b357deb86a62d347.pdf",
        "venue": "Health & Social Work",
        "citationCount": 14,
        "score": 2.8000000000000003,
        "summary": "s, only 14 articles (see Table 1) were deemed relevant. Half of these were conceptual articles, and the other half were research articles. Six were qualitative studies; only one quantitative research article on this topic was identified, which was a survey of 55 nongovernmental organizations in Hong Kong that initiated actions and responses to the SARS crisis (Wong & Leung, 2008). Their study reflected that the loss of confidence in the ruling elite and a substantially low level of trust toward the Hong Kong government hindered the efficiency of relief work. Nonetheless, maintaining such a dialectic relationship between civil society and the state might at the same time pose an expectation for enhancement of the government\u2019s function through adversarial endeavors. Academics in social work are unable to provide an immediahte and substantial response to capture lessons learned from frontline practitioners\u2019 involvement. Although five articles were published two years after the outbreak of SARS, they were all conceptual articles published in the same journal and included in the same special issue. The first study on social work intervention in response to SARS was not published until 2007, that is, four years after the initial outbreak. Another four research articles on SARS were published between 2007 and 2010. After that, one article on H1N1 was published in 2012 and one on MERS in 2016. Only a very small number of studies were conducted years after the major outbreaks. Evidence-informed practice standards could not be established in a timely manner; moreover, frontline practitioners were unable to locate important and useful resources during their ongoing fight against disease. Unlike studies in medical disciplines, which are published rapidly during the outbreak period on a daily basis, the first study of the medical social work practice of combating SARS in Singapore was published in the aftermath of the outbreak. Other studies were also mainly conducted retrospectively. doi: 10.1093/hsw/hlaa017 VC 2020 National Association of Social Workers 1 Ta b le 1 : A rt ic le s o n S o ci a lW o rk R e sp o n se to R e sp ir a to ry D is e a se O u tb re a k s (2 0 0 0 \u20132 0 2 0 ) A u th o rs Y e a r D is e a se C o u n tr y /R e g io n A rt ic le Ty p e N Jo u rn a l 1 . R ez ae i, F ., M ar ac y , M . R ., Y ar m o h am m ad ia n , M . H ., & K ey v an ar a, M . 2 0 2 0 N S Ir an R es ea rc h 1 7 A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 2 . P ar k , H . J. , & L ee , B . J. 2 0 1 6 M E R S K o re a R es ea rc h 2 2 S oc ia l W or k in P u bl ic H ea lt h (Q 4 ) 3 . S iu , J. Y .M . 2 0 1 2 H 1 N 1 H o n g K o n g R es ea rc h 4 0 H ea lt h an d S oc ia l C ar e in th e C om m u n it y (Q 1 ) 4 . K o ll er , D ., N ic h o la s, D ., G ea ri n g , R ., & K al fa , O . 2 0 1 0 S A R S C an ad a R es ea rc h 2 1 H ea lt h an d S oc ia l C ar e in th e C om m u n it y (Q 1 ) 5 . W o n g , H ., & L eu n g , T .T .F . 2 0 0 8 S A R S H o n g K o n g R es ea rc h 5 5 A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 6 . R o so ff , P . M . 2 0 0 8 In fl u en za N S C o n ce p tu al N A S oc ia l W or k in H ea lt h C ar e (Q 2 ) 7 . G ea ri n g , R . E ., S ai n i, M ., & M cN ei ll , T . 2 0 0 7 S A R S C an ad a R es ea rc h 1 9 H ea lt h & S oc ia l W or k (Q 2 ) 8 . R o w la n d s, A . 2 0 0 7 S A R S S in g ap o re R es ea rc h 2 8 S oc ia l W or k in H ea lt h C ar e (Q 2 ) 9 . S ei p el , M . 2 0 0 5 N S N S C o n ce p tu al N A In te rn at io n al S oc ia l W or k (Q 4 ) 1 0 . T io n g , T . N . 2 0 0 4 S A R S S in g ap o re C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 1 1 . Y u en -T sa n g , A .W .K ., & T si en -W o n g , T .B .K . 2 0 0 4 S A R S H o n g K o n g C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 1 2 . S ze , Y . H ., & T in g , W . F . 2 0 0 4 S A R S H o n g K o n g C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 1 3 . H u i, J. M .C ., & T su i, M . S . 2 0 0 4 S A R S H o n g K o n g C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) 1 4 . C h an , C . C ., C h an , K .H .W ., & C h o w , C . B . 2 0 0 4 S A R S H o n g K o n g C o n ce p tu al N A A si a P ac ifi c Jo u rn al of S oc ia l W or k an d D ev el op m en t (Q 4 ) N o te s: N S 1\u20444 n o t sp ec ifi ed ;N A 1\u20444 n o t ap p lic ab le ;M ER S 1\u20444 M id d le Ea st re sp ir at o ry sy n d ro m e; SA R S 1\u20444 se ve re ac u te re sp ir at o ry sy n d ro m e. Jo u rn al im p ac t fa ct o r q u ar ti le re fe rs to th e q u o ti en t o f a jo u rn al \u2019s ra n k in ca te g o ry .Q 1 d en o te s th e to p 25 p er ce n t o f th e jo u rn al im p ac t fa ct o r d is tr ib u ti o n ,Q 2 is b et w ee n to p 25 p er ce n t an d to p 50 p er ce n t, Q 3 is b et w ee n to p 50 p er ce n t an d to p 75 p er ce n t, an d Q 4 d en o te s th e b o tt o m 25 p er ce n t. SARS, MERS, and H1N1 have spread widely in developing countries. Nonetheless, the majority of articles (11 out of 14, 78.6 percent) were about the context of developed regions, including Canada, Hong Kong, Korea, and Singapore. Developing and underdeveloped regions were underrepresented in the studies, with only one article (7.1 percent) in Iran. No study was found related to highly and densely populated countries such as China or India, where respiratory diseases were most likely to be transmitted through human-to-human interaction. It remains questionable whether important lessons learned from developed places could be applied to disadvantaged zones. Scholars examined experiences of social workers practicing in a hospital environment during SARS and aimed to formulate essential themes and structures of social work practices for better preparedness to meet similar crises in the future (Gearing, Saini, & McNeill, 2007; Rowlands, 2007). Perspectives and recommendations of young participants who were hospitalized during SARS in a large pediatric hospital had also been covered (Koller, Nicholas, Gearing, & Kalfa, 2010). Social work scholars advocated the inclusion of children\u2019s omitted voices in health care decision making, policy planning, and the development of guidelines for future pandemics. More recent studies explored the social and psychoemotional difficulties of foreign residents in South Korea during the MERS outbreak in the community from a social work perspective (Park & Lee, 2016) as well as the functional role of community-based health organizations as gatekeepers of communities in Iran in times of biohazards (Rezaei, Maracy, Yarmohammadian, & Keyvanara, 2020). The role of social work in pandemics has long been significant throughout the years but has not been well documented in the literature. It can be traced back to as early as 1918 with the pivotal and central role hospital social workers played during the great worldwide influenza pandemic. But only sketchy and cryptic reference was provided to social workers during a pandemic by authoritative public health institutes or organizations around the globe (Rosoff, 2008). Social workers can perform an essential role in crisis management while keeping in mind that the basic human rights of every person must be safeguarded. The World Health Organization published guidelines titled Infection Prevention and Control of Epidemicand Pandemic-Prone Acute Respiratory Infections in Health Care in 2014 but placed social workers under the umbrella term \u2018\u2018healthcare worker\u201d with many other professionals. Professional bodies such as the National Association of Social Workers could and should take the lead in directing social workers to participate in prevention and control at the time of severe outbreaks. In our current battle against COVID-19, the Chinese Association of Social Workers, which helps coordinate practitioners in the forefront of the fighting, published The Social Worker Support Manual on the Prevention and Control of Pneumonia Caused by the Coronavirus Infection and proposed basic working principles and methods to social workers. Although the original version is in Chinese, it serves as an excellent foundation for further development of a more established international protocol. Sudden onset of novel respiratory diseases in the community created an immediate necessity for conducting studies in adapting to the new epidemic environment. Only traditional crisis management approaches and theories have now been applied to medical social work, which help little when facing emerging risks in the course of combating novel viruses and challenges. Theoretical frameworks of social work intervention in relation to these unprecedented types of emergency, such as SARS, have not been consciously developed (Rowlands, 2007). Social work departments at institutions should not only play their role in the ivory tower with laboratory studies. Practitioners and academics need to work in synchrony with each other at the time of infectious disease control. CONCLUSION In the last 20 years, high-impact social work journals remain quite detached from the outbreaks. Only two articles were accepted in Q1 journals (that is, the top 25 percent of the journal impact factor distribution). There is seemingly a lack of interest among academics and editors of the top journals on these issues. Only one journal had developed a special issue; however, it mainly included conceptual articles. Ample research has helped inform medical and nursing practice in dealing with tiny but deadly viruses and has been published and well cited in some of the most prestige academic journals in the world. High-impact social work journals should also take a much more proactive stance to map out a new research agenda in a timely manner. Only then can state-of-the-art and CHEUNG / What Have Hospital Social Workers Been Prep",
        "keywords": []
    },
    "827e91572a0e6e3dba0d0c81d0e4e74b99a39d74.pdf": {
        "title": "Rationalizing medical work",
        "authors": [
            "P. Taylor"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/827e91572a0e6e3dba0d0c81d0e4e74b99a39d74.pdf",
        "venue": "Journal of Health Services Research and Policy",
        "citationCount": 25,
        "score": 2.7777777777777777,
        "summary": "",
        "keywords": []
    },
    "69afe1e8f19e3a2c3837abff5151b2630b0de177.pdf": {
        "title": "Analyzing the Coronary Heart Disease Mortality Decline in a Mediterranean Population : Spain 1988-2005",
        "authors": [
            "G. Flores-Mateo",
            "M. Grau",
            "M. O\u2019Flaherty",
            "R. Ramos",
            "R. Elos\u00faa",
            "Concepci\u00f3n Viol\u00e1n-Fors",
            "M. Quesada",
            "J. Sala",
            "J. Marrugat",
            "Simon Capewelld"
        ],
        "published_date": "2011",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/69afe1e8f19e3a2c3837abff5151b2630b0de177.pdf",
        "venue": "",
        "citationCount": 36,
        "score": 2.571428571428571,
        "summary": "",
        "keywords": []
    },
    "c2ad93afc4b4bcde6f48b955386f091ffe930ff1.pdf": {
        "title": "Frequency of discussing and documenting advance care planning in primary care: secondary analysis of a multicenter cross-sectional observational study",
        "authors": [
            "J. Hamano",
            "A. Oishi",
            "T. Morita",
            "Y. Kizawa"
        ],
        "published_date": "2020",
        "abstract": "Background To improve the quality of advance care planning (ACP) in primary care, it is important to understand the frequency of and topics involved in the ACP discussion between patients and their family physicians (FPs). Methods A secondary analysis of a previous multicenter cross-sectional observational study was performed. The primary outcome of this analysis was the frequency of and topics involved in the ACP discussion between outpatients and FPs. In March 2017, 22 family physicians at 17 clinics scheduled a day to assess outpatients and enrolled patients older than 65\u2009years who were recognized by FPs as having regular visits. We defined three ACP discussion topics: 1) future decline in activities of daily living (ADL), 2) future inability to eat, and 3) surrogate decision makers. FPs assessed whether they had ever discussed any ACP topics with each patient and their family members, and if they had documented the results of these discussions in medical records before patients were enrolled in the present study. We defined patients as being at risk of deteriorating and dying if they had at least 2 positive general indicators or at least 1 positive disease-specific indicator in the Japanese version of the Supportive and Palliative Care Indicators Tool. Results In total, 382 patients with a mean age of 77.4\u2009\u00b1\u20097.9\u2009years were enrolled, and 63.1% were female. Seventy-nine patients (20.7%) had discussed at least one ACP topic with their FPs. However, only 23 patients (6.0%) had discussed an ACP topic with family members and their FPs, with the results being documented in their medical records. The topic of future ADL decline was discussed and documented more often than the other two topics. Patients at risk of deteriorating and dying discussed ACP topics significantly more often than those not at risk of deteriorating and dying (39.4% vs. 16.8%, p \u2009<\u20090.001). Conclusion FPs may discuss ACP with some of their patients, but may not often document the results of this discussion in medical records. FPs need to be encouraged to discuss ACP with patients and family members and describe the decisions reached in medical records.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c2ad93afc4b4bcde6f48b955386f091ffe930ff1.pdf",
        "venue": "BMC Palliative Care",
        "citationCount": 12,
        "score": 2.4000000000000004,
        "summary": "Background To improve the quality of advance care planning (ACP) in primary care, it is important to understand the frequency of and topics involved in the ACP discussion between patients and their family physicians (FPs). Methods A secondary analysis of a previous multicenter cross-sectional observational study was performed. The primary outcome of this analysis was the frequency of and topics involved in the ACP discussion between outpatients and FPs. In March 2017, 22 family physicians at 17 clinics scheduled a day to assess outpatients and enrolled patients older than 65\u2009years who were recognized by FPs as having regular visits. We defined three ACP discussion topics: 1) future decline in activities of daily living (ADL), 2) future inability to eat, and 3) surrogate decision makers. FPs assessed whether they had ever discussed any ACP topics with each patient and their family members, and if they had documented the results of these discussions in medical records before patients were enrolled in the present study. We defined patients as being at risk of deteriorating and dying if they had at least 2 positive general indicators or at least 1 positive disease-specific indicator in the Japanese version of the Supportive and Palliative Care Indicators Tool. Results In total, 382 patients with a mean age of 77.4\u2009\u00b1\u20097.9\u2009years were enrolled, and 63.1% were female. Seventy-nine patients (20.7%) had discussed at least one ACP topic with their FPs. However, only 23 patients (6.0%) had discussed an ACP topic with family members and their FPs, with the results being documented in their medical records. The topic of future ADL decline was discussed and documented more often than the other two topics. Patients at risk of deteriorating and dying discussed ACP topics significantly more often than those not at risk of deteriorating and dying (39.4% vs. 16.8%, p \u2009<\u20090.001). Conclusion FPs may discuss ACP with some of their patients, but may not often document the results of this discussion in medical records. FPs need to be encouraged to discuss ACP with patients and family members and describe the decisions reached in medical records.",
        "keywords": []
    },
    "b85d6105f79e214efde9dfd93e1330cd86580db3.pdf": {
        "title": "Relationship of Serum Soluble Klotho Levels and Echocardiographic Parameters in Patients on Maintenance Hemodialysis",
        "authors": [
            "Ai-hua Zhang",
            "Weikang Guo",
            "Ling Yu",
            "Wen-Hu Liu"
        ],
        "published_date": "2019",
        "abstract": "Background: Cardiovascular disease is the leading cause of morbidity and mortality in maintenance hemodialysis (MHD) patients. Uremic cardiomyopathy, characterized by myocardial hypertrophy and fibrosis, has a significant contribution to these adverse cardiac outcomes. The protective effect of soluble Klotho (s-Klotho) on myocardial damage was demonstrated in in vitro and animal experiments. However, data from MHD patients is limited. The present study was designed to identify potential correlations between echocardiographic parameters and serum s-Klotho levels in MHD patients. Methods: This is a cross-sectional study involving 105 MHD patients from the Dialysis Center of Capital Medical University affiliated Beijing Friendship Hospital between March and October 2014. The general information for each patient was recorded. Fasting blood samples were collected prior to hemodialysis during the mid-week session in all patients. The echocardiogram and left lateral lumbar spine radiograph were performed after the same mid-week session. The dialysis records for each session within 3 months before the blood tests were documented. According to the quartiles of s-Klotho levels, patients were divided into four groups (Group 1\u20134). The demographic and clinical characteristics, echocardiographic parameters, and abdominal aortic calcification scores among the groups were compared. Results: The enrolled 105 patients were predominantly male (54.3%) with an average age of 59.9 \u00b1 11.2 years. Previous hemodialysis durations were 76 (42\u2013133) months. Sixteen (15.2%) patients had diabetes mellitus. Mean serum s-Klotho level was 411.83 \u00b1 152.95 pg/mL, and the 25th percentile, 50th percentile, and 75th percentile values of serum s-Klotho levels were 298.9, 412, and 498.2 pg/mL, respectively. Individuals in the bottom quartile of s-Klotho levels (Group 1) had significantly increased interventricular septal thickness (IVST) compared to those in the other three quartiles of s-Klotho levels (Group 1: 1.12 \u00b1 0.16 cm; vs. Group 2: 1.12 \u00b1 0.16 cm, p = 0.008; vs. Group 3: 0.94 \u00b1 0.13 cm, p < 0.001; vs. Group 4: 1.03 \u00b1 0.1 5 cm, p = 0.022). There were significant differences in the ratios of IVST and posterior wall thickness (PWT) between patients of Group 1 and Group 3 (1.12 \u00b1 0.1 2 vs. 1.00 \u00b1 0.1 4, p = 0.004). No significant differences were found for other parameters among the groups. The univariate correlation analyses showed that gender (r = \u20130.211, p = 0.030), Kt/V urea (r = \u20130.240, p = 0.014), hypersensitive C reactive protein (hs-CRP) (r = 0.196, p = 0.045), and serum s-Klotho levels (r = \u20130.260, p = 0.007) significantly correlated with IVST. Ultimately, only hs-CRP and serum s-Klotho levels were entered into a multiple regression model. Conclusions: The present study showed that patients with lower circulating s-Klotho levels were more often associated with larger IVST and greater ratios of IVST and PWT. There was an independent association between s-Klotho and IVST, and lower s-Klotho levels seem to be a potential risk factor of uremic cardiomyopathy in MHD patients.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/b85d6105f79e214efde9dfd93e1330cd86580db3.pdf",
        "venue": "Kidney & Blood Pressure Research",
        "citationCount": 14,
        "score": 2.333333333333333,
        "summary": "Background: Cardiovascular disease is the leading cause of morbidity and mortality in maintenance hemodialysis (MHD) patients. Uremic cardiomyopathy, characterized by myocardial hypertrophy and fibrosis, has a significant contribution to these adverse cardiac outcomes. The protective effect of soluble Klotho (s-Klotho) on myocardial damage was demonstrated in in vitro and animal experiments. However, data from MHD patients is limited. The present study was designed to identify potential correlations between echocardiographic parameters and serum s-Klotho levels in MHD patients. Methods: This is a cross-sectional study involving 105 MHD patients from the Dialysis Center of Capital Medical University affiliated Beijing Friendship Hospital between March and October 2014. The general information for each patient was recorded. Fasting blood samples were collected prior to hemodialysis during the mid-week session in all patients. The echocardiogram and left lateral lumbar spine radiograph were performed after the same mid-week session. The dialysis records for each session within 3 months before the blood tests were documented. According to the quartiles of s-Klotho levels, patients were divided into four groups (Group 1\u20134). The demographic and clinical characteristics, echocardiographic parameters, and abdominal aortic calcification scores among the groups were compared. Results: The enrolled 105 patients were predominantly male (54.3%) with an average age of 59.9 \u00b1 11.2 years. Previous hemodialysis durations were 76 (42\u2013133) months. Sixteen (15.2%) patients had diabetes mellitus. Mean serum s-Klotho level was 411.83 \u00b1 152.95 pg/mL, and the 25th percentile, 50th percentile, and 75th percentile values of serum s-Klotho levels were 298.9, 412, and 498.2 pg/mL, respectively. Individuals in the bottom quartile of s-Klotho levels (Group 1) had significantly increased interventricular septal thickness (IVST) compared to those in the other three quartiles of s-Klotho levels (Group 1: 1.12 \u00b1 0.16 cm; vs. Group 2: 1.12 \u00b1 0.16 cm, p = 0.008; vs. Group 3: 0.94 \u00b1 0.13 cm, p < 0.001; vs. Group 4: 1.03 \u00b1 0.1 5 cm, p = 0.022). There were significant differences in the ratios of IVST and posterior wall thickness (PWT) between patients of Group 1 and Group 3 (1.12 \u00b1 0.1 2 vs. 1.00 \u00b1 0.1 4, p = 0.004). No significant differences were found for other parameters among the groups. The univariate correlation analyses showed that gender (r = \u20130.211, p = 0.030), Kt/V urea (r = \u20130.240, p = 0.014), hypersensitive C reactive protein (hs-CRP) (r = 0.196, p = 0.045), and serum s-Klotho levels (r = \u20130.260, p = 0.007) significantly correlated with IVST. Ultimately, only hs-CRP and serum s-Klotho levels were entered into a multiple regression model. Conclusions: The present study showed that patients with lower circulating s-Klotho levels were more often associated with larger IVST and greater ratios of IVST and PWT. There was an independent association between s-Klotho and IVST, and lower s-Klotho levels seem to be a potential risk factor of uremic cardiomyopathy in MHD patients.",
        "keywords": []
    },
    "b3c553eda3f8684847f30ab44f34a741ad38b4ce.pdf": {
        "title": "Incidence and management of arthralgias in breast cancer patients treated with aromatase inhibitors in an outpatient oncology clinic",
        "authors": [
            "P. Menas",
            "D. Merkel",
            "W. Hui",
            "J. Lawton",
            "A. Harper",
            "G. Carro"
        ],
        "published_date": "2012",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/b3c553eda3f8684847f30ab44f34a741ad38b4ce.pdf",
        "venue": "Journal of Oncology Pharmacy Practice",
        "citationCount": 29,
        "score": 2.230769230769231,
        "summary": "",
        "keywords": []
    },
    "45b70dbd9fd89d0e00252d41469429569ee053ab.pdf": {
        "title": "Technical Design Report for the: PANDA Micro Vertex Detector",
        "authors": [
            "P. C. W. Erni",
            "I. Keshelashvili",
            "B. Krusche",
            "M. Steinacher",
            "Y. Heng",
            "Z. Liu",
            "H. Liu",
            "X. Shen",
            "Q. Wang",
            "H. Xu",
            "M. Albrecht",
            "J. Becker",
            "K. Eickel",
            "F. Feldbauer",
            "M. Fink",
            "P. Friedel",
            "F. Heinsius",
            "T. Held",
            "H. Koch",
            "B. Kopf",
            "M. Leyhe",
            "C. Motzko",
            "M. Pelizaus",
            "J. Pychy",
            "B. Roth",
            "T. Schroder",
            "J. Schulze",
            "M. Steinke",
            "T. Trifterer",
            "U. Wiedner",
            "J. Zhong",
            "R. Beck",
            "M. Becker",
            "S. Bianco",
            "K. Brinkmann",
            "C. Hammann",
            "F. Hinterberger",
            "R. Jakel",
            "D. Kaiser",
            "R. Kliemt",
            "K. Koop",
            "C. Schmidt",
            "R. Schnell",
            "U. Thoma",
            "P. Vlasov",
            "C. Wendel",
            "A. Winnebeck",
            "T. Wurschig",
            "H. Zaunick",
            "A. Bianconi",
            "M. Bragadireanu",
            "M. Caprini",
            "M. Ciubancan",
            "D. Pantea",
            "P. Tarta",
            "M. Napoli",
            "F. Giacoppo",
            "E. Rapisarda",
            "C. Sfienti",
            "T. Fiutowski",
            "N. Idzik",
            "B. Mindur",
            "D. Przyborowski",
            "K. \u015awientek",
            "E. Bialkowski",
            "A. Budzanowski",
            "B. Czech",
            "S. Kliczewski",
            "A. Kozela",
            "P. Kulessa",
            "P. Lebiedowicz",
            "K. Malgorzata",
            "K. Pysz",
            "W. Schafer",
            "R. Siudak",
            "A. Szczurek",
            "P. Brandys",
            "T. Czy\u017cewski",
            "W. Czy\u017cycki",
            "M. Domagala",
            "M. Hawryluk",
            "G. Filo",
            "D. Kwiatkowski",
            "E. Lisowski",
            "F. Lisowski",
            "W. Bardan",
            "D. Gil",
            "B. Kamys",
            "S. Kistryn",
            "K. Korcyl",
            "W. Krzemie\u0144",
            "A. Magiera",
            "P. Moskal",
            "Z. Rudy",
            "P. Salabura",
            "J. Smyrski",
            "A. Wro\u0144ska",
            "M. Al-Turany",
            "R. Arora",
            "I. Augustin",
            "H. Deppe",
            "D. Dutta",
            "H. Flemming",
            "K. Gotzen",
            "G. Hohler",
            "R. Karabowicz",
            "D. Lehmann",
            "B. Lewandowski",
            "J. Luhning",
            "F. Maas",
            "H. Orth",
            "K. Peters",
            "T. Saito",
            "G. Schepers",
            "C. Schmidt",
            "L. Schmitt",
            "C. Schwarz",
            "J. Schwiening",
            "B. Voss",
            "P. Wieczorek",
            "A. Wilms",
            "V. Abazov",
            "G. Alexeev",
            "V. Arefiev",
            "V. Astakhov",
            "M. Barabanov",
            "B. Batyunya",
            "Y. Davydov",
            "V. Dodokhov",
            "A. Efremov",
            "A. Fedunov",
            "A. Feshchenko",
            "A. Galoyan",
            "S. Grigoryan",
            "A. Karmokov",
            "E. Koshurnikov",
            "V. Lobanov",
            "Y. Lobanov",
            "A. Makarov",
            "L. Malinina",
            "V. Malyshev",
            "G. A. Mustafaev",
            "A. Olshevski",
            "M. Pasyuk",
            "E. Perevalova",
            "A. A. Piskun",
            "T. Pocheptsov",
            "G. Pontecorvo",
            "V. Rodionov",
            "Y. Rogov",
            "R. Salmin",
            "A. Samartsev",
            "M. Sapozhnikov",
            "G. Shabratova",
            "A. Skachkova",
            "N. B. Skachkov",
            "E. Strokovsky",
            "M. Suleimanov",
            "R. Teshev",
            "V. Tokmenin",
            "V. Uzhinsky",
            "A. Vodopyanov",
            "S. Zaporozhets",
            "N. Zhuravlev",
            "A. Zorin",
            "D. Branford",
            "D. Glazier",
            "D. Watts",
            "P. Woods",
            "A. Britting",
            "W. Eyrich",
            "A. Lehmann",
            "F. Uhlig",
            "S. Dobbs",
            "Z. Metreveli",
            "K. Seth",
            "B. Tann",
            "A. Tomaradze",
            "D. Bettoni",
            "V. Carassiti",
            "P. Dalpiaz",
            "A. Drago",
            "E. Fioravanti",
            "I. Garzia",
            "M. Negrini",
            "M. Savri\u00e9",
            "G. Stancari",
            "B. Dulach",
            "P. Gianotti",
            "C. Guaraldo",
            "V. Lucherini",
            "E. Pace",
            "A. Bersani",
            "M. Macri",
            "M. Marinelli",
            "R. Parodi",
            "V. Dormenev",
            "P. Drexler",
            "M. Duren",
            "T. Eisner",
            "K. Foehl",
            "A. Hayrapetyan",
            "P. Koch",
            "B. Krioch",
            "W. Kuhn",
            "S. Lange",
            "Y. Liang",
            "M. Liu",
            "O. Merle",
            "V. Metag",
            "M. Moritz",
            "M. Nanova",
            "R. Novotn\u00fd",
            "B. Spruck",
            "H. Stenzel",
            "C. Strackbein",
            "M. Thiel",
            "T. Clarkson",
            "C. Euan",
            "G. Hill",
            "M. Hoek",
            "D. Ireland",
            "R. Kaiser",
            "T. Keri",
            "I. Lehmann",
            "K. Livingston",
            "P. Lumsden",
            "D. MacGregor",
            "B. McKinnon",
            "R. Montgomery",
            "M. Murray",
            "D. Protopopescu",
            "G. Rosner",
            "B. Seitz",
            "G. Yang",
            "M. Babai",
            "A. Biegun",
            "A. Glazenborg-Kluttig",
            "E. Guliyev",
            "V. Jothi",
            "M. Kavatsyuk",
            "P. Lemmens",
            "H. Lohner",
            "J. Messchendorp",
            "T. Poelman",
            "H. Smit",
            "J. C. Weele",
            "H. Sohlbach",
            "M. Buscher",
            "R. Dosdall",
            "R. Dzhygadlo",
            "S. Esch",
            "A. Gillitzer",
            "F. Goldenbaum",
            "D. Grunwald",
            "V. Jha",
            "G. Kemmerling",
            "H. Kleines",
            "A. Lehrach",
            "R. Maier",
            "M. Mertens",
            "H. Ohm",
            "D. Pohl",
            "D. Prasuhn",
            "T. Randriamalala",
            "J. Ritman",
            "M. Roeder",
            "G. Sterzenbach",
            "T. Stockmanns",
            "P. Wintz",
            "P. Wustner",
            "J. Kisiel",
            "S. Li",
            "Z. Li",
            "Z. Sun",
            "K. Fissum",
            "K. Hansen",
            "L. Isaksson",
            "M. Lundin",
            "B. Schroder",
            "P. Achenbach",
            "A. Denig",
            "M. Distler",
            "M. Fritsch",
            "D. Kangh",
            "A. Karavdina",
            "W. Lauth",
            "M. Michel",
            "M. C. M. Espi",
            "J. Pochodzalla",
            "S. S\u00e1nchez",
            "A. Sanchez-Lorente",
            "T. Weber",
            "V. Dormenev",
            "A. Fedorov",
            "M. Korzhik",
            "O. Missevitch",
            "V. Balanutsa",
            "V. Chernetsky",
            "A. Demekhin",
            "A. Dolgolenko",
            "P. Fedorets",
            "A. Gerasimov",
            "V. Goryachev",
            "A. Boukharov",
            "O. Malyshev",
            "I. Marishev",
            "A. Semenov",
            "R. Varma",
            "B. Ketzer",
            "I. Konorov",
            "A. Mann",
            "S. Neubert",
            "S. Paul",
            "M. Vandenbroucke",
            "Q. Zhang",
            "A. Khoukaz",
            "T. Rausmann",
            "A. Taschner",
            "J. Wessels",
            "E. Baldin",
            "K. Kotov",
            "S. Peleganchuk",
            "Y. Tikhonov",
            "T. Hennino",
            "M. Imre",
            "R. Kunne",
            "C. L. Galliard",
            "J. L. Normand",
            "D. Marchand",
            "A. Maroni",
            "S. Ong",
            "J. Pouthas",
            "B. Ramstein",
            "P. Rosier",
            "M. Sudol",
            "C. Th\u00e9neau",
            "E. Tomasi-Gustafsson",
            "J. Wiele",
            "T. Zerguerras",
            "G. Boca",
            "A. Braghieri",
            "S. Costanza",
            "A. Fontana",
            "P. Genova",
            "L. Lavezzi",
            "P. Montagna",
            "A. Rotondi",
            "V. Buda",
            "V. Abramov",
            "A. Davidenko",
            "A. Derevschikov",
            "Y. Goncharenko",
            "V. Grishin",
            "V. Kachanov",
            "D. Konstantinov",
            "V. Kormilitsin",
            "Y. Matulenko",
            "Y. Melnik",
            "A. Meschanin",
            "N. Minaev",
            "V. Mochalov",
            "D. Morozov",
            "L. Nogach",
            "S. Nurushev",
            "A. Ryazantsev",
            "P. Semenov",
            "L. Soloviev",
            "A. Uzunian",
            "A. Vasiliev",
            "A. Yakutin",
            "S. Belostotski",
            "G. Gavrilov",
            "A. Itzotov",
            "A. Kisselev",
            "P. Kravchenko",
            "S. Manaenkov",
            "O. Miklukho",
            "Y. Naryshkin",
            "D. Veretennikov",
            "V. Vikhrov",
            "A. Zhadanov",
            "T. Back",
            "B. Cederwall",
            "C. Bargholtz",
            "L. Ger'en",
            "P. Tegn'er",
            "P. Thorngren",
            "K. M. V. Wurtemberg",
            "L. Fava",
            "D. Alberto",
            "A. Amoroso",
            "M. Bussa",
            "L. Busso",
            "F. Mori",
            "M. Destefanis",
            "L. Ferrero",
            "M. Greco",
            "T. Kugathasan",
            "M. Maggiora",
            "S. Marcello",
            "S. Sosio",
            "S. Spataro",
            "D. Calvo",
            "S. Coli",
            "P. Remigis",
            "A. Filippi",
            "G. Giraudo",
            "S. Lusso",
            "G. Mazza",
            "M. Mignone",
            "A. Rivetti",
            "R. Wheadon",
            "L. Zotti",
            "O. Morra",
            "F. Iazzi",
            "A. Lavagno",
            "P. Quarati",
            "K. Szymanska",
            "R. Birsa",
            "F. Bradamante",
            "A. Bressan",
            "A. Martin",
            "H. Clement",
            "B. G\u00e5lnander",
            "H. Cal'en",
            "K. Fransson",
            "T. Johansson",
            "A. Kupsc",
            "P. Marciniewski",
            "E. Thom'e",
            "M. Wolke",
            "J. Zlomanczuk",
            "J. D'iaz",
            "A. Ortiz",
            "P. Buda",
            "K. Dmowski",
            "R. Korzeniewski",
            "D. Przemyslaw",
            "B. Slowinski",
            "S. Borsuk",
            "A. Chlopik",
            "Z. Guzik",
            "J. Kopec",
            "T. Koz\u0142owski",
            "D. Melnychuk",
            "M. P\u0142omi\u0144ski",
            "J. Szewinski",
            "K. Traczyk",
            "B. Zwi\u0229gli\u0144ski",
            "P. Buhler",
            "A. Gruber",
            "P. Kienle",
            "J. Marton",
            "E. Widmann",
            "J. Z. U. B. Switzerland",
            "Institute for High Energy Physics",
            "Chinese Academy of Sciences",
            "Beijing China",
            "Universitat Bochum I. Institut fur Experimentalphysik",
            "Germany",
            "R. Germany",
            "Universita di Brescia Italy",
            "Institutul National de CD pentru Fizica si Inginerie Nucleara Hulubei",
            "Bukarest-Magurele Romania",
            "D. D. F. E. A. U. D. Catania",
            "Infn",
            "Sezione di Catania Italy",
            "A. U. O. Science",
            "Technology Cracow Poland",
            "Ifj",
            "Institute of Nuclear Physics Pan",
            "Cracow Poland",
            "I. Informatics",
            "University of Technology",
            "Instytut Fizyki",
            "Uniwersytet Jagiello\u0144ski",
            "Gesellschaft f\u00fcr Schwerionenforschung Mbh",
            "D. Germany",
            "Veksler-Baldin Laboratory of High Energies",
            "University of Edinburgh United Kingdom",
            "Friedrich Alexander Universitat Erlangen-Nurnberg Germany",
            "N. University",
            "A. EvanstonU.S.",
            "U. Ferrara",
            "Sezione di Ferrara",
            "Italy",
            "INFN-Laboratori Nazionali di Frascati Italy",
            "Sezione di Genova Italy",
            "J. N. I. P. Institut",
            "University of Edinburgh United Kingdom",
            "Kernfysisch Versneller Instituut",
            "University of Groningen Netherlands",
            "Fachhochschule Sudwestfalen",
            "Iserlohn Germany",
            "Forschungszentrum Julich",
            "I. F. Kernphysik",
            "J. Germany",
            "U. Silesia",
            "Katowice Poland"
        ],
        "published_date": "2012",
        "abstract": "This document illustrates the technical layout and the expected performance of the Micro Vertex Detector (MVD) of the PANDA experiment. The MVD will detect charged particles as close as possible to the interaction zone. Design criteria and the optimisation process as well as the technical solutions chosen are discussed and the results of this process are subjected to extensive Monte Carlo physics studies. The route towards realisation of the detector is outlined.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/45b70dbd9fd89d0e00252d41469429569ee053ab.pdf",
        "venue": "",
        "citationCount": 29,
        "score": 2.230769230769231,
        "summary": "This technical design report details the development and expected performance of the Micro Vertex Detector (MVD) for the PANDA experiment \\cite{erni2012uo0}.\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: The paper addresses the design and optimization of a high-resolution, radiation-hard Micro Vertex Detector (MVD) capable of precisely tracking charged particles and identifying secondary vertices very close to the interaction point in the PANDA experiment.\n*   **Importance and Challenge**: This problem is crucial for PANDA's scientific program, which focuses on precise studies of antiproton-proton annihilations, particularly in the charm quark sector, to explore the strong interaction and hadron structure. The challenges include:\n    *   Achieving excellent spatial resolution (vertexing and tracking) for short-lived particles (e.g., D mesons).\n    *   Operating in a high-luminosity, high-rate environment with significant background and radiation levels.\n    *   Minimizing the material budget to reduce multiple scattering, which degrades momentum and vertex resolution.\n    *   Implementing a robust and efficient readout system for continuous data acquisition without a hardware trigger.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**: The MVD design benefits from expertise gained during the construction of LHC detectors and B-factory experiments, leveraging advancements in silicon detector technology and readout electronics \\cite{erni2012uo0}. It adapts these established technologies to the unique requirements of the PANDA experiment.\n*   **Limitations of Previous Solutions**: While not explicitly detailing limitations of *other MVDs*, the paper positions PANDA as a unique tool for charm quark physics, surpassing electron machines in terms of final states and cross-sections, and complementing future upgrades at existing high-energy physics facilities. The MVD design aims to overcome the inherent challenges of high-rate hadron physics experiments by optimizing for minimal material, high resolution, and radiation hardness, which are critical for PANDA's specific physics goals.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The MVD employs a hybrid design combining two types of silicon detectors:\n    *   **Silicon Pixel Detectors**: Located in the innermost layers, providing very high spatial resolution for primary and secondary vertex reconstruction.\n    *   **Double-Sided Silicon Strip Detectors (DSSD)**: Placed in outer layers, offering good resolution over a larger area with a lower channel count per unit area compared to pixels.\n*   **Novelty/Differentiation**: The approach is novel due to its specific optimization for the PANDA environment \\cite{erni2012uo0}:\n    *   **Minimal Material Budget**: Extensive efforts in thinning sensors (down to 50-100 \u00b5m), using lightweight support structures (e.g., carbon fiber), and advanced cooling systems (e.g., micro-channel cooling) to minimize scattering.\n    *   **Radiation Hardness**: Selection of silicon sensor technologies (e.g., epitaxial or oxygenated silicon) and front-end electronics designed to withstand high radiation doses expected over the detector's lifetime.\n    *   **Triggerless Readout**: Development of custom front-end ASICs (e.g., PASTA for pixels, n-XYTER for strips) capable of self-triggering and continuous data streaming, essential for PANDA's high interaction rates and diverse physics channels.\n    *   **Integrated Design**: A comprehensive design covering sensors, front-end electronics, hybridisation, mechanical support, cooling, power supply, and data transmission, all optimized for the PANDA spectrometer.\n\n### 4. Key Technical Contributions\n\n*   **Novel Algorithms, Methods, or Techniques**:\n    *   Development and testing of thinned silicon pixel and strip sensors, including radiation damage studies to select optimal technologies \\cite{erni2012uo0}.\n    *   Design of custom front-end ASICs (PASTA for pixels, n-XYTER for strips) with features like self-triggering, high-rate capability, low power consumption, and integrated feature extraction (for strips).\n    *   Advanced hybridisation techniques for connecting ASICs to sensors and module data concentrators.\n    *   Sophisticated Monte Carlo simulation framework for detailed performance prediction, including particle transport, digitization, noise emulation, and local reconstruction.\n*   **System Design or Architectural Innovations**:\n    *   The modular MVD layout, integrating pixel barrels and disks with strip barrels and wedges, providing nearly 4\u03c0 coverage.\n    *   Optical data transmission system (GigaBit Transceiver) for high-bandwidth, low-noise data transfer.\n    *   Distributed power supply system designed for minimal heat dissipation and cable mass.\n    *   Lightweight mechanical support structures (e.g., carbon fiber frames) and efficient cooling systems (e.g., evaporative CO2 cooling for pixels, water cooling for strips) to maintain thermal stability while minimizing material.\n*   **Theoretical Insights or Analysis**: Extensive Monte Carlo studies provide detailed predictions for detector performance, including hit resolution, vertexing capabilities, particle identification via dE/dx, radiation damage effects, material budget, and rate estimations under various physics scenarios \\cite{erni2012uo0}.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**:\n    *   Tests of first thinned pixel prototypes and full-size prototype sensors.\n    *   Radiation damage studies on silicon sensors to evaluate performance degradation.\n    *   Characterization of ASIC prototypes (e.g., PASTA, n-XYTER) to verify functionality and performance.\n    *   Single chip assembly prototypes and test assemblies for strip hybrids to validate manufacturing processes.\n    *   Validation of the simulation framework using data from the Bonn Tracking Station, comparing simulated and measured resolutions and scattering effects \\cite{erni2012uo0}.\n*   **Key Performance Metrics and Comparison Results**:\n    *   **Hit Resolution**: Expected pixel resolution of 5-10 \u00b5m and strip resolution of 10-30 \u00b5m, crucial for vertexing.\n    *   **Vertexing Performance**: Demonstrated excellent primary vertex resolution (e.g., ~30 \u00b5m for pions) and secondary vertex resolution for D mesons (e.g., ~50 \u00b5m for D0 decay length) through simulations.\n    *   **Particle Identification (PID)**: Utilizes energy loss (dE/dx) information from the MVD for low-momentum particle identification.\n    *   **Material Budget**: Achieved a very low material budget (e.g., ~0.3% X0 per pixel layer, ~0.6% X0 per strip layer) to minimize multiple scattering.\n    *   **Radiation Hardness**: Sensors and electronics designed to withstand fluences up to 10^14 neq/cm^2 and doses up to 100 kGy.\n    *   **Rate Capability**: The triggerless readout system is designed to handle interaction rates up to 20 MHz.\n    *   **Physics Channels**: Performance validated through benchmark physics channels like pp -> psi(2S) -> J/psi + pi+pi- and D meson decays, showing high efficiency and resolution for reconstructing these processes \\cite{erni2012uo0}.\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**: As a technical design report, the document outlines the *planned* solutions and *expected* performance based on simulations and prototype tests. The actual performance will depend on the successful realization, integration, and commissioning of all components. Assumptions are made regarding the final performance of custom ASICs and the long-term stability of materials under radiation.\n*   **Scope of Applicability**: The MVD is specifically designed for the PANDA experiment at FAIR, operating with antiproton beams in the momentum range of 1.5 GeV/c to 15 GeV/c, corresponding to center-of-mass energies between 1 GeV and 5 GeV. Its design is tailored to the unique requirements of antiproton-proton annihilations and the specific detector environment of PANDA.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**: The PANDA MVD design represents a significant advancement in vertex detector technology for high-luminosity, high-rate hadron physics experiments \\cite{erni2012uo0}. It pushes the boundaries in achieving ultra-low material budget, high spatial resolution, and robust operation in a harsh radiation environment without a hardware trigger. The detailed engineering and validation through extensive simulations provide a comprehensive blueprint for future detectors.\n*   **Potential Impact on Future Research**: This work provides valuable insights and a validated design methodology for future experiments requiring precision tracking and vertexing in challenging environments. The specific developments in thinned silicon sensors, radiation-hard custom ASICs with triggerless readout, and advanced cooling/support structures could be adopted or adapted for other particle physics or nuclear physics experiments, particularly those focusing on charm or beauty physics, or operating at future high-intensity facilities.",
        "keywords": [
            "Micro Vertex Detector (MVD)",
            "PANDA experiment",
            "Silicon pixel and strip detectors",
            "High spatial resolution",
            "Vertexing",
            "Radiation hardness",
            "Minimal material budget",
            "Triggerless readout",
            "Custom front-end ASICs",
            "Charm quark physics",
            "Monte Carlo simulation",
            "Thinned silicon sensors",
            "Antiproton-proton annihilations",
            "High-rate capability"
        ],
        "paper_type": "based on the abstract and the provided introduction (which includes a table of contents snippet), this paper is best classified as **technical**.\n\nhere's why:\n\n*   **title:** \"technical design report for the: panda micro vertex detector\" explicitly indicates a technical document describing a system's design.\n*   **abstract:** focuses heavily on describing the architecture, components, firmware, and software of the \"j\u00fclich readout system.\" it details communication layers, hardware specifications (fpga boards, optical connections), software framework (mrf, gui), and modular design. keywords like \"defines,\" \"developed,\" \"implemented functionality,\" \"main hardware component,\" and \"main software component\" are strong indicators of presenting a new system or method.\n*   **introduction (table of contents):** outlines sections dedicated to \"powering concept,\" \"cables,\" \"mechanical structures,\" \"cooling system,\" and \"dcs,\" all of which are core elements of a technical design. while it also includes \"monte-carlo simulations and performance,\" \"tracking and vertexing,\" and \"pid algorithm,\" these are often included in technical design reports to demonstrate and validate the performance of the designed system.\n*   **appendix c (details on vertexing):** while this section describes empirical studies and simulations to characterize performance, it serves to validate the design of the mvd, which is the subject of the technical report. the overall paper's primary goal, as indicated by the abstract and title, is to present the design and implementation of the detector and its readout system.\n\nthe paper \"presents new methods, algorithms, or systems\" by detailing the design and implementation of the panda micro vertex detector's readout system."
    },
    "0b0e093a33e9538cbf04075220fc0f15e03c2807.pdf": {
        "title": "Abemaciclib with or without fulvestrant for the treatment of hormone receptor-positive and HER2-negative metastatic breast cancer with disease progression following prior treatment with palbociclib.",
        "authors": [
            "Keerthi Tamragouri",
            "M. Cobleigh",
            "R. Rao"
        ],
        "published_date": "2019",
        "abstract": "e12533 Background: Abemaciclib is a selective inhibitor of CDK4 and CDK6 kinase activity. It is approved for patients with hormone receptor (HR) positive, human epidermal growth factor receptor 2 (HER2) negative, advanced or metastatic breast cancer (MBC) previously treated: in combination with fulvestrant for patients with disease progression following endocrine therapy (MONARCH 2) and as monotherapy for patients with disease progression after endocrine therapy and chemotherapy for MBC (MONARCH 1). The patients in these trials were CDK 4/6 inhibitor-na\u00efve. It has not yet been studied in patients who previously received a CDK 4/6 inhibitor. Methods: We performed a chart review of patients with HR positive, HER2-negative MBC treated at Rush University Medical Center who progressed on palbociclib, either with an aromatase inhibitor (AI) or fulvestrant, and were subsequently treated with abemaciclib with or without fulvestrant. We documented patient demographics, prior treatment, and response to abemaciclib therapy. Results: 21 female patients, mean age 57.8 (+/- 13.2y), were included. Patients had received 1-5 prior lines of endocrine therapy and 0 \u2013 4 prior lines of chemotherapy for MBC. All patients received prior palbociclib: 14 patients with an AI, 6 patients with fulvestrant, and 1 patient received palbociclib with an AI and then with fulvestrant. Of the 21 patients, 17 were treated with abemaciclib monotherapy and 4 received abemaciclib with fulvestrant. SD was seen in 19% of patients (4/21) and 62% had PD (13/21). The CBR was 29% (6/21) and all of these patients received abemaciclib monotherapy. Due to expected toxicities of the drug (diarrhea, neutropenia, and thrombocytopenia), 19% (4/21) of patients discontinued treatment. 4 patients continued abemaciclib monotherapy for greater than 8.3M. 3 patients were on treatment for less than 35 days; 2 stopped due to expected toxicities and one had progression of disease on physical exam. Median treatment duration was 3.1M. Conclusions: This retrospective chart review of 21 patients demonstrates that abemaciclib has limited activity as a single agent in patients previously treated with palbociclib.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/0b0e093a33e9538cbf04075220fc0f15e03c2807.pdf",
        "venue": "Journal of Clinical Oncology",
        "citationCount": 12,
        "score": 2.0,
        "summary": "e12533 Background: Abemaciclib is a selective inhibitor of CDK4 and CDK6 kinase activity. It is approved for patients with hormone receptor (HR) positive, human epidermal growth factor receptor 2 (HER2) negative, advanced or metastatic breast cancer (MBC) previously treated: in combination with fulvestrant for patients with disease progression following endocrine therapy (MONARCH 2) and as monotherapy for patients with disease progression after endocrine therapy and chemotherapy for MBC (MONARCH 1). The patients in these trials were CDK 4/6 inhibitor-na\u00efve. It has not yet been studied in patients who previously received a CDK 4/6 inhibitor. Methods: We performed a chart review of patients with HR positive, HER2-negative MBC treated at Rush University Medical Center who progressed on palbociclib, either with an aromatase inhibitor (AI) or fulvestrant, and were subsequently treated with abemaciclib with or without fulvestrant. We documented patient demographics, prior treatment, and response to abemaciclib therapy. Results: 21 female patients, mean age 57.8 (+/- 13.2y), were included. Patients had received 1-5 prior lines of endocrine therapy and 0 \u2013 4 prior lines of chemotherapy for MBC. All patients received prior palbociclib: 14 patients with an AI, 6 patients with fulvestrant, and 1 patient received palbociclib with an AI and then with fulvestrant. Of the 21 patients, 17 were treated with abemaciclib monotherapy and 4 received abemaciclib with fulvestrant. SD was seen in 19% of patients (4/21) and 62% had PD (13/21). The CBR was 29% (6/21) and all of these patients received abemaciclib monotherapy. Due to expected toxicities of the drug (diarrhea, neutropenia, and thrombocytopenia), 19% (4/21) of patients discontinued treatment. 4 patients continued abemaciclib monotherapy for greater than 8.3M. 3 patients were on treatment for less than 35 days; 2 stopped due to expected toxicities and one had progression of disease on physical exam. Median treatment duration was 3.1M. Conclusions: This retrospective chart review of 21 patients demonstrates that abemaciclib has limited activity as a single agent in patients previously treated with palbociclib.",
        "keywords": []
    },
    "d32546802540410baaa90891671fc39bd4641648.pdf": {
        "title": "A comparative study of burnout syndrome among health professionals in a Nigerian teaching hospital 80",
        "authors": [
            "Olley"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/d32546802540410baaa90891671fc39bd4641648.pdf",
        "venue": "",
        "citationCount": 15,
        "score": 1.875,
        "summary": "",
        "keywords": []
    },
    "5541f38af192fb2df8f985b4f3dd0f3571aae20c.pdf": {
        "title": "Enhancing cardiovascular artificial intelligence (AI) research in the Netherlands: CVON-AI consortium",
        "authors": [
            "J. Benjamins",
            "K. V. Leeuwen",
            "L. Hofstra",
            "M. Rienstra",
            "Y. Appelman",
            "W. Nijhof",
            "B. Verlaat",
            "I. Everts",
            "H. D. Ruijter",
            "Ivana I\u0161gum",
            "T. Leiner",
            "R. Vliegenthart",
            "F. Asselbergs",
            "L. Ju\u00e1rez-Orozco",
            "P. Harst"
        ],
        "published_date": "2019",
        "abstract": "BackgroundMachine learning (ML) allows the exploration and progressive improvement of very complex high-dimensional data patterns that can be utilised to optimise specific classification and prediction tasks, outperforming traditional statistical approaches. An enormous acceleration of ready-to-use tools and artificial intelligence (AI) applications, shaped by the emergence, refinement, and application of powerful ML algorithms in several areas of knowledge, is ongoing. Although such progress has begun to permeate the medical sciences and clinical medicine, implementation in cardiovascular medicine and research is still in its infancy.ObjectivesTo lay out the theoretical framework, purpose, and structure of a\u00a0novel AI consortium.MethodsWe have established a\u00a0new Dutch research consortium, the CVON-AI, supported by the Netherlands Heart Foundation, to catalyse and facilitate the development and utilisation of AI solutions for existing and emerging cardiovascular research initiatives and to raise AI awareness in the cardiovascular research community. CVON-AI will connect to previously established CVON consortia and apply a\u00a0cloud-based AI platform to supplement their planned traditional data-analysis approach.ResultsA\u00a0pilot experiment on the CVON-AI cloud was conducted using cardiac magnetic resonance data. It demonstrated the feasibility of the platform and documented excellent correlation between AI-generated ventricular function estimates as compared to expert manual annotations. The resulting AI solution was then integrated in a\u00a0web application.ConclusionCVON-AI is a\u00a0new consortium meant to facilitate the implementation and raise awareness of AI in cardiovascular research in the Netherlands. CVON-AI will create an accessible cloud-based platform for cardiovascular researchers, demonstrate the clinical applicability of AI, optimise the analytical methodology of other ongoing CVON consortia, and promote AI awareness through education and training.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/5541f38af192fb2df8f985b4f3dd0f3571aae20c.pdf",
        "venue": "Netherlands Heart Journal",
        "citationCount": 10,
        "score": 1.6666666666666665,
        "summary": "BackgroundMachine learning (ML) allows the exploration and progressive improvement of very complex high-dimensional data patterns that can be utilised to optimise specific classification and prediction tasks, outperforming traditional statistical approaches. An enormous acceleration of ready-to-use tools and artificial intelligence (AI) applications, shaped by the emergence, refinement, and application of powerful ML algorithms in several areas of knowledge, is ongoing. Although such progress has begun to permeate the medical sciences and clinical medicine, implementation in cardiovascular medicine and research is still in its infancy.ObjectivesTo lay out the theoretical framework, purpose, and structure of a\u00a0novel AI consortium.MethodsWe have established a\u00a0new Dutch research consortium, the CVON-AI, supported by the Netherlands Heart Foundation, to catalyse and facilitate the development and utilisation of AI solutions for existing and emerging cardiovascular research initiatives and to raise AI awareness in the cardiovascular research community. CVON-AI will connect to previously established CVON consortia and apply a\u00a0cloud-based AI platform to supplement their planned traditional data-analysis approach.ResultsA\u00a0pilot experiment on the CVON-AI cloud was conducted using cardiac magnetic resonance data. It demonstrated the feasibility of the platform and documented excellent correlation between AI-generated ventricular function estimates as compared to expert manual annotations. The resulting AI solution was then integrated in a\u00a0web application.ConclusionCVON-AI is a\u00a0new consortium meant to facilitate the implementation and raise awareness of AI in cardiovascular research in the Netherlands. CVON-AI will create an accessible cloud-based platform for cardiovascular researchers, demonstrate the clinical applicability of AI, optimise the analytical methodology of other ongoing CVON consortia, and promote AI awareness through education and training.",
        "keywords": []
    },
    "fb627fa84455c92cc3b4c4dcf9ce69f732196d24.pdf": {
        "title": "Epidemiological survey and analysis on an outbreak of gastroenteritis due to water contamination.",
        "authors": [
            "Zhicong Yang",
            "Xinwei Wu",
            "Tiegang Li",
            "Mei-xia Li",
            "Yi Zhong",
            "Yu-fei Liu",
            "Z. Deng",
            "B. Di",
            "Cong Huang",
            "Hui-ying Liang",
            "Ming Wang"
        ],
        "published_date": "2011",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/fb627fa84455c92cc3b4c4dcf9ce69f732196d24.pdf",
        "venue": "Biomedical and environmental sciences : BES",
        "citationCount": 20,
        "score": 1.4285714285714284,
        "summary": "",
        "keywords": []
    },
    "e3674fafda3e960fdf6eaf3fe4e625078aab0281.pdf": {
        "title": "The 2012 Core Content of Medical Toxicology",
        "authors": [
            "Lewis S. Nelson",
            "Beth A. Baker",
            "K. Osterhoudt",
            "C. Snook",
            "Julia N. Keehbauch",
            "The Medical Toxicology Core Content Task Force for the Subboard",
            "for the American Board of Emergency Medicine"
        ],
        "published_date": "2012",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/e3674fafda3e960fdf6eaf3fe4e625078aab0281.pdf",
        "venue": "Journal of Medical Toxicology",
        "citationCount": 18,
        "score": 1.3846153846153846,
        "summary": "",
        "keywords": []
    },
    "becef0b31b43510c1d039bb0c7461abf04ce7cff.pdf": {
        "title": "Radiologists\u2019 Recommendations for Additional Imaging on Inpatient CT Studies: Do Referring Physicians Follow Them?",
        "authors": [
            "O. Hanley",
            "A. Lotfi",
            "Tiara Sanborn",
            "J. Friderici",
            "J. Fitzgerald",
            "P. Manikantan",
            "Linda Canty",
            "M. Stefan"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/becef0b31b43510c1d039bb0c7461abf04ce7cff.pdf",
        "venue": "Southern medical journal (Birmingham, Ala. Print)",
        "citationCount": 11,
        "score": 1.375,
        "summary": "",
        "keywords": []
    },
    "84c7c05ac44ecc4fce04d62b49d9ada2cdcc7e32.pdf": {
        "title": "Changes in Food Choices of Participants in the Special Diabetes Program for Indians\u2013Diabetes Prevention Demonstration Project, 2006\u20132010",
        "authors": [
            "N. Teufel-Shone",
            "Luohua Jiang",
            "J. Beals",
            "W. Henderson",
            "K. Acton",
            "Y. Roubideaux",
            "S. Manson"
        ],
        "published_date": "2015",
        "abstract": "Introduction American Indians/Alaska Natives (AI/ANs) have a disproportionately high rate of type 2 diabetes. Changing food choices plays a key role in preventing diabetes. This study documented changes in the food choices of AI/ANs with diagnosed prediabetes who participated in a diabetes prevention program. Methods The Special Diabetes Program for Indians\u2013Diabetes Prevention Demonstration Project implemented the evidence-based Diabetes Prevention Program (DPP) lifestyle intervention in 36 health care programs nationwide, engaging 80 AI/AN communities. At baseline, at 30 days post-curriculum, and at the first annual assessment, participants completed a sociodemographic survey and 27-item food frequency questionnaire and underwent a medical examination assessing fasting blood glucose (FBG), blood pressure, body mass index (BMI), low-density lipoprotein [LDL], high-density lipoprotein [HDL], and triglycerides. Multiple linear regressions were used to assess the relationship between temporal changes in food choice and other diabetes risk factors. Results From January 2006 to July 2010, baseline, post-curriculum, and first annual assessments were completed by 3,135 (100%), 2,046 (65%), and 1,480 (47%) participants, respectively. An increase in healthy food choices was associated initially with reduced bodyweight, BMI, FBG, and LDL and increased physical activity. At first annual assessment, the associations persisted between healthy food choices and bodyweight, BMI, and physical activity. Conclusion AI/AN adults from various tribal and urban communities participating in this preventive intervention made sustained changes in food choices and had reductions in diabetes risk factors. The outcomes demonstrate the feasibility and effectiveness of translating the DPP lifestyle intervention to community-based settings.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/84c7c05ac44ecc4fce04d62b49d9ada2cdcc7e32.pdf",
        "venue": "Preventing Chronic Disease",
        "citationCount": 13,
        "score": 1.3,
        "summary": "Introduction American Indians/Alaska Natives (AI/ANs) have a disproportionately high rate of type 2 diabetes. Changing food choices plays a key role in preventing diabetes. This study documented changes in the food choices of AI/ANs with diagnosed prediabetes who participated in a diabetes prevention program. Methods The Special Diabetes Program for Indians\u2013Diabetes Prevention Demonstration Project implemented the evidence-based Diabetes Prevention Program (DPP) lifestyle intervention in 36 health care programs nationwide, engaging 80 AI/AN communities. At baseline, at 30 days post-curriculum, and at the first annual assessment, participants completed a sociodemographic survey and 27-item food frequency questionnaire and underwent a medical examination assessing fasting blood glucose (FBG), blood pressure, body mass index (BMI), low-density lipoprotein [LDL], high-density lipoprotein [HDL], and triglycerides. Multiple linear regressions were used to assess the relationship between temporal changes in food choice and other diabetes risk factors. Results From January 2006 to July 2010, baseline, post-curriculum, and first annual assessments were completed by 3,135 (100%), 2,046 (65%), and 1,480 (47%) participants, respectively. An increase in healthy food choices was associated initially with reduced bodyweight, BMI, FBG, and LDL and increased physical activity. At first annual assessment, the associations persisted between healthy food choices and bodyweight, BMI, and physical activity. Conclusion AI/AN adults from various tribal and urban communities participating in this preventive intervention made sustained changes in food choices and had reductions in diabetes risk factors. The outcomes demonstrate the feasibility and effectiveness of translating the DPP lifestyle intervention to community-based settings.",
        "keywords": []
    },
    "f4ffeba6b7fd577b8266a94a6299920bd022530a.pdf": {
        "title": "Improving acute eye consultations in general practice: a practical approach",
        "authors": [
            "Michelle Ai Ling Teo"
        ],
        "published_date": "2014",
        "abstract": "Abstract There is significant evidence that patients with acute eye symptoms are poorly assessed in primary care. There is a tendency to diagnose viral or bacterial conjunctivitis in any acutely red eye. This has led to delays in treatment and in some cases, permanent loss of sight. The aim of this project was to improve acute eye consultations within the Birchwood Medical Practice. The project focused on the \"red flag\" findings that would identify patients who require referral for same-day ophthalmology assessment. A retrospective baseline audit was carried out on all cases read-coded \"conjunctivitis\" over the period of one year. Initially, only 2.8% of consultations had documented all four findings. By considering the main factors that lead to poor eye assessments, two main areas for improvement were identified. These were education (reinforced with memory aids) and improving the availability of eye examination equipment within each consultation room. An \"eye examination kit\" was developed with the needs of the general practitioner in mind. The practice was re-audited six weeks following the intervention. Consultations where all four red flag findings were documented rose from 2.8% to 50%. This was found to be a statistically significant difference (p < 0.01). Pain was checked 63% of the time, compared to 26% prior to intervention. Visual acuity screening had increased to from 35% to 69%. Photophobia was the most significantly increased metric, from being documented only 6% of the time to now 63% of the time. Documentation of whether the symptoms were unilateral or bilateral had also increased from 88% to 94% of consultations. The initial audit indicated that general practitioners often diagnosed conjunctivitis without screening for symptoms of sight-threatening disease. However, it was clear from the results that the doctors had made a significant change to their approach to acute eye consultations. This shows that doctors are willing to make changes to their behaviour when given the right tools and information. It was felt that a first-hand understanding of the problem and an open discussion regarding the changes required was key to the success of this project. This project has shown that significant improvements can be achieved with practical and inexpensive interventions. Therefore, general practices throughout the UK are encouraged to adopt similar strategies to improve the identification of patients needing same-day Ophthalmology assessment.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/f4ffeba6b7fd577b8266a94a6299920bd022530a.pdf",
        "venue": "BMJ Quality Improvement Reports",
        "citationCount": 14,
        "score": 1.2727272727272727,
        "summary": "Abstract There is significant evidence that patients with acute eye symptoms are poorly assessed in primary care. There is a tendency to diagnose viral or bacterial conjunctivitis in any acutely red eye. This has led to delays in treatment and in some cases, permanent loss of sight. The aim of this project was to improve acute eye consultations within the Birchwood Medical Practice. The project focused on the \"red flag\" findings that would identify patients who require referral for same-day ophthalmology assessment. A retrospective baseline audit was carried out on all cases read-coded \"conjunctivitis\" over the period of one year. Initially, only 2.8% of consultations had documented all four findings. By considering the main factors that lead to poor eye assessments, two main areas for improvement were identified. These were education (reinforced with memory aids) and improving the availability of eye examination equipment within each consultation room. An \"eye examination kit\" was developed with the needs of the general practitioner in mind. The practice was re-audited six weeks following the intervention. Consultations where all four red flag findings were documented rose from 2.8% to 50%. This was found to be a statistically significant difference (p < 0.01). Pain was checked 63% of the time, compared to 26% prior to intervention. Visual acuity screening had increased to from 35% to 69%. Photophobia was the most significantly increased metric, from being documented only 6% of the time to now 63% of the time. Documentation of whether the symptoms were unilateral or bilateral had also increased from 88% to 94% of consultations. The initial audit indicated that general practitioners often diagnosed conjunctivitis without screening for symptoms of sight-threatening disease. However, it was clear from the results that the doctors had made a significant change to their approach to acute eye consultations. This shows that doctors are willing to make changes to their behaviour when given the right tools and information. It was felt that a first-hand understanding of the problem and an open discussion regarding the changes required was key to the success of this project. This project has shown that significant improvements can be achieved with practical and inexpensive interventions. Therefore, general practices throughout the UK are encouraged to adopt similar strategies to improve the identification of patients needing same-day Ophthalmology assessment.",
        "keywords": []
    },
    "858f5cd62d8ee1fdcd13461989797a4f784fef37.pdf": {
        "title": "A Picture is Worth 1,000 Words",
        "authors": [
            "Angela Ai",
            "F. Maloney",
            "Thu-Trang T. Hickman",
            "A. Wilcox",
            "H. Ramelson",
            "A. Wright"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/858f5cd62d8ee1fdcd13461989797a4f784fef37.pdf",
        "venue": "Applied Clinical Informatics",
        "citationCount": 10,
        "score": 1.25,
        "summary": "The provided text is an **editorial board list** for the journal \"ACI - Applied Clinical Informatics,\" not a technical or research paper. It details the journal's leadership, including the Editor-in-Chief, Managing Editor, Senior Editors, and various other editorial roles, along with their affiliations and some biographical information for key individuals.\n\nAs such, it does not present a specific research problem, technical approach, experimental validation, or technical contributions that can be analyzed in the context of a literature review. The request to analyze a \"TECHNICAL/RESEARCH paper\" cannot be fulfilled with this content.\n\nTherefore, the following points are not applicable to the provided text:\n\n1.  **Research Problem & Motivation**: Not applicable. The document itself does not address a technical problem. It describes the scope of the journal \"Applied Clinical Informatics\" as being \"devoted to original research and commentary on the use of computer automation in the day-to-day practice of medicine,\" but this refers to the journal's focus, not the content of this specific document.\n2.  **Related Work & Positioning**: Not applicable.\n3.  **Technical Approach & Innovation**: Not applicable.\n4.  **Key Technical Contributions**: Not applicable.\n5.  **Experimental Validation**: Not applicable.\n6.  **Limitations & Scope**: The scope of this document is to list the editorial board members of the ACI journal. It has no technical limitations as it is an administrative list.\n7.  **Technical Significance**: Not applicable. This document does not advance the technical state-of-the-art.\n\nTo perform the requested analysis, a full research paper would need to be provided.",
        "keywords": [
            "ACI - Applied Clinical Informatics",
            "Clinical Informatics",
            "Computer automation in medicine",
            "Original research",
            "Day-to-day practice of medicine",
            "Editorial board list",
            "Journal leadership",
            "Editor-in-Chief",
            "Managing Editor",
            "Senior Editors",
            "Editorial roles",
            "Journal scope"
        ],
        "paper_type": "the provided \"abstract\" and \"introduction\" sections do not contain the typical content of a research paper's abstract or introduction. instead, they list the editorial board members of the journal \"applied clinical informatics,\" along with their affiliations and biographical information for the editor-in-chief. this content is characteristic of a journal's front matter, specifically the masthead or an editorial note.\n\nthe title of the piece is \"a picture is worth 1,000 words,\" and the author is the editor-in-chief, christoph ulrich lehmann. editorials written by the editor-in-chief often serve to express a viewpoint, discuss current issues in the field, propose future directions, or set the tone for the journal.\n\nwhile the provided text itself does not contain keywords like \"argue,\" \"position,\" or \"vision,\" the context strongly suggests that the full paper (of which this is the introductory material) is an editorial. editorials, by their nature, frequently function as **position** papers, arguing for a particular viewpoint or direction within the field. the thematic title \"a picture is worth 1,000 words\" further supports the idea that the piece will convey a specific message or argument related to communication or visualization in clinical informatics.\n\ntherefore, inferring from the context (editorial board listing, editor-in-chief as author, thematic title), the most appropriate classification is **position**."
    },
    "d4002164b92926c3ff409102e010147266bb6032.pdf": {
        "title": "[Relationship between endometriosis fertility index and pregnancies after laparoscopic surgery in endometriosis-associated infertility].",
        "authors": [
            "Dai-min Wei",
            "Qi Yu",
            "A. Sun",
            "Q. Tian",
            "Rong Chen",
            "Chengyan Deng",
            "Zhengyi Sun",
            "J. Zhen",
            "F. He"
        ],
        "published_date": "2011",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/d4002164b92926c3ff409102e010147266bb6032.pdf",
        "venue": "Zhonghua fu chan ke za zhi",
        "citationCount": 15,
        "score": 1.0714285714285714,
        "summary": "",
        "keywords": []
    },
    "7c38bdd888c1ff4e84125ccd54a0b515c02d585b.pdf": {
        "title": "Application of Minimum Effective Cuff Inflating Volume for Laryngeal Mask Airway and its Impact on Postoperative Pharyngeal Complications",
        "authors": [
            "Bing-bing Li",
            "Jie Yan",
            "Hong-Gang Zhou",
            "Jing Hao",
            "Ai-Jia Liu",
            "Zheng-liang Ma"
        ],
        "published_date": "2015",
        "abstract": "Background:High intracuff pressure can cause severe pharyngeal complications including sore throat or hoarseness after laryngeal mask airway (LMA) removal postoperatively. Though the application of minimum effective cuff inflating volume is suggested to maintain airway sealing and adequacy of ventilation for patients receiving general anesthesia with LMA at lower level of the intracuff pressure, it is currently not a standard care in most of the anesthetic departments. In this study, the minimum effective cuff inflating volume was determined for classic LMA Well Lead\u2122 (Well Lead Medical Co., Ltd., China) and its impact on postoperative pharyngeal complications was also explored. Methods:Patients with American Society of Anesthesiologists physical status (I\u2013III) undergoing the short-duration urological surgery were recruited in this trial. First, the minimum effective cuff inflating volume was determined for size 4 or 5 LMA Well LeadTM in the study 1. Immediately following placement and confirmation of ideal LMA position, the cuff was inflated with 5, 7, 10 ml of air and up to 30 ml at 5 ml increment. The intracuff pressure, oropharyngeal leak pressure (OLP), and inspiratory peak airway pressure under positive pressure ventilation at the corresponding cuff volume as indicated above were recorded. Second, the enrolled patients were randomly allocated into minimum effective cuff inflating volume group (MC) and routine care (RC) group in the study 2. The minimum effective cuff inflating volume was applied and maintained in MC group, whereas the cuff volume was inflated with half of the maximum cuff inflating volume recommended by manufacturer in RC group throughout the surgical procedure and stay in postanesthesia care unit prior to LMA removal. The incidence of pharyngeal complications at 0, 2, 24, and 48 h after removal of LMA and other intra-operative adverse events were also documented. Results:The intracuff pressure varied with the cuff inflating volume in a positive linear correlation manner (Y = 11.68X \u2212 42.1, r2 = 0.9191) under the range of 5\u201330 ml for size 4 LMA. In similar with size 4 LMA, the data were also showed the linear relationship between the intracuff pressure and the cuff inflating volume (Y = 7.39X \u2212 10.9, r2 = 0.8855) for size 5 LMA. The minimal effective cuff inflating volume for size 4 or 5 LMA was 7\u20139 ml in combination of considering OLP needed to maintain airway sealing during intermittently positive pressure ventilation. The intracuff pressure in MC group was lower compared with RC group (63.0 \u00b1 3.7 vs. 126.4 \u00b1 24.0 cmH2O for size 4 LMA; 55.6 \u00b1 2.4 vs. 138.5\u00b1 26.8 cmH2O for size 5 LMA; P < 0.0001). The incidence of pharyngeal adverse events was lower in MC group versus the RC group at 2, 24 h after LMA removal. Conclusions:The relationship between the cuff inflating volume and the intracuff pressure for size 4 or 5 LMA Well Lead\u2122 is in a linear correlation manner at the range of 5\u201330 ml. The minimal cuff inflating volume is adequate for satisfactory airway sealing and consequently associated with lower incidence of postoperative pharyngeal complications for LMA Well Lead.\u2122",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/7c38bdd888c1ff4e84125ccd54a0b515c02d585b.pdf",
        "venue": "Chinese Medical Journal",
        "citationCount": 10,
        "score": 1.0,
        "summary": "Background:High intracuff pressure can cause severe pharyngeal complications including sore throat or hoarseness after laryngeal mask airway (LMA) removal postoperatively. Though the application of minimum effective cuff inflating volume is suggested to maintain airway sealing and adequacy of ventilation for patients receiving general anesthesia with LMA at lower level of the intracuff pressure, it is currently not a standard care in most of the anesthetic departments. In this study, the minimum effective cuff inflating volume was determined for classic LMA Well Lead\u2122 (Well Lead Medical Co., Ltd., China) and its impact on postoperative pharyngeal complications was also explored. Methods:Patients with American Society of Anesthesiologists physical status (I\u2013III) undergoing the short-duration urological surgery were recruited in this trial. First, the minimum effective cuff inflating volume was determined for size 4 or 5 LMA Well LeadTM in the study 1. Immediately following placement and confirmation of ideal LMA position, the cuff was inflated with 5, 7, 10 ml of air and up to 30 ml at 5 ml increment. The intracuff pressure, oropharyngeal leak pressure (OLP), and inspiratory peak airway pressure under positive pressure ventilation at the corresponding cuff volume as indicated above were recorded. Second, the enrolled patients were randomly allocated into minimum effective cuff inflating volume group (MC) and routine care (RC) group in the study 2. The minimum effective cuff inflating volume was applied and maintained in MC group, whereas the cuff volume was inflated with half of the maximum cuff inflating volume recommended by manufacturer in RC group throughout the surgical procedure and stay in postanesthesia care unit prior to LMA removal. The incidence of pharyngeal complications at 0, 2, 24, and 48 h after removal of LMA and other intra-operative adverse events were also documented. Results:The intracuff pressure varied with the cuff inflating volume in a positive linear correlation manner (Y = 11.68X \u2212 42.1, r2 = 0.9191) under the range of 5\u201330 ml for size 4 LMA. In similar with size 4 LMA, the data were also showed the linear relationship between the intracuff pressure and the cuff inflating volume (Y = 7.39X \u2212 10.9, r2 = 0.8855) for size 5 LMA. The minimal effective cuff inflating volume for size 4 or 5 LMA was 7\u20139 ml in combination of considering OLP needed to maintain airway sealing during intermittently positive pressure ventilation. The intracuff pressure in MC group was lower compared with RC group (63.0 \u00b1 3.7 vs. 126.4 \u00b1 24.0 cmH2O for size 4 LMA; 55.6 \u00b1 2.4 vs. 138.5\u00b1 26.8 cmH2O for size 5 LMA; P < 0.0001). The incidence of pharyngeal adverse events was lower in MC group versus the RC group at 2, 24 h after LMA removal. Conclusions:The relationship between the cuff inflating volume and the intracuff pressure for size 4 or 5 LMA Well Lead\u2122 is in a linear correlation manner at the range of 5\u201330 ml. The minimal cuff inflating volume is adequate for satisfactory airway sealing and consequently associated with lower incidence of postoperative pharyngeal complications for LMA Well Lead.\u2122",
        "keywords": []
    },
    "7df3f31e4f57679346121df58c9c6d7bdc1609fd.pdf": {
        "title": "The vegetative state: A syndrome seeking revision?",
        "authors": [
            "G. Dolce",
            "W. Sannita",
            "for the European Task Force on the Vegetative Stat"
        ],
        "published_date": "2010",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/7df3f31e4f57679346121df58c9c6d7bdc1609fd.pdf",
        "venue": "Brain Injury",
        "citationCount": 14,
        "score": 0.9333333333333333,
        "summary": "",
        "keywords": []
    },
    "de5b8e016527f395d341bcbf4d519dd8cd6c5bb4.pdf": {
        "title": "Intestinal interposition: the prevalence and clinical relevance of non-hepatodiaphragmatic conditions (non-Chilaiditi forms) documented by CT and review of the literature",
        "authors": [
            "F. Bredolo",
            "Andrea Esposito",
            "Elena Casiraghi",
            "Gianpaolo Cornalba",
            "Pietro Biondetti"
        ],
        "published_date": "2011",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/de5b8e016527f395d341bcbf4d519dd8cd6c5bb4.pdf",
        "venue": "La radiologia medica",
        "citationCount": 13,
        "score": 0.9285714285714285,
        "summary": "",
        "keywords": []
    },
    "ce3b465ed43e4e5bd851812138948d06380e3dfc.pdf": {
        "title": "Disease distribution and medical resources during the Beijing 2008 Olympic and Paralympic Games.",
        "authors": [
            "Xue-ya Liang",
            "L. Lan",
            "Wei-na Chen",
            "Ai-ping Zhang",
            "Chao-ying L\u00fc",
            "Y. L\u00fc",
            "J. Dai"
        ],
        "published_date": "2011",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/ce3b465ed43e4e5bd851812138948d06380e3dfc.pdf",
        "venue": "Chinese Medical Journal",
        "citationCount": 12,
        "score": 0.8571428571428571,
        "summary": "",
        "keywords": []
    },
    "489fe30cd7221507126579f37694f5d99277bd91.pdf": {
        "title": "An early history of human breast cancer: West meets East",
        "authors": [
            "S. Yan"
        ],
        "published_date": "2013",
        "abstract": "Cancer has been increasingly recognized as a global issue. This is especially true in countries like China, where cancer incidence has increased likely because of changes in environment and lifestyle. However, cancer is not a modern disease; early cases have been recorded in ancient medical books in the West and in China. Here, we provide a brief history of cancer, focusing on cancer of the breast, and review the etymology of ai, the Chinese character for cancer. Notable findings from both Western and Chinese traditional medicine are presented to give an overview of the most important, early contributors to our evolving understanding of human breast cancer. We also discuss the earliest historical documents to record patients with breast cancer.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/489fe30cd7221507126579f37694f5d99277bd91.pdf",
        "venue": "Chinese journal of cancer",
        "citationCount": 10,
        "score": 0.8333333333333333,
        "summary": "Cancer has been increasingly recognized as a global issue. This is especially true in countries like China, where cancer incidence has increased likely because of changes in environment and lifestyle. However, cancer is not a modern disease; early cases have been recorded in ancient medical books in the West and in China. Here, we provide a brief history of cancer, focusing on cancer of the breast, and review the etymology of ai, the Chinese character for cancer. Notable findings from both Western and Chinese traditional medicine are presented to give an overview of the most important, early contributors to our evolving understanding of human breast cancer. We also discuss the earliest historical documents to record patients with breast cancer.",
        "keywords": []
    },
    "1c2a38413583c6836f11b88edebf65577e1450bd.pdf": {
        "title": "Exploring the role of ChatGPT in patient care (diagnosis and treatment) and medical research: A systematic review",
        "authors": [
            "R. Garg",
            "V. L. Urs",
            "Akshya Anand Agrawal",
            "S. Chaudhary",
            "V. Paliwal",
            "Sujita Kumar Kar"
        ],
        "published_date": "2023",
        "abstract": "Background ChatGPT(Chat Generative Pre-trained Transformer) is an artificial intelligence (AI) based on a natural language processing tool developed by OpenAI (California, USA). This systematic review examines the potential of Chat GPT in diagnosing and treating patients and its contributions to medical research. Methods In order to locate articles on ChatGPT's use in clinical practise and medical research, this systematic review used PRISMA standards and conducted database searches across several sources. Selected records were analysed using ChatGPT, which also produced a summary for each article. The resultant word document was transformed to a PDF and handled using ChatPDF. The review looked at topics pertaining to scholarly publishing, clinical practise, and medical research. Results We reviewed 118 publications. There are difficulties and moral conundrums associated with using ChatGPT in therapeutic settings and medical research. Patient inquiries, note writing, decision-making, trial enrolment, data management, decision support, research support, and patient education are all things that ChatGPT can help with. However, the solutions it provides are frequently inadequate and inconsistent, presenting issues with its originality, privacy, accuracy, bias, and legality. When utilising ChatGPT for academic writings, there are issues with prejudice and plagiarism, and because it lacks human-like characteristics, its authority as an author is called into question. Conclusions ChatGPT has limitations when used in research and healthcare. Even while it aids in patient treatment, concerns regarding accuracy, authorship, and bias arise. Currently, ChatGPT can serve as a \"clinical assistant\" and be a huge assistance with research and scholarly writing.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/1c2a38413583c6836f11b88edebf65577e1450bd.pdf",
        "venue": "medRxiv",
        "citationCount": 138,
        "score": 69.0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the potential applications and inherent challenges of ChatGPT, an AI-based natural language processing tool, in the critical domains of patient care (diagnosis and treatment) and medical research \\cite{garg2023pcm}.\n    *   **Importance and Challenge**: This problem is crucial due to ChatGPT's perceived \"enormous capability\" in assisting with clinical/laboratory diagnosis, research planning/execution, and medical writing \\cite{garg2023pcm}. However, its deployment in medicine is fraught with significant ethical and legal challenges, including concerns about accuracy, bias, originality, privacy, copyright, and the appropriate attribution of AI-generated content \\cite{garg2023pcm}. Understanding these facets systematically is vital for responsible integration.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a systematic review, positioning itself as a comprehensive synthesis of existing literature on ChatGPT's utility and limitations in clinical practice and medical research \\cite{garg2023pcm}.\n    *   **Limitations of Previous Solutions**: The paper implicitly addresses the fragmented nature of early research by consolidating findings from a large number of publications. It references other reviews (e.g., Ruksakulpiwat et al. and Levin et al.) that analyzed smaller subsets of articles, often focusing on specific applications or early publication trends \\cite{garg2023pcm}. This paper aims for a broader, more structured overview, uniquely leveraging AI tools in its own methodology.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The study employs a systematic review methodology, adhering to PRISMA guidelines, to identify and analyze articles on ChatGPT's medical applications \\cite{garg2023pcm}. Databases like PubMed, Scopus, Embase, and Google Scholar were searched for publications up to May 2023 \\cite{garg2023pcm}.\n    *   **Novelty/Difference**: A key innovation is the *meta-use of AI*: ChatGPT was extensively utilized for data analysis and manuscript preparation \\cite{garg2023pcm}. Specifically, ChatGPT generated point-wise summaries for each of the 118 reviewed articles. These summaries were then processed by ChatPDF, which answered a set of 12 predefined questions to extract insights on ChatGPT's roles, ethical issues, effectiveness, and future applications \\cite{garg2023pcm}. This approach demonstrates an AI-augmented method for literature synthesis.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**: The primary technical contribution is the pioneering methodology of using ChatGPT and ChatPDF as integral analytical tools within a systematic review framework to process and synthesize a large volume of literature about ChatGPT itself \\cite{garg2023pcm}. This showcases a novel application of LLMs for meta-analysis.\n    *   **Theoretical Insights or Analysis**: The review provides a structured overview of ChatGPT's potential in areas like patient inquiries, note writing, decision support, trial enrollment, data management, research assistance, and patient education \\cite{garg2023pcm}. Crucially, it also systematically identifies and categorizes its limitations, including issues with accuracy, consistency, originality, privacy, bias, and legality, particularly in academic writing and clinical diagnosis \\cite{garg2023pcm}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: As a systematic review, the \"experiment\" involved the systematic collection and AI-assisted analysis of 118 publications related to ChatGPT in medicine \\cite{garg2023pcm}. The process involved searching, screening, and then using ChatGPT/ChatPDF to summarize and extract information from these articles.\n    *   **Key Performance Metrics and Comparison Results**: The review identified 118 publications, comprising 33 original articles and a majority of commentaries, editorials, and reviews \\cite{garg2023pcm}. The AI-assisted analysis revealed that while ChatGPT can enhance productivity and expedite research workflows (e.g., data organization, manuscript review), its outputs are \"frequently inadequate and inconsistent\" \\cite{garg2023pcm}. It was found to lack the capability for comprehensive diagnoses and human-like qualities, raising concerns about \"hallucinations\" and the need for human validation of its output \\cite{garg2023pcm}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: A notable methodological limitation is that the systematic review did not perform a quality assessment of the included studies \\cite{garg2023pcm}. The reliance on ChatGPT/ChatPDF for summarization and analysis introduces the inherent risks of AI-generated content, such as potential inaccuracies, biases, or \"hallucinations,\" which the authors acknowledge require human checking and validation \\cite{garg2023pcm}.\n    *   **Scope of Applicability**: The review specifically focused on ChatGPT's role in patient care (diagnosis and treatment) and medical research, including medical writing and associated ethical issues, excluding medical education \\cite{garg2023pcm}. The findings reflect the state of literature up to May 2023.\n\n*   **7. Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This paper advances the state-of-the-art by providing one of the earliest and most comprehensive AI-assisted systematic reviews on ChatGPT's role in medicine \\cite{garg2023pcm}. Its innovative methodology of using AI tools to analyze literature *about* AI offers a novel paradigm for future systematic reviews, demonstrating how LLMs can augment research processes.\n    *   **Potential Impact on Future Research**: The findings provide a critical baseline for future research by clearly delineating ChatGPT's current capabilities and significant limitations in medical contexts \\cite{garg2023pcm}. It underscores the urgent need for research into improving AI accuracy, mitigating bias, ensuring data privacy, and establishing clear ethical guidelines for AI authorship and content validation. The paper's conclusion that ChatGPT serves as a \"clinical assistant\" but \"is nowhere near to replace human brain\" sets a realistic direction for future development and responsible integration of AI in healthcare \\cite{garg2023pcm}.",
        "keywords": [
            "ChatGPT in medicine",
            "AI-assisted systematic review",
            "Large Language Models (LLMs)",
            "Patient care",
            "Medical research",
            "Ethical and legal challenges",
            "Accuracy",
            "bias",
            "hallucinations",
            "Human validation",
            "Clinical decision support",
            "Medical writing",
            "Meta-use of AI",
            "AI output limitations"
        ],
        "paper_type": "the paper type is **survey**.\n\n**reasoning:**\n\n1.  **title:** the title explicitly states \"a systematic review,\" which is a direct indicator of a survey paper.\n2.  **abstract - background:** \"this systematic review examines the potential of chat gpt...\"\n3.  **abstract - methods:** \"this systematic review used prisma standards and conducted database searches across several sources. selected records were analysed... we reviewed 118 publications.\" this clearly describes a process of reviewing existing literature.\n4.  **abstract - results & conclusions:** these sections summarize the findings from the *review* of the 118 publications, discussing current capabilities, limitations, and ethical concerns identified in the literature.\n5.  **introduction:** discusses the \"possible uses of chatgpt in medicine is currently under intense investigation,\" setting the stage for a review of this investigation.\n\nall these points align perfectly with the definition of a **survey** paper, which \"reviews existing literature comprehensively.\""
    },
    "56df62407ba0878d34493f12a6ece8634ee0db9e.pdf": {
        "title": "A trustworthy AI reality-check: the lack of transparency of artificial intelligence products in healthcare",
        "authors": [
            "Jana Fehr",
            "Brian Citro",
            "Rohit Malpani",
            "Christoph Lippert",
            "V. Madai"
        ],
        "published_date": "2024",
        "abstract": "Trustworthy medical AI requires transparency about the development and testing of underlying algorithms to identify biases and communicate potential risks of harm. Abundant guidance exists on how to achieve transparency for medical AI products, but it is unclear whether publicly available information adequately informs about their risks. To assess this, we retrieved public documentation on the 14 available CE-certified AI-based radiology products of the II b risk category in the EU from vendor websites, scientific publications, and the European EUDAMED database. Using a self-designed survey, we reported on their development, validation, ethical considerations, and deployment caveats, according to trustworthy AI guidelines. We scored each question with either 0, 0.5, or 1, to rate if the required information was \u201cunavailable\u201d, \u201cpartially available,\u201d or \u201cfully available.\u201d The transparency of each product was calculated relative to all 55 questions. Transparency scores ranged from 6.4% to 60.9%, with a median of 29.1%. Major transparency gaps included missing documentation on training data, ethical considerations, and limitations for deployment. Ethical aspects like consent, safety monitoring, and GDPR-compliance were rarely documented. Furthermore, deployment caveats for different demographics and medical settings were scarce. In conclusion, public documentation of authorized medical AI products in Europe lacks sufficient public transparency to inform about safety and risks. We call on lawmakers and regulators to establish legally mandated requirements for public and substantive transparency to fulfill the promise of trustworthy AI for health.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/56df62407ba0878d34493f12a6ece8634ee0db9e.pdf",
        "venue": "Frontiers Digit. Health",
        "citationCount": 51,
        "score": 51.0,
        "summary": "Here's a focused summary of the provided guidance note for a literature review, emphasizing technical innovations and empirical validation where applicable:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of collecting and disaggregating data in a manner consistent with human rights principles, particularly the \"leave no one behind\" pledge of the 2030 Agenda for Sustainable Development \\cite{fehr2024nzb}. This involves moving beyond national averages to identify and measure inequalities among specific population groups, while mitigating the risks associated with collecting sensitive personal data \\cite{fehr2024nzb}.\n    *   **Importance & Challenge**: The problem is crucial because traditional data collection often masks underlying disparities, hindering efforts to address inequalities and fulfill human rights obligations. The challenge lies in developing data collection methodologies and systems that are inclusive, participatory, transparent, privacy-preserving, and accountable, especially for marginalized and vulnerable populations who are often \"hard-to-count\" \\cite{fehr2024nzb}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This guidance note draws from internationally agreed principles for statistics and aligns with the call for a data revolution for sustainable development that upholds human rights \\cite{fehr2024nzb}. It positions itself as a framework to integrate human rights norms and principles into existing data collection and statistical practices.\n    *   **Limitations of Previous Solutions**: The paper implicitly highlights the limitations of traditional data collection that focuses on national averages, which can obscure the realities of disadvantaged or marginalized groups \\cite{fehr2024nzb}. It also points to the potential for misuse of data and the need for robust safeguards to prevent discrimination or harm, suggesting that previous approaches may have lacked sufficient human rights considerations \\cite{fehr2024nzb}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a Human Rights-Based Approach to Data (HRBAD) structured around six key principles: Participation, Data Disaggregation, Self-identification, Transparency, Privacy, and Accountability \\cite{fehr2024nzb}. While not a computational algorithm, it provides a methodological framework for data collection.\n    *   **Novelty/Difference**: The innovation lies in systematically applying human rights principles to the entire data lifecycle, particularly in guiding *how* data should be collected and disaggregated. Key methodological recommendations include:\n        *   **Participatory approaches**: Involving marginalized groups in all stages of data collection, from planning to analysis and dissemination, and ensuring culturally appropriate data sharing \\cite{fehr2024nzb}.\n        *   **Advanced disaggregation strategies**: Moving beyond basic demographic disaggregation to include characteristics identified in international human rights law (e.g., sexual orientation, gender identity, disability, migration status) and enabling analysis of *multiple and intersecting disparities* \\cite{fehr2024nzb}.\n        *   **Specialized sampling methodologies**: Recommending techniques like oversampling, targeted sampling, random route sampling, respondent-driven sampling, and individual (intra-household) questionnaire modules to ensure representation of hard-to-reach populations \\cite{fehr2024nzb}.\n        *   **Self-identification principle**: Emphasizing voluntary disclosure of personal characteristics, providing non-response options, and adhering strictly to the \"do no harm\" principle, supported by appropriate interviewer training \\cite{fehr2024nzb}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**: While no novel computational algorithms are presented, the paper's primary technical contribution is a comprehensive set of *methodological guidelines* for data collection and disaggregation that are explicitly human rights-compliant \\cite{fehr2024nzb}. This includes specific recommendations for:\n        *   *Sampling techniques* to ensure representation of vulnerable groups (e.g., oversampling, respondent-driven sampling) \\cite{fehr2024nzb}.\n        *   *Data collection practices* that prioritize self-identification and voluntary participation \\cite{fehr2024nzb}.\n    *   **System design or architectural innovations**: The paper highlights the need for effective *data management systems* that can incorporate new data items, allow for varied cross-tabulation, and support detailed data analysis to facilitate disaggregation \\cite{fehr2024nzb}. It implicitly calls for robust data infrastructure capable of handling complex, disaggregated datasets ethically.\n    *   **Theoretical insights or analysis**: It provides a foundational conceptual framework for an HRBAD, linking statistical practices directly to international human rights law and principles, thereby offering a normative basis for technical data work \\cite{fehr2024nzb}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: This document is a guidance note and does not report on formal experiments or empirical studies.\n    *   **Key Performance Metrics and Comparison Results**: No performance metrics or comparative results are presented, as the paper outlines an approach and principles rather than a technical solution that has undergone empirical testing \\cite{fehr2024nzb}. The guidance is based on expert consensus and internationally agreed principles.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: The guidance assumes the availability of resources for acquiring and maintaining sophisticated data collection instruments and data management systems necessary for detailed disaggregation \\cite{fehr2024nzb}. It also assumes the capacity for training data collectors in human rights-sensitive approaches and for establishing partnerships with CSOs \\cite{fehr2024nzb}. It acknowledges that direct engagement with certain groups may be difficult or risky due to legal status or social stigma \\cite{fehr2024nzb}.\n    *   **Scope of Applicability**: The guidance is intended for policymakers, statisticians, data specialists (in government agencies or civil society organizations), development practitioners, and human rights advocates involved in the measurement and implementation of the 2030 Agenda for Sustainable Development \\cite{fehr2024nzb}. Its focus is specifically on data collection and disaggregation.\n\n7.  **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: While not introducing new computational technologies, this paper significantly advances the *methodological and ethical state-of-the-art* for data collection in sustainable development. It provides a structured, principled approach that ensures data collection is inclusive, equitable, and minimizes harm, moving beyond purely technical efficiency to incorporate human rights imperatives \\cite{fehr2024nzb}.\n    *   **Potential Impact on Future Research**: It can influence future research and development in statistical methodologies, particularly in sampling and survey design for marginalized populations. It highlights the need for data management systems that are not only efficient but also capable of handling sensitive, disaggregated data securely and ethically. It encourages interdisciplinary research at the intersection of statistics, human rights, and data science to operationalize these principles effectively \\cite{fehr2024nzb}.",
        "keywords": [
            "Human Rights-Based Approach to Data (HRBAD)",
            "Data disaggregation",
            "Leave no one behind",
            "Marginalized and vulnerable populations",
            "Participatory approaches",
            "Specialized sampling methodologies",
            "Self-identification principle",
            "Privacy and Accountability",
            "Multiple and intersecting disparities",
            "Methodological guidelines",
            "Ethical state-of-the-art",
            "2030 Agenda for Sustainable Development",
            "Data management systems"
        ],
        "paper_type": "the provided abstract and introduction are for a \"guidance note\" from ohchr on a \"human rights-based approach to data (hrbad)\". they explicitly state that the note \"aims to provide general guidance and elements of a common understanding\" on this approach, focusing on data collection and disaggregation in the context of the 2030 agenda for sustainable development.\n\nlet's evaluate this content against the classification criteria:\n\n*   **survey:** it does not review existing literature comprehensively or discuss classification schemes. it presents a framework.\n*   **technical:** it does not propose new methods, algorithms, or systems. it's about principles and approaches.\n*   **theoretical:** it does not involve mathematical analysis, proofs, or formal models.\n*   **empirical:** it does not present data-driven studies, experiments, or statistical analysis.\n*   **case_study:** it is not a detailed analysis of a specific application or real-world scenario; it provides general guidance.\n*   **position:** this fits well. the document argues for a specific viewpoint (the hrbad approach), outlines a proposed direction for data collection and disaggregation, and provides guidance based on this position. it discusses current problems (risks associated with data operations) and proposes a direction (hrbad has much to offer). the term \"guidance note\" itself implies a recommended stance or approach.\n*   **short:** while it might be a concise document, its primary purpose is to establish a position and provide guidance, not merely to be a brief communication of preliminary results.\n\ntherefore, based on the provided content, the paper (or document, in this case) is best classified as a **position** paper."
    },
    "561df8e070393a981b7c4196e1c94b92876d4e5b.pdf": {
        "title": "A Declarative System for Optimizing AI Workloads",
        "authors": [
            "Chunwei Liu",
            "Matthew Russo",
            "Michael J. Cafarella",
            "Lei Cao",
            "Peter Baille Chen",
            "Zui Chen",
            "Michael J. Franklin",
            "T. Kraska",
            "Samuel Madden",
            "Gerardo Vitagliano"
        ],
        "published_date": "2024",
        "abstract": "A long-standing goal of data management systems has been to build systems which can compute quantitative insights over large corpora of unstructured data in a cost-effective manner. Until recently, it was difficult and expensive to extract facts from company documents, data from scientific papers, or metrics from image and video corpora. Today's models can accomplish these tasks with high accuracy. However, a programmer who wants to answer a substantive AI-powered query must orchestrate large numbers of models, prompts, and data operations. For even a single query, the programmer has to make a vast number of decisions such as the choice of model, the right inference method, the most cost-effective inference hardware, the ideal prompt design, and so on. The optimal set of decisions can change as the query changes and as the rapidly-evolving technical landscape shifts. In this paper we present Palimpzest, a system that enables anyone to process AI-powered analytical queries simply by defining them in a declarative language. The system uses its cost optimization framework to implement the query plan with the best trade-offs between runtime, financial cost, and output data quality. We describe the workload of AI-powered analytics tasks, the optimization methods that Palimpzest uses, and the prototype system itself. We evaluate Palimpzest on tasks in Legal Discovery, Real Estate Search, and Medical Schema Matching. We show that even our simple prototype offers a range of appealing plans, including one that is 3.3x faster and 2.9x cheaper than the baseline method, while also offering better data quality. With parallelism enabled, Palimpzest can produce plans with up to a 90.3x speedup at 9.1x lower cost relative to a single-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the baseline. These require no additional work by the user.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/561df8e070393a981b7c4196e1c94b92876d4e5b.pdf",
        "venue": "arXiv.org",
        "citationCount": 38,
        "score": 38.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the challenge of efficiently and cost-effectively orchestrating complex AI models, prompts, and data operations to answer \"AI-powered analytical queries\" over large unstructured datasets \\cite{liu2024qwh}.\n    *   This problem is important because modern AI applications often involve complex systems combining data processing, model ensembles, and multi-step reasoning, leading to rapidly escalating runtime, cost, and complexity, especially with large datasets \\cite{liu2024qwh}.\n    *   It's challenging due to the vast decision space for AI engineers (model choice, inference method, hardware, prompt design, parallelism, external system integration), the profound performance gap between traditional data processing and AI components (e.g., LLMs are orders of magnitude slower and more expensive), and the constantly evolving AI landscape which quickly renders optimization choices obsolete \\cite{liu2024qwh}.\n\n*   **Related Work & Positioning**\n    *   The work positions itself against naive programming and existing frameworks that require engineers to manually make a vast number of optimization decisions for AI workloads \\cite{liu2024qwh}.\n    *   It draws an analogy to the development of relational database query optimizers in the 1970s, aiming to bring similar declarative optimization benefits to AI-powered analytics \\cite{liu2024qwh}.\n    *   Limitations of previous solutions (implicitly, manual approaches) include high engineering effort, error-proneness, and inability to adapt to the rapidly changing AI ecosystem \\cite{liu2024qwh}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is PALIMPZEST, a declarative system that allows users to define AI-powered analytical queries in a high-level language \\cite{liu2024qwh}.\n    *   PALIMPZEST employs a cost optimization framework to automatically generate and select an execution plan that best balances runtime, financial cost, and output data quality based on user-specified preferences \\cite{liu2024qwh}.\n    *   A key innovation is the \"relational convert operator,\" which transforms an object from one user-defined schema to another, often implemented using foundation models. This operator allows many AI tasks to be expressed in a relational and optimizable style, forming the intellectual difference from previous database-style systems \\cite{liu2024qwh}.\n    *   The system uses a Python library that implements a thin abstraction over an underlying relational algebra, enabling the optimizer to exploit many optimizations not available with low-level prompting and coding \\cite{liu2024qwh}.\n\n*   **Key Technical Contributions**\n    *   Introduction of Semantic Analytics Applications (SAPPs), a new class of data-intensive AI workloads that interleave traditional data processing with AI-like semantic reasoning, are data-intensive, decomposable into operation trees, and yield varying quality \\cite{liu2024qwh}.\n    *   The PALIMPZEST architecture, which compiles declarative programs, generates logical and physical plans, profiles sample plans, estimates costs, and selects an optimal plan based on user preferences \\cite{liu2024qwh}.\n    *   A set of physical and logical optimizations implemented in the prototype, designed to improve efficiency across AI and conventional data processing elements \\cite{liu2024qwh}.\n    *   The novel \"relational convert operator\" which enables relational optimization for AI tasks \\cite{liu2024qwh}.\n\n*   **Experimental Validation**\n    *   Experiments were conducted on three distinct SAPP workloads: Legal Discovery, Real Estate Search, and Medical Schema Matching \\cite{liu2024qwh}.\n    *   Key performance metrics included runtime, financial cost, and output data quality (F1-score) \\cite{liu2024qwh}.\n    *   Results showed that PALIMPZEST offers a range of appealing plans:\n        *   One plan was 3.3x faster and 2.9x cheaper than the baseline method, while also offering better data quality \\cite{liu2024qwh}.\n        *   Another plan achieved 4.7x faster execution and 9.1x cheaper cost, with a trade-off of 14.3% lower quality than its baseline \\cite{liu2024qwh}.\n        *   With parallelism enabled, PALIMPZEST produced plans with up to a 90.3x speedup and 9.1x lower cost relative to a single-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the baseline \\cite{liu2024qwh}. These benefits required no additional user effort \\cite{liu2024qwh}.\n\n*   **Limitations & Scope**\n    *   The paper describes PALIMPZEST as a \"simple prototype\" and an \"exciting prototype system,\" indicating that while promising, it is still in early development and has room for future optimizations and features \\cite{liu2024qwh}.\n    *   The scope of applicability is focused on Semantic Analytics Applications (SAPPs), which encompass large-scale information extraction, data integration, discovery from scientific papers, image understanding, and multimodal analytics \\cite{liu2024qwh}.\n\n*   **Technical Significance**\n    *   PALIMPZEST significantly advances the technical state-of-the-art by automating the complex optimization of AI workloads, abstracting away low-level decisions from engineers \\cite{liu2024qwh}.\n    *   It enables engineers to write AI programs declaratively, similar to how RDBMS allowed users to write database queries, leading to faster development and more correct, optimized implementations \\cite{liu2024qwh}.\n    *   The system's extensible design allows for easy integration of new optimizations, ensuring its relevance in the rapidly evolving AI landscape \\cite{liu2024qwh}.\n    *   Its potential impact on future research includes fostering new declarative programming paradigms for AI, driving further innovation in AI system optimization, and making AI-powered analytics more accessible and efficient for a broader range of applications \\cite{liu2024qwh}.",
        "keywords": [
            "PALIMPZEST",
            "AI-powered analytical queries",
            "declarative AI system",
            "cost optimization framework",
            "relational convert operator",
            "Semantic Analytics Applications (SAPPs)",
            "automated AI workload optimization",
            "large unstructured datasets",
            "execution plan generation",
            "runtime",
            "cost",
            "data quality optimization",
            "foundation models",
            "speedup and cost reduction"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"in this paper we present palimpzest, a system that enables anyone to process ai-powered analytical queries simply by defining them in a declarative language.\"\n*   it further describes the system's \"cost optimization framework\" and \"optimization methods.\"\n*   the introduction sets up a technical problem (complexity and cost of orchestrating ai models) and then introduces the system as a solution.\n*   while the paper includes an \"evaluation\" with \"findings\" (e.g., \"3.3x faster and 2.9x cheaper\"), these empirical results are presented to demonstrate the effectiveness of the *new system* and its *methods*, rather than being the sole focus of a data-driven study on existing phenomena.\n\nthis aligns perfectly with the criteria for a **technical** paper: \"presents new methods, algorithms, or systems\" and discusses a \"technical problem, proposed solution.\"\n\n**classification: technical**"
    },
    "c4034bb6f3e29ab0adcb3423d5acfbbf28623f94.pdf": {
        "title": "Exploring AI-driven approaches for unstructured document analysis and future horizons",
        "authors": [
            "Supriya V. Mahadevkar",
            "S. Patil",
            "K. Kotecha",
            "Lim Way Soong",
            "Tanupriya Choudhury"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c4034bb6f3e29ab0adcb3423d5acfbbf28623f94.pdf",
        "venue": "Journal of Big Data",
        "citationCount": 29,
        "score": 29.0,
        "summary": "Here's a focused summary of the paper \"Discriminative Multinominal Naive Bayes for Text Classi\ufb01cation\" \\cite{mahadevkar2024xn8} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multinominal Naive Bayes (MNB), while computationally efficient and simple for text classification, suffers from poor effectiveness compared to discriminative classifiers. This is primarily due to its objective function mismatch: MNB maximizes likelihood rather than the classification objective (e.g., conditional likelihood or accuracy) \\cite{mahadevkar2024xn8}.\n    *   **Importance and Challenge**: Text classification applications, such as online content recommenders, demand algorithms that are simultaneously effective (even with small training data), highly efficient (linear time, minimal cost for fast response), and capable of online learning. Existing discriminative classifiers are effective but lack MNB's computational efficiency, while MNB lacks the necessary effectiveness. The challenge is to develop an algorithm that combines the best of both worlds.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper positions its work against traditional MNB, which is efficient but less accurate, and state-of-the-art discriminative classifiers like Support Vector Machines (SVM) and Logistic Regression (LR), which are effective but computationally more expensive.\n    *   **Limitations of Previous Solutions**:\n        *   **MNB**: Its generative parameter learning (Frequency Estimate, FE) maximizes log likelihood, which can be dominated by the joint distribution of words, especially with large vocabularies, leading to suboptimal classification accuracy \\cite{mahadevkar2024xn8}. It also struggles with word dependencies, violating its independence assumption.\n        *   **Discriminative Classifiers (SVM, LR)**: Despite significant efforts to improve their efficiency (e.g., SV Mperf, cyclic coordinate descent for LR), they generally cannot compete with MNB's computational speed, particularly for online learning scenarios.\n        *   **Complement Naive Bayes (CNB)**: An attempt to improve MNB's performance, particularly for imbalanced class distributions, but doesn't fundamentally address the objective function mismatch.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Discriminative Multinominal Naive Bayes (DMNB), which adapts the Discriminative Frequency Estimate (DFE) parameter learning method from Bayesian networks to an MNB model for text classification \\cite{mahadevkar2024xn8}.\n    *   **Novelty**: DMNB integrates both generative (likelihood) and discriminative (classification objective) learning during the frequency counting process. Instead of simply counting word occurrences (as in MNB's FE), DMNB updates word frequencies based on the prediction loss (difference between true and predicted posterior probability) for each training document. This allows the model to \"learn\" from its classification errors while still leveraging frequency information.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: The DMNB algorithm itself, which iteratively updates word frequencies ($f_{ic}$) by adding the prediction loss $L(d_t) = P(c|d_t) - \\hat{P}(c|d_t)$ for each non-zero word in a document \\cite{mahadevkar2024xn8}. This ensures that parameters are adjusted based on how well the model predicts the class.\n    *   **Hybrid Learning Paradigm**: DMNB effectively combines the advantages of generative learning (utilizing all data information, including word distributions) and discriminative learning (directly optimizing for classification accuracy). It is conceptualized as searching a larger hypothesis space than FE but more efficiently than Stochastic Gradient Descent (SGD) \\cite{mahadevkar2024xn8}.\n    *   **Preservation of MNB Advantages**: DMNB maintains MNB's computational efficiency (roughly as fast, single pass often sufficient for convergence) and online learning capability, making it suitable for real-time applications.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Empirical studies were performed to compare DMNB against MNB and state-of-the-art discriminative classifiers like SVM (e.g., SMO, SV Mperf) and Logistic Regression (LR). The comparisons focused on accuracy across different training dataset sizes and computational cost (training and prediction time). An illustrative example also demonstrated DMNB's ability to handle word dependencies better than MNB.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Accuracy**: DMNB performs competitively with SVM and LR on relatively large training datasets and may even outperform them on smaller datasets.\n        *   **Computational Efficiency**: DMNB is shown to be significantly faster than any other known discriminative classifier, including highly optimized versions like SV Mperf, while retaining MNB's speed advantages \\cite{mahadevkar2024xn8}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper assumes a bag-of-words representation and focuses on binary classification problems for its discussion. While DMNB addresses the independence assumption issue of MNB to some extent, it still operates within the MNB framework. The paper does not explicitly detail specific limitations of DMNB itself, but acknowledges the general challenges of high-dimensional and sparse text data.\n    *   **Scope of Applicability**: Primarily focused on text classification tasks, particularly those requiring high efficiency and online learning capabilities, such as content-based recommenders.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: DMNB significantly advances the technical state-of-the-art by providing a text classification algorithm that achieves competitive accuracy with discriminative classifiers while maintaining the unparalleled computational efficiency and online learning capabilities of MNB \\cite{mahadevkar2024xn8}. This bridges a critical gap between effectiveness and efficiency in text classification.\n    *   **Potential Impact**: The algorithm has high potential impact for practical applications in text mining that require both high performance and real-time processing, such as online recommendation systems, spam filtering, and sentiment analysis, where traditional discriminative models are too slow and MNB is not accurate enough.",
        "keywords": [
            "Discriminative Multinominal Naive Bayes (DMNB)",
            "Text Classification",
            "Multinominal Naive Bayes (MNB)",
            "Discriminative Classifiers",
            "Objective Function Mismatch",
            "Computational Efficiency",
            "Online Learning",
            "Discriminative Frequency Estimate (DFE)",
            "Hybrid Learning Paradigm",
            "Prediction Loss",
            "Competitive Accuracy",
            "Real-time Applications"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"in this paper, we **propose a new text classification algorithm**, called discriminative multinominal naive bayes (dmnb)...\"\n*   it then describes the novelty of this algorithm and mentions \"our **empirical studies show that dmnb performs competitively**...\" which indicates the evaluation of the proposed method.\n*   the introduction sets up a technical problem (limitations of mnb for text classification) that the proposed algorithm aims to solve.\n\nthese points strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. while it includes empirical studies, these are in support of validating the *new algorithm* being proposed, making the primary classification \"technical.\"\n\n**classification: technical**"
    },
    "bdba9bd3e75b1899824dcddcaa5a707fe3ad40ee.pdf": {
        "title": "Explainable AI improves task performance in human\u2013AI collaboration",
        "authors": [
            "J. Senoner",
            "Simon Schallmoser",
            "Bernhard Kratzwald",
            "Stefan Feuerriegel",
            "Torbj\u00f8rn Netland"
        ],
        "published_date": "2024",
        "abstract": "Artificial intelligence (AI) provides considerable opportunities to assist human work. However, one crucial challenge of human\u2013AI collaboration is that many AI algorithms operate in a black-box manner where the way how the AI makes predictions remains opaque. This makes it difficult for humans to validate a prediction made by AI against their own domain knowledge. For this reason, we hypothesize that augmenting humans with explainable AI improves task performance in human\u2013AI collaboration. To test this hypothesis, we implement explainable AI in the form of visual heatmaps in inspection tasks conducted by domain experts. Visual heatmaps have the advantage that they are easy to understand and help to localize relevant parts of an image. We then compare participants that were either supported by (a) black-box AI or (b) explainable AI, where the latter supports them to follow AI predictions when the AI is accurate or overrule the AI when the AI predictions are wrong. We conducted two preregistered experiments with representative, real-world visual inspection tasks from manufacturing and medicine. The first experiment was conducted with factory workers from an electronics factory, who performed \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$N=9,600$$\\end{document} assessments of whether electronic products have defects. The second experiment was conducted with radiologists, who performed \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$N=5,650$$\\end{document} assessments of chest X-ray images to identify lung lesions. The results of our experiments with domain experts performing real-world tasks show that task performance improves when participants are supported by explainable AI with heatmaps instead of black-box AI. We find that explainable AI as a decision aid improved the task performance by 7.7 percentage points (95% confidence interval [CI]: 3.3% to 12.0%, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$P=0.001$$\\end{document}) in the manufacturing experiment and by 4.7 percentage points (95% CI: 1.1% to 8.3%, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$P=0.010$$\\end{document}) in the medical experiment compared to black-box AI. These gains represent a significant improvement in task performance.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/bdba9bd3e75b1899824dcddcaa5a707fe3ad40ee.pdf",
        "venue": "Scientific Reports",
        "citationCount": 26,
        "score": 26.0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem**: The paper addresses the challenge of opaque \"black-box\" AI algorithms in human-AI collaboration, where the internal workings and decision-making processes are not transparent to human users.\n*   **Importance and Challenge**: This opacity makes it difficult for humans to validate AI predictions against their own domain knowledge. Consequently, humans cannot effectively correct erroneous AI predictions, leading to a loss of unique human expertise and rendering human-AI collaboration largely ineffective. The problem is crucial given the increasing integration of AI into various professional domains (e.g., manufacturing, medicine).\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches**: The work builds upon the field of Explainable AI (XAI), specifically post-hoc explanation techniques like visual heatmaps, which are commonly used by AI engineers. It also relates to behavioral studies on human-AI collaboration (e.g., algorithm aversion, trust, overreliance).\n*   **Limitations of Previous Solutions**:\n    *   Existing research on XAI's effect on task performance often suffers from key limitations:\n        *   Recruiting laypeople instead of domain experts.\n        *   Using overly simplified tasks not representative of real job tasks.\n        *   Research designs that do not effectively isolate the effect of XAI on task performance (e.g., comparing XAI vs. humans alone, or not using \"real\" XAI).\n*   **Positioning**: This paper distinguishes itself by studying the effect of XAI on task performance *relative to black-box AI* in *real-world job tasks* performed by *actual domain experts*, thereby overcoming the limitations of prior work.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method**: The paper employs explainable AI in the form of visual heatmaps as a decision aid. These heatmaps visually highlight areas of an input (e.g., an image) that are most relevant to the AI's prediction. The AI itself provides a numerical \"quality score\" or prediction, and the heatmap augments this score by explaining *why* the AI made that prediction.\n*   **Novelty/Difference**:\n    *   The primary innovation lies in the rigorous empirical validation of XAI's impact on *task performance* in highly realistic, high-stakes settings.\n    *   It directly compares human-AI collaboration with black-box AI versus explainable AI, isolating the effect of explanations.\n    *   The use of state-of-the-art visual heatmaps, which do not provide *additional predictive information* from the AI's perspective but make the AI's reasoning *accessible* to humans, is key. This allows domain experts to validate or overrule AI predictions based on their expertise.\n\n**4. Key Technical Contributions**\n*   **Novel Methods/Techniques**: The application and empirical validation of visual heatmaps as a decision aid for improving human-AI task performance in complex visual inspection tasks. The method facilitates human validation of AI predictions, leading to better decision-making.\n*   **System Design/Architectural Innovations**: A robust, preregistered, between-subject experimental design comparing black-box AI and explainable AI in two distinct, real-world visual inspection contexts (manufacturing and medicine). This design allows for a clear causal inference regarding XAI's impact.\n*   **Theoretical Insights/Analysis**: The hypothesis that XAI improves task performance by enabling domain experts to:\n    1.  More accurately follow AI predictions when they are correct.\n    2.  More effectively overrule AI predictions when they are wrong.\n\n**5. Experimental Validation**\n*   **Experiments Conducted**: Two preregistered, randomized experiments were conducted:\n    *   **Study 1 (Manufacturing)**: Factory workers from Siemens performed N=9,600 assessments to identify quality defects in electronic products.\n    *   **Study 2 (Medicine)**: Radiologists performed N=5,650 assessments of chest X-ray images to identify lung lesions.\n*   **Key Performance Metrics**: Balanced accuracy and defect detection rate were used to measure task performance. Decision speed was also monitored.\n*   **Comparison Results**:\n    *   **Manufacturing Study**: Augmenting participants with explainable AI led to a **five-fold decrease in the median error rate** of human decisions compared to black-box AI.\n        *   Balanced accuracy improved from 88.6% (black-box AI) to 96.3% (explainable AI) \\cite{senoner2024wsd}.\n        *   Defect detection rate improved from 82.0% (black-box AI) to 93.0% (explainable AI) \\cite{senoner2024wsd}.\n        *   Explainable AI users were significantly better at following accurate AI predictions (98.6% vs. 93.5%) and overruling wrong AI predictions (96.9% vs. 86.4%) \\cite{senoner2024wsd}.\n        *   No statistically significant difference in decision speed was observed, indicating performance improvement without productivity loss \\cite{senoner2024wsd}.\n    *   **Medical Study**: The paper states that in both studies, participants performed better when supported by explainable AI, indicating similar positive results for radiologists \\cite{senoner2024wsd}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions**: The study focuses on visual inspection tasks and uses heatmaps as the explanation format. While heatmaps are state-of-the-art for localization, the generalizability to other AI tasks or explanation types is not directly tested. The effectiveness of XAI is dependent on the human's ability to interpret and act upon the explanations.\n*   **Scope of Applicability**: The findings are highly relevant for visual inspection tasks in manufacturing, healthcare (e.g., radiology, dermatology, pathology), and other domains where human experts collaborate with AI for visual assessment.\n\n**7. Technical Significance**\n*   **Advance State-of-the-Art**: This paper provides compelling empirical evidence, derived from real-world settings with domain experts, that explainable AI significantly improves human-AI collaboration task performance. It moves beyond theoretical discussions and studies with laypeople, establishing a strong practical case for XAI.\n*   **Potential Impact on Future Research**: The findings underscore the importance of designing transparent AI systems for effective human-AI teaming. It encourages further research into optimal XAI methods for various tasks and user groups, and how XAI can be integrated into workflows to leverage both AI's predictive power and human domain expertise. It suggests that XAI is not just about trust or acceptance, but a critical component for enhancing overall system performance.",
        "keywords": [
            "Explainable AI (XAI)",
            "human-AI collaboration",
            "black-box AI opacity",
            "visual heatmaps",
            "empirical validation",
            "task performance improvement",
            "domain experts",
            "visual inspection tasks",
            "error rate reduction",
            "overruling AI predictions",
            "manufacturing and medicine applications",
            "preregistered experimental design"
        ],
        "paper_type": "based on the abstract and introduction, this paper is an **empirical** type.\n\nhere's why:\n\n*   **abstract mentions:** \"we hypothesize that...\", \"to test this hypothesis, we analyze the effect...\", \"we conducted two preregistered experiments...\", \"the first experiment was conducted with factory workers... n= 9,600 assessments...\", \"the second experiment was conducted with radiologists... n= 5,650 assessments...\", \"the results of our experiments... show that...\", \"we find that augmenting participants... leads to a five-fold decrease in the median error rate... which gives a significant improvement in task performance.\"\n*   **introduction discusses:** the problem (black-box ai opacity) that the experiments aim to address, setting the stage for the research questions and methodology.\n*   **keywords:** \"task performance\", \"decision-making\", \"human-ai collaboration\" all point to studies involving human subjects and measurable outcomes.\n\nthese elements strongly indicate a data-driven study with experiments and statistical analysis to test a hypothesis, which is the core characteristic of an empirical paper."
    },
    "d3abdfe5f5f260e28c7d989dbf5fee9c232a0584.pdf": {
        "title": "AI-generated text may have a role in evidence-based medicine",
        "authors": [
            "Yifan Peng",
            "Justin F. Rousseau",
            "E. Shortliffe",
            "C. Weng"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/d3abdfe5f5f260e28c7d989dbf5fee9c232a0584.pdf",
        "venue": "Nature Network Boston",
        "citationCount": 50,
        "score": 25.0,
        "summary": "",
        "keywords": []
    },
    "a2f5c019c8017e10850fc32fc08e2457c81f8df5.pdf": {
        "title": "Utilizing Large Language Models for Enhanced Clinical Trial Matching: A Study on Automation in Patient Screening",
        "authors": [
            "J. Beattie",
            "Sarah Neufeld",
            "Daniel Yang",
            "C. Chukwuma",
            "A. Gul",
            "Neil Desai",
            "Steve Jiang",
            "M. Dohopolski"
        ],
        "published_date": "2024",
        "abstract": "Background: Clinical trial matching, essential for advancing medical research, involves detailed screening of potential participants to ensure alignment with specific trial requirements. Research staff face challenges due to the high volume of eligible patients and the complexity of varying eligibility criteria. The traditional manual process, both time-consuming and error-prone, often leads to missed opportunities. Utilizing Artificial Intelligence (AI) and Natural Language Processing (NLP) can significantly enhance the accuracy and efficiency of this process through automated patient screening against established criteria. Methods: Utilizing data from the National NLP Clinical Challenges (n2c2) 2018 Challenge, we utilized 202 longitudinal patient records. These records were annotated by medical professionals and evaluated against 13 selection criteria encompassing various health assessments. Our approach involved embedding medical documents into a vector database to determine relevant document sections, then using a large language model (GPT-3.5 Turbo and GPT-4 OpenAI API) in tandem with structured and chain-of-thought prompting techniques for systematic document assessment against the criteria. Misclassified criteria were also examined to identify classification challenges. Results: This study achieved an accuracy of 0.81, sensitivity of 0.80, specificity of 0.82, and a micro F1 score of 0.79 using GPT-3.5 Turbo, and an accuracy of 0.87, sensitivity of 0.85, specificity of 0.89, and micro F1 score of 0.86 using GPT-4 Turbo. Notably, some criteria in the ground truth appeared mislabeled, an issue we could not explore further due to insufficient label generation guidelines. Conclusion: Our findings underscore the significant potential of AI and NLP technologies, including large language models, in the clinical trial matching process. The study demonstrated strong capabilities in identifying eligible patients and minimizing false inclusions. Such automated systems promise to greatly alleviate the workload of research staff and improve clinical trial enrollment, thus accelerating the process and enhancing the overall feasibility of clinical research.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/a2f5c019c8017e10850fc32fc08e2457c81f8df5.pdf",
        "venue": "medRxiv",
        "citationCount": 24,
        "score": 24.0,
        "summary": "Background: Clinical trial matching, essential for advancing medical research, involves detailed screening of potential participants to ensure alignment with specific trial requirements. Research staff face challenges due to the high volume of eligible patients and the complexity of varying eligibility criteria. The traditional manual process, both time-consuming and error-prone, often leads to missed opportunities. Utilizing Artificial Intelligence (AI) and Natural Language Processing (NLP) can significantly enhance the accuracy and efficiency of this process through automated patient screening against established criteria. Methods: Utilizing data from the National NLP Clinical Challenges (n2c2) 2018 Challenge, we utilized 202 longitudinal patient records. These records were annotated by medical professionals and evaluated against 13 selection criteria encompassing various health assessments. Our approach involved embedding medical documents into a vector database to determine relevant document sections, then using a large language model (GPT-3.5 Turbo and GPT-4 OpenAI API) in tandem with structured and chain-of-thought prompting techniques for systematic document assessment against the criteria. Misclassified criteria were also examined to identify classification challenges. Results: This study achieved an accuracy of 0.81, sensitivity of 0.80, specificity of 0.82, and a micro F1 score of 0.79 using GPT-3.5 Turbo, and an accuracy of 0.87, sensitivity of 0.85, specificity of 0.89, and micro F1 score of 0.86 using GPT-4 Turbo. Notably, some criteria in the ground truth appeared mislabeled, an issue we could not explore further due to insufficient label generation guidelines. Conclusion: Our findings underscore the significant potential of AI and NLP technologies, including large language models, in the clinical trial matching process. The study demonstrated strong capabilities in identifying eligible patients and minimizing false inclusions. Such automated systems promise to greatly alleviate the workload of research staff and improve clinical trial enrollment, thus accelerating the process and enhancing the overall feasibility of clinical research.",
        "keywords": []
    },
    "e3ee318d593729352f991142e6e3bef62640c5a5.pdf": {
        "title": "AI in imaging: the regulatory landscape",
        "authors": [
            "Derek L G Hill"
        ],
        "published_date": "2024",
        "abstract": "Abstract Artificial intelligence (AI) methods have been applied to medical imaging for several decades, but in the last few years, the number of publications and the number of AI-enabled medical devices coming on the market have significantly increased. While some AI-enabled approaches are proving very valuable, systematic reviews of the AI imaging field identify significant weaknesses in a significant proportion of the literature. Medical device regulators have recently become more proactive in publishing guidance documents and recognizing standards that will require that the development and validation of AI-enabled medical devices need to be more rigorous than required for tradition \u201crule-based\u201d software. In particular, developers are required to better identify and mitigate risks (such as bias) that arise in AI-enabled devices, and to ensure that the devices are validated in a realistic clinical setting to ensure their output is clinically meaningful. While this evolving regulatory landscape will mean that device developers will take longer to bring novel AI-based medical imaging devices to market, such additional rigour is necessary to address existing weaknesses in the field and ensure that patients and healthcare professionals can trust AI-enabled devices. There would also be benefits in the academic community taking into account this regulatory framework, to improve the quality of the literature and make it easier for academically developed AI tools to make the transition to medical devices that impact healthcare.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/e3ee318d593729352f991142e6e3bef62640c5a5.pdf",
        "venue": "British Journal of Radiology",
        "citationCount": 20,
        "score": 20.0,
        "summary": "Abstract Artificial intelligence (AI) methods have been applied to medical imaging for several decades, but in the last few years, the number of publications and the number of AI-enabled medical devices coming on the market have significantly increased. While some AI-enabled approaches are proving very valuable, systematic reviews of the AI imaging field identify significant weaknesses in a significant proportion of the literature. Medical device regulators have recently become more proactive in publishing guidance documents and recognizing standards that will require that the development and validation of AI-enabled medical devices need to be more rigorous than required for tradition \u201crule-based\u201d software. In particular, developers are required to better identify and mitigate risks (such as bias) that arise in AI-enabled devices, and to ensure that the devices are validated in a realistic clinical setting to ensure their output is clinically meaningful. While this evolving regulatory landscape will mean that device developers will take longer to bring novel AI-based medical imaging devices to market, such additional rigour is necessary to address existing weaknesses in the field and ensure that patients and healthcare professionals can trust AI-enabled devices. There would also be benefits in the academic community taking into account this regulatory framework, to improve the quality of the literature and make it easier for academically developed AI tools to make the transition to medical devices that impact healthcare.",
        "keywords": []
    },
    "3bf118f2f918ad121aa3983479241d8b09f9c071.pdf": {
        "title": "Autonomous Artificial Intelligence Agents for Clinical Decision Making in Oncology",
        "authors": [
            "Dyke Ferber",
            "O. E. Nahhas",
            "Georg W\u00f6lflein",
            "I. Wiest",
            "J. Clusmann",
            "Marie-Elisabeth Lessman",
            "S. Foersch",
            "Jacqueline Lammert",
            "Maximilian Tschochohei",
            "Dirk J\u00e4ger",
            "M. Salto-Tellez",
            "N. Schultz",
            "Daniel Truhn",
            "J. Kather"
        ],
        "published_date": "2024",
        "abstract": "Multimodal artificial intelligence (AI) systems have the potential to enhance clinical decision-making by interpreting various types of medical data. However, the effectiveness of these models across all medical fields is uncertain. Each discipline presents unique challenges that need to be addressed for optimal performance. This complexity is further increased when attempting to integrate different fields into a single model. Here, we introduce an alternative approach to multimodal medical AI that utilizes the generalist capabilities of a large language model (LLM) as a central reasoning engine. This engine autonomously coordinates and deploys a set of specialized medical AI tools. These tools include text, radiology and histopathology image interpretation, genomic data processing, web searches, and document retrieval from medical guidelines. We validate our system across a series of clinical oncology scenarios that closely resemble typical patient care workflows. We show that the system has a high capability in employing appropriate tools (97%), drawing correct conclusions (93.6%), and providing complete (94%), and helpful (89.2%) recommendations for individual patient cases while consistently referencing relevant literature (82.5%) upon instruction. This work provides evidence that LLMs can effectively plan and execute domain-specific models to retrieve or synthesize new information when used as autonomous agents. This enables them to function as specialist, patient-tailored clinical assistants. It also simplifies regulatory compliance by allowing each component tool to be individually validated and approved. We believe, that our work can serve as a proof-of-concept for more advanced LLM-agents in the medical domain.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/3bf118f2f918ad121aa3983479241d8b09f9c071.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 18.0,
        "summary": "Here is a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the challenge of developing effective multimodal Artificial Intelligence (AI) systems for complex clinical decision-making in oncology \\cite{ferber20240lt}.\n    *   Existing generalist multimodal AI models face several limitations: uncertain effectiveness across all medical fields, difficulty in capturing the wide and complex distribution of human diseases, computational expense for frequent retraining to keep up with evolving medical knowledge, and regulatory hurdles for universal multi-purpose AI models (which are often restricted to a singular purpose).\n    *   Real-world clinical decision-making requires multi-step reasoning, planning, and repeated interactions with diverse data, which current models often lack.\n\n*   **Related Work & Positioning**\n    *   Previous multimodal AI systems integrate various data types (e.g., radiology with clinical data, histopathology with genomics or text) but often focus on specific tasks.\n    *   While Large Language Models (LLMs) have been augmented with domain-specific information through fine-tuning or Retrieval-Augmented Generation (RAG), these approaches primarily position LLMs as \"information extraction tools only\" rather than true clinical assistants capable of reasoning, strategizing, and performing actions \\cite{ferber20240lt}.\n    *   This work differentiates itself from the philosophy of an \"all-encompassing multimodal generalist foundation model\" by leveraging the strengths of specialized unimodal deep learning models, coordinated by an LLM agent \\cite{ferber20240lt}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is an autonomous AI agent that utilizes a generalist LLM (specifically GPT-4) as a \"central reasoning engine\" \\cite{ferber20240lt}.\n    *   This LLM engine autonomously coordinates and deploys a suite of specialized medical AI tools to interpret multimodal patient data.\n    *   The integrated tools include:\n        *   Text interpretation.\n        *   Radiology image interpretation (using GPT-4V for MRI/CT scans).\n        *   Histopathology image interpretation (e.g., MedSAM for segmentation, in-house vision transformers for predicting genetic alterations like MSI/MSS, KRAS, BRAF mutations from slides).\n        *   Genomic data processing.\n        *   Web searches (Google, PubMed).\n        *   Document retrieval from medical guidelines and the OncoKB precision oncology database.\n    *   The agent follows a two-stage process: autonomously selecting and applying relevant tools to derive supplementary insights, followed by a document retrieval step to ground responses in substantiated medical evidence with citations.\n\n*   **Key Technical Contributions**\n    *   **Novel Agent Architecture**: Introduction of an LLM-based autonomous agent that acts as a reasoning and coordination engine for a diverse set of specialized medical AI tools, moving beyond mere information retrieval.\n    *   **Modular Integration of Specialized Tools**: Demonstrates the effectiveness of combining precision medicine solutions (specialist unimodal deep learning models) with an LLM agent, allowing each component tool to be individually developed, validated, and updated.\n    *   **Enhanced Regulatory Compliance Pathway**: The modular design simplifies regulatory approval by enabling individual validation and approval of each specialized tool, circumventing current restrictions on universal multi-purpose AI models.\n    *   **Dynamic Knowledge Update**: Allows for rapid updates of medical knowledge by replacing pertinent documents in the database or leveraging real-time web searches, without requiring retraining of the core LLM.\n\n*   **Experimental Validation**\n    *   **Benchmark Strategy**: A novel benchmark strategy was devised using a dataset of eleven realistic and multidimensional patient cases focused on gastrointestinal oncology, designed to mimic typical patient care workflows \\cite{ferber20240lt}. This dataset includes multimodal data (CT/MRI images, microscopic data, genetic data, textual reports).\n    *   **Evaluation Method**: Blinded manual evaluation by four human medical experts, focusing on three areas: agent's tool utilization, quality and completeness of textual outputs, and precision of citations.\n    *   **Key Performance Metrics & Results**:\n        *   **Tool Utilization**: High capability in employing appropriate tools (97% success rate across 33 invocations), with only one instance of calling an unrequired tool and one omission of a necessary tool \\cite{ferber20240lt}.\n        *   **Pathology Tools**: High accuracy in predicting mutation status (e.g., MSI, KRAS, BRAF) from histopathology data, with correct predictions in all seven relevant cases.\n        *   **Radiology Tools**: GPT-4V effectively guided clinical decisions towards accurate disease trajectory assessments in all cases, despite occasional omissions or extraneous details.\n        *   **Completeness**: Achieved a 94% rate in resolving essential statements compiled by medical experts, even handling contradictory information by pointing out inconsistencies and recommending further steps \\cite{ferber20240lt}.\n        *   **Helpfulness**: 89.2% of 37 queries were effectively addressed by the model \\cite{ferber20240lt}.\n        *   **Accuracy of Conclusions**: 93.6% of 140 assessable assertions were factually correct, with only 4.3% incorrect and 2.1% potentially detrimental \\cite{ferber20240lt}.\n        *   **Referencing**: 82.5% of 171 provided citations were accurately aligned with the model's assertions, indicating limited instances of erroneous extrapolation (hallucinations) \\cite{ferber20240lt}.\n\n*   **Limitations & Scope**\n    *   The agent is in a premature and experimental stage, limiting immediate clinical applicability \\cite{ferber20240lt}.\n    *   Specific technical restrictions include the provision of only a singular slice of radiology images and the current limitations of GPT-4V in interpreting complex medical images.\n    *   The evaluation was confined to a single interaction without follow-up questions for simplicity.\n    *   The current scope is restricted to oncological use cases, though the underlying framework is adaptable to other medical specialties.\n\n*   **Technical Significance**\n    *   This work provides a proof-of-concept that LLMs can effectively plan and execute domain-specific models as autonomous agents, functioning as specialist, patient-tailored clinical assistants \\cite{ferber20240lt}.\n    *   It advances the technical state-of-the-art by demonstrating a viable alternative to monolithic generalist multimodal AI, enabling the integration of highly specialized precision medicine tools.\n    *   The modular approach facilitates circumventing data availability constraints in medicine, allowing entities to leverage smaller, specialized models developed with direct access to specific data.\n    *   It offers a blueprint for future research into more advanced LLM-agents in the medical domain, potentially leading to frameworks that can refine or innovate new tools from scratch when existing ones are insufficient.",
        "keywords": [
            "LLM-based autonomous AI agent",
            "Multimodal clinical decision-making",
            "Oncology",
            "Specialized medical AI tools",
            "Modular integration",
            "Precision medicine solutions",
            "Regulatory compliance pathway",
            "Dynamic knowledge update",
            "Gastrointestinal oncology benchmark",
            "Human expert evaluation",
            "Tool utilization accuracy",
            "Factually correct assertions",
            "Patient-tailored clinical assistants",
            "Alternative to generalist foundation models"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **presents new methods, algorithms, or systems:** the abstract explicitly states, \"here, we introduce an alternative approach to multimodal medical ai that utilizes the generalist capabilities of a large language model (llm) as a central reasoning engine. this engine autonomously coordinates and deploys a set of specialized medical ai tools.\" the introduction further details the construction of this \"ai agent tailored to interact with and draw conclusions from multimodal patient data through separate tools.\"\n*   **discusses technical problem, proposed solution:** the introduction identifies the limitations of existing generalist multimodal ai models and current llm augmentation strategies as a technical problem, then proposes their autonomous llm agent as a solution.\n*   **validation of a system:** while it includes empirical validation (\"we validate our system across a series of clinical oncology scenarios,\" \"we show that the system has a high capability...\"), the primary focus is on the *development and introduction* of the new system, with the empirical results serving to demonstrate its effectiveness. this is typical for a technical paper."
    },
    "139728e2c178cbb2c6efd13dac8c9c622242b448.pdf": {
        "title": "Advancements in AI for Oncology: Developing an Enhanced YOLOv5-based Cancer Cell Detection System",
        "authors": [
            "Xin Chen",
            "Yuxiang Hu",
            "Ting Xu",
            "Haowei Yang",
            "Tong Wu"
        ],
        "published_date": "2024",
        "abstract": "As artificial intelligence (AI) theory becomes more sophisticated and its utilization spreads across daily life, education, and professional settings, the adoption of AI for medical diagnostic and service purposes stands as a logical progression in the evolution of medical technologies. This document outlines a novel approach to detecting cancer cell targets using a deep learning-based system, marking a critical step towards integrating AI into cancer diagnostics. The process of detecting cancer cell targets entails the localization of cell types within images of cells. By capitalizing on the strengths of the YOLOv5 model\u2014a deep learning-driven, end-to-end, real-time object detection framework known for its efficiency, superior performance, adaptability, and user-friendly PyTorch integration\u2014this research presents an enhanced YOLOv5 model incorporating both a feature pyramid network and the original YOLOv5 architecture. The ultimate aim is to facilitate precise detection of targets in cancer cell images. The experimental data demonstrate the system's negligible error rate in detection, swift processing capabilities, and exceptional reliability.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/139728e2c178cbb2c6efd13dac8c9c622242b448.pdf",
        "venue": "International Journal of Innovative Research in Computer Science & Technology",
        "citationCount": 17,
        "score": 17.0,
        "summary": "Here's a focused summary of the paper \"Advancements in AI for Oncology: Developing an Enhanced YOLOv5-based Cancer Cell Detection System\" \\cite{chen2024hn6} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Accurate and timely detection and localization of cancer cells within complex medical images, which is crucial for early diagnosis and effective treatment.\n    *   **Importance and Challenge:**\n        *   Traditional methods rely on subjective physician judgment, leading to time-consuming operations and susceptibility to bias.\n        *   Conventional object detectors struggle with cancer cell detection due to limited inter-class variation between malignant and benign cells.\n        *   Real-world deployment of AI-based diagnostic tools demands high robustness, optimized computational resource utilization, and awareness of platform/environmental constraints.\n        *   Detecting small cancer cells presents challenges: scarcity of usable features, need for extreme localization precision, sample imbalance, and target clustering.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon the advancements in deep learning-based object detection, specifically leveraging the YOLOv5 model.\n    *   **Limitations of Previous Solutions:**\n        *   Traditional manual methods are subjective and inefficient.\n        *   General-purpose object detectors are not directly applicable to cancer cell detection due to the subtle differences between cell types and the stringent requirements for medical deployment.\n        *   The paper positions its enhanced YOLOv5 as a lightweight solution designed to overcome these limitations by balancing energy consumption, accuracy, and speed, specifically tailored for cancer cell images.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** An enhanced lightweight YOLOv5 model for cancer cell target detection. The framework includes modules for data augmentation, preprocessing (labeling), anchor box determination, feature extraction via a Feature Pyramid Network (FPN) integrated with a Perceptual Adversarial Network (PAN), and loss calculation using GIoU_Loss.\n    *   **Novelty/Difference:**\n        *   **Enhanced YOLOv5 Architecture:** Integrates a Feature Pyramid Network (FPN) and the original YOLOv5 architecture, segmented into input, central backbone (with Focus and CSP architectures), Neck (FPN+PAN), and output stages.\n        *   **Strategic Positive Sample Matching:** Incorporates an approach to match positive sample anchor boxes in proximity, increasing the count of positive samples.\n        *   **Dynamic Anchor Box Optimization:** Dynamically derives optimal anchor box dimensions for the dataset during training using genetic algorithms and k-means clustering, aiming for a recall rate of 0.98 or higher.\n        *   **Mosaic Data Augmentation:** Utilizes Mosaic data enhancement at the input stage to diversify the dataset and significantly improve detection accuracy for smaller objects like cancer cells.\n        *   **FPN+PAN Integration:** Employs an FPN for multilevel prediction, merging lower-level detail features with higher-level abstract features, and further enhances it with a PAN to improve the conveyance of location information, enriching the feature structure with both semantic and locational insights.\n        *   **GIoU_Loss Optimization:** Uses Generalized Intersection Over Union (GIoU_Loss) to address the gradient vanishing problem in IoU calculations, ensuring robust loss computation even when predicted and ground-truth bounding boxes do not intersect.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Development of an improved lightweight YOLOv5 network specifically optimized for cancer cell detection, balancing accuracy with reduced computational complexity.\n        *   Integration of a sophisticated FPN+PAN architecture within the YOLOv5 Neck for superior multi-scale feature extraction and precise localization of cancer cells.\n        *   Dynamic anchor box optimization strategy using genetic algorithms and k-means clustering for dataset-specific anchor box refinement.\n        *   Effective application of Mosaic data augmentation to enhance the detection of small and clustered cancer cells.\n        *   Adoption of GIoU_Loss to improve bounding box regression robustness, particularly for non-overlapping predictions.\n    *   **System Design/Architectural Innovations:** The proposed system design significantly reduces model parameters and storage overhead while accelerating inference, making it practical for medical applications.\n    *   **Theoretical Insights/Analysis:** The paper implicitly addresses the challenges of small target detection (feature scarcity, localization precision, sample imbalance, clustering) through its architectural choices and optimization strategies.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:** The system was evaluated on its ability to detect cancer cells in various image types, including normal cells, cells with a small number of cancer cells, and mixed cancer cell types.\n    *   **Dataset:** A dataset of 7,000 cell images (2,000 breast cancer, 2,000 cervical cancer, 2,000 healthy, 1,000 mixed healthy/cancerous) was used. This dataset was expanded to 10,000 images through data augmentation, with an 80/20 split for training and validation.\n    *   **Key Performance Metrics and Comparison Results:** The model's performance was assessed using standard metrics: Precision (P), Recall (R), Average Precision (AP), and F1-score.\n        *   Recall: 0.807\n        *   AP: 0.77\n        *   Precision: 0.911\n        *   F1-score: 0.867\n    *   **Overall Findings:** The experimental data demonstrated the system's \"negligible error rate in detection, swift processing capabilities, and exceptional reliability,\" indicating high detection accuracy while maintaining reduced model complexity and operational time.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper's future work suggests that further optimization of model structure for lower computational costs is still possible. The current model's adaptability might be limited to the cancer types included in the dataset (breast and cervical cancer), as expanding to cover more cancer types is a stated future direction.\n    *   **Scope of Applicability:** Primarily focused on cancer cell detection in medical imaging. The authors suggest its generalizability to other automated detection tasks in medical imaging domains.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art by introducing an improved lightweight YOLOv5 network that effectively addresses critical challenges in cancer cell detection, such as balancing speed and accuracy, and mitigating issues like excessive model weight parameters and high storage consumption.\n    *   **Potential Impact on Future Research:**\n        *   Provides a robust and efficient AI-driven solution for early cancer diagnosis, offering crucial decision support for treatment.\n        *   Offers valuable insights and a reference for automated detection tasks in other medical imaging domains.\n        *   Lays the groundwork for future research in optimizing model structures for even lower computational costs and expanding adaptability to a wider range of cancer types.\n        *   Highlights the immense potential of deep learning in healthcare, particularly in clinical applications for cancer detection and diagnosis.",
        "keywords": [
            "Cancer cell detection",
            "Enhanced YOLOv5",
            "Deep learning object detection",
            "Medical imaging",
            "FPN+PAN architecture",
            "Dynamic anchor box optimization",
            "Mosaic data augmentation",
            "GIoU_Loss",
            "Computational efficiency",
            "High detection accuracy",
            "Early cancer diagnosis",
            "Small target detection"
        ],
        "paper_type": "based on the abstract and introduction, this paper falls under the **technical** classification.\n\nhere's why:\n\n*   **abstract mentions:** \"outlines a novel approach,\" \"presents an enhanced yolov5 model,\" \"deep learning-based system.\" these phrases directly indicate the development and presentation of a new method or system.\n*   **introduction discusses:** it identifies a \"significant global health challenge\" (cancer detection), points out limitations of \"traditional methods,\" and highlights \"algorithmic challenges\" and \"difficulties in real-world deployments\" for conventional object detectors in the context of cancer cell detection. this sets up a clear technical problem that the proposed system aims to solve.\n*   the mention of \"experimental data demonstrate the system's negligible error rate\" in the abstract indicates an evaluation of the *newly developed system*, which is a common component of technical papers."
    },
    "c570f0fce54ffcf666a60b23203e64e5d5d6f1e7.pdf": {
        "title": "Distribution shift detection for the postmarket surveillance of medical AI algorithms: a retrospective simulation study",
        "authors": [
            "Lisa M. Koch",
            "Christian F. Baumgartner",
            "Philipp Berens"
        ],
        "published_date": "2024",
        "abstract": "Distribution shifts remain a problem for the safe application of regulated medical AI systems, and may impact their real-world performance if undetected. Postmarket shifts can occur for example if algorithms developed on data from various acquisition settings and a heterogeneous population are predominantly applied in hospitals with lower quality data acquisition or other centre-specific acquisition factors, or where some ethnicities are over-represented. Therefore, distribution shift detection could be important for monitoring AI-based medical products during postmarket surveillance. We implemented and evaluated three deep-learning based shift detection techniques (classifier-based, deep kernel, and multiple univariate kolmogorov-smirnov tests) on simulated shifts in a dataset of 130\u2019486 retinal images. We trained a deep learning classifier for diabetic retinopathy grading. We then simulated population shifts by changing the prevalence of patients\u2019 sex, ethnicity, and co-morbidities, and example acquisition shifts by changes in image quality. We observed classification subgroup performance disparities w.r.t. image quality, patient sex, ethnicity and co-morbidity presence. The sensitivity at detecting referable diabetic retinopathy ranged from 0.50 to 0.79 for different ethnicities. This motivates the need for detecting shifts after deployment. Classifier-based tests performed best overall, with perfect detection rates for quality and co-morbidity subgroup shifts at a sample size of 1000. It was the only method to detect shifts in patient sex, but required large sample sizes (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$> 30^{\\prime} 000$$\\end{document}>30\u2032000). All methods identified easier-to-detect out-of-distribution shifts with small (\u2264300) sample sizes. We conclude that effective tools exist for detecting clinically relevant distribution shifts. In particular classifier-based tests can be easily implemented components in the post-market surveillance strategy of medical device manufacturers.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c570f0fce54ffcf666a60b23203e64e5d5d6f1e7.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Distribution shifts remain a problem for the safe application of regulated medical AI systems, and may impact their real-world performance if undetected. Postmarket shifts can occur for example if algorithms developed on data from various acquisition settings and a heterogeneous population are predominantly applied in hospitals with lower quality data acquisition or other centre-specific acquisition factors, or where some ethnicities are over-represented. Therefore, distribution shift detection could be important for monitoring AI-based medical products during postmarket surveillance. We implemented and evaluated three deep-learning based shift detection techniques (classifier-based, deep kernel, and multiple univariate kolmogorov-smirnov tests) on simulated shifts in a dataset of 130\u2019486 retinal images. We trained a deep learning classifier for diabetic retinopathy grading. We then simulated population shifts by changing the prevalence of patients\u2019 sex, ethnicity, and co-morbidities, and example acquisition shifts by changes in image quality. We observed classification subgroup performance disparities w.r.t. image quality, patient sex, ethnicity and co-morbidity presence. The sensitivity at detecting referable diabetic retinopathy ranged from 0.50 to 0.79 for different ethnicities. This motivates the need for detecting shifts after deployment. Classifier-based tests performed best overall, with perfect detection rates for quality and co-morbidity subgroup shifts at a sample size of 1000. It was the only method to detect shifts in patient sex, but required large sample sizes (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$> 30^{\\prime} 000$$\\end{document}>30\u2032000). All methods identified easier-to-detect out-of-distribution shifts with small (\u2264300) sample sizes. We conclude that effective tools exist for detecting clinically relevant distribution shifts. In particular classifier-based tests can be easily implemented components in the post-market surveillance strategy of medical device manufacturers.",
        "keywords": []
    },
    "470b6f476c26b140461119130a7ee3da59b97504.pdf": {
        "title": "High-reward, high-risk technologies? An ethical and legal account of AI development in healthcare",
        "authors": [
            "Maelenn Corfmat",
            "Jo\u00e9 T. Martineau",
            "Catherine R\u00e9gis"
        ],
        "published_date": "2025",
        "abstract": "Background Considering the disruptive potential of AI technology, its current and future impact in healthcare, as well as healthcare professionals\u2019 lack of training in how to use it, the paper summarizes how to approach the challenges of AI from an ethical and legal perspective. It concludes with suggestions for improvements to help healthcare professionals better navigate the AI wave. Methods We analyzed the literature that specifically discusses ethics and law related to the development and implementation of AI in healthcare as well as relevant normative documents that pertain to both ethical and legal issues. After such analysis, we created categories regrouping the most frequently cited and discussed ethical and legal issues. We then proposed a breakdown within such categories that emphasizes the different - yet often interconnecting - ways in which ethics and law are approached for each category of issues. Finally, we identified several key ideas for healthcare professionals and organizations to better integrate ethics and law into their practices. Results We identified six categories of issues related to AI development and implementation in healthcare: (1) privacy; (2) individual autonomy; (3) bias; (4) responsibility and liability; (5) evaluation and oversight; and (6) work, professions and the job market. While each one raises different questions depending on perspective, we propose three main legal and ethical priorities: education and training of healthcare professionals, offering support and guidance throughout the use of AI systems, and integrating the necessary ethical and legal reflection at the heart of the AI tools themselves. Conclusions By highlighting the main ethical and legal issues involved in the development and implementation of AI technologies in healthcare, we illustrate their profound effects on professionals as well as their relationship with patients and other organizations in the healthcare sector. We must be able to identify AI technologies in medical practices and distinguish them by their nature so we can better react and respond to them. Healthcare professionals need to work closely with ethicists and lawyers involved in the healthcare system, or the development of reliable and trusted AI will be jeopardized.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/470b6f476c26b140461119130a7ee3da59b97504.pdf",
        "venue": "BMC Medical Ethics",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Background Considering the disruptive potential of AI technology, its current and future impact in healthcare, as well as healthcare professionals\u2019 lack of training in how to use it, the paper summarizes how to approach the challenges of AI from an ethical and legal perspective. It concludes with suggestions for improvements to help healthcare professionals better navigate the AI wave. Methods We analyzed the literature that specifically discusses ethics and law related to the development and implementation of AI in healthcare as well as relevant normative documents that pertain to both ethical and legal issues. After such analysis, we created categories regrouping the most frequently cited and discussed ethical and legal issues. We then proposed a breakdown within such categories that emphasizes the different - yet often interconnecting - ways in which ethics and law are approached for each category of issues. Finally, we identified several key ideas for healthcare professionals and organizations to better integrate ethics and law into their practices. Results We identified six categories of issues related to AI development and implementation in healthcare: (1) privacy; (2) individual autonomy; (3) bias; (4) responsibility and liability; (5) evaluation and oversight; and (6) work, professions and the job market. While each one raises different questions depending on perspective, we propose three main legal and ethical priorities: education and training of healthcare professionals, offering support and guidance throughout the use of AI systems, and integrating the necessary ethical and legal reflection at the heart of the AI tools themselves. Conclusions By highlighting the main ethical and legal issues involved in the development and implementation of AI technologies in healthcare, we illustrate their profound effects on professionals as well as their relationship with patients and other organizations in the healthcare sector. We must be able to identify AI technologies in medical practices and distinguish them by their nature so we can better react and respond to them. Healthcare professionals need to work closely with ethicists and lawyers involved in the healthcare system, or the development of reliable and trusted AI will be jeopardized.",
        "keywords": []
    },
    "17de71b58044d771beca14bcd8ef18b4f7cff213.pdf": {
        "title": "ChatGPT\u2019s performance in German OB/GYN exams \u2013 paving the way for AI-enhanced medical education and clinical practice",
        "authors": [
            "Maximilian Riedel",
            "Katharina Kaefinger",
            "Antonia Stuehrenberg",
            "Viktoria Ritter",
            "Niklas Amann",
            "Anna Graf",
            "Florian Recker",
            "Evelyn Klein",
            "M. Kiechle",
            "Fabian Riedel",
            "Bastian Meyer"
        ],
        "published_date": "2023",
        "abstract": "Background Chat Generative Pre-Trained Transformer (ChatGPT) is an artificial learning and large language model tool developed by OpenAI in 2022. It utilizes deep learning algorithms to process natural language and generate responses, which renders it suitable for conversational interfaces. ChatGPT\u2019s potential to transform medical education and clinical practice is currently being explored, but its capabilities and limitations in this domain remain incompletely investigated. The present study aimed to assess ChatGPT\u2019s performance in medical knowledge competency for problem assessment in obstetrics and gynecology (OB/GYN). Methods Two datasets were established for analysis: questions (1) from OB/GYN course exams at a German university hospital and (2) from the German medical state licensing exams. In order to assess ChatGPT\u2019s performance, questions were entered into the chat interface, and responses were documented. A quantitative analysis compared ChatGPT\u2019s accuracy with that of medical students for different levels of difficulty and types of questions. Additionally, a qualitative analysis assessed the quality of ChatGPT\u2019s responses regarding ease of understanding, conciseness, accuracy, completeness, and relevance. Non-obvious insights generated by ChatGPT were evaluated, and a density index of insights was established in order to quantify the tool\u2019s ability to provide students with relevant and concise medical knowledge. Results ChatGPT demonstrated consistent and comparable performance across both datasets. It provided correct responses at a rate comparable with that of medical students, thereby indicating its ability to handle a diverse spectrum of questions ranging from general knowledge to complex clinical case presentations. The tool\u2019s accuracy was partly affected by question difficulty in the medical state exam dataset. Our qualitative assessment revealed that ChatGPT provided mostly accurate, complete, and relevant answers. ChatGPT additionally provided many non-obvious insights, especially in correctly answered questions, which indicates its potential for enhancing autonomous medical learning. Conclusion ChatGPT has promise as a supplementary tool in medical education and clinical practice. Its ability to provide accurate and insightful responses showcases its adaptability to complex clinical scenarios. As AI technologies continue to evolve, ChatGPT and similar tools may contribute to more efficient and personalized learning experiences and assistance for health care providers.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/17de71b58044d771beca14bcd8ef18b4f7cff213.pdf",
        "venue": "Frontiers in Medicine",
        "citationCount": 29,
        "score": 14.5,
        "summary": "Here's a focused summary of the provided document for a literature review, interpreting \"technical\" in the context of procedural design and ethical governance, and noting the absence of empirical validation for this type of document.\n\n---\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** The document addresses the challenge of ensuring ethical conduct in research involving human participants within Bavarian academic institutions. This includes identifying when ethical review is necessary, what constitutes an ethically sound research plan, and how to navigate the application process.\n*   **Importance & Challenge:** Ethical review is crucial for protecting research participants from potential physical or psychological harm, upholding fundamental ethical principles (autonomy, beneficence, non-maleficence, justice), and maintaining public trust in scientific endeavors. The challenge lies in providing clear, consistent, and accessible guidelines for researchers, especially concerning vulnerable populations (e.g., minors) and complex data handling, to ensure compliance and facilitate responsible research.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:** The GEHBa (Joint Ethics Committee of Bavarian Universities) operates within established international ethical frameworks. It explicitly references the WMA Declaration of Helsinki \\cite{riedel2023s36} for medical research ethics and the principles of biomedical ethics by Beauchamp & Childress \\cite{riedel2023s36}. The GEHBa positions itself as the central ethics review body for Bavarian universities lacking their own ethics committees and as a resource for collaborative research projects.\n*   **Limitations of Previous Solutions:** The document does not discuss limitations of prior technical solutions or research methodologies. Instead, it clarifies common ambiguities and potential pitfalls in the *process* of ethical review itself, such as distinguishing between projects requiring full review and those that are exempt (e.g., low-risk expert surveys, most student theses), and outlining procedures for handling applications previously reviewed by other committees.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The \"technical approach\" is a structured, standardized, and transparent procedural framework for ethical review. Key components include:\n    *   **Defined Application Triggers:** Clear criteria for when an ethics application is mandatory, based on research involving human participants, potential for harm or undue burden, and possible violation of ethical principles, particularly for vulnerable groups.\n    *   **Standardized Application Package:** A comprehensive and mandatory set of documents required for submission, including a detailed application form, participant information letter, informed consent form, documentation of study data, data security information, a statement from the institutional data protection officer (aligned with GDPR), and a confidentiality declaration.\n    *   **Iterative Review Process:** A multi-stage review mechanism involving an initial pre-screening, monthly committee meetings, internal expert opinions, voting, and the provision of detailed written feedback with recommendations for improvement.\n*   **Novelty/Differentiation:** The innovation lies in the *clarity, comprehensiveness, and supportive nature* of the procedural guidance. The GEHBa offers pre-screening services, allows for revision and re-submission of applications, and provides opportunities for direct discussion with the committee (via video conference or phone), fostering a collaborative approach to achieving ethical compliance. The explicit exclusion of reviewers with conflicts of interest ensures impartiality.\n\n**4. Key Technical Contributions**\n*   **System Design/Architectural Innovations:**\n    *   **Standardized Documentation Requirements:** The detailed specification of required application documents (e.g., \"Antragsformular,\" \"Informationsschreiben,\" \"Einwilligungserkl\u00e4rung,\" \"Stellungnahme der/des Datenschutzbeauftragten\") represents a robust system design for ensuring all critical ethical and data protection aspects are systematically addressed and documented.\n    *   **Structured Review Protocol:** The GEHBa's internal operational protocol, involving monthly video conferences, voting, and internal written expert opinions with built-in conflict-of-interest safeguards, establishes a transparent and reliable system for ethical assessment.\n    *   **Iterative Feedback and Revision Mechanism:** The provision of constructive feedback, recommendations, and the option for applicants to revise and resubmit, or engage in personal discussions, is a significant contribution to supporting researchers in navigating complex ethical considerations and achieving compliance.\n\n**5. Experimental Validation**\n*   No experimental validation was conducted. This document is a procedural guide detailing the operational framework of an ethics committee, not a research paper presenting empirical findings or evaluating a novel technical system through experimentation.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:** The GEHBa's jurisdiction is geographically limited to Bavarian universities and their collaborators; institutions with their own ethics committees should be approached first. Applications for projects already underway are generally not reviewed, with specific exceptions for retrospective or secondary analyses of fully anonymized data. The GEHBa issues an ethical \"Votum\" (statement/assessment) rather than an outright \"rejection,\" focusing on providing recommendations for improvement.\n*   **Scope of Applicability:** The guidelines apply broadly to both medical and non-medical research involving human participants, particularly where potential risks, burdens, or ethical principle violations are present, or when vulnerable groups are involved.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** While not advancing a scientific or engineering state-of-the-art, the document significantly advances the *state-of-the-art in ethical governance and procedural clarity* for research within its specific academic region. It provides a robust, transparent, and supportive framework for ethical review.\n*   **Potential Impact on Future Research:**\n    *   **Enhanced Ethical Rigor:** By clearly outlining requirements and offering an iterative review process, it promotes higher ethical standards in research, thereby increasing the trustworthiness and societal acceptance of research outcomes.\n    *   **Streamlined Compliance:** It simplifies the process for researchers to comply with complex ethical and data protection regulations (e.g., GDPR), potentially reducing administrative burden and project delays.\n    *   **Educational Resource:** Serves as an invaluable educational tool for students and early-career researchers, embedding ethical considerations into research design from the outset.",
        "keywords": [
            "Ethical review process",
            "Human participants research",
            "Procedural framework",
            "Standardized documentation",
            "Informed consent",
            "Data protection (GDPR)",
            "Vulnerable populations",
            "Iterative review mechanism",
            "Ethical governance",
            "Bavarian academic institutions",
            "WMA Declaration of Helsinki",
            "GEHBa (Joint Ethics Committee)",
            "Research ethics compliance"
        ],
        "paper_type": "the provided \"abstract\" and \"introduction\" content is a faq document from the gehba (joint ethics committee of bavarian universities) regarding ethics applications. this content does not match the provided \"metadata\" (title: \"chatgpt\u2019s performance in german ob/gyn exams...\").\n\nbased *strictly* on the provided \"abstract\" and \"introduction\" content, which outlines guidelines and principles for submitting ethics applications, it is not a typical research paper. it is an informational document stating rules and the committee's stance.\n\namong the given classification criteria for research papers, the best, albeit imperfect, fit is:\n\n6.  **position** - argues for viewpoint or future direction\n    *   the document presents the official viewpoint and guidelines of the gehba regarding when and why ethics applications are required, and the ethical principles involved. while it doesn't \"argue\" in the sense of debating a new idea, it formally states the committee's established \"position\" on research ethics.\n\nthe other categories are clearly not applicable:\n*   it is not a review of literature (**survey**).\n*   it does not present new methods or algorithms (**technical**).\n*   it does not involve mathematical analysis or proofs (**theoretical**).\n*   it is not a data-driven study with statistical analysis (**empirical**).\n*   it is not a detailed analysis of a specific application (**case_study**).\n*   there is no indication it is a brief communication or work-in-progress (**short**).\n\ntherefore, classifying the provided content as a **position** paper is the closest fit, interpreting \"viewpoint\" as the official stance and guidelines of an organization.\n\n**classification:** position"
    },
    "3015956a254139547cb350f5dbdd8edde298ac0d.pdf": {
        "title": "Accelerating clinical evidence synthesis with large language models",
        "authors": [
            "Zifeng Wang",
            "Lang Cao",
            "Benjamin P. Danek",
            "Yichi Zhang",
            "Qiao Jin",
            "Zhiyong Lu",
            "Jimeng Sun"
        ],
        "published_date": "2024",
        "abstract": "Clinical evidence synthesis largely relies on systematic reviews (SR) of clinical studies from medical literature. Here, we propose a generative artificial intelligence (AI) pipeline named TrialMind to streamline study search, study screening, and data extraction tasks in SR. We chose published SRs to build TrialReviewBench, which contains 100 SRs and 2,220 clinical studies. For study search, it achieves high recall rates (Ours 0.711\u20130.834 v.s. Human baseline 0.138\u20130.232). For study screening, TrialMind beats previous document ranking methods in a 1.5\u20132.6 fold change. For data extraction, it outperforms a GPT-4\u2019s accuracy by 16\u201332%. In a pilot study, human-AI collaboration with TrialMind improved recall by 71.4% and reduced screening time by 44.2%, while in data extraction, accuracy increased by 23.5% with a 63.4% time reduction. Medical experts preferred TrialMind\u2019s synthesized evidence over GPT-4\u2019s in 62.5%-100% of cases. These findings show the promise of accelerating clinical evidence synthesis driven by human-AI collaboration.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/3015956a254139547cb350f5dbdd8edde298ac0d.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{wang2024p5f}\n\n1.  **Research Problem & Motivation**\n    *   **Problem**: The process of synthesizing clinical evidence, primarily through systematic reviews, is highly inefficient, time-consuming, and expensive. It struggles to keep pace with the rapid expansion of medical literature, leading to outdated evidence and challenges in efficiently identifying, summarizing, and updating crucial clinical information \\cite{wang2024p5f}.\n    *   **Importance & Challenge**: Clinical evidence is vital for guiding clinical practices and advancing drug development, requiring regular updates. However, systematic reviews typically demand significant human expert effort (e.g., 5 experts, 67.3 weeks) and are quickly rendered obsolete by the vast and continuously growing volume of publications (e.g., PubMed adds over 1 million citations annually), necessitating a streamlined approach \\cite{wang2024p5f}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: Previous research has explored the application of Large Language Models (LLMs) for individual tasks within the evidence synthesis process, such as generating search queries, extracting PICO elements, screening citations, and summarizing findings \\cite{wang2024p5f}.\n    *   **Limitations of Previous Solutions**: A critical gap exists in investigating LLMs' effectiveness *across the entire evidence synthesis process* in a holistic manner. This lack of integrated solutions hinders seamless AI integration, overall efficiency, and a comprehensive understanding of LLM capabilities. Furthermore, LLMs often exhibit limitations such as hallucinations, weakness in numerical reasoning, overly generic outputs, and a lack of transparency and reliability, which are significant concerns in high-stakes medical applications \\cite{wang2024p5f}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces `TrialMind`, a generative AI pipeline designed to facilitate human-AI collaboration across three crucial tasks for evidence synthesis: study search, screening, and data extraction \\cite{wang2024p5f}.\n    *   **Novelty/Difference**:\n        *   **Holistic Pipeline Integration**: `TrialMind` provides an end-to-end LLM-driven pipeline that covers the entire systematic review workflow, from initial literature search to evidence synthesis, ensuring seamless AI assistance \\cite{wang2024p5f}.\n        *   **Decomposition for Human-AI Collaboration**: It decomposes complex evidence synthesis tasks into smaller, verifiable subtasks that adhere to established systematic review practices. This design allows human experts to monitor, edit, and verify intermediate outputs, mitigating LLM limitations like hallucinations and enhancing transparency and reliability \\cite{wang2024p5f}.\n        *   **Advanced Query Generation**: For study search, `TrialMind` employs a pipeline involving query generation, augmentation, and refinement, coupled with user adjustment capabilities, to produce comprehensive Boolean queries that overcome the incompleteness often seen in direct LLM query generation \\cite{wang2024p5f}.\n        *   **Structured and Source-Linked Data Extraction**: It extracts specific information (e.g., study characteristics, clinical outcomes) into structured data formats, with each output linked directly to its source document for manual inspection, ensuring data accuracy and integrity \\cite{wang2024p5f}.\n        *   **Meta-analysis Readiness**: The system standardizes extracted numerical clinical outcomes, making them directly usable as input for meta-analysis tools and visualizations like forest plots \\cite{wang2024p5f}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A multi-stage LLM-driven pipeline for comprehensive clinical evidence synthesis, featuring a robust query generation and augmentation module for literature search, an eligibility criteria generation and prediction module for screening, and a structured data extraction module with source linking \\cite{wang2024p5f}.\n    *   **System Design/Architectural Innovations**:\n        *   The `TrialMind` architecture is specifically designed for human-AI collaboration, breaking down the systematic review process into transparent, verifiable steps where experts can intervene, monitor, and refine outputs, thereby enhancing the reliability and trustworthiness of AI-assisted evidence synthesis \\cite{wang2024p5f}.\n    *   **Dataset Contribution**:\n        *   Introduction of `TrialReviewBench`, a novel benchmark dataset comprising 100 systematic reviews and 2,220 associated clinical studies. This dataset includes manual annotations for 1,334 study characteristics and 1,049 study results, specifically created to evaluate LLMs across the full spectrum of evidence synthesis tasks \\cite{wang2024p5f}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Performance evaluation of `TrialMind` on the `TrialReviewBench` dataset across study search, screening, and data extraction tasks \\cite{wang2024p5f}.\n        *   User studies comparing the efficiency and quality of AI-assisted experts versus standalone experts in study screening and data extraction \\cite{wang2024p5f}.\n        *   Human evaluation by medical experts to assess the quality of synthesized clinical evidence (e.g., forest plots) generated by `TrialMind` compared to a GPT-4 baseline \\cite{wang2024p5f}.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Study Search**: `TrialMind` achieved an average recall of 0.782, significantly outperforming a GPT-4 baseline (0.073) and a Human baseline (0.187). It consistently maintained high recall rates (0.711-0.834) across various topics, even with a large number of target studies \\cite{wang2024p5f}.\n        *   **Study Screening**: `TrialMind` surpassed traditional embedding-based methods (MedCPT, MPNet) by 30% to 160% in Recall@k metrics (e.g., 2.6x fold change for Recall@20 in Immunotherapy) \\cite{wang2024p5f}.\n        *   **Data Extraction**: `TrialMind` outperformed a GPT-4 baseline by 29.6% to 61.5% in accuracy (e.g., 0.739 vs. 0.570 for study characteristics, 0.711 vs. 0.439 for study results) \\cite{wang2024p5f}.\n        *   **Human-AI Collaboration (User Study)**: In study screening, `TrialMind` yielded a 71.4% recall lift and 44.2% time savings. In data extraction, it achieved a 23.5% accuracy lift and 63.4% time savings compared to manual efforts \\cite{wang2024p5f}.\n        *   **Expert Preference**: Medical experts preferred `TrialMind`'s synthesized evidence (forest plots) over GPT-4's in 62.5% to 100% of cases, indicating higher perceived quality and reliability \\cite{wang2024p5f}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The system's design emphasizes human-in-the-loop collaboration, implying that full automation is not yet feasible or desirable for critical clinical evidence synthesis tasks, and expert oversight remains essential for verification and refinement \\cite{wang2024p5f}.\n    *   **Scope of Applicability**: The experimental validation primarily focuses on systematic reviews related to cancer treatments across four major topics (Immunotherapy, Radiation/Chemotherapy, Hormone Therapy, and Hyperthermia). Its generalizability to other medical domains or different types of evidence synthesis would require further investigation \\cite{wang2024p5f}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `TrialMind` represents a significant advancement by demonstrating the practical feasibility and substantial benefits of integrating LLMs into a comprehensive, human-AI collaborative pipeline for clinical evidence synthesis. It moves beyond isolated task automation to a holistic, verifiable workflow, addressing key limitations of LLMs in complex, high-stakes medical domains \\cite{wang2024p5f}.\n    *   **Potential Impact on Future Research**: This work lays a strong foundation for future research in developing more robust, transparent, and human-centric AI systems for medical literature analysis. It highlights the transformative potential of LLMs to accelerate the generation of timely and accurate clinical evidence, which can profoundly impact clinical practice, drug development, and healthcare policy, ultimately improving patient outcomes. The `TrialReviewBench` dataset also provides a valuable resource for further LLM development and evaluation in this critical area \\cite{wang2024p5f}.",
        "keywords": [
            "TrialMind",
            "Generative AI pipeline",
            "Clinical evidence synthesis",
            "Human-AI collaboration",
            "Systematic reviews",
            "Large Language Models (LLMs)",
            "TrialReviewBench",
            "Evidence synthesis tasks",
            "Holistic pipeline integration",
            "Structured data extraction",
            "Meta-analysis readiness",
            "Efficiency and accuracy improvements",
            "Transparency and reliability"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"here, we introduce trialmind, a generative artificial intelligence (ai) pipeline...\"** - this explicitly states the presentation of a new system or method.\n2.  the abstract details the components and functionality of trialmind for \"study search, screening, and data extraction.\"\n3.  the introduction sets up a \"technical problem\" (inefficient and time-consuming systematic reviews) and positions llms as a \"proposed solution.\"\n4.  while the paper includes extensive \"empirical\" evaluation (building a benchmark dataset, reporting quantitative results, conducting user studies), these evaluations are performed *on the newly introduced system* to demonstrate its performance and utility. the primary contribution is the development and presentation of the trialmind pipeline itself.\n\ntherefore, the paper is best classified as **technical**."
    },
    "8135f7dd37df7cf4ad61756fc1152c792be3d821.pdf": {
        "title": "The ethical requirement of explainability for AI-DSS in healthcare: a systematic review of reasons",
        "authors": [
            "Nils Freyer",
            "Dominik Gro\u00df",
            "Myriam Lipprandt"
        ],
        "published_date": "2024",
        "abstract": "Background Despite continuous performance improvements, especially in clinical contexts, a major challenge of Artificial Intelligence based Decision Support Systems (AI-DSS) remains their degree of epistemic opacity. The conditions of and the solutions for the justified use of the occasionally unexplainable technology in healthcare are an active field of research. In March 2024, the European Union agreed upon the Artificial Intelligence Act (AIA), requiring medical AI-DSS to be ad-hoc explainable or to use post-hoc explainability methods. The ethical debate does not seem to settle on this requirement yet. This systematic review aims to outline and categorize the positions and arguments in the ethical debate. Methods We conducted a literature search on PubMed, BASE, and Scopus for English-speaking scientific peer-reviewed publications from 2016 to 2024. The inclusion criterion was to give explicit requirements of explainability for AI-DSS in healthcare and reason for it. Non-domain-specific documents, as well as surveys, reviews, and meta-analyses were excluded. The ethical requirements for explainability outlined in the documents were qualitatively analyzed with respect to arguments for the requirement of explainability and the required level of explainability. Results The literature search resulted in 1662 documents; 44 documents were included in the review after eligibility screening of the remaining full texts. Our analysis showed that 17 records argue in favor of the requirement of explainable AI methods (xAI) or ad-hoc explainable models, providing 9 categories of arguments. The other 27 records argued against a general requirement, providing 11 categories of arguments. Also, we found that 14 works advocate the need for context-dependent levels of explainability, as opposed to 30 documents, arguing for context-independent, absolute standards. Conclusions The systematic review of reasons shows no clear agreement on the requirement of post-hoc explainability methods or ad-hoc explainable models for AI-DSS in healthcare. The arguments found in the debate were referenced and responded to from different perspectives, demonstrating an interactive discourse. Policymakers and researchers should watch the development of the debate closely. Conversely, ethicists should be well informed by empirical and technical research, given the frequency of advancements in the field. Supplementary Information The online version contains supplementary material available at 10.1186/s12910-024-01103-2.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/8135f7dd37df7cf4ad61756fc1152c792be3d821.pdf",
        "venue": "BMC Medical Ethics",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing its technical aspects as a systematic review and its validation through literature analysis:\n\n**CITATION**: \\cite{freyer2024s2t}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the ethical challenge posed by the \"epistemic opacity\" (inaccessibility of internal processes and attributes) of Artificial Intelligence-based Decision Support Systems (AI-DSS), particularly deep learning models, when deployed in clinical healthcare contexts.\n    *   **Importance & Challenge:** This problem is crucial because AI-DSS offer significant performance improvements but raise profound ethical and regulatory concerns regarding trustworthiness, human oversight, and accountability. Regulatory frameworks, such as the EU AI Act (AIA), mandate explainability (either intrinsically explainable \"ad-hoc\" models or \"post-hoc\" Explainable AI (xAI) methods) for high-risk medical AI systems. However, the ethical debate on whether explainability is a necessary requirement, and to what degree, remains unsettled and characterized by terminological inconsistencies, creating a \"Collingridge dilemma\" where technological advancement outpaces ethical consensus.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work systematically reviews and synthesizes the diverse ethical arguments and positions within the ongoing debate on AI explainability in healthcare. It positions itself as a comprehensive mapping of the current discourse, including the implications of emerging regulatory frameworks like the EU AI Act.\n    *   **Limitations of Previous Solutions:** The paper implicitly highlights the limitations of a fragmented ethical discussion by aiming to provide a structured, categorized overview. It explicitly excludes other literature reviews and surveys to avoid redundancy and ensure a novel synthesis of primary ethical arguments, thereby addressing the lack of a consolidated \"review of reasons\" in this specific domain.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm:** The core method is a **systematic review of reasons**, conducted according to PRISMA standards. This involved a rigorous literature search across PubMed, BASE, and Scopus for English-speaking, peer-reviewed publications from 2016 to 2024. The search string combined terms related to ethics, explainability (and its synonyms), healthcare, and AI/machine learning. Included documents were qualitatively analyzed to identify explicit requirements for AI-DSS explainability, the reasons provided for these requirements, and the advocated levels of explainability (relative vs. absolute).\n    *   **Novelty/Difference:** While systematic reviews are a standard methodology, the innovation lies in its specific application to systematically map and categorize *ethical arguments and normative standards* concerning AI explainability in healthcare. The paper's explicit definition of key terminology (explainability, transparency, ad-hoc, post-hoc) provides a crucial common ground for analysis, addressing existing \"terminological incoherences.\" The resulting categorization of arguments for and against explainability, and the differentiation of context-dependent versus absolute standards, represents a novel analytical framework for understanding this complex ethical landscape.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:** The primary contribution is the application of a systematic review of reasons to comprehensively map the ethical arguments surrounding AI explainability in healthcare. This method provides a structured approach to identifying, analyzing, and categorizing diverse ethical positions and their justifications.\n    *   **System Design or Architectural Innovations:** Not applicable in the traditional sense of an AI system. However, the *analytical framework* developed for categorizing arguments (e.g., 9 categories for proponents, 11 for opponents, and a distinction between context-dependent and absolute standards of explainability) constitutes an innovation in structuring and understanding the ethical discourse.\n    *   **Theoretical Insights or Analysis:** The review provides a critical synthesis, revealing \"no clear agreement\" on the requirement of post-hoc explainability methods or ad-hoc explainable models. It highlights prominent arguments such as the \"double standard argument\" (comparing AI opacity to atheoretic evidence-based medicine) and the \"fool's gold\" critique (post-hoc xAI methods introduce false security or bias), alongside their counter-arguments. It also categorizes the factors influencing context-dependent explainability (e.g., risks/benefits, best practices, patient/HCP values).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The \"experiment\" was the rigorous execution of the systematic literature search and qualitative analysis methodology. This involved screening 1662 initial documents, identifying 1524 unique records, and ultimately including 44 eligible documents for in-depth analysis.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Argument Distribution:** The analysis found 27 documents arguing *against* a general requirement for explainability (presenting 11 categories of arguments), and 17 documents arguing *in favor* (presenting 9 categories of arguments).\n        *   **Explainability Levels:** 14 works advocated for context-dependent levels of explainability, while 30 argued for context-independent, absolute standards.\n        *   **Temporal Trend:** The high number of publications in recent years (16 in 2022, 10 in 2023) empirically validates the claim that the debate is active and unsettled.\n        *   **Specific Argument Identification:** The review successfully identified and detailed key arguments such as the \"double standard argument\" (comparing AI opacity to existing opaque medical practices) and the \"fool's gold\" argument (critiquing the reliability and potential for bias in xAI methods).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The authors acknowledge potential limitations, including the use of single screening (which might miss up to 13% of relevant records) and the exclusion of non-English literature, which could introduce a language bias. The review also explicitly excludes technical/empirical literature that only implicitly touches ethical questions, focusing strictly on explicit ethical arguments.\n    *   **Scope of Applicability:** The review is specifically scoped to the ethical requirements of explainability for AI-DSS within the *healthcare domain*. It focuses on the philosophical-ethical debate, deliberately excluding broader empirical studies on AI acceptance or purely technical discussions without explicit ethical considerations.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This systematic review significantly advances the technical state-of-the-art in understanding the ethical landscape of AI in healthcare by providing a structured, evidence-based map of the complex and often contradictory arguments surrounding explainability. It moves beyond anecdotal or fragmented discussions to a categorized, comprehensive synthesis.\n    *   **Potential Impact on Future Research:**\n        *   **Informing Policy and Regulation:** The findings directly inform policymakers (e.g., those implementing the EU AI Act) about the lack of consensus and the nuanced complexity of the ethical debate, underscoring the need for careful monitoring of its development.\n        *   **Guiding Interdisciplinary Research:** It highlights the critical need for ethicists to be \"well informed by empirical and technical research,\" fostering interdisciplinary collaboration to bridge the gap between rapid AI advancements and ethical framework development.\n        *   **Identifying Research Gaps:** By clearly outlining the specific arguments and counter-arguments, the review implicitly identifies areas where further ethical, philosophical, and empirical research is needed to resolve disagreements, strengthen justifications, or develop more robust ethical guidelines for AI-DSS.\n        *   **Standardizing Terminology:** The explicit definitions provided contribute to a more coherent and precise discourse in future research on AI ethics.",
        "keywords": [
            "AI-based Decision Support Systems (AI-DSS)",
            "epistemic opacity",
            "AI explainability",
            "Explainable AI (xAI) methods",
            "clinical healthcare ethics",
            "systematic review of reasons",
            "ethical arguments categorization",
            "EU AI Act",
            "no clear agreement on explainability",
            "double standard argument",
            "fool's gold critique",
            "context-dependent explainability",
            "terminological inconsistencies",
            "deep learning models"
        ],
        "paper_type": "**survey**\n\n**reasoning:**\nthe title explicitly states \"the ethical requirement of explainability for ai-dss in healthcare: a **systematic review** of reasons.\"\nthe abstract clearly states: \"this **systematic review** aims to outline and categorize the positions and arguments in the ethical debate.\"\nthe methods section describes a \"literature search\" and qualitative analysis of documents, which are characteristic activities of a review paper.\n\nthese elements directly align with the criteria for a **survey** paper: \"reviews existing literature comprehensively\" and \"abstract mentions: 'survey', 'review'\"."
    },
    "f51c5f6f2a5454b117bb2cb0e7ac7a9faeb65012.pdf": {
        "title": "MediGPT: Exploring Potentials of Conventional and Large Language Models on Medical Data",
        "authors": [
            "Mohammad Abu Tareq Rony",
            "Mohammad Shariful Islam",
            "Tipu Sultan",
            "S. Alshathri",
            "W. El-shafai"
        ],
        "published_date": "2024",
        "abstract": "Medical text classification organizes medical documents into categories to streamline information retrieval and support clinical decision-making. Traditional machine learning techniques, including pre-trained language models, are effective but require extensive domain-specific training data, often underperform across languages, and are costly and complex to deploy on a large scale. In this study, we employed four datasets: Clinical trials on cancer, encompassing 6 million statements from interventional cancer clinical trial protocols; the Illness-dataset, consisting of 22,660 categorized tweets from 2018 and 2019; the Multi-View active learning for short medical text classification in user-generated data, an extended version of the Illness-dataset including 22,660 documents from the same period; and the Symptom2Disease dataset, containing 1,200 data points used to predict diseases based on symptom descriptions. This study uses ChatGPT, particularly its ChatGPT-3.5 and ChatGPT-4 versions, as a viable alternative for classifying medical texts. We investigate essential aspects, including the construction of prompts, the parsing of responses, and the various strategic use of GPT models to optimize outcomes. Through comparative analysis with established methods like pre-trained language model fine-tuning and prompt-tuning, our findings indicate that ChatGPT addresses these challenges efficiently and matches the performance of traditional methods. Furthermore, the enhanced capabilities of the proposed MediGPT (Medical Generative Pre-Trained Transformers) have led to performance improvements of 14.3%, 22.3%, 13.6%, and 13.7% across the datasets, highlighting its adaptability and robustness in diverse medical text scenarios without the need for specialized domain adjustments. This research underscores the capability of ChatGPT to facilitate a versatile AI framework in medical text processing, which could revolutionize medical informatics practices.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/f51c5f6f2a5454b117bb2cb0e7ac7a9faeb65012.pdf",
        "venue": "IEEE Access",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Medical text classification organizes medical documents into categories to streamline information retrieval and support clinical decision-making. Traditional machine learning techniques, including pre-trained language models, are effective but require extensive domain-specific training data, often underperform across languages, and are costly and complex to deploy on a large scale. In this study, we employed four datasets: Clinical trials on cancer, encompassing 6 million statements from interventional cancer clinical trial protocols; the Illness-dataset, consisting of 22,660 categorized tweets from 2018 and 2019; the Multi-View active learning for short medical text classification in user-generated data, an extended version of the Illness-dataset including 22,660 documents from the same period; and the Symptom2Disease dataset, containing 1,200 data points used to predict diseases based on symptom descriptions. This study uses ChatGPT, particularly its ChatGPT-3.5 and ChatGPT-4 versions, as a viable alternative for classifying medical texts. We investigate essential aspects, including the construction of prompts, the parsing of responses, and the various strategic use of GPT models to optimize outcomes. Through comparative analysis with established methods like pre-trained language model fine-tuning and prompt-tuning, our findings indicate that ChatGPT addresses these challenges efficiently and matches the performance of traditional methods. Furthermore, the enhanced capabilities of the proposed MediGPT (Medical Generative Pre-Trained Transformers) have led to performance improvements of 14.3%, 22.3%, 13.6%, and 13.7% across the datasets, highlighting its adaptability and robustness in diverse medical text scenarios without the need for specialized domain adjustments. This research underscores the capability of ChatGPT to facilitate a versatile AI framework in medical text processing, which could revolutionize medical informatics practices.",
        "keywords": []
    },
    "b2515168dd17240480a0462360d8e48ab2fc2639.pdf": {
        "title": "Large Language Models in the Clinic: A Comprehensive Benchmark",
        "authors": [
            "Andrew Liu",
            "Hongjian Zhou",
            "Yining Hua",
            "Omid Rohanian",
            "Anshul Thakur",
            "Lei A. Clifton",
            "David A. Clifton"
        ],
        "published_date": "2024",
        "abstract": "The adoption of large language models (LLMs) to assist clinicians has attracted remarkable attention. Existing works mainly adopt the close-ended question-answering (QA) task with answer options for evaluation. However, many clinical decisions involve answering open-ended questions without pre-set options. To better understand LLMs in the clinic, we construct a benchmark ClinicBench. We first collect eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks. Furthermore, we construct six novel datasets and clinical tasks that are complex but common in real-world practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis. We conduct an extensive evaluation of twenty-two LLMs under both zero-shot and few-shot settings. Finally, we invite medical experts to evaluate the clinical usefulness of LLMs. The benchmark data is available at https://github.com/AI-in-Health/ClinicBench.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/b2515168dd17240480a0462360d8e48ab2fc2639.pdf",
        "venue": "",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical need for a comprehensive and realistic evaluation of Large Language Models (LLMs) in clinical settings.\n    *   Existing evaluations are limited, primarily focusing on close-ended, exam-style Question-Answering (QA) tasks with pre-set options. This overlooks many real-world clinical decisions that involve open-ended questions, complex language generation, understanding, and reasoning.\n    *   The problem is important because LLMs show promise in healthcare, but their true capabilities and limitations in diverse, complex clinical scenarios (e.g., long document processing, emerging drug analysis) are not well understood, hindering safe and effective adoption.\n\n*   **Related Work & Positioning**\n    *   Previous works mainly evaluate LLMs on close-ended (exam-style) QA tasks, often using non-clinical machine learning tasks from existing benchmarks like BLUE and BLURB.\n    *   Limitations of previous solutions include:\n        *   **Limited evaluation scope**: Neglecting clinical language understanding and generation.\n        *   **Limited task complexity**: Focusing on non-clinical tasks that don't adequately assess complex clinical problems like treatment recommendation or emerging drug analysis.\n        *   **Limited comparison**: Often using only a few LLMs (e.g., LLaMA, GPT-3.5, GPT-4) or providing limited qualitative examples, lacking thorough comparative analysis across diverse LLMs and scenarios.\n    *   This work positions itself by constructing a novel, comprehensive benchmark (`ClinicBench`) that addresses these limitations through diverse tasks, novel clinical datasets, and extensive LLM evaluation.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the construction of `ClinicBench`, a comprehensive benchmark for evaluating LLMs in clinical contexts.\n    *   **Novelty/Innovation**:\n        *   **Expanded Task & Dataset Scope**: `ClinicBench` integrates eleven existing datasets covering diverse clinical language generation, understanding, and reasoning tasks, and *introduces six novel datasets and clinical tasks*.\n        *   **Focus on Real-world Clinical Complexity**: The six novel tasks are specifically designed to evaluate LLMs in complex, common real-world clinical problems:\n            *   Open-ended decision-making (e.g., Treatment Recommendation, Referral QA).\n            *   Long document processing (e.g., Hospitalization Summarization, Patient Education, requiring processing 2,000-3,000 word documents).\n            *   Emerging drug analysis (e.g., Pharmacology QA for Emerging Drugs, Drug Interaction for Emerging Drugs).\n        *   **Extensive LLM Evaluation**: It conducts an extensive evaluation of twenty-two diverse LLMs (11 general, 11 medical, ranging from 7B to 70B parameters, including both open-source and closed-source commercial models).\n        *   **Multi-setting Evaluation**: Evaluation is performed under both zero-shot and few-shot (1-shot, 3-shot, 5-shot) settings.\n        *   **Human Expert Validation**: Medical experts are invited to evaluate the clinical usefulness of LLM responses, providing crucial qualitative insights beyond automatic metrics.\n        *   **Fine-tuning Data Analysis**: Preliminary exploration of using clinical-standard knowledge bases for fine-tuning medical LLMs, analyzing the effect of different fine-tuning data types and diversity.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark (`ClinicBench`)**: Construction of `ClinicBench` comprising 3 scenarios (reasoning, generation, understanding), 11 tasks, and 17 datasets (over 20,000 test samples) for benchmarking 22 LLMs.\n    *   **Six Novel Clinical Datasets and Tasks**: Development of specific datasets and tasks tailored to complex, common clinical problems: open-ended decision-making, long document processing, and new drug analysis.\n    *   **Comprehensive LLM Evaluation**: Extensive quantitative evaluation of 22 LLMs across zero-shot and few-shot settings using automatic metrics (Accuracy, F1, ROUGE-L).\n    *   **Human Evaluation Framework**: Integration of human expert evaluation to assess the clinical usefulness, factuality, safety, completeness, and user preference of LLM outputs.\n    *   **Insights into Fine-tuning Data**: Preliminary analysis demonstrating the importance of clinical-standard knowledge bases and diverse instruction fine-tuning (IFT) data for developing effective medical LLMs.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Automatic evaluation of 22 LLMs (11 general, 11 medical) on 17 datasets across 11 tasks.\n        *   Evaluation under zero-shot, 1-shot, 3-shot, and 5-shot settings.\n        *   Human evaluation by medical experts on clinical usefulness, factuality, safety, completeness, and user preference.\n        *   Analysis of the impact of different instruction fine-tuning (IFT) data types and diversity.\n    *   **Key Performance Metrics**: Accuracy (for QA, classification), F1 (for NER, RE, treatment recommendation), ROUGE-L (for summarization, generation).\n    *   **Comparison Results (Key Findings)**:\n        *   **Commercial LLMs (e.g., GPT-4)** consistently outperform all existing open-source LLMs across all tasks and datasets.\n        *   LLMs achieve superior performance, comparable to human experts and outperforming task-specific SOTA, *only on close-ended exam-style QA tasks*.\n        *   LLMs perform *poorly* in open-ended decision-making, generation, and understanding tasks, demonstrating a significant performance drop on complex clinical tasks.\n        *   **Medical LLMs** generally adapt better to clinical tasks than general LLMs and produce more factual and safe responses, but may decrease summarization ability.\n        *   **Few-shot learning** improves reasoning and generation (1-shot/3-shot best for reasoning, more shots for generation) but impairs understanding performance.\n        *   A certain degree of hallucination from LLMs might offer benefits in providing broader diagnostic suggestions for rare diseases.\n        *   **Instruction Fine-tuning**: Diverse IFT data, especially clinical-standard knowledge bases, is crucial for developing better medical LLMs, highlighting diversity's importance over mere quantity.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: Current LLMs perform poorly in complex, open-ended clinical tasks, long document processing, and emerging drug analysis, indicating a gap in their capabilities for real-world clinical application beyond exam-style QA.\n    *   **Scope of Applicability**: While `ClinicBench` is comprehensive, it represents a specific set of clinical tasks and datasets. The findings are specific to the evaluated LLMs and prompt engineering strategies used. The exploration of fine-tuning data is preliminary.\n\n*   **Technical Significance**\n    *   `\\cite{liu20245q5}` significantly advances the technical state-of-the-art by providing the first comprehensive benchmark (`ClinicBench`) that moves beyond traditional close-ended QA to evaluate LLMs on complex, real-world clinical tasks.\n    *   It offers a holistic view of LLMs' capabilities and limitations in healthcare, identifying critical areas where current models fall short (e.g., open-ended decision-making, long document processing, emerging drug analysis).\n    *   The benchmark, extensive evaluation, and human expert validation provide crucial insights for future research, guiding the development of more robust, clinically useful, and safe medical LLMs.\n    *   The findings on fine-tuning data highlight the importance of diverse, clinical-standard knowledge for improving medical LLM performance, impacting future model training strategies.\n    *   This work aims to bridge current gaps and accelerate the responsible integration of LLMs into clinical applications by providing a rigorous evaluation framework.",
        "keywords": [
            "Large Language Models (LLMs)",
            "Clinical settings",
            "ClinicBench",
            "Comprehensive LLM evaluation",
            "Open-ended clinical tasks",
            "Novel clinical datasets",
            "Long document processing",
            "Emerging drug analysis",
            "Human expert validation",
            "Instruction fine-tuning (IFT)",
            "Medical LLMs",
            "Performance limitations",
            "Diverse IFT data",
            "Zero-shot and few-shot learning"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **new system/method:** the abstract explicitly states, \"to better understand llms in the clinic, we **construct a benchmark clinicbench**.\" it further details, \"we **construct six novel datasets** and clinical tasks.\" the creation of a new benchmark and novel datasets for evaluation constitutes the development of a new system or method for assessing llms, which aligns with the \"technical\" criteria.\n2.  **problem and proposed solution:** the abstract identifies a problem with existing evaluation methods (\"existing works mainly adopt the close-ended question-answering (qa) task... however, many clinical decisions involve answering open-ended questions...\") and proposes a concrete technical solution: the clinicbench benchmark.\n3.  **evaluation as application:** while the paper *conducts an extensive evaluation* (which is an empirical activity), this evaluation is performed *using* the newly constructed benchmark. the benchmark itself is the primary technical contribution that enables this empirical study. the title \"a comprehensive benchmark\" also emphasizes the benchmark as the core artifact."
    },
    "1e62d8b0db0731179b6513ae442fe29bc8f4eb2d.pdf": {
        "title": "Automating Evaluation of AI Text Generation in Healthcare with a Large Language Model (LLM)-as-a-Judge",
        "authors": [
            "E. Croxford",
            "Yanjun Gao",
            "Elliot First",
            "Nicholas Pellegrino",
            "Miranda Schnier",
            "J. Caskey",
            "M. Oguss",
            "Graham Wills",
            "Guanhua Chen",
            "D. Dligach",
            "Matthew M. Churpek",
            "Anoop M. Mayampurath",
            "Frank J Liao",
            "Cherodeep Goswami",
            "Karen K. Wong",
            "Brian W Patterson",
            "Majid Afshar"
        ],
        "published_date": "2025",
        "abstract": "Electronic Health Records (EHRs) store vast amounts of clinical information that are difficult for healthcare providers to summarize and synthesize relevant details to their practice. To reduce cognitive load on providers, generative AI with Large Language Models have emerged to automatically summarize patient records into clear, actionable insights and offload the cognitive burden for providers. However, LLM summaries need to be precise and free from errors, making evaluations on the quality of the summaries necessary. While human experts are the gold standard for evaluations, their involvement is time-consuming and costly. Therefore, we introduce and validate an automated method for evaluating real-world EHR multi-document summaries using an LLM as the evaluator, referred to as LLM-as-a-Judge. Benchmarking against the validated Provider Documentation Summarization Quality Instrument (PDSQI)-9 for human evaluation, our LLM-as-a-Judge framework uses the PDSQI-9 rubric and demonstrated strong inter-rater reliability with human evaluators. GPT-o3-mini achieved the highest intraclass correlation coefficient of 0.818 (95% CI 0.772, 0.854), with a median score difference of 0 from human evaluators, and completes evaluations in just 22 seconds. Overall, the reasoning models excelled in inter-rater reliability, particularly in evaluations that require advanced reasoning and domain expertise, outperforming non-reasoning models, those trained on the task, and multi-agent workflows. Cross-task validation on the Problem Summarization task similarly confirmed high reliability. By automating high-quality evaluations, medical LLM-as-a-Judge offers a scalable, efficient solution to rapidly identify accurate and safe AI-generated summaries in healthcare settings.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/1e62d8b0db0731179b6513ae442fe29bc8f4eb2d.pdf",
        "venue": "medRxiv",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Here is a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Evaluating the quality, precision, and safety of AI-generated clinical summaries from Electronic Health Records (EHRs) is critical for their safe implementation in healthcare \\cite{croxford2025sz2}.\n    *   **Importance & Challenge:** EHRs contain vast amounts of data, leading to clinician information overload. Generative AI (LLMs) can summarize this data, but their outputs are vulnerable to errors like hallucinations, omissions, and inaccuracies, especially in multi-document clinical contexts with \"lost-in-the-middle\" effects \\cite{croxford2025sz2}. While human expert evaluation is the gold standard, it is time-consuming, costly, and resource-intensive, creating a significant bottleneck for scalable deployment \\cite{croxford2025sz2}. Traditional automated metrics (e.g., ROUGE, BERTScore) are inadequate for the nuanced, contextual, and abstractive nature of clinical language, lacking sensitivity to factual accuracy, logical coherence, and clinical relevance \\cite{croxford2025sz2}.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Human evaluation by clinical experts is the gold standard but is resource-intensive and slow (e.g., 10 minutes per evaluation using PDSQI-9) \\cite{croxford2025sz2}. Traditional automated metrics (ROUGE, BERTScore) are designed for simpler natural language tasks and poorly correlate with human evaluations in the medical domain, failing to capture clinical nuances or abstractive summarization complexities \\cite{croxford2025sz2}.\n    *   **Limitations of Previous Solutions:** Current human evaluation practices often lack psychometrically validated instruments for real-world, multi-document EHR data, and existing instruments like PDQI-9 were not designed to capture LLM-specific phenomena like hallucinations \\cite{croxford2025sz2}. The recently developed PDSQI-9 instrument addresses these gaps for human evaluation but still requires significant human time and expertise \\cite{croxford2025sz2}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** The paper introduces and validates an \"LLM-as-a-Judge\" framework for automated evaluation of real-world EHR multi-document summaries \\cite{croxford2025sz2}. This framework leverages LLMs' contextual comprehension and reasoning capabilities to automate evaluations traditionally performed by human experts \\cite{croxford2025sz2}.\n    *   **Novelty:** The approach is novel in its application and rigorous validation of LLM-as-a-Judge in the high-stakes medical domain, specifically for multi-document clinical summarization, using a psychometrically validated human evaluation instrument (PDSQI-9) as the benchmark \\cite{croxford2025sz2}. It systematically evaluates state-of-the-art open- and closed-source LLMs using various prompting strategies (zero-shot, few-shot, supervised fine-tuning (SFT), direct preference optimization (DPO)) and multi-agent workflows \\cite{croxford2025sz2}. The PDSQI-9 instrument itself was adapted to capture LLM-specific issues like hallucinations and omissions \\cite{croxford2025sz2}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework:** Development and validation of a medical LLM-as-a-Judge framework capable of evaluating multi-document EHR summaries with high inter-rater reliability compared to human experts \\cite{croxford2025sz2}.\n    *   **Evaluation Methodology:** Systematic evaluation of different LLM models and prompting strategies (zero-shot, few-shot, SFT, DPO, multi-agent) for the LLM-as-a-Judge task in a clinical context \\cite{croxford2025sz2}.\n    *   **Performance Benchmarking:** Identification of specific LLM configurations (e.g., GPT-o3-mini with few-shot prompting) that achieve superior performance, particularly in tasks requiring advanced reasoning and domain expertise \\cite{croxford2025sz2}.\n    *   **Cross-Task Validation:** Demonstration of the framework's generalizability through successful cross-task validation on a separate medical summarization task (ProbSum 2023 Shared Task) \\cite{croxford2025sz2}.\n\n*   **Experimental Validation**\n    *   **Experiments:** The study evaluated single LLM-as-a-Judge models and multi-agent LLM-as-a-Judge frameworks using various prompting strategies (zero-shot, few-shot, SFT, DPO) \\cite{croxford2025sz2}. Cross-task validation was performed on the Problem List BioNLP Summarization (ProbSum) 2023 Shared Task \\cite{croxford2025sz2}.\n    *   **Datasets:** The primary evaluation used a corpus of 200 real-world EHR multi-document summaries (160 train/development, 40 test) from the original PDSQI-9 study, along with human expert evaluation scores \\cite{croxford2025sz2}. The cross-task validation used 31 summaries from the MIMIC-III EHR database for the ProbSum task \\cite{croxford2025sz2}.\n    *   **Metrics:** The primary outcome was the Intraclass Correlation Coefficient (ICC) to assess agreement between LLM-as-a-Judge and human evaluators, complemented by Krippendorf\u2019s \u03b1 and Gwet\u2019s Ac2, and median score differences \\cite{croxford2025sz2}.\n    *   **Key Results:**\n        *   GPT-o3-mini (2024-01-31) 5-shot achieved the highest ICC of 0.818 (95% CI 0.772, 0.854) with a median score difference of 0 from human evaluators (p-value <0.001) \\cite{croxford2025sz2}.\n        *   This top-performing model completed evaluations in just 22 seconds, significantly faster than human evaluators \\cite{croxford2025sz2}.\n        *   Reasoning models generally excelled in inter-rater reliability, outperforming non-reasoning models, those trained solely on the task, and multi-agent workflows \\cite{croxford2025sz2}.\n        *   Smaller open-source models (e.g., Llama 3.1 8B) showed substantial performance gains with SFT and DPO (ICC improved from 0.332 to 0.560) \\cite{croxford2025sz2}.\n        *   The best multi-agent approach achieved an ICC of 0.768 (95% CI 0.710, 0.814) \\cite{croxford2025sz2}.\n        *   Cross-task validation with GPT-o3-mini yielded an ICC of 0.710 (95% CI 0.662, 0.752) \\cite{croxford2025sz2}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** While effective, the multi-agent framework did not surpass the performance of the best single LLM-as-a-Judge model \\cite{croxford2025sz2}. Smaller open-source models required additional fine-tuning (SFT, DPO) to achieve competitive performance, indicating a need for significant training resources for certain models \\cite{croxford2025sz2}.\n    *   **Scope of Applicability:** The framework is validated for multi-document EHR summarization and problem list summarization in specific clinical contexts \\cite{croxford2025sz2}. Further validation across diverse clinical specialties, patient populations, and LLM-generated summary types would broaden its established applicability.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing a scalable, efficient, and reliable automated evaluation method for AI-generated clinical summaries \\cite{croxford2025sz2}. It bridges the gap between the need for rigorous evaluation in high-stakes domains and the resource constraints of human expert review \\cite{croxford2025sz2}.\n    *   **Potential Impact:** The LLM-as-a-Judge framework offers a practical solution to rapidly identify accurate and safe AI-generated summaries, thereby accelerating the safe deployment and iteration of generative AI in healthcare settings \\cite{croxford2025sz2}. It enables continuous quality improvement of clinical LLMs by providing high-quality, automated feedback, which is crucial for reducing cognitive load on providers and improving patient care \\cite{croxford2025sz2}.",
        "keywords": [
            "AI-generated clinical summaries",
            "Electronic Health Records (EHRs)",
            "LLM-as-a-Judge framework",
            "automated evaluation",
            "multi-document clinical summarization",
            "human expert evaluation",
            "PDSQI-9 instrument",
            "inter-rater reliability",
            "prompting strategies",
            "generative AI in healthcare",
            "hallucinations and omissions",
            "scalable deployment",
            "cross-task validation",
            "Intraclass Correlation Coefficient (ICC)"
        ],
        "paper_type": "the paper introduces and validates a new automated method called \"llm-as-a-judge\" for evaluating ai-generated text in healthcare. it describes the problem (difficulty in summarizing ehrs, need for evaluation), proposes a specific solution (the llm-as-a-judge method), and then presents empirical results from benchmarking and validation against human evaluators, including statistical measures like intraclass correlation coefficient and score differences.\n\nthis aligns best with the **technical** classification criteria:\n*   **abstract mentions:** \"we introduce and validate an automated method\", \"llm-as-a-judge framework\". this directly corresponds to \"propose\", \"develop\", \"present\", \"method\".\n*   **introduction discusses:** the technical problem (information overload, need for efficient evaluation) and the proposed solution (generative ai with llms).\n\nwhile the paper also contains strong empirical elements (benchmarking, statistical analysis, findings), these are presented as the validation of the *new method* being introduced. the core contribution is the development and presentation of this novel evaluation method.\n\n**classification: technical**"
    },
    "70d1805367e053588ad237d6e3d7aaf08b4e42b9.pdf": {
        "title": "\u201cDr. AI Will See You Now\u201d: How Do ChatGPT-4 Treatment Recommendations Align With Orthopaedic Clinical Practice Guidelines?",
        "authors": [
            "Tanios Dagher",
            "Emma Dwyer",
            "Hayden P. Baker",
            "Senthooran Kalidoss",
            "Jason A. Strelzow"
        ],
        "published_date": "2024",
        "abstract": "Background Artificial intelligence (AI) is engineered to emulate tasks that have historically required human interaction and intellect, including learning, pattern recognition, decision-making, and problem-solving. Although AI models like ChatGPT-4 have demonstrated satisfactory performance on medical licensing exams, suggesting a potential for supporting medical diagnostics and decision-making, no study of which we are aware has evaluated the ability of these tools to make treatment recommendations when given clinical vignettes and representative medical imaging of common orthopaedic conditions. As AI continues to advance, a thorough understanding of its strengths and limitations is necessary to inform safe and helpful integration into medical practice. Questions/purposes (1) What is the concordance between ChatGPT-4-generated treatment recommendations for common orthopaedic conditions with both the American Academy of Orthopaedic Surgeons (AAOS) clinical practice guidelines (CPGs) and an orthopaedic attending physician\u2019s treatment plan? (2) In what specific areas do the ChatGPT-4-generated treatment recommendations diverge from the AAOS CPGs? Methods Ten common orthopaedic conditions with associated AAOS CPGs were identified: carpal tunnel syndrome, distal radius fracture, glenohumeral joint osteoarthritis, rotator cuff injury, clavicle fracture, hip fracture, hip osteoarthritis, knee osteoarthritis, ACL injury, and acute Achilles rupture. For each condition, the medical records of 10 deidentified patients managed at our facility were used to construct clinical vignettes that each had an isolated, single diagnosis with adequate clarity. The vignettes also encompassed a range of diagnostic severity to evaluate more thoroughly adherence to the treatment guidelines outlined by the AAOS. These clinical vignettes were presented alongside representative radiographic imaging. The model was prompted to provide a single treatment plan recommendation. Each treatment plan was compared with established AAOS CPGs and to the treatment plan documented by the attending orthopaedic surgeon treating the specific patient. Vignettes where ChatGPT-4 recommendations diverged from CPGs were reviewed to identify patterns of error and summarized. Results ChatGPT-4 provided treatment recommendations in accordance with the AAOS CPGs in 90% (90 of 100) of clinical vignettes. Concordance between ChatGPT-generated plans and the plan recommended by the treating orthopaedic attending physician was 78% (78 of 100). One hundred percent (30 of 30) of ChatGPT-4 recommendations for fracture vignettes and hip and knee arthritis vignettes matched with CPG recommendations, whereas the model struggled most with recommendations for carpal tunnel syndrome (3 of 10 instances demonstrated discordance). ChatGPT-4 recommendations diverged from AAOS CPGs for three carpal tunnel syndrome vignettes; two ACL injury, rotator cuff injury, and glenohumeral joint osteoarthritis vignettes; as well as one acute Achilles rupture vignette. In these situations, ChatGPT-4 most often struggled to correctly interpret injury severity and progression, incorporate patient factors (such as lifestyle or comorbidities) into decision-making, and recognize a contraindication to surgery. Conclusion ChatGPT-4 can generate accurate treatment plans aligned with CPGs but can also make mistakes when it is required to integrate multiple patient factors into decision-making and understand disease severity and progression. Physicians must critically assess the full clinical picture when using AI tools to support their decision-making. Clinical Relevance ChatGPT-4 may be used as an on-demand diagnostic companion, but patient-centered decision-making should continue to remain in the hands of the physician.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/70d1805367e053588ad237d6e3d7aaf08b4e42b9.pdf",
        "venue": "Clinical Orthopaedics and Related Research",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Background Artificial intelligence (AI) is engineered to emulate tasks that have historically required human interaction and intellect, including learning, pattern recognition, decision-making, and problem-solving. Although AI models like ChatGPT-4 have demonstrated satisfactory performance on medical licensing exams, suggesting a potential for supporting medical diagnostics and decision-making, no study of which we are aware has evaluated the ability of these tools to make treatment recommendations when given clinical vignettes and representative medical imaging of common orthopaedic conditions. As AI continues to advance, a thorough understanding of its strengths and limitations is necessary to inform safe and helpful integration into medical practice. Questions/purposes (1) What is the concordance between ChatGPT-4-generated treatment recommendations for common orthopaedic conditions with both the American Academy of Orthopaedic Surgeons (AAOS) clinical practice guidelines (CPGs) and an orthopaedic attending physician\u2019s treatment plan? (2) In what specific areas do the ChatGPT-4-generated treatment recommendations diverge from the AAOS CPGs? Methods Ten common orthopaedic conditions with associated AAOS CPGs were identified: carpal tunnel syndrome, distal radius fracture, glenohumeral joint osteoarthritis, rotator cuff injury, clavicle fracture, hip fracture, hip osteoarthritis, knee osteoarthritis, ACL injury, and acute Achilles rupture. For each condition, the medical records of 10 deidentified patients managed at our facility were used to construct clinical vignettes that each had an isolated, single diagnosis with adequate clarity. The vignettes also encompassed a range of diagnostic severity to evaluate more thoroughly adherence to the treatment guidelines outlined by the AAOS. These clinical vignettes were presented alongside representative radiographic imaging. The model was prompted to provide a single treatment plan recommendation. Each treatment plan was compared with established AAOS CPGs and to the treatment plan documented by the attending orthopaedic surgeon treating the specific patient. Vignettes where ChatGPT-4 recommendations diverged from CPGs were reviewed to identify patterns of error and summarized. Results ChatGPT-4 provided treatment recommendations in accordance with the AAOS CPGs in 90% (90 of 100) of clinical vignettes. Concordance between ChatGPT-generated plans and the plan recommended by the treating orthopaedic attending physician was 78% (78 of 100). One hundred percent (30 of 30) of ChatGPT-4 recommendations for fracture vignettes and hip and knee arthritis vignettes matched with CPG recommendations, whereas the model struggled most with recommendations for carpal tunnel syndrome (3 of 10 instances demonstrated discordance). ChatGPT-4 recommendations diverged from AAOS CPGs for three carpal tunnel syndrome vignettes; two ACL injury, rotator cuff injury, and glenohumeral joint osteoarthritis vignettes; as well as one acute Achilles rupture vignette. In these situations, ChatGPT-4 most often struggled to correctly interpret injury severity and progression, incorporate patient factors (such as lifestyle or comorbidities) into decision-making, and recognize a contraindication to surgery. Conclusion ChatGPT-4 can generate accurate treatment plans aligned with CPGs but can also make mistakes when it is required to integrate multiple patient factors into decision-making and understand disease severity and progression. Physicians must critically assess the full clinical picture when using AI tools to support their decision-making. Clinical Relevance ChatGPT-4 may be used as an on-demand diagnostic companion, but patient-centered decision-making should continue to remain in the hands of the physician.",
        "keywords": []
    },
    "96fe238568d192a30dc7ddd528ea35b23ab1bf70.pdf": {
        "title": "Updating the Checklist for Artificial Intelligence in Medical Imaging (CLAIM) for reporting AI research",
        "authors": [
            "Ali S. Tejani",
            "M. Klontzas",
            "Anthony A Gatti",
            "John Mongan",
            "Linda Moy",
            "Seong Ho Park",
            "Charles E. Kahn"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/96fe238568d192a30dc7ddd528ea35b23ab1bf70.pdf",
        "venue": "Nature Machine Intelligence",
        "citationCount": 19,
        "score": 9.5,
        "summary": "",
        "keywords": []
    },
    "98f16006f9492a77cba380f8c9e7eaf514092389.pdf": {
        "title": "Ethical dimensions of generative AI: a cross-domain analysis using machine learning structural topic modeling",
        "authors": [
            "Hassnian Ali",
            "A. Aysan"
        ],
        "published_date": "2024",
        "abstract": "\nPurpose\nThe purpose of this study is to comprehensively examine the ethical implications surrounding generative artificial intelligence (AI).\n\n\nDesign/methodology/approach\nLeveraging a novel methodological approach, the study curates a corpus of 364 documents from Scopus spanning 2022 to 2024. Using the term frequency-inverse document frequency (TF-IDF) and structural topic modeling (STM), it quantitatively dissects the thematic essence of the ethical discourse in generative AI across diverse domains, including education, healthcare, businesses and scientific research.\n\n\nFindings\nThe results reveal a diverse range of ethical concerns across various sectors impacted by generative AI. In academia, the primary focus is on issues of authenticity and intellectual property, highlighting the challenges of AI-generated content in maintaining academic integrity. In the healthcare sector, the emphasis shifts to the ethical implications of AI in medical decision-making and patient privacy, reflecting concerns about the reliability and security of AI-generated medical advice. The study also uncovers significant ethical discussions in educational and financial settings, demonstrating the broad impact of generative AI on societal and professional practices.\n\n\nResearch limitations/implications\nThis study provides a foundation for crafting targeted ethical guidelines and regulations for generative AI, informed by a systematic analysis using STM. It highlights the need for dynamic governance and continual monitoring of AI\u2019s evolving ethical landscape, offering a model for future research and policymaking in diverse fields.\n\n\nOriginality/value\nThe study introduces a unique methodological combination of TF-IDF and STM to analyze a large academic corpus, offering new insights into the ethical implications of generative AI across multiple domains.\n",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/98f16006f9492a77cba380f8c9e7eaf514092389.pdf",
        "venue": "International Journal of Ethics and Systems",
        "citationCount": 9,
        "score": 9.0,
        "summary": "\nPurpose\nThe purpose of this study is to comprehensively examine the ethical implications surrounding generative artificial intelligence (AI).\n\n\nDesign/methodology/approach\nLeveraging a novel methodological approach, the study curates a corpus of 364 documents from Scopus spanning 2022 to 2024. Using the term frequency-inverse document frequency (TF-IDF) and structural topic modeling (STM), it quantitatively dissects the thematic essence of the ethical discourse in generative AI across diverse domains, including education, healthcare, businesses and scientific research.\n\n\nFindings\nThe results reveal a diverse range of ethical concerns across various sectors impacted by generative AI. In academia, the primary focus is on issues of authenticity and intellectual property, highlighting the challenges of AI-generated content in maintaining academic integrity. In the healthcare sector, the emphasis shifts to the ethical implications of AI in medical decision-making and patient privacy, reflecting concerns about the reliability and security of AI-generated medical advice. The study also uncovers significant ethical discussions in educational and financial settings, demonstrating the broad impact of generative AI on societal and professional practices.\n\n\nResearch limitations/implications\nThis study provides a foundation for crafting targeted ethical guidelines and regulations for generative AI, informed by a systematic analysis using STM. It highlights the need for dynamic governance and continual monitoring of AI\u2019s evolving ethical landscape, offering a model for future research and policymaking in diverse fields.\n\n\nOriginality/value\nThe study introduces a unique methodological combination of TF-IDF and STM to analyze a large academic corpus, offering new insights into the ethical implications of generative AI across multiple domains.\n",
        "keywords": []
    },
    "4a9d3e39ecafb0487b54731d6d99c46770ce5965.pdf": {
        "title": "AI-powered topic modeling: comparing LDA and BERTopic in analyzing opioid-related cardiovascular risks in women",
        "authors": [
            "Li Ma",
            "Ru Chen",
            "W. Ge",
            "Paul Rogers",
            "Beverly Lyn-Cook",
            "H. Hong",
            "Weida Tong",
            "Ningning Wu",
            "Wen Zou"
        ],
        "published_date": "2025",
        "abstract": "Topic modeling is a crucial technique in natural language processing (NLP), enabling the extraction of latent themes from large text corpora. Traditional topic modeling, such as Latent Dirichlet Allocation (LDA), faces limitations in capturing the semantic relationships in the text document although it has been widely applied in text mining. BERTopic, created in 2022, leveraged advances in deep learning and can capture the contextual relationships between words. In this work, we integrated Artificial Intelligence (AI) modules to LDA and BERTopic and provided a comprehensive comparison on the analysis of prescription opioid-related cardiovascular risks in women. Opioid use can increase the risk of cardiovascular problems in women such as arrhythmia, hypotension etc. 1,837 abstracts were retrieved and downloaded from PubMed as of April 2024 using three Medical Subject Headings (MeSH) words: \u201copioid,\u201d \u201ccardiovascular,\u201d and \u201cwomen.\u201d Machine Learning of Language Toolkit (MALLET) was employed for the implementation of LDA. BioBERT was used for document embedding in BERTopic. Eighteen was selected as the optimal topic number for MALLET and 23 for BERTopic. ChatGPT-4-Turbo was integrated to interpret and compare the results. The short descriptions created by ChatGPT for each topic from LDA and BERTopic were highly correlated, and the performance accuracies of LDA and BERTopic were similar as determined by expert manual reviews of the abstracts grouped by their predominant topics. The results of the t-SNE (t-distributed Stochastic Neighbor Embedding) plots showed that the clusters created from BERTopic were more compact and well-separated, representing improved coherence and distinctiveness between the topics. Our findings indicated that AI algorithms could augment both traditional and contemporary topic modeling techniques. In addition, BERTopic has the connection port for ChatGPT-4-Turbo or other large language models in its algorithm for automatic interpretation, while with LDA interpretation must be manually, and needs special procedures for data pre-processing and stop words exclusion. Therefore, while LDA remains valuable for large-scale text analysis with resource constraints, AI-assisted BERTopic offers significant advantages in providing the enhanced interpretability and the improved semantic coherence for extracting valuable insights from textual data.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/4a9d3e39ecafb0487b54731d6d99c46770ce5965.pdf",
        "venue": "Experimental biology and medicine",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Topic modeling is a crucial technique in natural language processing (NLP), enabling the extraction of latent themes from large text corpora. Traditional topic modeling, such as Latent Dirichlet Allocation (LDA), faces limitations in capturing the semantic relationships in the text document although it has been widely applied in text mining. BERTopic, created in 2022, leveraged advances in deep learning and can capture the contextual relationships between words. In this work, we integrated Artificial Intelligence (AI) modules to LDA and BERTopic and provided a comprehensive comparison on the analysis of prescription opioid-related cardiovascular risks in women. Opioid use can increase the risk of cardiovascular problems in women such as arrhythmia, hypotension etc. 1,837 abstracts were retrieved and downloaded from PubMed as of April 2024 using three Medical Subject Headings (MeSH) words: \u201copioid,\u201d \u201ccardiovascular,\u201d and \u201cwomen.\u201d Machine Learning of Language Toolkit (MALLET) was employed for the implementation of LDA. BioBERT was used for document embedding in BERTopic. Eighteen was selected as the optimal topic number for MALLET and 23 for BERTopic. ChatGPT-4-Turbo was integrated to interpret and compare the results. The short descriptions created by ChatGPT for each topic from LDA and BERTopic were highly correlated, and the performance accuracies of LDA and BERTopic were similar as determined by expert manual reviews of the abstracts grouped by their predominant topics. The results of the t-SNE (t-distributed Stochastic Neighbor Embedding) plots showed that the clusters created from BERTopic were more compact and well-separated, representing improved coherence and distinctiveness between the topics. Our findings indicated that AI algorithms could augment both traditional and contemporary topic modeling techniques. In addition, BERTopic has the connection port for ChatGPT-4-Turbo or other large language models in its algorithm for automatic interpretation, while with LDA interpretation must be manually, and needs special procedures for data pre-processing and stop words exclusion. Therefore, while LDA remains valuable for large-scale text analysis with resource constraints, AI-assisted BERTopic offers significant advantages in providing the enhanced interpretability and the improved semantic coherence for extracting valuable insights from textual data.",
        "keywords": []
    },
    "9f69378a966de40ccb9a267df45ac1a926d8f0b4.pdf": {
        "title": "The 2024 revision of the Declaration of Helsinki: a modern ethical framework for medical research.",
        "authors": [
            "Boyuan Wen",
            "Guochao Zhang",
            "Chang Zhan",
            "Chen Chen",
            "Hang Yi"
        ],
        "published_date": "2024",
        "abstract": "The Declaration of Helsinki, established in 1964, remains a foundational document in medical research ethics. This review examines the 2024 revision, endorsed by the 75th World Medical Association (WMA) Assembly, highlighting its impact on modern clinical research. Major updates include the shift from \"subjects\" to \"participants,\" promoting autonomy and active involvement, and the introduction of dual ethical review requirements for cross-border studies to strengthen accountability. New guidelines for data privacy address AI-related ethical concerns, while enhanced community engagement fosters transparency and shared decision-making. Additionally, standards for environmental sustainability encourage research practices that minimize ecological impacts. In response to global health crises such as COVID-19, the revised Declaration sets forth ethical protections to balance participant safety with research urgency during emergencies. Despite these advances, areas for improvement remain, especially in AI ethics, emergency research protocols, and the extension the Declaration's scope to include forensic and specimen research. The 2024 revision thus strengthens the Declaration's role as an adaptive, relevant framework for safeguarding participant rights and research integrity in a changing landscape.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/9f69378a966de40ccb9a267df45ac1a926d8f0b4.pdf",
        "venue": "Postgraduate medical journal",
        "citationCount": 8,
        "score": 8.0,
        "summary": "The Declaration of Helsinki, established in 1964, remains a foundational document in medical research ethics. This review examines the 2024 revision, endorsed by the 75th World Medical Association (WMA) Assembly, highlighting its impact on modern clinical research. Major updates include the shift from \"subjects\" to \"participants,\" promoting autonomy and active involvement, and the introduction of dual ethical review requirements for cross-border studies to strengthen accountability. New guidelines for data privacy address AI-related ethical concerns, while enhanced community engagement fosters transparency and shared decision-making. Additionally, standards for environmental sustainability encourage research practices that minimize ecological impacts. In response to global health crises such as COVID-19, the revised Declaration sets forth ethical protections to balance participant safety with research urgency during emergencies. Despite these advances, areas for improvement remain, especially in AI ethics, emergency research protocols, and the extension the Declaration's scope to include forensic and specimen research. The 2024 revision thus strengthens the Declaration's role as an adaptive, relevant framework for safeguarding participant rights and research integrity in a changing landscape.",
        "keywords": []
    },
    "91925f988424a5ab55c5e3cf36cb55fea0b96ea8.pdf": {
        "title": "Promoting AI Competencies for Medical Students: A Scoping Review on Frameworks, Programs, and Tools",
        "authors": [
            "Yingbo Ma",
            "Yukyeong Song",
            "Jeremy A. Balch",
            "Yuanfang Ren",
            "Divya Vellanki",
            "Zhenhong Hu",
            "Meghan Brennan",
            "Suraj Kolla",
            "Ziyuan Guan",
            "Brooke Armfield",
            "T. Ozrazgat-Baslanti",
            "Parisa Rashidi",
            "Tyler J. Loftus",
            "A. Bihorac",
            "Benjamin Shickel"
        ],
        "published_date": "2024",
        "abstract": "As more clinical workflows continue to be augmented by artificial intelligence (AI), AI literacy among physicians will become a critical requirement for ensuring safe and ethical AI-enabled patient care. Despite the evolving importance of AI in healthcare, the extent to which it has been adopted into traditional and often-overloaded medical curricula is currently unknown. In a scoping review of 1,699 articles published between January 2016 and June 2024, we identified 18 studies which propose guiding frameworks, and 11 studies documenting real-world instruction, centered around the integration of AI into medical education. We found that comprehensive guidelines will require greater clinical relevance and personalization to suit medical student interests and career trajectories. Current efforts highlight discrepancies in the teaching guidelines, emphasizing AI evaluation and ethics over technical topics such as data science and coding. Additionally, we identified several challenges associated with integrating AI training into the medical education program, including a lack of guidelines to define medical students AI literacy, a perceived lack of proven clinical value, and a scarcity of qualified instructors. With this knowledge, we propose an AI literacy framework to define competencies for medical students. To prioritize relevant and personalized AI education, we categorize literacy into four dimensions: Foundational, Practical, Experimental, and Ethical, with tailored learning objectives to the pre-clinical, clinical, and clinical research stages of medical education. This review provides a road map for developing practical and relevant education strategies for building an AI-competent healthcare workforce.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/91925f988424a5ab55c5e3cf36cb55fea0b96ea8.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Medical students lack adequate AI competencies, which is crucial for safe and ethical AI-enabled patient care, despite the increasing integration of AI into clinical workflows \\cite{ma2024ggj}. There is no clear, consensus-driven AI literacy framework specifically tailored for medical students.\n    *   **Importance and Challenge**: AI literacy is becoming a critical requirement for future physicians. Existing AI literacy frameworks are often designed for K-12 students or the general public, not addressing the specific needs of medical students. Integrating new AI content into already overloaded medical curricula faces resistance due to a lack of consensus on implementation, perceived lack of proven clinical value, and a scarcity of qualified instructors \\cite{ma2024ggj}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This paper conducts a comprehensive scoping review of existing theoretical proposals/guidelines and practical teaching efforts (courses, programs, tools) for AI education in medical students \\cite{ma2024ggj}. It is positioned as the first review to comprehensively assess both theoretical guidelines and practical teaching efforts in this specific domain.\n    *   **Limitations of Previous Solutions**: Previous solutions lack consensus and best practices for AI education in medical curricula. Existing literature often focuses on general perspectives rather than specific teaching frameworks or educational programs \\cite{ma2024ggj}. The review identifies discrepancies where theoretical guidelines emphasize AI evaluation and ethics, while practical teaching often places less emphasis on technical topics like data science, coding, or critical skills for leveraging AI effectively \\cite{ma2024ggj}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**: The core method is a systematic scoping review (guided by PRISMA) of 1,699 articles published between 2016 and 2024, identifying 29 relevant studies \\cite{ma2024ggj}. Based on the synthesis of these findings, the paper proposes a novel AI literacy framework specifically designed for medical students.\n    *   **Novelty/Difference**: The key innovation is the proposed AI literacy framework, which categorizes competencies into four dimensions: Foundational, Practical, Experimental, and Ethical \\cite{ma2024ggj}. This framework is unique in its tailoring of learning objectives to the specific stages of medical education (pre-clinical, clinical, and clinical research), providing a personalized and clinically relevant roadmap for AI education.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A rigorous scoping review methodology to systematically map the landscape of AI education for medical students \\cite{ma2024ggj}.\n        *   A novel, stage-specific AI literacy framework for medical students, structured into Foundational, Practical, Experimental, and Ethical dimensions, offering a structured approach to curriculum development \\cite{ma2024ggj}.\n    *   **Theoretical Insights or Analysis**:\n        *   Identification of significant discrepancies between theoretical guidelines and actual practical implementation in AI education for medical students \\cite{ma2024ggj}.\n        *   Highlighting the current overemphasis in guidelines on AI evaluation and ethics, with a comparative underemphasis on technical topics such as data science and coding in practical teaching \\cite{ma2024ggj}.\n        *   Identification of critical challenges in integrating AI education: lack of defined AI literacy, perceived lack of clinical value, and scarcity of qualified instructors \\cite{ma2024ggj}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The paper conducted a scoping review of 1,699 articles, ultimately including 29 studies (18 proposing theoretical guidelines and 11 documenting real-world instruction) \\cite{ma2024ggj}. Data was extracted and synthesized to answer two main questions: (1) existing theoretical proposals/guidelines and (2) existing practical teachings.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Theoretical Guidelines (18 studies)**: Found strong emphasis on \"Know and Understand AI\" (17/18 studies), \"Use and Apply AI\" (16/18 studies), and \"AI Ethics\" (all 18 studies), but less on \"Evaluate and Create AI\" (7/18 studies) \\cite{ma2024ggj}. Specific sub-fields like deep learning, computer vision, and NLP were rarely recommended.\n        *   **Practical Teachings (11 studies)**: Most programs were elective (8/11), offering flexibility. Common topics included introduction to AI/ML (6/11), understanding/using AI applications in medicine (7/11), and AI safety/ethics (6/11) \\cite{ma2024ggj}. However, there was less emphasis on programming skills for creating AI models (3/11) or critically evaluating AI models (1/11). Instructor expertise was predominantly university faculty/AI researchers. Assessments primarily focused on self-reported knowledge uptake, interest, and confidence, with only one study using an objective AI knowledge test \\cite{ma2024ggj}. Overall, programs enhanced students' knowledge, confidence, and interest in AI.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The review is limited by the scope of published literature (2016-2024) and the specific inclusion/exclusion criteria \\cite{ma2024ggj}. The proposed framework is a synthesis of existing knowledge and requires empirical validation in real-world medical curricula.\n    *   **Scope of Applicability**: The findings and the proposed framework are specifically applicable to medical students and institutions involved in medical education \\cite{ma2024ggj}. The review also highlights limitations in existing practical programs, such as small sample sizes, overreliance on self-reported confidence assessments, and self-selection bias in elective enrollment \\cite{ma2024ggj}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: This paper significantly advances the technical state-of-the-art by providing the first comprehensive assessment of both theoretical and practical AI education for medical students, identifying critical gaps and discrepancies \\cite{ma2024ggj}. The proposed AI literacy framework offers a structured, personalized, and clinically relevant roadmap, moving beyond general AI literacy to define specific competencies for future physicians.\n    *   **Potential Impact on Future Research**: The framework and findings will guide researchers and medical educators in developing more effective, tailored, and integrated AI curricula. It provides a foundation for future research into curriculum design, pedagogical methods, and assessment strategies for AI education in medicine, ultimately aiming to prepare an AI-competent healthcare workforce for safe and ethical AI utilization \\cite{ma2024ggj}.",
        "keywords": [
            "AI literacy for medical students",
            "AI competencies",
            "systematic scoping review",
            "novel AI literacy framework",
            "Foundational Practical Experimental Ethical dimensions",
            "stage-specific medical education",
            "discrepancies in AI education",
            "AI-enabled patient care",
            "medical curricula integration",
            "data science and coding skills",
            "AI ethics",
            "healthcare workforce preparedness"
        ],
        "paper_type": "based on the provided abstract and introduction:\n\nthe abstract explicitly states: \"in a **scoping review** of 1,699 articles published between january 2016 and june 2024, we identified 18 studies...\" and concludes with \"this **review provides a road map** for developing practical and relevant education strategies...\".\n\nthe classification criteria for **survey** include:\n*   abstract mentions: \"**survey**\", \"**review**\", \"comprehensive analysis\"\n*   introduction discusses: literature organization, classification schemes (implicitly, by identifying existing studies and their discrepancies)\n\nwhile the paper does \"propose an ai literacy framework\" (which could lean towards \"technical\" or \"position\"), this proposal is a direct *outcome* and *synthesis* of the comprehensive review of existing literature. the primary methodology and contribution described is the systematic review of existing frameworks, programs, and tools.\n\ntherefore, the most fitting classification is **survey**."
    },
    "074382848cdfa466136392c9a106561972180d0f.pdf": {
        "title": "Documenting the de-identification process of clinical and imaging data for AI for health imaging projects",
        "authors": [
            "H. Kondylakis",
            "Rocio Catalan",
            "Sara Martinez Alabart",
            "Caroline Barelle",
            "Paschalis A. Bizopoulos",
            "Maciej Bobowicz",
            "Jonathan Bona",
            "Dimitrios I. Fotiadis",
            "Teresa Garcia",
            "Ignacio Gomez",
            "Ana Jim\u00e9nez-Pastor",
            "Giannis Karatzanis",
            "Karim Lekadir",
            "Magdalena Kogut-Czarkowska",
            "Antonios Lalas",
            "K. Marias",
            "Luis Mart\u00ed-Bonmat\u00ed",
            "Jose Munuera",
            "K. Nikiforaki",
            "Manon Pelissier",
            "Fred Prior",
            "Michael Rutherford",
            "Laure Saint-Aubert",
            "Zisis Sakellariou",
            "K. Seymour",
            "Thomas Trouillard",
            "Konstantinos Votis",
            "M. Tsiknakis"
        ],
        "published_date": "2024",
        "abstract": "Abstract Artificial intelligence (AI) is revolutionizing the field of medical imaging, holding the potential to shift medicine from a reactive \u201csick-care\u201d approach to a proactive focus on healthcare and prevention. The successful development of AI in this domain relies on access to large, comprehensive, and standardized real-world datasets that accurately represent diverse populations and diseases. However, images and data are sensitive, and as such, before using them in any way the data needs to be modified to protect the privacy of the patients. This paper explores the approaches in the domain of five EU projects working on the creation of ethically compliant and GDPR-regulated European medical imaging platforms, focused on cancer-related data. It presents the individual approaches to the de-identification of imaging data, and describes the problems and the solutions adopted in each case. Further, lessons learned are provided, enabling future projects to optimally handle the problem of data de-identification. Critical relevance statement This paper presents key approaches from five flagship EU projects for the de-identification of imaging and clinical data offering valuable insights and guidelines in the domain. Key Points \u0391\u0399 models for health imaging require access to large amounts of data. Access to large imaging datasets requires an appropriate de-identification process. This paper provides de-identification guidelines from the AI for health imaging (AI4HI) projects.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/074382848cdfa466136392c9a106561972180d0f.pdf",
        "venue": "Insights into Imaging",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Abstract Artificial intelligence (AI) is revolutionizing the field of medical imaging, holding the potential to shift medicine from a reactive \u201csick-care\u201d approach to a proactive focus on healthcare and prevention. The successful development of AI in this domain relies on access to large, comprehensive, and standardized real-world datasets that accurately represent diverse populations and diseases. However, images and data are sensitive, and as such, before using them in any way the data needs to be modified to protect the privacy of the patients. This paper explores the approaches in the domain of five EU projects working on the creation of ethically compliant and GDPR-regulated European medical imaging platforms, focused on cancer-related data. It presents the individual approaches to the de-identification of imaging data, and describes the problems and the solutions adopted in each case. Further, lessons learned are provided, enabling future projects to optimally handle the problem of data de-identification. Critical relevance statement This paper presents key approaches from five flagship EU projects for the de-identification of imaging and clinical data offering valuable insights and guidelines in the domain. Key Points \u0391\u0399 models for health imaging require access to large amounts of data. Access to large imaging datasets requires an appropriate de-identification process. This paper provides de-identification guidelines from the AI for health imaging (AI4HI) projects.",
        "keywords": []
    },
    "127b5fdae3d321cbf92a0ab39e97709afafe2191.pdf": {
        "title": "AI-based disease category prediction model using symptoms from low-resource Ethiopian language: Afaan Oromo text",
        "authors": [
            "Etana Fikadu Dinsa",
            "Mrinal Das",
            "Teklu Urgessa Abebe"
        ],
        "published_date": "2024",
        "abstract": "Automated disease diagnosis and prediction, powered by AI, play a crucial role in enabling medical professionals to deliver effective care to patients. While such predictive tools have been extensively explored in resource-rich languages like English, this manuscript focuses on predicting disease categories automatically from symptoms documented in the Afaan Oromo language, employing various classification algorithms. This study encompasses machine learning techniques such as support vector machines, random forests, logistic regression, and Na\u00efve Bayes, as well as deep learning approaches including LSTM, GRU, and Bi-LSTM. Due to the unavailability of a standard corpus, we prepared three data sets with different numbers of patient symptoms arranged into 10 categories. The two feature representations, TF-IDF and word embedding, were employed. The performance of the proposed methodology has been evaluated using accuracy, recall, precision, and F1 score. The experimental results show that, among machine learning models, the SVM model using TF-IDF had the highest accuracy and F1 score of 94.7%, while the LSTM model using word2vec embedding showed an accuracy rate of 95.7% and F1 score of 96.0% from deep learning models. To enhance the optimal performance of each model, several hyper-parameter tuning settings were used. This study shows that the LSTM model verifies to be the best of all the other models over the entire dataset.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/127b5fdae3d321cbf92a0ab39e97709afafe2191.pdf",
        "venue": "Scientific Reports",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n---\n\n### Focused Summary for Literature Review\n\n**1. Research Problem & Motivation**\n*   **Specific technical problem:** The paper addresses the challenge of automatically predicting disease categories from patient symptoms documented in the Afaan Oromo language.\n*   **Importance and challenge:**\n    *   Automated disease diagnosis and prediction are crucial for enabling medical professionals to deliver effective care.\n    *   While such predictive tools are extensively explored in resource-rich languages (e.g., English), there is a significant gap in their availability and development for low-resource languages like Afaan Oromo.\n    *   Afaan Oromo has a Subject-Object-Verb (SOV) sentence structure, which differs from English (SVO), making direct application of existing English-based models ineffective.\n    *   A major challenge is the unavailability of a standard, organized Afaan Oromo health text corpus suitable for disease diagnosis and classification.\n\n**2. Related Work & Positioning**\n*   **Relation to existing approaches:**\n    *   Previous research in Afaan Oromo has focused on tasks such as hate speech detection, emotion detection, and sentiment analysis, utilizing various machine learning (ML) and deep learning (DL) models \\cite{dinsa2024kdb}.\n    *   Studies in English and other languages (e.g., Bengali) have explored ML and DL methods for patient symptom analysis, health status classification, and sentiment analysis in healthcare contexts \\cite{dinsa2024kdb}.\n*   **Limitations of previous solutions:**\n    *   The literature review explicitly concludes that \"there is no disease category prediction model for Afaan Oromo health text documents available\" \\cite{dinsa2024kdb}.\n    *   A primary limitation identified is the absence of an organized Afaan Oromo dataset specifically for disease diagnosis and classification.\n    *   Existing models for resource-rich languages are not directly transferable due to fundamental linguistic differences.\n\n**3. Technical Approach & Innovation**\n*   **Core technical method:**\n    *   The study employs a comparative approach, evaluating both traditional Machine Learning (ML) algorithms (Support Vector Machine (SVM), Random Forest (RF), Logistic Regression (LR), Na\u00efve Bayes) and Deep Learning (DL) models (Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bi-directional LSTM (Bi-LSTM)).\n    *   Two feature representation techniques are used: TF-IDF (Term Frequency-Inverse Document Frequency) and word embedding (specifically word2vec).\n    *   Hyper-parameter tuning is applied to optimize the performance of each model.\n*   **Novelty/Difference:**\n    *   The core innovation is the application of these AI techniques to the *Afaan Oromo language* for the *specific task of disease category prediction*, a domain previously unaddressed for this low-resource language.\n    *   The creation of a custom Afaan Oromo Patient Symptoms (AOPS) corpus, comprising health text documents labeled into ten categories, directly tackles the critical data scarcity issue.\n    *   Development of a specialized word embedding (word2vec) trained on this newly created Afaan Oromo corpus, tailored for the linguistic nuances of health text in Afaan Oromo.\n\n**4. Key Technical Contributions**\n*   **Novel algorithms, methods, or techniques:**\n    *   Development of the Afaan Oromo Patient Symptoms (AOPS) corpus, a novel dataset of labeled health text documents in Afaan Oromo, which is a foundational contribution for future NLP research in this language.\n    *   Creation of a custom word embedding (word2vec) trained from the AOPS corpus, essential for effective deep learning model performance in a low-resource language context.\n*   **System design or architectural innovations:**\n    *   A comprehensive comparative evaluation framework for various ML and DL models on Afaan Oromo health text, providing empirical evidence for the most suitable approaches.\n*   **Theoretical insights or analysis:**\n    *   Demonstrates that deep learning models, particularly LSTM with custom word2vec embeddings, significantly outperform traditional machine learning algorithms for disease category prediction in Afaan Oromo, highlighting the importance of sequence modeling and learned representations for this language.\n\n**5. Experimental Validation**\n*   **Experiments conducted:**\n    *   Experiments were performed on three newly prepared datasets of Afaan Oromo patient symptoms, categorized into 10 disease classes.\n    *   Both ML (SVM, RF, LR, Na\u00efve Bayes) and DL (LSTM, GRU, Bi-LSTM) models were trained and evaluated using TF-IDF and word2vec feature representations.\n    *   Hyper-parameter tuning was employed to optimize each model's performance.\n*   **Key performance metrics and comparison results:**\n    *   Performance was evaluated using accuracy, recall, precision, and F1 score.\n    *   **Machine Learning Models:** The SVM model using TF-IDF achieved the highest accuracy and F1 score of 94.7%.\n    *   **Deep Learning Models:** The LSTM model using word2vec embedding demonstrated the highest accuracy rate of 95.7% and an F1 score of 96.0%.\n    *   **Overall:** The LSTM model with word2vec embedding was identified as the best-performing model across all evaluated algorithms and datasets \\cite{dinsa2024kdb}.\n\n**6. Limitations & Scope**\n*   **Technical limitations or assumptions:**\n    *   The study's reliance on a newly prepared corpus (AOPS) due to the absence of a standard dataset implies potential limitations in corpus size or diversity compared to resources available for high-resource languages.\n    *   The model predicts disease *categories* rather than specific diseases, providing a general idea for physicians.\n*   **Scope of applicability:**\n    *   The developed model is primarily applicable for initial disease category prediction from Afaan Oromo patient symptoms.\n    *   It aims to automate manual systems, reduce errors, and save time and human resources in healthcare settings where Afaan Oromo is spoken.\n\n**7. Technical Significance**\n*   **Advancement of technical state-of-the-art:**\n    *   This research significantly advances the state-of-the-art in natural language processing for low-resource languages by successfully demonstrating AI-based disease prediction in Afaan Oromo.\n    *   It addresses a critical need in healthcare AI for non-English speaking populations, particularly in the Horn of Africa.\n    *   The creation of the AOPS corpus and custom word embeddings provides invaluable foundational resources for future NLP research and application development in Afaan Oromo.\n*   **Potential impact on future research:**\n    *   Lays the groundwork for developing more sophisticated and specific AI diagnostic tools for Afaan Oromo and other under-resourced languages.\n    *   Encourages further research into expanding Afaan Oromo health datasets and exploring advanced deep learning architectures (e.g., transformer-based models) for improved accuracy and diagnostic specificity.\n    *   The methodology for corpus creation and comparative model evaluation can serve as a blueprint for similar initiatives in other low-resource linguistic contexts globally.",
        "keywords": [
            "Afaan Oromo language",
            "disease category prediction",
            "low-resource languages",
            "Afaan Oromo Patient Symptoms (AOPS) corpus",
            "custom word2vec embedding",
            "Deep Learning (DL)",
            "LSTM",
            "Machine Learning (ML)",
            "TF-IDF",
            "comparative evaluation",
            "healthcare AI",
            "data scarcity",
            "SOV sentence structure"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"predicting disease categories automatically from symptoms documented in the afaan oromo language, employing various classification algorithms.\" this indicates the development and application of a system/method.\n*   **introduction discusses:** \"the performance of the proposed methodology has been evaluated...\" and details the use of \"machine learning models,\" \"svm model,\" \"lstm model,\" \"tf-idf and word embedding\" as \"feature representations.\" it focuses on the \"optimal performance of each model\" and concludes which \"model verifies to be the best.\" this clearly aligns with presenting and evaluating new methods, algorithms, or systems (or new applications of existing ones).\n*   while the paper is also heavily **empirical** due to its data-driven experiments and statistical analysis (accuracy, recall, precision, f1 score), the primary focus is on the *development, application, and comparison of a prediction model and methodology*. the empirical evaluation serves to validate the technical solution. the core contribution is the \"ai-based disease category prediction model\" itself."
    },
    "5fa7ec7ba9af0665da61d95b68f755eab224b834.pdf": {
        "title": "Medical practitioner perspectives on AI in emergency triage",
        "authors": [
            "B. Townsend",
            "K. Plant",
            "Victoria J. Hodge",
            "Ol'tunde Ashaolu",
            "R. Calinescu"
        ],
        "published_date": "2023",
        "abstract": "Introduction A proposed Diagnostic AI System for Robot-Assisted Triage (\u201cDAISY\u201d) is under development to support Emergency Department (\u201cED\u201d) triage following increasing reports of overcrowding and shortage of staff in ED care experienced within National Health Service, England (\u201cNHS\u201d) but also globally. DAISY aims to reduce ED patient wait times and medical practitioner overload. The objective of this study was to explore NHS health practitioners' perspectives and attitudes towards the future use of AI-supported technologies in ED triage. Methods Between July and August 2022 a qualitative-exploratory research study was conducted to collect and capture the perceptions and attitudes of nine NHS healthcare practitioners to better understand the challenges and benefits of a DAISY deployment. The study was based on a thematic analysis of semi-structured interviews. The study involved qualitative data analysis of the interviewees' responses. Audio-recordings were transcribed verbatim, and notes included into data documents. The transcripts were coded line-by-line, and data were organised into themes and sub-themes. Both inductive and deductive approaches to thematic analysis were used to analyse such data. Results Based on a qualitative analysis of coded interviews with the practitioners, responses were categorised into broad main thematic-types, namely: trust; current practice; social, legal, ethical, and cultural concerns; and empathetic practice. Sub-themes were identified for each main theme. Further quantitative analyses explored the vocabulary and sentiments of the participants when talking generally about NHS ED practices compared to discussing DAISY. Limitations include a small sample size and the requirement that research participants imagine a prototype AI-supported system still under development. The expectation is that such a system would work alongside the practitioner. Findings can be generalisable to other healthcare AI-supported systems and to other domains. Discussion This study highlights the benefits and challenges for an AI-supported triage healthcare solution. The study shows that most NHS ED practitioners interviewed were positive about such adoption. Benefits cited were a reduction in patient wait times in the ED, assistance in the streamlining of the triage process, support in calling for appropriate diagnostics and for further patient examination, and identification of those very unwell and requiring more immediate and urgent attention. Words used to describe the system were that DAISY is a \u201cgood idea\u201d, \u201chelp\u201d, helpful, \u201ceasier\u201d, \u201cvalue\u201d, and \u201caccurate\u201d. Our study demonstrates that trust in the system is a significant driver of use and a potential barrier to adoption. Participants emphasised social, legal, ethical, and cultural considerations and barriers to DAISY adoption and the importance of empathy and non-verbal cues in patient interactions. Findings demonstrate how DAISY might support and augment human medical performance in ED care, and provide an understanding of attitudinal barriers and considerations for the development and implementation of future triage AI-supported systems.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/5fa7ec7ba9af0665da61d95b68f755eab224b834.pdf",
        "venue": "Frontiers Digit. Health",
        "citationCount": 15,
        "score": 7.5,
        "summary": "The provided content appears to be a report of survey findings rather than a traditional technical or research paper describing novel algorithms, systems, or theoretical insights. Therefore, some categories, particularly those related to \"technical innovation\" and \"novel algorithms,\" are not directly applicable. This summary will interpret the questions in the context of a survey report providing empirical data on operational challenges.\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper implicitly addresses the severe operational pressures and challenges faced by UK Emergency Departments (EDs) in managing patient flow, ambulance handovers, and staffing levels. It quantifies the extent of these issues.\n    *   **Importance and Challenge:** This problem is critical due to its direct impact on patient safety, quality of care, and staff well-being within the healthcare system. The challenge lies in understanding the scale and specific manifestations of these pressures across different regions and operational aspects.\n\n2.  **Related Work & Positioning**\n    *   The provided content does not discuss related work or position itself against existing approaches. It presents raw survey data without external contextualization.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The core method is a survey-based data collection approach, gathering self-reported information from Emergency Departments.\n    *   **Novelty:** The approach is not described as novel; it is a standard survey methodology applied to collect specific operational data points. No technical innovations in data collection, analysis, or modeling are presented.\n\n4.  **Key Technical Contributions**\n    *   The paper's primary contribution is empirical data and insights into the current state of operational pressures within UK EDs, rather than novel algorithms, system designs, or theoretical insights. It provides quantitative evidence of widespread issues.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** A survey was conducted among UK Emergency Departments, with 60 to 87 respondents depending on the question.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Geographic Distribution:** Responses were collected from various UK regions, with London (21.67%), North West (16.67%), and East of England/East Midlands/South West/West Midlands/Yorkshire and Humber (all 8.33%) being prominent \\cite{townsend2023mbq}.\n        *   **Ambulance Holding:** A significant majority (79.31%) of EDs reported holding ambulances \"Every day\" in the last week, with only 5.75% reporting \"Never\" \\cite{townsend2023mbq}.\n        *   **Care in Non-Designated Areas:** 71.26% of EDs reported providing care in non-designated areas (e.g., corridors) \"Every day,\" while 17.24% reported \"Never\" \\cite{townsend2023mbq}.\n        *   **Longest ED Stay:** No ED reported stays less than 12 hours. A substantial proportion reported stays of 24-48 hours (43.68%), 12-24 hours (20.69%), 48-72 hours (20.69%), and even more than 72 hours (14.94%) \\cite{townsend2023mbq}.\n        *   **Confidence in Managing Pressures:** A vast majority expressed low confidence, with 65.52% being \"Not at all confident\" and 27.59% \"Not so confident\" about managing pressures into winter. No respondents were \"Extremely\" or \"Very confident\" \\cite{townsend2023mbq}.\n        *   **Staffing Disruption due to COVID:** 76.91% reported \"Moderate\" (42.53%) or \"Significant\" (34.48%) staffing disruption due to COVID-related illness \\cite{townsend2023mbq}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary limitation is that the data is self-reported survey data, which may be subject to reporting biases. The sample size (60-87 respondents) is relatively small for a comprehensive national overview.\n    *   **Scope of Applicability:** The findings are applicable to Emergency Departments within the specified regions of the UK during the survey period (around June 2022, given the reference to \"since Monday 27th June\").\n\n7.  **Technical Significance**\n    *   The paper's significance is primarily in providing timely, quantitative empirical evidence of the severe operational challenges and pressures faced by UK Emergency Departments \\cite{townsend2023mbq}. While not advancing a specific technical field, this data is crucial for informing healthcare policy, resource allocation, and operational management strategies to address the crisis in EDs. It highlights areas requiring urgent intervention and further research into solutions.",
        "keywords": [
            "UK Emergency Departments",
            "Operational challenges",
            "Survey-based data collection",
            "Empirical data",
            "Patient flow",
            "Ambulance handovers",
            "Care in non-designated areas",
            "Extended ED stays",
            "Staffing levels",
            "COVID-related staffing disruption",
            "Low confidence in managing pressures",
            "Quantitative evidence",
            "Healthcare policy",
            "Resource allocation"
        ],
        "paper_type": "the provided abstract and introduction consist entirely of survey questions and their aggregated responses (percentages and counts). this indicates a study focused on collecting and presenting data from a specific group (medical practitioners).\n\nlet's evaluate against the criteria:\n\n*   **survey**: this paper is *not* a literature review or a comprehensive analysis of existing literature. it's a data collection survey.\n*   **technical**: there are no new methods, algorithms, or systems proposed.\n*   **theoretical**: there is no mathematical analysis, proofs, or formal models.\n*   **empirical**: the text presents \"data\" (survey responses) and implicitly involves \"statistical analysis\" (percentages, counts). the title \"medical practitioner perspectives on ai in emergency triage\" strongly suggests a data-driven study to gather these perspectives. the questions themselves are research questions, and the responses are the findings. this aligns perfectly with the definition of an empirical paper.\n*   **case_study**: while it deals with a real-world scenario (emergency triage), it's a broad survey of practitioners, not a detailed analysis of a single specific application or case.\n*   **position**: the paper does not argue for a viewpoint or future direction; it presents collected data.\n*   **short**: while the provided snippet is brief, the content itself (raw survey results) doesn't inherently classify it as a \"short communication\" without more context on the full paper's length or venue's typical submission types. however, the *nature* of the content is clearly empirical.\n\nthe content is a direct presentation of collected data from a study, which is the hallmark of an empirical paper.\n\n**classification: empirical**"
    },
    "26b3955c0c7caa01ec4ebe516ab06c86e1716971.pdf": {
        "title": "Artificial Intelligence in Hand Surgery - How Generative AI is Transforming the Hand Surgery Landscape.",
        "authors": [
            "Ruth En Si Tan",
            "Wendy Teo",
            "M. Puhaindran"
        ],
        "published_date": "2024",
        "abstract": "Artificial intelligence (AI) has witnessed significant advancements, reshaping various industries, including healthcare. The introduction of ChatGPT by OpenAI in November 2022 marked a pivotal moment, showcasing the potential of generative AI in revolutionising patient care, diagnosis and treatment. Generative AI, unlike traditional AI systems, possesses the ability to generate new content by understanding patterns within datasets. This article explores the evolution of AI in healthcare, tracing its roots to the term coined by John McCarthy in 1955 and the contributions of pioneers like John Von Neumann and Alan Turing. Currently, generative AI, particularly Large Language Models, holds promise across three broad categories in healthcare: patient care, education and research. In patient care, it offers solutions in clinical document management, diagnostic support and operative planning. Notable advancements include Microsoft's collaboration with Epic for integrating AI into electronic medical records (EMRs), enhancing clinical data management and patient care. Furthermore, generative AI aids in surgical decision-making, as demonstrated in plastic, orthopaedic and hepatobiliary surgeries. However, challenges such as bias, hallucination and integration with EMR systems necessitate caution and ongoing evaluation. The article also presents insights from the implementation of NUHS Russell-GPT, a generative AI chatbot, in a hand surgery department, showcasing its utility in administrative tasks but highlighting challenges in surgical planning and EMR integration. The survey showed unanimous support for incorporating AI into clinical settings, with all respondents being open to its use. In conclusion, generative AI is poised to enhance patient care and ease physician workloads, starting with automating administrative tasks and evolving to inform diagnoses, tailored treatment plans, as well as aid in surgical planning. As healthcare systems navigate the complexities of integrating AI, the potential benefits for both physicians and patients remain significant, offering a glimpse into a future where AI transforms healthcare delivery. Level of Evidence: Level V (Diagnostic).",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/26b3955c0c7caa01ec4ebe516ab06c86e1716971.pdf",
        "venue": "The Journal of Hand Surgery (Asian-Pacific Volume)",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Artificial intelligence (AI) has witnessed significant advancements, reshaping various industries, including healthcare. The introduction of ChatGPT by OpenAI in November 2022 marked a pivotal moment, showcasing the potential of generative AI in revolutionising patient care, diagnosis and treatment. Generative AI, unlike traditional AI systems, possesses the ability to generate new content by understanding patterns within datasets. This article explores the evolution of AI in healthcare, tracing its roots to the term coined by John McCarthy in 1955 and the contributions of pioneers like John Von Neumann and Alan Turing. Currently, generative AI, particularly Large Language Models, holds promise across three broad categories in healthcare: patient care, education and research. In patient care, it offers solutions in clinical document management, diagnostic support and operative planning. Notable advancements include Microsoft's collaboration with Epic for integrating AI into electronic medical records (EMRs), enhancing clinical data management and patient care. Furthermore, generative AI aids in surgical decision-making, as demonstrated in plastic, orthopaedic and hepatobiliary surgeries. However, challenges such as bias, hallucination and integration with EMR systems necessitate caution and ongoing evaluation. The article also presents insights from the implementation of NUHS Russell-GPT, a generative AI chatbot, in a hand surgery department, showcasing its utility in administrative tasks but highlighting challenges in surgical planning and EMR integration. The survey showed unanimous support for incorporating AI into clinical settings, with all respondents being open to its use. In conclusion, generative AI is poised to enhance patient care and ease physician workloads, starting with automating administrative tasks and evolving to inform diagnoses, tailored treatment plans, as well as aid in surgical planning. As healthcare systems navigate the complexities of integrating AI, the potential benefits for both physicians and patients remain significant, offering a glimpse into a future where AI transforms healthcare delivery. Level of Evidence: Level V (Diagnostic).",
        "keywords": []
    },
    "06ecc497edd61c2e548315a260edd0ca65242858.pdf": {
        "title": "Leveraging artificial intelligence to detect ethical concerns in medical research: a case study",
        "authors": [
            "K. Sridharan",
            "G. Sivaramakrishnan"
        ],
        "published_date": "2024",
        "abstract": "Background Institutional review boards (IRBs) have been criticised for delays in approvals for research proposals due to inadequate or inexperienced IRB staff. Artificial intelligence (AI), particularly large language models (LLMs), has significant potential to assist IRB members in a prompt and efficient reviewing process. Methods Four LLMs were evaluated on whether they could identify potential ethical issues in seven validated case studies. The LLMs were prompted with queries related to the proposed eligibility criteria of the study participants, vulnerability issues, information to be disclosed in the informed consent document (ICD), risk\u2013benefit assessment and justification of the use of a placebo. Another query was issued to the LLMs to generate ICDs for these case scenarios. Results All four LLMs were able to provide answers to the queries related to all seven cases. In general, the responses were homogeneous with respect to most elements. LLMs performed suboptimally in identifying the suitability of the placebo arm, risk mitigation strategies and potential risks to study participants in certain case studies with a single prompt. However, multiple prompts led to better outputs in all of these domains. Each of the LLMs included all of the fundamental elements of the ICD for all case scenarios. Use of jargon, understatement of benefits and failure to state potential risks were the key observations in the AI-generated ICD. Conclusion It is likely that LLMs can enhance the identification of potential ethical issues in clinical research, and they can be used as an adjunct tool to prescreen research proposals and enhance the efficiency of an IRB.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/06ecc497edd61c2e548315a260edd0ca65242858.pdf",
        "venue": "Journal of Medical Ethics",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Background Institutional review boards (IRBs) have been criticised for delays in approvals for research proposals due to inadequate or inexperienced IRB staff. Artificial intelligence (AI), particularly large language models (LLMs), has significant potential to assist IRB members in a prompt and efficient reviewing process. Methods Four LLMs were evaluated on whether they could identify potential ethical issues in seven validated case studies. The LLMs were prompted with queries related to the proposed eligibility criteria of the study participants, vulnerability issues, information to be disclosed in the informed consent document (ICD), risk\u2013benefit assessment and justification of the use of a placebo. Another query was issued to the LLMs to generate ICDs for these case scenarios. Results All four LLMs were able to provide answers to the queries related to all seven cases. In general, the responses were homogeneous with respect to most elements. LLMs performed suboptimally in identifying the suitability of the placebo arm, risk mitigation strategies and potential risks to study participants in certain case studies with a single prompt. However, multiple prompts led to better outputs in all of these domains. Each of the LLMs included all of the fundamental elements of the ICD for all case scenarios. Use of jargon, understatement of benefits and failure to state potential risks were the key observations in the AI-generated ICD. Conclusion It is likely that LLMs can enhance the identification of potential ethical issues in clinical research, and they can be used as an adjunct tool to prescreen research proposals and enhance the efficiency of an IRB.",
        "keywords": []
    },
    "133b6ae7cc26f5ca8b9df8aa0fae460b76e1e4a5.pdf": {
        "title": "Regulatory considerations for medical imaging AI/ML devices in the United States: concepts and challenges",
        "authors": [
            "N. Petrick",
            "Weijie Chen",
            "J. Delfino",
            "B. Gallas",
            "Y. Kang",
            "Daniel M. Krainak",
            "B. Sahiner",
            "Ravi K. Samala"
        ],
        "published_date": "2023",
        "abstract": "Abstract. Purpose To introduce developers to medical device regulatory processes and data considerations in artificial intelligence and machine learning (AI/ML) device submissions and to discuss ongoing AI/ML-related regulatory challenges and activities. Approach AI/ML technologies are being used in an increasing number of medical imaging devices, and the fast evolution of these technologies presents novel regulatory challenges. We provide AI/ML developers with an introduction to U.S. Food and Drug Administration (FDA) regulatory concepts, processes, and fundamental assessments for a wide range of medical imaging AI/ML device types. Results The device type for an AI/ML device and appropriate premarket regulatory pathway is based on the level of risk associated with the device and informed by both its technological characteristics and intended use. AI/ML device submissions contain a wide array of information and testing to facilitate the review process with the model description, data, nonclinical testing, and multi-reader multi-case testing being critical aspects of the AI/ML device review process for many AI/ML device submissions. The agency is also involved in AI/ML-related activities that support guidance document development, good machine learning practice development, AI/ML transparency, AI/ML regulatory research, and real-world performance assessment. Conclusion FDA\u2019s AI/ML regulatory and scientific efforts support the joint goals of ensuring patients have access to safe and effective AI/ML devices over the entire device lifecycle and stimulating medical AI/ML innovation.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/133b6ae7cc26f5ca8b9df8aa0fae460b76e1e4a5.pdf",
        "venue": "Journal of Medical Imaging",
        "citationCount": 13,
        "score": 6.5,
        "summary": "I apologize, but the \"Paper content\" section is empty. I cannot analyze the technical paper and provide a focused summary for literature review without the actual content of the paper.\n\nPlease provide the full text of the technical paper so I can perform the requested analysis.",
        "keywords": [
            "Paper content empty",
            "cannot analyze technical paper",
            "focused summary",
            "literature review",
            "actual content",
            "full text",
            "requested analysis"
        ],
        "paper_type": "the abstract and introduction content are missing. therefore, i cannot classify this paper."
    },
    "083705b712d3bc7871f788416dd1e5209fc8cefb.pdf": {
        "title": "Comparative study of Claude 3.5-Sonnet and human physicians in generating discharge summaries for patients with renal insufficiency: assessment of efficiency, accuracy, and quality",
        "authors": [
            "Haijiao Jin",
            "Jinglu Guo",
            "Qisheng Lin",
            "Shaun Wu",
            "Weiguo Hu",
            "Xiaoyang Li"
        ],
        "published_date": "2024",
        "abstract": "Background The rapid development of artificial intelligence (AI) has shown great potential in medical document generation. This study aims to evaluate the performance of Claude 3.5-Sonnet, an advanced AI model, in generating discharge summaries for patients with renal insufficiency, compared to human physicians. Methods A prospective, comparative study was conducted involving 100 patients (50 with acute kidney injury and 50 with chronic kidney disease) from the nephrology department of Ningbo Hangzhou Bay Hospital between January and June 2024. Discharge summaries were independently generated by Claude 3.5-Sonnet and human physicians. The main evaluation indicators included accuracy, generation time, and overall quality. Results Claude 3.5-Sonnet demonstrated comparable accuracy to human physicians in generating discharge summaries for both AKI (90 vs. 92 points, p\u2009>\u20090.05) and CKD patients (88 vs. 90 points, p\u2009>\u20090.05). The AI model significantly outperformed human physicians in terms of efficiency, requiring only about 30\u2005s to generate a summary compared to over 15\u2005min for physicians (p\u2009<\u20090.001). The overall quality scores showed no significant difference between AI-generated and physician-written summaries for both AKI (26 vs. 27 points, p\u2009>\u20090.05) and CKD patients (25 vs. 26 points, p\u2009>\u20090.05). Conclusion Claude 3.5-Sonnet demonstrates high efficiency and reliability in generating discharge summaries for patients with renal insufficiency, with accuracy and quality comparable to those of human physicians. These findings suggest that AI has significant potential to improve the efficiency of medical documentation, though further research is needed to optimize its integration into clinical practice and address ethical and privacy concerns.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/083705b712d3bc7871f788416dd1e5209fc8cefb.pdf",
        "venue": "Frontiers Digit. Health",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the continuous challenge of developing more capable, efficient, and safer large language models (LLMs) that surpass existing state-of-the-art, particularly in complex reasoning, multimodal understanding, advanced coding, and long-context processing.\n    *   **Importance & Challenge:** As LLMs become integral to diverse applications, there's a critical need to push performance boundaries (e.g., agentic coding, visual math reasoning) while simultaneously improving operational efficiency (speed, cost) and rigorously ensuring safety, especially as model capabilities increase.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work positions Claude 3.5 Sonnet as an evolution within the Claude 3 model family, directly comparing its performance against its predecessor, Claude 3 Opus, as well as other frontier models like GPT-4o, GPT-4T, Gemini 1.5 Pro, and Llama 3 400B \\cite{jin2024h9o}.\n    *   **Limitations of Previous Solutions:** While Claude 3 Opus was a highly capable model, Claude 3.5 Sonnet demonstrates superior performance across various benchmarks, operates faster, and at a lower cost, indicating limitations in the previous generation regarding efficiency and peak capability across diverse tasks \\cite{jin2024h9o}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** Claude 3.5 Sonnet is presented as an \"evolution\" of the Claude 3 model family, implying iterative advancements in its underlying architecture, training methodologies, and optimization techniques. While specific architectural details are not provided in this addendum, the focus is on the *outcome* of these improvements.\n    *   **Novelty:** The primary innovation lies in achieving significantly enhanced capabilities across multiple domains (reasoning, coding, visual processing, long-context retrieval, safety alignment) while simultaneously delivering faster inference and lower operational costs compared to its predecessor, Claude 3 Opus \\cite{jin2024h9o}. The development of a robust internal agentic coding evaluation, mimicking real-world software engineering tasks, is also a notable methodological innovation.\n\n*   **Key Technical Contributions**\n    *   **Empirical Performance Leap:** Claude 3.5 Sonnet establishes new state-of-the-art performance across a broad spectrum of industry-standard benchmarks for reasoning, coding, question answering, and multimodal understanding \\cite{jin2024h9o}.\n    *   **Advanced Agentic Coding:** Demonstrates a substantial improvement in practical coding proficiency, solving 64% of problems on an internal agentic coding evaluation (compared to 38% for Claude 3 Opus), which involves understanding codebases, implementing pull requests, and iterative self-correction \\cite{jin2024h9o}.\n    *   **Enhanced Multimodal Reasoning:** Achieves state-of-the-art results on challenging vision benchmarks such as MathVista (visual math reasoning), ChartQA (chart understanding), and DocVQA (document understanding) \\cite{jin2024h9o}.\n    *   **Superior Long-Context Retrieval:** Exhibits near-perfect recall (99.7% average) on the Needle In A Haystack evaluation for context lengths up to 200k tokens, outperforming previous models \\cite{jin2024h9o}.\n    *   **Improved Safety Alignment:** Shows better differentiation between harmful and benign requests, leading to fewer incorrect refusals and more correct refusals, and is classified as an ASL-2 model, indicating no risk of catastrophic harm \\cite{jin2024h9o}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive evaluations were performed using:\n        *   Industry-standard benchmarks: GPQA, MMLU, MATH, HumanEval, MGSM, DROP, BIG-Bench Hard, GSM8K for reasoning, math, coding, and QA \\cite{jin2024h9o}.\n        *   Multimodal benchmarks: MMMU, MathVista, AI2D, ChartQA, DocVQA for vision capabilities \\cite{jin2024h9o}.\n        *   Internal agentic coding evaluation: Custom benchmark simulating real-world software engineering tasks \\cite{jin2024h9o}.\n        *   Safety and refusal evaluations: Wildchat and XSTest datasets, along with frontier risk assessments (CBRN, cybersecurity, autonomous capabilities) \\cite{jin2024h9o}.\n        *   Long-context retrieval: Needle In A Haystack task up to 200k tokens \\cite{jin2024h9o}.\n        *   Human feedback evaluations: Direct comparisons against prior Claude models across various use cases and expert domains \\cite{jin2024h9o}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Reasoning/Coding/QA:** Achieved 59.4% on GPQA (Diamond 0-shot CoT), 90.4% on MMLU (5-shot CoT), and 92.0% on HumanEval (0-shot), consistently outperforming Claude 3 Opus and setting new benchmarks \\cite{jin2024h9o}.\n        *   **Vision:** Demonstrated state-of-the-art performance, e.g., 67.7% on MathVista, 94.7% on AI2D, 90.8% on ChartQA, and 95.2% on DocVQA \\cite{jin2024h9o}.\n        *   **Agentic Coding:** Solved 64% of problems, a significant improvement over Claude 3 Opus's 38% \\cite{jin2024h9o}.\n        *   **Long Context:** Achieved 99.7% average recall on Needle In A Haystack across all context lengths \\cite{jin2024h9o}.\n        *   **Human Preference:** Showed high win rates against Claude 3 Opus, including 60% in coding, 66% in documents, and up to 82% in Law \\cite{jin2024h9o}.\n        *   **Safety:** Classified as ASL-2, with improved correct and incorrect refusal rates on safety benchmarks \\cite{jin2024h9o}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper is an addendum and does not delve into the specific architectural or training innovations that underpin the performance improvements. It primarily presents empirical results. The evaluation of HHH-trained models acknowledges the challenge that safety guardrails might mask a model's full underlying capabilities \\cite{jin2024h9o}.\n    *   **Scope of Applicability:** Claude 3.5 Sonnet is a general-purpose LLM with broad applicability across text and vision-based tasks, including complex reasoning, advanced coding, document understanding, and agentic workflows.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** Claude 3.5 Sonnet significantly advances the technical state-of-the-art for LLMs by demonstrating superior performance across a wide array of benchmarks, particularly in multimodal reasoning, complex agentic coding, and long-context understanding, while also improving efficiency and safety \\cite{jin2024h9o}. Its agentic coding capabilities represent a notable step towards more autonomous software development agents.\n    *   **Potential Impact on Future Research:** This work sets new performance benchmarks, encouraging further research into more efficient and capable LLM architectures, advanced multimodal integration, and sophisticated agentic AI systems. The improved efficiency and safety profile also pave the way for broader and more responsible deployment of advanced AI in real-world applications.",
        "keywords": [
            "Claude 3.5 Sonnet",
            "Large Language Models (LLMs)",
            "state-of-the-art performance",
            "complex reasoning",
            "multimodal understanding",
            "advanced agentic coding",
            "long-context retrieval",
            "operational efficiency",
            "safety alignment",
            "internal agentic coding evaluation",
            "Needle In A Haystack evaluation",
            "empirical performance leap",
            "autonomous software development agents"
        ],
        "paper_type": "the provided abstract and introduction describe the evaluation of a new model, claude 3.5 sonnet, against industry-standard benchmarks. it details how the model was \"evaluated\" on various tasks (reasoning, coding, vision) and presents \"results\" showing its performance compared to previous models. this involves data-driven assessment and reporting of findings.\n\nthis aligns best with the criteria for an **empirical** paper:\n*   **abstract mentions:** \"evaluations\", \"results\", \"benchmarks\", \"performance standards\".\n*   **introduction discusses:** \"evaluated claude 3.5 sonnet on a series of industry-standard benchmarks\", \"outperforms\", \"sets new performance standards\", \"the results are shown in table 1/2\".\n\ntherefore, the paper is classified as **empirical**."
    },
    "a714d172bbd0cb5ec3a40742128cc77b7f887c40.pdf": {
        "title": "Interoperability and governance in the European Health Data Space regulation",
        "authors": [
            "P. Terzis",
            "(Enrique) OE Santamaria Echeverria"
        ],
        "published_date": "2023",
        "abstract": "The proposal for a regulation on the European Health Data Space (EHDS) is a much-awaited project. It aspires to create a harmonised framework \u2013 a common European data space \u2013 for the administration of health data (primary use) across Member States and the promotion of healthcare research and innovation (by establishing rules for the secondary use of health data). As such, although the EHDS proposal is a legal document, in its essence, it includes provisions that introduce not only legal, but also institutional, and technical-infrastructural changes. Overall, together with the Regulation 2017/745 on medical devices, the Data Governance Act (DGA), the Data Act, the AI Act, and the General Data Protection Regulation (GDPR), the EHDS proposal will complete the regulatory canvas for the use of health data in the European Union. Although we are supportive of the EHDS initiative, there are aspects of the proposal that require further debate, reconsideration, and amendments. Following previous work on potential power asymmetries encapsulated in the Proposal, in this commentary, we focus on the provisions of/for interoperability of the Electronic Health Record (EHR) systems (Ar. 14\u201332) as well as the provisions on the structure of Health Data Access bodies and their cross-border organisation (section 3). We recommend a series of amendments to orientate the EHDS project better to its constitutive goals: the promotion of public health research and respect for the rights of the individuals.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/a714d172bbd0cb5ec3a40742128cc77b7f887c40.pdf",
        "venue": "Medical Law International",
        "citationCount": 12,
        "score": 6.0,
        "summary": "The proposal for a regulation on the European Health Data Space (EHDS) is a much-awaited project. It aspires to create a harmonised framework \u2013 a common European data space \u2013 for the administration of health data (primary use) across Member States and the promotion of healthcare research and innovation (by establishing rules for the secondary use of health data). As such, although the EHDS proposal is a legal document, in its essence, it includes provisions that introduce not only legal, but also institutional, and technical-infrastructural changes. Overall, together with the Regulation 2017/745 on medical devices, the Data Governance Act (DGA), the Data Act, the AI Act, and the General Data Protection Regulation (GDPR), the EHDS proposal will complete the regulatory canvas for the use of health data in the European Union. Although we are supportive of the EHDS initiative, there are aspects of the proposal that require further debate, reconsideration, and amendments. Following previous work on potential power asymmetries encapsulated in the Proposal, in this commentary, we focus on the provisions of/for interoperability of the Electronic Health Record (EHR) systems (Ar. 14\u201332) as well as the provisions on the structure of Health Data Access bodies and their cross-border organisation (section 3). We recommend a series of amendments to orientate the EHDS project better to its constitutive goals: the promotion of public health research and respect for the rights of the individuals.",
        "keywords": []
    },
    "0dd9d7ed1c0ea7bbd605e0ed8daa26cfc21d274d.pdf": {
        "title": "AI-Assisted Detection and Localization of Spinal Metastatic Lesions",
        "authors": [
            "Edgars Edelmers",
            "Arturs Nikulins",
            "Klinta Lu\u012bze Spr\u016bd\u017ea",
            "Patr\u012bcija Stapulone",
            "Niks Saimons P\u016bce",
            "Elizabete Skrebele",
            "Everita El\u012bna Si\u0146icina",
            "Viktorija C\u012brule",
            "Ance Kazu\u0161a",
            "Katrina Bolo\u010dko"
        ],
        "published_date": "2024",
        "abstract": "Objectives: The integration of machine learning and radiomics in medical imaging has significantly advanced diagnostic and prognostic capabilities in healthcare. This study focuses on developing and validating an artificial intelligence (AI) model using U-Net architectures for the accurate detection and segmentation of spinal metastases from computed tomography (CT) images, addressing both osteolytic and osteoblastic lesions. Methods: Our methodology employs multiple variations of the U-Net architecture and utilizes two distinct datasets: one consisting of 115 polytrauma patients for vertebra segmentation and another comprising 38 patients with documented spinal metastases for lesion detection. Results: The model demonstrated strong performance in vertebra segmentation, achieving Dice Similarity Coefficient (DSC) values between 0.87 and 0.96. For metastasis segmentation, the model achieved a DSC of 0.71 and an F-beta score of 0.68 for lytic lesions but struggled with sclerotic lesions, obtaining a DSC of 0.61 and an F-beta score of 0.57, reflecting challenges in detecting dense, subtle bone alterations. Despite these limitations, the model successfully identified isolated metastatic lesions beyond the spine, such as in the sternum, indicating potential for broader skeletal metastasis detection. Conclusions: The study concludes that AI-based models can augment radiologists\u2019 capabilities by providing reliable second-opinion tools, though further refinements and diverse training data are needed for optimal performance, particularly for sclerotic lesion segmentation. The annotated CT dataset produced and shared in this research serves as a valuable resource for future advancements.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/0dd9d7ed1c0ea7bbd605e0ed8daa26cfc21d274d.pdf",
        "venue": "Diagnostics",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Objectives: The integration of machine learning and radiomics in medical imaging has significantly advanced diagnostic and prognostic capabilities in healthcare. This study focuses on developing and validating an artificial intelligence (AI) model using U-Net architectures for the accurate detection and segmentation of spinal metastases from computed tomography (CT) images, addressing both osteolytic and osteoblastic lesions. Methods: Our methodology employs multiple variations of the U-Net architecture and utilizes two distinct datasets: one consisting of 115 polytrauma patients for vertebra segmentation and another comprising 38 patients with documented spinal metastases for lesion detection. Results: The model demonstrated strong performance in vertebra segmentation, achieving Dice Similarity Coefficient (DSC) values between 0.87 and 0.96. For metastasis segmentation, the model achieved a DSC of 0.71 and an F-beta score of 0.68 for lytic lesions but struggled with sclerotic lesions, obtaining a DSC of 0.61 and an F-beta score of 0.57, reflecting challenges in detecting dense, subtle bone alterations. Despite these limitations, the model successfully identified isolated metastatic lesions beyond the spine, such as in the sternum, indicating potential for broader skeletal metastasis detection. Conclusions: The study concludes that AI-based models can augment radiologists\u2019 capabilities by providing reliable second-opinion tools, though further refinements and diverse training data are needed for optimal performance, particularly for sclerotic lesion segmentation. The annotated CT dataset produced and shared in this research serves as a valuable resource for future advancements.",
        "keywords": []
    },
    "ff72650acbfbbf38729ac540b6ff38530887cbae.pdf": {
        "title": "SOAP.AI: A Collaborative Tool for Documenting Human Behavior in Videos through Multimodal Generative AI",
        "authors": [
            "Qingxiao Zheng",
            "Parisa Rabbani",
            "Yu-Rou Lin",
            "Daan Mansour",
            "Yun Huang"
        ],
        "published_date": "2024",
        "abstract": "Large Multimodal Models offer new opportunities for analyzing human activities and social behavior in fields requiring expert knowledge. Their in-context learning and adaptive abilities make customization possible for experts without coding skills. This paper introduces SOAP.AI, a collaborative tool facilitating experts to analyze human behaviors using AI. SOAP.AI is designed to foster a sense of ownership during human-AI collaboration, encouraging task modifications and evaluations to meet diverse goals. For instance, teaching AI to recognize behavioral nuances in autistic individuals could enhance AI's inclusion and value alignment. Our demonstration will engage CSCW researchers and HCI practitioners to discuss the design of collaborative AI systems for behavioral insights generation in various settings, such as medical settings, sports, social media, education, home care, and more.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/ff72650acbfbbf38729ac540b6ff38530887cbae.pdf",
        "venue": "CSCW Companion",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large Multimodal Models offer new opportunities for analyzing human activities and social behavior in fields requiring expert knowledge. Their in-context learning and adaptive abilities make customization possible for experts without coding skills. This paper introduces SOAP.AI, a collaborative tool facilitating experts to analyze human behaviors using AI. SOAP.AI is designed to foster a sense of ownership during human-AI collaboration, encouraging task modifications and evaluations to meet diverse goals. For instance, teaching AI to recognize behavioral nuances in autistic individuals could enhance AI's inclusion and value alignment. Our demonstration will engage CSCW researchers and HCI practitioners to discuss the design of collaborative AI systems for behavioral insights generation in various settings, such as medical settings, sports, social media, education, home care, and more.",
        "keywords": []
    },
    "9be2253cd4780045263546ce52dae4d255766d05.pdf": {
        "title": "Identification of patients\u2019 smoking status using an explainable AI approach: a Danish electronic health records case study",
        "authors": [
            "Ali Ebrahimi",
            "M. Henriksen",
            "C.L. Brasen",
            "O. Hilberg",
            "T. Hansen",
            "L.H. Jensen",
            "A. Peimankar",
            "U. Wiil"
        ],
        "published_date": "2024",
        "abstract": "Background Smoking is a critical risk factor responsible for over eight million annual deaths worldwide. It is essential to obtain information on smoking habits to advance research and implement preventive measures such as screening of high-risk individuals. In most countries, including Denmark, smoking habits are not systematically recorded and at best documented within unstructured free-text segments of electronic health records (EHRs). This would require researchers and clinicians to manually navigate through extensive amounts of unstructured data, which is one of the main reasons that smoking habits are rarely integrated into larger studies. Our aim is to develop machine learning models to classify patients\u2019 smoking status from their EHRs. Methods This study proposes an efficient natural language processing (NLP) pipeline capable of classifying patients\u2019 smoking status and providing explanations for the decisions. The proposed NLP pipeline comprises four distinct components, which are; (1) considering preprocessing techniques to address abbreviations, punctuation, and other textual irregularities, (2) four cutting-edge feature extraction techniques, i.e. Embedding, BERT, Word2Vec, and Count Vectorizer, employed to extract the optimal features, (3) utilization of a Stacking-based Ensemble (SE) model and a Convolutional Long Short-Term Memory Neural Network (CNN-LSTM) for the identification of smoking status, and (4) application of a local interpretable model-agnostic explanation to explain the decisions rendered by the detection models. The EHRs of 23,132 patients with suspected lung cancer were collected from the Region of Southern Denmark during the period 1/1/2009-31/12/2018. A medical professional annotated the data into \u2018Smoker\u2019 and \u2018Non-Smoker\u2019 with further classifications as \u2018Active-Smoker\u2019, \u2018Former-Smoker\u2019, and \u2018Never-Smoker\u2019. Subsequently, the annotated dataset was used for the development of binary and multiclass classification models. An extensive comparison was conducted of the detection performance across various model architectures. Results The results of experimental validation confirm the consistency among the models. However, for binary classification, BERT method with CNN-LSTM architecture outperformed other models by achieving precision, recall, and F1-scores between 97% and 99% for both Never-Smokers and Active-Smokers. In multiclass classification, the Embedding technique with CNN-LSTM architecture yielded the most favorable results in class-specific evaluations, with equal performance measures of 97% for Never-Smoker and measures in the range of 86 to 89% for Active-Smoker and 91\u201392% for Never-Smoker. Conclusion Our proposed NLP pipeline achieved a high level of classification performance. In addition, we presented the explanation of the decision made by the best performing detection model. Future work will expand the model\u2019s capabilities to analyze longer notes and a broader range of categories to maximize its utility in further research and screening applications.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/9be2253cd4780045263546ce52dae4d255766d05.pdf",
        "venue": "BMC Medical Research Methodology",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Systematically obtaining information on patients' smoking habits is crucial for research (e.g., cardiovascular, pulmonary, cancer) and preventive measures, but this data is often not formally registered and is instead embedded within unstructured free-text segments of Electronic Health Records (EHRs) \\cite{ebrahimi2024i5r}.\n    *   **Importance & Challenge:** Manual extraction from extensive EHRs is impractical for large patient cohorts, hindering large-scale studies and screening efforts. The unstructured nature of EHRs, including abbreviations, spelling errors, and domain-specific jargon, makes automated extraction challenging, especially for languages like Danish where a high-performing model was previously lacking \\cite{ebrahimi2024i5r}. Additionally, the \"black box\" nature of advanced machine learning models necessitates explainability, particularly in critical medical contexts, to foster trust and enable error identification \\cite{ebrahimi2024i5r}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Natural Language Processing (NLP) has been applied in healthcare for various tasks (e.g., detecting heart failure, adverse drug effects). Previous \"Smoking challenges\" demonstrated the ability to classify smoking status using supervised and unsupervised classifiers \\cite{ebrahimi2024i5r}. Deep Neural Networks (DNNs) and contextualized embeddings like BERT have shown superior performance and reduced preprocessing requirements in recent years \\cite{ebrahimi2024i5r}.\n    *   **Limitations of Previous Solutions & Positioning:** Despite advancements, a high-performing model specifically for detecting smoking status in *Danish* EHRs was lacking due to limited text data availability, access restrictions, and the complex structure of EHRs which limits transfer learning from other languages \\cite{ebrahimi2024i5r}. Furthermore, while Explainable AI (XAI) has grown for image and structured data, its application to free-text datasets, especially for medical decision-making like smoking status detection, has received comparatively less attention \\cite{ebrahimi2024i5r}. This study addresses these gaps by developing a high-performing, explainable NLP model for Danish EHRs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes an efficient NLP pipeline for classifying patients' smoking status and providing explanations \\cite{ebrahimi2024i5r}. This pipeline comprises four key components:\n        1.  **Preprocessing:** Techniques to handle abbreviations, punctuation, and textual irregularities specific to Danish EHRs (e.g., conversion to lowercase, stop word removal, word tokenization) \\cite{ebrahimi2024i5r}.\n        2.  **Feature Extraction:** Utilizes four cutting-edge techniques: Word Embedding, BERT (specifically a Danish edition), Word2Vec, and Count Vectorizer, chosen to capture linguistic nuances of Danish \\cite{ebrahimi2024i5r}.\n        3.  **Classification Models:** Employs a Stacking-based Ensemble (SE) model (with K-Nearest Neighbor, Decision Trees, Random Forest, XGBoost as base learners and Logistic Regression as meta-learner) and a Convolutional Long Short-Term Memory Neural Network (CNN-LSTM) architecture \\cite{ebrahimi2024i5r}.\n        4.  **Explainability:** Applies Local Interpretable Model-agnostic Explanations (LIME) to provide post-hoc explanations for the models' decisions \\cite{ebrahimi2024i5r}.\n    *   **Novelty/Differentiation:** This work is novel as it is the first study to detect smoking status from *Danish* EHR text using a comprehensive NLP pipeline \\cite{ebrahimi2024i5r}. It integrates advanced feature extraction with both ensemble and deep learning models, and critically, it is the first to provide model explanations for smoking status detection based on EHRs using LIME \\cite{ebrahimi2024i5r}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Formulation of several NLP-based architectures integrating advanced feature extraction techniques (Embedding, BERT, Word2Vec, Count Vectorizer) with ensemble-based machine learning (Stacking-based Ensemble) and deep learning (CNN-LSTM) models for smoking status detection in Danish EHRs \\cite{ebrahimi2024i5r}.\n        *   Application of the state-of-the-art XAI approach, LIME, to provide post-hoc explanations for the models' decision-making processes, highlighting feature significance and underlying rationale \\cite{ebrahimi2024i2024i5r}.\n    *   **System Design/Architectural Innovations:** Development of a robust, multi-stage NLP pipeline specifically tailored for the complexities of Danish free-text clinical notes, including specialized preprocessing steps \\cite{ebrahimi2024i5r}.\n    *   **Theoretical Insights/Analysis:** Comprehensive analysis and comparison of the detection performance of developed models against existing state-of-the-art predictive models, including a non-parametric statistical assessment based on the Friedman test \\cite{ebrahimi2024i5r}.\n\n5.  **Experimental Validation**\n    *   **Experiments:** The study collected EHRs from 23,132 patients with suspected lung cancer from the Region of Southern Denmark (2009-2018) \\cite{ebrahimi2024i5r}. A medical professional manually annotated this dataset into 'Smoker' and 'Non-Smoker' for binary classification, and further into 'Active-Smoker', 'Former-Smoker', and 'Never-Smoker' for multiclass classification \\cite{ebrahimi2024i5r}. An extensive comparison of detection performance was conducted across various model architectures and feature extraction techniques \\cite{ebrahimi2024i5r}.\n    *   **Key Performance Metrics & Results:**\n        *   **Binary Classification:** The BERT method combined with the CNN-LSTM architecture achieved the best performance, with precision, recall, and F1-scores ranging between 97% and 99% for both 'Never-Smokers' and 'Active-Smokers' \\cite{ebrahimi2024i5r}.\n        *   **Multiclass Classification:** The Embedding technique with the CNN-LSTM architecture yielded the most favorable class-specific results, achieving 97% for 'Never-Smoker', 86-89% for 'Active-Smoker', and 91-92% for 'Former-Smoker' \\cite{ebrahimi2024i5r}.\n        *   The experimental validation confirmed consistency among the models \\cite{ebrahimi2024i5r}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The current model's capabilities are limited to the specific categories ('Active-Smoker', 'Former-Smoker', 'Never-Smoker') and the length of notes used in the study \\cite{ebrahimi2024i5r}.\n    *   **Scope of Applicability:** The model is developed and validated using Danish EHRs, specifically from patients with suspected lung cancer. While promising, its direct applicability to other languages or broader patient populations without re-training or adaptation is not explicitly covered \\cite{ebrahimi2024i5r}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing the first high-performing, explainable NLP model for automated smoking status detection from unstructured *Danish* EHRs \\cite{ebrahimi2024i5r}. The integration of advanced feature extraction, ensemble, and deep learning methods, coupled with XAI, sets a new benchmark for this specific task and language \\cite{ebrahimi2024i5r}.\n    *   **Potential Impact on Future Research:** The developed model and pipeline are expected to be highly valuable for future screening scenarios and various research fields, including other types of cancer and cardiovascular diseases, by enabling efficient extraction of critical smoking habit information \\cite{ebrahimi2024i5r}. The inclusion of explainability enhances trust and interpretability, paving the way for broader adoption of AI in clinical decision support and research involving sensitive medical data \\cite{ebrahimi2024i5r}. Future work aims to expand the model's capabilities to analyze longer notes and a broader range of categories, maximizing its utility \\cite{ebrahimi2024i5r}.",
        "keywords": [
            "Electronic Health Records (EHRs)",
            "Natural Language Processing (NLP)",
            "Smoking status detection",
            "Danish EHRs",
            "Explainable AI (XAI)",
            "LIME",
            "BERT",
            "CNN-LSTM architecture",
            "Stacking-based Ensemble",
            "NLP pipeline",
            "Feature extraction",
            "High-performing model",
            "Binary and multiclass classification",
            "Unstructured free-text"
        ],
        "paper_type": "this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract:** explicitly states the aim \"to develop machine learning models\" and \"proposes an efficient natural language processing (nlp) pipeline.\" it then details the components of this proposed pipeline, including feature extraction techniques, specific machine learning models (stacking-based ensemble, cnn-lstm), and an explainable ai application. this directly aligns with the \"presents new methods, algorithms, or systems\" criterion for technical papers.\n*   **introduction:** sets up the problem (lack of systematic smoking status recording in ehrs, challenges with unstructured text) that the proposed technical solution aims to address.\n*   **keywords:** \"propose\", \"develop\", \"pipeline\", \"algorithm\", \"method\" are all strong indicators of a technical paper.\n*   **empirical/case study components:** while the paper does involve data-driven studies (\"ehrs of 23,132 patients\") and is framed as a \"case study\" in the title, these aspects serve to validate and demonstrate the effectiveness of the *proposed technical solution*. the primary contribution is the development of the nlp pipeline itself, with the empirical evaluation and case study context providing evidence for its utility."
    },
    "977122e69a7a717d5f8c0038e99cc1e323f0c445.pdf": {
        "title": "Public data homogenization for AI model development in breast cancer",
        "authors": [
            "Vassilis Kilintzis",
            "Varvara Kalokyri",
            "H. Kondylakis",
            "Smriti Joshi",
            "K. Nikiforaki",
            "Oliver D\u00edaz",
            "Karim Lekadir",
            "M. Tsiknakis",
            "K. Marias"
        ],
        "published_date": "2024",
        "abstract": "Background Developing trustworthy artificial intelligence (AI) models for clinical applications requires access to clinical and imaging data cohorts. Reusing of publicly available datasets has the potential to fill this gap. Specifically in the domain of breast cancer, a large archive of publicly accessible medical images along with the corresponding clinical data is available at The Cancer Imaging Archive (TCIA). However, existing datasets cannot be directly used as they are heterogeneous and cannot be effectively filtered for selecting specific image types required to develop AI models. This work focuses on the development of a homogenized dataset in the domain of breast cancer including clinical and imaging data. Methods Five datasets were acquired from the TCIA and were harmonized. For the clinical data harmonization, a common data model was developed and a repeatable, documented \u201cextract-transform-load\u201d process was defined and executed for their homogenization. Further, Digital Imaging and COmmunications in Medicine (DICOM) information was extracted from magnetic resonance imaging (MRI) data and made accessible and searchable. Results The resulting harmonized dataset includes information about 2,035 subjects with breast cancer. Further, a platform named RV-Cherry-Picker enables search over both the clinical and diagnostic imaging datasets, providing unified access, facilitating the downloading of all study imaging that correspond to specific series\u2019 characteristics (e.g., dynamic contrast-enhanced series), and reducing the burden of acquiring the appropriate set of images for the respective AI model scenario. Conclusions RV-Cherry-Picker provides access to the largest, publicly available, homogenized, imaging/clinical dataset for breast cancer to develop AI models on top. Relevance statement We present a solution for creating merged public datasets supporting AI model development, using as an example the breast cancer domain and magnetic resonance imaging images. Key points \u2022 The proposed platform allows unified access to the largest, homogenized public imaging dataset for breast cancer. \u2022 A methodology for the semantically enriched homogenization of public clinical data is presented. \u2022 The platform is able to make a detailed selection of breast MRI data for the development of AI models. Graphical Abstract Supplementary Information The online version contains supplementary material available at 10.1186/s41747-024-00442-4.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/977122e69a7a717d5f8c0038e99cc1e323f0c445.pdf",
        "venue": "European Radiology Experimental",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Here's a focused summary of the paper by Kilintzis et al. \\cite{kilintzis20241z9} for a literature review:\n\n---\n\n### **1. Research Problem & Motivation**\n\n*   **Specific Technical Problem**: Publicly available clinical and imaging datasets, particularly in breast cancer (e.g., from The Cancer Imaging Archive - TCIA), are highly heterogeneous. This heterogeneity prevents their direct and effective reuse for developing robust Artificial Intelligence (AI) models, as data cannot be easily filtered for specific image types or clinical characteristics required for AI training.\n*   **Importance and Challenge**:\n    *   Developing trustworthy clinical AI models demands access to high-volume, high-quality, and homogenous data.\n    *   Acquiring adequate prospective data is costly and time-consuming.\n    *   While public retrospective datasets offer a solution for fast AI prototyping and model comparison, their inherent heterogeneity (due to varying study objectives, data models, acquisition protocols, and MRI sequence specifics like orientation, type, and fat suppression) makes integration and secondary usage extremely difficult.\n    *   Identifying and selecting specific MRI techniques (e.g., Dynamic Contrast-Enhanced (DCE) or Diffusion-Weighted Imaging (DWI)) and their associated metadata is a crucial, yet time-consuming, preliminary step for AI model development.\n\n### **2. Related Work & Positioning**\n\n*   The paper implicitly positions itself by addressing a significant gap: the lack of readily usable, integrated, and searchable public datasets for AI model development in breast cancer.\n*   It highlights that existing public repositories like TCIA, while valuable, present data in a heterogeneous format that is not directly amenable to AI training without extensive and complex harmonization efforts.\n*   The work aims to overcome the limitations of previous solutions, which primarily involve manual and ad-hoc data preparation by individual researchers, by providing a systematic, semantically enriched, and tool-supported homogenization approach.\n\n### **3. Technical Approach & Innovation**\n\n*   **Core Technical Method**: The paper proposes a comprehensive data homogenization pipeline for both clinical and imaging data, applied to five heterogeneous breast cancer datasets from TCIA.\n    *   **Clinical Data Homogenization**:\n        *   Development of a **common data model** to unify concepts across disparate datasets.\n        *   Identification of **semantic annotations** using standardized vocabularies (e.g., OMOP, SNOMED-CT, LOINC, RxNorm, Cancer Modifiers) accessed via ATHENA.\n        *   Implementation of a **repeatable, documented Extract-Transform-Load (ETL) process** to convert original data into the common model, ensuring semantic coherence and completeness.\n    *   **Imaging Data Homogenization**:\n        *   Leveraging the TCIA API to retrieve and **extract relevant Digital Imaging and COmmunications in Medicine (DICOM) metadata**.\n        *   Examining DICOM tags to generate statistics and enable **targeted selection of imaging series** based on specific MRI techniques (e.g., DCE, DWI) and characteristics (orientation, sequence type).\n*   **Novelty/Differentiation**:\n    *   The development of **RV-Cherry-Picker**, a free-to-access public platform, is a key innovation. This platform provides unified, searchable access to the resulting homogenized dataset, allowing users to filter and download specific imaging data subsets based on detailed MRI sequence characteristics. This significantly reduces the burden of data acquisition for AI model development.\n    *   The methodology for **semantically enriched homogenization of clinical data**, including detailed rules for OMOP ID mapping (prioritizing conceptual match, standard terminologies, valid/standard concepts, and synonyms) and the use of detailed semantic values instead of generic codes, is a novel contribution.\n\n### **4. Key Technical Contributions**\n\n*   **Novel Algorithms/Methods**: A systematic, semantically enriched homogenization pipeline for integrating heterogeneous public clinical and imaging data from multiple TCIA sources into a unified, AI-ready dataset.\n*   **System Design/Architectural Innovations**: The **RV-Cherry-Picker platform**, which offers unified access, search capabilities, and facilitated downloading of specific imaging cohorts based on detailed metadata, addressing a critical bottleneck in AI model development.\n*   **Theoretical Insights/Analysis**: Development of a robust common data model for breast cancer clinical data, mapped to standardized OMOP concept IDs, ensuring high reusability and semantic coherence. This includes a detailed set of rules for concept and value set selection during the harmonization process.\n*   **Enhanced Data Accessibility**: A method to extract, expose, and make searchable critical DICOM metadata, enabling fine-grained selection of MRI series based on specific technical characteristics.\n*   **Dataset Creation**: Creation of the largest publicly available, homogenized imaging/clinical dataset for breast cancer (2,035 subjects) specifically designed for AI model development.\n\n### **5. Experimental Validation**\n\n*   **Experiments Conducted**: The authors acquired and harmonized five publicly available breast cancer datasets from TCIA: I-SPY 2 (ISPY2), Duke-Breast-Cancer-MRI (DUKE), I-SPY 1/ACRIN 6657 (ISPY1), The Cancer Genome Atlas Breast Invasive Carcinoma Collection (TCGA-BRCA), and Breast-MRI-NACT-Pilot.\n*   **Key Performance Metrics and Comparison Results**:\n    *   The original datasets comprised 2,332 subjects, 251 domain concepts, and 24,589 MRI series, exhibiting significant heterogeneity in subject count (64 to 985), concept count (9 to 110), and data granularity.\n    *   The harmonization process successfully integrated these into a single, unified dataset containing information on **2,035 distinct subjects** with breast cancer.\n    *   The **RV-Cherry-Picker platform** was developed and demonstrated to:\n        *   Provide unified search capabilities across both clinical and diagnostic imaging datasets.\n        *   Enable facilitated downloading of all study imaging corresponding to specific series' characteristics (e.g., dynamic contrast-enhanced series).\n        *   Significantly reduce the burden and time required for researchers to acquire appropriate image sets for specific AI model scenarios.\n    *   The validation focuses on the successful creation and demonstrated functionality of the harmonized dataset and the RV-Cherry-Picker platform, rather than comparative quantitative performance metrics against other methods (as no direct competitors for this specific integrated homogenization approach are discussed).\n\n### **6. Limitations & Scope**\n\n*   **Technical Limitations/Assumptions**:\n    *   The common data model's concept selection criterion (concept present in at least two original datasets) might exclude valuable but less common concepts found in only one dataset.\n    *   The mapping of concepts to OMOP IDs and the handling of value sets involved manual analysis and decision-making, which could be labor-intensive for extremely large or highly diverse datasets.\n*   **Scope of Applicability**: The work is specifically focused on breast cancer data, particularly MRI images and associated clinical data from TCIA. While the underlying methodology is generalizable, the current implementation and the resulting dataset are domain-specific. The data sources are limited to TCIA.\n\n### **7. Technical Significance**\n\n*   **Advances State-of-the-Art**: This work significantly advances the state-of-the-art by providing a robust, semantically enriched methodology and a practical, publicly accessible tool (RV-Cherry-Picker) to overcome the critical challenge of data heterogeneity in public medical imaging repositories. This directly addresses a major bottleneck for AI model development in clinical domains.\n*   **Potential Impact on Future Research**:\n    *   **Enables Reproducible AI Research**: By offering a homogenized, FAIR-compliant dataset and a tool for targeted data selection, it enables researchers to more easily access and utilize high-quality data, fostering more reproducible and comparable AI model development in breast cancer.\n    *   **Reduces Data Preparation Burden**: Automates and streamlines the complex and time-consuming process of identifying, filtering, and acquiring specific imaging cohorts, allowing AI developers to focus more on model building and less on data wrangling.\n    *   **Accelerates AI Development**: Facilitates the creation of more robust and generalizable AI models by providing access to larger, more diverse, yet harmonized datasets.\n    *   **Blueprint for Generalization**: The methodology can serve as a blueprint for similar homogenization efforts in other medical domains or for integrating data from different public repositories, promoting the adoption of FAIR principles in medical data sharing and reuse.",
        "keywords": [
            "Data heterogeneity",
            "Data homogenization pipeline",
            "Semantic annotations",
            "Common data model",
            "DICOM metadata extraction",
            "OMOP concept IDs",
            "RV-Cherry-Picker platform",
            "Unified searchable access",
            "Targeted imaging series selection",
            "Breast cancer AI",
            "Public clinical and imaging datasets (TCIA)",
            "Homogenized breast cancer dataset",
            "Reduced data preparation burden",
            "Reproducible AI research"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the paper explicitly states its focus on the \"development of a homogenized dataset\" and describes the \"methods\" used, including the development of a \"common data model\" and a \"repeatable, documented 'extract-transform-load' process.\" it also mentions extracting and making dicom information accessible and searchable.\n*   these phrases (\"development,\" \"methods,\" \"common data model was developed,\" \"process was defined and executed,\" \"extracted... and made accessible and searchable\") directly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems.\n\ntherefore, the paper is classified as:\n\n**technical**"
    },
    "de039759b6da9ffd23dec6d272e76015e50d7156.pdf": {
        "title": "Requirements for AI Development and Reporting for MRI Prostate Cancer Detection in Biopsy-Naive Men: PI-RADS Steering Committee, Version 1.0.",
        "authors": [
            "B. Turkbey",
            "Henkjan Huisman",
            "Andriy Fedorov",
            "K. Macura",
            "D. Margolis",
            "V. Panebianco",
            "Aytekin Oto",
            "Ivo G. Schoots",
            "M. M. Siddiqui",
            "Caroline M Moore",
            "Olivier Rouvi\u00e8re",
            "L. K. Bittencourt",
            "A. Padhani",
            "Clare Tempany",
            "Masoom A Haider"
        ],
        "published_date": "2025",
        "abstract": "This document defines the key considerations for developing and reporting an artificial intelligence (AI) interpretation model for the detection of clinically significant prostate cancer (PCa) at MRI in biopsy-naive men with a positive clinical screening status. Specific data and performance metric requirements and a checklist are provided for this use case. Data requirements emphasize the need for sufficient information to provide transparency and characterization of training and test data. The definition of a true-negative examination (which includes a minimum of 2-year follow-up), the need for image quality assessments, and nonimaging metadata requirements are provided. Performance metrics ranges are included, such as a cancer detection rate of 40%-70% for Prostate Imaging Reporting and Data System, or PI-RADS, 4 or higher lesions and demonstration of equivalent or better than human performance using receiver operating characteristic and precision-recall curves. The use of open datasets such as those used in the AI challenge model is encouraged. The study design should include conformity with the Checklist for Artificial Intelligence in Medical Imaging requirements. This article should be taken in the context of the current and evolving regulatory landscape. This review provides guidance based on subspeciality expertise in prostate MRI and will hopefully accelerate the clinical translation of AI in PCa detection.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/de039759b6da9ffd23dec6d272e76015e50d7156.pdf",
        "venue": "Radiology",
        "citationCount": 5,
        "score": 5.0,
        "summary": "This document defines the key considerations for developing and reporting an artificial intelligence (AI) interpretation model for the detection of clinically significant prostate cancer (PCa) at MRI in biopsy-naive men with a positive clinical screening status. Specific data and performance metric requirements and a checklist are provided for this use case. Data requirements emphasize the need for sufficient information to provide transparency and characterization of training and test data. The definition of a true-negative examination (which includes a minimum of 2-year follow-up), the need for image quality assessments, and nonimaging metadata requirements are provided. Performance metrics ranges are included, such as a cancer detection rate of 40%-70% for Prostate Imaging Reporting and Data System, or PI-RADS, 4 or higher lesions and demonstration of equivalent or better than human performance using receiver operating characteristic and precision-recall curves. The use of open datasets such as those used in the AI challenge model is encouraged. The study design should include conformity with the Checklist for Artificial Intelligence in Medical Imaging requirements. This article should be taken in the context of the current and evolving regulatory landscape. This review provides guidance based on subspeciality expertise in prostate MRI and will hopefully accelerate the clinical translation of AI in PCa detection.",
        "keywords": []
    },
    "1aad7f61e4f8988a0b201c85de7740bf05d157c0.pdf": {
        "title": "Assessing the Role Ghana's Public Health Act, 2012 (Act 851) Can Play in Oversight of Artificial Intelligence Healthcare Systems to Prevent Medical Errors and Improve Patient Safety",
        "authors": [
            "George Benneh Mensah",
            "Maad M. Mijwil",
            "Mostafa Abotaleb",
            "Sayed M. El-kenawy",
            "Marwa M. Eid",
            "Pushan Kumar Dutta",
            "Alfred Addy"
        ],
        "published_date": "2023",
        "abstract": "Purpose: Evaluate the possibilities and limitations of implementing the Essential Ghana Public Health Act to manage the growing development of artificial intelligence (AI)-enabled health care in light of the contemporary gap in research so. \nMethodology: A review of the 2012 public health regulatory provisions and anti-technology surveillance mechanisms document detailing the needs and risks of AI regulation. \nResults: Current regulations have a customizable foundation for documenting policies, reporting algorithmic errors, creating updated workplace safety audits, and enforcing non-compliance but required by fully implemented investigations that strong material differences are addressed and that AI-specific rules are codified into new legal rules. \nConclusions: Ghana currently has the mechanisms in place for an interim administration to flexibly implement long-term healthcare legislation on gaps awaiting reconciliation through investment in specific sectors, staffing and reforms. \nRecommendations: Immediate training to prepare inspection personnel before the onset of the crisis is guidelines and rules for algorithmic accounting. What really matters is the effective implementation of existing legislation and the informing of strategies for modernization and certainly also the innovation of policy frameworks for the innovation of new health care systems. \nScientific contribution: addresses the knowledge gap in maintaining vulnerabilities for emerging technologies that are tracked by regulations in disruption.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/1aad7f61e4f8988a0b201c85de7740bf05d157c0.pdf",
        "venue": "Babylonian Journal of Artificial Intelligence",
        "citationCount": 9,
        "score": 4.5,
        "summary": "Purpose: Evaluate the possibilities and limitations of implementing the Essential Ghana Public Health Act to manage the growing development of artificial intelligence (AI)-enabled health care in light of the contemporary gap in research so. \nMethodology: A review of the 2012 public health regulatory provisions and anti-technology surveillance mechanisms document detailing the needs and risks of AI regulation. \nResults: Current regulations have a customizable foundation for documenting policies, reporting algorithmic errors, creating updated workplace safety audits, and enforcing non-compliance but required by fully implemented investigations that strong material differences are addressed and that AI-specific rules are codified into new legal rules. \nConclusions: Ghana currently has the mechanisms in place for an interim administration to flexibly implement long-term healthcare legislation on gaps awaiting reconciliation through investment in specific sectors, staffing and reforms. \nRecommendations: Immediate training to prepare inspection personnel before the onset of the crisis is guidelines and rules for algorithmic accounting. What really matters is the effective implementation of existing legislation and the informing of strategies for modernization and certainly also the innovation of policy frameworks for the innovation of new health care systems. \nScientific contribution: addresses the knowledge gap in maintaining vulnerabilities for emerging technologies that are tracked by regulations in disruption.",
        "keywords": []
    },
    "1ce60e219788fb5b69e5c85daecbdb00fb392dad.pdf": {
        "title": "Preparing Well for Esophageal Endoscopic Detection Using a Hybrid Model and Transfer Learning",
        "authors": [
            "C. Chou",
            "H. Nguyen",
            "Yao-Kuang Wang",
            "Tsung-Hsien Chen",
            "I-Chen Wu",
            "Chien-Wei Huang",
            "Hsiang-Chen Wang"
        ],
        "published_date": "2023",
        "abstract": "Simple Summary The timely detection and accurate classification of esophageal cancer are critical for providing optimal treatment. However, assessing and categorizing pathological conditions related to the esophagus face limitations as they rely on reference document photo-documentation, and the accuracy heavily relies on the endoscopist\u2019s expertise. In recent times, computer-aided endoscopic image classification has achieved remarkable success in this domain. For this study, a dataset of 1002 endoscopic images, comprising 650 white-light images and 352 narrow-band images, was collected for training. The esophageal neoplasms were categorized into three groups: squamous cell carcinoma, high-grade dysplasia, and normal cases. To enhance the prediction results, a hybrid model was proposed, yielding an impressive accuracy of 96.32%, precision of 96.44%, recall of 95.70%, and f1-score of 96.04%. The introduction of AI-based diagnostic platforms is expected to effectively support medical professionals in formulating well-informed treatment regimens. Abstract Early detection of esophageal cancer through endoscopic imaging is pivotal for effective treatment. However, the intricacies of endoscopic diagnosis, contingent on the physician\u2019s expertise, pose challenges. Esophageal cancer features often manifest ambiguously, leading to potential confusions with other inflammatory esophageal conditions, thereby complicating diagnostic accuracy. In recent times, computer-aided diagnosis has emerged as a promising solution in medical imaging, particularly within the domain of endoscopy. Nonetheless, contemporary AI-based diagnostic models heavily rely on voluminous data sources, limiting their applicability, especially in scenarios with scarce datasets. To address this limitation, our study introduces novel data training strategies based on transfer learning, tailored to optimize performance with limited data. Additionally, we propose a hybrid model integrating EfficientNet and Vision Transformer networks to enhance prediction accuracy. Conducting rigorous evaluations on a carefully curated dataset comprising 1002 endoscopic images (comprising 650 white-light images and 352 narrow-band images), our model achieved exceptional outcomes. Our combined model achieved an accuracy of 96.32%, precision of 96.44%, recall of 95.70%, and f1-score of 96.04%, surpassing state-of-the-art models and individual components, substantiating its potential for precise medical image classification. The AI-based medical image prediction platform presents several advantageous characteristics, encompassing superior prediction accuracy, a compact model size, and adaptability to low-data scenarios. This research heralds a significant stride in the advancement of computer-aided endoscopic imaging for improved esophageal cancer diagnosis.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/1ce60e219788fb5b69e5c85daecbdb00fb392dad.pdf",
        "venue": "Cancers",
        "citationCount": 8,
        "score": 4.0,
        "summary": "Simple Summary The timely detection and accurate classification of esophageal cancer are critical for providing optimal treatment. However, assessing and categorizing pathological conditions related to the esophagus face limitations as they rely on reference document photo-documentation, and the accuracy heavily relies on the endoscopist\u2019s expertise. In recent times, computer-aided endoscopic image classification has achieved remarkable success in this domain. For this study, a dataset of 1002 endoscopic images, comprising 650 white-light images and 352 narrow-band images, was collected for training. The esophageal neoplasms were categorized into three groups: squamous cell carcinoma, high-grade dysplasia, and normal cases. To enhance the prediction results, a hybrid model was proposed, yielding an impressive accuracy of 96.32%, precision of 96.44%, recall of 95.70%, and f1-score of 96.04%. The introduction of AI-based diagnostic platforms is expected to effectively support medical professionals in formulating well-informed treatment regimens. Abstract Early detection of esophageal cancer through endoscopic imaging is pivotal for effective treatment. However, the intricacies of endoscopic diagnosis, contingent on the physician\u2019s expertise, pose challenges. Esophageal cancer features often manifest ambiguously, leading to potential confusions with other inflammatory esophageal conditions, thereby complicating diagnostic accuracy. In recent times, computer-aided diagnosis has emerged as a promising solution in medical imaging, particularly within the domain of endoscopy. Nonetheless, contemporary AI-based diagnostic models heavily rely on voluminous data sources, limiting their applicability, especially in scenarios with scarce datasets. To address this limitation, our study introduces novel data training strategies based on transfer learning, tailored to optimize performance with limited data. Additionally, we propose a hybrid model integrating EfficientNet and Vision Transformer networks to enhance prediction accuracy. Conducting rigorous evaluations on a carefully curated dataset comprising 1002 endoscopic images (comprising 650 white-light images and 352 narrow-band images), our model achieved exceptional outcomes. Our combined model achieved an accuracy of 96.32%, precision of 96.44%, recall of 95.70%, and f1-score of 96.04%, surpassing state-of-the-art models and individual components, substantiating its potential for precise medical image classification. The AI-based medical image prediction platform presents several advantageous characteristics, encompassing superior prediction accuracy, a compact model size, and adaptability to low-data scenarios. This research heralds a significant stride in the advancement of computer-aided endoscopic imaging for improved esophageal cancer diagnosis.",
        "keywords": []
    },
    "31306a2f84da4f71a2cab9a3ffb9c9200b6dbc7e.pdf": {
        "title": "Fine-tuning and aligning question answering models for complex information extraction tasks",
        "authors": [
            "Matthias Engelbach",
            "Dennis Klau",
            "Felix Scheerer",
            "Jens Drawehn",
            "Maximilien Kintz"
        ],
        "published_date": "2023",
        "abstract": "The emergence of Large Language Models (LLMs) has boosted performance and possibilities in various NLP tasks. While the usage of generative AI models like ChatGPT opens up new opportunities for several business use cases, their current tendency to hallucinate fake content strongly limits their applicability to document analysis, such as information retrieval from documents. In contrast, extractive language models like question answering (QA) or passage retrieval models guarantee query results to be found within the boundaries of an according context document, which makes them candidates for more reliable information extraction in productive environments of companies. In this work we propose an approach that uses and integrates extractive QA models for improved feature extraction of German business documents such as insurance reports or medical leaflets into a document analysis solution. We further show that fine-tuning existing German QA models boosts performance for tailored extraction tasks of complex linguistic features like damage cause explanations or descriptions of medication appearance, even with using only a small set of annotated data. Finally, we discuss the relevance of scoring metrics for evaluating information extraction tasks and deduce a combined metric from Levenshtein distance, F1-Score, Exact Match and ROUGE-L to mimic the assessment criteria from human experts.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/31306a2f84da4f71a2cab9a3ffb9c9200b6dbc7e.pdf",
        "venue": "International Conference on Knowledge Discovery and Information Retrieval",
        "citationCount": 7,
        "score": 3.5,
        "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n---\n\n### Analysis of \"Fine-tuning and aligning question answering models for complex information extraction tasks\" \\cite{engelbach202314i}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the challenge of reliably extracting complex, free-form information from unstructured German business documents (e.g., insurance reports, medical leaflets) for industrial use cases \\cite{engelbach202314i}. This includes features like \"damage cause explanations\" or \"descriptions of medication appearance\" that span multiple sentences.\n    *   **Importance and Challenge**:\n        *   Automated feature extraction is crucial for many business processes, requiring unstructured text to be converted into structured data.\n        *   Traditional rule-based approaches struggle with the arbitrary complexity and variability of such features across different domains and query formulations \\cite{engelbach202314i}.\n        *   Generative Large Language Models (LLMs) like ChatGPT, while powerful, suffer from \"hallucination,\" producing unverified content, which limits their applicability for reliable document analysis in productive environments \\cite{engelbach202314i}.\n        *   Extractive Question Answering (QA) models, which guarantee answers within the document context, offer a more reliable alternative but require adaptation for specific, complex tasks and languages like German \\cite{engelbach202314i}.\n        *   Evaluating these models effectively requires metrics that align with human expert judgment, especially for fuzzy natural language outputs \\cite{engelbach202314i}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Builds upon existing document analysis pipelines that combine image analysis, OCR, and text classification \\cite{engelbach202314i}.\n        *   Extends previous work by the authors (Kintz et al., 2020; Engelbach et al., 2022) that used rule-based methods and Named Entity Recognition (NER) for information retrieval \\cite{engelbach202314i}.\n        *   Leverages pre-trained German QA models and datasets like GermanQuAD (M \u00a8oller et al., 2021) from the Huggingface community (Wolf et al., 2020) as a starting point \\cite{engelbach202314i}.\n    *   **Limitations of Previous Solutions**:\n        *   **Third-party cloud solutions (e.g., Microsoft Azure Document AI)**: Not viable for confidential or personal data due to reliance on external infrastructure; companies often require on-premise, fine-tunable solutions \\cite{engelbach202314i}.\n        *   **Rule-based and standard ML (e.g., NER) models**: Lack the contextual understanding capabilities of modern LLMs, making them insufficient for \"complex\" linguistic features \\cite{engelbach202314i}.\n        *   **Historical document systems (e.g., Transkribus)**: Lack flexibility for feature extraction and layout handling in modern business documents \\cite{engelbach202314i}.\n        *   **Generative LLMs**: Prone to hallucination, making their output unreliable for critical information extraction tasks \\cite{engelbach202314i}.\n        *   **General evaluation metrics (e.g., BLEU, hLEPOR for NMT)**: May not correlate well with human judgment for extractive QA tasks, especially when considering partial correctness or fuzziness in natural language \\cite{engelbach202314i}. Reinforcement Learning from Human Feedback (RLHF) is too resource-intensive for this context \\cite{engelbach202314i}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes an approach that integrates and fine-tunes extractive Question Answering (QA) models within a comprehensive document analysis pipeline \\cite{engelbach202314i}.\n    *   **Innovation**:\n        *   **Hybrid Pipeline Design**: Combines visual image processing (region detection, OCR), rule-based pre-filtering to narrow down search scope, extractive QA model querying, and rule-based post-validation to ensure output correctness \\cite{engelbach202314i}.\n        *   **Domain-Specific Fine-tuning**: Demonstrates that fine-tuning existing German QA models (specifically `gelectra-large-germanquad`) on small, domain-specific datasets significantly boosts performance for complex information extraction tasks \\cite{engelbach202314i}.\n        *   **Feature Complexity Classification**: Introduces a classification of feature extraction difficulty (Simple, Dynamic, Complex) and positions QA models as the recommended approach for \"Complex\" features (e.g., cause of an event, person with a given role) \\cite{engelbach202314i}.\n        *   **Novel Evaluation Metric**: Develops a combined evaluation metric derived from Levenshtein distance, F1-Score, Exact Match, and ROUGE-L, designed to mimic human expert assessment criteria for information extraction tasks, accounting for partial correctness and usefulness \\cite{engelbach202314i}.\n        *   **Doc Stride for Large Documents**: Implements a mechanism (`doc stride`) to handle large documents by splitting them into overlapping chunks, performing inference on smaller portions, and merging predictions to find the top confidence answer \\cite{engelbach202314i}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A methodology for integrating and fine-tuning extractive QA models for reliable extraction of complex, free-form information from German business documents \\cite{engelbach202314i}.\n        *   Demonstration of effective domain adaptation for QA models using *limited amounts of annotated data* for specific industrial use cases \\cite{engelbach202314i}.\n        *   A novel combined evaluation metric that synthesizes Levenshtein distance, F1-Score, Exact Match, and ROUGE-L, tailored to align with human expert judgment for information extraction tasks \\cite{engelbach202314i}.\n    *   **System Design or Architectural Innovations**:\n        *   A robust information extraction pipeline that integrates OCR, region detection, rule-based context filtering, and extractive QA models, followed by rule-based validation, suitable for on-premise deployment in sensitive data environments \\cite{engelbach202314i}.\n        *   A framework that categorizes information extraction tasks by complexity, guiding the selection of appropriate methods (e.g., rule-based for \"Simple,\" NER for \"Dynamic,\" QA for \"Complex\") \\cite{engelbach202314i}.\n    *   **Theoretical Insights or Analysis**:\n        *   Empirical evidence supporting the superiority of extractive QA models over generative LLMs for reliable information extraction due to their inherent robustness against hallucination \\cite{engelbach202314i}.\n        *   Analysis of the critical need for evaluation metrics that accurately reflect human perception of correctness and usefulness in NLP tasks, leading to the proposed combined metric \\cite{engelbach202314i}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The authors fine-tuned the `gelectra-large-germanquad` model on two domain-specific German datasets \\cite{engelbach202314i}. Hyperparameter optimization was performed using 5-fold cross-validation.\n    *   **Datasets**:\n        *   **Drug leaflet data set**: 170 medication leaflet documents, each with three QA pairs targeting \"Ingredient,\" \"Look,\" and \"Application\" \\cite{engelbach202314i}.\n        *   **Elemental damage report data set**: 47 elemental damage reports, each with two QA pairs targeting \"Damage Cause\" and \"Assessor Name\" \\cite{engelbach202314i}.\n        *   All documents were manually annotated using the Haystack QA annotation tool \\cite{engelbach202314i}.\n    *   **Key Performance Metrics**:\n        *   Automated metrics: Levenshtein distance, F1-Score, Exact Match, and ROUGE-L were used for initial evaluation and hyperparameter tuning \\cite{engelbach202314i}.\n        *   Manual expert metric: A combined metric was used for final evaluation, with specific criteria defined for each feature (e.g., all ingredients must be included for \"Ingredient,\" partial correctness for \"Look\" or \"Damage Cause\") to align with human judgment \\cite{engelbach202314i}.\n    *   **Comparison Results**: The paper *states* that fine-tuning \"boosts performance for tailored extraction tasks of complex linguistic features... even with using only a small set of annotated data\" \\cite{engelbach202314i}. However, the provided text content *does not include the specific numerical results, tables, or graphs* that quantify this performance boost or compare the metrics before and after fine-tuning.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The approach relies on the quality of OCR for scanned documents, which can introduce errors \\cite{engelbach202314i}.\n        *   The effectiveness of rule-based pre-filtering and post-validation is dependent on the accuracy and completeness of the defined rules \\cite{engelbach202314i}.\n        *   The fine-tuning was performed on \"limited amounts of samples\" (small datasets), which might affect generalizability to broader domains without further data \\cite{engelbach202314i}.\n    *   **Scope of Applicability**:\n        *   Primarily focused on German language documents and specific business domains (medical and insurance) \\cite{engelbach202314i}.\n        *   Aimed at industrial use cases where high reliability, verifiable outputs, and on-premise deployment are critical, especially for handling confidential data \\cite{engelbach202314i}.\n        *   Most applicable to \"complex\" information extraction tasks where answers are free-form text spans rather than simple entities \\cite{engelbach202314i}.\n\n7.  **Technical Significance**\n    *   **Advances the Technical State-of-the-Art**:\n        *   Provides a robust and practical framework for reliable, hallucination-free information extraction from complex documents, addressing a critical gap left by generative LLMs for industrial applications \\cite{engelbach202314i}.\n        *   Demonstrates that significant performance gains in domain-specific QA can be achieved with *minimal annotated data*, making advanced NLP accessible for companies with limited labeling resources \\cite{engelbach202314i}.\n        *   Introduces a more sophisticated and human-centric evaluation methodology for QA models, which is vital for developing trustworthy AI systems in real-world, high-stakes environments \\cite{engelbach202314i}.\n    *   **Potential Impact on Future Research**:\n        *   Encourages further exploration of efficient domain adaptation techniques for extractive QA, particularly for low-resource languages or highly specialized technical documents \\cite{engelbach202314i}.\n        *   Highlights the importance of developing and standardizing evaluation metrics that closely mimic human judgment, fostering more meaningful and applicable research in NLP \\cite{engelbach202314i}.\n        *   Offers a blueprint for integrating advanced neural models with traditional symbolic (rule-based) approaches in hybrid AI systems, potentially leading to more robust and interpretable solutions for document intelligence \\cite{engelbach202314i}.",
        "keywords": [
            "Extractive Question Answering (QA)",
            "Complex Information Extraction",
            "German Business Documents",
            "Fine-tuning QA Models",
            "Generative LLM Hallucination",
            "Hybrid Document Analysis Pipeline",
            "Domain-Specific Adaptation",
            "Novel Evaluation Metric",
            "Reliable Information Extraction",
            "On-Premise Deployment",
            "Limited Annotated Data",
            "Doc Stride",
            "Human Expert Judgment",
            "Feature Complexity Classification"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:**\n    *   \"in this work we **propose an approach** that uses and integrates extractive qa models for improved feature extraction...\" - this directly aligns with presenting a new method or system.\n    *   \"we further **show that fine-tuning existing german qa models boosts performance**...\" - while this indicates an empirical component, it's in service of demonstrating the effectiveness of their proposed approach/method.\n    *   \"...and **deduce a combined metric** from levenshtein distance, f1-score, exact match and rouge-l...\" - this is a technical contribution, presenting a new way to evaluate.\n*   **introduction discusses:**\n    *   the problem of \"automated feature extraction from text documents\" and the need for \"information retrieval systems (irs\u2019s)\".\n    *   the context of \"recent progress in the field of large language models (llm\u2019s)\" and how it \"has boosted capabilities\".\n    *   this sets the stage for presenting a technical solution to a practical problem.\n\nthe paper's primary contribution is the proposal of a new approach for integrating and fine-tuning qa models for complex information extraction, along with a new evaluation metric. the empirical results serve to validate the effectiveness of this technical contribution."
    },
    "5785e1136c7009f57b3ef4079a867c2435b0bdbb.pdf": {
        "title": "Trends in the Approval and Quality Management of Artificial Intelligence Medical Devices in the Republic of Korea",
        "authors": [
            "Kyoungtaek Lim",
            "Tae-Young Heo",
            "Jaesuk Yun"
        ],
        "published_date": "2022",
        "abstract": "Artificial intelligence (AI) is being implemented in many areas of medicine, such as patient-customized diagnosis. Growth in the artificial intelligence medical device (AIMD) field is expected in the coming years. Major countries are currently establishing systems and policies to gain a leading position in the medical artificial intelligence market. The Republic of Korea has initiated the Act on Nurturing the Medical Devices Industry and Supporting Innovative Medical Devices for the development of AIMDs and is implementing it preemptively. As a result, the country has achieved an effective strategy for coping with the COVID-19 pandemic, an increase in the number of AIMD approvals (85 approved as of September 2021), and the creation of a document pertaining to internationally harmonized guidelines on AIMD-related terms and definitions. However, in order to develop and activate more AIMD products, it is necessary to improve post-market management such as product change and quality control in addition to approval. Here, we review the current regulatory status of AIMD in the Republic of Korea and what needs to be improved for AIMD to be more developed and activated.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/5785e1136c7009f57b3ef4079a867c2435b0bdbb.pdf",
        "venue": "Diagnostics",
        "citationCount": 9,
        "score": 3.0,
        "summary": "Artificial intelligence (AI) is being implemented in many areas of medicine, such as patient-customized diagnosis. Growth in the artificial intelligence medical device (AIMD) field is expected in the coming years. Major countries are currently establishing systems and policies to gain a leading position in the medical artificial intelligence market. The Republic of Korea has initiated the Act on Nurturing the Medical Devices Industry and Supporting Innovative Medical Devices for the development of AIMDs and is implementing it preemptively. As a result, the country has achieved an effective strategy for coping with the COVID-19 pandemic, an increase in the number of AIMD approvals (85 approved as of September 2021), and the creation of a document pertaining to internationally harmonized guidelines on AIMD-related terms and definitions. However, in order to develop and activate more AIMD products, it is necessary to improve post-market management such as product change and quality control in addition to approval. Here, we review the current regulatory status of AIMD in the Republic of Korea and what needs to be improved for AIMD to be more developed and activated.",
        "keywords": []
    },
    "c8e7df0c5a875c40f652a00a00c6c32b2bc93f05.pdf": {
        "title": "Clinical Comparable Corpus Describing the Same Subjects with Different Expressions",
        "authors": [
            "Yuta Nakamura",
            "S. Hanaoka",
            "Y. Nomura",
            "N. Hayashi",
            "O. Abe",
            "Shunrato Yada",
            "Shoko Wakamiya",
            "E. Aramaki"
        ],
        "published_date": "2022",
        "abstract": "Medical artificial intelligence (AI) systems need to learn to recognize synonyms or paraphrases describing the same anatomy, disease, treatment, etc. to better understand real-world clinical documents. Existing linguistic resources focus on variants at the word or sentence level. To handle linguistic variations on a broader scale, we proposed the Medical Text Radiology Report section Japanese version (MedTxt-RR-JA), the first clinical comparable corpus. MedTxt-RR-JA was built by recruiting nine radiologists to diagnose the same 15 lung cancer cases in Radiopaedia, an open-access radiological repository. The 135 radiology reports in MedTxt-RR-JA were shown to contain word-, sentence- and document-level variations maintaining similarity of contents. MedTxt-RR-JA is also the first publicly available Japanese radiology report corpus that would help to overcome poor data availability for Japanese medical AI systems. Moreover, our methodology can be applied widely to building clinical corpora without privacy concerns.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c8e7df0c5a875c40f652a00a00c6c32b2bc93f05.pdf",
        "venue": "Medinfo",
        "citationCount": 6,
        "score": 2.0,
        "summary": "Medical artificial intelligence (AI) systems need to learn to recognize synonyms or paraphrases describing the same anatomy, disease, treatment, etc. to better understand real-world clinical documents. Existing linguistic resources focus on variants at the word or sentence level. To handle linguistic variations on a broader scale, we proposed the Medical Text Radiology Report section Japanese version (MedTxt-RR-JA), the first clinical comparable corpus. MedTxt-RR-JA was built by recruiting nine radiologists to diagnose the same 15 lung cancer cases in Radiopaedia, an open-access radiological repository. The 135 radiology reports in MedTxt-RR-JA were shown to contain word-, sentence- and document-level variations maintaining similarity of contents. MedTxt-RR-JA is also the first publicly available Japanese radiology report corpus that would help to overcome poor data availability for Japanese medical AI systems. Moreover, our methodology can be applied widely to building clinical corpora without privacy concerns.",
        "keywords": []
    },
    "2253c84f0d7422bbbec496893948e1e1297bcab2.pdf": {
        "title": "Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data",
        "authors": [
            "Felix J. Dorfner",
            "Amin Dada",
            "Felix Busch",
            "Marcus R. Makowski",
            "T. Han",
            "Daniel Truhn",
            "J. Kleesiek",
            "Madhumita Sushil",
            "Jacqueline Lammert",
            "Lisa C. Adams",
            "K. Bressem"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have shown potential in biomedical applications, leading to efforts to fine-tune them on domain-specific data. However, the effectiveness of this approach remains unclear. This study evaluates the performance of biomedically fine-tuned LLMs against their general-purpose counterparts on a variety of clinical tasks. We evaluated their performance on clinical case challenges from the New England Journal of Medicine (NEJM) and the Journal of the American Medical Association (JAMA) and on several clinical tasks (e.g., information extraction, document summarization, and clinical coding). Using benchmarks specifically chosen to be likely outside the fine-tuning datasets of biomedical models, we found that biomedical LLMs mostly perform inferior to their general-purpose counterparts, especially on tasks not focused on medical knowledge. While larger models showed similar performance on case tasks (e.g., OpenBioLLM-70B: 66.4% vs. Llama-3-70B-Instruct: 65% on JAMA cases), smaller biomedical models showed more pronounced underperformance (e.g., OpenBioLLM-8B: 30% vs. Llama-3-8B-Instruct: 64.3% on NEJM cases). Similar trends were observed across the CLUE (Clinical Language Understanding Evaluation) benchmark tasks, with general-purpose models often performing better on text generation, question answering, and coding tasks. Our results suggest that fine-tuning LLMs to biomedical data may not provide the expected benefits and may potentially lead to reduced performance, challenging prevailing assumptions about domain-specific adaptation of LLMs and highlighting the need for more rigorous evaluation frameworks in healthcare AI. Alternative approaches, such as retrieval-augmented generation, may be more effective in enhancing the biomedical capabilities of LLMs without compromising their general knowledge.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/2253c84f0d7422bbbec496893948e1e1297bcab2.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n**1. Research Problem & Motivation**\n\n*   **Problem**: The paper addresses the specific technical problem of evaluating the actual effectiveness of biomedically fine-tuned Large Language Models (LLMs) compared to their general-purpose counterparts, particularly when confronted with unseen medical data \\cite{dorfner2024vdu}.\n*   **Motivation**: LLMs demonstrate significant potential in biomedical applications (e.g., knowledge retrieval, data structuring, clinical coding). This has led to substantial efforts in fine-tuning these models on domain-specific datasets (e.g., BioMistral-7b, OpenBioLLM). However, the paper highlights challenges with existing evaluation methods (like USMLE questions, which are prone to data leakage) and questions the added value of fine-tuning, given that general-purpose LLMs are already trained on vast web data likely containing extensive medical information. This raises concerns about redundant learning or catastrophic forgetting in fine-tuned models \\cite{dorfner2024vdu}.\n\n**2. Related Work & Positioning**\n\n*   **Relation to existing approaches**: The work directly compares state-of-the-art biomedically fine-tuned LLMs (e.g., OpenBioLLM, BioMistral-7b, Meditron-7B, MedAlpaca-7B, PMC-Llama-7B, ClinicalCamel-70B, Med42, JSL-MedMNX-7B-SFT) against their general-purpose foundation models (Llama 2/3, Mistral 7B) \\cite{dorfner2024vdu}.\n*   **Limitations of previous solutions**: Previous evaluations often relied on benchmarks susceptible to data contamination (e.g., USMLE questions), which may not accurately reflect a model's performance in real clinical practice or its ability to generalize to truly novel medical information. The paper posits that the extensive pre-training of generalist LLMs on diverse web data might already encompass much of the medical knowledge, thus limiting the unique benefits of subsequent domain-specific fine-tuning \\cite{dorfner2024vdu}.\n\n**3. Technical Approach & Innovation**\n\n*   **Core technical method**: The study employs a comprehensive comparative evaluation framework. It systematically assesses the performance of various general-purpose and biomedically fine-tuned LLMs across a diverse set of clinical tasks. A key aspect is the use of *recently published and newly developed benchmarks* (e.g., clinical case challenges from NEJM/JAMA, CLUE benchmark tasks) that were specifically chosen to be likely outside the fine-tuning datasets of the biomedical models, ensuring an evaluation on \"unseen\" medical data \\cite{dorfner2024vdu}.\n*   **Novelty/Difference**: The primary innovation lies in its rigorous, contamination-aware evaluation methodology. By selecting benchmarks designed to be *unseen* during biomedical fine-tuning, the study directly challenges the prevailing assumption that domain-specific fine-tuning universally improves performance. It provides a direct, head-to-head comparison on a broad spectrum of clinical tasks, moving beyond traditional, potentially compromised medical benchmarks \\cite{dorfner2024vdu}.\n\n**4. Key Technical Contributions**\n\n*   **Empirical Challenge to Fine-tuning Efficacy**: The paper provides strong empirical evidence suggesting that biomedical fine-tuning may not yield superior performance over generalist LLMs on unseen medical data, and can even lead to performance degradation, particularly for smaller models \\cite{dorfner2024vdu}.\n*   **Robust Evaluation Framework**: It introduces and applies a comprehensive evaluation suite comprising diverse clinical tasks (e.g., case challenges, information extraction, summarization, coding, natural language inference, long document comprehension) using recent data sources (NEJM, JAMA, MIMIC-IV, MIMIC-III) designed to minimize data leakage and ensure a fair assessment of generalization capabilities \\cite{dorfner2024vdu}.\n*   **Comparative Performance Analysis**: Offers a detailed comparative analysis of state-of-the-art generalist LLMs (Llama 2/3, Mistral) against their biomedically fine-tuned counterparts across various model sizes (7B, 8B, 70B parameters), providing granular insights into their relative strengths and weaknesses on different clinical tasks \\cite{dorfner2024vdu}.\n\n**5. Experimental Validation**\n\n*   **Experiments Conducted**: The study evaluated 15 LLMs (7 generalist, 8 biomedical) across 7 distinct clinical benchmarks: Clinical Case Challenges (NEJM, JAMA), MeDiSumQA, MeDiSumCode, MedNLI, MeQSum, ProblemSummary, and LongHealth \\cite{dorfner2024vdu}.\n*   **Key Performance Metrics**: Accuracy for case challenges and MedNLI; ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L) and BERTScore for text generation tasks; F1-scores (exact/approx) and valid code ratio for MeDiSumCode; UMLS F1 for ProblemSummary; and accuracy for LongHealth subtasks \\cite{dorfner2024vdu}.\n*   **Comparison Results**:\n    *   **General Trend**: Biomedical LLMs generally performed *inferior* to their general-purpose counterparts, especially on tasks not solely focused on medical knowledge \\cite{dorfner2024vdu}.\n    *   **Case Challenges**: While larger biomedical models (e.g., OpenBioLLM-70B) showed *similar* performance to generalist models (e.g., Llama-3-70B-Instruct) (e.g., 66.4% vs. 65% on JAMA), smaller biomedical models (e.g., OpenBioLLM-8B) *significantly underperformed* their generalist counterparts (e.g., 30% vs. 64.3% on NEJM) \\cite{dorfner2024vdu}.\n    *   **CLUE Benchmarks**: General-purpose models often performed better across text generation, question answering, and coding tasks. Llama-3-70B-Instruct frequently achieved the highest scores. Biomedical models like BioMistral-7B consistently underperformed their base Mistral-7B-Instruct-v0.2 \\cite{dorfner2024vdu}.\n    *   **Hallucination**: OpenBioLLM-70B showed lower scores than its base model on LongHealth Task 3 (evaluating propensity to hallucinate non-existing information), suggesting a potential increase in hallucination \\cite{dorfner2024vdu}.\n\n**6. Limitations & Scope**\n\n*   **Technical Limitations/Assumptions**: The study's core assumption that benchmarks were \"likely outside the fine-tuning datasets\" is based on careful selection but cannot be definitively proven due to the opaque nature of LLM training data. The fine-tuning process itself might inadvertently introduce biases or overly narrow the models\u2019 focus, leading to reduced generalization capabilities or catastrophic forgetting \\cite{dorfner2024vdu}.\n*   **Scope of Applicability**: The findings are primarily applicable to the current generation of biomedically fine-tuned LLMs and their generalist counterparts, specifically when evaluated on *unseen* medical data. The study suggests that the benefits of fine-tuning might be limited when the domain-specific data does not introduce truly novel information beyond what generalist models have already learned from vast web corpora \\cite{dorfner2024vdu}.\n\n**7. Technical Significance**\n\n*   **Advancement of State-of-the-Art**: This paper significantly advances the technical understanding of LLM adaptation in healthcare by empirically challenging the widely held belief that domain-specific fine-tuning is inherently superior. It provides critical evidence that a broader, general knowledge base might be more beneficial for complex clinical reasoning and generalization to novel tasks \\cite{dorfner2024vdu}.\n*   **Potential Impact on Future Research**: The findings highlight the urgent need for more rigorous and robust evaluation frameworks in healthcare AI, emphasizing the use of unseen data to prevent benchmark corruption. It redirects research focus towards alternative strategies like Retrieval-Augmented Generation (RAG) to enhance biomedical capabilities without compromising general knowledge, and encourages a re-evaluation of fine-tuning methodologies to mitigate risks like catastrophic forgetting or overfitting \\cite{dorfner2024vdu}.",
        "keywords": [
            "Biomedically fine-tuned Large Language Models",
            "General-purpose LLMs",
            "Comparative evaluation framework",
            "Unseen medical data",
            "Data leakage",
            "Catastrophic forgetting",
            "Clinical tasks",
            "Empirical challenge to fine-tuning efficacy",
            "Performance degradation",
            "Generalization capabilities",
            "Hallucination",
            "Robust evaluation frameworks",
            "Retrieval-Augmented Generation (RAG)",
            "Healthcare AI"
        ],
        "paper_type": "**empirical**\n\n**reasoning:**\n\nthe abstract and introduction clearly describe a study that involves:\n*   **evaluation of performance:** \"this study evaluates the performance of biomedically fine-tuned llms against their general-purpose counterparts...\"\n*   **data-driven methodology:** \"we evaluated their performance on clinical case challenges from the new england journal of medicine (nejm) and the journal of the american medical association (jama) and on several clinical tasks...\"\n*   **specific benchmarks:** \"using benchmarks specifically chosen to be likely outside the fine-tuning datasets...\" and \"across the clue (clinical language understanding evaluation) benchmark tasks...\"\n*   **findings and statistical analysis (implied by performance metrics):** \"we found that biomedical llms mostly perform inferior...\", \"while larger models showed similar performance...\", \"smaller biomedical models showed more pronounced underperformance...\"\n*   **conclusions drawn from findings:** \"our results suggest that fine-tuning llms to biomedical data may not provide the expected benefits...\"\n\nthese elements directly align with the criteria for an **empirical** paper, which focuses on data-driven studies with statistical analysis and findings."
    },
    "48bae2de68dcf4535a7de76ce15d447e503fddc8.pdf": {
        "title": "Advancing Conversational Diagnostic AI with Multimodal Reasoning",
        "authors": [
            "Khaled Saab",
            "Jan Freyberg",
            "Chunjong Park",
            "Tim Strother",
            "Yong Cheng",
            "Wei-Hung Weng",
            "David G. T. Barrett",
            "David Stutz",
            "Nenad Toma\u0161ev",
            "Anil Palepu",
            "Valentin Li'evin",
            "Yash Sharma",
            "Roma Ruparel",
            "Abdullah Ahmed",
            "Elahe Vedadi",
            "K. Kanada",
            "C\u00edan Hughes",
            "Yun Liu",
            "Geoff Brown",
            "Yang Gao",
            "Sean Li",
            "S. Mahdavi",
            "James Manyika",
            "Katherine Chou",
            "Yossi Matias",
            "A. Hassidim",
            "Dale R. Webster",
            "Pushmeet Kohli",
            "S. M. A. Eslami",
            "Joelle K. Barral",
            "Adam Rodman",
            "Vivek Natarajan",
            "Mike Schaekermann",
            "Tao Tu",
            "A. Karthikesalingam",
            "Ryutaro Tanno"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) have demonstrated great potential for conducting diagnostic conversations but evaluation has been largely limited to language-only interactions, deviating from the real-world requirements of remote care delivery. Instant messaging platforms permit clinicians and patients to upload and discuss multimodal medical artifacts seamlessly in medical consultation, but the ability of LLMs to reason over such data while preserving other attributes of competent diagnostic conversation remains unknown. Here we advance the conversational diagnosis and management performance of the Articulate Medical Intelligence Explorer (AMIE) through a new capability to gather and interpret multimodal data, and reason about this precisely during consultations. Leveraging Gemini 2.0 Flash, our system implements a state-aware dialogue framework, where conversation flow is dynamically controlled by intermediate model outputs reflecting patient states and evolving diagnoses. Follow-up questions are strategically directed by uncertainty in such patient states, leading to a more structured multimodal history-taking process that emulates experienced clinicians. We compared AMIE to primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of chat-based consultations with patient actors. We constructed 105 evaluation scenarios using artifacts like smartphone skin photos, ECGs, and PDFs of clinical documents across diverse conditions and demographics. Our rubric assessed multimodal capabilities and other clinically meaningful axes like history-taking, diagnostic accuracy, management reasoning, communication, and empathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9 multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The results show clear progress in multimodal conversational diagnostic AI, but real-world translation needs further research.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/48bae2de68dcf4535a7de76ce15d447e503fddc8.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing Large Language Model (LLM)-based diagnostic AI systems are largely limited to text-only interactions, which deviates significantly from real-world remote care delivery where multimodal medical artifacts (images, documents) are routinely used and crucial for accurate diagnosis and management \\cite{saab20254gm}.\n    *   **Importance & Challenge**: This text-only limitation increases the risk of diagnostic errors, hinders the AI's ability to form a complete clinical picture, and can exacerbate disparities in telehealth access. The challenge lies in enabling LLMs to robustly request, interpret, and reason about diverse multimodal medical data within a dynamic conversational flow, while maintaining other attributes of competent diagnostic interaction.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon previous iterations of the Articulate Medical Intelligence Explorer (AMIE) system, which demonstrated physician-like capabilities in text-based clinical conversations \\cite{saab20254gm}.\n    *   **Limitations of Previous Solutions**: Prior AMIE versions and other LLM-based medical AI systems have predominantly been studied and implemented as text-only chatbots, lacking the ability to integrate and reason over multimodal information essential for effective remote care. Evidence validating LLMs for diagnostic conversations involving multimodal data was scarce.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces a novel **multimodal state-aware dialogue phase transition framework** for AMIE, leveraging Gemini 2.0 Flash \\cite{saab20254gm}. This framework dynamically controls the conversation flow through three distinct phases: (1) History Taking, (2) Diagnosis & Management, and (3) Follow-up.\n    *   **Novelty**: The approach's innovation lies in implementing an **explicit state-aware reasoning system layered on top of the LLM**, rather than relying solely on complex prompting. This system uses intermediate model outputs reflecting the evolving patient state, diagnostic hypotheses, and uncertainty to strategically direct follow-up questions, request relevant multimodal artifacts (e.g., skin images, ECGs, clinical documents), accurately interpret findings, and seamlessly integrate this information to refine diagnoses and guide further questioning, emulating experienced clinicians.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Multimodal State-Aware Reasoning at Inference**: A novel framework that orchestrates the conversation flow, dynamically adapting AMIE\u2019s responses based on intermediate model outputs reflecting evolving patient state, diagnostic hypotheses, and uncertainty. This enables strategic requesting, accurate interpretation, and seamless integration of multimodal artifacts (skin images, ECGs, clinical documents) into the dialogue \\cite{saab20254gm}.\n    *   **System Design/Architectural Innovations**:\n        *   **Comprehensive Simulation Environment for Dialogue Evaluation**: A framework for rapid iteration and robust automated assessment. It involves generating realistic patient scenarios with multimodal artifacts, simulating turn-by-turn multimodal dialogues between an AMIE agent and a patient agent, and employing an auto-rater agent to evaluate against clinical criteria \\cite{saab20254gm}.\n    *   **Evaluation Methodology Innovation**:\n        *   **Dedicated Multimodal Evaluation OSCE Rubric**: Development and application of a specific \"Multimodal Understanding & Handling (MUH)\" rubric within an Objective Structured Clinical Examination (OSCE) framework to rigorously assess AMIE's and PCPs' competence in handling and interpreting multimodal artifacts during consultations \\cite{saab20254gm}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: A randomized, double-blind study comparing AMIE to Primary Care Physicians (PCPs) in chat-based consultations, designed in the style of an Objective Structured Clinical Examination (OSCE) \\cite{saab20254gm}.\n    *   **Setup**: 25 patient actors engaged in synchronous chat consultations based on 105 diverse multimodal scenarios. Scenarios included common artifacts like smartphone photos of skin conditions, ECG tracings, and PDFs of clinical documents across various conditions and demographics.\n    *   **Evaluation**: 18 specialists evaluated AMIE and PCPs using a rubric assessing multimodal capability, history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy.\n    *   **Key Performance Metrics & Results**:\n        *   AMIE demonstrated **superior performance relative to PCPs in handling and reasoning about multimodal data on 7 out of 9 axes** of the multimodal rubric \\cite{saab20254gm}.\n        *   AMIE also displayed **similar or superior performance in non-multimodal metrics, including diagnostic accuracy, on 29 out of 32 axes** compared to PCPs \\cite{saab20254gm}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study focused on specific types of multimodal artifacts (skin photos, ECGs, clinical documents). The system's performance with other modalities or more complex multimodal reasoning tasks is not fully explored.\n    *   **Scope of Applicability**: While demonstrating clear progress, the authors state that \"real-world translation necessitates further research\" \\cite{saab20254gm}. The evaluation was conducted in a simulated OSCE environment with patient actors, not real clinical settings with actual patients.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work represents a significant advancement in conversational diagnostic AI by moving beyond text-only LLM systems to effectively integrate multimodal reasoning into clinical dialogues \\cite{saab20254gm}. It demonstrates that an AI system can strategically request, interpret, and reason about diverse medical artifacts in a structured, clinician-like manner.\n    *   **Potential Impact**: The ability to handle multimodal data in diagnostic conversations has the potential to greatly augment remote healthcare delivery, improve diagnostic accuracy, reduce clinician burden, and enhance patient convenience, particularly in regions with limited access to primary care. It lays foundational work for future research into more comprehensive and robust multimodal medical AI systems.",
        "keywords": [
            "Multimodal Diagnostic AI",
            "Large Language Models (LLMs)",
            "Remote Care Delivery",
            "Articulate Medical Intelligence Explorer (AMIE)",
            "Multimodal State-Aware Dialogue Framework",
            "Explicit State-Aware Reasoning System",
            "Multimodal Simulation Environment",
            "Multimodal Evaluation OSCE Rubric",
            "Objective Structured Clinical Examination (OSCE)",
            "Superior Multimodal Data Handling",
            "Diagnostic Accuracy",
            "Clinical Conversations"
        ],
        "paper_type": "based on the provided abstract and introduction:\n\n1.  **abstract analysis:** the abstract defines evaluation criteria (quantscore, qualentry, ddxaccuracy, managementappropriateness, gatheringinformation) and then presents specific quantitative scores and qualitative feedback for an \"auto-rater agent.\" this includes \"ddx top-1 accuracy: yes,\" \"management appropriateness: fair (3/5),\" and detailed qualitative feedback on treatment, follow-up, and information gathering. this content is a direct report of **findings** from an **experiment** or **study** evaluating an ai system.\n\n2.  **introduction analysis:** the introduction sets the context by discussing global healthcare challenges and the promise of ai systems (especially llms) to address them. it references previous work (amie system, real-world deployments) that demonstrated physician-like capabilities and promising performance in studies involving patient actors. this background information supports the relevance of an **empirical study** evaluating ai in a diagnostic context.\n\n**classification criteria match:**\n\n*   the abstract clearly mentions \"findings\" (the scores and qualitative feedback) from a \"study\" or \"experiment\" (the evaluation of the auto-rater agent).\n*   the introduction discusses the \"research questions\" implicitly (how ai can address healthcare challenges) and mentions \"methodology\" (evaluating ai systems, referencing previous studies with \"patient actors\").\n\nwhile the title \"advancing conversational diagnostic ai with multimodal reasoning\" suggests a technical paper (proposing new methods), the *provided abstract content* is entirely focused on the **results and findings of an evaluation**. many technical papers include an empirical evaluation, but if the abstract itself is primarily a report of these findings, it leans heavily towards empirical.\n\ntherefore, the most fitting classification for the provided content is **empirical**."
    },
    "30bee7c88ecb3a341904d674988bb1cece0fdec3.pdf": {
        "title": "MASS: A Multiattribute Sketch Secure Data Sharing Scheme for IoT Wearable Medical Devices Based on Blockchain",
        "authors": [
            "Lin Chen",
            "Yuxiang Chen",
            "Wei Liang",
            "Xiong Li",
            "Kuan Ching Li",
            "Jin Wang",
            "Neal N. Xiong"
        ],
        "published_date": "2025",
        "abstract": "With the swift advancement of the Internet of Things (IoT) and artificial intelligence (AI), various technologies have been integrated into wearable medical health devices, improving users\u2019 awareness of their physical states and enabling the analysis of a greater amount of human data. However, these sensitive pieces of information are prone to tampering or theft during storage and transmission, posing security risks. In this article, we propose a multiattribute sketch secure data sharing scheme for IoT wearable medical devices based on blockchain (MASS). We introduce a multiattribute sketch storage method that stores the encrypted hash of health data transmitted by medical wearable devices on the blockchain. This work also designs a ciphertext-policy attribute-based encryption (CP-ABE) access control mechanism that effectively addresses the secure sharing of data from wearable medical devices among healthcare professionals. Experimental findings indicate that with the rise in the number of medical health data documents, the costs associated with index generation and search time decrease by 55.3% and 10.83%, respectively. Additionally, as the frequency of data access increases, there is a 13.5% reduction in encryption time, and the implementation of multiattribute sketches results in a 24.8% and 11.3% reduction in index generation and search times, respectively.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/30bee7c88ecb3a341904d674988bb1cece0fdec3.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 5,
        "score": 5.0,
        "summary": "With the swift advancement of the Internet of Things (IoT) and artificial intelligence (AI), various technologies have been integrated into wearable medical health devices, improving users\u2019 awareness of their physical states and enabling the analysis of a greater amount of human data. However, these sensitive pieces of information are prone to tampering or theft during storage and transmission, posing security risks. In this article, we propose a multiattribute sketch secure data sharing scheme for IoT wearable medical devices based on blockchain (MASS). We introduce a multiattribute sketch storage method that stores the encrypted hash of health data transmitted by medical wearable devices on the blockchain. This work also designs a ciphertext-policy attribute-based encryption (CP-ABE) access control mechanism that effectively addresses the secure sharing of data from wearable medical devices among healthcare professionals. Experimental findings indicate that with the rise in the number of medical health data documents, the costs associated with index generation and search time decrease by 55.3% and 10.83%, respectively. Additionally, as the frequency of data access increases, there is a 13.5% reduction in encryption time, and the implementation of multiattribute sketches results in a 24.8% and 11.3% reduction in index generation and search times, respectively.",
        "keywords": []
    },
    "38e3c07394a9da343bbe3f3132cd79dec8c920b0.pdf": {
        "title": "AI-driven intelligent document processing for healthcare and insurance",
        "authors": [
            "Ramesh Pingili"
        ],
        "published_date": "2025",
        "abstract": "Healthcare and insurance industries handle millions of documents daily, leading to administrative bottlenecks, errors, and inefficiencies. This paper explores AI-driven Intelligent Document Processing (IDP) for automating claims, medical records, and regulatory compliance documents. Integrating machine learning (ML), natural language processing (NLP), and RPA, IDP reduces document processing time by 80% and error rates by 90%. Case studies from top hospitals and insurers demonstrate AI\u2019s role in streamlining workflows, enhancing patient care, and accelerating claims processing [1]. This research establishes IDP as a critical AI-driven transformation for document-heavy industries.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/38e3c07394a9da343bbe3f3132cd79dec8c920b0.pdf",
        "venue": "International Journal of Science and Research Archive",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Here is a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Analysis of \"AI-driven intelligent document processing for healthcare and insurance\" \\cite{pingili2025fug}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the significant administrative bottlenecks, high error rates, inefficiencies, and financial losses stemming from the manual or outdated processing of millions of documents daily in the healthcare and insurance industries \\cite{pingili2025fug}. This includes medical records, insurance claims, compliance reports, and policy documents.\n    *   **Importance and Challenge:** This problem is critical due to its impact on operational performance, customer experience, fraud prevention, and regulatory compliance. Challenges arise from the sheer volume of unstructured, handwritten, and semi-structured data, which traditional rule-based systems and manual methods cannot efficiently handle or scale with, leading to delays (e.g., 30-90 days for healthcare claims), high error rates (e.g., 22% billing errors), and substantial fraud losses (over $80 billion annually in insurance) \\cite{pingili2025fug}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The paper positions its work as an advanced evolution beyond traditional document processing methods, which include manual data entry, paper-based workflows, and fixed rule-based automation \\cite{pingili2025fug}.\n    *   **Limitations of Previous Solutions:** Traditional methods are highlighted as failing due to:\n        *   Inability to handle unstructured or complex data.\n        *   Reliance on predefined formats, making them inflexible to document variations.\n        *   Susceptibility to human errors (e.g., 22% billing errors in healthcare, 18% insurance claim denials due to inaccurate info).\n        *   Lack of self-learning capabilities, requiring constant manual updates.\n        *   Inefficient fraud detection and compliance tracking due to limited integration with AI-driven analytics \\cite{pingili2025fug}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes and details an AI-driven Intelligent Document Processing (IDP) solution. This approach integrates multiple advanced AI technologies: Machine Learning (ML), Natural Language Processing (NLP), Optical Character Recognition (OCR), and Robotic Process Automation (RPA) \\cite{pingili2025fug}.\n    *   **Novelty/Differentiation:** The innovation lies in the synergistic integration of these AI components to create a continuously learning and adaptive system. Unlike traditional automation, IDP can handle structured, semi-structured, and unstructured data, learn from historical data, adapt to new document variations, and improve accuracy over time through ML. NLP provides deep contextual understanding of complex text, while OCR ensures high-accuracy digitization, and RPA orchestrates automated workflows \\cite{pingili2025fug}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   **Machine Learning (ML) for Automated Learning:** Enables IDP to identify patterns, inconsistencies, and self-learn from past corrections, refining document classification and reducing errors by 85% \\cite{pingili2025fug}.\n        *   **Natural Language Processing (NLP) for Text Understanding:** Allows IDP to interpret complex text (handwritten notes, legal jargon), tokenize text into structured elements, classify documents contextually, and automate decision-making for approvals and risk assessments \\cite{pingili2025fug}.\n        *   **Optical Character Recognition (OCR) for Image-to-Text Extraction:** Digitizes various document types (printed, scanned, handwritten) with high accuracy (97%+) while recognizing structure and formatting, ensuring document integrity \\cite{pingili2025fug}.\n        *   **Robotic Process Automation (RPA) for Workflow Automation:** Acts as a bridge between IDP and legacy systems, automating repetitive tasks like data entry, claims adjudication, and record updates based on AI-extracted information, reducing human workload by 75% \\cite{pingili2025fug}.\n    *   **System Design/Architectural Innovations:** The IDP framework is designed to automate the entire document processing lifecycle, from ingestion and classification to data extraction, validation, and routing, eliminating manual intervention and continuously adapting \\cite{pingili2025fug}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The paper presents multiple case studies and comparative analyses demonstrating the impact of AI-driven IDP in both healthcare and insurance sectors.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Overall Efficiency:** Reduced document processing time by 80% and error rates by 90% \\cite{pingili2025fug}.\n        *   **Claims Processing:** Reduced time from 4-6 weeks to 24-48 hours (80-85% faster) in insurance, and from 30-90 days to 24-72 hours in healthcare. Denial rates cut from 18-25% to <5% \\cite{pingili2025fug}.\n        *   **Accuracy:** Document accuracy improved from 75% to 99.8%. Data entry accuracy in healthcare improved from 79% to 99.2% \\cite{pingili2025fug}.\n        *   **Fraud Detection:** Enhanced fraud detection rate, detecting 50% more fraud before settlement. Accuracy improved from 60-70% to 99.5% in healthcare and 65-75% to 99.2% in insurance, saving a European insurer $15 million annually \\cite{pingili2025fug}.\n        *   **Compliance:** Compliance review time decreased from 2-4 months to 2 weeks (85% reduction). Audit processing time reduced by 85% (from 3-6 months to 2-3 weeks) in insurance \\cite{pingili2025fug}.\n        *   **Medical Records:** Retrieval time reduced from 20 minutes per file to under 5 seconds. Errors reduced by 42% in a leading hospital \\cite{pingili2025fug}.\n        *   **Prior Authorization:** Approval time cut from 5-14 days to under 24 hours, reducing physician workload by 60% \\cite{pingili2025fug}.\n        *   **Medical Billing & Coding:** Coding accuracy enhanced from 85% to 99.6%. Billing time reduced from 2-4 hours to <30 minutes per claim \\cite{pingili2025fug}.\n        *   **Underwriting:** Policy approval time reduced from 2-4 weeks to under 48 hours, with risk assessment accuracy improving by 30% \\cite{pingili2025fug}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** While not explicitly stated, potential implicit limitations could include the initial investment required for implementation, the need for high-quality training data for ML models, and the ongoing maintenance and retraining of AI models to adapt to evolving document formats or regulatory changes. The paper assumes the availability of sufficient digital or scannable document inputs.\n    *   **Scope of Applicability:** The research is specifically focused on and validated within document-heavy industries, primarily healthcare and insurance \\cite{pingili2025fug}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** The paper demonstrates that AI-driven IDP significantly advances the technical state-of-the-art by moving beyond rigid rule-based automation to an adaptive, self-learning system capable of handling complex, unstructured data with near-human intelligence \\cite{pingili2025fug}. It establishes a new benchmark for accuracy, speed, and efficiency in document processing.\n    *   **Potential Impact on Future Research:** This work highlights IDP as a critical AI-driven transformation, paving the way for future research in:\n        *   Developing more sophisticated ML models for even higher accuracy in diverse document types.\n        *   Enhancing NLP for deeper contextual understanding and automated decision-making in highly nuanced scenarios.\n        *   Exploring IDP applications in other document-intensive sectors beyond healthcare and insurance.\n        *   Integrating IDP with blockchain for enhanced data security and auditability in regulated industries \\cite{pingili2025fug}.\n        *   Further optimizing the synergy between AI components for real-time, end-to-end autonomous document workflows.",
        "keywords": [
            "AI-driven Intelligent Document Processing (IDP)",
            "Machine Learning (ML)",
            "Natural Language Processing (NLP)",
            "Optical Character Recognition (OCR)",
            "Robotic Process Automation (RPA)",
            "Unstructured data processing",
            "Healthcare and insurance applications",
            "Synergistic AI integration",
            "Continuously learning systems",
            "Operational efficiency",
            "Fraud detection",
            "Reduced error rates",
            "Automated workflows",
            "Regulatory compliance"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **abstract analysis:**\n    *   \"this paper explores ai-driven intelligent document processing (idp) for automating claims, medical records, and regulatory compliance documents.\" - indicates an exploration of an application.\n    *   \"integrating machine learning (ml), natural language processing (nlp), and rpa, idp reduces document processing time by 80% and error rates by 90%.\" - describes the capabilities and benefits of idp.\n    *   \"**case studies from top hospitals and insurers demonstrate ai\u2019s role** in streamlining workflows, enhancing patient care, and accelerating claims processing [1].\" - this is a very strong indicator. the paper explicitly mentions using \"case studies\" to \"demonstrate\" the application and impact of idp in specific real-world contexts.\n    *   \"this research establishes idp as a critical ai-driven transformation for document-heavy industries.\" - a concluding statement about the significance, likely drawn from the exploration and case studies.\n\n2.  **introduction analysis:**\n    *   the introduction details \"the document overload problem in healthcare and insurance,\" providing specific challenges, statistics, and consequences. this sets the stage for a solution that would be demonstrated through practical applications.\n\n3.  **matching to criteria:**\n    *   **survey**: while it \"explores\" idp, it doesn't use keywords like \"review,\" \"comprehensive analysis,\" or \"state-of-the-art literature.\"\n    *   **technical**: it describes idp's components (ml, nlp, rpa) but doesn't \"propose,\" \"develop,\" or \"present\" a *new* method or algorithm.\n    *   **theoretical**: no mathematical analysis, proofs, or formal models are mentioned.\n    *   **empirical**: while it mentions \"case studies,\" it doesn't explicitly state \"experiment,\" \"data collection,\" or \"statistical analysis\" performed by *this paper* in the abstract. however, \"case studies\" often involve empirical observation.\n    *   **case_study**: this is the strongest fit. the abstract explicitly mentions \"case studies\" and focuses on the \"application\" of idp in specific industries (healthcare and insurance) to \"demonstrate\" its role. the introduction further details the specific context and real-world problems that these applications address.\n    *   **position**: while the paper aims to \"establish idp as critical,\" it does so by demonstrating its effectiveness through case studies, making \"case_study\" a more precise classification of its methodology.\n    *   **short**: no indicators of brevity or preliminary results.\n\nthe explicit mention of \"case studies\" in the abstract, combined with the focus on specific applications within the healthcare and insurance industries, aligns perfectly with the definition of a **case_study** paper.\n\n**classification:** case_study"
    },
    "89f1a1abb18ff4fcca3a54df51905a4647bf36ba.pdf": {
        "title": "Quantum-assisted federated intelligent diagnosis algorithm with variational training supported by 5G networks",
        "authors": [
            "Arnaldo Rafael Camara Araujo",
            "O. Okey",
            "Muhammad Saadi",
            "P. Adasme",
            "R. L. Rosa",
            "D. Z. Rodr\u00edguez"
        ],
        "published_date": "2024",
        "abstract": "In the realm of intelligent healthcare, there is a growing ambition to reshape medical services through the integration of artificial intelligence (AI). However, conventional machine learning faces inherent challenges such as privacy issues, delayed updates, and protracted training times, particularly due to the hesitance of medical institutions to directly share sensitive data, with possible noises. In response to these concerns, a Quantum-Assisted Federated Intelligent Diagnosis Algorithm (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\beta $$\\end{document}\u03b2-QuAFIDA) is proposed, applied into real medical data. Leveraging the capabilities of the 5G mobile network, this approach works the connection between Internet of Medical Things (IoMT) devices through the 5G, synchronizing training and updating the server model without disrupting their real-world applications. In our quest to safeguard patient data and enhance training efficiency, our study employs an innovative heuristic approach marked by a nested loop structure. Specifically, the inner loop is dedicated to training the beta-variational quantum eigensolver (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\beta $$\\end{document}\u03b2-VQE) to approximate the expectation values of the proposed algorithm; the outer loop trains the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\beta $$\\end{document}\u03b2-QuAFIDA to reduce the relative entropy towards the target. This approach involves a balance between privacy considerations and the urgency of training. Results demonstrate that representations with low-rank attained through \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\beta $$\\end{document}\u03b2-QuAFIDA offer an effective approach for acquiring low-rank states. This research signifies a step forward in the synergy between AI and 5G technologies, presenting a novel avenue for the advancement of intelligent healthcare.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/89f1a1abb18ff4fcca3a54df51905a4647bf36ba.pdf",
        "venue": "Scientific Reports",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Here's a focused summary of the technical paper \\cite{araujo2024mb6} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Conventional machine learning in intelligent healthcare faces significant challenges, including patient data privacy concerns, delayed model updates, and protracted training times, particularly due to medical institutions' reluctance to share sensitive data \\cite{araujo2024mb6}. While Federated Learning (FL) addresses privacy, it introduces communication overhead, vulnerability to attacks, and slow convergence, especially as the number of devices grows \\cite{araujo2024mb6}.\n    *   **Importance and Challenge:** Safeguarding highly sensitive patient data while enabling effective AI-driven medical diagnosis is crucial. The challenge lies in developing a robust, efficient, and secure learning paradigm that can leverage distributed data without compromising privacy, overcome FL's inherent drawbacks, and integrate with modern communication infrastructures like 5G for real-time applications \\cite{araujo2024mb6}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon Federated Learning (FL) and Quantum Federated Learning (QFL) concepts. Previous FL approaches in healthcare focused on privacy preservation using techniques like secure aggregation and homomorphic encryption \\cite{araujo2024mb6}. QFL has been explored for data security using blind quantum computing, but implementations often face challenges with hybrid QML algorithms, diverse scenarios, and dataset applicability \\cite{araujo2024mb6}. Variational Quantum Eigensolver (VQE) techniques have been applied to medical data, but often yield accuracy not superior to 85% \\cite{araujo2024mb6}.\n    *   **Limitations of Previous Solutions:** Existing FL methods can suffer from communication overhead, model inversion attacks, biased models, slow convergence, and increased computational costs \\cite{araujo2024mb6}. Current QFL implementations are not deeply explored in diverse scenarios and datasets, and VQE applications in healthcare have shown limited accuracy \\cite{araujo2024mb6}. The paper positions its work as a novel QFL algorithm that specifically addresses these limitations by integrating VQA with FL and 5G for intelligent diagnosis, aiming for higher fidelity and efficiency.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes the \u03b2-Quantum-Assisted Federated Intelligent Diagnosis Algorithm (\u03b2-QuAFIDA), a quantum federated learning framework. It integrates Variational Quantum Algorithms (VQAs), specifically the \u03b2-Variational Quantum Eigensolver (\u03b2-VQE), into an FL framework, supported by 5G mobile networks for communication between Internet of Medical Things (IoMT) devices \\cite{araujo2024mb6}.\n    *   **Novelty/Difference:**\n        *   **Nested Loop Heuristic:** \u03b2-QuAFIDA employs an innovative heuristic approach with a nested loop structure. The inner loop trains the \u03b2-VQE to approximate expectation values, while the outer loop trains \u03b2-QuAFIDA to reduce relative entropy towards the target \\cite{araujo2024mb6}. This balances privacy and training urgency.\n        *   **Quantum-Classical Hybrid Training:** Local clients function as quantum simulators, training circuit parameters via a hybrid quantum-classical approach. The central server aggregates these quantum-enhanced model parameters \\cite{araujo2024mb6}.\n        *   **5G Integration:** Leverages 5G networks to facilitate low-latency, high-bandwidth communication between IoMT devices, enabling synchronized training and model updates without disrupting real-world applications \\cite{araujo2024mb6}.\n        *   **Computational Efficiency:** Utilizes \u03b2-VQE Optimization to mitigate computational costs and exploit quantum parallelism and entanglement for potentially exponential speedup and enhanced convergence rates compared to classical methods \\cite{araujo2024mb6}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** Introduction of \u03b2-QuAFIDA, a quantum federated learning framework specifically designed for intelligent diagnosis within 5G mobile networks, demonstrating consistent accuracy even in noisy environments \\cite{araujo2024mb6}.\n    *   **Optimization Technique:** Mitigation of computational costs through the \u03b2-Variational Quantum Eigensolver Optimization process, which enhances computational efficiency and convergence rates by leveraging quantum principles to select relevant quantum states \\cite{araujo2024mb6}.\n    *   **Data Encoding Exploration:** Exploration of encoding classical method information into quantum states via the Quantum Circuit Optimization process \\cite{araujo2024mb6}.\n    *   **System Design:** A simulated architectural framework integrating QuEST for quantum simulation, PennyLane/PyTorch for VQNNs and \u03b2-VQE, FLSim for Federated Learning, and Pysim5G for 5G/IoMT integration \\cite{araujo2024mb6}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The proposed \u03b2-QuAFIDA model was rigorously tested on both classical and quantum medical data within a simulated healthcare system environment \\cite{araujo2024mb6}. The simulation framework integrated various tools: QuEST for quantum circuit simulation, PennyLane with PyTorch for VQNNs and \u03b2-VQE, FLSim for federated learning simulation, and Pysim5G for 5G/IoMT integration \\cite{araujo2024mb6}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The \u03b2-QuAFIDA demonstrated the ability to obtain a **high fidelity model** \\cite{araujo2024mb6}.\n        *   Results showed that low-rank representations attained through \u03b2-QuAFIDA offer an **effective approach for acquiring low-rank states**, applicable to classical data and low-temperature quantum tomography \\cite{araujo2024mb6}.\n        *   The model's **accuracy remained consistent even in noisy environments** \\cite{araujo2024mb6}.\n        *   The \u03b2-VQE Optimization process was shown to **mitigate computational costs** and enhance computational efficiency and convergence rates compared to classical machine learning methods \\cite{araujo2024mb6}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The work operates within a simulated environment, implying that real-world deployment on current noisy intermediate-scale quantum (NISQ) devices might face challenges related to limited qubit counts and inherent noise processes that restrict circuit depth \\cite{araujo2024mb6}. The paper assumes the availability of quantum-computing capabilities at \"Quantum Clients\" \\cite{araujo2024mb6}.\n    *   **Scope of Applicability:** The \u03b2-QuAFIDA is specifically crafted for \"intelligent diagnosis\" in healthcare, tailoring datasets for patients with distinct medical conditions \\cite{araujo2024mb6}. Its applicability is demonstrated within the context of 5G-enabled IoMT communication for decentralized medical data processing.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This research significantly advances the technical state-of-the-art by proposing a novel quantum-assisted federated learning framework that effectively addresses critical challenges in intelligent healthcare, such as privacy, efficiency, and communication overhead \\cite{araujo2024mb6}. It demonstrates a practical integration of quantum computing (VQA), federated learning, and 5G networks.\n    *   **Potential Impact on Future Research:** The \u03b2-QuAFIDA framework opens new avenues for research in hybrid quantum-classical machine learning, secure and efficient distributed AI in sensitive domains, and the practical application of quantum algorithms in real-world scenarios (via 5G integration) \\cite{araujo2024mb6}. It highlights the potential for quantum parallelism and entanglement to achieve exponential speedup and enhanced optimization in complex healthcare analytics, paving the way for more robust and privacy-preserving intelligent healthcare systems \\cite{araujo2024mb6}.",
        "keywords": [
            "\u03b2-Quantum-Assisted Federated Intelligent Diagnosis Algorithm (\u03b2-QuAFIDA)",
            "Quantum Federated Learning (QFL)",
            "Variational Quantum Algorithms (VQAs)",
            "\u03b2-Variational Quantum Eigensolver (\u03b2-VQE)",
            "Intelligent Healthcare",
            "Patient Data Privacy",
            "5G-enabled IoMT",
            "Quantum-Classical Hybrid Training",
            "Nested Loop Heuristic",
            "Computational Efficiency",
            "High Fidelity Model",
            "Consistent Accuracy in Noisy Environments"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the **title** explicitly mentions \"algorithm\" and \"variational training\".\n*   the **abstract** identifies challenges with conventional machine learning (privacy, delayed updates, training times) in healthcare, implying a need for a new solution.\n*   the **introduction** states: \"introduction to a quantum federated learning framework...\", \"the \u03b2-quafida model...\", \"mitigation of computational costs through the \u03b2-variational quantum eigensolver optimization process.\", \"exploration of the encoding classical method information into quantum states, though the quantum circuit optimization process.\", and \"the proposed framework utilizes the beta-variational quantum eigensolver optimization technique...\". it then explains how this technique operates and its advantages.\n\nthese phrases strongly indicate the paper is presenting a **new method, algorithm, or system** (a quantum federated learning framework, a model, optimization processes, and techniques). while it mentions a result (\"accuracy remained consistent\"), the primary focus of the provided text is on introducing and describing the proposed technical solution.\n\ntherefore, this paper is best classified as **technical**."
    },
    "fc4a9eb434c2ee54d977711f5aec6d39b1ca4283.pdf": {
        "title": "Integrating AI with MBSE for Data Extraction from Medical Standards",
        "authors": [
            "Ibrahim Ghanawi",
            "Mohammad Chami",
            "Mohammad Chami",
            "Marko Coric",
            "Nabil Abdoun"
        ],
        "published_date": "2024",
        "abstract": "The growing adoption of Model\u2010Based Systems Engineering (MBSE) in the medical sector has prompted a significant emphasis on the digitization of medical standards into norm models. This transformation promotes consistency and allows for tracing system model elements to the corresponding norm model elements. Despite these efforts, the current digitization activities heavily rely on manual extraction and transformation, particularly from PDF documents into SysML models. Concurrently, the proliferation of Artificial Intelligence (AI) applications in recent years presents an opportunity to automate such activities. This paper contributes to the integration of AI with MBSE, focusing solely on the extraction and transformation of medical standards information from documents into SysML norm models. It explores the initial outcomes of augmenting data extraction from medical standards using recent AI algorithms and integrating them into MBSE practices. The evaluation involves two approaches, an open\u2010source multimodal classifier model and a proprietary large language model. The study assesses these approaches on a medical standard and outlines future work, including the exploration of an open\u2010source large language model approach.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/fc4a9eb434c2ee54d977711f5aec6d39b1ca4283.pdf",
        "venue": "INCOSE International Symposium",
        "citationCount": 4,
        "score": 4.0,
        "summary": "The growing adoption of Model\u2010Based Systems Engineering (MBSE) in the medical sector has prompted a significant emphasis on the digitization of medical standards into norm models. This transformation promotes consistency and allows for tracing system model elements to the corresponding norm model elements. Despite these efforts, the current digitization activities heavily rely on manual extraction and transformation, particularly from PDF documents into SysML models. Concurrently, the proliferation of Artificial Intelligence (AI) applications in recent years presents an opportunity to automate such activities. This paper contributes to the integration of AI with MBSE, focusing solely on the extraction and transformation of medical standards information from documents into SysML norm models. It explores the initial outcomes of augmenting data extraction from medical standards using recent AI algorithms and integrating them into MBSE practices. The evaluation involves two approaches, an open\u2010source multimodal classifier model and a proprietary large language model. The study assesses these approaches on a medical standard and outlines future work, including the exploration of an open\u2010source large language model approach.",
        "keywords": []
    },
    "e458ecb3d481077045dd7a3060d094fab95e3602.pdf": {
        "title": "Artificial Intelligence (AI) in Endourology: Maximizing the Promise Through Consideration of the Principles of Diffusion of Innovation Theory.",
        "authors": [
            "Manoj Monga",
            "Natalie Edwards",
            "S. Rojanasarot",
            "Mital Patel",
            "Erin Turner",
            "Jenifer White",
            "Samir Bhattacharyya"
        ],
        "published_date": "2024",
        "abstract": "INTRODUCTION\nDiffusion of Innovation Theory explains how ideas or products gain momentum and diffuse (or spread) through specific populations or social systems over time. The theory analyzes primary influencers of the spread of new ideas, including the innovation itself, communication channels, time, and social systems.\n\n\nMETHODS\nThe current study reviewed published medical literature to identify studies and applications of artificial intelligence (AI) in endourology and utilized E.M. Rogers' Diffusion of Innovation Theory to analyze the primary influencers of the adoption of AI in endourological care. The insights gained were triaged and prioritized into AI application-related action items or 'tips' for facilitating the appropriate diffusion of the most valuable endourological innovations.\n\n\nRESULTS\nPublished medical literature indicates that AI is still a research-based tool in endourology and is not widely used in clinical practice. The published studies have presented AI models and algorithms to assist with stone disease detection (n=17), the prediction of management outcomes (n=18), the optimization of operative procedures (n=9), and the elucidation of stone disease chemistry and composition (n=24). Five tips for facilitating appropriate adoption of endourological AI are: (1) Develop/prioritize training programs to establish the foundation for effective use; (2) Create appropriate data infrastructure for implementation, including its maintenance and evolution over time; (3) Deliver AI transparency to gain the trust of endourology stakeholders; (4) Adopt innovations in the context of continuous quality improvement (CQI) Plan-Do-Study-Act (PDSA) cycles as these approaches have proven track records for improving care quality; and (5) Be realistic about what AI can/cannot currently do and document to establish the basis for shared understanding.\n\n\nCONCLUSION\nDiffusion of Innovation Theory provides a framework for analyzing the influencers of the adoption of AI in endourological care. The five tips identified through this research may be used to facilitate appropriate diffusion of the most valuable endourological innovations.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/e458ecb3d481077045dd7a3060d094fab95e3602.pdf",
        "venue": "Journal of endourology",
        "citationCount": 4,
        "score": 4.0,
        "summary": "INTRODUCTION\nDiffusion of Innovation Theory explains how ideas or products gain momentum and diffuse (or spread) through specific populations or social systems over time. The theory analyzes primary influencers of the spread of new ideas, including the innovation itself, communication channels, time, and social systems.\n\n\nMETHODS\nThe current study reviewed published medical literature to identify studies and applications of artificial intelligence (AI) in endourology and utilized E.M. Rogers' Diffusion of Innovation Theory to analyze the primary influencers of the adoption of AI in endourological care. The insights gained were triaged and prioritized into AI application-related action items or 'tips' for facilitating the appropriate diffusion of the most valuable endourological innovations.\n\n\nRESULTS\nPublished medical literature indicates that AI is still a research-based tool in endourology and is not widely used in clinical practice. The published studies have presented AI models and algorithms to assist with stone disease detection (n=17), the prediction of management outcomes (n=18), the optimization of operative procedures (n=9), and the elucidation of stone disease chemistry and composition (n=24). Five tips for facilitating appropriate adoption of endourological AI are: (1) Develop/prioritize training programs to establish the foundation for effective use; (2) Create appropriate data infrastructure for implementation, including its maintenance and evolution over time; (3) Deliver AI transparency to gain the trust of endourology stakeholders; (4) Adopt innovations in the context of continuous quality improvement (CQI) Plan-Do-Study-Act (PDSA) cycles as these approaches have proven track records for improving care quality; and (5) Be realistic about what AI can/cannot currently do and document to establish the basis for shared understanding.\n\n\nCONCLUSION\nDiffusion of Innovation Theory provides a framework for analyzing the influencers of the adoption of AI in endourological care. The five tips identified through this research may be used to facilitate appropriate diffusion of the most valuable endourological innovations.",
        "keywords": []
    },
    "da0e6d5131defb7991b51848ff34e40f20b4a390.pdf": {
        "title": "Application of AI in the creation of discharge summaries in psychiatric clinics.",
        "authors": [
            "Bertrand Janota",
            "Krzysztof Janota"
        ],
        "published_date": "2024",
        "abstract": "BACKGROUND\nThe integration of artificial intelligence (AI; ChatGPT 4.0) into medical workflows presents a great potential to enhance efficiency and quality. The use of artificial intelligence in the creation of discharge summaries seems particularly interesting and valid. The course of each hospitalization is described in the discharge summary, which is given to each patient and then to his general practitioner at the end of hospital treatment. An exploratory analysis of discharge summaries in psychiatric clinics underscores that these documents must fulfill diverse and specific requirements. Nevertheless, AI-generated discharge summaries offer the opportunity to optimize information transfer and alleviate the workload on physicians.\n\n\nMETHOD\nThe study evaluates the quality of discharge summaries produced by clinical staff and by an AI model (ChatGPT 4.0). The clinicians involved in writing of the discharge summaries were not informed about the study's purpose or methodology. The completed summaries were subsequently assessed by four attending physicians using predefined criteria. These physicians were also blinded to the study's objectives and were unaware of the individual authors of the summaries. The evaluation criteria included consistency, completeness, and comprehensibility. Additionally, the time required to prepare these summaries and its impact on overall quality were analyzed.\n\n\nRESULTS\nThe results of the study indicate that discharge summaries generated by AI are more efficient than discharge summaries prepared by clinic staff. The AI was particularly effective in terms of coherence and information structure.\n\n\nCONCLUSION\nFurther research, training and development is needed to improve the accuracy and reliability of AI-generated discharge summaries.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/da0e6d5131defb7991b51848ff34e40f20b4a390.pdf",
        "venue": "International Journal of Psychiatry in Medicine",
        "citationCount": 4,
        "score": 4.0,
        "summary": "BACKGROUND\nThe integration of artificial intelligence (AI; ChatGPT 4.0) into medical workflows presents a great potential to enhance efficiency and quality. The use of artificial intelligence in the creation of discharge summaries seems particularly interesting and valid. The course of each hospitalization is described in the discharge summary, which is given to each patient and then to his general practitioner at the end of hospital treatment. An exploratory analysis of discharge summaries in psychiatric clinics underscores that these documents must fulfill diverse and specific requirements. Nevertheless, AI-generated discharge summaries offer the opportunity to optimize information transfer and alleviate the workload on physicians.\n\n\nMETHOD\nThe study evaluates the quality of discharge summaries produced by clinical staff and by an AI model (ChatGPT 4.0). The clinicians involved in writing of the discharge summaries were not informed about the study's purpose or methodology. The completed summaries were subsequently assessed by four attending physicians using predefined criteria. These physicians were also blinded to the study's objectives and were unaware of the individual authors of the summaries. The evaluation criteria included consistency, completeness, and comprehensibility. Additionally, the time required to prepare these summaries and its impact on overall quality were analyzed.\n\n\nRESULTS\nThe results of the study indicate that discharge summaries generated by AI are more efficient than discharge summaries prepared by clinic staff. The AI was particularly effective in terms of coherence and information structure.\n\n\nCONCLUSION\nFurther research, training and development is needed to improve the accuracy and reliability of AI-generated discharge summaries.",
        "keywords": []
    },
    "67335a676ae3b4e17a4494edf19b2101484cd5b4.pdf": {
        "title": "AI-assisted Blockchain-enabled Smart and Secure E-prescription Management Framework",
        "authors": [
            "Siva Sai",
            "V. Chamola"
        ],
        "published_date": "2024",
        "abstract": "Traditional medical prescriptions based on physical paper-based documents are prone to manipulation, errors, and unauthorized reproduction due to their format. Addressing the limitations of the traditional prescription system, e-prescription systems have been introduced in several countries. However, e-prescription systems lead to several concerns like the risk of privacy loss, the problem of double-spending prescriptions, lack of interoperability, and single point of failure, all of which need to be addressed immediately. We propose an AI-assisted blockchain-enabled smart and secure e-prescription management framework to address these issues. Our proposed system overcomes the problems of the centralized e-prescription systems and enables efficient consent management to access prescriptions by incorporating blockchain-based smart contracts. Our work incorporates the Umbral proxy re-encryption scheme in the proposed system, avoiding the need for repeated encryption and decryption of the prescriptions when transferred between different entities in the network. In our work, we employ two different machine learning models(Random Forest classifier and LightGBM classifier) to assist the doctor in prescribing medicines. One is a drug recommendation model, which is aimed at providing drug recommendations considering the medical history of the patients and the general prescription pattern for the particular ailment of the patient. We have fine-tuned the SciBERT model for adverse drug reaction detection. The extensive experimentation and results show that the proposed e-prescription framework is secure, scalable, and interoperable. Further, the proposed machine learning models produce results higher than 95%.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/67335a676ae3b4e17a4494edf19b2101484cd5b4.pdf",
        "venue": "ACM Transactions on Internet Technology",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Traditional medical prescriptions based on physical paper-based documents are prone to manipulation, errors, and unauthorized reproduction due to their format. Addressing the limitations of the traditional prescription system, e-prescription systems have been introduced in several countries. However, e-prescription systems lead to several concerns like the risk of privacy loss, the problem of double-spending prescriptions, lack of interoperability, and single point of failure, all of which need to be addressed immediately. We propose an AI-assisted blockchain-enabled smart and secure e-prescription management framework to address these issues. Our proposed system overcomes the problems of the centralized e-prescription systems and enables efficient consent management to access prescriptions by incorporating blockchain-based smart contracts. Our work incorporates the Umbral proxy re-encryption scheme in the proposed system, avoiding the need for repeated encryption and decryption of the prescriptions when transferred between different entities in the network. In our work, we employ two different machine learning models(Random Forest classifier and LightGBM classifier) to assist the doctor in prescribing medicines. One is a drug recommendation model, which is aimed at providing drug recommendations considering the medical history of the patients and the general prescription pattern for the particular ailment of the patient. We have fine-tuned the SciBERT model for adverse drug reaction detection. The extensive experimentation and results show that the proposed e-prescription framework is secure, scalable, and interoperable. Further, the proposed machine learning models produce results higher than 95%.",
        "keywords": []
    },
    "932e9cc1777b5c7857d05e85073592aa57bd9c7a.pdf": {
        "title": "The Role of AI in Cardiovascular Event Monitoring and Early Detection: Scoping Literature Review",
        "authors": [
            "L. Elvas",
            "Ana Almeida",
            "Jo\u00e3o C. Ferreira"
        ],
        "published_date": "2025",
        "abstract": "Abstract Background Artificial intelligence (AI) has shown exponential growth and advancements, revolutionizing various fields, including health care. However, domain adaptation remains a significant challenge, as machine learning (ML) models often need to be applied across different health care settings with varying patient demographics and practices. This issue is critical for ensuring effective and equitable AI deployment. Cardiovascular diseases (CVDs), the leading cause of global mortality with 17.9 million annual deaths, encompass conditions like coronary heart disease and hypertension. The increasing availability of medical data, coupled with AI advancements, offers new opportunities for early detection and intervention in cardiovascular events, leveraging AI\u2019s capacity to analyze complex datasets and uncover critical patterns. Objective This review aims to examine AI methodologies combined with medical data to advance the intelligent monitoring and detection of CVDs, identifying areas for further research to enhance patient outcomes and support early interventions. Methods This review follows the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) methodology to ensure a rigorous and transparent literature review process. This structured approach facilitated a comprehensive overview of the current state of research in this field. Results Through the methodology used, 64 documents were retrieved, of which 40 documents met the inclusion criteria. The reviewed papers demonstrate advancements in AI and ML for CVD detection, classification, prediction, diagnosis, and patient monitoring. Techniques such as ensemble learning, deep neural networks, and feature selection improve prediction accuracy over traditional methods. ML models predict cardiovascular events and risks, with applications in monitoring via wearable technology. The integration of AI in health care supports early detection, personalized treatment, and risk assessment, possibly improving the management of CVDs. Conclusions The study concludes that AI and ML techniques can improve the accuracy of CVD classification, prediction, diagnosis, and monitoring. The integration of multiple data sources and noninvasive methods supports continuous monitoring and early detection. These advancements help enhance CVD management and patient outcomes, indicating the potential for AI to offer more precise and cost-effective solutions in health care.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/932e9cc1777b5c7857d05e85073592aa57bd9c7a.pdf",
        "venue": "JMIR Medical Informatics",
        "citationCount": 4,
        "score": 4.0,
        "summary": "This analysis focuses on the PRISMA-ScR Checklist itself, interpreting it as the \"technical paper\" provided, rather than the full explanatory article that describes its development.\n\n### Focused Summary for Literature Review\n\nThis paper, the Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR) Checklist \\cite{elvas202541o}, provides a standardized framework for reporting scoping reviews.\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The lack of a dedicated, standardized reporting guideline for scoping reviews, leading to heterogeneity, reduced transparency, and potential difficulties in assessing their quality and reproducibility.\n    *   **Importance and Challenge**: Scoping reviews are increasingly utilized to map evidence and identify research gaps across broad topics. Without clear reporting standards, it is challenging for authors to ensure comprehensive reporting and for readers/reviewers to critically appraise the methodology and findings, hindering the synthesis and utility of this type of evidence.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is an extension of the widely adopted PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) statement, which provides reporting guidelines for systematic reviews.\n    *   **Limitations of Previous Solutions**: The original PRISMA statement is tailored for systematic reviews, which typically focus on specific intervention questions and involve rigorous critical appraisal of individual studies. Scoping reviews, however, have distinct objectives (e.g., mapping concepts, identifying evidence types) and often include a broader range of evidence, making the original PRISMA less suitable for their comprehensive reporting.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The core method is the development of a structured, 22-item checklist specifically designed for reporting scoping reviews. This involves identifying and defining essential reporting elements across seven key sections: Title, Abstract, Introduction, Methods, Results, Discussion, and Funding.\n    *   **Novelty/Difference**: The innovation lies in adapting and extending an established reporting framework (PRISMA) to the unique methodological characteristics of scoping reviews. It provides a tailored set of reporting items that address aspects specific to scoping reviews, such as the broad nature of information sources, data charting processes, and the synthesis of diverse evidence types, which differ from traditional systematic reviews.\n\n4.  **Key Technical Contributions**\n    *   **Novel Methods/Techniques**: Introduces a novel, structured methodological framework for reporting scoping reviews, ensuring consistency and completeness. It clarifies and defines terms specific to scoping reviews (e.g., \"sources of evidence,\" \"data charting,\" \"critical appraisal of individual sources of evidence\" instead of \"risk of bias\").\n    *   **System Design/Architectural Innovations**: Provides a systematic \"architecture\" for structuring a scoping review report, guiding authors through all necessary components from rationale to conclusions.\n    *   **Theoretical Insights/Analysis**: Implicitly highlights the distinct methodological requirements of scoping reviews compared to systematic reviews, thereby justifying the need for a separate, specialized reporting guideline.\n\n5.  **Experimental Validation**\n    *   The provided text is the checklist itself and does not describe experimental validation in the traditional sense of a technical paper (e.g., empirical testing of an algorithm or system).\n    *   However, the development of such a guideline typically involves a rigorous, evidence-informed process (e.g., systematic review of reporting issues, expert consensus methods like Delphi studies). The checklist references its origin from a publication by Tricco et al. (2018), which would detail the systematic process used to develop and validate these reporting items. Based *solely* on the provided checklist, no direct experimental validation is presented.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The checklist serves as a reporting guideline, assuming the user has already conducted a scoping review according to established methodologies. It does not provide guidance on *how to conduct* a scoping review. It also assumes familiarity with the general principles of research reporting.\n    *   **Scope of Applicability**: The PRISMA-ScR Checklist \\cite{elvas202541o} is specifically applicable to scoping reviews. It explicitly differentiates its terminology and requirements from those of systematic reviews (e.g., regarding critical appraisal), indicating its focused scope.\n\n7.  **Technical Significance**\n    *   **Advance State-of-the-Art**: This checklist significantly advances the state-of-the-art in reporting methodological reviews by establishing a formal, standardized guideline for scoping reviews. It enhances the transparency, reproducibility, and overall quality of this increasingly important review methodology.\n    *   **Potential Impact on Future Research**: It provides a common language and structure for authors, reviewers, and readers, facilitating better synthesis of evidence, more reliable interpretation of findings, and clearer identification of research gaps. This standardization is crucial for improving the rigor and impact of future scoping review research.",
        "keywords": [
            "PRISMA-ScR Checklist",
            "Scoping reviews",
            "Standardized reporting guideline",
            "Reporting heterogeneity",
            "Transparency and reproducibility",
            "Evidence mapping",
            "Research gap identification",
            "Systematic reviews",
            "Tailored reporting items",
            "Data charting",
            "Methodological framework",
            "Expert consensus",
            "Quality enhancement"
        ],
        "paper_type": "based on the provided information:\n\n1.  **title:** \"the role of ai in cardiovascular event monitoring and early detection: **scoping literature review**\"\n    *   the term \"literature review\" is a direct indicator of a paper that reviews existing literature. a \"scoping review\" is a specific type of literature review.\n\n2.  **classification criteria for \"survey\":**\n    *   abstract mentions: \"survey\", \"**review**\", \"comprehensive analysis\", \"state-of-the-art\"\n    *   introduction discusses: literature organization, classification schemes\n\nthe title explicitly states \"scoping literature review,\" which perfectly aligns with the definition of a \"survey\" paper that \"reviews existing literature comprehensively.\" although the provided abstract and introduction are templates for a review rather than the actual content, they further reinforce the nature of the paper as a review by detailing the components of a review (objectives, eligibility criteria, sources of evidence, search strategy, selection process).\n\ntherefore, the paper is a **survey**."
    },
    "ef0281435edcd64ae07236fedbca3041f7de8fe4.pdf": {
        "title": "Assessing the documentation of publicly available medical image and signal datasets and their impact on bias using the BEAMRAD tool",
        "authors": [
            "Maria Galanty",
            "Dieuwertje Luitse",
            "Sijm H. Noteboom",
            "Philip Croon",
            "A. Vlaar",
            "Thomas Poell",
            "Clara I. S\u00e1nchez",
            "Tobias Blanke",
            "Ivana I\u0161gum"
        ],
        "published_date": "2024",
        "abstract": "Medical datasets are vital for advancing Artificial Intelligence (AI) in healthcare. Yet biases in these datasets on which deep-learning models are trained can compromise reliability. This study investigates biases stemming from dataset-creation practices. Drawing on existing guidelines, we first developed a BEAMRAD tool to assess the documentation of public Magnetic Resonance Imaging (MRI); Color Fundus Photography (CFP), and Electrocardiogram (ECG) datasets. In doing so, we provide an overview of the biases that may emerge due to inadequate dataset documentation. Second, we examine the current state of documentation for public medical images and signal data. Our research reveals that there is substantial variance in the documentation of image and signal datasets, even though guidelines have been developed in medical imaging. This indicates that dataset documentation is subject to individual discretionary decisions. Furthermore, we find that aspects such as hardware and data acquisition details are commonly documented, while information regarding data annotation practices, annotation error quantification, or data limitations are not consistently reported. This risks having considerable implications for the abilities of data users to detect potential sources of bias through these respective aspects and develop reliable and robust models that can be adapted for clinical practice. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-024-83218-5.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/ef0281435edcd64ae07236fedbca3041f7de8fe4.pdf",
        "venue": "Scientific Reports",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n---\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** The paper addresses the critical issue of biases in medical datasets, particularly those arising from inadequate or inconsistent documentation practices, which compromise the reliability and trustworthiness of deep-learning models in healthcare \\cite{galanty2024u7u}.\n*   **Importance & Challenge:**\n    *   Medical datasets are foundational for AI advancements, but inherent biases (e.g., due to insufficient demographic representation, inconsistent inclusion criteria, or missing data patterns) can lead to systematic errors, causing models to underperform or misclassify for specific patient subgroups \\cite{galanty2024u7u}.\n    *   Detecting and mitigating these biases is challenging because current dataset documentation often lacks standardization and critical details, making it difficult for data users to assess potential risks and develop robust, clinically adaptable models \\cite{galanty2024u7u}.\n    *   While guidelines exist for medical imaging, their adoption is inconsistent, and there's a notable gap in documentation standards for medical signal data \\cite{galanty2024u7u}.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:**\n    *   The work builds upon existing calls for standardized dataset documentation, such as \"datasheets,\" which aim to enhance transparency and accountability in AI research \\cite{galanty2024u7u}.\n    *   It draws from established guidelines like the Biomedical Image Analysis Challenge S (BIAS) initiative for medical imaging challenges and the Critical Appraisal and Data Extraction for Systematic Reviews of Prediction Modelling Studies (CHARMS) checklist \\cite{galanty2024u7u}.\n*   **Limitations of Previous Solutions:**\n    *   Despite guidelines, the paper highlights that documentation practices for medical image and signal datasets exhibit \"substantial variance,\" often relying on individual discretion \\cite{galanty2024u7u}.\n    *   Existing guidelines, particularly BIAS, are primarily focused on medical imaging, leaving a gap for the thorough assessment of documentation for medical signal data (e.g., ECGs) \\cite{galanty2024u7u}.\n    *   Previous approaches often do not consistently cover crucial aspects like detailed data annotation practices, quantification of annotation errors, or explicit reporting of data limitations, which are vital for bias detection \\cite{galanty2024u7u}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper introduces the `Bias Evaluation And Monitoring for Transparent And Reliable Medical Datasets (BEAMRAD)` tool, a comprehensive, questionnaire-based framework designed for the qualitative evaluation of medical dataset documentation \\cite{galanty2024u7u}.\n*   **Novelty/Difference:**\n    *   **Integrated & Bias-Centric Design:** BEAMRAD uniquely synthesizes and expands upon existing documentation frameworks (Datasheets for Datasets, BIAS, CHARMS) into a structured tool with 11 categories and 45 key items, explicitly linking documentation deficiencies to specific types of potential biases (e.g., sampling, representation, annotation, data leakage) \\cite{galanty2024u7u}.\n    *   **Multimodal Applicability:** A key innovation is its capability to thoroughly assess documentation for both medical image data (Magnetic Resonance Imaging - MRI, Color Fundus Photography - CFP) and, notably, medical signal data (Electrocardiograms - ECGs), addressing a significant gap in the latter domain \\cite{galanty2024u7u}.\n    *   **Focus on Overlooked Details:** The tool emphasizes critical documentation aspects often neglected, such as the disciplinary background and expertise of data annotators, the quantification of annotation errors, and explicit reporting of data limitations, which are crucial for understanding and mitigating bias \\cite{galanty2024u7u}.\n\n**4. Key Technical Contributions**\n*   **Novel Evaluation Tool:** The development of BEAMRAD, a structured, questionnaire-based framework specifically engineered to evaluate the completeness and quality of medical dataset documentation with an explicit focus on identifying potential sources of bias \\cite{galanty2024u7u}.\n*   **Systematic Bias-Documentation Mapping:** BEAMRAD provides a systematic mapping between specific elements of dataset documentation (e.g., data sources, metadata, annotation processes) and various types of biases that can emerge from their insufficient reporting, offering a clear pathway for bias detection \\cite{galanty2024u7u}.\n*   **Cross-Modality Documentation Standard:** It establishes a unified framework for assessing documentation across diverse medical data types, including both imaging (MRI, CFP) and signal (ECG) data, thereby promoting consistent quality standards in a broader range of medical AI applications \\cite{galanty2024u7u}.\n*   **Identification of Critical Documentation Gaps:** The tool's design inherently highlights common deficiencies in current documentation practices, particularly concerning data annotation details, error quantification, and explicit data limitations, which are paramount for developing fair and robust AI models \\cite{galanty2024u7u}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:** The authors performed a qualitative review of the documentation accompanying publicly available medical image (MRI and CFP) and signal (ECG) datasets using the newly developed BEAMRAD tool \\cite{galanty2024u7u}. The selection criteria for imaging datasets prioritized those released after the BIAS protocol, while ECG datasets were chosen to address the lack of specific documentation guidelines in that domain \\cite{galanty2024u7u}.\n*   **Key Performance Metrics and Comparison Results:**\n    *   The validation revealed \"substantial variance in the documentation of image and signal datasets,\" indicating a lack of consistent adherence to best practices \\cite{galanty2024u7u}.\n    *   It was observed that \"aspects such as hardware and data acquisition details are commonly documented\" across the evaluated datasets \\cite{galanty2024u7u}.\n    *   However, critical information \"regarding data annotation practices, annotation error quantification, or data limitations are not consistently reported,\" highlighting significant gaps that impede bias detection \\cite{galanty2024u7u}.\n    *   (Note: The provided text offers a high-level summary of the findings rather than detailed quantitative results or specific dataset-by-dataset comparisons, which would likely be present in the full paper.)\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:**\n    *   The BEAMRAD tool, being questionnaire-based, relies on the existence and clarity of documentation; it cannot infer or generate missing information \\cite{galanty2024u7u}.\n    *   The qualitative nature of the review means the assessment of documentation completeness is based on the BEAMRAD criteria, which are a synthesis of existing guidelines, and the interpretation of \"insufficient\" or \"not consistently reported\" is qualitative \\cite{galanty2024u7u}.\n*   **Scope of Applicability:**\n    *   The empirical validation focused on a specific subset of publicly available MRI, CFP, and ECG datasets, primarily those released after the BIAS protocol for imaging, which may not represent the entirety of medical datasets \\cite{galanty2024u7u}.\n    *   The study's primary scope is the identification of *potential* bias sources through documentation analysis, rather than a direct quantification of the impact of these biases on downstream AI model performance \\cite{galanty2024u7u}.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:**\n    *   BEAMRAD significantly advances the state-of-the-art by providing a structured, comprehensive, and bias-aware framework for evaluating medical dataset documentation, moving beyond general recommendations to an actionable assessment tool \\cite{galanty2024u7u}.\n    *   It uniquely extends rigorous documentation assessment to medical signal data, an area previously underserved by specific guidelines, thereby fostering better practices across a broader spectrum of medical AI \\cite{galanty2024u7u}.\n    *   By explicitly linking documentation elements to specific bias types, it offers a more proactive and targeted approach for dataset creators and users to prevent, identify, and mitigate biases at the data source \\cite{galanty2024u7u}.\n*   **Potential Impact on Future Research:**\n    *   **Improved Dataset Quality & Transparency:** BEAMRAD can serve as a standard for promoting more thorough, transparent, and standardized documentation practices, leading to higher-quality and less biased medical datasets for AI development \\cite{galanty2024u7u}.\n    *   **Enhanced Model Trustworthiness & Reproducibility:** It empowers researchers to better detect and address potential sources of bias in training data, facilitating the development of more reliable, robust, and reproducible deep-learning models that can generalize effectively to clinical practice \\cite{galanty2024u7u}.\n    *   **Foundation for AI Ethics:** The tool provides a practical contribution to AI ethics by fostering transparency and accountability in the entire lifecycle of medical dataset production and utilization, which is crucial for responsible AI in healthcare \\cite{galanty2024u7u}.",
        "keywords": [
            "BEAMRAD tool",
            "biases in medical datasets",
            "inadequate documentation practices",
            "deep learning models in healthcare",
            "medical image and signal data",
            "questionnaire-based framework",
            "systematic bias-documentation mapping",
            "data annotation practices",
            "cross-modality documentation standard",
            "improved dataset quality",
            "enhanced model trustworthiness",
            "AI ethics",
            "sampling representation annotation data leakage biases",
            "qualitative evaluation"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"this study investigates biases stemming from dataset-creation practices.\"** - this directly indicates a data-driven investigation.\n2.  **\"we first developed a beamrad tool to assess the documentation of public magnetic resonance imaging (mri); color fundus photography (cfp), and electrocardiogram (ecg) datasets.\"** - while a tool was developed (technical aspect), its purpose is explicitly \"to assess the documentation\" of *public datasets*. this involves collecting and analyzing data (documentation of datasets) to draw conclusions about bias.\n3.  the title \"assessing the documentation... and their impact on bias using the beamrad tool\" emphasizes the *assessment* and *impact* (empirical investigation) using the tool.\n\nthe development of the beamrad tool is presented as a methodology or a means to conduct the primary investigation, which is empirical in nature (studying data to find patterns/biases). the core contribution is the *findings* from applying this tool to real-world datasets.\n\ntherefore, the paper is best classified as **empirical**."
    },
    "e3b8abb83fbad21ef08b4963251e224ef53557f5.pdf": {
        "title": "Use of ambient AI scribing: Impact on physician administrative burden and patient care.",
        "authors": [
            "Gury K. Doshi",
            "T. Jensen",
            "Anna Graziano",
            "Chinenye Enenmoh",
            "James Lindsey"
        ],
        "published_date": "2024",
        "abstract": "\n 418\n \n \n Background:\n Medical documentation is necessary to describe encounters for clinical and billing purposes. The increasing time required for medical documentation is contributing to provider stress and burnout. Physicians divide their time and attention between patients and the electronic health record (EHR), negatively impacting the patient physician relationship.\n Methods:\n 49 physicians from Texas Oncology participated in a pilot from November 2023 to April 2024 using Deep Scribe (DS) ambient artificial intelligence (AI) technology to document patient clinic visits. Subspecialties of medical oncology, radiation oncology, urology, and breast surgery completed opinion surveys pre and post pilot. Provider adoption, weekly usage, satisfaction scores were measured. Evaluation and management (E&M) diagnosis codes for office visits conducted pre-pilot were reviewed for over 27,000 patient encounters and compared to over 4,500 DS patient encounters. Hierarchical condition category (HCC) codes captured via DS AI were compared to manual capture of billing codes in the pre-pilot period.\n Results:\n 77% of providers adopted DS AI technology during the pilot. Median turnaround time for notes was less than 3 minutes. Average satisfaction score was 81%. 91% of providers stated DS was easy to use. 60% of providers believed DS improved the quality of documentation. Providers reported on average 1.5 hours per provider time saved per week. The average number of billed diagnosis codes for non- DS patients was 3 per patient. For the DS patients, the average billed diagnosis per patient was 4.1. Diagnosis codes were categorized as cancer, and other non-cancer HCCs using the CMS-HCC model category version 22. Of the top ten HCC categories, cancer diagnosis codes were better captured by manual coding, whereas DS captured more non-cancer HCC codes across multiple systems, most significantly in endocrine and cardiovascular systems.\n Conclusions:\n Data from this pilot study shows physicians reported DS AI improved quality of their documentation and saved physicians\u2019 time. Billing data shows increase capture of non-cancer HCC codes. Use of AI in medical documentation can decrease physician burden and improve quality of patient care.\n \n \n \n \n E&M Diagnosis Codes\n Non-Deep Scribe\n Deep Scribe\n \n \n \n \n Patient visit count\n 27,236\n 4,584\n \n \n Diagnosis code count\n 82,231\n 18,702\n \n \n Average\n 3.0\n 4.1\n \n \n Median\n 2\n 3\n \n \n Mode\n 2.0\n 1.0\n \n \n \n \n",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/e3b8abb83fbad21ef08b4963251e224ef53557f5.pdf",
        "venue": "JCO Oncology Practice",
        "citationCount": 3,
        "score": 3.0,
        "summary": "\n 418\n \n \n Background:\n Medical documentation is necessary to describe encounters for clinical and billing purposes. The increasing time required for medical documentation is contributing to provider stress and burnout. Physicians divide their time and attention between patients and the electronic health record (EHR), negatively impacting the patient physician relationship.\n Methods:\n 49 physicians from Texas Oncology participated in a pilot from November 2023 to April 2024 using Deep Scribe (DS) ambient artificial intelligence (AI) technology to document patient clinic visits. Subspecialties of medical oncology, radiation oncology, urology, and breast surgery completed opinion surveys pre and post pilot. Provider adoption, weekly usage, satisfaction scores were measured. Evaluation and management (E&M) diagnosis codes for office visits conducted pre-pilot were reviewed for over 27,000 patient encounters and compared to over 4,500 DS patient encounters. Hierarchical condition category (HCC) codes captured via DS AI were compared to manual capture of billing codes in the pre-pilot period.\n Results:\n 77% of providers adopted DS AI technology during the pilot. Median turnaround time for notes was less than 3 minutes. Average satisfaction score was 81%. 91% of providers stated DS was easy to use. 60% of providers believed DS improved the quality of documentation. Providers reported on average 1.5 hours per provider time saved per week. The average number of billed diagnosis codes for non- DS patients was 3 per patient. For the DS patients, the average billed diagnosis per patient was 4.1. Diagnosis codes were categorized as cancer, and other non-cancer HCCs using the CMS-HCC model category version 22. Of the top ten HCC categories, cancer diagnosis codes were better captured by manual coding, whereas DS captured more non-cancer HCC codes across multiple systems, most significantly in endocrine and cardiovascular systems.\n Conclusions:\n Data from this pilot study shows physicians reported DS AI improved quality of their documentation and saved physicians\u2019 time. Billing data shows increase capture of non-cancer HCC codes. Use of AI in medical documentation can decrease physician burden and improve quality of patient care.\n \n \n \n \n E&M Diagnosis Codes\n Non-Deep Scribe\n Deep Scribe\n \n \n \n \n Patient visit count\n 27,236\n 4,584\n \n \n Diagnosis code count\n 82,231\n 18,702\n \n \n Average\n 3.0\n 4.1\n \n \n Median\n 2\n 3\n \n \n Mode\n 2.0\n 1.0\n \n \n \n \n",
        "keywords": []
    },
    "f1770c7894764645e710b9893f267c2b3b067b41.pdf": {
        "title": "Assessment of ChatGPT-4 in Family Medicine Board Examinations Using Advanced AI Learning and Analytical Methods: Observational Study",
        "authors": [
            "A. Goodings",
            "Sten Kajitani",
            "A. Chhor",
            "Ahmad Albakri",
            "M. Pastrak",
            "Megha Kodancha",
            "Rowan Ives",
            "Yoo Bin Lee",
            "Kari M. Kajitani"
        ],
        "published_date": "2024",
        "abstract": "Abstract Background This research explores the capabilities of ChatGPT-4 in passing the American Board of Family Medicine (ABFM) Certification Examination. Addressing a gap in existing literature, where earlier artificial intelligence (AI) models showed limitations in medical board examinations, this study evaluates the enhanced features and potential of ChatGPT-4, especially in document analysis and information synthesis. Objective The primary goal is to assess whether ChatGPT-4, when provided with extensive preparation resources and when using sophisticated data analysis, can achieve a score equal to or above the passing threshold for the Family Medicine Board Examinations. Methods In this study, ChatGPT-4 was embedded in a specialized subenvironment, \u201cAI Family Medicine Board Exam Taker,\u201d designed to closely mimic the conditions of the ABFM Certification Examination. This subenvironment enabled the AI to access and analyze a range of relevant study materials, including a primary medical textbook and supplementary web-based resources. The AI was presented with a series of ABFM-type examination questions, reflecting the breadth and complexity typical of the examination. Emphasis was placed on assessing the AI\u2019s ability to interpret and respond to these questions accurately, leveraging its advanced data processing and analysis capabilities within this controlled subenvironment. Results In our study, ChatGPT-4\u2019s performance was quantitatively assessed on 300 practice ABFM examination questions. The AI achieved a correct response rate of 88.67% (95% CI 85.08%-92.25%) for the Custom Robot version and 87.33% (95% CI 83.57%-91.10%) for the Regular version. Statistical analysis, including the McNemar test (P=.45), indicated no significant difference in accuracy between the 2 versions. In addition, the chi-square test for error-type distribution (P=.32) revealed no significant variation in the pattern of errors across versions. These results highlight ChatGPT-4\u2019s capacity for high-level performance and consistency in responding to complex medical examination questions under controlled conditions. Conclusions The study demonstrates that ChatGPT-4, particularly when equipped with specialized preparation and when operating in a tailored subenvironment, shows promising potential in handling the intricacies of medical board examinations. While its performance is comparable with the expected standards for passing the ABFM Certification Examination, further enhancements in AI technology and tailored training methods could push these capabilities to new heights. This exploration opens avenues for integrating AI tools such as ChatGPT-4 in medical education and assessment, emphasizing the importance of continuous advancement and specialized training in medical applications of AI.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/f1770c7894764645e710b9893f267c2b3b067b41.pdf",
        "venue": "JMIR Medical Education",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Abstract Background This research explores the capabilities of ChatGPT-4 in passing the American Board of Family Medicine (ABFM) Certification Examination. Addressing a gap in existing literature, where earlier artificial intelligence (AI) models showed limitations in medical board examinations, this study evaluates the enhanced features and potential of ChatGPT-4, especially in document analysis and information synthesis. Objective The primary goal is to assess whether ChatGPT-4, when provided with extensive preparation resources and when using sophisticated data analysis, can achieve a score equal to or above the passing threshold for the Family Medicine Board Examinations. Methods In this study, ChatGPT-4 was embedded in a specialized subenvironment, \u201cAI Family Medicine Board Exam Taker,\u201d designed to closely mimic the conditions of the ABFM Certification Examination. This subenvironment enabled the AI to access and analyze a range of relevant study materials, including a primary medical textbook and supplementary web-based resources. The AI was presented with a series of ABFM-type examination questions, reflecting the breadth and complexity typical of the examination. Emphasis was placed on assessing the AI\u2019s ability to interpret and respond to these questions accurately, leveraging its advanced data processing and analysis capabilities within this controlled subenvironment. Results In our study, ChatGPT-4\u2019s performance was quantitatively assessed on 300 practice ABFM examination questions. The AI achieved a correct response rate of 88.67% (95% CI 85.08%-92.25%) for the Custom Robot version and 87.33% (95% CI 83.57%-91.10%) for the Regular version. Statistical analysis, including the McNemar test (P=.45), indicated no significant difference in accuracy between the 2 versions. In addition, the chi-square test for error-type distribution (P=.32) revealed no significant variation in the pattern of errors across versions. These results highlight ChatGPT-4\u2019s capacity for high-level performance and consistency in responding to complex medical examination questions under controlled conditions. Conclusions The study demonstrates that ChatGPT-4, particularly when equipped with specialized preparation and when operating in a tailored subenvironment, shows promising potential in handling the intricacies of medical board examinations. While its performance is comparable with the expected standards for passing the ABFM Certification Examination, further enhancements in AI technology and tailored training methods could push these capabilities to new heights. This exploration opens avenues for integrating AI tools such as ChatGPT-4 in medical education and assessment, emphasizing the importance of continuous advancement and specialized training in medical applications of AI.",
        "keywords": []
    },
    "63f394b905ac6eec49cf0eaa42cbdaa35dd49d01.pdf": {
        "title": "A systematic review of AI-based chatbot usages in healthcare services.",
        "authors": [
            "K. Mohamed Jasim",
            "A. Malathi",
            "Seema Bhardwaj",
            "Eugene Cheng\u2010Xi Aw"
        ],
        "published_date": "2025",
        "abstract": "PURPOSE\nThis systematic literature review aims to provide a comprehensive and structured synthesis of the existing knowledge about chatbots in healthcare from both a theoretical and methodological perspective.\n\n\nDESIGN/METHODOLOGY/APPROACH\nTo this end, a systematic literature review was conducted with 89 articles selected through a SPAR-4-SLR systematic procedure. The document for this systematic review was collected from Scopus database. The VoSviewer software facilitates the analysis of keyword co-occurrence to form the fundamental structure of the subject field.\n\n\nFINDINGS\nIn addition, this study proposes a future research agenda revolving around three main themes such as (1) telemedicine, (2) mental health and (3) medical information.\n\n\nORIGINALITY/VALUE\nThis study underscores the significance, implications and predictors of chatbot usage in healthcare services. It is concluded that adopting the proposed future direction and further research on chatbots in healthcare will help to refine chatbot systems to better meet the needs of patients.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/63f394b905ac6eec49cf0eaa42cbdaa35dd49d01.pdf",
        "venue": "Journal of health organization and management",
        "citationCount": 3,
        "score": 3.0,
        "summary": "PURPOSE\nThis systematic literature review aims to provide a comprehensive and structured synthesis of the existing knowledge about chatbots in healthcare from both a theoretical and methodological perspective.\n\n\nDESIGN/METHODOLOGY/APPROACH\nTo this end, a systematic literature review was conducted with 89 articles selected through a SPAR-4-SLR systematic procedure. The document for this systematic review was collected from Scopus database. The VoSviewer software facilitates the analysis of keyword co-occurrence to form the fundamental structure of the subject field.\n\n\nFINDINGS\nIn addition, this study proposes a future research agenda revolving around three main themes such as (1) telemedicine, (2) mental health and (3) medical information.\n\n\nORIGINALITY/VALUE\nThis study underscores the significance, implications and predictors of chatbot usage in healthcare services. It is concluded that adopting the proposed future direction and further research on chatbots in healthcare will help to refine chatbot systems to better meet the needs of patients.",
        "keywords": []
    },
    "8b12f4303a7981806bf65c345f3b4291b556bfd8.pdf": {
        "title": "Investigating State-of-the-Art Frontiers in Artificial Intelligence: A Synopsis of Trends and Innovations",
        "authors": [
            "Sohana Akter"
        ],
        "published_date": "2024",
        "abstract": "Artificial intelligence (AI) has undergone rapid evolution in recent decades, catalysing the emergence of ground-breaking technologies that have reshaped various sectors. Among these advancements is the advent of autonomous vehicles, poised to revolutionize transportation and mobility. Moreover, AI has spurred the development of cutting-edge solutions in healthcare, exemplified by AI-powered medical imaging systems. This manuscript presents an overview of AI's evolution and explores the latest strides in autonomous vehicles and healthcare innovations. Delving into the foundational technologies like machine learning and computer vision, it elucidates the methodologies employed in crafting autonomous vehicles and healthcare solutions. The document also scrutinizes the advantages and hurdles inherent in these innovations, while offering insights into future avenues of research. Overall, it underscores AI's profound impact on transportation, healthcare, and beyond, underscoring the transformative potential of autonomous vehicles and healthcare technologies in fostering safer and more efficient mobility and healthcare systems.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/8b12f4303a7981806bf65c345f3b4291b556bfd8.pdf",
        "venue": "Journal of Artificial Intelligence General science (JAIGS) ISSN:3006-4023",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Artificial intelligence (AI) has undergone rapid evolution in recent decades, catalysing the emergence of ground-breaking technologies that have reshaped various sectors. Among these advancements is the advent of autonomous vehicles, poised to revolutionize transportation and mobility. Moreover, AI has spurred the development of cutting-edge solutions in healthcare, exemplified by AI-powered medical imaging systems. This manuscript presents an overview of AI's evolution and explores the latest strides in autonomous vehicles and healthcare innovations. Delving into the foundational technologies like machine learning and computer vision, it elucidates the methodologies employed in crafting autonomous vehicles and healthcare solutions. The document also scrutinizes the advantages and hurdles inherent in these innovations, while offering insights into future avenues of research. Overall, it underscores AI's profound impact on transportation, healthcare, and beyond, underscoring the transformative potential of autonomous vehicles and healthcare technologies in fostering safer and more efficient mobility and healthcare systems.",
        "keywords": []
    },
    "c993088ff20dc5d2647f81dd96a8dc46f3e68b60.pdf": {
        "title": "LLMs-in-the-loop Part-1: Expert Small AI Models for Bio-Medical Text Translation",
        "authors": [
            "Bunyamin Keles",
            "Murat Gunay",
            "Serdar I. Caglar"
        ],
        "published_date": "2024",
        "abstract": "Machine translation is indispensable in healthcare for enabling the global dissemination of medical knowledge across languages. However, complex medical terminology poses unique challenges to achieving adequate translation quality and accuracy. This study introduces a novel\"LLMs-in-the-loop\"approach to develop supervised neural machine translation models optimized specifically for medical texts. While large language models (LLMs) have demonstrated powerful capabilities, this research shows that small, specialized models trained on high-quality in-domain (mostly synthetic) data can outperform even vastly larger LLMs. Custom parallel corpora in six languages were compiled from scientific articles, synthetically generated clinical documents, and medical texts. Our LLMs-in-the-loop methodology employs synthetic data generation, rigorous evaluation, and agent orchestration to enhance performance. We developed small medical translation models using the MarianMT base model. We introduce a new medical translation test dataset to standardize evaluation in this domain. Assessed using BLEU, METEOR, ROUGE, and BERT scores on this test set, our MarianMT-based models outperform Google Translate, DeepL, and GPT-4-Turbo. Results demonstrate that our LLMs-in-the-loop approach, combined with fine-tuning high-quality, domain-specific data, enables specialized models to outperform general-purpose and some larger systems. This research, part of a broader series on expert small models, paves the way for future healthcare-related AI developments, including deidentification and bio-medical entity extraction models. Our study underscores the potential of tailored neural translation models and the LLMs-in-the-loop methodology to advance the field through improved data generation, evaluation, agent, and modeling techniques.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c993088ff20dc5d2647f81dd96a8dc46f3e68b60.pdf",
        "venue": "arXiv.org",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Machine translation is indispensable in healthcare for enabling the global dissemination of medical knowledge across languages. However, complex medical terminology poses unique challenges to achieving adequate translation quality and accuracy. This study introduces a novel\"LLMs-in-the-loop\"approach to develop supervised neural machine translation models optimized specifically for medical texts. While large language models (LLMs) have demonstrated powerful capabilities, this research shows that small, specialized models trained on high-quality in-domain (mostly synthetic) data can outperform even vastly larger LLMs. Custom parallel corpora in six languages were compiled from scientific articles, synthetically generated clinical documents, and medical texts. Our LLMs-in-the-loop methodology employs synthetic data generation, rigorous evaluation, and agent orchestration to enhance performance. We developed small medical translation models using the MarianMT base model. We introduce a new medical translation test dataset to standardize evaluation in this domain. Assessed using BLEU, METEOR, ROUGE, and BERT scores on this test set, our MarianMT-based models outperform Google Translate, DeepL, and GPT-4-Turbo. Results demonstrate that our LLMs-in-the-loop approach, combined with fine-tuning high-quality, domain-specific data, enables specialized models to outperform general-purpose and some larger systems. This research, part of a broader series on expert small models, paves the way for future healthcare-related AI developments, including deidentification and bio-medical entity extraction models. Our study underscores the potential of tailored neural translation models and the LLMs-in-the-loop methodology to advance the field through improved data generation, evaluation, agent, and modeling techniques.",
        "keywords": []
    },
    "c2d929ddeef1daf608bfd229266692c776931885.pdf": {
        "title": "Exploring ethical considerations in medical research: Harnessing pre-generated transformers for AI-powered ethics discussions",
        "authors": [
            "Takuya Mori",
            "Takuya Watanabe",
            "Shinji Kosugi"
        ],
        "published_date": "2025",
        "abstract": "Introduction In medical research involving human subjects, ethical review is essential to protect individuals. However, concerns have been raised about variations in ethical review opinions and a decline in review quality. Adequately protecting human subjects requires multifaceted opinions from ethics committee members. Despite the need to increase the number of committee members, resources are limited. To address these challenges, we explored the use of a generative pre- learning transformer, an interactive artificial intelligence (AI) tool, to discuss ethical issues in medical research. Methods The generation AI used in the research used ChatGPT3.5, which has learned ethical guidelines from various countries worldwide. We requested the generative AI to provide insights on ethical considerations for virtual research involving individuals. The obtained answers were documented and verified by experts. Results The AI successfully highlighted considerations for informed consent regarding individuals with dementia and mental illness, as well as concerns about invasiveness in research. It also raised points about potential side effects of off-label drug use. However, it could not offer specific measures for psychological considerations or broader ethical issues, providing limited ethical insights. This limitation may be attributed to biased opinions resulting from machine learning optimization, preventing comprehensive identification of certain ethical issues. Conclusion Although the validity of ethical opinions generated by the generative AI requires further examination, our findings suggest that this technology could be employed to prompt reviews and re-evaluate ethical concerns arising in research.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/c2d929ddeef1daf608bfd229266692c776931885.pdf",
        "venue": "PLoS ONE",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Introduction In medical research involving human subjects, ethical review is essential to protect individuals. However, concerns have been raised about variations in ethical review opinions and a decline in review quality. Adequately protecting human subjects requires multifaceted opinions from ethics committee members. Despite the need to increase the number of committee members, resources are limited. To address these challenges, we explored the use of a generative pre- learning transformer, an interactive artificial intelligence (AI) tool, to discuss ethical issues in medical research. Methods The generation AI used in the research used ChatGPT3.5, which has learned ethical guidelines from various countries worldwide. We requested the generative AI to provide insights on ethical considerations for virtual research involving individuals. The obtained answers were documented and verified by experts. Results The AI successfully highlighted considerations for informed consent regarding individuals with dementia and mental illness, as well as concerns about invasiveness in research. It also raised points about potential side effects of off-label drug use. However, it could not offer specific measures for psychological considerations or broader ethical issues, providing limited ethical insights. This limitation may be attributed to biased opinions resulting from machine learning optimization, preventing comprehensive identification of certain ethical issues. Conclusion Although the validity of ethical opinions generated by the generative AI requires further examination, our findings suggest that this technology could be employed to prompt reviews and re-evaluate ethical concerns arising in research.",
        "keywords": []
    },
    "0e526c254ff4c26bb42fd58d2a9eea5493c097da.pdf": {
        "title": "AI Scribes: Boosting Physician Efficiency in Clinical Documentation",
        "authors": [
            "Omosalewa Itauma",
            "I. Itauma"
        ],
        "published_date": "2024",
        "abstract": "The increasing demand on healthcare systems has amplified the burden on physicians and other healthcare professionals, with a huge portion of time dedicated to documenting patient encounters. Prolonged charting periods not only contribute to decreased physician productivity but also emerge as a prominent factor in physician burnout. This study investigates the potential of Artificial Intelligence (AI) to mitigate this challenge, focusing on AI-powered medical scribing as a solution to alleviate the burden of traditional charting methods in documentation of patient encounters and improve overall physician productivity. This research contributes to the ongoing discourse on the role of AI in healthcare and seeks to inform healthcare professionals, administrators, and policymakers about the potential benefits of integrating AI-powered medical scribing to improve physician efficiency and mitigate the impact of extensive charting on overall productivity and well-being.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/0e526c254ff4c26bb42fd58d2a9eea5493c097da.pdf",
        "venue": "International Journal on Bioinformatics &amp; Biosciences",
        "citationCount": 3,
        "score": 3.0,
        "summary": "The increasing demand on healthcare systems has amplified the burden on physicians and other healthcare professionals, with a huge portion of time dedicated to documenting patient encounters. Prolonged charting periods not only contribute to decreased physician productivity but also emerge as a prominent factor in physician burnout. This study investigates the potential of Artificial Intelligence (AI) to mitigate this challenge, focusing on AI-powered medical scribing as a solution to alleviate the burden of traditional charting methods in documentation of patient encounters and improve overall physician productivity. This research contributes to the ongoing discourse on the role of AI in healthcare and seeks to inform healthcare professionals, administrators, and policymakers about the potential benefits of integrating AI-powered medical scribing to improve physician efficiency and mitigate the impact of extensive charting on overall productivity and well-being.",
        "keywords": []
    },
    "82199d36d788f9683895ef4564103d2d51e0fd82.pdf": {
        "title": "Enhancing patient understanding in obstetrics: The role of generative AI in simplifying informed consent for labor induction with oxytocin",
        "authors": [
            "A. Gr\u00fcnebaum",
            "Joachim W. Dudenhausen",
            "F. Chervenak"
        ],
        "published_date": "2024",
        "abstract": "Abstract Informed consent is a cornerstone of ethical medical practice, particularly in obstetrics where procedures like labor induction carry significant risks and require clear patient understanding. Despite legal mandates for patient materials to be accessible, many consent forms remain too complex, resulting in patient confusion and dissatisfaction. This study explores the use of Generative Artificial Intelligence (GAI) to simplify informed consent for labor induction with oxytocin, ensuring content is both medically accurate and comprehensible at an 8th-grade readability level. GAI-generated consent forms streamline the process, automatically tailoring content to meet readability standards while retaining essential details such as the procedure\u2019s nature, risks, benefits, and alternatives. Through iterative prompts and expert refinement, the AI produces clear, patient-friendly language that bridges the gap between medical jargon and patient comprehension. Flesch Reading Ease scores show improved readability, meeting recommended levels for health literacy. GAI has the potential to revolutionize healthcare communication by enhancing patient understanding, promoting shared decision-making, and improving satisfaction with the consent process. However, human oversight remains critical to ensure that AI-generated content adheres to legal and ethical standards. This case study demonstrates that GAI can be an effective tool in creating accessible, standardized, yet personalized consent documents, contributing to better-informed patients and potentially reducing malpractice claims.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/82199d36d788f9683895ef4564103d2d51e0fd82.pdf",
        "venue": "Journal of Perinatal Medicine",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Abstract Informed consent is a cornerstone of ethical medical practice, particularly in obstetrics where procedures like labor induction carry significant risks and require clear patient understanding. Despite legal mandates for patient materials to be accessible, many consent forms remain too complex, resulting in patient confusion and dissatisfaction. This study explores the use of Generative Artificial Intelligence (GAI) to simplify informed consent for labor induction with oxytocin, ensuring content is both medically accurate and comprehensible at an 8th-grade readability level. GAI-generated consent forms streamline the process, automatically tailoring content to meet readability standards while retaining essential details such as the procedure\u2019s nature, risks, benefits, and alternatives. Through iterative prompts and expert refinement, the AI produces clear, patient-friendly language that bridges the gap between medical jargon and patient comprehension. Flesch Reading Ease scores show improved readability, meeting recommended levels for health literacy. GAI has the potential to revolutionize healthcare communication by enhancing patient understanding, promoting shared decision-making, and improving satisfaction with the consent process. However, human oversight remains critical to ensure that AI-generated content adheres to legal and ethical standards. This case study demonstrates that GAI can be an effective tool in creating accessible, standardized, yet personalized consent documents, contributing to better-informed patients and potentially reducing malpractice claims.",
        "keywords": []
    },
    "83cd33a1b71ce8b5df6289120df41b0f9da3542f.pdf": {
        "title": "Retrieval-augmented systems can be dangerous medical communicators",
        "authors": [
            "Lionel Wong",
            "Ayman Ali",
            "Raymond Xiong",
            "Shannon Shen",
            "Yoon Kim",
            "Monica Agrawal"
        ],
        "published_date": "2025",
        "abstract": "Patients have long sought health information online, and increasingly, they are turning to generative AI to answer their health-related queries. Given the high stakes of the medical domain, techniques like retrieval-augmented generation and citation grounding have been widely promoted as methods to reduce hallucinations and improve the accuracy of AI-generated responses and have been widely adopted into search engines. This paper argues that even when these methods produce literally accurate content drawn from source documents sans hallucinations, they can still be highly misleading. Patients may derive significantly different interpretations from AI-generated outputs than they would from reading the original source material, let alone consulting a knowledgeable clinician. Through a large-scale query analysis on topics including disputed diagnoses and procedure safety, we support our argument with quantitative and qualitative evidence of the suboptimal answers resulting from current systems. In particular, we highlight how these models tend to decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases. We propose a series of recommendations -- such as the incorporation of communication pragmatics and enhanced comprehension of source documents -- that could help mitigate these issues and extend beyond the medical domain.",
        "file_path": "paper_data/AI_for_Medical_Document_Understanding/info/83cd33a1b71ce8b5df6289120df41b0f9da3542f.pdf",
        "venue": "arXiv.org",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Here's a focused summary of the paper \\cite{wong2025sce} for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue that retrieval-augmented generation (RAG) systems, even when producing factually accurate content grounded in sources, can be \"pragmatically misaligned\" and highly misleading in high-stakes domains like healthcare. Patients may derive significantly different and potentially dangerous interpretations from AI-generated outputs compared to original sources or clinician advice.\n    *   **Importance & Challenge**: This problem is crucial because RAG and citation grounding are widely promoted and adopted to reduce hallucinations and improve accuracy in AI-generated responses, especially in search engines. However, \\cite{wong2025sce} demonstrates a novel failure mode where literal accuracy does not equate to safe or effective communication, posing significant risks to patient understanding and well-being. The challenge lies in moving beyond mere factual correctness to incorporate nuanced communicative intent and context.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon and critiques the prevalent RAG paradigm, which focuses on grounding responses in external sources to enhance factual accuracy and reduce hallucinations. It acknowledges the value of RAG in preventing outright fabrication but highlights its limitations in conveying appropriate context and pragmatic meaning.\n    *   **Limitations of Previous Solutions**: Previous solutions, primarily RAG and citation grounding, are limited by their narrow focus on literal accuracy. They often fail to:\n        *   Reason about the underlying goals or biases of a user's query.\n        *   Understand the broader communicative intent of source documents.\n        *   Account for the pragmatic implications and potential misinterpretations of generated text, leading to decontextualization, omission of critical information (e.g., rarity of complications, benefits of procedures), and reinforcement of patient misconceptions or biases.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is an extensive, theoretically-motivated large-scale query analysis of existing RAG-based search engines (Google AI Overview and Perplexity AI) in the medical domain. It systematically probes how these systems respond to queries designed to expose \"pragmatic misalignment.\"\n    *   **Novelty/Difference**: The innovation lies not in proposing a new RAG algorithm, but in:\n        *   **Introducing \"Pragmatic Misalignment\"**: A novel conceptual framework for evaluating AI communication beyond literal accuracy, focusing on how context and unspoken intentions influence interpretation.\n        *   **Systematic Query Design**: Developing a methodology with physician input to create procedurally generated queries that implicitly test for contextual presupposition, query bias, and decontextualization (e.g., \"Symptoms of <CONDITION>\" for disputed diagnoses, \"Why is <PROCEDURE> dangerous/safe,\" \"Complications of <PROCEDURE>\").\n        *   **LLM-as-a-Judge Evaluation**: Employing LLMs (GPT-4o) to programmatically evaluate responses against specific pragmatic criteria (e.g., mention of controversy, rarity statistics, or benefits), providing a scalable method for assessing communicative quality.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   Introduction and empirical demonstration of \"pragmatic misalignment\" as a critical failure mode for RAG systems in high-stakes domains.\n        *   A structured, large-scale query analysis methodology for evaluating the pragmatic communicative quality of RAG systems, including specific query templates and evaluation criteria.\n        *   Application of LLM-as-a-judge for automated evaluation of pragmatic aspects of generated text, such as identifying mentions of controversy or contextualizing statistics.\n    *   **Theoretical Insights or Analysis**: The paper provides a theoretical argument for the necessity of incorporating communication pragmatics into the design of AI systems, moving beyond purely semantic or factual correctness. It highlights the gap between \"narrowly accurate\" and \"pragmatically aligned\" communication.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: A large-scale query analysis was performed on Google AI Overview and Perplexity AI using 178 procedurally generated medical queries across several categories:\n        *   **Disputed conditions (n=13)**: Comparing direct queries vs. queries presupposing existence (e.g., \"Schoenfeld\u2019s Syndrome\" vs. \"Symptoms of Schoenfeld\u2019s Syndrome\").\n        *   **Safety of procedures (n=28)**: Comparing queries with opposing biases (e.g., \"Why is hysterectomy dangerous\" vs. \"Why is hysterectomy safe?\").\n        *   **Complications of procedures (n=28)**: Inquiring about complications to assess contextualization.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Disputed Conditions**: For presupposing queries, only 56% (Google AIO) and 69% (Perplexity) of responses mentioned the condition was controversial, a drastic reduction from 100% for direct queries.\n        *   **Safety of Procedures**: Systems selected significantly different supporting materials based on query bias (average Jaccard similarity of referenced webpages: Google AIO 0.16, Perplexity 0.31). Even when citing the same source, different portions were extracted to reinforce the query's bias.\n        *   **Complications of Procedures**: Responses rarely mentioned the rarity of complications (Google AIO 4%, Perplexity 5%) or the benefits of the procedure (Google AIO 6%, Perplexity 10%), leading to decontextualized and potentially anxiety-inducing information.\n        *   The results quantitatively demonstrate that current RAG systems frequently decontextualize facts, omit critical relevant sources, and reinforce patient misconceptions or biases, even when individual facts are technically accurate.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study primarily focuses on identifying and characterizing the problem of pragmatic misalignment rather than proposing a fully implemented technical solution. The LLM-as-a-judge method, while effective, relies on the capabilities and potential biases of the evaluating LLM. The study acknowledges the ethical complexity of directly quantifying user reasoning about dangerous beliefs, thus relying on quantitative analysis of system outputs rather than direct user studies.\n    *   **Scope of Applicability**: While the paper focuses on the medical domain due to its high stakes, it argues that \"pragmatic misalignment\" is broadly applicable to other consequential query types, suggesting the findings and proposed recommendations extend beyond healthcare.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the technical state-of-the-art by shifting the focus of RAG evaluation beyond mere factual accuracy to the crucial dimension of communicative pragmatics. It identifies a novel and dangerous failure mode in widely deployed AI systems, challenging the assumption that grounding in sources inherently leads to safe and effective communication.\n    *   **Potential Impact on Future Research**: The work has a profound potential impact by:\n        *   **Redefining RAG Evaluation**: Encouraging future research to incorporate pragmatic criteria into the design and evaluation of RAG systems.\n        *   **Guiding System Design**: Proposing a new paradigm for designing AI communicators that explicitly reason about user intent, source context, and the downstream consequences of generated text.\n        *   **Inspiring New Research Directions**: Opening avenues for research into incorporating communication pragmatics, leveraging human-clinician communication data, and developing systems capable of empathetic and context-aware dialogue, not just in medicine but across all high-stakes domains.",
        "keywords": [
            "Retrieval-Augmented Generation (RAG)",
            "Pragmatic Misalignment",
            "High-stakes domains (e.g.",
            "healthcare)",
            "Large-scale query analysis",
            "Systematic query design",
            "LLM-as-a-Judge evaluation",
            "Communicative pragmatics",
            "Decontextualization of facts",
            "Omission of critical information",
            "Reinforcement of misconceptions/biases",
            "Literal accuracy vs. safe communication",
            "AI communicators design"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as a **position** paper.\n\nhere's why:\n\n*   **abstract explicitly states an argument:** \"this paper argues that even when these methods produce literally accurate content... they can still be highly misleading.\" this directly aligns with the \"position\" criterion's mention of \"argue\" and \"viewpoint.\"\n*   **proposes future directions/recommendations:** \"we propose a series of recommendations\u2014such as the incorporation of communication pragmatics and enhanced comprehension of source documents\u2014that could help mitigate these issues...\" this aligns with \"future direction\" and what \"should\" be done.\n*   **empirical evidence supports the argument:** while the paper conducts a \"large-scale query analysis\" and provides \"quantitative and qualitative evidence,\" this evidence is explicitly stated as supporting the paper's central argument (\"we support our argument with quantitative and qualitative evidence\"). the empirical work serves to bolster the position being taken, rather than being the sole focus of reporting findings.\n*   **introduction sets up the problem and analytical lens for the argument:** it discusses current problems (patients turning to ai, rag systems promoted for accuracy) and then states, \"however, in this paper, we analyze responses generated by current rag systems through the lens of their pragmatic communicative implications,\" which leads into the paper's critical stance.\n\nthe paper uses empirical methods, but its primary goal, as framed in the abstract, is to present and support a specific viewpoint and propose solutions, making \"position\" the most fitting classification."
    }
}