\subsection{Motivation: The Imperative for Trustworthy AI}
The increasing integration of artificial intelligence (AI) systems into critical societal infrastructures and high-stakes applications necessitates an unwavering commitment to trustworthiness, reliability, and safety. A fundamental challenge that directly undermines this trust is the inherent overconfidence of deep learning models when confronted with inputs that deviate significantly from their training distribution, commonly referred to as Out-of-Distribution (OOD) data. This section articulates the compelling and urgent reasons behind the escalating importance of OOD detection in modern AI, emphasizing its role as an indispensable component for building truly trustworthy, robust, and safe artificial intelligence.

In numerous safety-critical domains, the consequences of unchecked model overconfidence on OOD data can be catastrophic, leading to unreliable decisions, system failures, and potentially severe harm. For instance, in autonomous driving, a vehicle's perception system misinterpreting an anomalous road condition, an unfamiliar object, or an unusual weather pattern as a familiar in-distribution (ID) input can lead to dangerous maneuvers or accidents \cite{cai2020lsi, kaur2022cty}. Similarly, in medical diagnosis, an AI system providing a highly confident but incorrect diagnosis for a rare or unseen patient condition, or misinterpreting an anomalous medical image, could have dire implications for patient well-being \cite{zimmerer2022rv6, kaur2022cty}. The deployment of deep reinforcement learning (RL) agents in real-world control systems also faces this challenge, where agents trained in simulated environments may encounter novel states in the physical world and fail silently without signaling uncertainty, posing significant safety risks \cite{haider20249q8}. This imperative for trustworthy AI demands that these systems not only perform well on familiar data but also express meaningful and calibrated uncertainty when encountering novel, unfamiliar, or anomalous inputs \cite{zisselman2020cmx, morningstar2020re9, lu2024j0n}.

The core problem stems from the "closed-world" assumption under which most deep learning models are traditionally trained. This assumption posits that test data will be drawn from the same statistical distribution as the training data \cite{yang2022ci8}. However, this premise rarely holds true in complex, dynamic, and open-world environments where unforeseen circumstances, sensor noise, adversarial attacks, or simply novel data points are inevitable \cite{zisselman2020cmx, chen2020mbk, morningstar2020re9, guerin202201y, schmidt2024syr, vishwakarma2024z1m}. When this closed-world assumption is violated, conventional models often produce high-confidence, yet incorrect, predictions for OOD samples \cite{song2022f5d, yu2022egq, ammar2023pr1, bitterwolf2022rw0, lu2024j0n}. This unwarranted overconfidence is a critical vulnerability that OOD detection aims to mitigate, providing a crucial safety mechanism to prevent models from making decisions outside their learned competence.

Furthermore, the very nature of OOD data can be complex and multifaceted, posing additional challenges to reliable detection. It is not always a simple binary distinction between "known" and "unknown." For instance, models can learn spurious correlations from their training data, leading them to confidently classify OOD inputs that share these irrelevant features as in-distribution, making such "spurious OOD" particularly difficult to detect \cite{ming2021wu7}. This highlights that a robust OOD detector must not only identify entirely novel semantic concepts but also be resilient to subtle shifts or misleading cues. Moreover, the definition of what constitutes "OOD" can even be model-specific, depending on whether a particular input leads to a misclassification for a given deployed model, rather than a universal distributional shift \cite{averly20239rv}. These nuances underscore the need for sophisticated and context-aware OOD detection mechanisms.

The practical deployment of AI systems further amplifies the need for robust OOD detection. Beyond theoretical performance, real-world systems require OOD detectors that are not only accurate but also provide reliable guarantees and manage false positives effectively. High false positive rates (FPR), where legitimate in-distribution samples are incorrectly flagged as OOD, can lead to user frustration, unnecessary human intervention, and a breakdown of trust in the system \cite{vishwakarma2024z1m}. Therefore, the development of OOD detection is intrinsically linked to the broader goal of building AI systems that are transparent about their limitations, can abstain from making potentially harmful decisions when faced with unfamiliar situations, and can operate predictably and safely in dynamic environments.

In summary, the motivation for robust OOD detection is deeply rooted in the urgent need to transition AI from research curiosities to reliably deployed systems that operate safely and responsibly in the real world. It is not merely about identifying novelty but about ensuring that AI systems are aware of their limitations, can express appropriate uncertainty, and can defer to human oversight or alternative safe actions when confronted with inputs beyond their learned experience. OOD detection, therefore, stands as a pivotal step towards enabling the responsible and reliable deployment of AI in complex, open-world environments, where unforeseen circumstances are not exceptions but inevitable realities. Continued research in this area is essential to bridge the gap between theoretical capabilities and the practical demands of trustworthy AI.