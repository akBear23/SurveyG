\subsection{Multimodal and Graph-Structured OOD Detection}
The landscape of Out-of-Distribution (OOD) detection is rapidly expanding beyond traditional unimodal image data to encompass the complexity of real-world multimodal and graph-structured information. This crucial extension addresses the inherent multimodal nature of many applications and the unique challenges posed by non-Euclidean data.

For graph-structured data, OOD detection presents distinct challenges due to its non-Euclidean nature and the high cost of labeling. Pioneering efforts have focused on unsupervised graph-level OOD. \cite{liu202227x} introduced GOOD-D, a novel framework for unsupervised graph-level OOD detection that learns robust in-distribution (ID) patterns through perturbation-free data augmentation and hierarchical graph contrastive learning across node, graph, and group levels. This approach was critical in formalizing the problem and providing a multi-granularity understanding of graph ID data. Building on the need for OOD exposure in graphs, \cite{wang2025xwm} proposed GOLD, which addresses the scarcity of auxiliary OOD data by implicitly generating pseudo-OOD samples through an adversarial latent generation framework, achieving superior performance without real OOD samples. Addressing practical deployment constraints, \cite{wang2024es5} introduced GOODAT, a test-time graph OOD detection method that operates without access to training data or requiring GNN architecture modifications, leveraging a graph masker and the Graph Information Bottleneck (GIB) principle for unsupervised OOD identification. To provide a unified evaluation framework for this nascent field, \cite{wang2024q01} presented UB-GOLD, a comprehensive benchmark that unifies unsupervised graph-level anomaly detection and OOD detection across 35 datasets and four distinct scenarios, enabling systematic comparison and analysis of diverse graph OOD methods.

The detection of OOD samples in multimodal settings is gaining traction as real-world data often comprises complementary information from diverse sources like vision, audio, and text. \cite{dong2024a8k} made a significant contribution by introducing MultiOOD, the first dedicated benchmark for multimodal OOD detection, alongside the Agree-to-Disagree (A2D) algorithm. A2D leverages the "modality prediction discrepancy" phenomenon, where softmax predictions across modalities show negligible differences for ID data but significant variability for OOD data, to enhance detection. Extending this concept, \cite{li2024rs5} proposed Dynamic Prototype Updating (DPU), a plug-and-play framework that addresses the limitation of uniform discrepancy amplification by dynamically adjusting intensification based on a sample's similarity to its class prototype, thereby balancing intra-class cohesion with inter-class separation.

Vision-Language Models (VLMs) have emerged as a powerful paradigm for multimodal OOD detection, particularly in zero-shot and open-vocabulary settings. \cite{miyai2023591} introduced GL-MCM, which enhances zero-shot OOD detection by combining CLIP's global and local features, offering flexibility in defining ID images in complex, multi-object scenes. Further refining VLM-based approaches, \cite{li20245b6} developed NegPrompt, a method that learns transferable "negative prompts" using only ID training data to delineate OOD boundaries, enabling open-vocabulary OOD detection without explicit OOD examples. Addressing the challenge of spurious OOD features that can arise from imperfect foreground-background decomposition in VLMs, \cite{yu20249dd} proposed Self-Calibrated Tuning (SCT), an adaptive framework that dynamically balances ID classification and OOD regularization based on prediction uncertainty. Beyond VLMs, Large Language Models (LLMs) are being explored for their world knowledge. \cite{dai2023mhn} leveraged LLMs for multi-modal OOD detection by generating descriptive features for ID classes, crucially developing a consistency-based uncertainty calibration method to mitigate LLM hallucinations and prevent performance degradation. This integration of LLMs with VLMs for OOD detection is further contextualized by \cite{miyai20247ro}, which provides a comprehensive survey of OOD detection in the VLM/LVLM era, highlighting the integration of related fields and identifying the most demanding challenges.

The expansion of OOD detection to multimodal and graph-structured data marks a significant step towards more holistic and context-rich detection capabilities. While promising advancements have been made in developing new benchmarks and algorithms that exploit inter-modal discrepancies and address the unique challenges of non-Euclidean data, several unresolved issues remain. These include the scalability of multimodal OOD methods to a wider array of modalities beyond vision-language, the robustness of graph OOD detectors to diverse and subtle structural shifts, and the development of theoretically grounded adaptive algorithms that can seamlessly handle the inherent noise and heterogeneity in real-world multimodal and graph data.