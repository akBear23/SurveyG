\subsection*{OOD in Medical Imaging and Healthcare}

The deployment of artificial intelligence (AI) in medical imaging and healthcare necessitates robust out-of-distribution (OOD) detection capabilities, as misinterpreting novel or anomalous inputs can have severe, life-threatening consequences for patient well-being. AI-powered clinical decision support systems, used for tasks like disease diagnosis and anomaly detection in medical scans, must reliably identify when an input falls outside their training distribution to prevent erroneous predictions. This domain presents unique challenges, including the high dimensionality of medical images, inherent class imbalance in rare disease detection, and the stringent requirement for robust performance on subtle OOD shifts that might indicate critical pathologies.

To address the foundational understanding of OOD in this high-stakes field, \textcite{hong2024xls} provide a comprehensive survey, establishing a critical taxonomy for distributional shifts in medical imaging. They delineate seven key factors causing OOD shifts and categorize them into semantic, covariate, and contextual shifts, clarifying the complex landscape and inconsistent terminology that hinders systematic research. Empirically validating the limitations of current approaches, \textcite{vasiliuk20233w9} expose the severe shortcomings of state-of-the-art OOD detection methods when applied to 3D medical image segmentation. Their work introduces a novel, publicly available benchmark that simulates diverse clinical OOD scenarios and, notably, demonstrates that a simple Intensity Histogram Features (IHF) baseline often outperforms complex deep learning methods, underscoring the profound challenges posed by 3D medical data and the need for more tailored solutions.

Early efforts to adapt general OOD detection techniques to medical imaging revealed significant performance discrepancies. \textcite{berger20214a3} conducted a comparative study of confidence-based OOD methods on chest X-rays, finding that methods performing well on standard computer vision benchmarks often failed in the medical context. Their analysis highlighted ODIN as a robust method due to its input perturbation mechanism, while Mahalanobis distance, a strong performer in general vision, proved ineffective in medical imaging due to less separable feature spaces. Directly addressing this limitation, \textcite{anthony2023slf} critically re-evaluated the use of Mahalanobis distance for OOD detection in medical imaging. Through a detailed layer-wise analysis, they demonstrated that the optimal detection layer is highly dependent on the specific OOD pattern, challenging previous assumptions. They then proposed Multi-branch Mahalanobis (MBM), a novel framework that significantly enhances OOD detection by employing multiple depth-specific detectors, showcasing a tailored solution that improves reliability for identifying unexpected anomalies like pacemakers or subtle demographic shifts.

Beyond adapting existing discriminative methods, novel generative approaches have emerged to tackle the high dimensionality and complexity of 3D medical data. \textcite{graham20232re} introduced an unsupervised 3D OOD detection method leveraging Latent Diffusion Models (LDMs). This innovative approach overcomes the memory and resolution limitations of prior generative models, enabling the generation of high-resolution spatial anomaly maps. Such capabilities are crucial for tasks like identifying unexpected tumors, lesions, or other pathologies in volumetric scans, where precise localization is paramount for clinical utility and ensuring the overall reliability of AI-powered diagnostic systems.

Despite these advancements, several challenges persist. The generalizability of OOD methods across the vast spectrum of medical imaging modalities, anatomical regions, and diverse OOD types remains an open problem. There is a continuous need for more comprehensive and clinically relevant benchmarks that capture the subtlety and variability of real-world OOD shifts. Furthermore, integrating these detection mechanisms seamlessly into clinical workflows, coupled with robust explainability and uncertainty quantification, is essential for fostering trust and enabling the safe and effective deployment of AI in patient care.