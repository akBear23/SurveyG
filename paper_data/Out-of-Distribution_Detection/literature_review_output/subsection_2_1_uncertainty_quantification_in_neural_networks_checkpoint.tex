\subsection{Uncertainty Quantification in Neural Networks}

Conventional neural networks, while achieving remarkable performance in complex classification tasks, are fundamentally designed to assign inputs to a fixed set of predefined classes rather than to explicitly quantify the uncertainty inherent in their predictions. Nevertheless, these classifiers offer implicit signals of confidence primarily through softmax probabilities and the entropy of the predicted class distribution. These metrics serve as initial, easily accessible indicators of a model's belief. A critical understanding of their foundational role and, more importantly, their profound limitations is indispensable for appreciating the subsequent evolution of Out-of-Distribution (OOD) detection methodologies that strive for truly calibrated uncertainty estimates.

A seminal contribution by \cite{Hendrycks_G_2017} formalized the use of Maximum Softmax Probability (MSP) as a straightforward baseline for identifying both misclassified in-distribution (ID) and OOD examples. This work, alongside others, critically exposed a pervasive and dangerous flaw: neural networks frequently exhibit severe overconfidence, assigning high softmax probabilities to OOD inputs. This leads to erroneous high-confidence predictions that are fundamentally unreliable, as the model confidently asserts an input belongs to a known class despite having never encountered anything similar during training. The root cause of this overconfidence lies in the very design of the softmax function. As \cite{Kendall_A_2017} elucidates, softmax is inherently a normalized probability distribution over a fixed set of *known* classes. It is optimized to express the model's certainty about which of the *trained* categories an input belongs to (reflecting *aleatoric uncertainty* due to inherent data noise), but it is ill-equipped to capture *epistemic uncertainty*â€”the model's lack of knowledge or confidence when confronted with inputs far removed from its training distribution. Consequently, an OOD input, by definition, falls outside the model's learned domain, yet the softmax mechanism forces it into one of the known categories, often with high confidence, simply by finding the "closest" match within its learned manifold.

Beyond this conceptual mismatch, modern deep neural networks are frequently poorly calibrated \cite{Guo_C_2017}. This means their predicted probabilities do not accurately reflect the true likelihood of correctness, exacerbating the overconfidence problem, particularly for OOD inputs. The architectural choices prevalent in deep learning, such as increased depth, ReLU activations, and optimization for accuracy rather than calibration, contribute to this miscalibration. When a model's confidence scores are unreliable even for ID data, their utility for discerning OOD samples becomes severely compromised. Furthermore, the issue of overconfidence is significantly compounded by the model's reliance on spurious correlations present in the training data. As \cite{ming2021wu7} rigorously demonstrated, models trained on datasets containing statistically informative but non-causal features tend to exploit these shortcuts. When an OOD input shares these spurious features with ID data, the model can confidently assign a high softmax probability, even if the input lacks the invariant, semantic features crucial for correct classification. This reliance on misleading environmental cues makes distinguishing spurious OOD samples from ID data inherently challenging, as the model's confidence is rooted in a superficial correlation rather than true semantic understanding \cite{ming2021wu7}.

Empirical and theoretical studies consistently underscore the inadequacy of raw softmax and entropy scores for robust OOD detection. \cite{kuan2022qzl} provided extensive evidence that simple prediction-based methods like MSP and entropy are reliably outperformed by methods leveraging learned intermediate representations (embeddings). Their work challenged the notion of MSP as a universally strong baseline, demonstrating that while it might offer some rudimentary signal, it often falls short compared to approaches that analyze the internal feature space, which are better equipped to capture deviations from the ID manifold. This empirical observation is theoretically grounded by \cite{peng20243ji}, who critically analyzed logit-based methods, including those derived from softmax. They explained that these methods are often not directly proportional to true data density. This fundamental disconnect implies that even when a model's logits are high, the resulting softmax probability does not necessarily reflect a high likelihood under the true in-distribution data manifold, leading to suboptimal OOD detection performance. Similarly, \cite{averly20239rv}'s comprehensive evaluation, while introducing a model-specific perspective, implicitly highlights the context-dependent and often inconsistent performance of MSP across different types of OOD shifts (e.g., semantic vs. covariate) and misclassifications, reinforcing its limitations as a standalone, universally reliable uncertainty measure.

In summary, while softmax probabilities and entropy offer initial, easily accessible indicators of a neural network's confidence, their inherent limitations are profound and multifaceted. These include their inability to capture epistemic uncertainty due to their closed-set design, the pervasive problem of miscalibration in modern deep networks, and their vulnerability to spurious correlations in training data. These shortcomings collectively render raw output-based uncertainty estimates unreliable for robust OOD detection, particularly in safety-critical applications where silent failures can have severe consequences. This fundamental inadequacy necessitates the development of more sophisticated OOD detection methodologies that move beyond simple output scores, paving the way for the advanced post-hoc, feature-space, generative, and training-time strategies discussed in subsequent sections of this review.