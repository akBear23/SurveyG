\subsection{Leveraging Pre-trained Foundation Models}
The emergence of large pre-trained foundation models, such as Vision-Language Models (VLMs) like CLIP \cite{radford2021learning} and Large Language Models (LLMs), has profoundly transformed the landscape of Out-of-Distribution (OOD) detection. These models, with their rich semantic understanding, vast world knowledge, and open-vocabulary capabilities, enable novel zero-shot and open-set OOD detection paradigms, moving towards more adaptable and versatile OOD systems.

The "Vision Language Model Era" marks a significant paradigm shift in OOD detection, as highlighted by \cite{miyai20247ro}. Early integration of VLMs for OOD detection focused on leveraging their inherent zero-shot capabilities. For instance, \cite{miyai2023591} introduced GL-MCM, extending CLIP's Maximum Concept Matching by utilizing both global and local visual-text alignments. This approach provides flexibility in defining in-distribution (ID) images in multi-object scenes, addressing the limitation of methods that assume single, centered objects. Building upon this, \cite{ding20242m0} proposed Outlier Label Exposure (OLE), a method that explicitly incorporates OOD knowledge by using a large set of diverse auxiliary outlier class labels as pseudo OOD text prompts for VLMs. OLE learns refined "outlier prototypes" and generates "hard outlier prototypes" to calibrate decision boundaries, overcoming the limitations of purely ID-label-based zero-shot methods that lack explicit OOD knowledge. Further advancing this direction, \cite{li20245b6} introduced NegPrompt, a prompt learning-based approach that learns transferable "negative prompts" for each ID class using *only* ID training data. These negative prompts implicitly define OOD boundaries by representing characteristics contrary to ID classes, enabling open-vocabulary OOD detection without the need for any external outlier data or additional encoders, which is a significant step towards data-efficient and generalizable OOD systems.

Beyond VLMs, Large Language Models (LLMs) are increasingly leveraged for their extensive world knowledge to generate synthetic OOD exposure. \cite{dai2023mhn} explored using LLMs to generate descriptive features for ID classes to enhance multimodal OOD detection. Crucially, they identified and addressed the LLM "hallucination" problem by proposing a novel consistency-based uncertainty calibration method. This method selectively integrates reliable LLM knowledge, preventing performance degradation caused by unfaithful LLM generations. Taking this concept further, \cite{cao20246gj} introduced Envisioning Outlier Exposure (EOE), which directly uses LLMs to *envision* and generate potential outlier class labels based on visual similarity to ID classes. EOE designs task-specific LLM prompts for far, near, and fine-grained OOD scenarios, effectively creating synthetic OOD labels without access to any real OOD data, thereby providing a powerful form of "outlier exposure" to VLMs.

The integration of foundation models also necessitates refining their behavior for OOD detection. \cite{yu20249dd} proposed Self-Calibrated Tuning (SCT) for VLMs, a novel framework designed to mitigate the issue of "spurious OOD features" that arise from VLMs' imperfect foreground-background decomposition. SCT adaptively adjusts the balance between ID classification and OOD regularization based on prediction uncertainty, making VLM-based OOD detection more robust to internal model limitations. In a broader context, the field is also expanding to multimodal OOD detection, where foundation models can play a crucial role. \cite{dong2024a8k} introduced the MultiOOD benchmark and the Agree-to-Disagree (A2D) algorithm, which leverages "modality prediction discrepancy" for OOD detection across multiple modalities. While not exclusively VLM/LLM-focused, this work highlights the growing need for robust multimodal OOD, a domain where foundation models are inherently well-suited to integrate and leverage diverse information streams. Finally, the increasing adoption of foundation models for OOD detection has led to dedicated benchmarks, such as the one presented by \cite{borlino20245ku}, which aims to properly assess the performance of these large pre-trained models in realistic yet harder OOD tasks, confirming their benefits and guiding future research into their fine-tuning strategies.

In conclusion, the advent of pre-trained foundation models has opened a new frontier in OOD detection, moving beyond traditional methods that often rely on explicit OOD data or complex architectural modifications. These models' inherent semantic understanding and vast knowledge enable sophisticated zero-shot and open-vocabulary OOD paradigms through techniques like learning transferable negative prompts, leveraging LLMs for envisioned outlier exposure, and self-calibrated tuning of VLMs. However, challenges remain in ensuring the robustness of LLM-generated information, fully integrating multimodal cues, and developing comprehensive benchmarks that capture the full spectrum of OOD scenarios for these powerful, general-purpose models.