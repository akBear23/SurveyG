\subsection{Feature-Space Distance-Based Methods}

Early efforts in out-of-distribution (OOD) detection quickly recognized the inherent limitations of relying solely on a neural network's final output probabilities, such as Maximum Softmax Probability (MSP). While simple, MSP often fails to capture the true uncertainty for samples significantly deviating from the in-distribution (ID) manifold, frequently exhibiting overconfidence on novel inputs. This critical observation spurred a shift towards leveraging the internal feature representations of neural networks, driven by the hypothesis that OOD samples would manifest as distinct patterns or lie significantly distant from the ID data within these learned embedding spaces.

A foundational exploration into using internal representations for OOD detection was presented by \cite{Hendrycks_G_2017}. While this work primarily established MSP as a baseline, it also investigated the efficacy of Mahalanobis distance computed on features from intermediate layers. By modeling the distribution of ID features for each class as a simple Gaussian, OOD samples could be identified as those with a large Mahalanobis distance to all ID class centroids. This early insight highlighted the potential of feature-level analysis to provide more robust OOD scores than simple output probabilities, laying crucial groundwork.

Building upon this concept, \cite{Lee_K_2018} introduced a more comprehensive framework that explicitly leverages Mahalanobis distance in the feature space for OOD detection. This method models the in-distribution feature representations for each class using class-conditional Gaussian distributions, where the mean and covariance are estimated from the training data. An input is then classified as OOD if its Mahalanobis distance to all ID class centroids is sufficiently large. Crucially, \cite{Lee_K_2018} also proposed using generative adversarial networks (GANs) to regularize the feature space during training. By training a GAN to generate OOD samples and then using these to push OOD representations away from ID clusters, the feature space becomes more discriminative, enhancing the separation between ID and OOD samples and making the Mahalanobis distance a more effective OOD score. This approach demonstrated a significant advancement by actively shaping the feature space to be more amenable for OOD discrimination, moving beyond merely observing existing features.

Despite its principled nature, the effectiveness of Mahalanobis distance-based methods can be limited by the strong assumption of Gaussianity for ID features and the quality of the learned features, particularly in high-dimensional spaces where the "curse of dimensionality" can render distance metrics less meaningful \cite{ghosal2023q20}. This motivated further research into refining the feature space and the distance calculations themselves. For instance, \cite{anthony2023slf} conducted an in-depth analysis of Mahalanobis distance for medical imaging OOD detection, challenging the common assumption of a single optimal layer for detection. They empirically demonstrated that the optimal network depth for OOD detection is highly dependent on the specific OOD pattern and proposed a Multi-branch Mahalanobis (MBM) framework. MBM employs multiple OOD detectors operating at different depths of the network, each combining normalized Mahalanobis scores from its constituent modules, significantly enhancing robustness by capturing diverse OOD signals across the feature hierarchy.

To mitigate the curse of dimensionality and improve feature space utility, researchers have explored learning more structured and compact representations. \cite{zaeemzadeh2021lmh} proposed training deep neural networks to embed ID samples onto a union of 1-dimensional subspaces. This compact representation ensures that OOD samples are less likely to occupy the same region as known classes, and robust representatives (singular vectors) can be used for distance calculations, thereby simplifying OOD detection. Similarly, \cite{ghosal2023q20} introduced Subspace Nearest Neighbor (SNN), a framework that regularizes the model and its feature representation by leveraging the most relevant subset of dimensions. This subspace learning yields highly distinguishable distance measures between ID and OOD data, demonstrating significant improvements over previous distance-based methods by making the distances more robust to high-dimensional noise. Extending this, \cite{li2025jdt} proposed a novel "tangent distance" that explicitly accounts for the data structure by mapping high-dimensional features to the manifold of ID samples. This method computes the Euclidean distance between samples and the nearest submanifold space (a linear approximation of the local region on the manifold), providing a more meaningful distance measure that is less sensitive to the curse of dimensionality.

Beyond Mahalanobis and its direct refinements, other distance-based approaches leverage different metrics or modeling assumptions. K-Nearest Neighbors (KNN) based methods, for example, directly quantify OODness by measuring the distance of a test sample to its $k$-nearest neighbors within the ID training data's feature space, offering a non-parametric alternative to Gaussian models. In a more modern context, \cite{vojivr202444c} introduced PixOOD for pixel-level OOD detection, which, while operating at a finer granularity, fundamentally relies on distances. It extracts pixel/patch feature representations and builds a 2D projection space where distances to multiple class etalons (learned prototypes) are used to model complex intra-class variability and identify OOD pixels. This demonstrates how distance-based principles can be adapted for fine-grained OOD detection by modeling ID distributions with multiple prototypes rather than a single centroid.

In summary, feature-space distance-based methods represent a crucial evolution in OOD detection, moving beyond simple output probabilities to leverage the richer information in internal representations. They have progressed from initial explorations of Mahalanobis distance on existing features to sophisticated techniques that actively regularize, learn, or project OOD-discriminative feature spaces. While these methods have shown promise in quantifying an input's deviation from the in-distribution manifold, challenges persist, including the sensitivity to the quality of learned features, the computational cost associated with training-time regularization, and the inherent difficulties of distance metrics in high-dimensional spaces. Future directions include developing more flexible models for feature distributions (e.g., non-parametric density estimation or advanced mixture models), integrating self-supervised learning to learn more robust features, and exploring adaptive distance metrics that can better capture complex manifold structures. Furthermore, the utility of these distance-based OOD scores is increasingly recognized in broader uncertainty quantification frameworks, such as their application as non-conformity scores within Conformal Prediction to provide statistically rigorous guarantees on OOD detection performance \cite{novello2024yco}.