\subsection{Likelihood-Based Deep Generative Models}

The detection of Out-of-Distribution (OOD) samples is a critical challenge for deploying reliable deep learning systems. A theoretically principled approach to OOD detection involves leveraging deep generative models to learn the underlying data distribution of in-distribution (ID) samples. The fundamental premise is that samples originating from outside this learned distribution, i.e., OOD samples, should exhibit a significantly lower likelihood or probability density under the model trained exclusively on ID data. This allows for their identification based on their deviation from the learned ID manifold, offering a theoretically grounded method for uncertainty quantification. Early methods primarily explored Variational Autoencoders (VAEs) \cite{Kingma_Welling_2013} and Generative Adversarial Networks (GANs) \cite{Goodfellow_etal_2014} for this purpose. VAEs provide an estimate of the data log-likelihood (or a lower bound, ELBO), while GANs can be adapted to provide density estimates or use discriminator scores as proxies for typicality.

Initially, the intuitive strategy was to directly use the raw likelihood or a related reconstruction error from a trained generative model as an OOD score. However, this straightforward application often proved problematic. Raw likelihood scores from deep generative models can be inherently misleading, frequently assigning higher likelihoods to certain OOD samples than to some ID samples \cite{Nalisnick_etal_2019_Do_Deep_Generative_Models_Know_What_They_Don_t_Know}. This counter-intuitive phenomenon, observed across various model architectures and datasets, stems from several factors. Deep generative models, particularly in high-dimensional spaces, may learn spurious low-level correlations or assign high probabilities to simple, out-of-distribution inputs that lie in regions of the input space not well-constrained by the ID data. This can be exacerbated by the "Gaussian Annulus Theorem," where in high dimensions, the typical set of data (where most of the probability mass lies) may not intersect with the region of highest density, leading to OOD samples with high likelihood but low typicality \cite{morningstar2020re9}. For VAEs, the Evidence Lower Bound (ELBO) is only a lower bound to the true log-likelihood, and a tighter bound does not necessarily correlate with better OOD detection performance. Even models capable of exact likelihood estimation, such as Normalizing Flows (NFs) \cite{Dinh_etal_2016} and Autoregressive models (e.g., PixelCNNs), suffer from this issue, often assigning higher likelihoods to less complex OOD images than to complex ID images, further demonstrating that raw likelihood alone is an unreliable indicator of semantic OODness \cite{osada20246an}.

To address the unreliability of raw likelihood scores, more robust statistical measures were introduced, marking an evolution from simple density estimation to more refined statistical tests. A pivotal advancement was proposed by \cite{Ren_J_2019}, who demonstrated that comparing the likelihood of a sample under the learned ID distribution to its likelihood under a simpler, "null" model provides a significantly more effective OOD score. Their work introduced the concept of likelihood ratio tests for OOD detection, where the ratio of the likelihood under a complex ID generative model (e.g., VAE or GAN) to that under a baseline model (e.g., a simple Gaussian distribution or a model trained on diverse OOD data) serves as a robust discriminator. This approach effectively normalizes the raw likelihood, focusing on how *much better* the ID model explains the data compared to a general or OOD model, thereby improving OOD discrimination and mitigating the issues associated with misleading raw likelihood values.

Beyond simple likelihood ratios, other sophisticated statistical approaches have emerged to leverage generative models more effectively. \cite{morningstar2020re9} introduced Density of States Estimation (DoSE), an unsupervised method that moves beyond direct model probabilities. Inspired by statistical physics, DoSE evaluates the "typicality" of an input by analyzing multiple summary statistics (e.g., negative log-likelihood, L2 norm of latent features) derived from a pre-trained generative model. Instead of directly comparing likelihoods, DoSE trains non-parametric density estimators (like Kernel Density Estimation or one-class SVMs) on the distribution of these statistics for ID data. An OOD sample is then identified if its derived statistics are atypical under these learned distributions, providing a more robust measure of OODness that accounts for the high-dimensional nature of the problem and the shortcomings of raw likelihood.

Furthermore, Normalizing Flows, which provide exact and tractable likelihoods, have seen increasing application in OOD detection, often by modeling densities in feature spaces rather than raw pixel space. For instance, \cite{zisselman2020cmx} proposed Deep Residual Flow, a novel flow architecture that learns the residual distribution from a base Gaussian distribution, improving OOD detection by modeling feature activations. Similarly, \cite{cook2024hyb} investigated feature density estimation via Normalizing Flows as a fully unsupervised, post-hoc method. By training a lightweight NF model on the feature representations of a pre-trained classifier, they demonstrated strong results for far-OOD detection, highlighting that while raw input likelihoods can be problematic, density estimation in a more semantically meaningful feature space can be highly effective.

The evolution from simple density estimation to more refined statistical measures like likelihood ratio tests and typicality-based approaches, coupled with the broader application of diverse generative architectures like Normalizing Flows, marks a significant progression in the field. While likelihood-based methods offer a theoretically grounded approach to OOD detection, challenges persist in accurately modeling complex, high-dimensional data distributions and ensuring that likelihood estimates genuinely correlate with semantic OODness across diverse scenarios. Future work continues to explore more robust generative architectures, improved background models for ratio tests, and sophisticated statistical tests to further bridge the gap between theoretical soundness and practical efficacy in real-world OOD detection tasks.