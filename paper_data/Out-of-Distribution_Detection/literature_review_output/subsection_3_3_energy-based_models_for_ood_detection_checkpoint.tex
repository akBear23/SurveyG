\subsection{Energy-Based Models for OOD Detection}

Energy-Based Models (EBMs) have emerged as a theoretically principled and increasingly effective framework for Out-of-Distribution (OOD) detection, offering a direct approach to modeling the underlying data distribution. At their core, EBMs define a probability distribution over inputs $x$ using an energy function $E(x)$ as $p(x) = \frac{\exp(-E(x))}{Z}$, where $Z = \int \exp(-E(x)) dx$ is the intractable partition function \cite{Grathwohl_etal_2019}. In this paradigm, in-distribution (ID) samples are characterized by low energy values, indicating high likelihood under the learned distribution, while OOD samples are assigned high energy values, signifying low likelihood. This direct modeling of data likelihood provides a more interpretable and robust measure of OODness compared to relying on proxy metrics like maximum softmax probabilities, which often exhibit overconfidence on OOD inputs \cite{Liu_etal_2020}.

A foundational insight into the connection between classifiers and EBMs was provided by \textcite{Grathwohl_etal_2019}, who demonstrated that a standard classifier's output logits can be interpreted as an unnormalized negative energy function. This perspective paved the way for explicitly leveraging energy functions for OOD detection. Building on this, \textcite{Liu_etal_2020} pioneered the use of Energy-based Models for OOD detection by defining the energy function directly from the output logits of a standard neural network classifier. Their significant contribution lay in developing specialized training objectives to explicitly learn these energy landscapes. This typically involves a contrastive learning approach, where the model is trained to push the energy of ID samples to be low while simultaneously increasing the energy of OOD samples. The challenge of the intractable partition function $Z$ is often circumvented during training by employing techniques like Stochastic Gradient Langevin Dynamics (SGLD) to sample from the model's distribution and approximate gradients, effectively optimizing the unnormalized density ratio rather than the absolute density \cite{Liu_etal_2020, lafon2023w37}. This direct optimization for OOD discrimination offers a more theoretically grounded and robust measure than post-hoc methods.

While many EBM approaches implicitly handle the intractable partition function through contrastive learning and sampling, \textcite{peng20243ji} directly addresses this challenge by proposing ConjNorm, a method for tractable density estimation for post-hoc OOD detection. Their work introduces a novel theoretical framework grounded in Bregman divergence, extending density considerations to the exponential family of distributions. Crucially, ConjNorm devises an unbiased and analytically tractable estimator for the partition function using a Monte Carlo-based importance sampling technique, providing a principled way to estimate true data density without strong distributional assumptions. This represents a significant advancement by offering a direct solution to a core theoretical hurdle in EBMs.

Subsequent research has explored diverse strategies to enhance EBMs for OOD detection. \textcite{lafon2023w37} introduced HEAT (Hybrid Energy Based Model in the Feature Space), a novel post-hoc method that refines existing OOD detectors (e.g., GMMs, energy logits) by complementing them with a data-driven residual EBM. HEAT uses the EBM framework to compose several energy terms from different refined priors, allowing for accurate and robust ID density estimation without requiring external OOD samples for training. This demonstrates how EBMs can be integrated to correct biases and enhance the expressiveness of other OOD scoring functions.

Furthermore, EBMs have been adapted to address specific challenges in OOD detection. \textcite{choi202367m} proposed a "balanced energy regularization loss" to tackle the problem of imbalanced auxiliary OOD data, which is often overlooked in methods like Outlier Exposure. Their approach adaptively applies larger regularization to auxiliary samples from majority classes, ensuring a more effective energy landscape shaping. Similarly, in the context of Class-Incremental Learning (CIL), \textcite{miao20246mk} introduced Bi-directional Energy Regularization (BER). BER mitigates biases in CIL models by using energy loss functions to enlarge decision boundaries for new classes (pushing OOD away) and boost confidence for old classes (preventing old ID from being misclassified as OOD), showcasing EBMs' utility in dynamic learning environments.

Beyond direct energy minimization, energy functions can also guide adaptive training. \textcite{hofmann2024gnx} introduced Hopfield Boosting, an OOD detection framework that leverages Modern Hopfield Energy (MHE) to adaptively sample "weak learners" from auxiliary outlier datasets that are hard to distinguish from ID data. By incorporating an MHE-based energy function into the training loss, this method explicitly sharpens the decision boundary between ID and OOD data, demonstrating a sophisticated use of energy to improve outlier exposure strategies.

The scalability of EBMs to modern deep learning architectures has also been a focus. \textcite{Ming_etal_2023} extended the EBM framework by demonstrating how to effectively fine-tune large pre-trained models, such as vision transformers, for energy-based OOD detection. This approach leverages the rich, generalizable representations learned by these powerful models, addressing the challenge of robust OOD detection in complex, high-dimensional data settings and highlighting the adaptability of the EBM paradigm.

In summary, EBMs provide a theoretically sound framework for OOD detection by directly modeling data likelihood through an energy function. The evolution of EBMs for OOD detection has progressed from foundational insights into their connection with classifiers \cite{Grathwohl_etal_2019} and pioneering contrastive training strategies \cite{Liu_etal_2020}, to more sophisticated approaches that directly address the partition function intractability \cite{peng20243ji}, integrate with and refine other OOD methods \cite{lafon2023w37}, adapt to specific learning challenges like imbalanced OOD data or incremental learning \cite{choi202367m, miao20246mk}, and leverage energy functions for adaptive training \cite{hofmann2024gnx}. Their ability to integrate with large pre-trained models further underscores their potential for robust OOD detection in real-world applications \cite{Ming_etal_2023}.

Future research in EBMs for OOD detection could explore more advanced techniques for approximating or tractably estimating the partition function in complex, high-dimensional settings, potentially drawing from advancements in score-matching or normalizing flows. Investigating adaptive energy functions that can dynamically adjust to different types of OOD shifts or domain contexts, perhaps through hypernetworks, could also yield more versatile detectors. Furthermore, exploring the interplay between EBMs and generative modeling to synthesize highly informative OOD examples for contrastive training, or to learn more expressive energy landscapes, remains a promising avenue.