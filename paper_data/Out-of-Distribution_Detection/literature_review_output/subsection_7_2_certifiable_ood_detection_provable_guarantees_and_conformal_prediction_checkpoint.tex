\subsection{Certifiable OOD Detection: Provable Guarantees and Conformal Prediction}

The deployment of machine learning systems in safety-critical applications necessitates not only high empirical performance but also strong theoretical guarantees regarding their reliability, particularly in identifying out-of-distribution (OOD) inputs. This subsection explores the critical advancements towards certifiable OOD detection, emphasizing methods that provide provable guarantees on false detection rates and leverage rigorous statistical frameworks like Conformal Prediction (CP).

A cornerstone of certifiable OOD detection is the integration of Conformal Prediction (CP), which offers a robust, model-agnostic framework for controlling false detection rates with statistical validity. Early work by \cite{cai2020lsi} introduced Inductive Conformal Anomaly Detection (ICAD) for real-time OOD detection in learning-enabled Cyber-Physical Systems (CPS). This approach overcame the scalability limitations of traditional CP by employing learned nonconformity measures (NCMs) based on Variational Autoencoders (VAEs) and Deep Support Vector Data Description (SVDD), ensuring a well-calibrated false alarm rate in high-dimensional settings. Building on this, \cite{kaur2022cty} proposed iDECODe, a novel ICAD framework leveraging in-distribution equivariance as its NCM. By aggregating scores from multiple transformations, iDECODe provides a theoretically guaranteed bounded false detection rate, demonstrating state-of-the-art performance in single-point OOD detection. Extending these guarantees to dynamic environments, \cite{kaur20248t3} developed CODiT, which applies conformal anomaly detection to dependent time-series data in CPS. CODiT uses deviation from in-distribution temporal equivariance as an NCM and combines predictions from multiple detectors via Fisher's method, offering bounded false alarms for both fixed-length windows and variable-length traces. These advancements collectively showcase CP's versatility in providing marginal coverage guarantees across diverse OOD scenarios, from static single-point detection to complex temporal data streams.

Beyond merely providing detection guarantees, CP is also crucial for establishing statistically rigorous evaluation metrics for OOD detectors themselves. Traditional OOD evaluation metrics, such as AUROC and FPR@TPR95, are empirical approximations that can be overly optimistic and fluctuate significantly with finite test sample sizes, lacking robust, conservative guarantees. Addressing this, \cite{novello2024yco} proposed a dual application of CP and OOD detection. They introduced "conformal AUROC" and "conformal FPR" metrics, which provide probabilistic conservativeness guarantees on the variability of these evaluation metrics. This ensures that the estimated performance of an OOD detector is conservative with high probability (e.g., $1-\delta$), thereby making the *evaluation* of OOD systems certifiable and more trustworthy. Furthermore, \cite{novello2024yco} demonstrated that sophisticated OOD scores, such as Mahalanobis distance or K-Nearest Neighbors (KNN) distance, can serve as highly effective non-conformity scores within the CP framework, often outperforming classical CP non-conformity scores in building prediction sets. This highlights a synergistic relationship where OOD methods can enhance CP, and CP can, in turn, provide robust evaluation for OOD.

While CP provides statistical guarantees, real-world systems often require adaptive control and human oversight, especially when faced with evolving OOD distributions. Addressing this, \cite{vishwakarma2024z1m} introduced a mathematically grounded human-in-the-loop framework for OOD detection that dynamically adjusts detection thresholds. This framework leverages importance sampling and an anytime-valid Upper Confidence Bound (UCB) based on the Law of Iterated Logarithm to provide provable guarantees on the false positive rate (FPR), even in the presence of distribution shifts. By taming false positives with minimal human feedback, this approach significantly enhances the practical deployability and trustworthiness of OOD systems in dynamic, open-world environments.

Beyond statistical guarantees and adaptive control, a deeper theoretical understanding of OOD detection learnability and data utility is crucial for a trustworthy foundation. \cite{du20248xe} made a significant contribution by providing the first framework that offers *provable guarantees* for leveraging unlabeled "wild" data in OOD detection. Their "Separate And Learn" (SAL) framework employs a novel gradient-based filtering mechanism and offers rigorous error bounds on outlier separability and classifier learnability, demonstrating how unlabeled data can provably enhance OOD awareness without requiring clean auxiliary OOD samples. Complementing this, \cite{fang20249gd} delved into the fundamental learnability of OOD detection, establishing necessary and sufficient conditions for Probably Approximately Correct (PAC) learnability under various risk and AUC metrics. This theoretical work highlights that OOD detection is not universally learnable and depends critically on the characteristics of the data and hypothesis spaces, providing crucial insights into the theoretical limits and possibilities of certifiable OOD systems. The implications for CP are profound: while CP offers statistical guarantees *given* an OOD score, the inherent quality and effectiveness of that score, and thus the practical utility of the CP-based detection, are constrained by the underlying learnability conditions of the specific OOD problem. This underscores the need for OOD scores that are well-aligned with the learnable properties of the data distribution for CP to be truly effective in practice.

In conclusion, the pursuit of certifiable OOD detection is rapidly evolving, moving from foundational statistical guarantees offered by Conformal Prediction for detection and evaluation, to adaptive, human-in-the-loop frameworks for dynamic FPR control. Simultaneously, theoretical work is establishing the learnability and data utility principles, collectively shaping a more robust and trustworthy understanding of what it means for an AI system to be "certifiably" aware of its own limitations. Despite these advancements, challenges remain in developing universally applicable nonconformity measures for CP that can provide strong guarantees across all types of distribution shifts, scaling CP to increasingly large foundation models, and extending these guarantees to more complex adaptive or continually learning systems, all while ensuring that the underlying OOD problem is indeed theoretically learnable.