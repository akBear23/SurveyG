\subsection{Practical Deployment Considerations and Human-in-the-Loop}
The successful transition of Out-of-Distribution (OOD) detection systems from controlled experimental settings to real-world applications hinges on addressing a critical set of practical deployment challenges. Beyond theoretical performance metrics, these systems must demonstrate computational efficiency, scalability, robustness to dynamic environments, provable guarantees on false detection rates, and the capacity for effective human-in-the-loop (HITL) interaction and interpretability. The overarching goal is to develop OOD solutions that are not only technically effective but also robust, efficient, interpretable, and ultimately practical for diverse operational settings, particularly in safety-critical domains.

A foundational requirement for practical deployment, especially in latency-sensitive systems, is computational efficiency and scalability. Early OOD methods, particularly those relying on complex generative models, often incurred significant computational overhead \cite{zisselman2020cmx}. Consequently, recent research has prioritized lightweight, post-hoc approaches that minimize inference time. Neural Mean Discrepancy (NMD) \cite{dong2021swz}, for instance, leverages running average means from Batch Normalization layers to approximate training data statistics, enabling real-time detection with minimal computational burden. Similarly, GradOrth \cite{behpour2023x13} offers an efficient gradient-based OOD detector by pre-computing a low-rank subspace of in-distribution data gradients, facilitating rapid OOD scoring during inference. While both methods offer efficiency gains, NMD's reliance on Batch Normalization statistics might limit its applicability to architectures without such layers or in scenarios where batch statistics are highly variable. GradOrth, conversely, requires gradient computations, which, while pre-computed, still adds a layer of complexity compared to simpler confidence-based scores. The computational burden of traditional kernel methods, such as Kernel PCA (KPCA), has also been significantly reduced by approaches like CoP and CoRP \cite{fang2024lv2}, which devise explicit non-linear feature mappings to achieve state-of-the-art performance with $O(1)$ or $O(M)$ complexity, a substantial improvement over $O(N_{tr})$ for methods like k-Nearest Neighbors (KNN). For Cyber-Physical Systems (CPS) demanding real-time responses, \cite{cai2020lsi} demonstrated efficient OOD detection by integrating learned nonconformity measures (from VAEs and Deep SVDD) into the Inductive Conformal Anomaly Detection (ICAD) framework, overcoming traditional conformal prediction's scalability limitations for high-dimensional sensor inputs. Furthermore, the efficiency challenge extends to large language models (LLMs) in natural language processing (NLP). \cite{ouyang2023wxc} proposed PTO, an unsupervised prefix-tuning based OOD detection framework that offers a parameter-efficient alternative to costly fine-tuning, demonstrating comparable or superior performance with significantly reduced storage and computational requirements. These diverse approaches highlight a collective effort to balance detection efficacy with the stringent computational constraints of real-world deployment.

Beyond raw efficiency, practical systems demand robustness to diverse OOD types and adversarial attacks, coupled with adaptive mechanisms for dynamic, non-stationary environments. Many methods improve intrinsic OOD robustness during training or representation learning (as discussed in Section 4), but deployment-time robustness necessitates adapting to unforeseen shifts. The conceptual shift from merely detecting "out-of-distribution" to "Out-of-Model-Scope" (OMS) detection \cite{guerin202201y} is crucial, emphasizing the identification of inputs leading to *prediction errors* of the specific deployed model, rather than a generic notion of novelty. This perspective is further refined by the Model-Specific Out-of-Distribution (MS-OOD) framework \cite{averly20239rv}, which unifies the detection of semantic shifts, covariate shifts, and even misclassified in-distribution examples based on the actual performance of a deployed classifier. This framework is vital for guiding adaptive behavior and dynamic thresholding, allowing systems to differentiate between inputs the model *can* handle despite a shift (e.g., a covariate shift it generalizes to) and those it *cannot* (e.g., a semantic shift or a covariate shift leading to misclassification). While MS-OOD provides a robust conceptual foundation, its practical implementation for continuous adaptation in dynamic environments remains an active area of research, often requiring continuous monitoring and re-calibration. Contributions like \cite{chen20247p7}'s sparsity-regularized tuning framework enhance the generalizability of OOD score functions, making them less dependent on specific datasets and more capable of dynamic adaptation. A particularly innovative adaptive mechanism for handling unforeseen OOD in open-world scenarios is \cite{cao20246gj}'s Envisioning Outlier Exposure (EOE). EOE leverages Large Language Models (LLMs) to synthetically generate potential outlier class labels based on visual similarity to in-distribution classes, effectively providing "outlier exposure" without requiring actual OOD data. This LLM-driven approach offers a scalable and flexible way to adapt to various OOD types (far, near, fine-grained) by dynamically envisioning new categories, thereby enhancing the model's ability to distinguish novel inputs in a zero-shot manner. However, the effectiveness of EOE relies heavily on the quality of LLM-generated prompts and the LLM's inherent knowledge, posing challenges for robust and unbiased outlier generation across all domains.

A critical aspect of practical deployment, especially in safety-critical domains, is the ability to provide reliable uncertainty estimates and control false detection rates. This is paramount for building trust and ensuring regulatory compliance. Conformal Prediction (CP), as detailed in Section 7.2, offers a principled approach to this, providing provably valid false detection rates. For instance, \cite{kaur2022cty} introduced iDECODe for single-point OOD detection with theoretically guaranteed bounded False Detection Rates (FDR). This work was significantly extended by \cite{kaur20248t3} to time-series data in Cyber-Physical Systems, providing conformal guarantees for OOD detection in dynamic, dependent data streams. This is a crucial advancement, as many real-world applications involve sequential data where independence assumptions of traditional CP might not hold. While CP offers strong theoretical guarantees, its practical application often requires careful calibration and consideration of the exchangeability assumption, which can be challenging in highly non-stationary environments.

Despite theoretical guarantees, managing false positives (FPs) in dynamic, open-world settings often requires human oversight, leading to the indispensable role of human-in-the-loop (HITL) approaches. HITL frameworks integrate human expert feedback to refine OOD detectors, manage trade-offs between safety and performance, and build trust. \cite{vishwakarma2024z1m} directly addressed the problem of high False Positive Rates (FPR) in OOD detection by proposing a mathematically grounded HITL framework. This framework adaptively updates the detection threshold over time by integrating human feedback and employing an anytime-valid Upper Confidence Bound (UCB) based on the Law of Iterated Logarithm, guaranteeing FPR control below a desired level while maximizing True Positive Rate (TPR) (further theoretical details are in Section 7.2). This approach offers a robust mechanism for dynamic threshold adjustment, but its effectiveness depends on the availability and reliability of human feedback. Beyond direct threshold adjustment, HITL also plays a crucial role in data acquisition and model refinement. \cite{schmidt2024syr} proposed SISOM, a unified approach for active learning and OOD detection. Active learning inherently involves human experts labeling uncertain or novel samples, thereby providing crucial feedback to improve both model performance and OOD detection capabilities. SISOM's self-deciding process for combining scores contributes to adaptive behavior, reducing the burden on human operators while maintaining robustness.

For HITL systems to be truly effective, building operator trust and ensuring practical utility requires OOD solutions that are not only effective but also interpretable and aligned with system safety goals. Human operators need to understand *why* a system flags an input as OOD to make informed decisions and foster confidence in AI systems. Methods that leverage intrinsic model properties or explanations can enhance this interpretability. For instance, Neuron Activation Coverage (NAC) \cite{liu2023zb3} quantifies the "coverage degree" of neuron states by in-distribution data, offering insights into model behavior under OOD conditions. Similarly, GAIA \cite{chen2023za1} detects OOD samples by quantifying "abnormality in gradient-based attribution results," directly linking model explanations to OOD detection (these methods are detailed in Section 4.3). While these approaches provide valuable diagnostic information, translating complex neural network activations or gradient attributions into easily digestible and actionable insights for human operators remains a significant challenge. The interpretability must be tailored to the human's cognitive load and the specific decision-making context.

In conclusion, the literature demonstrates a clear trajectory towards OOD detection solutions that prioritize practical deployment. This involves a strong emphasis on computational efficiency for real-time operation, robustness and adaptive mechanisms for dynamic environments, and the provision of theoretical guarantees on false alarm rates. Crucially, the field is increasingly recognizing the indispensable role of human-in-the-loop approaches for adaptive thresholding, managing false positives, and building trust in AI systems. Future work will likely continue to explore more nuanced human-AI collaboration models, develop methods for handling highly dynamic and unforeseen OOD shifts with minimal human intervention, and strive for truly interpretable OOD explanations that align with human decision-making in safety-critical contexts, ultimately enabling the responsible and reliable deployment of AI.