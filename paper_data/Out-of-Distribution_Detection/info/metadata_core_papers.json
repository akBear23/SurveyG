{
    "1f24e041e10239cba8ff26ffcff4902343e55cab.pdf": {
        "title": "Kernel PCA for Out-of-Distribution Detection",
        "authors": [
            "Kun Fang",
            "Qinghua Tao",
            "Kexin Lv",
            "M. He",
            "Xiaolin Huang",
            "Jie Yang"
        ],
        "published_date": "2024",
        "abstract": "Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper non-linear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, and seek suitable non-linear kernels that advocate the separability between InD and OoD data in the subspace spanned by the principal components. Besides, explicit feature mappings induced from the devoted task-specific kernels are adopted so that the KPCA reconstruction error for new test samples can be efficiently obtained with large-scale data. Extensive theoretical and empirical results on multiple OoD data sets and network structures verify the superiority of our KPCA detector in efficiency and efficacy with state-of-the-art detection performance.",
        "file_path": "paper_data/Out-of-Distribution_Detection/1f24e041e10239cba8ff26ffcff4902343e55cab.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "af5b1a35271efd17ff3d5ddd152bacc96dff0e81.pdf": {
        "title": "Deep Residual Flow for Out of Distribution Detection",
        "authors": [
            "E. Zisselman",
            "Aviv Tamar"
        ],
        "published_date": "2020",
        "abstract": "The effective application of neural networks in the real-world relies on proficiently detecting out-of-distribution examples. Contemporary methods seek to model the distribution of feature activations in the training data for adequately distinguishing abnormalities, and the state-of-the-art method uses Gaussian distribution models. In this work, we present a novel approach that improves upon the state-of-the-art by leveraging an expressive density model based on normalizing flows. We introduce the residual flow, a novel flow architecture that learns the residual distribution from a base Gaussian distribution. Our model is general, and can be applied to any data that is approximately Gaussian. For out of distribution detection in image datasets, our approach provides a principled improvement over the state-of-the-art. Specifically, we demonstrate the effectiveness of our method in ResNet and DenseNet architectures trained on various image datasets. For example, on a ResNet trained on CIFAR-100 and evaluated on detection of out-of-distribution samples from the ImageNet dataset, holding the true positive rate (TPR) at 95%, we improve the true negative rate (TNR) from 56.7% (current state of-the-art) to 77.5% (ours).",
        "file_path": "paper_data/Out-of-Distribution_Detection/af5b1a35271efd17ff3d5ddd152bacc96dff0e81.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "f911f3b51fcc88f2240def8f38ed8dff1da2e605.pdf": {
        "title": "VRA: Variational Rectified Activation for Out-of-distribution Detection",
        "authors": [
            "Ming Xu",
            "Zheng Lian",
            "B. Liu",
            "Jianhua Tao"
        ],
        "published_date": "2023",
        "abstract": "Out-of-distribution (OOD) detection is critical to building reliable machine learning systems in the open world. Researchers have proposed various strategies to reduce model overconfidence on OOD data. Among them, ReAct is a typical and effective technique to deal with model overconfidence, which truncates high activations to increase the gap between in-distribution and OOD. Despite its promising results, is this technique the best choice for widening the gap? To answer this question, we leverage the variational method to find the optimal operation and verify the necessity of suppressing abnormally low and high activations and amplifying intermediate activations in OOD detection, rather than focusing only on high activations like ReAct. This motivates us to propose a novel technique called ``Variational Rectified Activation (VRA)'', which simulates these suppression and amplification operations using piecewise functions. Experimental results on multiple benchmark datasets demonstrate that our method outperforms existing post-hoc strategies. Meanwhile, VRA is compatible with different scoring functions and network architectures. \\textcolor[rgb]{0.93,0.0,0.47}{Our code can be found in Supplementary Material}.",
        "file_path": "paper_data/Out-of-Distribution_Detection/f911f3b51fcc88f2240def8f38ed8dff1da2e605.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "2e6813cad2e41c683277aa2d400dc2a2761309a2.pdf": {
        "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
        "authors": [
            "Atsuyuki Miyai",
            "Jingkang Yang",
            "Jingyang Zhang",
            "Yifei Ming",
            "Yueqian Lin",
            "Qing Yu",
            "Go Irie",
            "Shafiq R. Joty",
            "Yixuan Li",
            "Hai Li",
            "Ziwei Liu",
            "T. Yamasaki",
            "Kiyoharu Aizawa"
        ],
        "published_date": "2024",
        "abstract": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the safety of machine learning systems and has shaped the field of OOD detection. Meanwhile, several other problems are closely related to OOD detection, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD). To unify these problems, a generalized OOD detection framework was proposed, taxonomically categorizing these five problems. However, Vision Language Models (VLMs) such as CLIP have significantly changed the paradigm and blurred the boundaries between these fields, again confusing researchers. In this survey, we first present a generalized OOD detection v2, encapsulating the evolution of these fields in the VLM era. Our framework reveals that, with some field inactivity and integration, the demanding challenges have become OOD detection and AD. Then, we highlight the significant shift in the definition, problem settings, and benchmarks; we thus feature a comprehensive review of the methodology for OOD detection and related tasks to clarify their relationship to OOD detection. Finally, we explore the advancements in the emerging Large Vision Language Model (LVLM) era, such as GPT-4V. We conclude with open challenges and future directions. The resource is available at https://github.com/AtsuMiyai/Awesome-OOD-VLM.",
        "file_path": "paper_data/Out-of-Distribution_Detection/2e6813cad2e41c683277aa2d400dc2a2761309a2.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 0,
        "score": 0
    },
    "50864505777b344d2ee4b4d18880f3ba3ca58836.pdf": {
        "title": "Outlier Synthesis via Hamiltonian Monte Carlo for Out-of-Distribution Detection",
        "authors": [
            "Hengzhuang Li",
            "Teng Zhang"
        ],
        "published_date": "2025",
        "abstract": "Out-of-distribution (OOD) detection is crucial for developing trustworthy and reliable machine learning systems. Recent advances in training with auxiliary OOD data demonstrate efficacy in enhancing detection capabilities. Nonetheless, these methods heavily rely on acquiring a large pool of high-quality natural outliers. Some prior methods try to alleviate this problem by synthesizing virtual outliers but suffer from either poor quality or high cost due to the monotonous sampling strategy and the heavy-parameterized generative models. In this paper, we overcome all these problems by proposing the Hamiltonian Monte Carlo Outlier Synthesis (HamOS) framework, which views the synthesis process as sampling from Markov chains. Based solely on the in-distribution data, the Markov chains can extensively traverse the feature space and generate diverse and representative outliers, hence exposing the model to miscellaneous potential OOD scenarios. The Hamiltonian Monte Carlo with sampling acceptance rate almost close to 1 also makes our framework enjoy great efficiency. By empirically competing with SOTA baselines on both standard and large-scale benchmarks, we verify the efficacy and efficiency of our proposed HamOS.",
        "file_path": "paper_data/Out-of-Distribution_Detection/50864505777b344d2ee4b4d18880f3ba3ca58836.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "14cfe2588311870325e2770c5159d3100d7031ea.pdf": {
        "title": "Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection",
        "authors": [
            "Chentao Cao",
            "Zhun Zhong",
            "Zhanke Zhou",
            "Yang Liu",
            "Tongliang Liu",
            "Bo Han"
        ],
        "published_date": "2024",
        "abstract": "Detecting out-of-distribution (OOD) samples is essential when deploying machine learning models in open-world scenarios. Zero-shot OOD detection, requiring no training on in-distribution (ID) data, has been possible with the advent of vision-language models like CLIP. Existing methods build a text-based classifier with only closed-set labels. However, this largely restricts the inherent capability of CLIP to recognize samples from large and open label space. In this paper, we propose to tackle this constraint by leveraging the expert knowledge and reasoning capability of large language models (LLM) to Envision potential Outlier Exposure, termed EOE, without access to any actual OOD data. Owing to better adaptation to open-world scenarios, EOE can be generalized to different tasks, including far, near, and fine-grained OOD detection. Technically, we design (1) LLM prompts based on visual similarity to generate potential outlier class labels specialized for OOD detection, as well as (2) a new score function based on potential outlier penalty to distinguish hard OOD samples effectively. Empirically, EOE achieves state-of-the-art performance across different OOD tasks and can be effectively scaled to the ImageNet-1K dataset. The code is publicly available at: https://github.com/tmlr-group/EOE.",
        "file_path": "paper_data/Out-of-Distribution_Detection/14cfe2588311870325e2770c5159d3100d7031ea.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0
    },
    "34d35e460b39edb19581ef345c4b32ce45aa9eae.pdf": {
        "title": "iDECODe: In-distribution Equivariance for Conformal Out-of-distribution Detection",
        "authors": [
            "R. Kaur",
            "Susmit Jha",
            "Anirban Roy",
            "Sangdon Park",
            "Edgar Dobriban",
            "O. Sokolsky",
            "Insup Lee"
        ],
        "published_date": "2022",
        "abstract": "Machine learning methods such as deep neural networks (DNNs), despite their success across different domains, are known to often generate incorrect predictions with high confidence on inputs outside their training distribution. The deployment of DNNs in safety-critical domains requires detection of out-of-distribution (OOD) data so that DNNs can abstain from making predictions on those. A number of methods have been recently developed for OOD detection, but there is still room for improvement. We propose the new method iDECODe, leveraging in-distribution equivariance for conformal OOD detection. It relies on a novel base non-conformity measure and a new aggregation method, used in the inductive conformal anomaly detection framework, thereby guaranteeing a bounded false detection rate. We demonstrate the efficacy of iDECODe by experiments on image and audio datasets, obtaining state-of-the-art results. We also show that iDECODe can detect adversarial examples. Code, pre-trained models, and data are available at https://github.com/ramneetk/iDECODe.",
        "file_path": "paper_data/Out-of-Distribution_Detection/34d35e460b39edb19581ef345c4b32ce45aa9eae.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0
    },
    "977384045381a2c45dfac4797196d34658d8a44f.pdf": {
        "title": "Out-of-Distribution Detection with Semantic Mismatch under Masking",
        "authors": [
            "Yijun Yang",
            "Ruiyuan Gao",
            "Qiang Xu"
        ],
        "published_date": "2022",
        "abstract": "This paper proposes a novel out-of-distribution (OOD) detection framework named MoodCat for image classifiers. MoodCat masks a random portion of the input image and uses a generative model to synthesize the masked image to a new image conditioned on the classification result. It then calculates the semantic difference between the original image and the synthesized one for OOD detection. Compared to existing solutions, MoodCat naturally learns the semantic information of the in-distribution data with the proposed mask and conditional synthesis strategy, which is critical to identifying OODs. Experimental results demonstrate that MoodCat outperforms state-of-the-art OOD detection solutions by a large margin.",
        "file_path": "paper_data/Out-of-Distribution_Detection/977384045381a2c45dfac4797196d34658d8a44f.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 0,
        "score": 0
    },
    "b3f21af3032246b6fa87e05a6d9455433b25ce55.pdf": {
        "title": "Taming False Positives in Out-of-Distribution Detection with Human Feedback",
        "authors": [
            "Harit Vishwakarma",
            "Heguang Lin",
            "Ramya Korlakai Vinayak"
        ],
        "published_date": "2024",
        "abstract": "Robustness to out-of-distribution (OOD) samples is crucial for safely deploying machine learning models in the open world. Recent works have focused on designing scoring functions to quantify OOD uncertainty. Setting appropriate thresholds for these scoring functions for OOD detection is challenging as OOD samples are often unavailable up front. Typically, thresholds are set to achieve a desired true positive rate (TPR), e.g., $95\\%$ TPR. However, this can lead to very high false positive rates (FPR), ranging from 60 to 96\\%, as observed in the Open-OOD benchmark. In safety-critical real-life applications, e.g., medical diagnosis, controlling the FPR is essential when dealing with various OOD samples dynamically. To address these challenges, we propose a mathematically grounded OOD detection framework that leverages expert feedback to \\emph{safely} update the threshold on the fly. We provide theoretical results showing that it is guaranteed to meet the FPR constraint at all times while minimizing the use of human feedback. Another key feature of our framework is that it can work with any scoring function for OOD uncertainty quantification. Empirical evaluation of our system on synthetic and benchmark OOD datasets shows that our method can maintain FPR at most $5\\%$ while maximizing TPR.",
        "file_path": "paper_data/Out-of-Distribution_Detection/b3f21af3032246b6fa87e05a6d9455433b25ce55.pdf",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "citationCount": 0,
        "score": 0
    },
    "8529e0bbf80f36998f9b65b11bc0177099f11b07.pdf": {
        "title": "Pursuing Feature Separation based on Neural Collapse for Out-of-Distribution Detection",
        "authors": [
            "Yingwen Wu",
            "Ruiji Yu",
            "Xinwen Cheng",
            "Zhengbao He",
            "Xiaolin Huang"
        ],
        "published_date": "2024",
        "abstract": "In the open world, detecting out-of-distribution (OOD) data, whose labels are disjoint with those of in-distribution (ID) samples, is important for reliable deep neural networks (DNNs). To achieve better detection performance, one type of approach proposes to fine-tune the model with auxiliary OOD datasets to amplify the difference between ID and OOD data through a separation loss defined on model outputs. However, none of these studies consider enlarging the feature disparity, which should be more effective compared to outputs. The main difficulty lies in the diversity of OOD samples, which makes it hard to describe their feature distribution, let alone design losses to separate them from ID features. In this paper, we neatly fence off the problem based on an aggregation property of ID features named Neural Collapse (NC). NC means that the penultimate features of ID samples within a class are nearly identical to the last layer weight of the corresponding class. Based on this property, we propose a simple but effective loss called Separation Loss, which binds the features of OOD data in a subspace orthogonal to the principal subspace of ID features formed by NC. In this way, the features of ID and OOD samples are separated by different dimensions. By optimizing the feature separation loss rather than purely enlarging output differences, our detection achieves SOTA performance on CIFAR10, CIFAR100 and ImageNet benchmarks without any additional data augmentation or sampling, demonstrating the importance of feature separation in OOD detection. Code is available at https://github.com/Wuyingwen/Pursuing-Feature-Separation-for-OOD-Detection.",
        "file_path": "paper_data/Out-of-Distribution_Detection/8529e0bbf80f36998f9b65b11bc0177099f11b07.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "06436653774a7cb8d53005d3f25af2a7229c1f8b.pdf": {
        "title": "PixOOD: Pixel-Level Out-of-Distribution Detection",
        "authors": [
            "Tom'avs Voj'ivr",
            "Jan Sochman",
            "Jivr'i Matas"
        ],
        "published_date": "2024",
        "abstract": "We propose a dense image prediction out-of-distribution detection algorithm, called PixOOD, which does not require training on samples of anomalous data and is not designed for a specific application which avoids traditional training biases. In order to model the complex intra-class variability of the in-distribution data at the pixel level, we propose an online data condensation algorithm which is more robust than standard K-means and is easily trainable through SGD. We evaluate PixOOD on a wide range of problems. It achieved state-of-the-art results on four out of seven datasets, while being competitive on the rest. The source code is available at https://github.com/vojirt/PixOOD.",
        "file_path": "paper_data/Out-of-Distribution_Detection/06436653774a7cb8d53005d3f25af2a7229c1f8b.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 0,
        "score": 0
    },
    "ff29bf27e1c4e95c4eec448ed1d4adfa81983302.pdf": {
        "title": "NECO: NEural Collapse Based Out-of-distribution detection",
        "authors": [
            "Mouin Ben Ammar",
            "Nacim Belkhir",
            "Sebastian Popescu",
            "Antoine Manzanera",
            "Gianni Franchi"
        ],
        "published_date": "2023",
        "abstract": "Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. Code is available at https://gitlab.com/drti/neco",
        "file_path": "paper_data/Out-of-Distribution_Detection/ff29bf27e1c4e95c4eec448ed1d4adfa81983302.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "69c2808097e7dfd357856f1ae82dcb6ce1bf64df.pdf": {
        "title": "ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms",
        "authors": [
            "William Yang",
            "Byron Zhang",
            "Olga Russakovsky"
        ],
        "published_date": "2023",
        "abstract": "The task of out-of-distribution (OOD) detection is notoriously ill-defined. Earlier works focused on new-class detection, aiming to identify label-altering data distribution shifts, also known as\"semantic shift.\"However, recent works argue for a focus on failure detection, expanding the OOD evaluation framework to account for label-preserving data distribution shifts, also known as\"covariate shift.\"Intriguingly, under this new framework, complex OOD detectors that were previously considered state-of-the-art now perform similarly to, or even worse than the simple maximum softmax probability baseline. This raises the question: what are the latest OOD detectors actually detecting? Deciphering the behavior of OOD detection algorithms requires evaluation datasets that decouples semantic shift and covariate shift. To aid our investigations, we present ImageNet-OOD, a clean semantic shift dataset that minimizes the interference of covariate shift. Through comprehensive experiments, we show that OOD detectors are more sensitive to covariate shift than to semantic shift, and the benefits of recent OOD detection algorithms on semantic shift detection is minimal. Our dataset and analyses provide important insights for guiding the design of future OOD detectors.",
        "file_path": "paper_data/Out-of-Distribution_Detection/69c2808097e7dfd357856f1ae82dcb6ce1bf64df.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "60108b8e0d7204fa33f686b09128c7fc8489a224.pdf": {
        "title": "Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection",
        "authors": [
            "Yibo Zhou"
        ],
        "published_date": "2022",
        "abstract": "In some scenarios, classifier requires detecting out-of-distribution samples far from its training data. With desirable characteristics, reconstruction autoencoder-based methods deal with this problem by using input reconstruction error as a metric of novelty vs. normality. We formulate the essence of such approach as a quadruplet domain translation with an intrinsic bias to only query for a proxy of conditional data uncertainty. Accordingly, an improvement direction is formalized as maximumly compressing the autoencoder's latent space while ensuring its reconstructive power for acting as a described domain translator. From it, strategies are introduced including semantic reconstruction, data certainty decomposition and normalized L2 distance to substantially improve original methods, which together establish state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method works without any additional data, hard-to-implement structure, time-consuming pipeline, and even harming the classification accuracy of known classes.",
        "file_path": "paper_data/Out-of-Distribution_Detection/60108b8e0d7204fa33f686b09128c7fc8489a224.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "aaedc4d1d19a1e82cd4880c1b414593e766a1f31.pdf": {
        "title": "On the Impact of Spurious Correlation for Out-of-distribution Detection",
        "authors": [
            "Yifei Ming",
            "Hang Yin",
            "Yixuan Li"
        ],
        "published_date": "2021",
        "abstract": "Modern neural networks can assign high confidence to inputs drawn from outside the training distribution, posing threats to models in real-world deployments. While much research attention has been placed on designing new out-of-distribution (OOD) detection methods, the precise definition of OOD is often left in vagueness and falls short of the desired notion of OOD in reality. In this paper, we present a new formalization and model the data shifts by taking into account both the invariant and environmental (spurious) features. Under such formalization, we systematically investigate how spurious correlation in the training set impacts OOD detection. Our results suggest that the detection performance is severely worsened when the correlation between spurious features and labels is increased in the training set. We further show insights on detection methods that are more effective in reducing the impact of spurious correlation, and provide theoretical analysis on why reliance on environmental features leads to high OOD detection error. Our work aims to facilitate better understanding of OOD samples and their formalization, as well as the exploration of methods that enhance OOD detection. Code is available at https://github.com/deeplearning-wisc/Spurious_OOD.",
        "file_path": "paper_data/Out-of-Distribution_Detection/aaedc4d1d19a1e82cd4880c1b414593e766a1f31.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0
    },
    "531762d327ac99a898f4976181c1c69e2e3076cb.pdf": {
        "title": "Learning Transferable Negative Prompts for Out-of-Distribution Detection",
        "authors": [
            "Tianqi Li",
            "Guansong Pang",
            "Xiaolong Bai",
            "Wenjun Miao",
            "Jingyi Zheng"
        ],
        "published_date": "2024",
        "abstract": "Existing prompt learning methods have shown certain capabilities in Out-of-Distribution (OOD) detection, but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) categories, resulting in a high false positive rate. To address this issue, we introduce a novel OOD detection method, named \u2018NegPrompt\u2019, to learn a set of negative prompts, each representing a negative connotation of a given class label, for delineating the boundaries between ID and OOD images. It learns such negative prompts with ID data only, without any reliance on external out-lier data. Further, current methods assume the availability of samples of all ID classes, rendering them ineffective in open-vocabulary learning scenarios where the inference stage can contain novel ID classes not present during training. In contrast, our learned negative prompts are transferable to novel class labels. Experiments on various ImageNet benchmarks show that NegPrompt surpasses state-of-the-art prompt-learning-based OOD detection methods and maintains a consistent lead in hard OOD detection in closed- and open-vocabulary classification scenarios. Code is available at https://github.com/mala-lab/negprompt.",
        "file_path": "paper_data/Out-of-Distribution_Detection/531762d327ac99a898f4976181c1c69e2e3076cb.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "728afc51ac20d79133d8c747a2d18b01c6a6de5e.pdf": {
        "title": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank Approximation",
        "authors": [
            "Yixia Li",
            "Boya Xiong",
            "Guanhua Chen",
            "Yun Chen"
        ],
        "published_date": "2024",
        "abstract": "Out-of-distribution (OOD) detection is crucial for the safe deployment of neural networks. Existing CLIP-based approaches perform OOD detection by devising novel scoring functions or sophisticated fine-tuning methods. In this work, we propose SeTAR, a novel, training-free OOD detection method that leverages selective low-rank approximation of weight matrices in vision-language and vision-only models. SeTAR enhances OOD detection via post-hoc modification of the model's weight matrices using a simple greedy search algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning extension optimizing model performance for OOD detection tasks. Extensive evaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior performance, reducing the relatively false positive rate by up to 18.95% and 36.80% compared to zero-shot and fine-tuning baselines. Ablation studies further validate SeTAR's effectiveness, robustness, and generalizability across different model backbones. Our work offers a scalable, efficient solution for OOD detection, setting a new state-of-the-art in this area.",
        "file_path": "paper_data/Out-of-Distribution_Detection/728afc51ac20d79133d8c747a2d18b01c6a6de5e.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "350b00baaddd9f42dd3689f475bea3139e24099d.pdf": {
        "title": "Revisit PCA-based technique for Out-of-Distribution Detection",
        "authors": [
            "Xiaoyuan Guan",
            "Zhouwu Liu",
            "Weishi Zheng",
            "Yuren Zhou",
            "Ruixuan Wang"
        ],
        "published_date": "2023",
        "abstract": "Out-of-distribution (OOD) detection is a desired ability to ensure the reliability and safety of intelligent systems. A scoring function is often designed to measure the degree of any new data being an OOD sample. While most designed scoring functions are based on a single source of information (e.g., the classifier\u2019s output, logits, or feature vector), recent studies demonstrate that fusion of multiple sources may help better detect OOD data. In this study, after detailed analysis of the issue in OOD detection by the conventional principal component analysis (PCA), we propose fusing a simple regularized PCA-based reconstruction error with other source of scoring function to further improve OOD detection performance. In particular, when combined with a strong energy score-based OOD method, the regularized reconstruction error helps achieve new state-of the-art OOD detection results on multiple standard benchmarks. The code is available at https://github.com/SYSUMIA-GROUP/pca-based-out-of-distribution-detection.",
        "file_path": "paper_data/Out-of-Distribution_Detection/350b00baaddd9f42dd3689f475bea3139e24099d.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 0,
        "score": 0
    },
    "df8176027e3b9857e6bc6f45b3fc183351571fbd.pdf": {
        "title": "Boosting Out-of-distribution Detection with Typical Features",
        "authors": [
            "Yao Zhu",
            "YueFeng Chen",
            "Chuanlong Xie",
            "Xiaodan Li",
            "Rong Zhang",
            "Hui Xue",
            "Xiang Tian",
            "Bolun Zheng",
            "Yao-wu Chen"
        ],
        "published_date": "2022",
        "abstract": "Out-of-distribution (OOD) detection is a critical task for ensuring the reliability and safety of deep neural networks in real-world scenarios. Different from most previous OOD detection methods that focus on designing OOD scores or introducing diverse outlier examples to retrain the model, we delve into the obstacle factors in OOD detection from the perspective of typicality and regard the feature's high-probability region of the deep model as the feature's typical set. We propose to rectify the feature into its typical set and calculate the OOD score with the typical features to achieve reliable uncertainty estimation. The feature rectification can be conducted as a {plug-and-play} module with various OOD scores. We evaluate the superiority of our method on both the commonly used benchmark (CIFAR) and the more challenging high-resolution benchmark with large label space (ImageNet). Notably, our approach outperforms state-of-the-art methods by up to 5.11$\\%$ in the average FPR95 on the ImageNet benchmark.",
        "file_path": "paper_data/Out-of-Distribution_Detection/df8176027e3b9857e6bc6f45b3fc183351571fbd.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "e75e08851675eb506ea0149b0403828b6fb24900.pdf": {
        "title": "Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection",
        "authors": [
            "Marc Lafon",
            "Elias Ramzi",
            "Cl\u00e9ment Rambour",
            "Nicolas Thome"
        ],
        "published_date": "2023",
        "abstract": "Out-of-distribution (OOD) detection is a critical requirement for the deployment of deep neural networks. This paper introduces the HEAT model, a new post-hoc OOD detection method estimating the density of in-distribution (ID) samples using hybrid energy-based models (EBM) in the feature space of a pre-trained backbone. HEAT complements prior density estimators of the ID density, e.g. parametric models like the Gaussian Mixture Model (GMM), to provide an accurate yet robust density estimation. A second contribution is to leverage the EBM framework to provide a unified density estimation and to compose several energy terms. Extensive experiments demonstrate the significance of the two contributions. HEAT sets new state-of-the-art OOD detection results on the CIFAR-10 / CIFAR-100 benchmark as well as on the large-scale Imagenet benchmark. The code is available at: https://github.com/MarcLafon/heatood.",
        "file_path": "paper_data/Out-of-Distribution_Detection/e75e08851675eb506ea0149b0403828b6fb24900.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0
    },
    "2815a5e7ba661ae278aa7c19e08ac884cde17bf7.pdf": {
        "title": "Igeood: An Information Geometry Approach to Out-of-Distribution Detection",
        "authors": [
            "Eduardo Dadalto Camara Gomes",
            "F. Alberge",
            "P. Duhamel",
            "P. Piantanida"
        ],
        "published_date": "2022",
        "abstract": "Reliable out-of-distribution (OOD) detection is fundamental to implementing safer modern machine learning (ML) systems. In this paper, we introduce Igeood, an effective method for detecting OOD samples. Igeood applies to any pre-trained neural network, works under various degrees of access to the ML model, does not require OOD samples or assumptions on the OOD data but can also benefit (if available) from OOD samples. By building on the geodesic (Fisher-Rao) distance between the underlying data distributions, our discriminator can combine confidence scores from the logits outputs and the learned features of a deep neural network. Empirically, we show that Igeood outperforms competing state-of-the-art methods on a variety of network architectures and datasets.",
        "file_path": "paper_data/Out-of-Distribution_Detection/2815a5e7ba661ae278aa7c19e08ac884cde17bf7.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "71fdc063701dc3f431942398d53b0290a9975d32.pdf": {
        "title": "Representation Norm Amplification for Out-of-Distribution Detection in Long-Tail Learning",
        "authors": [
            "Dong Geun Shin",
            "Hye Won Chung"
        ],
        "published_date": "2024",
        "abstract": "Detecting out-of-distribution (OOD) samples is a critical task for reliable machine learning. However, it becomes particularly challenging when the models are trained on long-tailed datasets, as the models often struggle to distinguish tail-class in-distribution samples from OOD samples. We examine the main challenges in this problem by identifying the trade-offs between OOD detection and in-distribution (ID) classification, faced by existing methods. We then introduce our method, called \\textit{Representation Norm Amplification} (RNA), which solves this challenge by decoupling the two problems. The main idea is to use the norm of the representation as a new dimension for OOD detection, and to develop a training method that generates a noticeable discrepancy in the representation norm between ID and OOD data, while not perturbing the feature learning for ID classification. Our experiments show that RNA achieves superior performance in both OOD detection and classification compared to the state-of-the-art methods, by 1.70\\% and 9.46\\% in FPR95 and 2.43\\% and 6.87\\% in classification accuracy on CIFAR10-LT and ImageNet-LT, respectively. The code for this work is available at https://github.com/dgshin21/RNA.",
        "file_path": "paper_data/Out-of-Distribution_Detection/71fdc063701dc3f431942398d53b0290a9975d32.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 0,
        "score": 0
    },
    "8f53788139d97189af8204a36b109473a0a2b61f.pdf": {
        "title": "Unified Out-Of-Distribution Detection: A Model-Specific Perspective",
        "authors": [
            "Reza Averly",
            "Wei-Lun Chao"
        ],
        "published_date": "2023",
        "abstract": "Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is \"model-specific\". We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that involves a variety of models (e.g., different architectures and training strategies), sources of OOD examples, and OOD detection approaches, and reveal several insights into improving and understanding OOD detection in uncontrolled environments.",
        "file_path": "paper_data/Out-of-Distribution_Detection/8f53788139d97189af8204a36b109473a0a2b61f.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 0,
        "score": 0
    },
    "305941292b59d808af1f6646993747ba0f76f4ac.pdf": {
        "title": "GOOD-D: On Unsupervised Graph Out-Of-Distribution Detection",
        "authors": [
            "Yixin Liu",
            "Kaize Ding",
            "Huan Liu",
            "Shirui Pan"
        ],
        "published_date": "2022",
        "abstract": "Most existing deep learning models are trained based on the closed-world assumption, where the test data is assumed to be drawn i.i.d. from the same distribution as the training data, known as in-distribution (ID). However, when models are deployed in an open-world scenario, test samples can be out-of-distribution (OOD) and therefore should be handled with caution. To detect such OOD samples drawn from unknown distribution, OOD detection has received increasing attention lately. However, current endeavors mostly focus on grid-structured data and its application for graph-structured data remains under-explored. Considering the fact that data labeling on graphs is commonly time-expensive and labor-intensive, in this work we study the problem of unsupervised graph OOD detection, aiming at detecting OOD graphs solely based on unlabeled ID data. To achieve this goal, we develop a new graph contrastive learning framework GOOD-D for detecting OOD graphs without using any ground-truth labels. By performing hierarchical contrastive learning on the augmented graphs generated by our perturbation-free graph data augmentation method, GOOD-D is able to capture the latent ID patterns and accurately detect OOD graphs based on the semantic inconsistency in different granularities (i.e., node-level, graph-level, and group-level). As a pioneering work in unsupervised graph-level OOD detection, we build a comprehensive benchmark to compare our proposed approach with different state-of-the-art methods. The experiment results demonstrate the superiority of our approach over different methods on various datasets.",
        "file_path": "paper_data/Out-of-Distribution_Detection/305941292b59d808af1f6646993747ba0f76f4ac.pdf",
        "venue": "Web Search and Data Mining",
        "citationCount": 0,
        "score": 0
    },
    "0e3a01e0bd1beff9e77d8809629db24fc706c085.pdf": {
        "title": "Understanding the Feature Norm for Out-of-Distribution Detection",
        "authors": [
            "Jaewoo Park",
            "Jacky Chen Long Chai",
            "Jaeho Yoon",
            "Andrew Beng Jin Teoh"
        ],
        "published_date": "2023",
        "abstract": "A neural network trained on a classification dataset often exhibits a higher vector norm of hidden layer features for in-distribution (ID) samples, while producing relatively lower norm values on unseen instances from out-of-distribution (OOD). Despite this intriguing phenomenon being utilized in many applications, the underlying cause has not been thoroughly investigated. In this study, we demystify this very phenomenon by scrutinizing the discriminative structures concealed in the intermediate layers of a neural network. Our analysis leads to the following discoveries: (1) The feature norm is a confidence value of a classifier hidden in the network layer, specifically its maximum logit. Hence, the feature norm distinguishes OOD from ID in the same manner that a classifier confidence does. (2) The feature norm is class-agnostic, thus it can detect OOD samples across diverse discriminative models. (3) The conventional feature norm fails to capture the deactivation tendency of hidden layer neurons, which may lead to misidentification of ID samples as OOD instances. To resolve this drawback, we propose a novel negative-aware norm (NAN) that can capture both the activation and deactivation tendencies of hidden layer neurons. We conduct extensive experiments on NAN, demonstrating its efficacy and compatibility with existing OOD detectors, as well as its capability in label-free environments.",
        "file_path": "paper_data/Out-of-Distribution_Detection/0e3a01e0bd1beff9e77d8809629db24fc706c085.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 0,
        "score": 0
    },
    "f9ac68dc1fdd070a65a71c739e7135361c0d3006.pdf": {
        "title": "MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities",
        "authors": [
            "Hao Dong",
            "Yue Zhao",
            "Eleni Chatzi",
            "Olga Fink"
        ],
        "published_date": "2024",
        "abstract": "Detecting out-of-distribution (OOD) samples is important for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. Existing research has mainly focused on unimodal scenarios on image data. However, real-world applications are inherently multimodal, which makes it essential to leverage information from multiple modalities to enhance the efficacy of OOD detection. To establish a foundation for more realistic Multimodal OOD Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations. We first evaluate existing unimodal OOD detection algorithms on MultiOOD, observing that the mere inclusion of additional modalities yields substantial improvements. This underscores the importance of utilizing multiple modalities for OOD detection. Based on the observation of Modality Prediction Discrepancy between in-distribution (ID) and OOD data, and its strong correlation with OOD performance, we propose the Agree-to-Disagree (A2D) algorithm to encourage such discrepancy during training. Moreover, we introduce a novel outlier synthesis method, NP-Mix, which explores broader feature spaces by leveraging the information from nearest neighbor classes and complements A2D to strengthen OOD detection performance. Extensive experiments on MultiOOD demonstrate that training with A2D and NP-Mix improves existing OOD detection algorithms by a large margin. Our source code and MultiOOD benchmark are available at https://github.com/donghao51/MultiOOD.",
        "file_path": "paper_data/Out-of-Distribution_Detection/f9ac68dc1fdd070a65a71c739e7135361c0d3006.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "33fb671a3289027c84a71fc996f948195b1baeb4.pdf": {
        "title": "OpenCIL: Benchmarking Out-of-Distribution Detection in Class-Incremental Learning",
        "authors": [
            "Wenjun Miao",
            "Guansong Pang",
            "Trong-Tung Nguyen",
            "Ruohang Fang",
            "Jingyi Zheng",
            "Xiaolong Bai"
        ],
        "published_date": "2024",
        "abstract": "Class incremental learning (CIL) aims to learn a model that can not only incrementally accommodate new classes, but also maintain the learned knowledge of old classes. Out-of-distribution (OOD) detection in CIL is to retain this incremental learning ability, while being able to reject unknown samples that are drawn from different distributions of the learned classes. This capability is crucial to the safety of deploying CIL models in open worlds. However, despite remarkable advancements in the respective CIL and OOD detection, there lacks a systematic and large-scale benchmark to assess the capability of advanced CIL models in detecting OOD samples. To fill this gap, in this study we design a comprehensive empirical study to establish such a benchmark, named $\\textbf{OpenCIL}$. To this end, we propose two principled frameworks for enabling four representative CIL models with 15 diverse OOD detection methods, resulting in 60 baseline models for OOD detection in CIL. The empirical evaluation is performed on two popular CIL datasets with six commonly-used OOD datasets. One key observation we find through our comprehensive evaluation is that the CIL models can be severely biased towards the OOD samples and newly added classes when they are exposed to open environments. Motivated by this, we further propose a new baseline for OOD detection in CIL, namely Bi-directional Energy Regularization ($\\textbf{BER}$), which is specially designed to mitigate these two biases in different CIL models by having energy regularization on both old and new classes. Its superior performance is justified in our experiments. All codes and datasets are open-source at https://github.com/mala-lab/OpenCIL.",
        "file_path": "paper_data/Out-of-Distribution_Detection/33fb671a3289027c84a71fc996f948195b1baeb4.pdf",
        "venue": "Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "903966632e84a59ca49914ebbadbbfbfe84e7c29.pdf": {
        "title": "Neural Mean Discrepancy for Efficient Out-of-Distribution Detection",
        "authors": [
            "Xin Dong",
            "Junfeng Guo",
            "Ang Li",
            "W. Ting",
            "Cong Liu",
            "H. Kung"
        ],
        "published_date": "2021",
        "abstract": "Various approaches have been proposed for out-of-distribution (OOD) detection by augmenting models, input examples, training sets, and optimization objectives. Deviating from existing work, we have a simple hypothesis that standard off-the-shelf models may already contain sufficient information about the training set distribution which can be leveraged for reliable OOD detection. Our empirical study on validating this hypothesis, which measures the model activation's mean for OOD and in-distribution (ID) minibatches, surprisingly finds that activation means of OOD mini-batches consistently deviate more from those of the training data. In addition, training data's activation means can be computed offline efficiently or retrieved from batch normalization layers as a \u2018free lunch\u2019. Based upon this observation, we propose a novel metric called Neural Mean Discrepancy (NMD), which compares neural means of the input examples and training data. Leveraging the simplicity of NMD, we propose an efficient OOD detector that computes neural means by a standard forward pass followed by a lightweight classifier. Extensive experiments show that NMD outperforms state-of-the-art OOD approaches across multiple datasets and model architectures in terms of both detection accuracy and computational cost.",
        "file_path": "paper_data/Out-of-Distribution_Detection/903966632e84a59ca49914ebbadbbfbfe84e7c29.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "726cf970e8dc6642bb6064f78e7279cee50a9222.pdf": {
        "title": "A Unified Approach Towards Active Learning and Out-of-Distribution Detection",
        "authors": [
            "Sebastian Schmidt",
            "Leonard Schenk",
            "Leo Schwinn",
            "Stephan G\u00fcnnemann"
        ],
        "published_date": "2024",
        "abstract": "When applying deep learning models in open-world scenarios, active learning (AL) strategies are crucial for identifying label candidates from a nearly infinite amount of unlabeled data. In this context, robust out-of-distribution (OOD) detection mechanisms are essential for handling data outside the target distribution of the application. However, current works investigate both problems separately. In this work, we introduce SISOM as the first unified solution for both AL and OOD detection. By leveraging feature space distance metrics SISOM combines the strengths of the currently independent tasks to solve both effectively. We conduct extensive experiments showing the problems arising when migrating between both tasks. In these evaluations SISOM underlined its effectiveness by achieving first place in two of the widely used OpenOOD benchmarks and second place in the remaining one. In AL, SISOM outperforms others and delivers top-1 performance in three benchmarks",
        "file_path": "paper_data/Out-of-Distribution_Detection/726cf970e8dc6642bb6064f78e7279cee50a9222.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 0,
        "score": 0
    },
    "a58000542be3b6c6f9d275c31c64ec2b55cbf9f7.pdf": {
        "title": "Out-of-Distribution Detection: A Task-Oriented Survey of Recent Advances",
        "authors": [
            "Shuo Lu",
            "Yingsheng Wang",
            "Lijun Sheng",
            "Lingxiao He",
            "Aihua Zheng",
            "Jian Liang"
        ],
        "published_date": "2024",
        "abstract": "Out-of-distribution (OOD) detection aims to detect test samples outside the training category space, which is an essential component in building reliable machine learning systems. Existing reviews on OOD detection primarily focus on method taxonomy, surveying the field by categorizing various approaches. However, many recent works concentrate on non-traditional OOD detection scenarios, such as test-time adaptation, multi-modal data sources and other novel contexts. In this survey, we uniquely review recent advances in OOD detection from the task-oriented perspective for the first time. According to the user's access to the model, that is, whether the OOD detection method is allowed to modify or retrain the model, we classify the methods as training-driven or training-agnostic. Besides, considering the rapid development of pre-trained models, large pre-trained model-based OOD detection is also regarded as an important category and discussed separately. Furthermore, we provide a discussion of the evaluation scenarios, a variety of applications, and several future research directions. We believe this survey with new taxonomy will benefit the proposal of new methods and the expansion of more practical scenarios. A curated list of related papers is provided in the Github repository: https://github.com/shuolucs/Awesome-Out-Of-Distribution-Detection.",
        "file_path": "paper_data/Out-of-Distribution_Detection/a58000542be3b6c6f9d275c31c64ec2b55cbf9f7.pdf",
        "venue": "",
        "citationCount": 0,
        "score": 0
    },
    "41e68a78f5bd266b1ae54d521ebd0be0e9314cd8.pdf": {
        "title": "Watermarking for Out-of-distribution Detection",
        "authors": [
            "Qizhou Wang",
            "Feng Liu",
            "Yonggang Zhang",
            "Jing Zhang",
            "Chen Gong",
            "Tongliang Liu",
            "Bo Han"
        ],
        "published_date": "2022",
        "abstract": "Out-of-distribution (OOD) detection aims to identify OOD data based on representations extracted from well-trained deep models. However, existing methods largely ignore the reprogramming property of deep models and thus may not fully unleash their intrinsic strength: without modifying parameters of a well-trained deep model, we can reprogram this model for a new purpose via data-level manipulation (e.g., adding a specific feature perturbation to the data). This property motivates us to reprogram a classification model to excel at OOD detection (a new task), and thus we propose a general methodology named watermarking in this paper. Specifically, we learn a unified pattern that is superimposed onto features of original data, and the model's detection capability is largely boosted after watermarking. Extensive experiments verify the effectiveness of watermarking, demonstrating the significance of the reprogramming property of deep models in OOD detection.",
        "file_path": "paper_data/Out-of-Distribution_Detection/41e68a78f5bd266b1ae54d521ebd0be0e9314cd8.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "a43f7d6a751a6ad8667272f1176d2f15dbd8feb6.pdf": {
        "title": "Out-of-Distribution Detection Using Union of 1-Dimensional Subspaces",
        "authors": [
            "Alireza Zaeemzadeh",
            "Niccol\u00f3 Bisagno",
            "Zeno Sambugaro",
            "N. Conci",
            "Nazanin Rahnavard",
            "M. Shah"
        ],
        "published_date": "2021",
        "abstract": "The goal of out-of-distribution (OOD) detection is to handle the situations where the test samples are drawn from a different distribution than the training data. In this paper, we argue that OOD samples can be detected more easily if the training data is embedded into a low-dimensional space, such that the embedded training samples lie on a union of 1-dimensional subspaces. We show that such embedding of the in-distribution (ID) samples provides us with two main advantages. First, due to compact representation in the feature space, OOD samples are less likely to occupy the same region as the known classes. Second, the first singular vector of ID samples belonging to a 1-dimensional subspace can be used as their robust representative. Motivated by these observations, we train a deep neural network such that the ID samples are embedded onto a union of 1-dimensional subspaces. At the test time, employing sampling techniques used for approximate Bayesian inference in deep learning, input samples are detected as OOD if they occupy the region corresponding to the ID samples with probability 0. Spectral components of the ID samples are used as robust representative of this region. Our method does not have any hyperparameter to be tuned using extra information and it can be applied on different modalities with minimal change. The effectiveness of the proposed method is demonstrated on different benchmark datasets, both in the image and video classification domains.",
        "file_path": "paper_data/Out-of-Distribution_Detection/a43f7d6a751a6ad8667272f1176d2f15dbd8feb6.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "5d73ce0b2f017ed141aaaa3fdfee90d2098d1b2d.pdf": {
        "title": "Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization",
        "authors": [
            "Y. Liu",
            "Chris Xing Tian",
            "Haoliang Li",
            "Lei Ma",
            "Shiqi Wang"
        ],
        "published_date": "2023",
        "abstract": "The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution, i.e., in-distribution (InD). In this paper, we study the OOD problem from a neuron activation view. We first formulate neuron activation states by considering both the neuron output and its influence on model decisions. Then, to characterize the relationship between neurons and OOD issues, we introduce the \\textit{neuron activation coverage} (NAC) -- a simple measure for neuron behaviors under InD data. Leveraging our NAC, we show that 1) InD and OOD inputs can be largely separated based on the neuron behavior, which significantly eases the OOD detection problem and beats the 21 previous methods over three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1K). 2) a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets, which enables a NAC-based criterion for evaluating model robustness. Compared to prevalent InD validation criteria, we show that NAC not only can select more robust models, but also has a stronger correlation with OOD test performance.",
        "file_path": "paper_data/Out-of-Distribution_Detection/5d73ce0b2f017ed141aaaa3fdfee90d2098d1b2d.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "08925eef04eada4dd46dd3a33ea35f05795b12a9.pdf": {
        "title": "GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection",
        "authors": [
            "Jinggang Chen",
            "Junjie Li",
            "Xiaoyang Qu",
            "Jianzong Wang",
            "Jiguang Wan",
            "Jing Xiao"
        ],
        "published_date": "2023",
        "abstract": "Detecting out-of-distribution (OOD) examples is crucial to guarantee the reliability and safety of deep neural networks in real-world settings. In this paper, we offer an innovative perspective on quantifying the disparities between in-distribution (ID) and OOD data -- analyzing the uncertainty that arises when models attempt to explain their predictive decisions. This perspective is motivated by our observation that gradient-based attribution methods encounter challenges in assigning feature importance to OOD data, thereby yielding divergent explanation patterns. Consequently, we investigate how attribution gradients lead to uncertain explanation outcomes and introduce two forms of abnormalities for OOD detection: the zero-deflation abnormality and the channel-wise average abnormality. We then propose GAIA, a simple and effective approach that incorporates Gradient Abnormality Inspection and Aggregation. The effectiveness of GAIA is validated on both commonly utilized (CIFAR) and large-scale (ImageNet-1k) benchmarks. Specifically, GAIA reduces the average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100 compared to advanced post-hoc methods.",
        "file_path": "paper_data/Out-of-Distribution_Detection/08925eef04eada4dd46dd3a33ea35f05795b12a9.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "23bbd94f93e360f373f78ce20f61ec3486b1923d.pdf": {
        "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
        "authors": [
            "Yi Dai",
            "Hao Lang",
            "Kaisheng Zeng",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published_date": "2023",
        "abstract": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.",
        "file_path": "paper_data/Out-of-Distribution_Detection/23bbd94f93e360f373f78ce20f61ec3486b1923d.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "f72c1bfe25d68a7d6d008b0d500d2670ebe2bf4f.pdf": {
        "title": "ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation",
        "authors": [
            "Zhitong Gao",
            "Shipeng Yan",
            "Xuming He"
        ],
        "published_date": "2023",
        "abstract": "Recent advancements in dense out-of-distribution (OOD) detection have primarily focused on scenarios where the training and testing datasets share a similar domain, with the assumption that no domain shift exists between them. However, in real-world situations, domain shift often exits and significantly affects the accuracy of existing out-of-distribution (OOD) detection models. In this work, we propose a dual-level OOD detection framework to handle domain shift and semantic shift jointly. The first level distinguishes whether domain shift exists in the image by leveraging global low-level features, while the second level identifies pixels with semantic shift by utilizing dense high-level feature maps. In this way, we can selectively adapt the model to unseen domains as well as enhance model's capacity in detecting novel classes. We validate the efficacy of our proposed method on several OOD segmentation benchmarks, including those with significant domain shifts and those without, observing consistent performance improvements across various baseline models. Code is available at ${\\href{https://github.com/gaozhitong/ATTA}{https://github.com/gaozhitong/ATTA}}$.",
        "file_path": "paper_data/Out-of-Distribution_Detection/f72c1bfe25d68a7d6d008b0d500d2670ebe2bf4f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "565d5a9038154fbbcba3d4a6f17671af9515fbcc.pdf": {
        "title": "Out-Of-Distribution Detection with Diversification (Provably)",
        "authors": [
            "Haiyu Yao",
            "Zongbo Han",
            "Huazhu Fu",
            "Xi Peng",
            "Qinghua Hu",
            "Changqing Zhang"
        ],
        "published_date": "2024",
        "abstract": "Out-of-distribution (OOD) detection is crucial for ensuring reliable deployment of machine learning models. Recent advancements focus on utilizing easily accessible auxiliary outliers (e.g., data from the web or other datasets) in training. However, we experimentally reveal that these methods still struggle to generalize their detection capabilities to unknown OOD data, due to the limited diversity of the auxiliary outliers collected. Therefore, we thoroughly examine this problem from the generalization perspective and demonstrate that a more diverse set of auxiliary outliers is essential for enhancing the detection capabilities. However, in practice, it is difficult and costly to collect sufficiently diverse auxiliary outlier data. Therefore, we propose a simple yet practical approach with a theoretical guarantee, termed Diversity-induced Mixup for OOD detection (diverseMix), which enhances the diversity of auxiliary outlier set for training in an efficient way. Extensive experiments show that diverseMix achieves superior performance on commonly used and recent challenging large-scale benchmarks, which further confirm the importance of the diversity of auxiliary outliers.",
        "file_path": "paper_data/Out-of-Distribution_Detection/565d5a9038154fbbcba3d4a6f17671af9515fbcc.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "b723d4e9fbe81890624d11c873acb63ddf21b64b.pdf": {
        "title": "On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging",
        "authors": [
            "Harry Anthony",
            "K. Kamnitsas"
        ],
        "published_date": "2023",
        "abstract": "Implementing neural networks for clinical use in medical applications necessitates the ability for the network to detect when input data differs significantly from the training data, with the aim of preventing unreliable predictions. The community has developed several methods for out-of-distribution (OOD) detection, within which distance-based approaches - such as Mahalanobis distance - have shown potential. This paper challenges the prevailing community understanding that there is an optimal layer, or combination of layers, of a neural network for applying Mahalanobis distance for detection of any OOD pattern. Using synthetic artefacts to emulate OOD patterns, this paper shows the optimum layer to apply Mahalanobis distance changes with the type of OOD pattern, showing there is no one-fits-all solution. This paper also shows that separating this OOD detector into multiple detectors at different depths of the network can enhance the robustness for detecting different OOD patterns. These insights were validated on real-world OOD tasks, training models on CheXpert chest X-rays with no support devices, then using scans with unseen pacemakers (we manually labelled 50% of CheXpert for this research) and unseen sex as OOD cases. The results inform best-practices for the use of Mahalanobis distance for OOD detection. The manually annotated pacemaker labels and the project's code are available at: https://github.com/HarryAnthony/Mahalanobis-OOD-detection.",
        "file_path": "paper_data/Out-of-Distribution_Detection/b723d4e9fbe81890624d11c873acb63ddf21b64b.pdf",
        "venue": "UNSURE@MICCAI",
        "citationCount": 0,
        "score": 0
    },
    "1007a43d42c7c92d765cdf614c98f6fc974aaf15.pdf": {
        "title": "Full-Spectrum Out-of-Distribution Detection",
        "authors": [
            "Jingkang Yang",
            "Kaiyang Zhou",
            "Ziwei Liu"
        ],
        "published_date": "2022",
        "abstract": "Existing out-of-distribution (OOD) detection literature clearly defines semantic shift as a sign of OOD but does not have a consensus over covariate shift. Samples experiencing covariate shift but not semantic shift from the in-distribution (ID) are either excluded from the test set or treated as OOD, which contradicts the primary goal in machine learning\u2014being able to generalize beyond the training distribution. In this paper, we take into account both shift types and introduce full-spectrum OOD (F-OOD) detection, a more realistic problem setting that considers both detecting semantic shift and being tolerant to covariate shift; and design three benchmarks. These new benchmarks have a more fine-grained categorization of distributions ( i.e let@tokeneonedot, training ID, covariate-shifted ID, near-OOD, and far-OOD) for the purpose of more comprehensively evaluating the pros and cons of algorithms. To address the F-OOD detection problem, we propose SEM, a simple feature-based semantics score function. SEM is mainly composed of two probability measures: one is based on high-level features containing both semantic and non-semantic information, while the other is based on low-level feature statistics only capturing non-semantic image styles. With a simple combination, the non-semantic part is canceled out, which leaves only semantic information in SEM that can better handle F-OOD detection. Extensive experiments on the three new benchmarks show that SEM significantly outperforms current state-of-the-art methods. Our code and benchmarks are released in https://github.com/Jingkang50/OpenOOD .",
        "file_path": "paper_data/Out-of-Distribution_Detection/1007a43d42c7c92d765cdf614c98f6fc974aaf15.pdf",
        "venue": "International Journal of Computer Vision",
        "citationCount": 0,
        "score": 0
    },
    "7d826dfb184be983018590c64cfb4a79349472a4.pdf": {
        "title": "Block Selection Method for Using Feature Norm in Out-of-Distribution Detection",
        "authors": [
            "Yeonguk Yu",
            "Sungho Shin",
            "Seongju Lee",
            "C. Jun",
            "Kyoobin Lee"
        ],
        "published_date": "2022",
        "abstract": "Detecting out-of-distribution (OOD) inputs during the inference stage is crucial for deploying neural networks in the real world. Previous methods typically relied on the highly activated feature map outputted by the network. In this study, we revealed that the norm of the feature map obtained from a block other than the last block can serve as a better indicator for OOD detection. To leverage this insight, we propose a simple framework that comprises two metrics: FeatureNorm, which computes the norm of the feature map, and NormRatio, which calculates the ratio of FeatureNorm for ID and OOD samples to evaluate the OOD detection performance of each block. To identify the block that provides the largest difference between FeatureNorm of ID and FeatureNorm of OOD, we create jigsaw puzzles as pseudo OOD from ID training samples and compute NormRatio, selecting the block with the highest value. After identifying the suitable block, OOD detection using FeatureNorm outperforms other methods by reducing FPR95 by up to 52.77% on CIFAR10 benchmark and up to 48.53% on ImageNet benchmark. We demonstrate that our framework can generalize to various architectures and highlight the significance of block selection, which can also improve previous OOD detection methods. Our code is available at https://github.com/gistailab/block-selection-for-OOD-detection.",
        "file_path": "paper_data/Out-of-Distribution_Detection/7d826dfb184be983018590c64cfb4a79349472a4.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "87268ea5825cd65c1c3151d6ecc0973f267b3c68.pdf": {
        "title": "When and How Does In-Distribution Label Help Out-of-Distribution Detection?",
        "authors": [
            "Xuefeng Du",
            "Yiyou Sun",
            "Yixuan Li"
        ],
        "published_date": "2024",
        "abstract": "Detecting data points deviating from the training distribution is pivotal for ensuring reliable machine learning. Extensive research has been dedicated to the challenge, spanning classical anomaly detection techniques to contemporary out-of-distribution (OOD) detection approaches. While OOD detection commonly relies on supervised learning from a labeled in-distribution (ID) dataset, anomaly detection may treat the entire ID data as a single class and disregard ID labels. This fundamental distinction raises a significant question that has yet to be rigorously explored: when and how does ID label help OOD detection? This paper bridges this gap by offering a formal understanding to theoretically delineate the impact of ID labels on OOD detection. We employ a graph-theoretic approach, rigorously analyzing the separability of ID data from OOD data in a closed-form manner. Key to our approach is the characterization of data representations through spectral decomposition on the graph. Leveraging these representations, we establish a provable error bound that compares the OOD detection performance with and without ID labels, unveiling conditions for achieving enhanced OOD detection. Lastly, we present empirical results on both simulated and real datasets, validating theoretical guarantees and reinforcing our insights. Code is publicly available at https://github.com/deeplearning-wisc/id_label.",
        "file_path": "paper_data/Out-of-Distribution_Detection/87268ea5825cd65c1c3151d6ecc0973f267b3c68.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0
    },
    "74491e50e381210badd7c8a0eee69d10410f6a68.pdf": {
        "title": "Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites Paradox",
        "authors": [
            "Xingming Long",
            "Jie Zhang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "published_date": "2024",
        "abstract": "Most existing out-of-distribution (OOD) detection benchmarks classify samples with novel labels as the OOD data. However, some marginal OOD samples actually have close semantic contents to the in-distribution (ID) sample, which makes determining the OOD sample a Sorites Paradox. In this paper, we construct a benchmark named Incremental Shift OOD (IS-OOD) to address the issue, in which we divide the test samples into subsets with different semantic and covariate shift degrees relative to the ID dataset. The data division is achieved through a shift measuring method based on our proposed Language Aligned Image feature Decomposition (LAID). Moreover, we construct a Synthetic Incremental Shift (Syn-IS) dataset that contains high-quality generated images with more diverse covariate contents to complement the IS-OOD benchmark. We evaluate current OOD detection methods on our benchmark and find several important insights: (1) The performance of most OOD detection methods significantly improves as the semantic shift increases; (2) Some methods like GradNorm may have different OOD detection mechanisms as they rely less on semantic shifts to make decisions; (3) Excessive covariate shifts in the image are also likely to be considered as OOD for some methods. Our code and data are released in https://github.com/qqwsad5/IS-OOD.",
        "file_path": "paper_data/Out-of-Distribution_Detection/74491e50e381210badd7c8a0eee69d10410f6a68.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "79c72327dd14466c4db3865902c8317f74bb4c56.pdf": {
        "title": "Learning with Mixture of Prototypes for Out-of-Distribution Detection",
        "authors": [
            "Haodong Lu",
            "Dong Gong",
            "Shuo Wang",
            "Jason Xue",
            "Lina Yao",
            "Kristen Moore"
        ],
        "published_date": "2024",
        "abstract": "Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, which is crucial for the safe deployment of machine learning models in the real world. Distance-based OOD detection methods have emerged with enhanced deep representation learning. They identify unseen OOD samples by measuring their distances from ID class centroids or prototypes. However, existing approaches learn the representation relying on oversimplified data assumptions, e.g, modeling ID data of each class with one centroid class prototype or using loss functions not designed for OOD detection, which overlook the natural diversities within the data. Naively enforcing data samples of each class to be compact around only one prototype leads to inadequate modeling of realistic data and limited performance. To tackle these issues, we propose PrototypicAl Learning with a Mixture of prototypes (PALM) which models each class with multiple prototypes to capture the sample diversities, and learns more faithful and compact samples embeddings to enhance OOD detection. Our method automatically identifies and dynamically updates prototypes, assigning each sample to a subset of prototypes via reciprocal neighbor soft assignment weights. PALM optimizes a maximum likelihood estimation (MLE) loss to encourage the sample embeddings to be compact around the associated prototypes, as well as a contrastive loss on all prototypes to enhance intra-class compactness and inter-class discrimination at the prototype level. Moreover, the automatic estimation of prototypes enables our approach to be extended to the challenging OOD detection task with unlabelled ID data. Extensive experiments demonstrate the superiority of PALM, achieving state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark. Code is available at https://github.com/jeff024/PALM.",
        "file_path": "paper_data/Out-of-Distribution_Detection/79c72327dd14466c4db3865902c8317f74bb4c56.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "913d26360f1a715f6ae80f5a775f398aa2f66c9d.pdf": {
        "title": "DOS: Diverse Outlier Sampling for Out-of-Distribution Detection",
        "authors": [
            "Wenyu Jiang",
            "Hao Cheng",
            "Mingcai Chen",
            "Chongjun Wang",
            "Hongxin Wei"
        ],
        "published_date": "2023",
        "abstract": "Modern neural networks are known to give overconfident prediction for out-of-distribution inputs when deployed in the open world. It is common practice to leverage a surrogate outlier dataset to regularize the model during training, and recent studies emphasize the role of uncertainty in designing the sampling strategy for outlier dataset. However, the OOD samples selected solely based on predictive uncertainty can be biased towards certain types, which may fail to capture the full outlier distribution. In this work, we empirically show that diversity is critical in sampling outliers for OOD detection performance. Motivated by the observation, we propose a straightforward and novel sampling strategy named DOS (Diverse Outlier Sampling) to select diverse and informative outliers. Specifically, we cluster the normalized features at each iteration, and the most informative outlier from each cluster is selected for model training with absent category loss. With DOS, the sampled outliers efficiently shape a globally compact decision boundary between ID and OOD data. Extensive experiments demonstrate the superiority of DOS, reducing the average FPR95 by up to 25.79% on CIFAR-100 with TI-300K.",
        "file_path": "paper_data/Out-of-Distribution_Detection/913d26360f1a715f6ae80f5a775f398aa2f66c9d.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "8fe4a9aec9185a2f9da79571f8d239816d4a23d2.pdf": {
        "title": "RankFeat: Rank-1 Feature Removal for Out-of-distribution Detection",
        "authors": [
            "Yue Song",
            "N. Sebe",
            "Wei Wang"
        ],
        "published_date": "2022",
        "abstract": "The task of out-of-distribution (OOD) detection is crucial for deploying machine learning models in real-world settings. In this paper, we observe that the singular value distributions of the in-distribution (ID) and OOD features are quite different: the OOD feature matrix tends to have a larger dominant singular value than the ID feature, and the class predictions of OOD samples are largely determined by it. This observation motivates us to propose \\texttt{RankFeat}, a simple yet effective \\texttt{post hoc} approach for OOD detection by removing the rank-1 matrix composed of the largest singular value and the associated singular vectors from the high-level feature (\\emph{i.e.,} $\\mathbf{X}{-} \\mathbf{s}_{1}\\mathbf{u}_{1}\\mathbf{v}_{1}^{T}$). \\texttt{RankFeat} achieves the \\emph{state-of-the-art} performance and reduces the average false positive rate (FPR95) by 17.90\\% compared with the previous best method. Extensive ablation studies and comprehensive theoretical analyses are presented to support the empirical results.",
        "file_path": "paper_data/Out-of-Distribution_Detection/8fe4a9aec9185a2f9da79571f8d239816d4a23d2.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "3f18ce9eeb62a8d8abcc5627e9e1b8af2a902129.pdf": {
        "title": "Long-Tailed Out-of-Distribution Detection via Normalized Outlier Distribution Adaptation",
        "authors": [
            "Wenjun Miao",
            "Guansong Pang",
            "Jingyi Zheng",
            "Xiaolong Bai"
        ],
        "published_date": "2024",
        "abstract": "One key challenge in Out-of-Distribution (OOD) detection is the absence of ground-truth OOD samples during training. One principled approach to address this issue is to use samples from external datasets as outliers (i.e., pseudo OOD samples) to train OOD detectors. However, we find empirically that the outlier samples often present a distribution shift compared to the true OOD samples, especially in Long-Tailed Recognition (LTR) scenarios, where ID classes are heavily imbalanced, \\ie, the true OOD samples exhibit very different probability distribution to the head and tailed ID classes from the outliers. In this work, we propose a novel approach, namely normalized outlier distribution adaptation (AdaptOD), to tackle this distribution shift problem. One of its key components is dynamic outlier distribution adaptation that effectively adapts a vanilla outlier distribution based on the outlier samples to the true OOD distribution by utilizing the OOD knowledge in the predicted OOD samples during inference. Further, to obtain a more reliable set of predicted OOD samples on long-tailed ID data, a novel dual-normalized energy loss is introduced in AdaptOD, which leverages class- and sample-wise normalized energy to enforce a more balanced prediction energy on imbalanced ID samples. This helps avoid bias toward the head samples and learn a substantially better vanilla outlier distribution than existing energy losses during training. It also eliminates the need of manually tuning the sensitive margin hyperparameters in energy losses. Empirical results on three popular benchmarks for OOD detection in LTR show the superior performance of AdaptOD over state-of-the-art methods. Code is available at https://github.com/mala-lab/AdaptOD.",
        "file_path": "paper_data/Out-of-Distribution_Detection/3f18ce9eeb62a8d8abcc5627e9e1b8af2a902129.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "c5b439fa6766e4d9dabf09d1b0d686311b494914.pdf": {
        "title": "Gradient-Regularized Out-of-Distribution Detection",
        "authors": [
            "Sina Sharifi",
            "Taha Entesari",
            "Bardia Safaei",
            "Vishal M. Patel",
            "Mahyar Fazlyab"
        ],
        "published_date": "2024",
        "abstract": "One of the challenges for neural networks in real-life applications is the overconfident errors these models make when the data is not from the original training distribution. Addressing this issue is known as Out-of-Distribution (OOD) detection. Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate for OOD data during training to achieve improved performance. However, these methods fail to fully exploit the local information embedded in the auxiliary dataset. In this work, we propose the idea of leveraging the information embedded in the gradient of the loss function during training to enable the network to not only learn a desired OOD score for each sample but also to exhibit similar behavior in a local neighborhood around each sample. We also develop a novel energy-based sampling method to allow the network to be exposed to more informative OOD samples during the training phase. This is especially important when the auxiliary dataset is large. We demonstrate the effectiveness of our method through extensive experiments on several OOD benchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet experiment. We further provide a theoretical analysis through the lens of certified robustness and Lipschitz analysis to showcase the theoretical foundation of our work. Our code is available at https://github.com/o4lc/Greg-OOD.",
        "file_path": "paper_data/Out-of-Distribution_Detection/c5b439fa6766e4d9dabf09d1b0d686311b494914.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 0,
        "score": 0
    },
    "bea84d4f28799628fa91585690088c00e8dca827.pdf": {
        "title": "How Does Unlabeled Data Provably Help Out-of-Distribution Detection?",
        "authors": [
            "Xuefeng Du",
            "Zhen Fang",
            "Ilias Diakonikolas",
            "Yixuan Li"
        ],
        "published_date": "2024",
        "abstract": "Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our theory shows that SAL can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned OOD classifier. Empirically, SAL achieves state-of-the-art performance on common benchmarks, reinforcing our theoretical insights. Code is publicly available at https://github.com/deeplearning-wisc/sal.",
        "file_path": "paper_data/Out-of-Distribution_Detection/bea84d4f28799628fa91585690088c00e8dca827.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "a1ce596ef67f28f433f3de1001774211d00b54f0.pdf": {
        "title": "GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation",
        "authors": [
            "Danny Wang",
            "Ruihong Qiu",
            "Guangdong Bai",
            "Zi Huang"
        ],
        "published_date": "2025",
        "abstract": "Despite graph neural networks' (GNNs) great success in modelling graph-structured data, out-of-distribution (OOD) test instances still pose a great challenge for current GNNs. One of the most effective techniques to detect OOD nodes is to expose the detector model with an additional OOD node-set, yet the extra OOD instances are often difficult to obtain in practice. Recent methods for image data address this problem using OOD data synthesis, typically relying on pre-trained generative models like Stable Diffusion. However, these approaches require vast amounts of additional data, as well as one-for-all pre-trained generative models, which are not available for graph data. Therefore, we propose the GOLD framework for graph OOD detection, an implicit adversarial learning pipeline with synthetic OOD exposure without pre-trained models. The implicit adversarial training process employs a novel alternating optimisation framework by training: (1) a latent generative model to regularly imitate the in-distribution (ID) embeddings from an evolving GNN, and (2) a GNN encoder and an OOD detector to accurately classify ID data while increasing the energy divergence between the ID embeddings and the generative model's synthetic embeddings. This novel approach implicitly transforms the synthetic embeddings into pseudo-OOD instances relative to the ID data, effectively simulating exposure to OOD scenarios without auxiliary data. Extensive OOD detection experiments are conducted on five benchmark graph datasets, verifying the superior performance of GOLD without using real OOD data compared with the state-of-the-art OOD exposure and non-exposure baselines.",
        "file_path": "paper_data/Out-of-Distribution_Detection/a1ce596ef67f28f433f3de1001774211d00b54f0.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "5df7dcb96a465ed4d4d2fa2414413a41494fee8c.pdf": {
        "title": "ATS: Adaptive Temperature Scaling for Enhancing Out-of-Distribution Detection Methods",
        "authors": [
            "Gerhard Krumpl",
            "H. Avenhaus",
            "Horst Possegger",
            "Horst Bischof"
        ],
        "published_date": "2024",
        "abstract": "Out-of-distribution (OOD) detection is essential to ensure the reliability and robustness of machine learning models in real-world applications. Post-hoc OOD detection methods have gained significant attention due to the fact that they offer the advantage of not requiring additional re-training, which could degrade model performance and increase training time. However, most existing post-hoc methods rely only on the encoder output (features), logits, or the softmax probability, meaning they have no access to information that might be lost in the feature extraction process. In this work, we address this limitation by introducing Adaptive Temperature Scaling (ATS), a novel approach that dynamically calculates a temperature value based on activations of the intermediate layers. Fusing this sample-specific adjustment with class-dependent logits, our ATS captures additional statistical information before they are lost in the feature extraction process, leading to a more robust and powerful OOD detection method. We conduct extensive experiments to demonstrate the efficacy of our approach. Notably, our method can be seamlessly combined with SOTA post-hoc OOD detection methods that rely on the logits, thereby enhancing their performance and improving their robustness.",
        "file_path": "paper_data/Out-of-Distribution_Detection/5df7dcb96a465ed4d4d2fa2414413a41494fee8c.pdf",
        "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
        "citationCount": 0,
        "score": 0
    },
    "5f8ccbe2a87df5e1340d4bda8f6e3458ef1bf6ae.pdf": {
        "title": "Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)",
        "authors": [
            "Paul Novello",
            "Joseba Dalmau",
            "L'eo Andeol"
        ],
        "published_date": "2024",
        "abstract": "Research on Out-Of-Distribution (OOD) detection focuses mainly on building scores that efficiently distinguish OOD data from In Distribution (ID) data. On the other hand, Conformal Prediction (CP) uses non-conformity scores to construct prediction sets with probabilistic coverage guarantees. In this work, we propose to use CP to better assess the efficiency of OOD scores. Specifically, we emphasize that in standard OOD benchmark settings, evaluation metrics can be overly optimistic due to the finite sample size of the test dataset. Based on the work of (Bates et al., 2022), we define new conformal AUROC and conformal FRP@TPR95 metrics, which are corrections that provide probabilistic conservativeness guarantees on the variability of these metrics. We show the effect of these corrections on two reference OOD and anomaly detection benchmarks, OpenOOD (Yang et al., 2022) and ADBench (Han et al., 2022). We also show that the benefits of using OOD together with CP apply the other way around by using OOD scores as non-conformity scores, which results in improving upon current CP methods. One of the key messages of these contributions is that since OOD is concerned with designing scores and CP with interpreting these scores, the two fields may be inherently intertwined.",
        "file_path": "paper_data/Out-of-Distribution_Detection/5f8ccbe2a87df5e1340d4bda8f6e3458ef1bf6ae.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    }
}