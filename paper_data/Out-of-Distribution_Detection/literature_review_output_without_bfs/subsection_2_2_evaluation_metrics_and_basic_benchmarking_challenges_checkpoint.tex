\subsection{Evaluation Metrics and Basic Benchmarking Challenges}
The reliable deployment of machine learning models hinges on their ability to detect out-of-distribution (OOD) inputs, yet the methodologies for evaluating these algorithms and the inherent challenges in creating comprehensive benchmarks remain significant hurdles. Standard evaluation metrics, such as Area Under the Receiver Operating Characteristic curve (AUROC), Area Under the Precision-Recall curve (AUPR), and False Positive Rate at 95\% True Positive Rate (FPR@95TPR), are widely adopted due to their threshold-independent nature. However, these metrics often provide a binary assessment of OOD detection, failing to capture the nuanced spectrum of distribution shifts encountered in real-world scenarios \cite{long2024os1}.

A primary limitation of existing evaluation paradigms is their oversimplified definition of OOD, which often conflates different types of distribution shifts. \cite{yang2022it3} critically highlights this by introducing the Full-Spectrum OOD (FS-OOD) problem, which explicitly distinguishes between semantic shift (novel classes) and covariate shift (label-preserving appearance changes). Their work demonstrates that many state-of-the-art OOD methods struggle to differentiate these shifts, often misclassifying covariate-shifted in-distribution (ID) data as OOD or vice-versa. Building on this, \cite{yang2023ckx} exposes severe shortcomings in popular ImageNet-based OOD benchmarks, identifying issues like ID contamination, semantic ambiguities, and unintended covariate shifts. Their meticulously curated ImageNet-OOD dataset reveals that modern OOD algorithms are disproportionately sensitive to covariate shifts rather than genuine semantic novelty, showing minimal improvement over simple baselines for new-class detection when covariate shifts are minimized. This observation is further corroborated by \cite{wang2024is1}, which provides a critical dissection of OOD detection and Open-Set Recognition (OSR). Their proposed large-scale benchmark systematically disentangles semantic and covariate shifts, confirming that many methods' reported performance gains are often attributable to their sensitivity to covariate changes rather than true OOD detection capabilities.

The difficulty of curating diverse and representative OOD datasets is a persistent challenge. The "Sorites Paradox" in OOD evaluation, as articulated by \cite{long2024os1}, underscores the ambiguity in defining "how different" an OOD sample must be, leading to subjective and labor-intensive manual filtering in benchmarks. To address this, \cite{long2024os1} introduces the Incremental Shift OOD (IS-OOD) benchmark, which categorizes OOD samples by continuous semantic and covariate shift degrees, providing a more granular and semantically informed evaluation. Beyond semantic and covariate shifts, \cite{ming2021wu7} sheds light on the critical impact of spurious correlations, formalizing "spurious OOD" where models rely on non-causal features. Their findings demonstrate that increasing spurious correlation in training data severely degrades OOD detection performance, a factor often overlooked in standard benchmarks.

The impact of dataset choice on reported performance is profound and often domain-specific. In medical imaging, for instance, \cite{berger20214a3} conducts a comparative study revealing that high OOD detection performance on general computer vision benchmarks does not translate to medical tasks, which present unique challenges. \cite{vasiliuk20233w9} further highlights limitations in 3D medical image segmentation, designing a new public benchmark and showing that even a simple Intensity Histogram Features (IHF) baseline can outperform complex deep learning methods for certain OOD shifts. \cite{hong2024xls} provides a comprehensive survey for medical image analysis, proposing a systematic taxonomy of distributional shifts (semantic, covariate, contextual) to guide more targeted evaluation. Similarly, in graph-structured data, \cite{liu202227x} pioneers unsupervised graph-level OOD detection and constructs a comprehensive benchmark, while \cite{wang2024q01} unifies unsupervised graph-level anomaly detection and OOD detection under a single benchmark (UB-GOLD) to enable cross-comparison across previously disparate evaluation setups. For generative language models, \cite{wang2024rej} identifies "pattern collapse" in mathematical reasoning as a unique challenge, making traditional static embedding-based OOD detection ineffective.

A recurring challenge across domains is the scarcity of real OOD data for training, which limits the effectiveness of methods like Outlier Exposure (OE). \cite{wang2025xwm} addresses this for graph OOD by proposing an implicit adversarial latent generation method to synthesize pseudo-OOD samples. In the multi-modal domain, \cite{dai2023mhn} and \cite{cao20246gj} leverage Large Language Models (LLMs) to "envision" or generate synthetic outlier exposure, overcoming the practical unavailability of true OOD labels, though \cite{dai2023mhn} stresses the need for uncertainty calibration to mitigate LLM hallucinations. The theoretical underpinnings of OOD detection also highlight fundamental limitations: \cite{fang20249gd} provides a PAC learning theory for OOD detection, demonstrating that it is not universally learnable under all conditions, which implies that a single benchmark or metric cannot capture all aspects of performance. Furthermore, \cite{du2024aea} theoretically explores when and how in-distribution labels help OOD detection, showing their benefit is more pronounced for "near OOD" scenarios.

The persistent need for unified benchmarks to enable fair and reproducible comparisons is evident in the continuous efforts to create more robust evaluation frameworks. From the FS-OOD benchmarks \cite{yang2022it3} and ImageNet-OOD \cite{yang2023ckx} for disentangling shifts, to the Model-Specific OOD (MS-OOD) framework by \cite{averly20239rv} which defines OOD based on a model's actual misclassification, researchers are striving for more realistic and comprehensive evaluations. The introduction of benchmarks specifically designed for foundation models \cite{borlino20245ku} also addresses the saturation of existing testbeds by increasingly powerful models. Despite these advancements, the field still grapples with the inherent complexity of defining and measuring "out-of-distribution" in a manner that is both theoretically sound and practically relevant, necessitating continued innovation in both metrics and benchmark design to ensure meaningful progress.