\subsection{OOD in Long-Tailed Recognition and Fine-Grained Environments}

Out-of-Distribution (OOD) detection in scenarios characterized by imbalanced class distributions, commonly known as long-tailed recognition, and in fine-grained environments where inter-class differences are subtle, presents a particularly complex challenge for robust AI systems. These settings exacerbate the fundamental OOD problem by introducing ambiguities where rare in-distribution (ID) tail classes can be easily misclassified as OOD, while true OOD samples might be erroneously absorbed into dominant head classes due to their statistical prevalence \cite{miao2023brn}. Similarly, in fine-grained tasks, the subtle distinctions between ID categories can lead to OOD samples being confused with semantically similar ID classes, demanding highly discriminative and nuanced detection mechanisms. Addressing these complexities requires specialized strategies that move beyond conventional OOD detection methods, which often assume balanced ID distributions and clear OOD boundaries.

Early efforts to enhance OOD detection in fine-grained contexts, where unknown samples share significant semantic similarity with known classes, focused on refining outlier exposure techniques. For instance, \cite{zhang20212tb} introduced Mixture Outlier Exposure (MixOE), a method designed to generate "virtual" outlier samples by mixing ID data with auxiliary outlier data. The core idea is to expand the coverage of the feature space, thereby enabling the model to learn a confidence score that linearly decays from ID to OOD. This linear decay is crucial for fine-grained discrimination, as it provides a more granular measure of OODness, allowing for better separation of samples that lie close to the ID manifold. However, the effectiveness of such methods is inherently tied to the quality and diversity of the auxiliary outlier data, and the synthetic nature of virtual outliers may not always capture the full spectrum of real-world OOD variations.

The challenge intensifies significantly in long-tailed recognition, where the scarcity of data for tail classes makes them inherently difficult to model, rendering them highly susceptible to being misclassified as OOD. Simultaneously, the abundance of head class data can lead to models over-prioritizing these classes, potentially absorbing novel OOD samples into their broad decision regions. To mitigate these issues, \cite{miao2023brn} proposed Calibrated Outlier Class Learning (COCL). COCL extends the traditional label space by explicitly incorporating an outlier class, transforming OOD detection into a multi-class classification problem. This is coupled with debiased large margin learning, which includes OOD-aware tail class prototype learning and debiased head class learning. By learning distinct representation regions for head, tail, and outlier classes, COCL aims to prevent the collapse of tail class representations and the over-expansion of head class boundaries. The subsequent outlier-class-aware logit calibration during inference further refines decision boundaries, significantly reducing the confusion between OOD, head, and tail samples. This approach offers a principled way to balance the trade-off between ID accuracy and OOD detection in imbalanced settings.

Complementing COCL, \cite{wei2023f15} introduced EAT, a framework that leverages dynamic virtual labels for OOD data and context-rich tail class augmentation. EAT addresses the long-tail problem by assigning multiple abstention classes to OOD data and dynamically updating these virtual labels based on model predictions, allowing for more adaptive learning of OOD boundaries. Furthermore, its context-rich augmentation strategy, which overlays tail-class images onto diverse OOD backgrounds, directly tackles the data scarcity for tail classes. This augmentation not only enriches the tail class representation but also forces the model to learn more robust features that distinguish tail classes from various OOD contexts, thereby enhancing generalization and OOD distinction. While COCL focuses on explicit class learning and calibration, EAT emphasizes dynamic labeling and data augmentation, offering alternative yet complementary strategies for long-tailed OOD.

Beyond specific long-tailed methods, the quality and balance of auxiliary OOD data itself can significantly impact performance, especially when using Outlier Exposure-like techniques. Recognizing that auxiliary OOD datasets can also exhibit class imbalance, \cite{choi202367m} proposed a balanced energy regularization loss. This loss adaptively applies stronger regularization to auxiliary OOD samples originating from majority classes within the auxiliary dataset. This nuanced approach is critical for long-tailed classification, as it ensures that the model learns more effectively from diverse auxiliary outliers, preventing bias towards dominant auxiliary categories and promoting a more uniform OOD boundary learning.

With the advent of powerful Vision-Language Models (VLMs), new avenues have opened for OOD detection in fine-grained and complex multi-object environments, often enabling zero-shot and open-vocabulary capabilities. \cite{miyai2023591} introduced Global-Local Maximum Concept Matching (GL-MCM) for zero-shot OOD detection in multi-object scenes. This method combines global image features with local visual-text alignments, which is particularly effective in scenarios where OOD objects might contaminate global image features. By allowing flexibility in defining ID images even when multiple objects are present, GL-MCM helps avoid misclassifying ID objects in complex, fine-grained contexts where the presence of an unknown object might otherwise trigger a false OOD alarm. This represents a significant step towards more semantically aware OOD detection.

Further leveraging VLMs, \cite{li20245b6} proposed NegPrompt, a method that learns transferable negative prompts for each ID class using only ID training data. These negative prompts serve to delineate OOD boundaries by making OOD samples more similar to negative connotations associated with ID classes. This innovative approach enables open-vocabulary OOD detection and significantly improves robustness against hard OOD samples, crucially without requiring any auxiliary OOD data during training. The ability to define OOD through learned negative semantic spaces represents a paradigm shift, reducing the dependency on often-scarce or unrepresentative auxiliary OOD datasets. Complementing this, \cite{yu20249dd} developed Self-Calibrated Tuning (SCT) for VLMs, which adaptively balances ID classification and OOD regularization. SCT directly addresses the issue of "spurious OOD features" that can be extracted from ID data due to imperfect foreground-background decomposition or subtle contextual variations. By dynamically adjusting optimization based on prediction uncertainty, SCT prevents the misclassification of ID samples as OOD, ensuring that the model maintains high ID accuracy while effectively regularizing against OOD inputs in fine-grained settings.

In conclusion, OOD detection in long-tailed and fine-grained environments demands a sophisticated array of strategies to navigate inherent ambiguities, data imbalances, and subtle semantic distinctions. Methods employing outlier class learning, dynamic virtual labels, and context-rich augmentation have significantly improved OOD discrimination in long-tailed settings by explicitly addressing the confusion between head, tail, and OOD classes. The emergence of VLM-based approaches, leveraging local features, negative prompts, and self-calibrated tuning, offers promising avenues for fine-grained and open-vocabulary environments by enabling more semantically informed OOD detection without explicit OOD data. Despite these advancements, significant challenges remain. Developing solutions that are robust to diverse forms of OOD (e.g., near-OOD vs. far-OOD), highly adaptable to varying degrees of class imbalance, and capable of maintaining high ID accuracy while minimizing false positives for rare ID classes, particularly in real-world, dynamic scenarios, continues to be an active area of research. The inherent difficulty of precisely defining OOD boundaries in fine-grained contexts, where the line between novelty and subtle ID variation is blurred, underscores the need for continued innovation in this critical domain.