\subsection{OOD in Continual Learning and Active Learning Systems}
The deployment of intelligent systems in dynamic, open-world environments necessitates models that can adapt to evolving data streams while maintaining reliability and safety. Out-of-Distribution (OOD) detection plays a crucial and multifaceted role in these adaptive learning paradigms, particularly in continual learning (CL) and active learning (AL), by enabling the identification of novel concepts, managing knowledge evolution through uncertainty quantification, and guiding the selection of informative samples.

In the realm of continual learning, where models incrementally acquire new knowledge without catastrophically forgetting previously learned concepts, OOD detection is vital for identifying novel data that represents new classes or tasks, distinguishing it from mere noise or known but uncertain samples. This capability is paramount for open-world CL systems that must not only adapt to new information but also recognize when data falls outside their current competence. \cite{aguilar2023ms5} addresses this by proposing Continual Evidential Deep Learning (CEDL), which integrates evidential deep learning (EDL) into a CL framework to simultaneously perform incremental object classification and OOD detection. Their method leverages a novel loss function combining evidential cross-entropy, KL-divergence regularization, and knowledge distillation, demonstrating that evidential uncertainty (specifically vacuity) effectively identifies OOD data in dynamic settings while maintaining classification performance. Extending this, \cite{miao2023zf5} explores few-shot traffic classification with OOD detection using a Siamese Prototypical Network (SPN), enabling rapid learning of new traffic types while detecting unknown patterns, which is critical for security in evolving network environments. Furthermore, for systems that need to expand their recognizable categories, \cite{liu20245e5} introduces a category-extensible OOD detection framework via hierarchical context descriptions in vision-language models. This approach, using perceptual and spurious contexts, allows for precise boundary definitions for unseen categories without fine-tuning the base encoders, thereby supporting adaptation to evolving semantic spaces.

However, integrating OOD detection into CL is not without its challenges, often revealing inherent biases and trade-offs. \cite{miao20246mk} highlights these complexities through the OpenCIL benchmark, the first comprehensive evaluation framework for OOD detection in Class-Incremental Learning (CIL). Their work reveals that CIL models exhibit significant biases: they tend to over-confidently classify OOD samples into new classes and, conversely, show low confidence for old class samples due to catastrophic forgetting, leading to their misclassification as OOD. To mitigate these issues, \cite{miao20246mk} proposes Bi-directional Energy Regularization (BER), a novel fine-tuning approach that uses targeted energy loss to enlarge decision boundaries for new classes (pushing OOD away) and boost confidence for old classes (preventing old ID from being seen as OOD). This research underscores the critical need for CL methods to explicitly account for OOD detection during training, rather than relying solely on post-hoc applications, to overcome the inherent plasticity-stability dilemma in open-world scenarios.

For active learning, OOD detection can significantly enhance efficiency by guiding the selection of the most informative samples for labeling, optimizing the learning process by prioritizing data that lies at the boundary of known distributions or represents entirely novel concepts. Active learning strategies typically balance two types of uncertainty: *epistemic uncertainty*, which reflects the model's lack of knowledge about known concepts (e.g., samples near decision boundaries), and *aleatoric uncertainty* or *novelty*, which indicates that a sample is OOD. OOD detection directly addresses the latter, enabling the model to explore truly unknown regions of the data space. \cite{schmidt2024syr} proposes SISOM (Simultaneous Informative Sampling and Outlier Mining), a pioneering unified approach that addresses both active learning and OOD detection concurrently. SISOM leverages gradient-enhanced feature space distances and a self-adaptive mechanism to identify both diverse, unlabeled samples for acquisition (addressing epistemic uncertainty and diversity) and anomalous OOD inputs (addressing novelty). This integration optimizes the learning process by prioritizing data that offers the most learning signal, whether it's refining existing class boundaries or discovering entirely new phenomena. The challenge in AL lies in effectively balancing the acquisition of samples that improve existing classification boundaries with those that expand the model's understanding of its operational domain by identifying OOD data.

Beyond these direct integrations, the reliability of OOD detection in dynamic CL and AL systems often requires strong theoretical guarantees and adaptive mechanisms. While Section 7.1 discusses conformal prediction in general, its application within CL and AL loops is particularly impactful. For instance, in continual learning, statistically sound OOD detection with calibrated false alarm rates, as provided by methods like Inductive Conformal Anomaly Detection (ICAD) \cite{cai2020lsi} or iDECODe \cite{kaur2022cty}, can be crucial for deciding whether a new batch of data represents a genuinely novel task requiring model adaptation or merely OOD noise that should be rejected. Similarly, for time-series data in learning-enabled Cyber-Physical Systems (CPS), \cite{kaur20248t3} applies conformal guarantees to OOD detection, exploiting temporal relationships to detect anomalies in real-time traces with bounded false alarms, which is vital for safe operation in continuously evolving environments. These guarantees provide a rigorous framework for making decisions about data streams in CL.

Practical deployment in evolving environments also necessitates adaptive thresholding and a critical re-evaluation of OOD definitions. \cite{vishwakarma2024z1m} tackles the pervasive issue of high false positive rates in OOD detection by proposing a mathematically grounded human-in-the-loop framework that adaptively updates detection thresholds. This system ensures guaranteed False Positive Rate (FPR) control in dynamic settings by leveraging human feedback and anytime-valid confidence bounds, making OOD detectors safer and more practical for CL and AL where data distributions are constantly shifting. Furthermore, \cite{guerin202201y} offers a crucial conceptual critique, arguing that "Out-of-Model-Scope" (OMS) detection—identifying inputs that lead to actual model errors—is a more appropriate objective than mere OOD detection for safety-critical systems. This distinction is particularly relevant for continual and active learning systems, where the model's "scope" of competence is constantly evolving, making a static definition of OOD insufficient.

In conclusion, the integration of OOD detection within continual and active learning systems is moving towards more adaptive, efficient, and theoretically grounded solutions. While significant progress has been made in developing methods for identifying novel concepts, guiding data selection, and providing statistical guarantees in dynamic environments, challenges remain. Future research needs to further explore unified theoretical frameworks that seamlessly balance the plasticity-stability dilemma of continual learning with robust OOD detection, enhance real-time efficiency for complex multimodal models, and develop more sophisticated human-in-the-loop strategies that can adapt to unforeseen distribution shifts with minimal intervention, all while rigorously defining what constitutes "out-of-scope" for an evolving AI system.