\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 186 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction}

\section{Introduction}
\label{sec:introduction}

\subsection{Defining Out-of-Distribution Detection and its Importance}
\label{sec:1\_1\_defining\_out-of-distribution\_detection\_\_and\_\_its\_importance}

The reliable deployment of artificial intelligence (AI) systems in real-world, open-world environments hinges critically on their ability to recognize when input data deviates significantly from their training distribution. This challenge forms the core of Out-of-Distribution (OOD) detection, a fundamental problem in machine learning aimed at identifying inputs that a model has not been trained on, leading to potentially unreliable and overconfident predictions.

Initially, OOD data was broadly understood as any input semantically distinct from the in-distribution (ID) training data. However, research has increasingly refined this definition by disentangling various types of distribution shifts. For instance, \textcite{yang2022it3} introduced the concept of "Full-Spectrum OOD" detection, highlighting the crucial distinction between \textit{semantic shift} (entirely new classes) and \textit{covariate shift} (label-preserving changes in appearance, like lighting or style). Similarly, \textcite{lang20237w3} emphasized that in Natural Language Processing (NLP), OOD detection primarily concerns semantic shifts, where samples originate from unknown categories with different label spaces, contrasting it with domain generalization which handles non-semantic shifts. This nuanced understanding is vital, as a model might generalize well to covariate shifts (e.g., a car in different weather) but fail catastrophically on semantic shifts (e.g., an animal mistaken for a car).

Further complicating the definition, \textcite{ming2021wu7} pointed out the "vague definition of OOD" in the context of spurious correlations, where models might rely on non-causal features (e.g., background) to make predictions. This leads to "spurious OOD" samples that share these spurious features with ID data, making them particularly challenging to detect. The inherent ambiguity in defining what constitutes an OOD sample, especially when semantic content is very close to ID data, has even been likened to a "Sorites Paradox" by \textcite{long2024os1}, underscoring the lack of a clear boundary for "how different" a sample must be to be considered OOD.

A more radical re-evaluation of the OOD definition comes from works that tie OOD directly to model failure. \textcite{averly20239rv} proposed "Model-Specific Out-of-Distribution (MS-OOD) Detection," where an input is considered OOD if the deployed model is unable to predict it correctly, unifying semantic shifts, covariate shifts, and even misclassified ID examples under a single performance-based criterion. Building on this, \textcite{guerin202201y} critically argued that "Out-of-Distribution Detection Is Not All You Need," proposing "Out-of-Model-Scope (OMS) detection" as a more appropriate objective. OMS focuses unambiguously on identifying inputs that lead to \textit{prediction errors} of the specific DNN model, rather than merely inputs from a different distribution, which might still be correctly classified. This perspective highlights that OOD detection can be a misleading proxy for the true goal of ensuring model safety and trustworthiness. The challenges in defining OOD are further exacerbated by its inherent unavailability and diversity during training, making theoretical guarantees on learnability difficult \textcite{fang20249gd}.

The critical need for robust OOD detection cannot be overstated, as it directly impacts the safety, robustness, and trustworthiness of AI systems across a myriad of applications. In safety-critical domains such as autonomous driving, medical diagnosis, and cyber-physical systems (CPS), models making high-confidence but incorrect predictions on OOD inputs can have catastrophic consequences. For instance, an autonomous vehicle's perception system might misclassify a subtly perturbed stop sign as OOD and ignore it, or conversely, misclassify a truly OOD object as a stop sign, leading to severe accidents \textcite{chen2020mbk}. In medical diagnosis, misinterpreting an OOD scan as a benign ID case could lead to missed diagnoses, while in mathematical reasoning, OOD data can pose security threats and lead to unexpected performance deterioration \textcite{wang2024rej}.

The problem's practical and ethical urgency stems from the inherent "closed-world" assumption under which most deep learning models are trained, which is frequently violated in dynamic "open-world" deployments \textcite{lang20237w3}. Without effective OOD detection, models can exhibit unwarranted overconfidence, leading to large errors and compromising system safety \textcite{cai2020lsi}. Furthermore, the inability to control false positive rates in OOD detection can lead to an unacceptably high number of false alarms, which, while not as immediately catastrophic as false negatives, can erode trust and render systems impractical in human-in-the-loop scenarios \textcite{vishwakarma2024z1m}. This necessitates robust mechanisms to not only identify OOD inputs but also to provide reliable uncertainty quantification, ensuring that AI systems know "what they don't know" and can defer to human experts when necessary.

In conclusion, defining Out-of-Distribution data is far from trivial, encompassing a spectrum of distribution shifts and even extending to the very notion of model failure. The profound implications of failing to detect OOD inputs, ranging from safety hazards in autonomous systems to compromised trustworthiness in medical AI, underscore the immense practical and ethical urgency of this problem. This foundational understanding sets the stage for the subsequent exploration of diverse methodological approaches designed to tackle this multifaceted challenge.
\subsection{Evolution of OOD Research: A Historical Perspective}
\label{sec:1\_2\_evolution\_of\_ood\_research:\_a\_historical\_perspective}

The quest for robust uncertainty quantification in open-world AI systems has driven the continuous evolution of Out-of-Distribution (OOD) detection research, tracing a path from rudimentary statistical anomaly detection to sophisticated deep learning-based approaches. Before the advent of deep learning, the foundations of OOD detection were laid by classical statistical anomaly and novelty detection methods. Techniques such as One-Class Support Vector Machines (OC-SVMs) \cite{Scholkopf\_2001\_Estimating\_The\_Support\_Of\_A\_High-Dimensional\_Distribution} and Isolation Forests \cite{Liu\_2008\_Isolation\_Forest} aimed to model the boundary of in-distribution data or isolate outliers based on statistical properties. These methods, while effective for structured or lower-dimensional data, often struggled with the high-dimensionality, complexity, and intricate feature correlations inherent in real-world data like images, text, and sensor streams. The rise of deep neural networks (DNNs) in the 2010s presented a dual challenge and opportunity. While DNNs achieved unprecedented performance in discriminative tasks, they were notoriously overconfident on inputs far removed from their training distribution, making them unreliable in open-world scenarios. This critical vulnerability spurred a renewed and intensified focus on OOD detection, shifting the paradigm towards leveraging and adapting deep learning architectures themselves for uncertainty quantification.

The initial wave of deep learning-based OOD research, emerging around 2017, largely adopted a \textit{post-hoc} strategy. Researchers sought to extract OOD signals from already trained deep classifiers without modifying their internal structure or retraining. This approach prioritized efficiency and ease of integration. A seminal contribution was the observation that the Maximum Softmax Probability (MSP) of a classifier could serve as a surprisingly effective baseline for OOD detection \cite{Hendrycks\_2017\_A\_Baseline}. Samples with low MSP were deemed OOD, leveraging the classifier's inherent confidence scores. This simple yet powerful idea quickly led to refinements. Methods like ODIN \cite{Liang\_2018\_Enhancing\_The\_Reliability} enhanced MSP's discriminative power through post-processing techniques such as temperature scaling and input perturbation, demonstrating that even minor adjustments could significantly amplify OOD signals. Concurrently, the utility of the feature space was explored, with Mahalanobis distance proposed as a robust OOD score by modeling class-conditional Gaussian distributions in intermediate feature layers \cite{Lee\_2018\_A\_Simple\_Unified}. Further theoretical grounding came with energy-based models, which offered a principled way to derive OOD scores directly from logits, often with a small trainable component for calibration \cite{Liu\_2020\_Energy-based\_Out-of-Distribution}. While these early score-based methods provided efficient solutions, their fundamental limitation stemmed from their reliance on representations primarily optimized for in-distribution classification, often leading to struggles with "near OOD" examples that shared superficial similarities with known data. This highlighted the need for methods that actively learned OOD-discriminative features.

Recognizing the inherent limitations of purely post-hoc analysis, the field underwent a significant paradigm shift towards \textit{training-time strategies} designed to imbue models with intrinsic OOD awareness. A cornerstone of this era was \textbf{Outlier Exposure (OE)}, which revolutionized OOD detection by incorporating auxiliary OOD data during training \cite{hendrycks2018deep}. By explicitly exposing the model to diverse outliers, OE enabled the network to learn tighter in-distribution boundaries and push OOD samples to lower confidence regions, fundamentally altering the model's decision landscape. This concept has since evolved, with methods like Mixture Outlier Exposure (MixOE) specifically targeting fine-grained OOD by generating virtual outliers through mixing in-distribution and auxiliary data, demonstrating improved robustness in challenging scenarios \cite{zhang20212tb}.

Parallel to discriminative training with outliers, research also explored \textit{generative models} to explicitly model the in-distribution data density. Early approaches leveraged Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to estimate likelihoods, identifying OOD samples as those with low probability under the learned in-distribution model \cite{Ren\_2019\_Likelihood-based\_Deep}. However, raw likelihoods often proved unreliable, prompting advancements in more robust density estimation. Normalizing Flows, for instance, offered a powerful way to learn complex data distributions and provide more accurate likelihoods for OOD detection \cite{zisselman2020cmx}. More recently, methods like CONJNORM have provided a unified theoretical framework for tractable density estimation within the exponential family, significantly improving post-hoc OOD performance by accurately reflecting true data densities \cite{peng20243ji}.

Further deepening the understanding of internal model behavior, the analysis of feature spaces gained prominence. While Mahalanobis distance was an early exploration \cite{Lee\_2018\_A\_Simple\_Unified}, theoretical work began to demystify \textit{why} certain feature properties, like feature norms, correlate with OODness. For instance, it was shown that the feature norm can be theoretically linked to the maximum logit of a hidden classifier, providing a principled explanation for its OOD detection capability and demonstrating its class-agnostic nature \cite{park2023n97}. This theoretical grounding was complemented by insights into how in-distribution labels can fundamentally aid OOD detection, particularly for near-OOD samples, by facilitating better separation in the learned representation space \cite{du2024aea}. These developments underscored a move towards not just detecting OOD, but understanding the underlying mechanisms that enable such detection, often through robust training objectives like VOS that actively learn features crucial for OOD discrimination \cite{Wang\_2022\_VOS}.

The continuous scaling of AI models and their deployment in increasingly complex, real-world applications has ushered in a new era for OOD detection, characterized by the leveraging of powerful foundation models and the development of highly specialized and guaranteed solutions. The emergence of Vision-Language Models (VLMs) like CLIP has been particularly transformative, enabling \textit{zero-shot} and \textit{open-vocabulary} OOD detection capabilities that were previously unattainable. These models, pre-trained on vast amounts of multimodal data, possess a rich semantic understanding that can be leveraged to define OOD boundaries without explicit OOD examples during inference. Techniques such as Outlier Label Exposure (OLE) utilize Large Language Models (LLMs) to generate "envisioned" outlier class labels, effectively providing synthetic outlier exposure to VLMs and significantly boosting zero-shot OOD performance, especially for hard OOD cases \cite{ding20242m0, cao20246gj}. Surveys like \cite{miyai20247ro} now explore the evolving landscape of OOD in the VLM era, highlighting new challenges and opportunities.

Beyond VLMs, other advanced generative architectures, such as diffusion models, have also been adapted for OOD detection, particularly in domains with high in-distribution heterogeneity or structural similarity between in-distribution and OOD classes, like medical imaging \cite{mishra20236n9}. The field has also expanded its scope to address OOD challenges in diverse data modalities and specialized contexts, including pixel-level OOD in semantic segmentation, OOD detection in graph-structured data \cite{wang2024q01, cai2025ez2}, and in scenarios with complex class distributions like long-tailed recognition \cite{zhang20212tb}. This diversification underscores the field's maturation and its adaptation to the nuances of real-world data.

Crucially, as OOD detection moves into safety-critical domains like autonomous systems and medical diagnosis, there has been a growing emphasis on providing \textit{formal guarantees} on detection performance. Conformal Prediction (CP) has emerged as a powerful framework to achieve this, offering provable bounds on false positive rates. Methods integrating CP with learned nonconformity measures, such as VAEs or Deep SVDD, enable real-time OOD detection in Cyber-Physical Systems (CPS) with calibrated false alarm rates \cite{cai2020lsi, kaur20248t3}. The establishment of comprehensive benchmarks, such as UB-GOLD for graph OOD \cite{wang2024q01} and OOD-Bench \cite{Huang\_S\_2023} for general OOD, has become vital for standardized evaluation, ensuring rigorous comparison and driving progress in these complex scenarios. This trajectory reflects a continuous drive towards more robust, generalizable, and trustworthy AI systems capable of operating reliably in open-world environments.

The historical progression of OOD research thus reveals a dynamic interplay between theoretical understanding and practical necessity. It has evolved from initial heuristic post-hoc analyses of classifier outputs to sophisticated training-time strategies that actively shape model representations for OOD discrimination. The journey has seen a continuous expansion from generic image classification to specialized contexts like graph data and multimodal inputs, culminating in the integration of powerful foundation models. This evolution has been driven by a persistent intellectual tension: balancing the desire for efficient, model-agnostic solutions with the need for robust, high-performance detectors, especially for challenging "near OOD" cases. The field has consistently adapted to the increasing complexity of AI models and real-world data, moving towards more principled uncertainty quantification and, increasingly, towards methods offering formal guarantees. While significant strides have been made, the quest for universal, adaptable, and provably reliable OOD detection in highly dynamic, open-world scenarios continues to define its future trajectory, emphasizing the ongoing need for innovative solutions and comprehensive evaluation paradigms.


\label{sec:foundational_concepts,_problem_formulation,_and_theoretical_underpinnings}

\section{Foundational Concepts, Problem Formulation, and Theoretical Underpinnings}
\label{sec:foundational\_concepts,\_problem\_formulation,\_\_and\_\_theoretical\_underpinnings}

\subsection{Categorizing Distribution Shifts: Covariate, Semantic, and Spurious OOD}
\label{sec:2\_1\_categorizing\_distribution\_shifts:\_covariate,\_semantic,\_\_and\_\_spurious\_ood}

The efficacy of Out-of-Distribution (OOD) detection hinges on moving beyond a simplistic binary definition to embrace the diverse nature of real-world distribution shifts. A nuanced understanding of these categories—covariate, semantic, and spurious OOD—is paramount for developing robust detection mechanisms and designing comprehensive benchmarks.

Early work highlighted the challenge posed by spurious correlations, where models learn to rely on statistically informative but non-causal features, leading to overconfident predictions on OOD inputs. \cite{ming2021wu7} formally introduced the concept of "spurious OOD," distinguishing it from conventional OOD by defining samples that contain environmental (spurious) features but lack the invariant features essential for in-distribution (ID) labels. Their theoretical and empirical analysis demonstrated that increasing spurious correlation in training data significantly degrades OOD detection performance, particularly for spurious OOD samples, which remain challenging to detect due to the model's reliance on these non-causal cues.

Building on the recognition of diverse shifts, \cite{yang2022it3} introduced the "Full-Spectrum OOD (FS-OOD)" problem, explicitly distinguishing between semantic shift (novel classes or concepts) and covariate shift (changes in input features while the label-conditional distribution remains constant). The authors proposed the SEM score function, designed to be sensitive only to semantic shifts and robust to covariate shifts, and developed new benchmarks (DIGITS, OBJECTS, COVID) with fine-grained categorizations to evaluate methods in this more realistic setting. This work underscored that many existing OOD methods struggle to differentiate between true semantic novelty and mere appearance variations.

Further dissecting the complexities of OOD evaluation, \cite{yang2023ckx} critically analyzed existing ImageNet-based OOD benchmarks, revealing significant shortcomings such as ID contamination, semantic ambiguities, and unintended covariate shifts. To address this, they meticulously curated \texttt{ImageNet-OOD}, a clean semantic shift dataset, and demonstrated that many state-of-the-art OOD detection algorithms are disproportionately sensitive to covariate shifts rather than genuine semantic novelty. This finding challenged the perceived efficacy of complex OOD detectors, suggesting their performance gains often stemmed from exploiting covariate shifts rather than truly identifying new classes.

Complementing these data-centric definitions, \cite{averly20239rv} proposed a "Model-Specific OOD (MS-OOD)" framework, which unifies the detection of semantic shift, covariate shift, and even misclassified in-distribution examples. This framework defines OOD based on whether a \textit{deployed machine learning model} is unable to correctly predict an example, thus offering a nuanced, context-dependent definition of OOD that accounts for the model's actual performance. This perspective highlights that the "OOD" status of a sample can be dynamic and model-dependent, rather than solely an intrinsic property of the data. The practical relevance of these distinctions is further emphasized by \cite{hong2024xls}, which, in a comprehensive survey on OOD detection in medical image analysis, explicitly categorizes distributional shifts into semantic, covariate, and contextual shifts, providing a structured framework for understanding anomalies in high-stakes clinical settings.

The evolution of OOD understanding also extends to more granular evaluations. \cite{long2024os1} introduced the "Sorites Paradox" in OOD evaluation, arguing that current benchmarks, by relying on binary semantic labels, fail to capture the continuous "degree" of shift. They proposed the "Incremental Shift OOD (IS-OOD)" benchmark and the Language Aligned Image feature Decomposition (LAID) method, which leverages CLIP to decompose image features into distinct semantic and covariate components. This allows for evaluating OOD detection methods across incremental levels of both semantic and covariate shifts, providing a more nuanced understanding of their sensitivities. Similarly, \cite{wang2024is1} provided a critical analysis of OOD detection and Open-Set Recognition (OSR), proposing a conceptual framework to disentangle semantic and covariate shifts and introducing a new large-scale benchmark for this purpose. Their findings revealed that magnitude-aware scoring rules consistently show promise across tasks, often due to the lower feature magnitude of "unfamiliar" examples, while methods like Outlier Exposure struggle to scale effectively to large datasets due to the inherent difficulty in representing diverse shifts.

In conclusion, the field has moved significantly beyond a simplistic binary view of OOD, recognizing the critical need to differentiate between covariate, semantic, and spurious shifts. Works such as \cite{ming2021wu7}, \cite{yang2022it3}, and \cite{yang2023ckx} have been instrumental in defining and disentangling these categories, while \cite{averly20239rv} and \cite{hong2024xls} have provided unifying frameworks and domain-specific applications. The development of more sophisticated benchmarks by \cite{long2024os1} and \cite{wang2024is1} that account for continuous and disentangled shifts is crucial. Unresolved challenges remain in developing methods that are truly robust to covariate shifts while being highly sensitive to semantic novelty, and in creating scalable benchmarks that accurately reflect the full spectrum of real-world OOD scenarios.
\subsection{Evaluation Metrics and Basic Benchmarking Challenges}
\label{sec:2\_2\_evaluation\_metrics\_\_and\_\_basic\_benchmarking\_challenges}

The reliable deployment of machine learning models hinges on their ability to detect out-of-distribution (OOD) inputs, yet the methodologies for evaluating these algorithms and the inherent challenges in creating comprehensive benchmarks remain significant hurdles. Standard evaluation metrics, such as Area Under the Receiver Operating Characteristic curve (AUROC), Area Under the Precision-Recall curve (AUPR), and False Positive Rate at 95\\% True Positive Rate (FPR@95TPR), are widely adopted due to their threshold-independent nature. However, these metrics often provide a binary assessment of OOD detection, failing to capture the nuanced spectrum of distribution shifts encountered in real-world scenarios \cite{long2024os1}.

A primary limitation of existing evaluation paradigms is their oversimplified definition of OOD, which often conflates different types of distribution shifts. \cite{yang2022it3} critically highlights this by introducing the Full-Spectrum OOD (FS-OOD) problem, which explicitly distinguishes between semantic shift (novel classes) and covariate shift (label-preserving appearance changes). Their work demonstrates that many state-of-the-art OOD methods struggle to differentiate these shifts, often misclassifying covariate-shifted in-distribution (ID) data as OOD or vice-versa. Building on this, \cite{yang2023ckx} exposes severe shortcomings in popular ImageNet-based OOD benchmarks, identifying issues like ID contamination, semantic ambiguities, and unintended covariate shifts. Their meticulously curated ImageNet-OOD dataset reveals that modern OOD algorithms are disproportionately sensitive to covariate shifts rather than genuine semantic novelty, showing minimal improvement over simple baselines for new-class detection when covariate shifts are minimized. This observation is further corroborated by \cite{wang2024is1}, which provides a critical dissection of OOD detection and Open-Set Recognition (OSR). Their proposed large-scale benchmark systematically disentangles semantic and covariate shifts, confirming that many methods' reported performance gains are often attributable to their sensitivity to covariate changes rather than true OOD detection capabilities.

The difficulty of curating diverse and representative OOD datasets is a persistent challenge. The "Sorites Paradox" in OOD evaluation, as articulated by \cite{long2024os1}, underscores the ambiguity in defining "how different" an OOD sample must be, leading to subjective and labor-intensive manual filtering in benchmarks. To address this, \cite{long2024os1} introduces the Incremental Shift OOD (IS-OOD) benchmark, which categorizes OOD samples by continuous semantic and covariate shift degrees, providing a more granular and semantically informed evaluation. Beyond semantic and covariate shifts, \cite{ming2021wu7} sheds light on the critical impact of spurious correlations, formalizing "spurious OOD" where models rely on non-causal features. Their findings demonstrate that increasing spurious correlation in training data severely degrades OOD detection performance, a factor often overlooked in standard benchmarks.

The impact of dataset choice on reported performance is profound and often domain-specific. In medical imaging, for instance, \cite{berger20214a3} conducts a comparative study revealing that high OOD detection performance on general computer vision benchmarks does not translate to medical tasks, which present unique challenges. \cite{vasiliuk20233w9} further highlights limitations in 3D medical image segmentation, designing a new public benchmark and showing that even a simple Intensity Histogram Features (IHF) baseline can outperform complex deep learning methods for certain OOD shifts. \cite{hong2024xls} provides a comprehensive survey for medical image analysis, proposing a systematic taxonomy of distributional shifts (semantic, covariate, contextual) to guide more targeted evaluation. Similarly, in graph-structured data, \cite{liu202227x} pioneers unsupervised graph-level OOD detection and constructs a comprehensive benchmark, while \cite{wang2024q01} unifies unsupervised graph-level anomaly detection and OOD detection under a single benchmark (UB-GOLD) to enable cross-comparison across previously disparate evaluation setups. For generative language models, \cite{wang2024rej} identifies "pattern collapse" in mathematical reasoning as a unique challenge, making traditional static embedding-based OOD detection ineffective.

A recurring challenge across domains is the scarcity of real OOD data for training, which limits the effectiveness of methods like Outlier Exposure (OE). \cite{wang2025xwm} addresses this for graph OOD by proposing an implicit adversarial latent generation method to synthesize pseudo-OOD samples. In the multi-modal domain, \cite{dai2023mhn} and \cite{cao20246gj} leverage Large Language Models (LLMs) to "envision" or generate synthetic outlier exposure, overcoming the practical unavailability of true OOD labels, though \cite{dai2023mhn} stresses the need for uncertainty calibration to mitigate LLM hallucinations. The theoretical underpinnings of OOD detection also highlight fundamental limitations: \cite{fang20249gd} provides a PAC learning theory for OOD detection, demonstrating that it is not universally learnable under all conditions, which implies that a single benchmark or metric cannot capture all aspects of performance. Furthermore, \cite{du2024aea} theoretically explores when and how in-distribution labels help OOD detection, showing their benefit is more pronounced for "near OOD" scenarios.

The persistent need for unified benchmarks to enable fair and reproducible comparisons is evident in the continuous efforts to create more robust evaluation frameworks. From the FS-OOD benchmarks \cite{yang2022it3} and ImageNet-OOD \cite{yang2023ckx} for disentangling shifts, to the Model-Specific OOD (MS-OOD) framework by \cite{averly20239rv} which defines OOD based on a model's actual misclassification, researchers are striving for more realistic and comprehensive evaluations. The introduction of benchmarks specifically designed for foundation models \cite{borlino20245ku} also addresses the saturation of existing testbeds by increasingly powerful models. Despite these advancements, the field still grapples with the inherent complexity of defining and measuring "out-of-distribution" in a manner that is both theoretically sound and practically relevant, necessitating continued innovation in both metrics and benchmark design to ensure meaningful progress.
\subsection{Theoretical Foundations and Learnability of OOD Detection}
\label{sec:2\_3\_theoretical\_foundations\_\_and\_\_learnability\_of\_ood\_detection}

The development of robust and trustworthy Out-of-Distribution (OOD) detection systems necessitates a deep theoretical understanding of its learnability and the fundamental conditions under which it can be provably effective. Moving beyond purely empirical observations, this subsection delves into the mathematical and conceptual underpinnings that govern OOD phenomena, providing a principled framework for understanding existing methods and guiding the creation of more resilient algorithms.

A foundational inquiry in this domain concerns the very learnability of OOD detection. \cite{fang20249gd} provides a comprehensive theoretical framework, investigating the Probably Approximately Correct (PAC) learnability of OOD detection. Their work establishes necessary and sufficient conditions for learnability under various risk and AUC metrics, crucially revealing impossibility theorems. These theorems demonstrate that OOD detection is not universally learnable, particularly when in-distribution (ID) and OOD data exhibit significant overlap or when the hypothesis space is too rich. This theoretical grounding highlights the inherent challenges in designing a single, universally effective OOD detector, suggesting that learnability is highly contingent on the characteristics of the data distributions and the chosen model class. Complementing this, \cite{guerin202201y} critically questions whether OOD detection, as conventionally defined by distributional shifts, is the most appropriate objective for safety-critical systems. They propose "Out-of-Model-Scope (OMS) detection" as a more unambiguous goal, focusing on identifying inputs that lead to model errors rather than merely distributional shifts. This redefinition directly impacts the theoretical target of learnability, shifting the focus from statistical divergence to predictive reliability.

The intrinsic characteristics of learned representations and the nature of training data profoundly impact OOD learnability and the reliability of detection signals. \cite{ming2021wu7} provides a rigorous analysis of how spurious correlations in training data detrimentally affect OOD detection. They formalize "spurious OOD" and demonstrate that models relying on non-causal features can exhibit arbitrarily high confidence on such OOD inputs, making them inherently difficult to detect. This theoretical insight explains why models often fail catastrophically on OOD data that shares superficial similarities with ID data, fundamentally limiting learnability in these scenarios. Furthermore, the continuous nature of OOD boundaries, as highlighted by the "Sorites Paradox" in OOD evaluation \cite{long2024os1}, poses a fundamental theoretical challenge to current binary classification-based OOD approaches. This suggests that the crisp binary decision of ID/OOD might be an oversimplification, complicating the theoretical guarantees of any hard-thresholding method.

To understand why certain OOD signals \textit{do} work despite these challenges, theoretical explanations for empirically observed phenomena are crucial. \cite{park2023n97} offers a theoretical explanation for the efficacy of feature norms, proving that the L1-norm of a hidden layer feature vector converges to the maximum logit of a hidden classifier. This links feature norm to classifier confidence and establishes its class-agnostic nature for OOD detection, providing a mathematical basis for a widely used heuristic. Extending this, \cite{ammar2023pr1} introduces and empirically validates the "ID/OOD Orthogonality (NC5)" property of Neural Collapse, showing that OOD data tends to become orthogonal to ID data's class means during training. This provides a geometric explanation for feature-based OOD separation, suggesting that deep networks naturally learn to push OOD samples into distinct subspaces. Beyond feature norms, \cite{gomes2022zyv} introduces IGEOOD, leveraging information geometry, specifically the Fisher-Rao distance, to measure dissimilarity between probability distributions in both softmax output and latent feature spaces. This provides a unified, theoretically grounded metric that adapts to various levels of model access, offering a robust alternative to simpler distance metrics. Similarly, \cite{chen2023za1} proposes GAIA, which quantifies "abnormality" in gradient-based attribution results, providing a novel theoretical perspective that OOD samples elicit divergent and meaningless explanation patterns, which can be leveraged as a powerful OOD signal.

The ability to leverage auxiliary data sources also plays a crucial role in enhancing OOD learnability, with recent work providing formal guarantees. \cite{du20248xe} makes a significant theoretical contribution by providing the first framework with \textit{provable guarantees} for using unlabeled "wild" data in OOD detection. Their SAL (Separate And Learn) framework employs a novel gradient-based filtering mechanism to identify candidate outliers from noisy unlabeled data, and their theorems quantify the separability and learnability of the OOD classifier, demonstrating how unlabeled data can provably help in specific settings. Complementing this, \cite{du2024aea} formally investigates the role of in-distribution (ID) labels, using a graph-theoretic approach to derive a provable error bound that compares OOD detection performance with and without ID labels. Their work identifies specific conditions under which ID labels are most beneficial, particularly for "near-OOD" scenarios where semantic information is critical.

Beyond traditional likelihoods, generative models offer new theoretical avenues for OOD detection. \cite{morningstar2020re9} addresses the critical failure of direct likelihood comparison in high dimensions, where deep generative models often assign higher likelihoods to OOD data. Their Density of States Estimator (DoSE) shifts the focus from raw likelihoods to the "typicality" of multiple summary statistics derived from the generative model, inspired by statistical physics. This approach provides a theoretical understanding of the shortcomings of MLE-fitted distributions for single-sample OOD detection through a bias/variance tradeoff for typicality. Building on generative models, \cite{heng2024fjd} introduces Diffusion Paths (DiffPath), a novel approach that leverages a \textit{single} unconditional diffusion model. Instead of relying on likelihoods, DiffPath measures the rate-of-change and curvature of diffusion paths, connecting a sample to a standard normal distribution. This work provides a theoretical framework linking these OOD statistics to derivatives of optimal transport paths, demonstrating that a single generative model can detect OOD across diverse, unseen inlier distributions, a significant theoretical and practical advancement.

In conclusion, the theoretical foundations of OOD detection are rapidly evolving, moving from an initial focus on empirical performance to a deeper, principled understanding of its learnability and the factors that influence it. While foundational work highlights the inherent impossibility of universal OOD learnability \cite{fang20249gd}, subsequent research has carved out specific conditions where provable success is possible, whether through understanding intrinsic model behaviors \cite{park2023n97, ammar2023pr1, gomes2022zyv, chen2023za1} or leveraging auxiliary data with theoretical backing \cite{du20248xe, du2024aea}. Challenges remain in achieving universal learnability across all OOD types, particularly in the presence of spurious correlations \cite{ming2021wu7} and the ambiguity of OOD boundaries \cite{long2024os1}. However, the increasing integration of rigorous statistical frameworks, such as Conformal Prediction, offers a promising direction for providing formal guarantees on OOD detection performance, a topic explored in detail in Section 7.1 \cite{novello2024yco}. This ongoing theoretical grounding is vital for ensuring scientific rigor and guiding the development of truly robust and provably safe OOD systems in complex, dynamic real-world environments.


\label{sec:early_approaches:_post-hoc_score-based_methods}

\section{Early Approaches: Post-hoc Score-Based Methods}
\label{sec:early\_approaches:\_post-hoc\_score-based\_methods}

\subsection{Maximum Softmax Probability (MSP) and its Enhancements}
\label{sec:3\_1\_maximum\_softmax\_probability\_(msp)\_\_and\_\_its\_enhancements}

The foundational challenge of Out-of-Distribution (OOD) detection, critical for the safe deployment of machine learning models, found a pioneering and surprisingly effective baseline in the Maximum Softmax Probability (MSP) \cite{Hendrycks\_G\_2017}. This straightforward approach leverages the output of a pre-trained neural network classifier, positing that in-distribution (ID) samples, which align with the model's training data, should elicit high confidence in one of the known classes. Conversely, OOD samples, being unfamiliar, are expected to yield lower confidence scores. \cite{Hendrycks\_G\_2017} empirically demonstrated MSP's utility as a simple yet powerful OOD indicator, establishing it as a widely adopted benchmark for subsequent, more complex detection methods.

Despite its conceptual simplicity and computational efficiency, MSP suffers from inherent limitations. Deep neural networks are frequently observed to exhibit overconfidence, assigning high softmax probabilities even to OOD inputs that lie far from the training data manifold \cite{Ren\_J\_2019}. This phenomenon, where models confidently misclassify novel inputs, underscores MSP's inadequacy as a standalone OOD score in many real-world scenarios. Furthermore, while MSP provides a basic measure of predictive uncertainty, it often fails to capture the nuanced epistemic uncertainty associated with truly novel inputs, leading to its performance being reliably outperformed by methods leveraging learned representations \cite{kuan2022qzl}. This critical observation highlighted the need for enhancements that could amplify the discriminative power of MSP.

A significant advancement in this direction was the introduction of Out-of-Distribution Detector for Neural Networks (ODIN) by \cite{Liang\_L\_2018}. ODIN enhances MSP through two key post-processing techniques: temperature scaling and input perturbation. Temperature scaling involves dividing the logits (pre-softmax outputs) by a scalar parameter $T > 1$. This process softens the probability distribution, making the model less overconfident and improving the calibration of confidence scores. Input perturbation, on the other hand, is a gradient-based method where the input image is slightly perturbed in a direction that maximizes the softmax probability of the predicted class. Specifically, it applies a small adversarial perturbation (e.g., using a single-step gradient ascent similar to Fast Gradient Sign Method, FGSM) to push the input closer to the decision boundary of its predicted class. For ID samples, this perturbation typically increases the maximum softmax probability, while for OOD samples, it often leads to a less pronounced increase or even a decrease, effectively exaggerating the difference between ID and OOD samples in the confidence space. Empirical studies have shown that ODIN's effectiveness is primarily attributed to this input perturbation mechanism, which significantly enhances the separation of ID and OOD samples in the feature space \cite{berger20214a3}. These techniques, applied post-training without requiring model retraining, transformed MSP into a more robust OOD score, demonstrating substantial performance improvements across various benchmarks \cite{Liang\_L\_2018}.

Building upon ODIN, \cite{Hsu\_Y\_2020} proposed Generalized ODIN, which further refines the input perturbation strategy by leveraging batch normalization statistics. This approach enhances the discriminative power without requiring access to OOD data during the enhancement process, making it more practical in scenarios where OOD examples are scarce or unknown. The core principle behind these post-processing methods is their ability to manipulate the model's output space, making the confidence scores of ID samples even higher and those of OOD samples even lower, thereby creating a clearer separation for OOD detection.

Despite their empirical success, ODIN and its variants introduce practical challenges. A major limitation is the requirement to tune the hyperparameters, specifically the temperature $T$ and the perturbation magnitude $\epsilon$, which typically necessitates a validation set of OOD samples \cite{Liang\_L\_2018}. This contradicts the purely post-hoc, no-OOD-data-needed appeal of the method, as representative OOD validation data is often unavailable in real-world open-set scenarios. Furthermore, the stability of OOD detection performance, even for methods like MSP and ODIN, can fluctuate significantly during neural network training, exhibiting overfit-like behavior and instability in later stages \cite{cheng20233yi}. To address this, methods like Average of Pruning (AoP) have been proposed, which combine model averaging and network pruning to improve the stability and performance of OOD detection scores, including those derived from MSP and ODIN, by mitigating overfitting and finding more robust optima \cite{cheng20233yi}.

Further refinements have sought to improve the core components of ODIN. For instance, Adaptive Temperature Scaling (ATS) dynamically calculates a sample-specific temperature value based on activations of intermediate layers, fusing this adjustment with class-dependent logits to capture additional statistical information before it is lost in the feature extraction process \cite{krumpl2024n1w}. This approach enhances the robustness and power of temperature scaling. Similarly, the principles of temperature-scaled softmax probabilities and FGSM-style input perturbation have been integrated into more principled frameworks, such as Information Geometry approaches, which leverage Fisher-Rao distance to measure dissimilarity between probability distributions for OOD scoring \cite{gomes2022zyv}. These methods demonstrate how the fundamental ideas introduced by MSP and ODIN continue to inspire more theoretically grounded and robust OOD detection techniques.

The enduring legacy of MSP and its subsequent enhancements is profound. They established a strong, efficient, and widely adopted baseline that laid the groundwork for the entire category of post-hoc OOD detection methods. These techniques demonstrated that significant improvements could be achieved through clever manipulation of a pre-trained model's outputs, inspiring further research into how model outputs and internal representations could be better leveraged. However, their reliance on heuristic adjustments and the need for OOD validation data for hyperparameter tuning highlighted the need for more principled and data-agnostic scoring functions. This paved the way for the exploration of alternative post-hoc methods, such as energy-based scores, which offer a more theoretically grounded approach to OOD detection, as discussed in the following subsection.
\subsection{Energy-Based Models for OOD Scoring}
\label{sec:3\_2\_energy-based\_models\_for\_ood\_scoring}

Traditional methods for out-of-distribution (OOD) detection often rely on confidence scores derived from standard discriminative classifiers, such as the maximum softmax probability (MSP) \cite{Hendrycks\_G\_2017}. While empirically effective and further enhanced by post-hoc techniques like temperature scaling and input perturbation in methods like ODIN \cite{Liang\_L\_2018}, these approaches frequently lack a strong theoretical foundation for quantifying uncertainty and can be sensitive to heuristic adjustments. This limitation has spurred research into more principled alternatives, with Energy-Based Models (EBMs) emerging as a robust framework for OOD scoring.

Energy-Based Models offer a principled approach to modeling data distributions by assigning an 'energy' score to each input, where lower energy corresponds to in-distribution (ID) samples and higher energy to OOD samples. Theoretically, EBMs define a probability distribution over inputs $x$ using an energy function $E(x; \theta)$ as $p(x) = \frac{\exp(-E(x; \theta))}{Z}$, where $Z$ is the intractable normalization constant (partition function). This formulation implies that inputs with low energy are assigned high probability, aligning naturally with the goal of OOD detection: ID samples should reside in low-energy regions, while OOD samples should be pushed to high-energy regions.

A seminal work in this area is \cite{Liu\_Y\_2020}, which introduced a direct application of EBMs for OOD detection. This approach leverages the logits of a pre-trained neural network classifier to define the energy function, typically as the negative log-sum-exp of the logits. Crucially, \cite{Liu\_Y\_2020} proposed a novel training objective that explicitly fine-tunes the model to optimize OOD discrimination. This objective combines the standard cross-entropy loss for ID classification with a contrastive term that pushes the energy of ID samples below a certain margin and the energy of OOD samples (drawn from an auxiliary OOD dataset) above another margin. By directly optimizing this energy function, the model learns to assign distinct energy values to ID and OOD data, providing a more calibrated and robust measure of uncertainty than raw softmax probabilities.

The strength of EBMs for OOD scoring lies in their ability to explicitly shape the energy landscape during training. Unlike methods that merely interpret or adjust existing classifier outputs, EBMs are trained or fine-tuned to directly learn an energy function that is discriminative for OOD samples. This direct optimization for OOD detection represents a significant step towards more theoretically grounded post-hoc scoring, moving beyond heuristic adjustments. The resulting energy scores offer a more robust and calibrated measure of uncertainty, as they are not merely byproducts of a classification task but are learned specifically to reflect the distributional anomaly of an input.

While EBMs provide a powerful framework, their effectiveness can depend on the quality and diversity of the auxiliary OOD dataset used during training to shape the energy landscape. Furthermore, the computational cost associated with training EBMs, particularly those involving sampling from the energy function, can be a consideration. Nevertheless, the principled nature of energy-based scoring offers a compelling direction for developing more reliable and trustworthy AI systems, paving the way for future research into more efficient training strategies and broader applications across diverse OOD scenarios.
\subsection{Mahalanobis Distance and Feature Norm Analysis}
\label{sec:3\_3\_mahalanobis\_distance\_\_and\_\_feature\_norm\_analysis}

Deep neural networks often make overconfident predictions on out-of-distribution (OOD) inputs, necessitating robust detection mechanisms. A promising avenue for OOD detection leverages the internal feature space of pre-trained neural networks, offering a geometric perspective distinct from output probabilities. This approach analyzes how OOD samples deviate in learned feature spaces, either through distance metrics, feature magnitudes, or more sophisticated density estimations.

The Mahalanobis distance is a foundational method in this domain, measuring the distance of a test sample's features from class-conditional Gaussian distributions learned for in-distribution (ID) data. This method is statistically principled, assuming that ID features for each class can be modeled by a Gaussian distribution, and OOD samples will lie far from these learned centroids in the feature space. However, its efficacy is critically dependent on the validity of this Gaussian assumption and the separability of ID/OOD clusters. A comprehensive comparative study by \cite{berger20214a3} rigorously evaluated several confidence-based OOD methods, including Mahalanobis distance, finding that while it performed strongly on standard computer vision tasks like CIFAR-10, its performance significantly degraded in medical imaging. This degradation was attributed to less separable ID/OOD clusters in the feature space that challenge the underlying Gaussian assumption, highlighting a key limitation. Furthermore, in high-dimensional feature spaces, distance-based methods, including Mahalanobis, can suffer from the "curse of dimensionality," where distances become less meaningful and discriminative, potentially leading to unreliable OOD scores \cite{ghosal2023q20, li2025jdt}.

Addressing these limitations, subsequent research has sought to refine or extend the Mahalanobis approach. \cite{anthony2023slf} conducted a deep investigation into Mahalanobis distance for medical imaging, empirically demonstrating that the optimal network depth for OOD detection is highly dependent on the specific OOD pattern. They proposed Multi-branch Mahalanobis (MBM), a novel system that employs multiple depth-specific detectors, significantly enhancing robustness by leveraging feature representations from various layers. This implicitly acknowledges that a single Gaussian model might not capture the complex, evolving feature distributions across different network depths. Extending the idea of distance in feature space while relaxing strict covariance assumptions, \cite{gomes2022zyv} introduced IGEOOD. This method employs Information Geometry, specifically the Fisher-Rao distance, to measure dissimilarity between probability distributions of latent features. By modeling these features as a mixture of Gaussian PDFs with diagonal covariance, IGEOOD provides a more flexible and robust OOD score, outperforming traditional Mahalanobis-based methods by moving beyond the restrictive full covariance matrix assumption. To directly combat the curse of dimensionality, \cite{ghosal2023q20} proposed Subspace Nearest Neighbor (SNN), a framework that regularizes the model and its feature representation by leveraging the most relevant subset of dimensions (i.e., a subspace). This subspace learning yields more distinguishable distance measures between ID and OOD data, offering a critical improvement for distance-based methods in high-dimensional settings. Similarly, \cite{li2025jdt} introduced a novel "tangent distance" that mitigates the sensitivity of distances to high dimensionality by mapping features to the manifold of ID samples. This approach, inspired by manifold learning, computes the Euclidean distance between samples and the nearest submanifold space, providing a data structure-aware distance that is more meaningful than direct Euclidean or Mahalanobis distances in high-dimensional feature spaces.

Beyond explicit distance metrics, the magnitude or "norm" of feature representations provides another valuable, computationally efficient signal for OOD detection. The core observation is that OOD samples often exhibit different feature magnitudes compared to in-distribution data. \cite{yu2022egq} introduced \texttt{FeatureNorm}, defined as the channel-wise averaged L2-norm of rectified feature maps. Their work revealed that intermediate blocks, rather than just the last layer, often provide superior OOD separation, attributing this to the "overconfidence issue" in deeper layers where features might become less discriminative for OOD. To optimize this, they proposed \texttt{NormRatio} which uses pseudo-OOD samples for optimal block selection. Complementing this, \cite{dong2021swz} developed Neural Mean Discrepancy (NMD), which quantifies the deviation of an input's average activations (neural mean) from the training data's neural mean across multiple layers. NMD leverages Batch Normalization statistics for efficiency and demonstrates that OOD samples consistently exhibit larger deviations, even with small batch sizes, making it a robust norm-based indicator. These methods highlight that simple magnitude differences in feature space can serve as effective, yet computationally light, OOD indicators, particularly when carefully chosen from appropriate network layers.

While Mahalanobis distance implicitly relies on Gaussian density estimation, more explicit and flexible density estimation techniques in the feature space offer a powerful alternative, overcoming the restrictive assumptions of simple parametric models. Normalizing Flows (NFs) have emerged as a promising tool for this purpose. \cite{cook2024hyb} investigated the use of feature density estimation via normalizing flows for OOD detection, presenting a fully unsupervised, post-hoc approach. By training a lightweight auxiliary NF model on the features of in-distribution data, they perform OOD detection via density thresholding. This method demonstrates strong results for far-OOD data, showcasing the ability of NFs to model complex, non-Gaussian feature distributions more accurately than simple Gaussian assumptions, thus providing a more robust measure of typicality.

Further advancing tractable density estimation, \cite{peng20243ji} introduced CONJNORM, a novel theoretical framework grounded in Bregman divergence that unifies various post-hoc OOD approaches by considering an expansive exponential family of distributions. CONJNORM reframes density function design as a search for the optimal norm coefficient for a given dataset, and crucially, devises an unbiased and analytically tractable estimator for the partition function using Monte Carlo-based importance sampling. This overcomes the computational intractability and reliance on strong distributional assumptions (like Gaussian) that plague many density-based methods, including the implicit Gaussian assumption of Mahalanobis distance.

Another approach to typicality, moving beyond direct likelihoods, is the Density of States Estimator (DoSE) proposed by \cite{morningstar2020re9}. DoSE addresses the critical issue where raw likelihoods from generative models can assign higher probabilities to OOD data in high dimensions. Instead of comparing model probabilities directly, DoSE leverages the "probability of the model probability" or, more generally, the frequency (typicality) of \textit{multiple summary statistics} derived from a generative model's features. By training non-parametric density estimators (e.g., Kernel Density Estimation or one-class Support Vector Machine) on these statistics, DoSE identifies OOD samples as those with low typicality across these combined metrics. This modular, unsupervised method provides a robust alternative to direct likelihood or distance comparisons by focusing on the multi-faceted typicality of samples in the feature space, making it highly effective even when simple likelihoods fail.

The methods leveraging feature space for OOD detection present a spectrum of approaches, each with distinct strengths and weaknesses. Mahalanobis distance, while statistically grounded, is highly sensitive to its core assumption of class-conditional Gaussian feature distributions and can be hampered by the curse of dimensionality in high-dimensional, complex feature spaces \cite{berger20214a3, ghosal2023q20}. Its performance can degrade significantly when ID and OOD clusters are not well-separated or when the true data distribution deviates from Gaussianity. Improvements like MBM \cite{anthony2023slf} and IGEOOD \cite{gomes2022zyv} directly tackle these issues by adapting to network depth or relaxing strict covariance assumptions, respectively, thereby enhancing the flexibility of distance-based metrics. Furthermore, SNN \cite{ghosal2023q20} and tangent distance \cite{li2025jdt} offer crucial advancements by explicitly addressing the curse of dimensionality, making distance metrics more robust and meaningful by operating in learned subspaces or manifolds.

Feature norm-based methods, such as \texttt{FeatureNorm} \cite{yu2022egq} and NMD \cite{dong2021swz}, offer a computationally lighter alternative. They exploit the observation that OOD samples often manifest as deviations in feature magnitudes, providing an efficient signal. However, their simplicity means they might miss more subtle OOD patterns that do not primarily manifest as magnitude changes. They also require careful selection of the optimal layer or block for feature extraction, as deeper layers can sometimes become less discriminative due to overconfidence.

More advanced density estimation techniques, including Normalizing Flows \cite{cook2024hyb}, CONJNORM \cite{peng20243ji}, and DoSE \cite{morningstar2020re9}, represent a significant leap. They move beyond the restrictive parametric assumptions of Mahalanobis by modeling complex, non-Gaussian feature distributions more accurately. Normalizing Flows offer a flexible, unsupervised way to learn the true ID feature density, while CONJNORM provides a principled, tractable framework for density estimation across a broader family of distributions, overcoming the challenges of partition function estimation. DoSE, by focusing on the typicality of multiple summary statistics, robustly addresses the counter-intuitive high-likelihood-for-OOD problem that can plague direct likelihood methods. These advanced density-based approaches generally offer superior performance and robustness compared to simpler distance or norm-based methods, but often come with increased computational complexity for training the auxiliary density models. The choice between these methods often involves a trade-off between computational efficiency, model flexibility, and the specific characteristics of the OOD problem (e.g., near-OOD vs. far-OOD).

The literature on Mahalanobis distance and feature norm analysis, alongside more advanced feature-space density estimation, highlights a continuous effort to exploit the geometric and statistical properties of learned feature representations for robust OOD detection. The evolution from the foundational Mahalanobis distance, with its inherent Gaussian assumptions and susceptibility to high dimensionality, to more flexible distance metrics, subspace learning, and sophisticated density estimation techniques, demonstrates a clear progression towards more accurate and robust uncertainty quantification. A persistent challenge remains in developing methods that are universally robust across diverse OOD types and domains, particularly in safety-critical applications where the assumptions of feature distribution or separability may not hold, or where the "curse of dimensionality" continues to obscure meaningful distinctions. Future directions could involve integrating these geometric and density-based insights with more adaptive learning paradigms, exploring novel feature transformations that inherently maximize ID-OOD separation during training, or developing hybrid approaches that combine the efficiency of norm-based methods with the robustness of advanced density estimators. Furthermore, the rigorous evaluation of these scores using frameworks like conformal prediction, as highlighted by \cite{novello2024yco}, will be crucial to provide provable guarantees on their performance, moving beyond empirical observations to verifiable assurances for real-world deployment.


\label{sec:training-time_strategies_and_generative_models}

\section{Training-Time Strategies and Generative Models}
\label{sec:training-time\_strategies\_\_and\_\_generative\_models}

\subsection{Outlier Exposure and Synthetic OOD Generation}
\label{sec:4\_1\_outlier\_exposure\_\_and\_\_synthetic\_ood\_generation}

Out-of-Distribution (OOD) detection aims to equip models with the crucial ability to identify inputs that deviate from their training distribution, a necessity for reliable deployment in open-world scenarios. While purely post-hoc methods analyze a trained model's outputs, Outlier Exposure (OE) represents a powerful training strategy that explicitly teaches models to distinguish in-distribution (ID) from OOD samples by exposing them to auxiliary OOD data during training. This paradigm is often complemented by methods that synthesize diverse and informative OOD examples, thereby enhancing the model's capacity to form tighter ID boundaries and improve OOD discrimination.

The theoretical underpinnings of OE reveal that many such methods asymptotically converge to a binary discriminator between in-distribution and the auxiliary OOD data \cite{bitterwolf2022rw0}. This insight underscores the critical importance of the quality and diversity of the auxiliary OOD data. Building on this, early approaches to synthetic OOD generation focused on mixing strategies. \textcite{zhang20212tb} introduced Mixture Outlier Exposure (MixOE), which generates "virtual" outliers by mixing ID data with auxiliary OOD samples. This technique effectively expands the coverage of the feature space, enabling more robust OOD detection, particularly in challenging fine-grained environments where OOD samples are semantically close to ID data. Similarly, \textcite{yang2023pre} proposed MixOOD, an enhanced Mixup-based strategy that generates augmented images from ID data to serve as auxiliary OOD, demonstrating its plug-and-play capability with various threshold-based OOD detection methods. To bolster robustness against adversarial attacks, \textcite{chen2020mbk} developed Adversarial Learning with inlier and Outlier Exposure (ALOE), which generates adversarial outliers by maximizing KL-divergence to a uniform distribution, thus explicitly training the model to reject perturbed OOD inputs. Furthermore, \textcite{yu2022egq} utilized Jigsaw puzzles of ID images to create "pseudo OOD" samples, which are then used to select optimal intermediate feature blocks for OOD detection, circumventing the need for real OOD data during development.

The challenge of limited diversity and representativeness in auxiliary OOD datasets led to more sophisticated sampling and generation strategies. \textcite{jiang2023vzb} introduced Diverse Outlier Sampling (DOS), which addresses the bias of uncertainty-based sampling by clustering normalized latent features of auxiliary OOD data and actively selecting the most informative outliers from each cluster. This ensures a broader representation of the OOD space, leading to more globally compact decision boundaries. Extending this, \textcite{yao2024epq} presented \texttt{diverseMix}, a theoretically-guaranteed mixup strategy that dynamically adjusts interpolation to generate novel and distinct auxiliary outliers, with formal proofs demonstrating its ability to enhance diversity and reduce generalization error. Addressing the often-overlooked issue of class imbalance within auxiliary OOD data, \textcite{choi202367m} proposed a balanced energy regularization loss. This loss adaptively applies stronger regularization to auxiliary samples from majority classes, leading to improved OOD detection in scenarios like semantic segmentation and long-tailed classification. In a different vein, \textcite{nie2024ghv} introduced Virtual Outlier Smoothing (VOSo), which constructs virtual outliers by perturbing semantic regions of ID samples in the image space, assigning them dynamic soft labels based on perturbation extent. This approach creates a smoother transition in decision boundaries, enhancing uncertainty estimation without relying on external OOD datasets. \textcite{hofmann2024gnx} further refined outlier sampling with Hopfield Boosting, leveraging a novel energy function derived from Modern Hopfield Networks to adaptively sample "hard" outliers that lie close to the ID-OOD decision boundary.

Beyond general classification, synthetic OOD generation has been tailored for specialized contexts. In semantic segmentation, \textcite{besnier2021jgn} developed ObsNet+LAA, which generates OOD-like training data through local adversarial attacks on in-distribution images. This creates synthetic failure modes to train an auxiliary OOD detector, achieving both speed and accuracy. For long-tailed recognition, where distinguishing tail classes from OOD is difficult, \textcite{wei2023f15} proposed Context-rich Tail Class Augmentation, overlaying tail-class images onto OOD data to improve generalization. Similarly, \textcite{miao2023brn} introduced Calibrated Outlier Class Learning (COCL), which leverages auxiliary OOD data within a debiased large margin learning framework to better separate OOD from both head and tail ID classes. The advent of generative models, particularly diffusion models, has also opened new avenues. \textcite{gao2023kmk} presented DiffGuard, which uses pre-trained diffusion models to synthesize images conditioned on a classifier's predicted label. OOD samples are then identified by measuring the semantic mismatch between the original and synthesized images, demonstrating a powerful application of synthetic data generation for OOD scoring. In 3D LiDAR object detection, \textcite{ksel20246fe} tackled data scarcity by generating synthetic OOD objects through unusual scaling perturbations of known ID objects, providing a realistic training signal for OOD detection. Multimodal OOD detection also benefits from synthetic data; \textcite{dong2024a8k} proposed Nearest Neighbor Prototype-based Mixup (NP-Mix) to generate outliers by leveraging nearest neighbor class prototypes, exploring broader feature spaces. \textcite{zhang2024cx0} (MIntOOD) generates pseudo-OOD data through convex combinations of ID features to improve multimodal intent understanding.

Vision-Language Models (VLMs) have further expanded the scope of synthetic OOD generation. \textcite{ding20242m0} introduced Outlier Label Exposure (OLE), which uses auxiliary outlier class labels as pseudo OOD text prompts and generates "hard outlier prototypes" by mixing fringe ID embeddings with refined outlier prototypes. This creates synthetic OOD knowledge in the textual embedding space. \textcite{li20245b6} proposed NegPrompt, a method that learns transferable "negative prompts" for each ID class using only ID training data. These negative prompts implicitly define OOD boundaries by representing characteristics contrary to ID classes, effectively synthesizing textual representations of "non-ID" for OOD discrimination. \textcite{yu20249dd} developed Self-Calibrated Tuning (SCT) for VLMs, which uses ID-irrelevant local context as surrogate OOD data for regularization during prompt tuning, adaptively balancing ID classification and OOD regularization based on prediction uncertainty. Finally, \textcite{liu20245e5} introduced a perturbation-guided spurious synthesis strategy to generate high-quality adversarial samples for training hierarchical contexts, enhancing OOD detection in VLMs. Even Masked Image Modeling (MIM), as explored by \textcite{li2024n34} (MOODv2), contributes to OOD detection by learning comprehensive, pixel-level ID representations. This robust ID modeling implicitly creates a larger domain gap, making OOD samples inherently more distinguishable during reconstruction, thus serving as an indirect but powerful form of synthetic OOD generation.

In conclusion, Outlier Exposure and synthetic OOD generation have evolved from simple mixing techniques to sophisticated, theoretically-grounded strategies that adaptively sample or generate diverse and informative outliers. These methods are crucial for training models that can form tighter in-distribution boundaries and achieve superior OOD discrimination, often with theoretical guarantees for diversity. While significant progress has been made in addressing limitations such as auxiliary data diversity, class imbalance, and domain-specific challenges, future work must continue to explore the representativeness of synthetic data, scalability to truly open-ended OOD types, and the integration of these techniques with emerging foundation models for even more robust and reliable AI systems.
\subsection{Likelihood-Based and Density Estimation Methods}
\label{sec:4\_2\_likelihood-based\_\_and\_\_density\_estimation\_methods}

Out-of-distribution (OOD) detection approaches that explicitly model the probability density of in-distribution (ID) data offer a principled alternative to heuristic confidence scores, aiming to identify atypical samples based on their divergence from the learned data manifold. These methods leverage deep generative models, such as Variational Autoencoders (VAEs) \cite{kingma2013auto} and Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, to learn the complex underlying distribution of ID data. The core idea is that OOD samples will exhibit low likelihood or high reconstruction error under this learned ID model, thereby providing a theoretical grounding for OOD detection by quantifying how typical a given sample is under the learned ID distribution. Early applications of VAEs and GANs for anomaly detection often relied on reconstruction error or the evidence lower bound (ELBO) as an OOD score \cite{an2015variational, schlegl2017unsupervised}.

Despite the theoretical appeal of density estimation, relying solely on raw likelihoods or reconstruction errors for OOD detection presents notable challenges. A seminal work by Nalisnick et al. \cite{nalisnick2019do} critically demonstrated the "bad likelihoods" problem, where state-of-the-art deep generative models, including VAEs and Normalizing Flows, often assign higher likelihoods to OOD inputs than to ID data. This counter-intuitive phenomenon undermines the direct utility of raw density estimates. The issue stems from the high-dimensional nature of data, where the typical set of a distribution (where most of the probability mass lies) may not coincide with the region of highest density, leading models to assign high likelihoods to "simple" OOD samples that fall outside the typical ID manifold but are easily reconstructed or have high density in regions irrelevant to the ID data's semantic content \cite{morningstar2020re9, osada20246an}. For instance, \cite{osada20246an} specifically investigates this phenomenon in Normalizing Flows, hypothesizing that less complex OOD images can concentrate in high-density regions of the latent space, leading to untrustworthy likelihood assignments. Indeed, comprehensive evaluations in complex domains like 3D medical image segmentation have highlighted the severe limitations of reconstruction-based anomaly detection methods, often finding them to be inferior to other self-supervised or confidence-based approaches \cite{vasiliuk20233w9}. This suggests that while generative models excel at learning the ID manifold, the direct interpretation of their reconstruction quality or estimated likelihood as a robust OOD score remains a significant hurdle, as simple reconstruction errors do not always capture the true typicality of a sample.

To address these limitations, research has explored more sophisticated techniques beyond simple reconstruction errors or raw likelihoods. One promising direction involves \textbf{tractable density estimators}, such as Normalizing Flows (NFs) \cite{rezende2015variational, dinh2014nice}. Unlike VAEs, which provide an approximate lower bound on the likelihood, NFs allow for exact and efficient computation of log-likelihoods, bypassing the "amortization gap" and offering more reliable density estimates. Early works explored NFs for OOD detection, leveraging their ability to model complex distributions and provide precise likelihoods \cite{nalisnick2019do}. More recently, \cite{zisselman2020cmx} introduced Deep Residual Flow for OOD detection, a novel flow architecture that learns the residual distribution from a base Gaussian distribution in the feature space of a pre-trained classifier, demonstrating significant improvements over Gaussian distribution models. Further advancing this, \cite{peng20243ji} proposed ConjNorm, a method for tractable density estimation that unifies density function design within the exponential family of distributions using Bregman divergence. ConjNorm devises an unbiased and analytically tractable estimator for the partition function using Monte Carlo-based importance sampling, overcoming a major computational challenge and achieving state-of-the-art performance in post-hoc OOD detection by providing more accurate density estimates.

Another critical approach to overcome "bad likelihoods" is the use of \textbf{likelihood ratios} and \textbf{typicality-based methods}. Instead of relying on the absolute likelihood under a single ID model, likelihood ratio tests compare the likelihood of a sample under an ID model versus a background or OOD model, providing a more discriminative score \cite{ren2019likelihood}. This contrastive approach helps to filter out OOD samples that might coincidentally have high likelihoods under the ID model but would have even higher likelihoods under a more general or OOD-specific model. Building on this, \cite{morningstar2020re9} introduced Density of States Estimation (DoSE), an unsupervised method that moves beyond direct model probabilities. DoSE leverages the "probability of the model probability" or, more generally, the frequency (typicality) of \textit{multiple summary statistics} derived from a generative model (e.g., VAEs, Glow/NFs). By training nonparametric density estimators (like KDE or one-class SVMs) on these statistics, DoSE identifies atypical points even if they have high raw likelihoods, directly addressing the core failure mode of direct likelihood comparison in high dimensions. Similarly, energy-based models, which define an OOD score based on an energy function, can be seen as implicitly leveraging density differences. For example, \cite{hofmann2024gnx} introduced Energy-based Hopfield Boosting, which defines an OOD score by contrasting information from ID and auxiliary OOD memories, effectively creating a score that reflects the relative "energy" or typicality of a sample with respect to both distributions.

Recent advancements in deep generative models have also pushed the boundaries of likelihood-based OOD detection. A significant development, particularly for high-resolution volumetric data, is the application of Latent Diffusion Models (LDMs) for unsupervised 3D OOD detection \cite{graham20232re}. This approach employs a two-stage generative process, initially using a Vector Quantized Generative Adversarial Network (VQ-GAN) to compress 3D medical images into a latent representation. Subsequently, a Denoising Diffusion Probabilistic Model (DDPM) is trained on these compressed latent codes to learn the ID data distribution through an iterative denoising process. For OOD detection, an input is noised and then denoised by the LDM, with the OOD score derived from the reconstruction error (e.g., Mean Squared Error and perceptual similarity) between the original and reconstructed images. This LDM-based framework demonstrates superior performance over previous likelihood-based Latent Transformer Models (LTMs) by offering better memory scaling, reduced sensitivity to latent representation quality, and the ability to generate high-resolution, accurate spatial anomaly maps, which are crucial for precise localization of abnormalities in medical imaging \cite{graham20232re}. While powerful, this method notes reduced performance on certain low-intensity OOD classes, indicating that even advanced generative models can struggle with specific types of OOD data where reconstruction error might not perfectly correlate with OODness \cite{graham20232re}.

In conclusion, likelihood-based and density estimation methods hold immense promise for OOD detection by providing a principled framework grounded in probability theory. The field has evolved significantly from relying on raw likelihoods or simple reconstruction errors, which were found to be unreliable due to the "bad likelihoods" problem. Modern approaches emphasize the use of tractable density estimators like Normalizing Flows for exact likelihood computation and the development of more robust scores based on likelihood ratios or the typicality of multiple summary statistics from generative models. While recent advancements, such as Latent Diffusion Models, demonstrate impressive capabilities in modeling complex data distributions and generating high-resolution anomaly maps, the field continues to grapple with the challenge of translating these density estimates into robust and semantically meaningful OOD scores. Future research must focus on developing more robust likelihood-based metrics, exploring novel generative architectures that inherently yield more discriminative OOD indicators, and further understanding the interplay between image complexity, latent space density, and OOD detection performance, particularly in high-stakes applications where the consequences of misclassification are severe.
\subsection{Robust Training Objectives and Feature Separation}
\label{sec:4\_3\_robust\_training\_objectives\_\_and\_\_feature\_separation}

Moving beyond post-hoc analysis, a critical line of research in Out-of-Distribution (OOD) detection focuses on fundamentally altering the model's training process to learn representations inherently more robust to OOD data or to explicitly separate in-distribution (ID) and OOD features. These methods aim to bake OOD discrimination directly into the model's internal representations from the outset, often leading to more resilient and reliable systems. This subsection explores approaches that introduce specialized loss functions, leverage geometric properties like Neural Collapse, or employ adversarial training techniques to achieve this goal.

A significant direction involves enforcing explicit geometric separation in the feature space. Deep neural networks, particularly in the final layers, exhibit a phenomenon known as Neural Collapse (NC), where features of ID samples within a class converge to their class mean, and these class means are maximally separated \cite{wu20242p3}. OOD detection methods can leverage this property by constraining OOD features to lie in a distinct region, orthogonal to the collapsed ID feature space. For instance, \cite{wu20242p3} introduces a novel "Separation Loss" and "Clustering Loss" during fine-tuning. The Separation Loss explicitly pushes OOD features into a subspace orthogonal to the principal subspace spanned by the final layer weights, thereby ensuring ID and OOD features are separated along different dimensions. Simultaneously, the Clustering Loss enhances ID feature compactness, reinforcing the Neural Collapse phenomenon for ID classes. This approach offers a principled way to achieve feature separation, moving beyond simply increasing output discrepancies. Similarly, \cite{feng2024ma3} proposes CASE, a method that explicitly improves intra-class compactness and inter-class separability of feature embeddings. CASE employs a dual-loss framework, including a separability loss to maximize Euclidean distance between class centers and a compactness loss to minimize intra-class distances, with class centers treated as learnable parameters. By optimizing these geometric properties, CASE enhances the intrinsic separability between ID and OOD samples.

Adversarial training represents another powerful paradigm for learning robust representations, particularly against various distribution shifts and malicious perturbations. Traditional OOD detectors often prove brittle when faced with adversarial examples, whether they are perturbed ID inputs (leading to false rejections as OOD) or perturbed OOD inputs (leading to false acceptances as ID). To address this, \cite{chen2020mbk} introduces ALOE (Adversarial Learning with inlier and Outlier Exposure), a pioneering method that robustifies OOD detectors against both types of adversarial shifts. ALOE formulates the training as a min-max game, where the model is exposed to adversarially crafted inlier examples (to prevent false rejections) and adversarial outlier examples (to prevent false acceptances). This unified approach fundamentally alters the model's decision boundary, making it more robust to subtle, yet critical, shifts. In the context of dense prediction tasks, \cite{besnier2021jgn} proposes ObsNet+LAA for semantic segmentation. This method trains an auxiliary module using local adversarial attacks to generate OOD-like data, which then enables fast and accurate pixel-wise OOD detection without modifying the main segmentation network. The core idea is to teach the auxiliary module to identify patterns that resemble OOD data, even if synthetically generated through adversarial perturbations, thereby enhancing its discriminative power during inference.

Beyond geometric constraints and adversarial robustness, specialized loss functions are designed to explicitly optimize for OOD discrimination during training. Recognizing that auxiliary OOD data, when used, can suffer from class imbalance, \cite{choi202367m} proposes a balanced energy regularization loss. This loss adaptively applies stronger regularization to auxiliary samples from majority classes, leveraging a \texttt{Zγ} term that quantifies the likelihood of an auxiliary sample belonging to a majority class. By dynamically adjusting the loss margin and weight based on this term, the method ensures a more effective and balanced shaping of the energy landscape for OOD detection, leading to superior performance across various tasks. Another approach, explored by \cite{chen20247p7}, investigates feature sparsity for OOD detection. They propose a sparsity-regularized (SR) tuning framework that aims to enlarge the distinguishability between ID and OOD features by promoting sparsity. This modification simplifies the original training loss and enhances the model's adaptive ability and detection performance, demonstrating that feature properties beyond simple magnitude can be leveraged. For pixel-wise OOD detection, \cite{liu2022fdj} presents Residual Pattern Learning (RPL), an external module trained with contrastive learning and a positive energy loss. This module learns residual anomaly patterns from intermediate features, inducing high uncertainty for OOD pixels without requiring retraining of the base segmentation model. The contrastive objective explicitly pushes OOD features away from ID features, while the energy loss encourages low energy for ID and high energy for OOD.

While these robust training objectives and feature separation techniques have made significant strides, challenges persist. Many methods, particularly those leveraging auxiliary data (even with specialized losses), still rely on the availability and quality of such data, which may not always be representative of truly unknown OOD samples. The trade-off between improving OOD detection and maintaining in-distribution classification accuracy remains a critical area, as overly aggressive feature separation could degrade ID performance. Furthermore, the generalizability of geometric priors, like Neural Collapse, to highly diverse or complex OOD scenarios requires further investigation. Future research could focus on developing more data-efficient and adaptive methods for learning OOD-robust representations, exploring theoretical frameworks that unify representation learning and OOD robustness, and designing objectives that are less sensitive to the specific characteristics of auxiliary OOD data.


\label{sec:advanced_representation_learning_and_feature_space_analysis}

\section{Advanced Representation Learning and Feature Space Analysis}
\label{sec:advanced\_representation\_learning\_\_and\_\_feature\_space\_analysis}

\subsection{Multi-Scale and Prototype-Based Feature Learning}
\label{sec:5\_1\_multi-scale\_\_and\_\_prototype-based\_feature\_learning}

Advanced representation learning techniques are crucial for enhancing Out-of-Distribution (OOD) detection by moving beyond simplistic feature norm analyses to capture more nuanced aspects of in-distribution (ID) data. This subsection explores methods that leverage multi-scale representations, combining global and local features to provide richer context for OOD discrimination, and prototype-based learning, which models ID classes with a mixture of prototypes to account for intra-class variability. These approaches collectively aim to create more discriminative and fine-grained feature spaces for robust OOD detection.

A significant direction in this area involves integrating information from various scales within a neural network's architecture. Traditional OOD detection often relies on features from a single layer, typically the penultimate one, which can overlook valuable contextual information. \cite{zhang202312h} introduces Multi-scale OOD DEtection (MODE), a framework that explicitly leverages both global visual information and local region details. Their Attention-based Local PropAgation (ALPA) objective encourages locally discriminative representations during training, while a Cross-Scale Decision (CSD) function combines these multi-scale features at test time, significantly improving OOD detection by addressing limitations of single-scale global representations. Extending this concept to Vision-Language Models (VLMs), \cite{miyai2023591} proposes GL-MCM, which combines global features from CLIP with local visual-text alignments to achieve zero-shot OOD detection, offering flexibility in defining ID images in multi-object scenes. This method addresses the "contamination" of global features when OOD objects are present, a limitation of purely global approaches.

Other works also implicitly or explicitly utilize multi-scale information. \cite{song2022f5d} presents RankFeat, a post-hoc method that performs Singular Value Decomposition (SVD) on high-level feature maps to remove a dominant rank-1 component, observed to disproportionately influence OOD predictions. It further incorporates multi-scale fusion by combining scores from features at different network depths, demonstrating that diverse semantic information across layers can enhance OOD discrimination. Similarly, \cite{liu2022fdj} addresses pixel-wise OOD detection in semantic segmentation by leveraging intermediate features and a Context-robust Contrastive Learning (CoroCL) module. This approach, by operating at a pixel level and utilizing features from different depths, implicitly incorporates multi-scale information to learn residual patterns of anomalies. From a generative perspective, \cite{zhou202250i} rethinks reconstruction autoencoder-based OOD detection by proposing a layerwise semantic reconstruction framework. This method decomposes data certainty across different encoding layers, effectively using multi-scale features for reconstruction, and introduces a Normalized L2 Distance (NL2) to robustly measure reconstruction error, thereby creating a more maximally compressed and discriminative latent space.

Beyond multi-scale feature aggregation, prototype-based learning has emerged as a powerful paradigm for modeling the intricate structure of ID data, especially its intra-class variability. Traditional distance-based OOD methods often oversimplify ID classes by representing each with a single centroid, which fails to capture the inherent diversity within real-world data. To address this, \cite{lu20249d4} introduces Prototypic ALearning with a Mixture of prototypes (PALM), which models each ID class with multiple prototypes using a mixture of von Mises-Fisher (vMF) distributions in a hyperspherical embedding space. This allows for a more faithful and compact representation of intra-class diversity, leading to improved OOD detection by better separating ID and OOD samples. Building on the concept of prototypes, \cite{li2024rs5} proposes Dynamic Prototype Updating (DPU) for multimodal OOD detection. DPU dynamically adjusts multimodal prediction discrepancy intensification based on a sample's similarity to its class prototype, accounting for intra-class variations and preventing uniform discrepancy amplification from degrading ID accuracy. This method uses Cohesive-Separate Contrastive Training (CSCT) to build a robust representation space and dynamically refines prototypes using batch-wise variance. At an even finer granularity, \cite{vojivr202444c} introduces PixOOD for pixel-level OOD detection without requiring OOD training samples. PixOOD employs a novel incremental soft-to-hard data condensation algorithm to model complex intra-class variability using multiple etalons (prototypes) at the pixel level, achieving state-of-the-art results across diverse benchmarks.

Further advancements in learning discriminative and fine-grained feature spaces include methods that refine the underlying data representation. \cite{zaeemzadeh2021lmh} argues for embedding training data into a low-dimensional space where ID samples lie on a union of 1-dimensional subspaces, providing a compact representation that makes OOD samples less likely to occupy the same region. \cite{ammar2023pr1} leverages the Neural Collapse phenomenon, proposing NECO, which identifies OOD samples by projecting features onto the principal component space derived from ID data. This method exploits the observed ID/OOD orthogonality during neural network training to enhance OOD detection. \cite{liu2023zb3} introduces Neuron Activation Coverage (NAC), a statistical measure that quantifies the "coverage degree" of neuron states under ID training data. By formulating a novel neuron activation state that considers both raw output and gradient-based influence, NAC-UE achieves state-of-the-art OOD detection by identifying abnormal neuron behaviors in OOD samples. Lastly, \cite{fang2024lv2} addresses the linear inseparability of ID and OOD features by employing Kernel PCA (KPCA) with task-specific non-linear feature mappings, such as Cosine Mapping (CoP) and Cosine-Gaussian Mapping (CoRP). These mappings enable more effective separation of ID and OOD data in the transformed feature space, offering a computationally efficient alternative to traditional KPCA and k-Nearest Neighbors.

In conclusion, the progression from simpler feature norm analyses to multi-scale and prototype-based feature learning marks a significant step towards more robust OOD detection. These advanced techniques provide richer contextual understanding, better model intra-class diversity, and create more discriminative feature spaces. However, challenges remain, including the computational overhead of multi-scale feature extraction, the optimal determination and dynamic adaptation of prototype mixtures, and the generalizability of learned feature transformations to highly diverse and unforeseen OOD distributions. Future research could focus on developing more adaptive and computationally efficient mechanisms for these nuanced feature learning paradigms.
\subsection{Reconstruction-Based Methods and Autoencoders}
\label{sec:5\_2\_reconstruction-based\_methods\_\_and\_\_autoencoders}

Out-of-distribution (OOD) detection methods leveraging reconstruction errors operate on the fundamental premise that models trained exclusively on in-distribution (ID) data will reconstruct ID samples faithfully while struggling with OOD samples, leading to higher reconstruction errors. This section explores the evolution of such approaches, from classical autoencoders and their variants to advanced generative models and innovative clustering-based techniques. It encompasses both literal pixel-level reconstruction and conceptual extensions that assess semantic consistency or reconstruct features in a learned latent space.

Historically, autoencoders (AEs) served as a foundational model for anomaly detection, including OOD. The core idea involved training an AE to minimize reconstruction error on ID data, then using a high reconstruction error as a signal for OOD samples. However, a critical limitation quickly emerged: deep autoencoders, due to their high capacity, often learned to reconstruct various OOD samples surprisingly well, undermining the assumption that high reconstruction error reliably signals OOD data \cite{perera2019ocd}. Simpler linear methods, such as Principal Component Analysis (PCA), also utilize reconstruction error by projecting data onto a lower-dimensional subspace and measuring the deviation from this subspace. While straightforward, conventional PCA-based methods can be limited by their linearity and sensitivity to noise. To address this, \cite{guan2023dwv} proposed fusing a regularized PCA-based reconstruction error with other scoring functions, such as energy scores, demonstrating that even foundational reconstruction techniques can be enhanced through combination, achieving improved OOD detection results.

Variational Autoencoders (VAEs) offered a more principled generative approach by learning a latent distribution and providing a probabilistic reconstruction. \cite{cai2020lsi} leveraged VAEs within an Inductive Conformal Anomaly Detection (ICAD) framework for real-time OOD detection in Cyber-Physical Systems (CPS). By using VAEs to efficiently compute nonconformity scores based on reconstruction accuracy, they overcame the scalability limitations of traditional ICAD for high-dimensional inputs. This approach, while robust and providing calibrated false alarm rates, still relies on the VAE's ability to accurately model the ID distribution and may struggle if the VAE's generative capacity inadvertently extends to certain OOD patterns, or if the exchangeability assumption is violated.

Addressing the inherent flaw of autoencoders generalizing too well, \cite{zhou202250i} presented a significant "rethinking" of reconstruction autoencoder-based OOD detection. They formalized the preconditions for reconstruction error to be a valid uncertainty measure and introduced a novel framework centered on "layerwise semantic reconstruction." Instead of reconstructing raw pixels, their method reconstructs Activation Vector (AV) features from a pre-trained classifier, which are lower-dimensional and semantically rich. This is coupled with a maximally compressed latent space and a Normalized L2 Distance (NL2) metric, which normalizes reconstruction by the input's norm to counteract misleadingly small L2 errors for OOD samples. This comprehensive approach significantly enhanced the efficacy of autoencoder-based methods by focusing on semantic fidelity rather than pixel-level exactness. A key limitation, however, is its dependency on the quality and robustness of the pre-trained classifier used to extract the semantic features.

Further advancements in autoencoder architectures specifically designed for OOD detection include the multi-decoder autoencoder proposed by \cite{du2024kj8} for vocoder recognition in deepfake detection. This method employs a single encoder but multiple decoders, each specialized to reconstruct acoustic features corresponding to a specific in-distribution vocoder class. An input is classified as OOD if none of the class-specific decoders can satisfactorily reconstruct its features, effectively leveraging reconstruction error in a fine-grained manner. To enhance distinctiveness and constrain the encoder's output, they integrated contrastive learning and an auxiliary classifier. While innovative, this approach still relies on a predefined Mean Square Error (MSE) threshold for OOD classification, which requires careful tuning and might be sensitive to variations in reconstruction quality.

The advent of more sophisticated generative models, particularly diffusion models, further expanded the capabilities of reconstruction-based OOD detection by enabling more complex semantic consistency checks. \cite{gao2023kmk} proposed DiffGuard, a semantic mismatch-guided OOD detection method that utilizes pre-trained diffusion models. Unlike prior generative approaches that struggled with scalability, DiffGuard leverages the stable and flexible conditional generation of diffusion models. It detects OOD samples by measuring the dissimilarity between an input image and a new image synthesized by the diffusion model conditioned on the classifier's predicted label. This effectively highlights semantic mismatches for OOD inputs, making it scalable to large datasets like ImageNet. While powerful, DiffGuard's inference latency and computational cost can be substantial due to the iterative nature of diffusion models, and its performance is inherently tied to the accuracy of the underlying classifier's predictions. Similarly, diffusion models are being explored for OOD detection in specialized domains, such as digital pathology \cite{linmans2024pi9}, indicating their growing versatility.

Beyond direct image generation, reconstruction principles have been adapted to address scalability and performance for complex data modalities like text. \cite{gulati2024dbi} introduced a novel post-hoc OOD detection method that employs soft clustering with Non-Negative Kernel Regression (NNK-Means). This approach learns a compact dictionary of in-distribution representations, and OOD scores are derived from the reconstruction error of new data points against this dictionary. To mitigate hyperparameter sensitivity and improve efficiency, they developed Entropy-Constrained NNK-Means (EC-NNK-Means), which adaptively prunes less important dictionary atoms. This method effectively bridges the gap between the high performance of storage-intensive distance-based methods and the efficiency of classifier-based techniques, particularly for large language models and text data, by providing a scalable and storage-efficient way to model the ID manifold and detect deviations. However, the performance can still be sensitive to the choice of kernel and the dictionary size, requiring careful validation.

In conclusion, reconstruction-based OOD detection has evolved significantly from its early autoencoder roots. Researchers have moved beyond simple pixel-level reconstruction to focus on semantic fidelity, leveraging advanced autoencoder architectures, VAEs, and modern generative models like diffusion models for more robust density estimation and semantic mismatch detection. Furthermore, innovative techniques like soft clustering with non-negative kernel regression demonstrate a continuous effort to refine the concept of "reconstruction error" as an OOD signal, addressing critical challenges such as scalability and performance for complex data modalities like text. These advancements highlight a persistent drive to develop more accurate, efficient, and context-aware OOD detection systems, while also acknowledging the ongoing challenges related to computational expense, reliance on auxiliary components, and threshold sensitivity.
\subsection{Gradient and Activation-Based OOD Detection}
\label{sec:5\_3\_gradient\_\_and\_\_activation-based\_ood\_detection}

Out-of-distribution (OOD) detection methods that leverage the internal dynamics of neural networks, such as gradients or activation patterns, offer a powerful avenue to discern anomalous inputs beyond mere output probabilities. These techniques exploit the observation that OOD inputs often elicit distinct internal responses compared to in-distribution (ID) samples, providing a deeper, more mechanistic insight into model behavior. By analyzing how information flows and is transformed within the network, these methods aim to uncover subtle anomalies that might be missed by examining only the final output layer.

A primary category of these methods focuses on the \textbf{statistical and density-based analysis of activations}. Early approaches sought to characterize the distribution of internal features. \cite{zisselman2020cmx} proposed Deep Residual Flow, which models feature activations using normalizing flows. This approach offers a more expressive representation of the ID distribution compared to traditional Gaussian models, allowing for more nuanced anomaly detection. Building on this, \cite{cook2024hyb} further explored feature density estimation via normalizing flows, demonstrating a fully unsupervised, post-hoc method that trains a lightweight auxiliary flow on features. This method showed strong performance for far-OOD detection, highlighting the potential of learning complex feature distributions. Similarly, \cite{dong2021swz} introduced Neural Mean Discrepancy (NMD), which quantifies the deviation of activation means from training data, efficiently leveraging statistics stored in Batch Normalization layers. This provides a simple yet effective statistical measure of abnormality. Expanding on statistical distance, \cite{gomes2022zyv} presented IGEOOD, an information geometry approach that applies the Fisher-Rao distance to measure dissimilarity between probability distributions derived from latent features. By modeling latent representations as a mixture of Gaussian PDFs, IGEOOD offers a unified framework that can analyze the statistical divergence of internal feature distributions, providing a more robust measure of OODness. These methods collectively demonstrate that a detailed understanding of the statistical properties of internal activations can yield significant mechanistic insights into how OOD inputs perturb the model's learned representations.

Another significant line of research focuses on \textbf{rectifying or gating activation patterns} to explicitly amplify OOD signals. The underlying mechanistic insight here is that OOD inputs often trigger "abnormal" or "extreme" activations that can be suppressed or enhanced to improve discrimination. \cite{zhu2022oir} proposed Batch Normalization Assisted Typical Set Estimation (BATS) with Truncated BN (TrBN), a post-hoc method that rectifies deep features by clamping activations. This process effectively pushes features into their "typical set," mitigating the negative impact of extreme features on uncertainty estimation. While effective, such methods were often heuristic. Addressing this, \cite{xu2023767} provided a theoretical foundation for optimal activation rectification, deriving a variational rectified activation (VRA and VRA+) function. Their work mechanistically demonstrated that optimal OOD separation requires not only suppressing abnormally high activations (as in prior heuristic methods like ReAct) but also suppressing abnormally low activations and amplifying intermediate ones, leading to state-of-the-art performance. This theoretical grounding transforms activation rectification from a heuristic trick into a principled approach for enhancing OOD signals by actively shaping the internal feature space.

Beyond simple statistics and rectification, methods have explored more intricate \textbf{advanced feature space pattern analysis}. These techniques delve into the structural and geometric properties of intermediate feature maps. \cite{yu2022egq} introduced FeatureNorm, which utilizes the channel-wise averaged L2-norm of \textit{rectified} feature maps from \textit{intermediate} convolutional blocks. Their key mechanistic insight was demonstrating that earlier layers can offer superior OOD separation compared to the last layer, which often exhibits an "overconfidence issue" for OOD inputs. This highlights that OOD signals are not uniformly distributed across layers. Taking a spectral approach, \cite{song2022f5d} proposed RankFeat, which performs Singular Value Decomposition on high-level feature maps and removes the dominant rank-1 component. This method leverages the observation that OOD features tend to have a significantly larger dominant singular value, and its removal disproportionately affects OOD predictions, providing a mechanistic understanding of how OOD inputs alter the principal components of feature representations. Further leveraging feature space geometry, \cite{ammar2023pr1} introduced NECO, which exploits the "ID/OOD Orthogonality" property of Neural Collapse. NECO projects feature vectors onto the principal component space derived from ID training data, using the relative norm of this projection as an OOD score. This method offers a geometric interpretation of OOD detection, where OOD samples deviate from the orthogonal structure learned by ID data, providing a mechanistic link between model collapse and OOD discrimination.

A distinct and increasingly important category of methods focuses on \textbf{gradient-based OOD detection}, which probes the model's sensitivity and decision-making process. These methods offer a direct mechanistic insight into how the model reacts to OOD inputs by analyzing the derivatives of its outputs or internal states with respect to inputs or parameters. \cite{behpour2023x13} proposed GradOrth, which identifies OOD data by computing the norm of the gradient projection onto a low-rank subspace derived from important ID parameters. This method leverages the insight that crucial discriminative features for OOD data reside in the gradient subspace of ID data, indicating a mechanistic difference in how OOD inputs activate these critical directions. Expanding on gradient analysis, \cite{chen2023za1} introduced GAIA (Gradient Abnormality Inspection and Aggregation), which quantifies "abnormality" in gradient-based attribution results (input gradients) for OOD detection. GAIA identifies two forms of abnormality: zero-deflation (abnormal non-zero density) and channel-wise average abnormality. This provides a novel mechanistic perspective by linking model interpretability to OOD detection, suggesting that OOD inputs lead to "meaningless" or "noisy" explanations. Bridging activations and gradients, \cite{liu2023zb3} developed Neuron Activation Coverage (NAC-UE), which formulates a neuron activation state by considering both the neuron's raw output and its gradient-based influence on model decisions. NAC-UE then quantifies the "coverage degree" of these neuron states by ID training data, using lower coverage as an indicator of OOD samples. This method provides a mechanistic view of how OOD inputs activate novel combinations of neurons and their associated decision-influencing gradients. More broadly, \cite{schmidt2024syr} presented SISOM, a unified approach for active learning and OOD detection that leverages gradient-enhanced feature representations. By weighting neurons based on their gradient contribution to KL divergence, SISOM enriches feature representations for more effective OOD discrimination, demonstrating how gradients can mechanistically guide the learning of more robust features. Furthermore, some methods leverage gradients during training to instill OOD robustness. \cite{sharifi2024gok} proposed Gradient-Regularized OOD Detection (GReg), which introduces a regularization term that penalizes the norm of the score function's gradient for correctly detected ID and OOD samples. This promotes a smoother score manifold, ensuring local stability and preventing abrupt changes in OOD scores due to minor input perturbations, thereby offering a training-time mechanistic approach to enhance OOD robustness.

In conclusion, the field has progressed from simple statistical analyses of activations to theoretically-grounded rectification techniques and sophisticated geometric interpretations of feature spaces. Concurrently, a growing body of work is demonstrating the rich information embedded within gradients, both in terms of their subspace properties and their patterns of attribution, offering a deeper mechanistic understanding of model behavior. While significant advancements have been made in leveraging these internal model dynamics for OOD detection, challenges remain in developing methods that are robust across all types of OOD data, computationally efficient for real-time deployment, and equipped with strong theoretical guarantees. Future directions may involve combining the strengths of activation rectification with gradient-based insights, exploring more adaptive and dynamic internal analysis techniques, and integrating these methods into comprehensive uncertainty quantification frameworks that provide transparent mechanistic explanations for OOD decisions.
\subsection{Pruning and Model Refinement for OOD}
\label{sec:5\_4\_pruning\_\_and\_\_model\_refinement\_for\_ood}

The inherent overconfidence of deep neural networks on out-of-distribution (OOD) samples poses a significant challenge for their reliable deployment in open-world scenarios. Addressing this, a distinct line of research focuses on refining or pruning neural network models, often as a post-training optimization, to improve their OOD detection capabilities. These techniques aim to mitigate overconfidence, enhance stability, and foster models that are inherently better at distinguishing novel inputs without necessarily relying on explicit OOD data during their primary training phase.

One prominent direction involves post-training refinement through the modification of neuron activations or feature representations. \cite{zhu2022oir} introduced Batch Normalization Assisted Typical Set Estimation (BATS) with Truncated Batch Normalization (TrBN), a plug-and-play module that rectifies neuron activations by clamping them within a "typical set" derived from Batch Normalization statistics. This post-hoc approach effectively makes the model more conservative, boosting OOD detection by mitigating the impact of extreme features. Building upon this concept, \cite{xu2023767} provided a theoretical foundation for optimal activation rectification with Variational Rectified Activation (VRA). Their work demonstrated that an optimal strategy involves not only suppressing abnormally high activations (as in TrBN) but also low ones, while amplifying intermediate activations, thereby offering a more generalized and theoretically grounded approach to feature-level refinement. Complementing these activation-based methods, \cite{song2022f5d} proposed RankFeat, a post-hoc technique that leverages spectral analysis of high-level features. RankFeat improves OOD detection by identifying and removing the dominant rank-1 component from feature maps, which is observed to disproportionately contribute to OOD overconfidence, thus refining the feature space for better discrimination.

Beyond modifying activations or features, direct pruning of model parameters and neurons has emerged as a powerful strategy. \cite{chen2024kl7} introduced Optimal Parameter and Neuron Pruning (OPNP), a training-free method that optimizes parameter and neuron pruning based on gradient sensitivity. OPNP identifies and removes both exceptionally sensitive (risky) and least sensitive (redundant) parameters and neurons by averaging the magnitude of energy score gradients over training samples. This targeted pruning reduces model complexity and overconfidence, leading to significant improvements in OOD detection performance without requiring any retraining or auxiliary OOD data.

While many methods focus on post-training adjustments, another crucial area explores pruning and refinement during the training process itself to build inherently more robust models. \cite{cheng20233yi} addressed the critical issues of OOD detection instability and overfitting during neural network training. Their Average of Pruning (AoP) method combines model averaging (e.g., Stochastic Weight Averaging) to achieve stable OOD performance by smoothing the loss landscape, with network pruning (via the Lottery Ticket Hypothesis) to alleviate overfitting by removing redundant and noisy features that cause overlap between in-distribution (ID) and OOD data. This holistic approach ensures that models are more stable and less prone to overconfidence on novel inputs from the outset. Further enhancing inherent OOD capabilities, \cite{wu20242p3} proposed a novel Separation Loss (\texttt{LSep}) that explicitly separates ID and OOD features in the latent space during training. Leveraging the Neural Collapse phenomenon, which describes the geometric structure of ID features, \texttt{LSep} constrains OOD features to lie in a subspace orthogonal to the principal subspace of ID features, thereby creating models inherently better at distinguishing novel inputs. Similarly, \cite{li2024n34} demonstrated the efficacy of Masked Image Modeling (MIM) as a pre-training objective for robust OOD detection. Their MOODv2 framework shows that MIM-pretrained models learn comprehensive, pixel-level ID representations that inherently create a larger and more exploitable domain gap between ID and OOD samples, leading to superior OOD detection without requiring explicit OOD data for the detection phase.

In conclusion, the landscape of pruning and model refinement for OOD detection spans both post-training optimizations and integrated training-time strategies. Post-hoc methods like activation rectification (\cite{zhu2022oir, xu2023767}) and feature modification (\cite{song2022f5d}) offer efficient ways to recalibrate existing models. More direct post-hoc pruning (\cite{chen2024kl7}) provides structural optimization without retraining. Crucially, training-time approaches (\cite{cheng20233yi, wu20242p3, li2024n34}) aim to embed OOD robustness directly into the model's learning process, fostering stability and inherently better discrimination of novel inputs. A key challenge remains in developing universally applicable methods that can adapt to diverse OOD scenarios without compromising in-distribution performance or incurring significant computational overhead during inference. Future research may explore more adaptive pruning strategies that dynamically adjust model complexity based on input uncertainty, or hybrid approaches that combine the benefits of both post-hoc efficiency and training-time robustness.


\label{sec:specialized_contexts_and_multimodal_ood}

\section{Specialized Contexts and Multimodal OOD}
\label{sec:specialized\_contexts\_\_and\_\_multimodal\_ood}

\subsection{OOD Detection in Semantic Segmentation and 3D Data}
\label{sec:6\_1\_ood\_detection\_in\_semantic\_segmentation\_\_and\_\_3d\_data}

Out-of-distribution (OOD) detection presents specialized challenges in dense prediction tasks like semantic segmentation and for complex 3D data, such as LiDAR point clouds and medical images, where unique structural and contextual properties necessitate tailored solutions. Unlike image classification, these domains require pixel-wise or object-level OOD identification, often exhibiting distinct OOD patterns.

For semantic segmentation, a key challenge is to detect OOD pixels without compromising the primary segmentation task. \cite{liu2022fdj} addresses this with Residual Pattern Learning (RPL), an external module that decouples OOD detection from the segmentation network. RPL leverages intermediate features and employs Context-robust Contrastive Learning (CoroCL) to enhance generalization across diverse scene contexts, overcoming limitations of prior re-training methods that often degrade inlier accuracy. Complementing this, \cite{besnier2021jgn} proposes ObsNet+LAA, an observer network trained using local adversarial attacks to generate OOD-like training data. This approach enables fast and accurate pixel-wise OOD detection by learning from localized failure modes, addressing the scarcity of real OOD training samples. Furthermore, real-world deployments of segmentation models face dynamic environmental changes. \cite{gao2023epm} introduces ATTA (Anomaly-aware Test-Time Adaptation), a dual-level framework that adapts to both domain and semantic shifts at test time. ATTA selectively updates Batch Normalization statistics and uses an anomaly-aware self-training loss to maintain performance under covariate shifts while detecting novel objects. When auxiliary OOD data is utilized, \cite{choi202367m} identifies and mitigates the overlooked problem of class imbalance within these datasets. Their balanced energy regularization loss adaptively applies stronger regularization to majority OOD classes, leading to improved performance in semantic segmentation tasks. Moving towards more general solutions, \cite{vojivr202444c} presents PixOOD, a pixel-level OOD detection method that remarkably does not require OOD training samples. PixOOD models intra-class variability using a novel incremental soft-to-hard data condensation algorithm and formulates OOD detection as a calibrated Neyman-Pearson task, demonstrating state-of-the-art performance across diverse domains like road anomaly detection and industrial inspection.

OOD detection in 3D data, particularly medical imaging and LiDAR, introduces its own set of complexities due to data sparsity, high dimensionality, and the critical nature of applications. The need for robust benchmarks in medical imaging was highlighted by \cite{zimmerer2022rv6} with the introduction of the MOOD 2020 challenge. Building on this, \cite{vasiliuk20233w9} critically evaluates state-of-the-art OOD methods for 3D medical image segmentation, revealing their severe limitations and proposing a simple yet highly effective Intensity Histogram Features (IHF) baseline that often outperforms complex deep learning approaches. This underscores the necessity for solutions tailored to the unique properties of medical data. For unsupervised 3D OOD detection in high-resolution medical data, \cite{graham20232re} proposes using Latent Diffusion Models (LDMs). This innovative approach scales Denoising Diffusion Probabilistic Models (DDPMs) to 3D by first compressing data with a VQ-GAN, overcoming memory constraints of prior methods and producing high-resolution, accurate spatial anomaly maps. Addressing the inconsistent performance of feature-based methods like Mahalanobis distance in medical contexts, \cite{anthony2023slf} conducts a layer-wise analysis, demonstrating that optimal detection layers vary significantly with the OOD pattern. They introduce Multi-branch Mahalanobis (MBM), a robust framework employing multiple depth-specific detectors, which substantially improves OOD detection for unseen medical anomalies. A comprehensive overview of OOD detection in medical image analysis, including a structured taxonomy of distributional shifts and solution frameworks, is provided by the survey from \cite{hong2024xls}, which is crucial for guiding future research in this specialized domain.

For LiDAR-based 3D object detection, where real OOD data is scarce, \cite{ksel20246fe} tackles the problem by generating synthetic OOD objects through geometric perturbations of known in-distribution objects. Their post-hoc multilayer perceptron (MLP) approach, combined with a novel, realistic evaluation protocol for nuScenes, offers a practical solution for identifying unknown foreground objects in autonomous driving scenarios. Beyond specific 3D modalities, the principles of multimodal OOD detection are increasingly relevant for complex sensor setups. \cite{dong2024a8k} introduces the MultiOOD benchmark and the Agree-to-Disagree (A2D) algorithm, which leverages inter-modal prediction discrepancies (e.g., between video and audio) to enhance OOD detection. Further refining this, \cite{li2024rs5} proposes Dynamic Prototype Updating (DPU), a plug-and-play framework that adaptively intensifies multimodal prediction discrepancies based on each sample's similarity to its class prototype, thereby accounting for intra-class variability and improving robustness in multimodal settings.

In conclusion, significant strides have been made in adapting OOD detection to the intricacies of semantic segmentation and 3D data. Solutions range from decoupling OOD detection from the primary task and leveraging local adversarial attacks to test-time adaptation for domain shifts and generative models for complex 3D data. The emphasis on synthetic OOD generation, handling auxiliary data imbalances, and developing OOD-sample-free pixel-level methods highlights the field's maturity. However, challenges persist in achieving universal context robustness, ensuring reliable performance across the vast diversity of real-world OOD patterns, and developing truly generalizable solutions that do not rely on any form of OOD data or strong assumptions about data distributions. Future research will likely focus on more sophisticated generative models, adaptive learning strategies that account for nuanced intra-class variability, and robust evaluation protocols that truly reflect the unpredictability of real-world OOD scenarios in these complex domains.
\subsection{OOD for Graph-Structured Data and NLP}
\label{sec:6\_2\_ood\_for\_graph-structured\_data\_\_and\_\_nlp}

Out-of-distribution (OOD) detection in graph-structured data and Natural Language Processing (NLP) presents unique challenges that traditional image-based methods often fail to address, necessitating specialized approaches that account for the relational nature of graphs and the sequential, semantic complexities of text. These domains push the boundaries of OOD applicability by requiring models to understand intricate structural dependencies and nuanced semantic meanings.

For graph-structured data, the core challenge lies in defining and detecting OOD samples when data is inherently relational and often lacks explicit labels. Pioneering work by \cite{liu202227x} introduced \texttt{GOOD-D}, a self-supervised framework for unsupervised graph-level OOD detection. This method addresses the limitations of traditional graph contrastive learning (GCL) augmentations, which can inadvertently introduce OOD-like samples, by proposing perturbation-free graph data augmentation and hierarchical contrastive learning at node, graph, and group levels to capture comprehensive in-distribution (ID) patterns. Building on the need for unsupervised graph OOD, \cite{wang2025xwm} proposed \texttt{GOLD}, an implicit adversarial latent generation framework. \texttt{GOLD} tackles the critical limitation of requiring auxiliary OOD data for exposure-based methods by implicitly synthesizing pseudo-OOD samples solely from ID training data, enhancing OOD detection without external outliers.

Addressing the practical constraints of real-world deployment, \cite{wang2024es5} introduced \texttt{GOODAT}, a novel approach for test-time graph OOD detection. This method is plug-and-play, operates without access to original training data, and requires no modifications to the pre-trained Graph Neural Network (GNN) architecture, overcoming the computational and data-dependency limitations of prior methods by leveraging a graph masker guided by the Graph Information Bottleneck principle. Furthermore, to provide a unified evaluation framework for the fragmented fields of graph anomaly detection and graph OOD detection, \cite{wang2024q01} presented \texttt{UB-GOLD}. This comprehensive benchmark unifies unsupervised graph-level anomaly detection (GLAD) and OOD detection (GLOD) across 35 datasets and four distinct scenarios, facilitating a systematic comparison of methods and highlighting the inherent challenges in distinguishing various types of graph-level distribution shifts.

In the realm of Natural Language Processing, OOD detection faces distinct challenges due to the discrete nature of text, complex output structures, and the paramount importance of semantic understanding. A comprehensive survey by \cite{lang20237w3} highlights these NLP-specific considerations, categorizing methods based on OOD data availability and emphasizing the role of pre-trained transformer models in learning robust representations for semantic shift detection. Leveraging the vast world knowledge of Large Language Models (LLMs) to address semantic OOD, \cite{dai2023mhn} explored multi-modal OOD detection by integrating LLM-generated descriptive features. Critically, this work introduces a novel consistency-based uncertainty calibration method to mitigate LLM hallucinations, which can otherwise degrade OOD performance, thus enabling the selective and safe integration of LLM knowledge.

Extending the application of LLMs for OOD detection in zero-shot settings, \cite{cao20246gj} proposed Envisioning Outlier Exposure (EOE). EOE utilizes LLMs to "envision" potential outlier class labels based on visual similarity to ID classes, effectively generating synthetic outlier exposure without requiring actual OOD data. This approach, combined with a novel OOD score function, significantly improves detection performance across far, near, and fine-grained OOD scenarios, demonstrating how LLMs can bridge the gap between theoretical benefits of OOD knowledge and practical data unavailability. For highly specialized NLP tasks, such as mathematical reasoning, \cite{wang2024rej} introduced the Trajectory Volatility (TV) Score for OOD detection in Generative Language Models (GLMs). This method addresses the unique "pattern collapse" phenomenon in mathematical reasoning, where distinct samples converge in static embedding spaces, by focusing on the dynamic embedding trajectory volatility across GLM layers, showcasing a specialized solution for complex sequential and semantic challenges.

The literature demonstrates a clear progression towards more sophisticated and domain-aware OOD detection methods for graphs and NLP. While significant strides have been made in developing unsupervised, test-time, and LLM-enhanced solutions, a persistent challenge remains in creating truly universal OOD detectors that are robust to the full spectrum of semantic and covariate shifts without requiring extensive domain-specific engineering or auxiliary OOD data. Future work could focus on developing more generalized frameworks that can adapt to diverse data structures and semantic complexities with minimal human intervention, potentially by further exploring the intrinsic properties of foundation models and their emergent capabilities for uncertainty quantification.
\subsection{Leveraging Vision-Language Models (VLMs) and Multimodal OOD}
\label{sec:6\_3\_leveraging\_vision-language\_models\_(vlms)\_\_and\_\_multimodal\_ood}

The advent of large pre-trained Vision-Language Models (VLMs) and other foundation models has instigated a paradigm shift in Out-of-Distribution (OOD) detection, enabling zero-shot and open-vocabulary capabilities by moving towards a more human-like conceptual understanding of novelty. This section explores cutting-edge techniques that leverage the rich semantic understanding and vast pre-training of these models to define OOD boundaries without explicit OOD data.

Initial efforts to integrate VLMs into OOD detection focused on leveraging their inherent representational power. \cite{miyai2023591} introduced GL-MCM, which utilized both global and local features from models like CLIP for zero-shot OOD detection. This approach addressed the challenge of global features being contaminated by OOD objects in complex, multi-object images, providing flexibility in defining what constitutes an in-distribution (ID) image. However, these early methods often lacked explicit knowledge about OOD samples, leading to higher false positive rates.

To inject OOD awareness without relying on actual OOD images, subsequent research explored prompt-based learning. \cite{ding20242m0} proposed Outlier Label Exposure (OLE), which used diverse auxiliary outlier \textit{class labels} as pseudo OOD text prompts for VLMs. OLE learned pivotal outlier prototypes through unsupervised clustering and filtering, and further generated "hard" outlier prototypes by mixing fringe ID embeddings with refined outlier prototypes, thereby enhancing decision boundary calibration. Building on this, \cite{li20245b6} introduced NegPrompt, a method that learns \textit{transferable negative prompts} for each ID class using \textit{only} ID training data. These negative prompts encapsulate characteristics contrary to ID classes, allowing OOD samples to exhibit higher similarity to them than to positive prompts, enabling truly open-vocabulary OOD detection without external outlier data or additional encoders. This marked a significant advancement in defining OOD boundaries purely from ID knowledge and VLM semantics. Further extending this conceptual understanding, \cite{cao20246gj} presented Envisioning Outlier Exposure (EOE), which leveraged Large Language Models (LLMs) to \textit{envision} potential outlier class labels based on visual similarity to ID classes. By designing task-specific LLM prompts, EOE generated synthetic outlier exposure for VLMs, effectively addressing the practical unavailability of true OOD data and significantly boosting zero-shot OOD performance across far, near, and fine-grained OOD scenarios.

While prompt-based methods showed great promise, challenges remained in refining their application. \cite{yu20249dd} addressed the issue of "spurious OOD features" arising from VLMs' imperfect foreground-background decomposition, which could lead to unreliable OOD features. Their Self-Calibrated Tuning (SCT) method introduced adaptive modulating factors during prompt tuning that dynamically adjusted ID classification and OOD regularization losses based on prediction uncertainty, mitigating the impact of unreliable OOD features without introducing extra hyperparameters.

The scope of OOD detection with foundation models has also expanded to multimodal settings. \cite{dong2024a8k} pioneered multimodal OOD detection by introducing the MultiOOD benchmark and identifying "Modality Prediction Discrepancy" (MPD) as a key signal, where OOD samples exhibit higher inter-modal prediction variability. Their Agree-to-Disagree (A2D) algorithm explicitly amplified this discrepancy during training, encouraging agreement on ground-truth classes and disagreement on others. Building on MPD, \cite{li2024rs5} proposed Dynamic Prototype Updating (DPU) to address the overlooked issue of intra-class variability in multimodal OOD. DPU dynamically adjusted prediction discrepancy intensification based on each sample's similarity to its class prototype, ensuring that only samples distant from their class center experienced amplified discrepancies, thus balancing OOD sensitivity with ID accuracy. Furthermore, \cite{dai2023mhn} integrated LLM capabilities into multimodal OOD by leveraging LLM-derived "world knowledge" as descriptive features. A crucial aspect of their work was a consistency-based uncertainty calibration method for LLM-generated descriptors, mitigating hallucinations and ensuring that only high-confidence knowledge was selectively used to augment multimodal class representations for OOD scoring.

Beyond VLMs, other foundation models are also being repurposed for OOD detection. \cite{gao2023kmk} introduced DiffGuard, which utilized pre-trained diffusion models for semantic mismatch-guided OOD detection. By synthesizing images conditioned on the input and its predicted label, DiffGuard identified OOD samples through dissimilarity between original and synthesized images, overcoming scalability issues of prior generative methods. The growing adoption of these powerful models has also necessitated specialized evaluation; \cite{borlino20245ku} introduced a new benchmark to rigorously assess the performance of large pre-trained models for OOD detection, confirming their inherent generalization benefits while identifying scenarios where task-specific fine-tuning remains optimal. A comprehensive overview of this evolving landscape is provided by \cite{miyai20247ro}, which clarifies the definitions and interrelations of OOD detection and related fields in the VLM era, proposing "Generalized OOD Detection v2" to guide future research.

In conclusion, the integration of VLMs and other foundation models has profoundly reshaped OOD detection, shifting from data-intensive, closed-set approaches to zero-shot, open-vocabulary capabilities driven by rich semantic understanding. Techniques like negative prompt learning, self-calibrated tuning, and leveraging inter-modal prediction discrepancies demonstrate a significant move towards more human-like conceptual understanding of novelty without explicit OOD data. Future research must focus on enhancing the robustness of these models to subtle shifts, developing efficient and adaptive tuning strategies, and exploring the ethical implications of LLM-generated OOD knowledge to ensure trustworthy AI in increasingly open-world scenarios.
\subsection{OOD in Long-Tailed Recognition and Fine-Grained Environments}
\label{sec:6\_4\_ood\_in\_long-tailed\_recognition\_\_and\_\_fine-grained\_environments}

Out-of-Distribution (OOD) detection in scenarios characterized by imbalanced class distributions, commonly known as long-tailed recognition, and in fine-grained environments where inter-class differences are subtle, presents a particularly complex challenge for robust AI systems. These settings exacerbate the fundamental OOD problem by introducing ambiguities where rare in-distribution (ID) tail classes can be easily misclassified as OOD, while true OOD samples might be erroneously absorbed into dominant head classes due to their statistical prevalence \cite{miao2023brn}. Similarly, in fine-grained tasks, the subtle distinctions between ID categories can lead to OOD samples being confused with semantically similar ID classes, demanding highly discriminative and nuanced detection mechanisms. Addressing these complexities requires specialized strategies that move beyond conventional OOD detection methods, which often assume balanced ID distributions and clear OOD boundaries.

Early efforts to enhance OOD detection in fine-grained contexts, where unknown samples share significant semantic similarity with known classes, focused on refining outlier exposure techniques. For instance, \cite{zhang20212tb} introduced Mixture Outlier Exposure (MixOE), a method designed to generate "virtual" outlier samples by mixing ID data with auxiliary outlier data. The core idea is to expand the coverage of the feature space, thereby enabling the model to learn a confidence score that linearly decays from ID to OOD. This linear decay is crucial for fine-grained discrimination, as it provides a more granular measure of OODness, allowing for better separation of samples that lie close to the ID manifold. However, the effectiveness of such methods is inherently tied to the quality and diversity of the auxiliary outlier data, and the synthetic nature of virtual outliers may not always capture the full spectrum of real-world OOD variations.

The challenge intensifies significantly in long-tailed recognition, where the scarcity of data for tail classes makes them inherently difficult to model, rendering them highly susceptible to being misclassified as OOD. Simultaneously, the abundance of head class data can lead to models over-prioritizing these classes, potentially absorbing novel OOD samples into their broad decision regions. To mitigate these issues, \cite{miao2023brn} proposed Calibrated Outlier Class Learning (COCL). COCL extends the traditional label space by explicitly incorporating an outlier class, transforming OOD detection into a multi-class classification problem. This is coupled with debiased large margin learning, which includes OOD-aware tail class prototype learning and debiased head class learning. By learning distinct representation regions for head, tail, and outlier classes, COCL aims to prevent the collapse of tail class representations and the over-expansion of head class boundaries. The subsequent outlier-class-aware logit calibration during inference further refines decision boundaries, significantly reducing the confusion between OOD, head, and tail samples. This approach offers a principled way to balance the trade-off between ID accuracy and OOD detection in imbalanced settings.

Complementing COCL, \cite{wei2023f15} introduced EAT, a framework that leverages dynamic virtual labels for OOD data and context-rich tail class augmentation. EAT addresses the long-tail problem by assigning multiple abstention classes to OOD data and dynamically updating these virtual labels based on model predictions, allowing for more adaptive learning of OOD boundaries. Furthermore, its context-rich augmentation strategy, which overlays tail-class images onto diverse OOD backgrounds, directly tackles the data scarcity for tail classes. This augmentation not only enriches the tail class representation but also forces the model to learn more robust features that distinguish tail classes from various OOD contexts, thereby enhancing generalization and OOD distinction. While COCL focuses on explicit class learning and calibration, EAT emphasizes dynamic labeling and data augmentation, offering alternative yet complementary strategies for long-tailed OOD.

Beyond specific long-tailed methods, the quality and balance of auxiliary OOD data itself can significantly impact performance, especially when using Outlier Exposure-like techniques. Recognizing that auxiliary OOD datasets can also exhibit class imbalance, \cite{choi202367m} proposed a balanced energy regularization loss. This loss adaptively applies stronger regularization to auxiliary OOD samples originating from majority classes within the auxiliary dataset. This nuanced approach is critical for long-tailed classification, as it ensures that the model learns more effectively from diverse auxiliary outliers, preventing bias towards dominant auxiliary categories and promoting a more uniform OOD boundary learning.

With the advent of powerful Vision-Language Models (VLMs), new avenues have opened for OOD detection in fine-grained and complex multi-object environments, often enabling zero-shot and open-vocabulary capabilities. \cite{miyai2023591} introduced Global-Local Maximum Concept Matching (GL-MCM) for zero-shot OOD detection in multi-object scenes. This method combines global image features with local visual-text alignments, which is particularly effective in scenarios where OOD objects might contaminate global image features. By allowing flexibility in defining ID images even when multiple objects are present, GL-MCM helps avoid misclassifying ID objects in complex, fine-grained contexts where the presence of an unknown object might otherwise trigger a false OOD alarm. This represents a significant step towards more semantically aware OOD detection.

Further leveraging VLMs, \cite{li20245b6} proposed NegPrompt, a method that learns transferable negative prompts for each ID class using only ID training data. These negative prompts serve to delineate OOD boundaries by making OOD samples more similar to negative connotations associated with ID classes. This innovative approach enables open-vocabulary OOD detection and significantly improves robustness against hard OOD samples, crucially without requiring any auxiliary OOD data during training. The ability to define OOD through learned negative semantic spaces represents a paradigm shift, reducing the dependency on often-scarce or unrepresentative auxiliary OOD datasets. Complementing this, \cite{yu20249dd} developed Self-Calibrated Tuning (SCT) for VLMs, which adaptively balances ID classification and OOD regularization. SCT directly addresses the issue of "spurious OOD features" that can be extracted from ID data due to imperfect foreground-background decomposition or subtle contextual variations. By dynamically adjusting optimization based on prediction uncertainty, SCT prevents the misclassification of ID samples as OOD, ensuring that the model maintains high ID accuracy while effectively regularizing against OOD inputs in fine-grained settings.

In conclusion, OOD detection in long-tailed and fine-grained environments demands a sophisticated array of strategies to navigate inherent ambiguities, data imbalances, and subtle semantic distinctions. Methods employing outlier class learning, dynamic virtual labels, and context-rich augmentation have significantly improved OOD discrimination in long-tailed settings by explicitly addressing the confusion between head, tail, and OOD classes. The emergence of VLM-based approaches, leveraging local features, negative prompts, and self-calibrated tuning, offers promising avenues for fine-grained and open-vocabulary environments by enabling more semantically informed OOD detection without explicit OOD data. Despite these advancements, significant challenges remain. Developing solutions that are robust to diverse forms of OOD (e.g., near-OOD vs. far-OOD), highly adaptable to varying degrees of class imbalance, and capable of maintaining high ID accuracy while minimizing false positives for rare ID classes, particularly in real-world, dynamic scenarios, continues to be an active area of research. The inherent difficulty of precisely defining OOD boundaries in fine-grained contexts, where the line between novelty and subtle ID variation is blurred, underscores the need for continued innovation in this critical domain.


\label{sec:system_integration_and_guaranteed_ood_detection}

\section{System Integration and Guaranteed OOD Detection}
\label{sec:system\_integration\_\_and\_\_guaranteed\_ood\_detection}

\subsection{Conformal Prediction and Guaranteed OOD Detection}
\label{sec:7\_1\_conformal\_prediction\_\_and\_\_guaranteed\_ood\_detection}

The deployment of deep learning models in safety-critical applications necessitates robust mechanisms for detecting out-of-distribution (OOD) data, moving beyond mere empirical performance to verifiable statistical assurances. Traditional OOD detection methods often rely on heuristic scoring functions, which, despite achieving high Area Under the Receiver Operating Characteristic Curve (AUROC) scores, may not reliably identify actual model errors or provide guarantees on false positive rates (FPR) \cite{guerin202201y}. This critical gap highlights the need for frameworks that offer provable bounds on error rates, ensuring trustworthiness and reliability.

Conformal Prediction (CP) and its variant, Inductive Conformal Anomaly Detection (ICAD), provide a statistically rigorous framework for OOD detection by offering finite-sample guarantees on false detection rates, irrespective of the underlying data distribution, assuming exchangeability. Early work by \textcite{cai2020lsi} demonstrated the application of ICAD for real-time OOD detection in learning-enabled Cyber-Physical Systems (CPS). They addressed the challenge of high-dimensional inputs by proposing learned nonconformity measures based on Variational Autoencoders (VAEs) and Deep Support Vector Data Description (SVDD), which efficiently compute scores without storing the entire training dataset. This approach ensures a well-calibrated false alarm rate, a crucial property for safety-critical CPS.

Building upon the ICAD framework, \textcite{kaur2022cty} introduced iDECODe, a method that leverages in-distribution equivariance as a novel non-conformity measure. By quantifying the error in expected behavior under various transformations, iDECODe provides a robust aggregated non-conformity score. This approach offers a theoretical guarantee that the probability of false OOD detection is upper-bounded by a user-defined significance level, further strengthening the provable bounds on false positive rates for single-point OOD detection. Extending these guarantees to more complex, real-world scenarios, \textcite{kaur20248t3} tackled OOD detection in dependent time-series data for CPS. Their method, CODiT, utilizes deviations from in-distribution temporal equivariance as a non-conformity measure within the conformal framework, combining predictions from multiple detectors using Fisher's method to achieve bounded false alarms for both fixed-length windows and variable-length traces. This addresses a significant limitation of earlier CP-based methods that often assume independent and identically distributed data.

While conformal prediction offers strong theoretical guarantees, practical deployment often encounters dynamic OOD shifts and the challenge of setting appropriate thresholds that minimize human intervention while maintaining safety. To address the pervasive issue of unacceptably high false positive rates in real-world OOD detection, \textcite{vishwakarma2024z1m} proposed a mathematically grounded human-in-the-loop framework. This adaptive system dynamically updates the detection threshold using expert feedback and leverages the Law of Iterated Logarithm (LIL) to provide anytime-valid upper confidence bounds on the FPR. This ensures that the FPR remains below a desired level at all times, even in the presence of distribution shifts, thereby taming false positives and maximizing true positive rates in practical deployments.

Complementing these conformal approaches, other methods also aim for reliable uncertainty quantification to achieve controlled error rates. For instance, \textcite{aguilar2023ms5} integrated Evidential Deep Learning (EDL) into a continual learning framework for simultaneous incremental classification and OOD detection. While not providing conformal guarantees, their Continual Evidential Deep Learning (CEDL) method focuses on robust uncertainty estimation (vacuity and dissonance) in dynamic, open-world settings, demonstrating superior OOD detection performance compared to post-hoc methods in continual learning scenarios. This highlights alternative avenues for building trustworthy AI systems that can adapt to evolving data distributions while maintaining controlled error rates.

In conclusion, the integration of conformal prediction and related uncertainty quantification techniques marks a significant shift towards building OOD detection systems with verifiable assurances, moving beyond purely empirical performance metrics. From early applications in CPS with learned nonconformity measures \cite{cai2020lsi} to refined nonconformity scores for single points \cite{kaur2022cty} and extensions to dependent time-series data \cite{kaur20248t3}, the field is continuously enhancing the scope and robustness of statistical guarantees. Furthermore, adaptive human-in-the-loop systems \cite{vishwakarma2024z1m} are crucial for managing false positives in dynamic real-world environments. Future research will likely focus on scaling these guarantees to more complex models and data modalities, integrating diverse uncertainty quantification methods, and developing more sophisticated adaptive mechanisms for seamless and safe deployment in highly dynamic and safety-critical domains.
\subsection{OOD in Continual Learning and Active Learning Systems}
\label{sec:7\_2\_ood\_in\_continual\_learning\_\_and\_\_active\_learning\_systems}

The deployment of intelligent systems in dynamic, open-world environments necessitates models that can adapt to evolving data streams while maintaining reliability and safety. Out-of-Distribution (OOD) detection plays a crucial and multifaceted role in these adaptive learning paradigms, particularly in continual learning (CL) and active learning (AL), by enabling the identification of novel concepts, managing knowledge evolution through uncertainty quantification, and guiding the selection of informative samples.

In the realm of continual learning, where models incrementally acquire new knowledge without catastrophically forgetting previously learned concepts, OOD detection is vital for identifying novel data that represents new classes or tasks, distinguishing it from mere noise or known but uncertain samples. This capability is paramount for open-world CL systems that must not only adapt to new information but also recognize when data falls outside their current competence. \cite{aguilar2023ms5} addresses this by proposing Continual Evidential Deep Learning (CEDL), which integrates evidential deep learning (EDL) into a CL framework to simultaneously perform incremental object classification and OOD detection. Their method leverages a novel loss function combining evidential cross-entropy, KL-divergence regularization, and knowledge distillation, demonstrating that evidential uncertainty (specifically vacuity) effectively identifies OOD data in dynamic settings while maintaining classification performance. Extending this, \cite{miao2023zf5} explores few-shot traffic classification with OOD detection using a Siamese Prototypical Network (SPN), enabling rapid learning of new traffic types while detecting unknown patterns, which is critical for security in evolving network environments. Furthermore, for systems that need to expand their recognizable categories, \cite{liu20245e5} introduces a category-extensible OOD detection framework via hierarchical context descriptions in vision-language models. This approach, using perceptual and spurious contexts, allows for precise boundary definitions for unseen categories without fine-tuning the base encoders, thereby supporting adaptation to evolving semantic spaces.

However, integrating OOD detection into CL is not without its challenges, often revealing inherent biases and trade-offs. \cite{miao20246mk} highlights these complexities through the OpenCIL benchmark, the first comprehensive evaluation framework for OOD detection in Class-Incremental Learning (CIL). Their work reveals that CIL models exhibit significant biases: they tend to over-confidently classify OOD samples into new classes and, conversely, show low confidence for old class samples due to catastrophic forgetting, leading to their misclassification as OOD. To mitigate these issues, \cite{miao20246mk} proposes Bi-directional Energy Regularization (BER), a novel fine-tuning approach that uses targeted energy loss to enlarge decision boundaries for new classes (pushing OOD away) and boost confidence for old classes (preventing old ID from being seen as OOD). This research underscores the critical need for CL methods to explicitly account for OOD detection during training, rather than relying solely on post-hoc applications, to overcome the inherent plasticity-stability dilemma in open-world scenarios.

For active learning, OOD detection can significantly enhance efficiency by guiding the selection of the most informative samples for labeling, optimizing the learning process by prioritizing data that lies at the boundary of known distributions or represents entirely novel concepts. Active learning strategies typically balance two types of uncertainty: \textit{epistemic uncertainty}, which reflects the model's lack of knowledge about known concepts (e.g., samples near decision boundaries), and \textit{aleatoric uncertainty} or \textit{novelty}, which indicates that a sample is OOD. OOD detection directly addresses the latter, enabling the model to explore truly unknown regions of the data space. \cite{schmidt2024syr} proposes SISOM (Simultaneous Informative Sampling and Outlier Mining), a pioneering unified approach that addresses both active learning and OOD detection concurrently. SISOM leverages gradient-enhanced feature space distances and a self-adaptive mechanism to identify both diverse, unlabeled samples for acquisition (addressing epistemic uncertainty and diversity) and anomalous OOD inputs (addressing novelty). This integration optimizes the learning process by prioritizing data that offers the most learning signal, whether it's refining existing class boundaries or discovering entirely new phenomena. The challenge in AL lies in effectively balancing the acquisition of samples that improve existing classification boundaries with those that expand the model's understanding of its operational domain by identifying OOD data.

Beyond these direct integrations, the reliability of OOD detection in dynamic CL and AL systems often requires strong theoretical guarantees and adaptive mechanisms. While Section 7.1 discusses conformal prediction in general, its application within CL and AL loops is particularly impactful. For instance, in continual learning, statistically sound OOD detection with calibrated false alarm rates, as provided by methods like Inductive Conformal Anomaly Detection (ICAD) \cite{cai2020lsi} or iDECODe \cite{kaur2022cty}, can be crucial for deciding whether a new batch of data represents a genuinely novel task requiring model adaptation or merely OOD noise that should be rejected. Similarly, for time-series data in learning-enabled Cyber-Physical Systems (CPS), \cite{kaur20248t3} applies conformal guarantees to OOD detection, exploiting temporal relationships to detect anomalies in real-time traces with bounded false alarms, which is vital for safe operation in continuously evolving environments. These guarantees provide a rigorous framework for making decisions about data streams in CL.

Practical deployment in evolving environments also necessitates adaptive thresholding and a critical re-evaluation of OOD definitions. \cite{vishwakarma2024z1m} tackles the pervasive issue of high false positive rates in OOD detection by proposing a mathematically grounded human-in-the-loop framework that adaptively updates detection thresholds. This system ensures guaranteed False Positive Rate (FPR) control in dynamic settings by leveraging human feedback and anytime-valid confidence bounds, making OOD detectors safer and more practical for CL and AL where data distributions are constantly shifting. Furthermore, \cite{guerin202201y} offers a crucial conceptual critique, arguing that "Out-of-Model-Scope" (OMS) detection—identifying inputs that lead to actual model errors—is a more appropriate objective than mere OOD detection for safety-critical systems. This distinction is particularly relevant for continual and active learning systems, where the model's "scope" of competence is constantly evolving, making a static definition of OOD insufficient.

In conclusion, the integration of OOD detection within continual and active learning systems is moving towards more adaptive, efficient, and theoretically grounded solutions. While significant progress has been made in developing methods for identifying novel concepts, guiding data selection, and providing statistical guarantees in dynamic environments, challenges remain. Future research needs to further explore unified theoretical frameworks that seamlessly balance the plasticity-stability dilemma of continual learning with robust OOD detection, enhance real-time efficiency for complex multimodal models, and develop more sophisticated human-in-the-loop strategies that can adapt to unforeseen distribution shifts with minimal intervention, all while rigorously defining what constitutes "out-of-scope" for an evolving AI system.


\label{sec:conclusion,_challenges,_and_future_directions}

\section{Conclusion, Challenges, and Future Directions}
\label{sec:conclusion,\_challenges,\_\_and\_\_future\_directions}

\subsection{Synthesis of Current State and Key Achievements}
\label{sec:8\_1\_synthesis\_of\_current\_state\_\_and\_\_key\_achievements}

The field of Out-of-Distribution (OOD) detection has undergone a profound transformation, evolving from nascent, heuristic approaches to a sophisticated, multi-faceted discipline. This maturation is fundamentally driven by the imperative to build reliable and trustworthy AI systems capable of robust uncertainty quantification in dynamic, real-world deployments. The intellectual journey reflects a continuous cycle of identifying limitations in existing methods, spurring the development of increasingly complex, theoretically grounded, and context-aware solutions.

Early OOD detection efforts predominantly focused on extracting uncertainty signals post-hoc from already trained models, often by analyzing their output probabilities or internal representations. While maximum softmax probability (MSP) provided a simple baseline, its inherent brittleness and tendency for overconfidence on OOD samples quickly highlighted the need for deeper insights \cite{hendrycks2017baseline}. This led to a significant research thrust into leveraging internal model states. Techniques like analyzing feature norms and Mahalanobis distance in latent spaces offered a more geometric perspective, aiming to characterize the in-distribution manifold \cite{lee2018mahalanobis}. Similarly, reconstruction-based methods, initially plagued by autoencoders' ability to reconstruct even OOD samples, were refined through innovations like layerwise semantic reconstruction to make reconstruction error a more reliable OOD indicator \cite{zhou202250i}. The realization that different layers or specific feature characteristics might offer superior OOD separation spurred methods leveraging block selection or multi-scale representations to mitigate issues like background clutter \cite{yu2022egq, zhang202312h}. More recently, the field has delved into gradient-based analyses, exploiting the distinct internal responses of OOD inputs by examining low-rank gradient subspaces or attribution abnormalities \cite{behpour2023x13, chen2023za1}. While these post-hoc advancements have significantly improved OOD signal extraction, they remain inherently constrained by the fixed representations of a pre-trained model, often struggling with subtle covariate shifts or exhibiting "model-specific" effectiveness, where optimal methods vary greatly depending on the classifier's robustness and the nature of the OOD data \cite{averly20239rv}. This limitation underscored the necessity of moving beyond mere observation to active intervention during training.

This critical juncture ushered in a paradigm shift towards training-time strategies, explicitly imbuing models with OOD awareness from the outset. Outlier Exposure (OE) emerged as a cornerstone, leveraging auxiliary OOD data during training to sculpt more robust decision boundaries and explicitly teach models what "out-of-distribution" entails \cite{hendrycks2018deep}. The theoretical understanding of OE's asymptotic equivalence to a binary discriminator guided efforts to enhance its practical application. A major challenge within this paradigm has been the scarcity or limited diversity of auxiliary OOD data, leading to a surge in methods focused on generating effective virtual outliers. Techniques like Mixture Outlier Exposure (MixOE) and Virtual Outlier Smoothing (VOSo) create synthetic outliers by perturbing or mixing in-distribution samples, addressing the need for diverse and informative examples, particularly in challenging fine-grained environments \cite{zhang20212tb, nie2024ghv}. Concurrently, robust training objectives have been developed to intrinsically separate in-distribution and OOD features in the embedding space, for instance, by leveraging principles like Neural Collapse to enforce orthogonality between ID and OOD representations \cite{wu20242p3}. While likelihood-based methods, particularly those employing deep generative models, offer a principled approach to density estimation, they historically faced hurdles where raw likelihoods often failed to correlate with OODness. This spurred advancements in more robust density estimation techniques, such as normalizing flows, which provide a more principled way to model complex data distributions and derive reliable OOD scores \cite{zisselman2020cmx}.

The field's maturity is further evidenced by its expansion into increasingly complex and specialized contexts, moving beyond standard image classification. For dense prediction tasks like pixel-wise semantic segmentation, tailored methods have emerged that decouple OOD detection from the primary task or generate OOD-like training data through local adversarial attacks \cite{liu2022fdj, besnier2021jgn}. Similarly, OOD detection in long-tailed recognition, where the distinction between rare in-distribution classes and true OOD is blurred, has necessitated specialized frameworks that employ dynamic virtual labels and context-rich augmentation \cite{miao2023brn, wei2023f15}. The unique structural and relational complexities of graph-structured data and the sequential, semantic nuances of Natural Language Processing (NLP) have also driven the development of specialized unsupervised graph-level OOD methods and parameter-efficient techniques for text \cite{liu202227x, ouyang2023wxc, wang2025xwm}. A transformative development has been the leveraging of large pre-trained Vision-Language Models (VLMs) and Large Language Models (LLMs) for zero-shot and open-vocabulary OOD detection. These foundation models, with their vast semantic understanding, enable OOD detection without explicit outlier data, using techniques like learning transferable "negative prompts" or self-calibrated prompt tuning \cite{li20245b6, yu20249dd}. This marks a significant shift towards more generalized OOD capabilities, though it also introduces new challenges related to prompt sensitivity, reliance on pre-training biases, and the blurring of traditional OOD definitions with related tasks like anomaly detection and open-set recognition in the VLM era \cite{miyai20247ro}. Multimodal OOD detection further exemplifies this trend, leveraging inter-modal prediction discrepancies to identify novelty \cite{dong2024a8k}.

Beyond algorithmic innovation, the field has matured through a rigorous focus on theoretical understanding, comprehensive benchmarking, and system-level considerations. The limitations of prior evaluation paradigms, which often conflated different types of distribution shifts, led to the introduction of Full-Spectrum OOD (FS-OOD) detection and new benchmarks designed to disentangle semantic and covariate shifts \cite{yang2022it3}. Meticulously curated benchmarks like ImageNet-OOD have revealed the nuanced sensitivity of modern algorithms to different shift types \cite{yang2023ckx}, while the "Sorites Paradox" in OOD evaluation has been addressed by benchmarks like IS-OOD \cite{long2024os1}. Theoretically, the field has gained deeper insights into fundamental questions, formalizing "spurious OOD" and analyzing the impact of spurious correlations \cite{ming2021wu7}, and investigating the PAC learnability of OOD detection to establish necessary and sufficient conditions for its effectiveness \cite{fang20249gd}. For practical deployment, the integration of OOD detection with rigorous statistical frameworks like Conformal Prediction has provided provable bounds on false positive rates, a critical requirement for safety-critical applications \cite{kaur2022cty}. Recent advancements even propose "conformal AUROC" and "conformal FPR" to provide statistically rigorous, finite-sample guarantees for OOD score evaluation \cite{novello2024yco}. Furthermore, OOD detection has been integrated into adaptive learning paradigms such as continual and active learning systems, and human-in-the-loop adaptive thresholding frameworks, showcasing its contribution to building robust and efficient AI systems that operate reliably in evolving environments \cite{aguilar2023ms5, schmidt2024syr, vishwakarma2024z1m}.

In summary, the OOD detection landscape has evolved dramatically, offering a diverse and increasingly sophisticated toolkit. From fundamental improvements in leveraging internal model representations and post-hoc scoring, through a paradigm shift enabled by explicit training-time strategies like outlier exposure, to specialized solutions for complex data modalities and the transformative impact of foundation models, the field has made significant strides. This progression, coupled with a growing emphasis on theoretical guarantees and robust evaluation, underscores the remarkable progress made in enhancing the reliability and trustworthiness of AI systems. While substantial achievements bring us closer to robust uncertainty quantification in real-world deployments, the inherent challenges of defining and detecting truly unknown unknowns in dynamic and complex environments continue to lay the groundwork for future advancements.
\subsection{Remaining Challenges and Open Problems}
\label{sec:8\_2\_remaining\_challenges\_\_and\_\_open\_problems}

Despite significant advancements, Out-of-Distribution (OOD) detection faces persistent challenges that hinder its widespread adoption and reliable deployment in real-world, open-set environments. A fundamental hurdle lies in the ambiguous definition and nuanced evaluation of OOD samples, particularly distinguishing between near-OOD and far-OOD instances. Current benchmarks often suffer from semantic ambiguities, ID contamination, and unintended covariate shifts, making it difficult to accurately assess a method's true OOD detection capability \cite{yang2023ckx}. For instance, \cite{long2024os1} highlights the "Sorites Paradox," where the boundary for "how different" an OOD sample must be is unclear, leading to a need for continuous measurement of shift degrees rather than binary classification. Similarly, \cite{ming2021wu7} demonstrates that spurious correlations in training data can severely degrade OOD detection performance, especially for "spurious OOD" samples that share non-causal features with in-distribution (ID) data. This issue is further complicated by the inconsistent treatment of covariate shift across different evaluation frameworks, with some considering it OOD and others as ID, as discussed by \cite{yang2022it3} and \cite{averly20239rv}. The lack of standardized, disentangled benchmarks, particularly for complex domains like 3D medical imaging, remains a critical gap, as underscored by \cite{zimmerer2022rv6} and \cite{vasiliuk20233w9}, which show that even state-of-the-art methods perform poorly or inconsistently in these safety-critical applications. The survey by \cite{miyai20247ro} further emphasizes the blurred boundaries and lack of a unified taxonomy for OOD and related tasks in the era of Vision-Language Models (VLMs), contributing to evaluation confusion.

Another significant challenge is the pervasive reliance on auxiliary OOD data for training, a paradigm known as Outlier Exposure (OE). While methods like Mixture Outlier Exposure \cite{zhang20212tb}, Diverse Outlier Sampling \cite{jiang2023vzb}, and diverseMix \cite{yao2024epq} leverage auxiliary data to improve OOD detection, their effectiveness is inherently tied to the availability, quality, and diversity of this auxiliary data. \cite{wang2024is1} critically notes that OE surprisingly struggles to scale to large datasets due to the difficulty of finding suitable auxiliary OOD data that covers the vast space of potential distribution shifts. Moreover, \cite{bitterwolf2022rw0} provides theoretical evidence that many OE methods are asymptotically equivalent to a simple binary discriminator, suggesting that performance gains often stem from better estimation procedures or choice of auxiliary data rather than fundamentally new principles. The complexity of generating effective synthetic outliers, as explored by Virtual Outlier Smoothing \cite{nie2024ghv} and local adversarial attacks \cite{besnier2021jgn}, or handling class imbalance within auxiliary OOD data \cite{choi202367m}, adds further overhead. To circumvent this reliance, a growing body of research focuses on OOD-free methods, such as rethinking reconstruction autoencoders \cite{zhou202250i}, leveraging generic representations \cite{vojr2023ee1}, or applying post-hoc pruning \cite{chen2024kl7}. However, some of these approaches, particularly density-based methods, can incur high computational costs during inference due to the intractability of normalization constants \cite{peng20243ji}. Other advanced approaches, like ensemble methods for semantic segmentation \cite{besnier2021jgn} or certain generative models \cite{gao2023kmk}, also face computational intensity challenges, limiting their real-time applicability.

The scalability and generalizability of OOD detection solutions across diverse and unpredictable real-world distribution shifts remain a critical gap between laboratory performance and practical deployment. For large models and datasets, methods must be efficient. For instance, while diffusion models show promise, DiffGuard \cite{gao2023kmk} was developed to overcome the prior scalability issues of cGAN-based generative methods. Similarly, for graph-structured data, methods like GOODAT \cite{wang2024es5} aim for test-time, training data-independent solutions to avoid the computational and data access burdens of traditional approaches, which often struggle with the lack of auxiliary OOD data or pre-trained generative models for graphs \cite{wang2025xwm}. In the context of Vision-Language Models (VLMs), zero-shot OOD detection, as pursued by GL-MCM \cite{miyai2023591} and Outlier Label Exposure \cite{ding20242m0}, aims to improve scalability by avoiding per-task training, but still grapples with the lack of explicit OOD knowledge or the presence of spurious OOD features \cite{yu20249dd}. The challenge of "pattern collapse" in generative language models for mathematical reasoning further exemplifies the domain-specific hurdles to generalizability \cite{wang2024rej}. Moreover, methods often struggle with fine-grained distinctions, such as differentiating OOD from tail classes in long-tailed recognition \cite{miao2023brn, wei2023f15} or handling intra-class variability in multimodal data \cite{li2024rs5}. The inherent limitations of current evaluation benchmarks, as highlighted by \cite{borlino20245ku} for foundation models, mean that even seemingly high-performing lab results may not translate to reliable performance in unpredictable real-world scenarios, emphasizing the urgent need for more robust, theoretically grounded \cite{fang20249gd, du2024aea}, and truly generalizable solutions.

In conclusion, the field of OOD detection is still grappling with fundamental issues ranging from the precise definition and robust evaluation of OOD samples to the practical constraints of data availability and computational efficiency. The persistent challenges of distinguishing near-OOD from far-OOD, ensuring scalability for large models, reducing reliance on auxiliary OOD data, and mitigating computational intensity collectively underscore a significant gap between current laboratory performance and the demands of practical, safety-critical deployments. Future research must prioritize the development of theoretically sound, resource-efficient, and context-aware solutions that can reliably generalize across the diverse and unpredictable distribution shifts encountered in real-world applications.
\subsection{Advanced Benchmarking and Evaluation Challenges}
\label{sec:8\_3\_advanced\_benchmarking\_\_and\_\_evaluation\_challenges}

The advancement of Out-of-Distribution (OOD) detection research critically hinges on the development of comprehensive benchmarks and sophisticated evaluation frameworks that move beyond simplistic metrics. A key challenge lies in rigorously defining OOD and creating standardized datasets that allow for accurate and nuanced comparison of different methods across diverse OOD types and scenarios.

Early work highlighted the ill-defined nature of OOD, particularly concerning the impact of spurious correlations. \cite{ming2021wu7} formalized "spurious OOD" and empirically demonstrated how increasing spurious correlation in training data significantly degrades detection performance, underscoring the need for benchmarks that disentangle these factors. Addressing this, \cite{yang2022it3} introduced the "Full-Spectrum OOD (FS-OOD)" problem, explicitly differentiating semantic and covariate shifts, and proposed new benchmarks (DIGITS, OBJECTS, COVID) to evaluate methods under these more realistic conditions. Building on this, \cite{yang2023ckx} meticulously curated \texttt{ImageNet-OOD}, a benchmark designed to isolate semantic shift from covariate shift, revealing that many modern OOD detectors are disproportionately sensitive to covariate shifts rather than true semantic novelty. This work critically analyzed the limitations of existing ImageNet-based OOD datasets, which often suffered from contamination and ambiguity, emphasizing the labor-intensive nature of creating truly clean and nuanced benchmarks.

To provide a more holistic evaluation, researchers have developed unified frameworks that consider a broader spectrum of model failures. \cite{averly20239rv} proposed Model-Specific OOD (MS-OOD) detection, which defines OOD based on whether a deployed model misclassifies an input, thereby unifying the detection of semantic shifts, covariate shifts, and misclassified in-distribution examples. This framework provides a model-centric ground truth for evaluation, offering a more practical perspective. Further consolidating the field, \cite{wang2024is1} presented a critical analysis of OOD detection and Open-Set Recognition (OSR), introducing a large-scale benchmark that systematically disentangles semantic and covariate shifts using ImageNet-1K, ImageNet-21K-P, and ImageNet-C/R. They also proposed "Outlier-Aware Accuracy" (OAA) to reconcile robustness to covariate shift with the ability to detect its presence, providing a more comprehensive metric. In the graph domain, \cite{wang2024q01} unified unsupervised graph-level anomaly detection and OOD detection into a single benchmark, UB-GOLD, defining four distinct scenarios (intrinsic anomaly, class-based anomaly, inter-dataset shift, intra-dataset shift) to facilitate rigorous cross-comparison.

A profound challenge in OOD evaluation is the inherent ambiguity in defining "out-of-distribution" itself, exemplified by the Sorites Paradox. \cite{long2024os1} directly addressed this by introducing the Incremental Shift OOD (IS-OOD) benchmark, which categorizes OOD samples by continuous semantic and covariate shift degrees. They also proposed Language Aligned Image feature Decomposition (LAID) to separately measure these shifts using CLIP features, guiding research towards more nuanced and robust solutions. Complementing empirical evaluations, theoretical work has begun to explore the fundamental learnability of OOD detection. \cite{fang20249gd} provided a PAC learning theory for OOD detection, presenting impossibility theorems that highlight the conditions under which OOD detection is \textit{not} universally learnable, thereby informing the design of benchmarks and the realistic expectations for OOD algorithms. Similarly, \cite{du2024aea} offered theoretical insights into when and how in-distribution labels provably help OOD detection, particularly for near-OOD scenarios, which has implications for benchmark design and the interpretation of results. Foundational understanding of OOD signals, such as the feature norm, has also been advanced by \cite{park2023n97}, which theoretically explained its OOD detection capability and proposed the Negative-Aware Norm (NAN).

Domain-specific challenges further complicate OOD evaluation. In medical imaging, \cite{berger20214a3} conducted a comparative study revealing that OOD detection performance on general computer vision tasks does not directly translate to medical tasks, emphasizing the need for domain-specific benchmarks. \cite{vasiliuk20233w9} further exposed the severe limitations of state-of-the-art OOD methods in 3D medical image segmentation, introducing a new public benchmark and demonstrating that even a simple Intensity Histogram Features (IHF) baseline could outperform complex deep learning models for certain shifts. \cite{hong2024xls} provided a comprehensive survey for OOD in medical image analysis, offering a taxonomy of distributional shifts and evaluation protocols tailored to clinical contexts. For generative language models, especially in mathematical reasoning, \cite{wang2024rej} identified "pattern collapse" in the output space as a unique OOD detection challenge, proposing the "Trajectory Volatility Score" to leverage dynamic embedding trajectories. The emergence of powerful foundation models also necessitates new evaluation paradigms, with \cite{borlino20245ku} introducing benchmarks for harder OOD tasks that existing testbeds easily saturate, aiming to properly assess the performance benefits and limitations of these large pretrained models.

In conclusion, the field is rapidly evolving towards more sophisticated OOD evaluation, moving beyond binary classifications to disentangle semantic, covariate, and contextual shifts. The critical analysis of existing paradigms, such as the Sorites Paradox, and the development of theoretically grounded frameworks are crucial for guiding future research. However, challenges remain in creating scalable, truly clean, and universally representative benchmarks, especially for complex data modalities and advanced models. Future progress will depend on continued efforts to develop rigorous, multi-faceted evaluation methodologies that accurately measure the robustness and generalizability of OOD solutions across diverse, real-world scenarios.
\subsection{Emerging Trends and Ethical Considerations}
\label{sec:8\_4\_emerging\_trends\_\_and\_\_ethical\_considerations}

The landscape of Out-of-Distribution (OOD) detection is rapidly evolving, driven by advancements in foundational AI models and an increasing imperative for trustworthy and responsible AI systems. This subsection explores nascent trends, including the increasing integration of large foundation models, the development of adaptive and human-in-the-loop systems, and the focus on explainable OOD decisions. Crucially, it critically addresses the ethical considerations of fairness, bias, and potential misuse in sensitive applications, emphasizing the importance of developing OOD technologies responsibly to ensure they contribute to safer and more equitable AI systems.

A significant emerging trend is the increasing integration of large foundation models, such as Large Language Models (LLMs) and Vision-Language Models (VLMs), into OOD detection frameworks. As comprehensively surveyed by \cite{miyai20247ro} and \cite{lu2024j0n}, these powerful models are reshaping the field by enabling zero-shot and open-vocabulary OOD detection, moving beyond the need for explicit OOD data during training (as discussed in Section 6.3). However, this trend introduces new ethical and reliability challenges. For instance, LLMs, while rich in world knowledge, are prone to "hallucination," which can catastrophically degrade OOD performance if their generated descriptors are indiscriminately used \cite{dai2023mhn}. To mitigate this, \cite{dai2023mhn} proposes a novel consistency-based uncertainty calibration method that leverages retrieval feedback from unlabeled images, ensuring only reliable LLM knowledge is integrated. Similarly, in generative language models, unique OOD challenges like "pattern collapse" in output spaces, where distinct OOD samples converge to high-density regions, necessitate specialized solutions like the Trajectory Volatility (TV) Score \cite{wang2024rej}. The ethical implication here is that while foundation models offer unprecedented generalization, their inherent complexities (e.g., hallucination, domain specificity, computational cost) demand sophisticated strategies to ensure their OOD predictions are not only accurate but also trustworthy and free from unintended biases amplified by vast, uncurated training data. The ability of LLMs to generate synthetic outlier class labels for "Envisioning Outlier Exposure" (EOE) \cite{cao20246gj} or learn hierarchical contexts for precise OOD detection \cite{liu20245e5} highlights their potential, but also underscores the need for careful validation against the ethical risks of generating biased or misleading synthetic data.

The demand for robust and accountable AI systems is also fostering the development of adaptive, human-in-the-loop, and explainable OOD detection mechanisms. This trend is a direct response to the ethical imperative of controlling false positive rates (FPR) in high-stakes domains. For instance, static OOD detectors often yield unacceptably high FPRs, posing significant safety risks. To address this, \cite{vishwakarma2024z1m} introduces a mathematically grounded human-in-the-loop framework that adaptively updates detection thresholds with theoretical guarantees on FPR control, significantly enhancing safety by minimizing human intervention while ensuring safety. This approach is crucial for sensitive applications, where the cost of a false positive (misclassifying an ID sample as OOD) or false negative (misclassifying an OOD sample as ID) can be severe. Building on the concept of provable guarantees, \cite{novello2024yco} advocates for the integration of Conformal Prediction (CP) into OOD evaluation, proposing "conformal AUROC" and "conformal FPR" metrics that provide statistically rigorous, finite-sample guarantees on OOD score performance. This moves beyond empirical approximations, offering a more trustworthy assessment of OOD detectors, which is vital for certification in safety-critical systems (as further elaborated in Section 7.1).

Beyond mere detection, the field is moving towards explainable OOD decisions, a critical component for human trust and accountability. Methods like GAIA \cite{chen2023za1} detect OOD samples by quantifying "abnormality" in gradient-based attribution results, leveraging the observation that OOD inputs often yield meaningless explanation patterns. Similarly, Neuron Activation Coverage (NAC) \cite{liu2023zb3} offers an interpretable statistical measure of neuron activation states to detect OOD and evaluate generalization. These explainability efforts are essential for human operators to understand \textit{why} a system flags an input as OOD, enabling informed decisions and fostering trust. Critically, \cite{guerin202201y} argues that "Out-of-Distribution Detection Is Not All You Need," advocating for "Out-of-Model-Scope" (OMS) detection. This framework posits that the true ethical goal for safety-critical runtime monitors is to identify actual model errors, rather than merely distributional shifts. This distinction is paramount because a model might correctly classify an OOD sample (generalization) or incorrectly classify an in-distribution sample (error), neither of which is fully captured by traditional OOD metrics, highlighting a fundamental ethical misalignment in the problem formulation itself.

These emerging trends are inextricably linked with critical ethical considerations, such as fairness, bias, and the potential for misuse. A fundamental challenge is the impact of spurious correlations in training data, which can lead models to confidently misclassify OOD inputs that share these non-causal features \cite{ming2021wu7}. This directly contributes to bias and unfair decisions, especially if spurious correlations are tied to sensitive attributes. As discussed in Section 2.1, the entanglement of semantic and covariate shifts further complicates this, as current OOD algorithms are often disproportionately sensitive to covariate shifts rather than pure semantic novelty \cite{yang2023ckx}. This suggests that methods might inadvertently perpetuate biases if not carefully evaluated against diverse shift types. The inherent ambiguity in defining OOD, termed a "Sorites Paradox" by \cite{long2024os1}, underscores the difficulty in establishing fair and consistent rejection criteria, as the "degree of difference" for OOD can be subjective and context-dependent. This ambiguity can lead to arbitrary rejections that disproportionately affect certain demographic groups or edge cases.

Furthermore, \cite{averly20239rv} proposes a "Model-Specific OOD" framework, defining OOD based on a model's \textit{actual misclassification}. While practical for deployment, this implies that any inherent biases in the model's classification performance will directly translate into its OOD decisions. If a model performs poorly on a specific demographic group, its OOD detector, under this definition, will be more likely to flag inputs from that group as "out-of-scope," even if they are semantically in-distribution. This highlights the critical need for fairness-aware OOD detection, where methods are explicitly designed and evaluated to ensure equitable performance across different subgroups. The potential for misuse, especially through adversarial attacks, also necessitates robust OOD detection. As explored by \cite{chen2020mbk}, OOD detectors themselves must be robust against adversarial in-distribution and OOD examples to prevent malicious manipulation of AI systems.

Developing OOD technologies responsibly requires a proactive and interdisciplinary approach to these complex challenges. Future research must prioritize robust LLM integration that accounts for and mitigates hallucination and bias, as demonstrated by \cite{dai2023mhn}. It must also focus on developing provably fair OOD detection methods, moving beyond aggregate performance metrics to ensure equitable treatment across subgroups. This necessitates comprehensive benchmarks that explicitly disentangle shift types and evaluate ethical dimensions alongside performance, as highlighted by \cite{wang2024is1}. Finally, the field must continue to explore the conceptual re-framing of OOD detection towards identifying actual model errors, as advocated by \cite{guerin202201y}, to ensure that OOD technologies truly contribute to safer, more equitable, and ultimately more trustworthy AI systems.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{186}

\bibitem{han2022ixj}
Te Han, and Yanfang Li (2022). \textit{Out-of-distribution detection-assisted trustworthy machinery fault diagnosis approach with uncertainty-aware deep ensembles}. Reliability Engineering & System Safety.

\bibitem{zhou202250i}
Yibo Zhou (2022). \textit{Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{yang2022it3}
Jingkang Yang, Kaiyang Zhou, and Ziwei Liu (2022). \textit{Full-Spectrum Out-of-Distribution Detection}. International Journal of Computer Vision.

\bibitem{zisselman2020cmx}
E. Zisselman, and Aviv Tamar (2020). \textit{Deep Residual Flow for Out of Distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{song2022f5d}
Yue Song, N. Sebe, and Wei Wang (2022). \textit{RankFeat: Rank-1 Feature Removal for Out-of-distribution Detection}. Neural Information Processing Systems.

\bibitem{liu202227x}
Yixin Liu, Kaize Ding, Huan Liu, et al. (2022). \textit{GOOD-D: On Unsupervised Graph Out-Of-Distribution Detection}. Web Search and Data Mining.

\bibitem{zaeemzadeh2021lmh}
Alireza Zaeemzadeh, Niccoló Bisagno, Zeno Sambugaro, et al. (2021). \textit{Out-of-Distribution Detection Using Union of 1-Dimensional Subspaces}. Computer Vision and Pattern Recognition.

\bibitem{ming2021wu7}
Yifei Ming, Hang Yin, and Yixuan Li (2021). \textit{On the Impact of Spurious Correlation for Out-of-distribution Detection}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2022oir}
Yao Zhu, YueFeng Chen, Chuanlong Xie, et al. (2022). \textit{Boosting Out-of-distribution Detection with Typical Features}. Neural Information Processing Systems.

\bibitem{jeong2020z5c}
Taewon Jeong, and Heeyoung Kim (2020). \textit{OOD-MAML: Meta-Learning for Few-Shot Out-of-Distribution Detection and Classification}. Neural Information Processing Systems.

\bibitem{chen2020mbk}
Jiefeng Chen, Yixuan Li, Xi Wu, et al. (2020). \textit{Robust Out-of-distribution Detection for Neural Networks}. Unpublished manuscript.

\bibitem{morningstar2020re9}
W. Morningstar, Cusuh Ham, Andrew Gallagher, et al. (2020). \textit{Density of States Estimation for Out-of-Distribution Detection}. International Conference on Artificial Intelligence and Statistics.

\bibitem{li20227o1}
Zenan Li, Qitian Wu, Fan Nie, et al. (2022). \textit{GraphDE: A Generative Framework for Debiased Learning and Out-of-Distribution Detection on Graphs}. Neural Information Processing Systems.

\bibitem{xie2023uki}
W. Xie, Te Han, Zhong Pei, et al. (2023). \textit{A unified out-of-distribution detection framework for trustworthy prognostics and health management in renewable energy systems}. Engineering applications of artificial intelligence.

\bibitem{liu2022fdj}
Yuyuan Liu, Choubo Ding, Yu Tian, et al. (2022). \textit{Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation}. IEEE International Conference on Computer Vision.

\bibitem{zimmerer2022rv6}
David Zimmerer, Peter M. Full, Fabian Isensee, et al. (2022). \textit{MOOD 2020: A Public Benchmark for Out-of-Distribution Detection and Localization on Medical Images}. IEEE Transactions on Medical Imaging.

\bibitem{kaur2022cty}
R. Kaur, Susmit Jha, Anirban Roy, et al. (2022). \textit{iDECODe: In-distribution Equivariance for Conformal Out-of-distribution Detection}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang20212tb}
Jingyang Zhang, Nathan Inkawhich, Randolph Linderman, et al. (2021). \textit{Mixture Outlier Exposure: Towards Out-of-Distribution Detection in Fine-grained Environments}. IEEE Workshop/Winter Conference on Applications of Computer Vision.

\bibitem{dong2021swz}
Xin Dong, Junfeng Guo, Ang Li, et al. (2021). \textit{Neural Mean Discrepancy for Efficient Out-of-Distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{cai2020lsi}
Feiyang Cai, and X. Koutsoukos (2020). \textit{Real-time Out-of-distribution Detection in Learning-Enabled Cyber-Physical Systems}. International Conference on Cyber-Physical Systems.

\bibitem{gawlikowski2022p4r}
J. Gawlikowski, Sudipan Saha, Anna M. Kruspe, et al. (2022). \textit{An Advanced Dirichlet Prior Network for Out-of-Distribution Detection in Remote Sensing}. IEEE Transactions on Geoscience and Remote Sensing.

\bibitem{paper2020kkd}
Unknown Authors (2020). \textit{Hyperparameter-Free Out-of-Distribution Detection Using Cosine Similarity}. Asian Conference on Computer Vision.

\bibitem{kirchheim20229jl}
Konstantin Kirchheim, Marco Filax, and F. Ortmeier (2022). \textit{PyTorch-OOD: A Library for Out-of-Distribution Detection based on PyTorch}. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).

\bibitem{lang20237w3}
Hao Lang, Yinhe Zheng, Yixuan Li, et al. (2023). \textit{A Survey on Out-of-Distribution Detection in NLP}. Trans. Mach. Learn. Res..

\bibitem{yu2022egq}
Yeonguk Yu, Sungho Shin, Seongju Lee, et al. (2022). \textit{Block Selection Method for Using Feature Norm in Out-of-Distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{zhao20221ag}
Qingling Zhao, Mingqiang Chen, Zonghua Gu, et al. (2022). \textit{CAN Bus Intrusion Detection Based on Auxiliary Classifier GAN and Out-of-distribution Detection}. ACM Transactions on Embedded Computing Systems.

\bibitem{ammar2023pr1}
Mouin Ben Ammar, Nacim Belkhir, Sebastian Popescu, et al. (2023). \textit{NECO: NEural Collapse Based Out-of-distribution detection}. International Conference on Learning Representations.

\bibitem{besnier2021jgn}
Victor Besnier, Andrei Bursuc, David Picard, et al. (2021). \textit{Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation}. IEEE International Conference on Computer Vision.

\bibitem{guerin202201y}
Joris Gu'erin, Kevin Delmas, Raul Sena Ferreira, et al. (2022). \textit{Out-Of-Distribution Detection Is Not All You Need}. AAAI Conference on Artificial Intelligence.

\bibitem{liu2023zb3}
Y. Liu, Chris Xing Tian, Haoliang Li, et al. (2023). \textit{Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization}. International Conference on Learning Representations.

\bibitem{berger20214a3}
Christoph Berger, Magdalini Paschali, Ben Glocker, et al. (2021). \textit{Confidence-based Out-of-Distribution Detection: A Comparative Study and Analysis}. UNSURE/PIPPI@MICCAI.

\bibitem{yang2022ci8}
Yijun Yang, Ruiyuan Gao, and Qiang Xu (2022). \textit{Out-of-Distribution Detection with Semantic Mismatch under Masking}. European Conference on Computer Vision.

\bibitem{lu2023i8o}
Fan Lu, Kai Zhu, Wei Zhai, et al. (2023). \textit{Uncertainty-Aware Optimal Transport for Semantically Coherent Out-of-Distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{kim2024nhz}
B. Kim, B. Kim, and Y. Hyun (2024). \textit{Investigation of out-of-distribution detection across various models and training methodologies}. Neural Networks.

\bibitem{miao2023brn}
Wenjun Miao, Guansong Pang, Tianqi Li, et al. (2023). \textit{Out-of-Distribution Detection in Long-Tailed Recognition with Calibrated Outlier Class Learning}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2022mbf}
Qizhou Wang, Feng Liu, Yonggang Zhang, et al. (2022). \textit{Watermarking for Out-of-distribution Detection}. Neural Information Processing Systems.

\bibitem{zhang202312h}
Ji Zhang, Lianli Gao, Bingguang Hao, et al. (2023). \textit{From Global to Local: Multi-Scale Out-of-Distribution Detection}. IEEE Transactions on Image Processing.

\bibitem{kuan2022qzl}
Jo-Lan Kuan, and Jonas W. Mueller (2022). \textit{Back to the Basics: Revisiting Out-of-Distribution Detection Baselines}. arXiv.org.

\bibitem{choi202367m}
Hyunjun Choi, Hawook Jeong, and Jin Young Choi (2023). \textit{Balanced Energy Regularization Loss for Out-of-distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{liu2025wgr}
Bojun Liu, Jordan G Boysen, I. C. Unarta, et al. (2025). \textit{Exploring transition states of protein conformational changes via out-of-distribution detection in the hyperspherical latent space}. Nature Communications.

\bibitem{miyai2023591}
Atsuyuki Miyai, Qing Yu, Go Irie, et al. (2023). \textit{GL-MCM: Global and Local Maximum Concept Matching for Zero-Shot Out-of-Distribution Detection}. International Journal of Computer Vision.

\bibitem{bitterwolf2022rw0}
Julian Bitterwolf, Alexander Meinke, Maximilian Augustin, et al. (2022). \textit{Breaking Down Out-of-Distribution Detection: Many Methods Based on OOD Training Data Estimate a Combination of the Same Core Quantities}. International Conference on Machine Learning.

\bibitem{gomes2022zyv}
Eduardo Dadalto Camara Gomes, F. Alberge, P. Duhamel, et al. (2022). \textit{Igeood: An Information Geometry Approach to Out-of-Distribution Detection}. International Conference on Learning Representations.

\bibitem{gao2023kmk}
Ruiyuan Gao, Chenchen Zhao, Lanqing Hong, et al. (2023). \textit{DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models}. IEEE International Conference on Computer Vision.

\bibitem{cao20224r3}
Senqi Cao, and Zhongfei Zhang (2022). \textit{Deep Hybrid Models for Out-of-Distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{wei2023f15}
Tong Wei, Bo-Lin Wang, and Min-Ling Zhang (2023). \textit{EAT: Towards Long-Tailed Out-of-Distribution Detection}. AAAI Conference on Artificial Intelligence.

\bibitem{behpour2023x13}
Sima Behpour, T. Doan, Xin Li, et al. (2023). \textit{GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients}. Neural Information Processing Systems.

\bibitem{wang2025xwm}
Danny Wang, Ruihong Qiu, Guangdong Bai, et al. (2025). \textit{GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation}. International Conference on Learning Representations.

\bibitem{graham20232re}
M. Graham, W. H. Pinaya, P. Wright, et al. (2023). \textit{Unsupervised 3D out-of-distribution detection with latent diffusion models}. International Conference on Medical Image Computing and Computer-Assisted Intervention.

\bibitem{bao2024kfh}
Tianyi Bao, Qitian Wu, Zetian Jiang, et al. (2024). \textit{Graph Out-of-Distribution Detection Goes Neighborhood Shaping}. International Conference on Machine Learning.

\bibitem{park2023n97}
Jaewoo Park, Jacky Chen Long Chai, Jaeho Yoon, et al. (2023). \textit{Understanding the Feature Norm for Out-of-Distribution Detection}. IEEE International Conference on Computer Vision.

\bibitem{haider2023vid}
Tom Haider, Karsten Roscher, Felippe Schmoeller da Roza, et al. (2023). \textit{Out-of-Distribution Detection for Reinforcement Learning Agents with Probabilistic Dynamics Models}. Adaptive Agents and Multi-Agent Systems.

\bibitem{ghosal2023q20}
Soumya Suvra Ghosal, Yiyou Sun, and Yixuan Li (2023). \textit{How to Overcome Curse-of-Dimensionality for Out-of-Distribution Detection?}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2023ckx}
William Yang, Byron Zhang, and Olga Russakovsky (2023). \textit{ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms}. International Conference on Learning Representations.

\bibitem{chen2023tz9}
Sishuo Chen, Wenkai Yang, Xiaohan Bi, et al. (2023). \textit{Fine-Tuning Deteriorates General Textual Out-of-Distribution Detection by Distorting Task-Agnostic Features}. Findings.

\bibitem{averly20239rv}
Reza Averly, and Wei-Lun Chao (2023). \textit{Unified Out-Of-Distribution Detection: A Model-Specific Perspective}. IEEE International Conference on Computer Vision.

\bibitem{zhu2023u9p}
Jianing Zhu, Hengzhuang Li, Jiangchao Yao, et al. (2023). \textit{Unleashing Mask: Explore the Intrinsic Out-of-Distribution Detection Capability}. International Conference on Machine Learning.

\bibitem{mishra20236n9}
Divyanshu Mishra, He Zhao, Pramit Saha, et al. (2023). \textit{Dual Conditioned Diffusion Models for Out-of-Distribution Detection: Application to Fetal Ultrasound Videos}. International Conference on Medical Image Computing and Computer-Assisted Intervention.

\bibitem{yu2023r3c}
Shuyang Yu, Junyuan Hong, Haotao Wang, et al. (2023). \textit{Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection}. International Conference on Learning Representations.

\bibitem{dai2023mhn}
Yi Dai, Hao Lang, Kaisheng Zeng, et al. (2023). \textit{Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{arajo2023dau}
Teresa Araújo, Guilherme Aresta, U. Schmidt-Erfurth, et al. (2023). \textit{Few-shot out-of-distribution detection for automated screening in retinal OCT images using deep learning}. Scientific Reports.

\bibitem{xu2023767}
Ming Xu, Zheng Lian, B. Liu, et al. (2023). \textit{VRA: Variational Rectified Activation for Out-of-distribution Detection}. Neural Information Processing Systems.

\bibitem{cheng20233yi}
Zhen Cheng, Fei Zhu, Xu-Yao Zhang, et al. (2023). \textit{Average of Pruning: Improving Performance and Stability of Out-of-Distribution Detection}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{vasiliuk20233w9}
Anton Vasiliuk, Daria Frolova, M. Belyaev, et al. (2023). \textit{Limitations of Out-of-Distribution Detection in 3D Medical Image Segmentation}. Journal of Imaging.

\bibitem{anthony2023slf}
Harry Anthony, and K. Kamnitsas (2023). \textit{On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging}. UNSURE@MICCAI.

\bibitem{yang2023pre}
Taocun Yang, Y. Huang, Yanlin Xie, et al. (2023). \textit{MixOOD: Improving Out-of-distribution Detection with Enhanced Data Mixup}. ACM Trans. Multim. Comput. Commun. Appl..

\bibitem{liu2023i6i}
Bo Liu, Li-Ming Zhan, Zexin Lu, et al. (2023). \textit{How Good Are LLMs at Out-of-Distribution Detection?}. International Conference on Language Resources and Evaluation.

\bibitem{guan2023dwv}
Xiaoyuan Guan, Zhouwu Liu, Weishi Zheng, et al. (2023). \textit{Revisit PCA-based technique for Out-of-Distribution Detection}. IEEE International Conference on Computer Vision.

\bibitem{zhang2024mgg}
Zihan Zhang, Zhuo Xu, and Xiang Xiang (2024). \textit{Vision-Language Dual-Pattern Matching for Out-of-Distribution Detection}. European Conference on Computer Vision.

\bibitem{liu20245e5}
Kai Liu, Zhihang Fu, Chao Chen, et al. (2024). \textit{Category-Extensible Out-of-Distribution Detection via Hierarchical Context Descriptions}. Neural Information Processing Systems.

\bibitem{jia2024zld}
Yulong Jia, Jiaming Li, Ganlong Zhao, et al. (2024). \textit{Enhancing out-of-distribution detection via diversified multi-prototype contrastive learning}. Pattern Recognition.

\bibitem{jiang2023vzb}
Wenyu Jiang, Hao Cheng, Mingcai Chen, et al. (2023). \textit{DOS: Diverse Outlier Sampling for Out-of-Distribution Detection}. International Conference on Learning Representations.

\bibitem{gao2023epm}
Zhitong Gao, Shipeng Yan, and Xuming He (2023). \textit{ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation}. Neural Information Processing Systems.

\bibitem{henriksson20233hb}
Jens Henriksson, Stig Ursing, Murat Erdogan, et al. (2023). \textit{Out-of-Distribution Detection as Support for Autonomous Driving Safety Lifecycle}. Requirements Engineering: Foundation for Software Quality.

\bibitem{saadati2023i8u}
M. Saadati, Aditya Balu, Shivani Chiranjeevi, et al. (2023). \textit{Out-of-Distribution Detection Algorithms for Robust Insect Classification}. Plant Phenomics.

\bibitem{miao2023zf5}
Gongxun Miao, Guohua Wu, Zhen Zhang, et al. (2023). \textit{SPN: A Method of Few-Shot Traffic Classification With Out-of-Distribution Detection Based on Siamese Prototypical Network}. IEEE Access.

\bibitem{chen2023za1}
Jinggang Chen, Junjie Li, Xiaoyang Qu, et al. (2023). \textit{GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection}. Neural Information Processing Systems.

\bibitem{aguilar2023ms5}
Eduardo Aguilar, B. Raducanu, P. Radeva, et al. (2023). \textit{Continual Evidential Deep Learning for Out-of-Distribution Detection}. 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW).

\bibitem{ouyang2023wxc}
Yawen Ouyang, Yongchang Cao, Yuan Gao, et al. (2023). \textit{On Prefix-tuning for Lightweight Out-of-distribution Detection}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lafon2023w37}
Marc Lafon, Elias Ramzi, Clément Rambour, et al. (2023). \textit{Hybrid Energy Based Model in the Feature Space for Out-of-Distribution Detection}. International Conference on Machine Learning.

\bibitem{ksel20246fe}
Michael Kösel, M. Schreiber, Michael Ulrich, et al. (2024). \textit{Revisiting Out-of-Distribution Detection in LiDAR-based 3D Object Detection}. 2024 IEEE Intelligent Vehicles Symposium (IV).

\bibitem{ding20242m0}
Choubo Ding, and Guansong Pang (2024). \textit{Zero-Shot Out-of-Distribution Detection with Outlier Label Exposure}. IEEE International Joint Conference on Neural Network.

\bibitem{zhang2024d24}
Yonggang Zhang, Jie Lu, Bo Peng, et al. (2024). \textit{Learning to Shape In-distribution Feature Space for Out-of-distribution Detection}. Neural Information Processing Systems.

\bibitem{wu20242p3}
Yingwen Wu, Ruiji Yu, Xinwen Cheng, et al. (2024). \textit{Pursuing Feature Separation based on Neural Collapse for Out-of-Distribution Detection}. International Conference on Learning Representations.

\bibitem{yao2024epq}
Haiyu Yao, Zongbo Han, Huazhu Fu, et al. (2024). \textit{Out-Of-Distribution Detection with Diversification (Provably)}. Neural Information Processing Systems.

\bibitem{chen2024kl7}
Chao Chen, Zhihang Fu, Kai Liu, et al. (2024). \textit{Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection}. Neural Information Processing Systems.

\bibitem{kaur20248t3}
Ramneet Kaur, Yahan Yang, O. Sokolsky, et al. (2024). \textit{Out-of-distribution Detection in Dependent Data for Cyber-physical Systems with Conformal Guarantees}. ACM Trans. Cyber Phys. Syst..

\bibitem{zhang2024cx0}
Hanlei Zhang, Qianrui Zhou, Hua Xu, et al. (2024). \textit{Multimodal Classification and Out-of-distribution Detection for Multimodal Intent Understanding}. arXiv.org.

\bibitem{schmidt2024syr}
Sebastian Schmidt, Leonard Schenk, Leo Schwinn, et al. (2024). \textit{A Unified Approach Towards Active Learning and Out-of-Distribution Detection}. Trans. Mach. Learn. Res..

\bibitem{nie2024ghv}
Jun Nie, Yadan Luo, Shanshan Ye, et al. (2024). \textit{Out-of-Distribution Detection with Virtual Outlier Smoothing}. International Journal of Computer Vision.

\bibitem{xu2025hom}
Haoyan Xu, Zhengtao Yao, Yushun Dong, et al. (2025). \textit{Few-Shot Graph Out-of-Distribution Detection with LLMs}. arXiv.org.

\bibitem{vojr2023ee1}
Tomás Vojír, Jan Sochman, Rahaf Aljundi, et al. (2023). \textit{Calibrated Out-of-Distribution Detection with a Generic Representation}. 2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW).

\bibitem{lu20249d4}
Haodong Lu, Dong Gong, Shuo Wang, et al. (2024). \textit{Learning with Mixture of Prototypes for Out-of-Distribution Detection}. International Conference on Learning Representations.

\bibitem{nie20240bk}
Jun Nie, Yonggang Zhang, Zhen Fang, et al. (2024). \textit{Out-of-Distribution Detection with Negative Prompts}. International Conference on Learning Representations.

\bibitem{du20248xe}
Xuefeng Du, Zhen Fang, Ilias Diakonikolas, et al. (2024). \textit{How Does Unlabeled Data Provably Help Out-of-Distribution Detection?}. International Conference on Learning Representations.

\bibitem{li20245b6}
Tianqi Li, Guansong Pang, Xiaolong Bai, et al. (2024). \textit{Learning Transferable Negative Prompts for Out-of-Distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{fang20248g5}
Xiang Fang, A. Easwaran, B. Genest, et al. (2024). \textit{Your data is not perfect: Towards cross-domain out-of-distribution detection in class-imbalanced data}. Expert systems with applications.

\bibitem{linmans2024pi9}
J. Linmans, Gabriel Raya, J. Laak, et al. (2024). \textit{Diffusion models for out-of-distribution detection in digital pathology}. Medical Image Anal..

\bibitem{chen20243na}
Jiaqi Chen, T. H. Teo, C. Kok, et al. (2024). \textit{A Novel Single-Word Speech Recognition on Embedded Systems Using a Convolution Neuron Network with Improved Out-of-Distribution Detection}. Electronics.

\bibitem{dong2024a8k}
Hao Dong, Yue Zhao, Eleni Chatzi, et al. (2024). \textit{MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities}. Neural Information Processing Systems.

\bibitem{miyai20247ro}
Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, et al. (2024). \textit{Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey}. Trans. Mach. Learn. Res..

\bibitem{zhang2024hh0}
Xiaochen Zhang, Chen Wang, Wei Zhou, et al. (2024). \textit{Trustworthy Diagnostics With Out-of-Distribution Detection: A Novel Max-Consistency and Min-Similarity Guided Deep Ensembles for Uncertainty Estimation}. IEEE Internet of Things Journal.

\bibitem{cao20246gj}
Chentao Cao, Zhun Zhong, Zhanke Zhou, et al. (2024). \textit{Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection}. International Conference on Machine Learning.

\bibitem{peng20243ji}
Bo Peng, Yadan Luo, Yonggang Zhang, et al. (2024). \textit{ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection}. International Conference on Learning Representations.

\bibitem{wang2024es5}
Luzhi Wang, Dongxiao He, He Zhang, et al. (2024). \textit{GOODAT: Towards Test-time Graph Out-of-Distribution Detection}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2024q01}
Yili Wang, Yixin Liu, Xu Shen, et al. (2024). \textit{Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark}. International Conference on Learning Representations.

\bibitem{wang2024rej}
Yiming Wang, Pei Zhang, Baosong Yang, et al. (2024). \textit{Embedding Trajectory for Out-of-Distribution Detection in Mathematical Reasoning}. Neural Information Processing Systems.

\bibitem{hong2024xls}
Zesheng Hong, Yubiao Yue, Yubin Chen, et al. (2024). \textit{Out-of-distribution Detection in Medical Image Analysis: A survey}. arXiv.org.

\bibitem{yu20249dd}
Geng Yu, Jianing Zhu, Jiangchao Yao, et al. (2024). \textit{Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection}. Neural Information Processing Systems.

\bibitem{fan2024u9i}
Ke Fan, Tong Liu, Xingyu Qiu, et al. (2024). \textit{Test-Time Linear Out-of-Distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{fang2024lv2}
Kun Fang, Qinghua Tao, Kexin Lv, et al. (2024). \textit{Kernel PCA for Out-of-Distribution Detection}. Neural Information Processing Systems.

\bibitem{vojivr202444c}
Tom'avs Voj'ivr, Jan Sochman, and Jivr'i Matas (2024). \textit{PixOOD: Pixel-Level Out-of-Distribution Detection}. European Conference on Computer Vision.

\bibitem{li2024rs5}
Li Li, Huixian Gong, Hao Dong, et al. (2024). \textit{DPU: Dynamic Prototype Updating for Multimodal Out-of-Distribution Detection}. arXiv.org.

\bibitem{tang20243rx}
Keke Tang, Chao Hou, Weilong Peng, et al. (2024). \textit{CORES: Convolutional Response-based Score for Out-of-distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{li2024n34}
Jingyao Li, Pengguang Chen, Shaozuo Yu, et al. (2024). \textit{MOODv2: Masked Image Modeling for Out-of-Distribution Detection}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{hofmann2024gnx}
Claus Hofmann, Simon Schmid, Bernhard Lehner, et al. (2024). \textit{Energy-based Hopfield Boosting for Out-of-Distribution Detection}. Neural Information Processing Systems.

\bibitem{zhou20243bx}
Zhi Zhou, Ming Yang, Jiang-Xin Shi, et al. (2024). \textit{DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection}. International Conference on Machine Learning.

\bibitem{vishwakarma2024z1m}
Harit Vishwakarma, Heguang Lin, and Ramya Korlakai Vinayak (2024). \textit{Taming False Positives in Out-of-Distribution Detection with Human Feedback}. International Conference on Artificial Intelligence and Statistics.

\bibitem{wang2024y55}
Kaizheng Wang, Fabio Cuzzolin, Keivan K1 Shariatmadar, et al. (2024). \textit{Credal Wrapper of Model Averaging for Uncertainty Estimation on Out-Of-Distribution Detection}. arXiv.org.

\bibitem{xu2024ufg}
Ruiyao Xu, and Kaize Ding (2024). \textit{Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey}. North American Chapter of the Association for Computational Linguistics.

\bibitem{du2024aea}
Xuefeng Du, Yiyou Sun, and Yixuan Li (2024). \textit{When and How Does In-Distribution Label Help Out-of-Distribution Detection?}. International Conference on Machine Learning.

\bibitem{zamzmi20240s6}
Ghada Zamzmi, Kesavan Venkatesh, Brandon Nelson, et al. (2024). \textit{Out-of-Distribution Detection and Radiological Data Monitoring Using Statistical Process Control}. Journal of imaging informatics in medicine.

\bibitem{zhu2024awk}
Armando Zhu, Jiabei Liu, Keqin Li, et al. (2024). \textit{Exploiting Diffusion Prior for Out-of-Distribution Detection}. Irish Interdisciplinary Journal of Science &amp; Research.

\bibitem{nasvytis2024mmr}
L. Nasvytis, Kai Sandbrink, Jakob Foerster, et al. (2024). \textit{Rethinking Out-of-Distribution Detection for Reinforcement Learning: Advancing Methods for Evaluation and Detection}. Adaptive Agents and Multi-Agent Systems.

\bibitem{yuan2024ug7}
Yue Yuan, Rundong He, Yicong Dong, et al. (2024). \textit{Discriminability-Driven Channel Selection for Out-of-Distribution Detection}. Computer Vision and Pattern Recognition.

\bibitem{fang20249gd}
Zhen Fang, Yixuan Li, Feng Liu, et al. (2024). \textit{On the Learnability of Out-of-distribution Detection}. Journal of machine learning research.

\bibitem{cao20250gu}
Yanan Cao, Fengzhao Shi, Qing Yu, et al. (2025). \textit{IBPL: Information Bottleneck-based Prompt Learning for graph out-of-distribution detection}. Neural Networks.

\bibitem{heng2024fjd}
Alvin Heng, A. Thiéry, and Harold Soh (2024). \textit{Out-of-Distribution Detection with a Single Unconditional Diffusion Model}. Neural Information Processing Systems.

\bibitem{zhao2024u4m}
Qinyu Zhao, Ming Xu, Kartik Gupta, et al. (2024). \textit{Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection}. International Conference on Learning Representations.

\bibitem{xu20242cq}
Chenhui Xu, Fuxun Yu, Zirui Xu, et al. (2024). \textit{Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble}. International Conference on Machine Learning.

\bibitem{mirzaei2024dad}
Hossein Mirzaei, and Mackenzie W. Mathis (2024). \textit{Adversarially Robust Out-of-Distribution Detection Using Lyapunov-Stabilized Embeddings}. International Conference on Learning Representations.

\bibitem{sharifi2024gok}
Sina Sharifi, Taha Entesari, Bardia Safaei, et al. (2024). \textit{Gradient-Regularized Out-of-Distribution Detection}. European Conference on Computer Vision.

\bibitem{kirchheim20243gn}
Konstantin Kirchheim, Tim Gonschorek, and F. Ortmeier (2024). \textit{Out-of-Distribution Detection with Logical Reasoning}. IEEE Workshop/Winter Conference on Applications of Computer Vision.

\bibitem{shi2024rfk}
Xiangxi Shi, and Stefan Lee (2024). \textit{Benchmarking Out-of-Distribution Detection in Visual Question Answering}. IEEE Workshop/Winter Conference on Applications of Computer Vision.

\bibitem{feng2024r4v}
Shuai Feng, and Chongjun Wang (2024). \textit{When an extra rejection class meets out-of-distribution detection in long-tailed image classification}. Neural Networks.

\bibitem{li2024ypq}
Yixia Li, Boya Xiong, Guanhua Chen, et al. (2024). \textit{SeTAR: Out-of-Distribution Detection with Selective Low-Rank Approximation}. Neural Information Processing Systems.

\bibitem{zeng2024bti}
Fanhu Zeng, Zhen Cheng, Fei Zhu, et al. (2024). \textit{Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution Detection}. International Conference on Learning Representations.

\bibitem{krumpl2024n1w}
Gerhard Krumpl, H. Avenhaus, Horst Possegger, et al. (2024). \textit{ATS: Adaptive Temperature Scaling for Enhancing Out-of-Distribution Detection Methods}. IEEE Workshop/Winter Conference on Applications of Computer Vision.

\bibitem{hogeweg2024tw3}
L. Hogeweg, Rajesh Gangireddy, Django Brunink, et al. (2024). \textit{COOD: Combined out-of-distribution detection using multiple measures for anomaly & novel class detection in large-scale hierarchical classification}. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).

\bibitem{wang2024is1}
Hongjun Wang, S. Vaze, and Kai Han (2024). \textit{Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks}. International Journal of Computer Vision.

\bibitem{lu2024j0n}
Shuo Lu, Yingsheng Wang, Lijun Sheng, et al. (2024). \textit{Out-of-Distribution Detection: A Task-Oriented Survey of Recent Advances}. Unpublished manuscript.

\bibitem{tan2024oj5}
Qiaozhi Tan, Long Bai, Guan-Feng Wang, et al. (2024). \textit{Endoood: Uncertainty-Aware Out-of-Distribution Detection in Capsule Endoscopy Diagnosis}. IEEE International Symposium on Biomedical Imaging.

\bibitem{ma202440a}
Longfei Ma, Yiyou Sun, Kaize Ding, et al. (2024). \textit{Revisiting Score Propagation in Graph Out-of-Distribution Detection}. Neural Information Processing Systems.

\bibitem{yang2025z62}
Shenzhi Yang, Bin Liang, An Liu, et al. (2025). \textit{Bounded and Uniform Energy-based Out-of-distribution Detection for Graphs}. International Conference on Machine Learning.

\bibitem{zhang2024z2g}
Yuhang Zhang, Jiani Hu, Dongchao Wen, et al. (2024). \textit{Unsupervised evaluation for out-of-distribution detection}. Pattern Recognition.

\bibitem{abdi2024mvh}
Lemar Abdi, M. Valiuddin, Christiaan G. A. Viviers, et al. (2024). \textit{Typicality Excels Likelihood for Unsupervised Out-of-Distribution Detection in Medical Imaging}. UNSURE@MICCAI.

\bibitem{haider20249q8}
Tom Haider, Karsten Roscher, Benjamin Herd, et al. (2024). \textit{Can you trust your Agent? The Effect of Out-of-Distribution Detection on the Safety of Reinforcement Learning Systems}. ACM Symposium on Applied Computing.

\bibitem{qu202422m}
Jingen Qu, Yufei Chen, Xiaodong Yue, et al. (2024). \textit{Hyper-opinion Evidential Deep Learning for Out-of-Distribution Detection}. Neural Information Processing Systems.

\bibitem{novello2024yco}
Paul Novello, Joseba Dalmau, and L'eo Andeol (2024). \textit{Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)}. arXiv.org.

\bibitem{chen2024f28}
Jiankang Chen, Tong Zhang, Weishi Zheng, et al. (2024). \textit{TagFog: Textual Anchor Guidance and Fake Outlier Generation for Visual Out-of-Distribution Detection}. AAAI Conference on Artificial Intelligence.

\bibitem{miao2024318}
Wenjun Miao, Guansong Pang, Jingyi Zheng, et al. (2024). \textit{Long-Tailed Out-of-Distribution Detection via Normalized Outlier Distribution Adaptation}. Neural Information Processing Systems.

\bibitem{oh2024opf}
Ji-Hun Oh, Kianoush Falahkheirkhah, and Rohit Bhargava (2024). \textit{Are We Ready for Out-of-Distribution Detection in Digital Pathology?}. International Conference on Medical Image Computing and Computer-Assisted Intervention.

\bibitem{dong2024t8f}
Jiuqing Dong, Yifan Yao, Wei Jin, et al. (2024). \textit{Enhancing Few-Shot Out-of-Distribution Detection With Pre-Trained Model Features}. IEEE Transactions on Image Processing.

\bibitem{feng2024ma3}
Shuai Feng, Pengsheng Jin, and Chongjun Wang (2024). \textit{CASE: Exploiting Intra-class Compactness and Inter-class Separability of Feature Embeddings for Out-of-Distribution Detection}. AAAI Conference on Artificial Intelligence.

\bibitem{he2024e9z}
Rundong He, Yue Yuan, Zhongyi Han, et al. (2024). \textit{Exploring Channel-Aware Typical Features for Out-of-Distribution Detection}. AAAI Conference on Artificial Intelligence.

\bibitem{gong2024n0t}
Mingrong Gong, Chaoqi Chen, Qingqiang Sun, et al. (2024). \textit{Out-of-Distribution Detection with Prototypical Outlier Proxy}. AAAI Conference on Artificial Intelligence.

\bibitem{cook2024hyb}
Evan D. Cook, Marc-Antoine Lavoie, and Steven L. Waslander (2024). \textit{Feature Density Estimation for Out-of-Distribution Detection via Normalizing Flows}. Proceedings of the 21st Conference on Robots and Vision.

\bibitem{miao20246mk}
Wenjun Miao, Guansong Pang, Trong-Tung Nguyen, et al. (2024). \textit{OpenCIL: Benchmarking Out-of-Distribution Detection in Class-Incremental Learning}. Pattern Recognition.

\bibitem{lee2025gu9}
Yuxiao Lee, Xiaofeng Cao, Jingcai Guo, et al. (2025). \textit{Concept Matching with Agent for Out-of-Distribution Detection}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2025v65}
Jinglong Wang, and Ridong Zhang (2025). \textit{Open-Set Fault Diagnosis Based on 1D-ResNet With Fusion of Cross-Class and Extreme Information for Out-of-Distribution Detection}. IEEE Transactions on Instrumentation and Measurement.

\bibitem{he2024s9w}
Rundong He, Zhongyi Han, Xiushan Nie, et al. (2024). \textit{Visual Out-of-Distribution Detection in Open-Set Noisy Environments}. International Journal of Computer Vision.

\bibitem{li2025jdt}
Xuhui Li, Zhen Fang, Yonggang Zhang, et al. (2025). \textit{Characterizing Submanifold Region for Out-of-Distribution Detection}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{gulati2024dbi}
Aryan Gulati, Xingjian Dong, Carlos Hurtado, et al. (2024). \textit{Out-of-Distribution Detection through Soft Clustering with Non-Negative Kernel Regression}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{osada20246an}
Genki Osada, Tsubasa Takahashi, and Takashi Nishide (2024). \textit{Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection}. AAAI Conference on Artificial Intelligence.

\bibitem{mei20248tm}
Yihan Mei, Xinyu Wang, De-Fu Zhang, et al. (2024). \textit{Multi-Label Out-of-Distribution Detection with Spectral Normalized Joint Energy}. APWeb/WAIM.

\bibitem{kim2024vqc}
Jeonghyeon Kim, Jihyo Kim, and Sangheum Hwang (2024). \textit{Comparison of Out-of-Distribution Detection Performance of CLIP-based Fine-Tuning Methods}. International Conference on Electronics, Information and Communications.

\bibitem{kahya2024ywf}
Sabri Mustafa Kahya, Boran Hamdi Sivrikaya, Muhammet Sami Yavuz, et al. (2024). \textit{FOOD: Facial Authentication and Out-of-Distribution Detection with Short-Range FMCW Radar}. International Conference on Information Photonics.

\bibitem{ekim2024zwd}
Burak Ekim, G. Tadesse, Caleb Robinson, et al. (2024). \textit{Distribution Shifts at Scale: Out-of-distribution Detection in Earth Observation}. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).

\bibitem{shin2024lnf}
Dong Geun Shin, and Hye Won Chung (2024). \textit{Representation Norm Amplification for Out-of-Distribution Detection in Long-Tail Learning}. Trans. Mach. Learn. Res..

\bibitem{du2024kj8}
Renmingyue Du, Jixun Yao, Qiuqiang Kong, et al. (2024). \textit{Towards Out-of-Distribution Detection in Vocoder Recognition via Latent Feature Reconstruction}. Unpublished manuscript.

\bibitem{mao20244lp}
Zhenjiang Mao, Dong-You Jhong, Ao Wang, et al. (2024). \textit{Language-Enhanced Latent Representations for Out-of-Distribution Detection in Autonomous Driving}. arXiv.org.

\bibitem{borlino20245ku}
Francesco Cappio Borlino, L. Lu, and Tatiana Tommasi (2024). \textit{Foundation Models and Fine-Tuning: A Benchmark for Out of Distribution Detection}. IEEE Access.

\bibitem{li2024tk8}
SiCong Li, Ning Li, Min Jing, et al. (2024). \textit{Evaluation of Ten Deep-Learning-Based Out-of-Distribution Detection Methods for Remote Sensing Image Scene Classification}. Remote Sensing.

\bibitem{chen20247p7}
Qichao Chen, Kuan Li, Zhiyuan Chen, et al. (2024). \textit{Exploring feature sparsity for out-of-distribution detection}. Scientific Reports.

\bibitem{zhou2024ae1}
Jingqiu Zhou, Aojun Zhou, and Hongsheng Li (2024). \textit{NODI: Out-Of-Distribution Detection with Noise from Diffusion}. arXiv.org.

\bibitem{galesso2024g7t}
Silvio Galesso, Philipp Schröppel, Hssan Driss, et al. (2024). \textit{Diffusion for Out-of-Distribution Detection on Road Scenes and Beyond}. European Conference on Computer Vision.

\bibitem{chen202491k}
Yang Chen, Chih-Li Sung, A. Kusari, et al. (2024). \textit{Uncertainty-Aware Out-of-Distribution Detection with Gaussian Processes}. arXiv.org.

\bibitem{long2024os1}
Xingming Long, Jie Zhang, Shiguang Shan, et al. (2024). \textit{Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites Paradox}. Neural Information Processing Systems.

\bibitem{cai2025ez2}
Tingyi Cai, Yunliang Jiang, Yixin Liu, et al. (2025). \textit{Out-of-Distribution Detection on Graphs: A Survey}. arXiv.org.

\bibitem{liu2025m5u}
Moru Liu, Hao Dong, Jessica Kelly, et al. (2025). \textit{Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation}. arXiv.org.

\bibitem{li2025xv2}
Hengzhuang Li, and Teng Zhang (2025). \textit{Outlier Synthesis via Hamiltonian Monte Carlo for Out-of-Distribution Detection}. International Conference on Learning Representations.

\bibitem{liu2024m2l}
Xixi Liu, and Christopher Zach (2024). \textit{TAG: Text Prompt Augmentation for Zero-Shot Out-of-Distribution Detection}. European Conference on Computer Vision.

\bibitem{yu2024ez3}
Yeonguk Yu, Sungho Shin, Minhwan Ko, et al. (2024). \textit{Exploring using jigsaw puzzles for out-of-distribution detection}. Computer Vision and Image Understanding.

\bibitem{ahsan20241ht}
Syed Safwan Ahsan, Alireza Esmaeilzehi, and M. O. Ahmad (2024). \textit{OODNet: A deep blind JPEG image compression deblocking network using out-of-distribution detection}. Journal of Visual Communication and Image Representation.

\bibitem{ma202473w}
Xinsong Ma, Xin Zou, and Weiwei Liu (2024). \textit{A Provable Decision Rule for Out-of-Distribution Detection}. International Conference on Machine Learning.

\bibitem{isaku2025kiz}
Erblin Isaku, Hassan Sartaj, and Shaukat Ali (2025). \textit{Digital Twin-based Out-of-Distribution Detection in Autonomous Vessels}. arXiv.org.

\end{thebibliography}

\end{document}