\subsection*{Evolution of OOD Research: A Historical Perspective}

The quest for robust uncertainty quantification in open-world AI systems has driven the continuous evolution of Out-of-Distribution (OOD) detection research, tracing a path from rudimentary statistical anomaly detection to sophisticated deep learning-based approaches. Before the advent of deep learning, the foundations of OOD detection were laid by classical statistical anomaly and novelty detection methods. Techniques such as One-Class Support Vector Machines (OC-SVMs) \cite{Scholkopf_2001_Estimating_The_Support_Of_A_High-Dimensional_Distribution} and Isolation Forests \cite{Liu_2008_Isolation_Forest} aimed to model the boundary of in-distribution data or isolate outliers based on statistical properties. These methods, while effective for structured or lower-dimensional data, often struggled with the high-dimensionality, complexity, and intricate feature correlations inherent in real-world data like images, text, and sensor streams. The rise of deep neural networks (DNNs) in the 2010s presented a dual challenge and opportunity. While DNNs achieved unprecedented performance in discriminative tasks, they were notoriously overconfident on inputs far removed from their training distribution, making them unreliable in open-world scenarios. This critical vulnerability spurred a renewed and intensified focus on OOD detection, shifting the paradigm towards leveraging and adapting deep learning architectures themselves for uncertainty quantification.

The initial wave of deep learning-based OOD research, emerging around 2017, largely adopted a *post-hoc* strategy. Researchers sought to extract OOD signals from already trained deep classifiers without modifying their internal structure or retraining. This approach prioritized efficiency and ease of integration. A seminal contribution was the observation that the Maximum Softmax Probability (MSP) of a classifier could serve as a surprisingly effective baseline for OOD detection \cite{Hendrycks_2017_A_Baseline}. Samples with low MSP were deemed OOD, leveraging the classifier's inherent confidence scores. This simple yet powerful idea quickly led to refinements. Methods like ODIN \cite{Liang_2018_Enhancing_The_Reliability} enhanced MSP's discriminative power through post-processing techniques such as temperature scaling and input perturbation, demonstrating that even minor adjustments could significantly amplify OOD signals. Concurrently, the utility of the feature space was explored, with Mahalanobis distance proposed as a robust OOD score by modeling class-conditional Gaussian distributions in intermediate feature layers \cite{Lee_2018_A_Simple_Unified}. Further theoretical grounding came with energy-based models, which offered a principled way to derive OOD scores directly from logits, often with a small trainable component for calibration \cite{Liu_2020_Energy-based_Out-of-Distribution}. While these early score-based methods provided efficient solutions, their fundamental limitation stemmed from their reliance on representations primarily optimized for in-distribution classification, often leading to struggles with "near OOD" examples that shared superficial similarities with known data. This highlighted the need for methods that actively learned OOD-discriminative features.

Recognizing the inherent limitations of purely post-hoc analysis, the field underwent a significant paradigm shift towards *training-time strategies* designed to imbue models with intrinsic OOD awareness. A cornerstone of this era was **Outlier Exposure (OE)**, which revolutionized OOD detection by incorporating auxiliary OOD data during training \cite{hendrycks2018deep}. By explicitly exposing the model to diverse outliers, OE enabled the network to learn tighter in-distribution boundaries and push OOD samples to lower confidence regions, fundamentally altering the model's decision landscape. This concept has since evolved, with methods like Mixture Outlier Exposure (MixOE) specifically targeting fine-grained OOD by generating virtual outliers through mixing in-distribution and auxiliary data, demonstrating improved robustness in challenging scenarios \cite{zhang20212tb}.

Parallel to discriminative training with outliers, research also explored *generative models* to explicitly model the in-distribution data density. Early approaches leveraged Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to estimate likelihoods, identifying OOD samples as those with low probability under the learned in-distribution model \cite{Ren_2019_Likelihood-based_Deep}. However, raw likelihoods often proved unreliable, prompting advancements in more robust density estimation. Normalizing Flows, for instance, offered a powerful way to learn complex data distributions and provide more accurate likelihoods for OOD detection \cite{zisselman2020cmx}. More recently, methods like CONJNORM have provided a unified theoretical framework for tractable density estimation within the exponential family, significantly improving post-hoc OOD performance by accurately reflecting true data densities \cite{peng20243ji}.

Further deepening the understanding of internal model behavior, the analysis of feature spaces gained prominence. While Mahalanobis distance was an early exploration \cite{Lee_2018_A_Simple_Unified}, theoretical work began to demystify *why* certain feature properties, like feature norms, correlate with OODness. For instance, it was shown that the feature norm can be theoretically linked to the maximum logit of a hidden classifier, providing a principled explanation for its OOD detection capability and demonstrating its class-agnostic nature \cite{park2023n97}. This theoretical grounding was complemented by insights into how in-distribution labels can fundamentally aid OOD detection, particularly for near-OOD samples, by facilitating better separation in the learned representation space \cite{du2024aea}. These developments underscored a move towards not just detecting OOD, but understanding the underlying mechanisms that enable such detection, often through robust training objectives like VOS that actively learn features crucial for OOD discrimination \cite{Wang_2022_VOS}.

The continuous scaling of AI models and their deployment in increasingly complex, real-world applications has ushered in a new era for OOD detection, characterized by the leveraging of powerful foundation models and the development of highly specialized and guaranteed solutions. The emergence of Vision-Language Models (VLMs) like CLIP has been particularly transformative, enabling *zero-shot* and *open-vocabulary* OOD detection capabilities that were previously unattainable. These models, pre-trained on vast amounts of multimodal data, possess a rich semantic understanding that can be leveraged to define OOD boundaries without explicit OOD examples during inference. Techniques such as Outlier Label Exposure (OLE) utilize Large Language Models (LLMs) to generate "envisioned" outlier class labels, effectively providing synthetic outlier exposure to VLMs and significantly boosting zero-shot OOD performance, especially for hard OOD cases \cite{ding20242m0, cao20246gj}. Surveys like \cite{miyai20247ro} now explore the evolving landscape of OOD in the VLM era, highlighting new challenges and opportunities.

Beyond VLMs, other advanced generative architectures, such as diffusion models, have also been adapted for OOD detection, particularly in domains with high in-distribution heterogeneity or structural similarity between in-distribution and OOD classes, like medical imaging \cite{mishra20236n9}. The field has also expanded its scope to address OOD challenges in diverse data modalities and specialized contexts, including pixel-level OOD in semantic segmentation, OOD detection in graph-structured data \cite{wang2024q01, cai2025ez2}, and in scenarios with complex class distributions like long-tailed recognition \cite{zhang20212tb}. This diversification underscores the field's maturation and its adaptation to the nuances of real-world data.

Crucially, as OOD detection moves into safety-critical domains like autonomous systems and medical diagnosis, there has been a growing emphasis on providing *formal guarantees* on detection performance. Conformal Prediction (CP) has emerged as a powerful framework to achieve this, offering provable bounds on false positive rates. Methods integrating CP with learned nonconformity measures, such as VAEs or Deep SVDD, enable real-time OOD detection in Cyber-Physical Systems (CPS) with calibrated false alarm rates \cite{cai2020lsi, kaur20248t3}. The establishment of comprehensive benchmarks, such as UB-GOLD for graph OOD \cite{wang2024q01} and OOD-Bench \cite{Huang_S_2023} for general OOD, has become vital for standardized evaluation, ensuring rigorous comparison and driving progress in these complex scenarios. This trajectory reflects a continuous drive towards more robust, generalizable, and trustworthy AI systems capable of operating reliably in open-world environments.

The historical progression of OOD research thus reveals a dynamic interplay between theoretical understanding and practical necessity. It has evolved from initial heuristic post-hoc analyses of classifier outputs to sophisticated training-time strategies that actively shape model representations for OOD discrimination. The journey has seen a continuous expansion from generic image classification to specialized contexts like graph data and multimodal inputs, culminating in the integration of powerful foundation models. This evolution has been driven by a persistent intellectual tension: balancing the desire for efficient, model-agnostic solutions with the need for robust, high-performance detectors, especially for challenging "near OOD" cases. The field has consistently adapted to the increasing complexity of AI models and real-world data, moving towards more principled uncertainty quantification and, increasingly, towards methods offering formal guarantees. While significant strides have been made, the quest for universal, adaptable, and provably reliable OOD detection in highly dynamic, open-world scenarios continues to define its future trajectory, emphasizing the ongoing need for innovative solutions and comprehensive evaluation paradigms.