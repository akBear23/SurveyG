\subsection{Mahalanobis Distance and Feature Norm Analysis}

Deep neural networks often make overconfident predictions on out-of-distribution (OOD) inputs, necessitating robust detection mechanisms. A promising avenue for OOD detection leverages the internal feature space of pre-trained neural networks, offering a geometric perspective distinct from output probabilities. This approach analyzes how OOD samples deviate in learned feature spaces, either through distance metrics, feature magnitudes, or more sophisticated density estimations.

The Mahalanobis distance is a foundational method in this domain, measuring the distance of a test sample's features from class-conditional Gaussian distributions learned for in-distribution (ID) data. This method is statistically principled, assuming that ID features for each class can be modeled by a Gaussian distribution, and OOD samples will lie far from these learned centroids in the feature space. However, its efficacy is critically dependent on the validity of this Gaussian assumption and the separability of ID/OOD clusters. A comprehensive comparative study by \cite{berger20214a3} rigorously evaluated several confidence-based OOD methods, including Mahalanobis distance, finding that while it performed strongly on standard computer vision tasks like CIFAR-10, its performance significantly degraded in medical imaging. This degradation was attributed to less separable ID/OOD clusters in the feature space that challenge the underlying Gaussian assumption, highlighting a key limitation. Furthermore, in high-dimensional feature spaces, distance-based methods, including Mahalanobis, can suffer from the "curse of dimensionality," where distances become less meaningful and discriminative, potentially leading to unreliable OOD scores \cite{ghosal2023q20, li2025jdt}.

Addressing these limitations, subsequent research has sought to refine or extend the Mahalanobis approach. \cite{anthony2023slf} conducted a deep investigation into Mahalanobis distance for medical imaging, empirically demonstrating that the optimal network depth for OOD detection is highly dependent on the specific OOD pattern. They proposed Multi-branch Mahalanobis (MBM), a novel system that employs multiple depth-specific detectors, significantly enhancing robustness by leveraging feature representations from various layers. This implicitly acknowledges that a single Gaussian model might not capture the complex, evolving feature distributions across different network depths. Extending the idea of distance in feature space while relaxing strict covariance assumptions, \cite{gomes2022zyv} introduced IGEOOD. This method employs Information Geometry, specifically the Fisher-Rao distance, to measure dissimilarity between probability distributions of latent features. By modeling these features as a mixture of Gaussian PDFs with diagonal covariance, IGEOOD provides a more flexible and robust OOD score, outperforming traditional Mahalanobis-based methods by moving beyond the restrictive full covariance matrix assumption. To directly combat the curse of dimensionality, \cite{ghosal2023q20} proposed Subspace Nearest Neighbor (SNN), a framework that regularizes the model and its feature representation by leveraging the most relevant subset of dimensions (i.e., a subspace). This subspace learning yields more distinguishable distance measures between ID and OOD data, offering a critical improvement for distance-based methods in high-dimensional settings. Similarly, \cite{li2025jdt} introduced a novel "tangent distance" that mitigates the sensitivity of distances to high dimensionality by mapping features to the manifold of ID samples. This approach, inspired by manifold learning, computes the Euclidean distance between samples and the nearest submanifold space, providing a data structure-aware distance that is more meaningful than direct Euclidean or Mahalanobis distances in high-dimensional feature spaces.

Beyond explicit distance metrics, the magnitude or "norm" of feature representations provides another valuable, computationally efficient signal for OOD detection. The core observation is that OOD samples often exhibit different feature magnitudes compared to in-distribution data. \cite{yu2022egq} introduced \texttt{FeatureNorm}, defined as the channel-wise averaged L2-norm of rectified feature maps. Their work revealed that intermediate blocks, rather than just the last layer, often provide superior OOD separation, attributing this to the "overconfidence issue" in deeper layers where features might become less discriminative for OOD. To optimize this, they proposed \texttt{NormRatio} which uses pseudo-OOD samples for optimal block selection. Complementing this, \cite{dong2021swz} developed Neural Mean Discrepancy (NMD), which quantifies the deviation of an input's average activations (neural mean) from the training data's neural mean across multiple layers. NMD leverages Batch Normalization statistics for efficiency and demonstrates that OOD samples consistently exhibit larger deviations, even with small batch sizes, making it a robust norm-based indicator. These methods highlight that simple magnitude differences in feature space can serve as effective, yet computationally light, OOD indicators, particularly when carefully chosen from appropriate network layers.

While Mahalanobis distance implicitly relies on Gaussian density estimation, more explicit and flexible density estimation techniques in the feature space offer a powerful alternative, overcoming the restrictive assumptions of simple parametric models. Normalizing Flows (NFs) have emerged as a promising tool for this purpose. \cite{cook2024hyb} investigated the use of feature density estimation via normalizing flows for OOD detection, presenting a fully unsupervised, post-hoc approach. By training a lightweight auxiliary NF model on the features of in-distribution data, they perform OOD detection via density thresholding. This method demonstrates strong results for far-OOD data, showcasing the ability of NFs to model complex, non-Gaussian feature distributions more accurately than simple Gaussian assumptions, thus providing a more robust measure of typicality.

Further advancing tractable density estimation, \cite{peng20243ji} introduced CONJNORM, a novel theoretical framework grounded in Bregman divergence that unifies various post-hoc OOD approaches by considering an expansive exponential family of distributions. CONJNORM reframes density function design as a search for the optimal norm coefficient for a given dataset, and crucially, devises an unbiased and analytically tractable estimator for the partition function using Monte Carlo-based importance sampling. This overcomes the computational intractability and reliance on strong distributional assumptions (like Gaussian) that plague many density-based methods, including the implicit Gaussian assumption of Mahalanobis distance.

Another approach to typicality, moving beyond direct likelihoods, is the Density of States Estimator (DoSE) proposed by \cite{morningstar2020re9}. DoSE addresses the critical issue where raw likelihoods from generative models can assign higher probabilities to OOD data in high dimensions. Instead of comparing model probabilities directly, DoSE leverages the "probability of the model probability" or, more generally, the frequency (typicality) of *multiple summary statistics* derived from a generative model's features. By training non-parametric density estimators (e.g., Kernel Density Estimation or one-class Support Vector Machine) on these statistics, DoSE identifies OOD samples as those with low typicality across these combined metrics. This modular, unsupervised method provides a robust alternative to direct likelihood or distance comparisons by focusing on the multi-faceted typicality of samples in the feature space, making it highly effective even when simple likelihoods fail.

The methods leveraging feature space for OOD detection present a spectrum of approaches, each with distinct strengths and weaknesses. Mahalanobis distance, while statistically grounded, is highly sensitive to its core assumption of class-conditional Gaussian feature distributions and can be hampered by the curse of dimensionality in high-dimensional, complex feature spaces \cite{berger20214a3, ghosal2023q20}. Its performance can degrade significantly when ID and OOD clusters are not well-separated or when the true data distribution deviates from Gaussianity. Improvements like MBM \cite{anthony2023slf} and IGEOOD \cite{gomes2022zyv} directly tackle these issues by adapting to network depth or relaxing strict covariance assumptions, respectively, thereby enhancing the flexibility of distance-based metrics. Furthermore, SNN \cite{ghosal2023q20} and tangent distance \cite{li2025jdt} offer crucial advancements by explicitly addressing the curse of dimensionality, making distance metrics more robust and meaningful by operating in learned subspaces or manifolds.

Feature norm-based methods, such as \texttt{FeatureNorm} \cite{yu2022egq} and NMD \cite{dong2021swz}, offer a computationally lighter alternative. They exploit the observation that OOD samples often manifest as deviations in feature magnitudes, providing an efficient signal. However, their simplicity means they might miss more subtle OOD patterns that do not primarily manifest as magnitude changes. They also require careful selection of the optimal layer or block for feature extraction, as deeper layers can sometimes become less discriminative due to overconfidence.

More advanced density estimation techniques, including Normalizing Flows \cite{cook2024hyb}, CONJNORM \cite{peng20243ji}, and DoSE \cite{morningstar2020re9}, represent a significant leap. They move beyond the restrictive parametric assumptions of Mahalanobis by modeling complex, non-Gaussian feature distributions more accurately. Normalizing Flows offer a flexible, unsupervised way to learn the true ID feature density, while CONJNORM provides a principled, tractable framework for density estimation across a broader family of distributions, overcoming the challenges of partition function estimation. DoSE, by focusing on the typicality of multiple summary statistics, robustly addresses the counter-intuitive high-likelihood-for-OOD problem that can plague direct likelihood methods. These advanced density-based approaches generally offer superior performance and robustness compared to simpler distance or norm-based methods, but often come with increased computational complexity for training the auxiliary density models. The choice between these methods often involves a trade-off between computational efficiency, model flexibility, and the specific characteristics of the OOD problem (e.g., near-OOD vs. far-OOD).

The literature on Mahalanobis distance and feature norm analysis, alongside more advanced feature-space density estimation, highlights a continuous effort to exploit the geometric and statistical properties of learned feature representations for robust OOD detection. The evolution from the foundational Mahalanobis distance, with its inherent Gaussian assumptions and susceptibility to high dimensionality, to more flexible distance metrics, subspace learning, and sophisticated density estimation techniques, demonstrates a clear progression towards more accurate and robust uncertainty quantification. A persistent challenge remains in developing methods that are universally robust across diverse OOD types and domains, particularly in safety-critical applications where the assumptions of feature distribution or separability may not hold, or where the "curse of dimensionality" continues to obscure meaningful distinctions. Future directions could involve integrating these geometric and density-based insights with more adaptive learning paradigms, exploring novel feature transformations that inherently maximize ID-OOD separation during training, or developing hybrid approaches that combine the efficiency of norm-based methods with the robustness of advanced density estimators. Furthermore, the rigorous evaluation of these scores using frameworks like conformal prediction, as highlighted by \cite{novello2024yco}, will be crucial to provide provable guarantees on their performance, moving beyond empirical observations to verifiable assurances for real-world deployment.