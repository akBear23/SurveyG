\subsection{Leveraging Vision-Language Models (VLMs) and Multimodal OOD}

The advent of large pre-trained Vision-Language Models (VLMs) and other foundation models has instigated a paradigm shift in Out-of-Distribution (OOD) detection, enabling zero-shot and open-vocabulary capabilities by moving towards a more human-like conceptual understanding of novelty. This section explores cutting-edge techniques that leverage the rich semantic understanding and vast pre-training of these models to define OOD boundaries without explicit OOD data.

Initial efforts to integrate VLMs into OOD detection focused on leveraging their inherent representational power. \cite{miyai2023591} introduced GL-MCM, which utilized both global and local features from models like CLIP for zero-shot OOD detection. This approach addressed the challenge of global features being contaminated by OOD objects in complex, multi-object images, providing flexibility in defining what constitutes an in-distribution (ID) image. However, these early methods often lacked explicit knowledge about OOD samples, leading to higher false positive rates.

To inject OOD awareness without relying on actual OOD images, subsequent research explored prompt-based learning. \cite{ding20242m0} proposed Outlier Label Exposure (OLE), which used diverse auxiliary outlier *class labels* as pseudo OOD text prompts for VLMs. OLE learned pivotal outlier prototypes through unsupervised clustering and filtering, and further generated "hard" outlier prototypes by mixing fringe ID embeddings with refined outlier prototypes, thereby enhancing decision boundary calibration. Building on this, \cite{li20245b6} introduced NegPrompt, a method that learns *transferable negative prompts* for each ID class using *only* ID training data. These negative prompts encapsulate characteristics contrary to ID classes, allowing OOD samples to exhibit higher similarity to them than to positive prompts, enabling truly open-vocabulary OOD detection without external outlier data or additional encoders. This marked a significant advancement in defining OOD boundaries purely from ID knowledge and VLM semantics. Further extending this conceptual understanding, \cite{cao20246gj} presented Envisioning Outlier Exposure (EOE), which leveraged Large Language Models (LLMs) to *envision* potential outlier class labels based on visual similarity to ID classes. By designing task-specific LLM prompts, EOE generated synthetic outlier exposure for VLMs, effectively addressing the practical unavailability of true OOD data and significantly boosting zero-shot OOD performance across far, near, and fine-grained OOD scenarios.

While prompt-based methods showed great promise, challenges remained in refining their application. \cite{yu20249dd} addressed the issue of "spurious OOD features" arising from VLMs' imperfect foreground-background decomposition, which could lead to unreliable OOD features. Their Self-Calibrated Tuning (SCT) method introduced adaptive modulating factors during prompt tuning that dynamically adjusted ID classification and OOD regularization losses based on prediction uncertainty, mitigating the impact of unreliable OOD features without introducing extra hyperparameters.

The scope of OOD detection with foundation models has also expanded to multimodal settings. \cite{dong2024a8k} pioneered multimodal OOD detection by introducing the MultiOOD benchmark and identifying "Modality Prediction Discrepancy" (MPD) as a key signal, where OOD samples exhibit higher inter-modal prediction variability. Their Agree-to-Disagree (A2D) algorithm explicitly amplified this discrepancy during training, encouraging agreement on ground-truth classes and disagreement on others. Building on MPD, \cite{li2024rs5} proposed Dynamic Prototype Updating (DPU) to address the overlooked issue of intra-class variability in multimodal OOD. DPU dynamically adjusted prediction discrepancy intensification based on each sample's similarity to its class prototype, ensuring that only samples distant from their class center experienced amplified discrepancies, thus balancing OOD sensitivity with ID accuracy. Furthermore, \cite{dai2023mhn} integrated LLM capabilities into multimodal OOD by leveraging LLM-derived "world knowledge" as descriptive features. A crucial aspect of their work was a consistency-based uncertainty calibration method for LLM-generated descriptors, mitigating hallucinations and ensuring that only high-confidence knowledge was selectively used to augment multimodal class representations for OOD scoring.

Beyond VLMs, other foundation models are also being repurposed for OOD detection. \cite{gao2023kmk} introduced DiffGuard, which utilized pre-trained diffusion models for semantic mismatch-guided OOD detection. By synthesizing images conditioned on the input and its predicted label, DiffGuard identified OOD samples through dissimilarity between original and synthesized images, overcoming scalability issues of prior generative methods. The growing adoption of these powerful models has also necessitated specialized evaluation; \cite{borlino20245ku} introduced a new benchmark to rigorously assess the performance of large pre-trained models for OOD detection, confirming their inherent generalization benefits while identifying scenarios where task-specific fine-tuning remains optimal. A comprehensive overview of this evolving landscape is provided by \cite{miyai20247ro}, which clarifies the definitions and interrelations of OOD detection and related fields in the VLM era, proposing "Generalized OOD Detection v2" to guide future research.

In conclusion, the integration of VLMs and other foundation models has profoundly reshaped OOD detection, shifting from data-intensive, closed-set approaches to zero-shot, open-vocabulary capabilities driven by rich semantic understanding. Techniques like negative prompt learning, self-calibrated tuning, and leveraging inter-modal prediction discrepancies demonstrate a significant move towards more human-like conceptual understanding of novelty without explicit OOD data. Future research must focus on enhancing the robustness of these models to subtle shifts, developing efficient and adaptive tuning strategies, and exploring the ethical implications of LLM-generated OOD knowledge to ensure trustworthy AI in increasingly open-world scenarios.