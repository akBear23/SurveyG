\subsection{Conformal Prediction and Guaranteed OOD Detection}

The deployment of deep learning models in safety-critical applications necessitates robust mechanisms for detecting out-of-distribution (OOD) data, moving beyond mere empirical performance to verifiable statistical assurances. Traditional OOD detection methods often rely on heuristic scoring functions, which, despite achieving high Area Under the Receiver Operating Characteristic Curve (AUROC) scores, may not reliably identify actual model errors or provide guarantees on false positive rates (FPR) \cite{guerin202201y}. This critical gap highlights the need for frameworks that offer provable bounds on error rates, ensuring trustworthiness and reliability.

Conformal Prediction (CP) and its variant, Inductive Conformal Anomaly Detection (ICAD), provide a statistically rigorous framework for OOD detection by offering finite-sample guarantees on false detection rates, irrespective of the underlying data distribution, assuming exchangeability. Early work by \textcite{cai2020lsi} demonstrated the application of ICAD for real-time OOD detection in learning-enabled Cyber-Physical Systems (CPS). They addressed the challenge of high-dimensional inputs by proposing learned nonconformity measures based on Variational Autoencoders (VAEs) and Deep Support Vector Data Description (SVDD), which efficiently compute scores without storing the entire training dataset. This approach ensures a well-calibrated false alarm rate, a crucial property for safety-critical CPS.

Building upon the ICAD framework, \textcite{kaur2022cty} introduced iDECODe, a method that leverages in-distribution equivariance as a novel non-conformity measure. By quantifying the error in expected behavior under various transformations, iDECODe provides a robust aggregated non-conformity score. This approach offers a theoretical guarantee that the probability of false OOD detection is upper-bounded by a user-defined significance level, further strengthening the provable bounds on false positive rates for single-point OOD detection. Extending these guarantees to more complex, real-world scenarios, \textcite{kaur20248t3} tackled OOD detection in dependent time-series data for CPS. Their method, CODiT, utilizes deviations from in-distribution temporal equivariance as a non-conformity measure within the conformal framework, combining predictions from multiple detectors using Fisher's method to achieve bounded false alarms for both fixed-length windows and variable-length traces. This addresses a significant limitation of earlier CP-based methods that often assume independent and identically distributed data.

While conformal prediction offers strong theoretical guarantees, practical deployment often encounters dynamic OOD shifts and the challenge of setting appropriate thresholds that minimize human intervention while maintaining safety. To address the pervasive issue of unacceptably high false positive rates in real-world OOD detection, \textcite{vishwakarma2024z1m} proposed a mathematically grounded human-in-the-loop framework. This adaptive system dynamically updates the detection threshold using expert feedback and leverages the Law of Iterated Logarithm (LIL) to provide anytime-valid upper confidence bounds on the FPR. This ensures that the FPR remains below a desired level at all times, even in the presence of distribution shifts, thereby taming false positives and maximizing true positive rates in practical deployments.

Complementing these conformal approaches, other methods also aim for reliable uncertainty quantification to achieve controlled error rates. For instance, \textcite{aguilar2023ms5} integrated Evidential Deep Learning (EDL) into a continual learning framework for simultaneous incremental classification and OOD detection. While not providing conformal guarantees, their Continual Evidential Deep Learning (CEDL) method focuses on robust uncertainty estimation (vacuity and dissonance) in dynamic, open-world settings, demonstrating superior OOD detection performance compared to post-hoc methods in continual learning scenarios. This highlights alternative avenues for building trustworthy AI systems that can adapt to evolving data distributions while maintaining controlled error rates.

In conclusion, the integration of conformal prediction and related uncertainty quantification techniques marks a significant shift towards building OOD detection systems with verifiable assurances, moving beyond purely empirical performance metrics. From early applications in CPS with learned nonconformity measures \cite{cai2020lsi} to refined nonconformity scores for single points \cite{kaur2022cty} and extensions to dependent time-series data \cite{kaur20248t3}, the field is continuously enhancing the scope and robustness of statistical guarantees. Furthermore, adaptive human-in-the-loop systems \cite{vishwakarma2024z1m} are crucial for managing false positives in dynamic real-world environments. Future research will likely focus on scaling these guarantees to more complex models and data modalities, integrating diverse uncertainty quantification methods, and developing more sophisticated adaptive mechanisms for seamless and safe deployment in highly dynamic and safety-critical domains.