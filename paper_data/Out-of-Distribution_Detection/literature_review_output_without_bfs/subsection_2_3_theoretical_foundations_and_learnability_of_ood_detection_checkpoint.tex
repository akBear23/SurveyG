\subsection{Theoretical Foundations and Learnability of OOD Detection}

The development of robust and trustworthy Out-of-Distribution (OOD) detection systems necessitates a deep theoretical understanding of its learnability and the fundamental conditions under which it can be provably effective. Moving beyond purely empirical observations, this subsection delves into the mathematical and conceptual underpinnings that govern OOD phenomena, providing a principled framework for understanding existing methods and guiding the creation of more resilient algorithms.

A foundational inquiry in this domain concerns the very learnability of OOD detection. \cite{fang20249gd} provides a comprehensive theoretical framework, investigating the Probably Approximately Correct (PAC) learnability of OOD detection. Their work establishes necessary and sufficient conditions for learnability under various risk and AUC metrics, crucially revealing impossibility theorems. These theorems demonstrate that OOD detection is not universally learnable, particularly when in-distribution (ID) and OOD data exhibit significant overlap or when the hypothesis space is too rich. This theoretical grounding highlights the inherent challenges in designing a single, universally effective OOD detector, suggesting that learnability is highly contingent on the characteristics of the data distributions and the chosen model class. Complementing this, \cite{guerin202201y} critically questions whether OOD detection, as conventionally defined by distributional shifts, is the most appropriate objective for safety-critical systems. They propose "Out-of-Model-Scope (OMS) detection" as a more unambiguous goal, focusing on identifying inputs that lead to model errors rather than merely distributional shifts. This redefinition directly impacts the theoretical target of learnability, shifting the focus from statistical divergence to predictive reliability.

The intrinsic characteristics of learned representations and the nature of training data profoundly impact OOD learnability and the reliability of detection signals. \cite{ming2021wu7} provides a rigorous analysis of how spurious correlations in training data detrimentally affect OOD detection. They formalize "spurious OOD" and demonstrate that models relying on non-causal features can exhibit arbitrarily high confidence on such OOD inputs, making them inherently difficult to detect. This theoretical insight explains why models often fail catastrophically on OOD data that shares superficial similarities with ID data, fundamentally limiting learnability in these scenarios. Furthermore, the continuous nature of OOD boundaries, as highlighted by the "Sorites Paradox" in OOD evaluation \cite{long2024os1}, poses a fundamental theoretical challenge to current binary classification-based OOD approaches. This suggests that the crisp binary decision of ID/OOD might be an oversimplification, complicating the theoretical guarantees of any hard-thresholding method.

To understand why certain OOD signals *do* work despite these challenges, theoretical explanations for empirically observed phenomena are crucial. \cite{park2023n97} offers a theoretical explanation for the efficacy of feature norms, proving that the L1-norm of a hidden layer feature vector converges to the maximum logit of a hidden classifier. This links feature norm to classifier confidence and establishes its class-agnostic nature for OOD detection, providing a mathematical basis for a widely used heuristic. Extending this, \cite{ammar2023pr1} introduces and empirically validates the "ID/OOD Orthogonality (NC5)" property of Neural Collapse, showing that OOD data tends to become orthogonal to ID data's class means during training. This provides a geometric explanation for feature-based OOD separation, suggesting that deep networks naturally learn to push OOD samples into distinct subspaces. Beyond feature norms, \cite{gomes2022zyv} introduces IGEOOD, leveraging information geometry, specifically the Fisher-Rao distance, to measure dissimilarity between probability distributions in both softmax output and latent feature spaces. This provides a unified, theoretically grounded metric that adapts to various levels of model access, offering a robust alternative to simpler distance metrics. Similarly, \cite{chen2023za1} proposes GAIA, which quantifies "abnormality" in gradient-based attribution results, providing a novel theoretical perspective that OOD samples elicit divergent and meaningless explanation patterns, which can be leveraged as a powerful OOD signal.

The ability to leverage auxiliary data sources also plays a crucial role in enhancing OOD learnability, with recent work providing formal guarantees. \cite{du20248xe} makes a significant theoretical contribution by providing the first framework with *provable guarantees* for using unlabeled "wild" data in OOD detection. Their SAL (Separate And Learn) framework employs a novel gradient-based filtering mechanism to identify candidate outliers from noisy unlabeled data, and their theorems quantify the separability and learnability of the OOD classifier, demonstrating how unlabeled data can provably help in specific settings. Complementing this, \cite{du2024aea} formally investigates the role of in-distribution (ID) labels, using a graph-theoretic approach to derive a provable error bound that compares OOD detection performance with and without ID labels. Their work identifies specific conditions under which ID labels are most beneficial, particularly for "near-OOD" scenarios where semantic information is critical.

Beyond traditional likelihoods, generative models offer new theoretical avenues for OOD detection. \cite{morningstar2020re9} addresses the critical failure of direct likelihood comparison in high dimensions, where deep generative models often assign higher likelihoods to OOD data. Their Density of States Estimator (DoSE) shifts the focus from raw likelihoods to the "typicality" of multiple summary statistics derived from the generative model, inspired by statistical physics. This approach provides a theoretical understanding of the shortcomings of MLE-fitted distributions for single-sample OOD detection through a bias/variance tradeoff for typicality. Building on generative models, \cite{heng2024fjd} introduces Diffusion Paths (DiffPath), a novel approach that leverages a *single* unconditional diffusion model. Instead of relying on likelihoods, DiffPath measures the rate-of-change and curvature of diffusion paths, connecting a sample to a standard normal distribution. This work provides a theoretical framework linking these OOD statistics to derivatives of optimal transport paths, demonstrating that a single generative model can detect OOD across diverse, unseen inlier distributions, a significant theoretical and practical advancement.

In conclusion, the theoretical foundations of OOD detection are rapidly evolving, moving from an initial focus on empirical performance to a deeper, principled understanding of its learnability and the factors that influence it. While foundational work highlights the inherent impossibility of universal OOD learnability \cite{fang20249gd}, subsequent research has carved out specific conditions where provable success is possible, whether through understanding intrinsic model behaviors \cite{park2023n97, ammar2023pr1, gomes2022zyv, chen2023za1} or leveraging auxiliary data with theoretical backing \cite{du20248xe, du2024aea}. Challenges remain in achieving universal learnability across all OOD types, particularly in the presence of spurious correlations \cite{ming2021wu7} and the ambiguity of OOD boundaries \cite{long2024os1}. However, the increasing integration of rigorous statistical frameworks, such as Conformal Prediction, offers a promising direction for providing formal guarantees on OOD detection performance, a topic explored in detail in Section 7.1 \cite{novello2024yco}. This ongoing theoretical grounding is vital for ensuring scientific rigor and guiding the development of truly robust and provably safe OOD systems in complex, dynamic real-world environments.