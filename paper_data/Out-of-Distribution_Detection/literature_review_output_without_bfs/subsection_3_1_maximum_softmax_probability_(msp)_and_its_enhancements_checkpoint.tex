\subsection{Maximum Softmax Probability (MSP) and its Enhancements}

The foundational challenge of Out-of-Distribution (OOD) detection, critical for the safe deployment of machine learning models, found a pioneering and surprisingly effective baseline in the Maximum Softmax Probability (MSP) \cite{Hendrycks_G_2017}. This straightforward approach leverages the output of a pre-trained neural network classifier, positing that in-distribution (ID) samples, which align with the model's training data, should elicit high confidence in one of the known classes. Conversely, OOD samples, being unfamiliar, are expected to yield lower confidence scores. \cite{Hendrycks_G_2017} empirically demonstrated MSP's utility as a simple yet powerful OOD indicator, establishing it as a widely adopted benchmark for subsequent, more complex detection methods.

Despite its conceptual simplicity and computational efficiency, MSP suffers from inherent limitations. Deep neural networks are frequently observed to exhibit overconfidence, assigning high softmax probabilities even to OOD inputs that lie far from the training data manifold \cite{Ren_J_2019}. This phenomenon, where models confidently misclassify novel inputs, underscores MSP's inadequacy as a standalone OOD score in many real-world scenarios. Furthermore, while MSP provides a basic measure of predictive uncertainty, it often fails to capture the nuanced epistemic uncertainty associated with truly novel inputs, leading to its performance being reliably outperformed by methods leveraging learned representations \cite{kuan2022qzl}. This critical observation highlighted the need for enhancements that could amplify the discriminative power of MSP.

A significant advancement in this direction was the introduction of Out-of-Distribution Detector for Neural Networks (ODIN) by \cite{Liang_L_2018}. ODIN enhances MSP through two key post-processing techniques: temperature scaling and input perturbation. Temperature scaling involves dividing the logits (pre-softmax outputs) by a scalar parameter $T > 1$. This process softens the probability distribution, making the model less overconfident and improving the calibration of confidence scores. Input perturbation, on the other hand, is a gradient-based method where the input image is slightly perturbed in a direction that maximizes the softmax probability of the predicted class. Specifically, it applies a small adversarial perturbation (e.g., using a single-step gradient ascent similar to Fast Gradient Sign Method, FGSM) to push the input closer to the decision boundary of its predicted class. For ID samples, this perturbation typically increases the maximum softmax probability, while for OOD samples, it often leads to a less pronounced increase or even a decrease, effectively exaggerating the difference between ID and OOD samples in the confidence space. Empirical studies have shown that ODIN's effectiveness is primarily attributed to this input perturbation mechanism, which significantly enhances the separation of ID and OOD samples in the feature space \cite{berger20214a3}. These techniques, applied post-training without requiring model retraining, transformed MSP into a more robust OOD score, demonstrating substantial performance improvements across various benchmarks \cite{Liang_L_2018}.

Building upon ODIN, \cite{Hsu_Y_2020} proposed Generalized ODIN, which further refines the input perturbation strategy by leveraging batch normalization statistics. This approach enhances the discriminative power without requiring access to OOD data during the enhancement process, making it more practical in scenarios where OOD examples are scarce or unknown. The core principle behind these post-processing methods is their ability to manipulate the model's output space, making the confidence scores of ID samples even higher and those of OOD samples even lower, thereby creating a clearer separation for OOD detection.

Despite their empirical success, ODIN and its variants introduce practical challenges. A major limitation is the requirement to tune the hyperparameters, specifically the temperature $T$ and the perturbation magnitude $\epsilon$, which typically necessitates a validation set of OOD samples \cite{Liang_L_2018}. This contradicts the purely post-hoc, no-OOD-data-needed appeal of the method, as representative OOD validation data is often unavailable in real-world open-set scenarios. Furthermore, the stability of OOD detection performance, even for methods like MSP and ODIN, can fluctuate significantly during neural network training, exhibiting overfit-like behavior and instability in later stages \cite{cheng20233yi}. To address this, methods like Average of Pruning (AoP) have been proposed, which combine model averaging and network pruning to improve the stability and performance of OOD detection scores, including those derived from MSP and ODIN, by mitigating overfitting and finding more robust optima \cite{cheng20233yi}.

Further refinements have sought to improve the core components of ODIN. For instance, Adaptive Temperature Scaling (ATS) dynamically calculates a sample-specific temperature value based on activations of intermediate layers, fusing this adjustment with class-dependent logits to capture additional statistical information before it is lost in the feature extraction process \cite{krumpl2024n1w}. This approach enhances the robustness and power of temperature scaling. Similarly, the principles of temperature-scaled softmax probabilities and FGSM-style input perturbation have been integrated into more principled frameworks, such as Information Geometry approaches, which leverage Fisher-Rao distance to measure dissimilarity between probability distributions for OOD scoring \cite{gomes2022zyv}. These methods demonstrate how the fundamental ideas introduced by MSP and ODIN continue to inspire more theoretically grounded and robust OOD detection techniques.

The enduring legacy of MSP and its subsequent enhancements is profound. They established a strong, efficient, and widely adopted baseline that laid the groundwork for the entire category of post-hoc OOD detection methods. These techniques demonstrated that significant improvements could be achieved through clever manipulation of a pre-trained model's outputs, inspiring further research into how model outputs and internal representations could be better leveraged. However, their reliance on heuristic adjustments and the need for OOD validation data for hyperparameter tuning highlighted the need for more principled and data-agnostic scoring functions. This paved the way for the exploration of alternative post-hoc methods, such as energy-based scores, which offer a more theoretically grounded approach to OOD detection, as discussed in the following subsection.