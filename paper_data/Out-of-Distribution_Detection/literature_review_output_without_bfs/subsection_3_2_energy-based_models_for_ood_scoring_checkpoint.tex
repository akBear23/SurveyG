\subsection*{Energy-Based Models for OOD Scoring}

Traditional methods for out-of-distribution (OOD) detection often rely on confidence scores derived from standard discriminative classifiers, such as the maximum softmax probability (MSP) \cite{Hendrycks_G_2017}. While empirically effective and further enhanced by post-hoc techniques like temperature scaling and input perturbation in methods like ODIN \cite{Liang_L_2018}, these approaches frequently lack a strong theoretical foundation for quantifying uncertainty and can be sensitive to heuristic adjustments. This limitation has spurred research into more principled alternatives, with Energy-Based Models (EBMs) emerging as a robust framework for OOD scoring.

Energy-Based Models offer a principled approach to modeling data distributions by assigning an 'energy' score to each input, where lower energy corresponds to in-distribution (ID) samples and higher energy to OOD samples. Theoretically, EBMs define a probability distribution over inputs $x$ using an energy function $E(x; \theta)$ as $p(x) = \frac{\exp(-E(x; \theta))}{Z}$, where $Z$ is the intractable normalization constant (partition function). This formulation implies that inputs with low energy are assigned high probability, aligning naturally with the goal of OOD detection: ID samples should reside in low-energy regions, while OOD samples should be pushed to high-energy regions.

A seminal work in this area is \cite{Liu_Y_2020}, which introduced a direct application of EBMs for OOD detection. This approach leverages the logits of a pre-trained neural network classifier to define the energy function, typically as the negative log-sum-exp of the logits. Crucially, \cite{Liu_Y_2020} proposed a novel training objective that explicitly fine-tunes the model to optimize OOD discrimination. This objective combines the standard cross-entropy loss for ID classification with a contrastive term that pushes the energy of ID samples below a certain margin and the energy of OOD samples (drawn from an auxiliary OOD dataset) above another margin. By directly optimizing this energy function, the model learns to assign distinct energy values to ID and OOD data, providing a more calibrated and robust measure of uncertainty than raw softmax probabilities.

The strength of EBMs for OOD scoring lies in their ability to explicitly shape the energy landscape during training. Unlike methods that merely interpret or adjust existing classifier outputs, EBMs are trained or fine-tuned to directly learn an energy function that is discriminative for OOD samples. This direct optimization for OOD detection represents a significant step towards more theoretically grounded post-hoc scoring, moving beyond heuristic adjustments. The resulting energy scores offer a more robust and calibrated measure of uncertainty, as they are not merely byproducts of a classification task but are learned specifically to reflect the distributional anomaly of an input.

While EBMs provide a powerful framework, their effectiveness can depend on the quality and diversity of the auxiliary OOD dataset used during training to shape the energy landscape. Furthermore, the computational cost associated with training EBMs, particularly those involving sampling from the energy function, can be a consideration. Nevertheless, the principled nature of energy-based scoring offers a compelling direction for developing more reliable and trustworthy AI systems, paving the way for future research into more efficient training strategies and broader applications across diverse OOD scenarios.