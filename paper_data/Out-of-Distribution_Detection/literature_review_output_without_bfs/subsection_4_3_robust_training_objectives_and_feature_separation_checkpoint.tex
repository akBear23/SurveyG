\subsection{Robust Training Objectives and Feature Separation}
Moving beyond post-hoc analysis, a critical line of research in Out-of-Distribution (OOD) detection focuses on fundamentally altering the model's training process to learn representations inherently more robust to OOD data or to explicitly separate in-distribution (ID) and OOD features. These methods aim to bake OOD discrimination directly into the model's internal representations from the outset, often leading to more resilient and reliable systems. This subsection explores approaches that introduce specialized loss functions, leverage geometric properties like Neural Collapse, or employ adversarial training techniques to achieve this goal.

A significant direction involves enforcing explicit geometric separation in the feature space. Deep neural networks, particularly in the final layers, exhibit a phenomenon known as Neural Collapse (NC), where features of ID samples within a class converge to their class mean, and these class means are maximally separated \cite{wu20242p3}. OOD detection methods can leverage this property by constraining OOD features to lie in a distinct region, orthogonal to the collapsed ID feature space. For instance, \cite{wu20242p3} introduces a novel "Separation Loss" and "Clustering Loss" during fine-tuning. The Separation Loss explicitly pushes OOD features into a subspace orthogonal to the principal subspace spanned by the final layer weights, thereby ensuring ID and OOD features are separated along different dimensions. Simultaneously, the Clustering Loss enhances ID feature compactness, reinforcing the Neural Collapse phenomenon for ID classes. This approach offers a principled way to achieve feature separation, moving beyond simply increasing output discrepancies. Similarly, \cite{feng2024ma3} proposes CASE, a method that explicitly improves intra-class compactness and inter-class separability of feature embeddings. CASE employs a dual-loss framework, including a separability loss to maximize Euclidean distance between class centers and a compactness loss to minimize intra-class distances, with class centers treated as learnable parameters. By optimizing these geometric properties, CASE enhances the intrinsic separability between ID and OOD samples.

Adversarial training represents another powerful paradigm for learning robust representations, particularly against various distribution shifts and malicious perturbations. Traditional OOD detectors often prove brittle when faced with adversarial examples, whether they are perturbed ID inputs (leading to false rejections as OOD) or perturbed OOD inputs (leading to false acceptances as ID). To address this, \cite{chen2020mbk} introduces ALOE (Adversarial Learning with inlier and Outlier Exposure), a pioneering method that robustifies OOD detectors against both types of adversarial shifts. ALOE formulates the training as a min-max game, where the model is exposed to adversarially crafted inlier examples (to prevent false rejections) and adversarial outlier examples (to prevent false acceptances). This unified approach fundamentally alters the model's decision boundary, making it more robust to subtle, yet critical, shifts. In the context of dense prediction tasks, \cite{besnier2021jgn} proposes ObsNet+LAA for semantic segmentation. This method trains an auxiliary module using local adversarial attacks to generate OOD-like data, which then enables fast and accurate pixel-wise OOD detection without modifying the main segmentation network. The core idea is to teach the auxiliary module to identify patterns that resemble OOD data, even if synthetically generated through adversarial perturbations, thereby enhancing its discriminative power during inference.

Beyond geometric constraints and adversarial robustness, specialized loss functions are designed to explicitly optimize for OOD discrimination during training. Recognizing that auxiliary OOD data, when used, can suffer from class imbalance, \cite{choi202367m} proposes a balanced energy regularization loss. This loss adaptively applies stronger regularization to auxiliary samples from majority classes, leveraging a `ZÎ³` term that quantifies the likelihood of an auxiliary sample belonging to a majority class. By dynamically adjusting the loss margin and weight based on this term, the method ensures a more effective and balanced shaping of the energy landscape for OOD detection, leading to superior performance across various tasks. Another approach, explored by \cite{chen20247p7}, investigates feature sparsity for OOD detection. They propose a sparsity-regularized (SR) tuning framework that aims to enlarge the distinguishability between ID and OOD features by promoting sparsity. This modification simplifies the original training loss and enhances the model's adaptive ability and detection performance, demonstrating that feature properties beyond simple magnitude can be leveraged. For pixel-wise OOD detection, \cite{liu2022fdj} presents Residual Pattern Learning (RPL), an external module trained with contrastive learning and a positive energy loss. This module learns residual anomaly patterns from intermediate features, inducing high uncertainty for OOD pixels without requiring retraining of the base segmentation model. The contrastive objective explicitly pushes OOD features away from ID features, while the energy loss encourages low energy for ID and high energy for OOD.

While these robust training objectives and feature separation techniques have made significant strides, challenges persist. Many methods, particularly those leveraging auxiliary data (even with specialized losses), still rely on the availability and quality of such data, which may not always be representative of truly unknown OOD samples. The trade-off between improving OOD detection and maintaining in-distribution classification accuracy remains a critical area, as overly aggressive feature separation could degrade ID performance. Furthermore, the generalizability of geometric priors, like Neural Collapse, to highly diverse or complex OOD scenarios requires further investigation. Future research could focus on developing more data-efficient and adaptive methods for learning OOD-robust representations, exploring theoretical frameworks that unify representation learning and OOD robustness, and designing objectives that are less sensitive to the specific characteristics of auxiliary OOD data.