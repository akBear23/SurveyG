\subsection{Historical Characterization and Evolving Taxonomies}
The challenge of hallucination in natural language generation (NLG) has evolved from an observed anomaly to a systematically categorized and theoretically bounded problem. Initial empirical studies laid the groundwork for understanding and classifying these errors, providing the intellectual lineage for subsequent research and guiding early detection efforts across diverse tasks.

Pioneering work in abstractive summarization first systematically identified and distinguished different types of unfaithful content. \cite{maynez2020h3q} conducted a large-scale human evaluation, categorizing hallucinations into "intrinsic" (misrepresenting source information) and "extrinsic" (adding uninferable information). This study not only quantified the high prevalence of hallucinations (over 70\% of summaries) but also highlighted the inadequacy of traditional metrics like ROUGE, advocating for semantically-aware evaluation. Building on this foundational understanding, researchers began to develop more comprehensive taxonomies to classify hallucinations by their source, type, and severity, particularly with the advent of Large Language Models (LLMs).

As LLMs became more prevalent, the scope of hallucination expanded beyond summarization. \cite{zhang2023k1j} introduced a broader taxonomy for LLMs, distinguishing between "input-conflicting" (diverging from user input), "context-conflicting" (inconsistent with prior generated text), and "fact-conflicting" (misaligning with world knowledge) hallucinations. This framework emphasized the unique challenges posed by LLMs' scale and versatility. Further deepening the understanding of hallucination sources, \cite{du2023qu7} proposed an association analysis framework that attributed hallucinations to specific model capability deficiencies, such as commonsense memorization, relational reasoning, and instruction following, offering a more nuanced, capability-based characterization.

The need for fine-grained classification to enable effective detection and mitigation led to increasingly detailed taxonomies. \cite{ye2023yom} provided a comprehensive review, organizing hallucinations by various text generation tasks (e.g., QA, dialogue, cross-modal systems) and analyzing their origins from data collection, knowledge gaps, and optimization processes. Expanding on this, \cite{rawte2023ao8} introduced a highly granular framework, profiling hallucination by its degree (mild, moderate, alarming), orientation (Factual Mirage, Silver Lining, each with intrinsic/extrinsic sub-categories), and six specific types (e.g., Numeric Nuisance, Geographic Erratum). This work also proposed the Hallucination Vulnerability Index (HVI) for quantitative assessment, pushing towards standardized measurement. Similarly, \cite{li2024qrj} developed HaluEval 2.0, a benchmark featuring a detailed taxonomy of factuality hallucinations including Entity-error, Relation-error, Incompleteness, Outdatedness, Overclaim, and Unverifiability, which is crucial for fine-grained detection and analysis.

The evolution of taxonomies also extended to the granularity of detection. \cite{liu2021mo6} pioneered the concept of token-level, reference-free hallucination detection for free-form text generation, introducing the HADES dataset. This marked a significant step towards real-time, fine-grained identification of errors, moving beyond coarse sentence or document-level assessments. Furthermore, as LLMs became multimodal, new forms of hallucination emerged, necessitating specialized characterization. \cite{li2023249} conducted the first systematic empirical study on "object hallucination" in Large Vision-Language Models (LVLMs), where generated descriptions contradict visual input, and proposed POPE as a more stable evaluation method. This was further refined by \cite{liu2023882}, who characterized LMM hallucination by the types of negative instructions (e.g., nonexistent object manipulation, knowledge manipulation) that models fail to follow.

Collectively, these studies illustrate a clear intellectual progression from initial empirical observation to sophisticated, multi-dimensional classification schemes. This systematic structuring of the problem space has been instrumental in guiding the development of targeted detection benchmarks, such as those by \cite{cao2023ecl} for automated dataset generation and \cite{oh2024xa3} for entity-relationship based verification, and laying the groundwork for effective mitigation strategies. While these evolving taxonomies provide powerful tools for analysis and intervention, the theoretical proof by \cite{xu2024n76} that hallucination is an inherent and inevitable limitation for all computable LLMs sets a fundamental ceiling on the problem, suggesting that complete elimination is impossible. This theoretical understanding reinforces the critical importance of robust detection and mitigation, which heavily rely on these sophisticated characterization frameworks.