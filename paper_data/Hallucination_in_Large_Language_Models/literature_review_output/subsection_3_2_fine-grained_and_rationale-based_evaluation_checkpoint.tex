\subsection{Fine-Grained and Rationale-Based Evaluation}

Traditional evaluations of Large Language Model (LLM) outputs often rely on coarse-grained metrics that only assess overall answer correctness, failing to pinpoint the precise nature or location of factual errors and reasoning flaws. To foster greater confidence and enable targeted improvements, advanced evaluation methodologies have emerged, focusing on fine-grained analysis and the verifiability of an LLM's underlying reasoning process, or "rationales." These approaches move beyond simple correctness to provide a deeper, more granular understanding of LLM hallucinations, identifying errors at token or sentence levels and often leveraging structured data to automatically check logical steps.

Early efforts to characterize hallucination, such as the distinction between intrinsic and extrinsic types in abstractive summarization by \cite{maynez2020h3q}, laid the groundwork for more detailed analysis. A significant step towards fine-grained error localization was the introduction of HADES, the first token-level, reference-free hallucination detection benchmark for free-form text generation by \cite{liu2021mo6}. This benchmark, created through contextual perturbation and iterative human annotation, enabled the identification of hallucinated tokens without relying on a ground-truth reference, addressing a critical limitation for real-time applications. Building on this, \cite{dziri2021bw9} developed the Neural Path Hunter, which employs a token-level hallucination critic to identify and mask erroneous entity mentions in dialogue systems, subsequently correcting them by retrieving facts from a Knowledge Graph (KG).

The need to verify an LLM's reasoning process directly led to the development of rationale-based evaluation. \cite{gao2023ht7} introduced ALCE, a benchmark that enables LLMs to generate text with explicit citations, using NLI-based metrics to automatically evaluate whether generated statements are supported by their cited passages. This marked an important shift towards making LLM claims verifiable. Further advancing this, \cite{oh2024xa3} proposed ERBench, a novel benchmark that leverages existing relational databases (RDBs) to generate complex, automatically verifiable questions and, crucially, to check the LLM's *rationales*. By utilizing database integrity constraints like Functional Dependencies (FDs) and Foreign Key Constraints (FKCs), ERBench can automatically verify if an LLM's step-by-step reasoning contains FD-inferred critical keywords, providing a robust mechanism to assess factual consistency and logical coherence. Similarly, \cite{wen2023t6v} introduced MindMap, a prompting pipeline that guides LLMs to reason over structured Knowledge Graph inputs and explicitly generate a "Graph of Thoughts" (or mind map), thereby making their reasoning pathways transparent and verifiable against KG facts. \cite{li2023v3v} extended this concept with Chain-of-Knowledge, a framework that dynamically adapts knowledge from heterogeneous sources (structured and unstructured) and performs progressive rationale correction, preventing error propagation in multi-step reasoning. The OKGQA benchmark by \cite{sui20242u1} further evaluates LLM+KG integration in open-ended question answering, employing metrics like FActScore and SAFE to measure factual precision and assess factual support, demonstrating how KGs can enhance trustworthiness.

A parallel development involves leveraging LLMs themselves as evaluators for fine-grained and rationale-based checks. \cite{du2023qu7} quantified and attributed hallucination by analyzing its association with specific model capability deficiencies, moving towards understanding the *reasons* behind errors. \cite{cao2023ecl} introduced AutoHall, an automated dataset generation method, and a zero-resource hallucination detection technique based on LLM self-contradiction, where the model queries itself multiple times to check for consistency. This concept was significantly refined by \cite{dhuliawala2023rqn} with Chain-of-Verification (CoVe), a multi-step process where an LLM generates specific verification questions about its own initial response and then answers them independently to self-correct factual errors. This "factored" approach effectively reduces the LLM's tendency to repeat its own hallucinations. Extending this, \cite{ji2023vhv} proposed an iterative self-reflection methodology for medical generative question-answering, where the LLM systematically generates, scores, and refines both its background knowledge and answers for factuality and consistency, demonstrating strong performance in high-stakes domains. More recently, \cite{li2024qrj} presented HaluEval 2.0, a benchmark that uses powerful LLMs like GPT-4 to extract factual statements from responses and judge their truthfulness, considering interrelations between statements for a fine-grained assessment. \cite{yang20251dw} introduced MetaQA, a self-contained detection method that employs metamorphic relations and prompt mutation to expose factual inconsistencies by having the LLM verify transformed versions of its own output.

Beyond direct verification, comprehensive taxonomies and diagnostic benchmarks also contribute to fine-grained understanding. \cite{zhang2023k1j} and \cite{ye2023yom} provided extensive taxonomies of hallucination, categorizing error types and their underlying mechanisms. \cite{rawte2023ao8} further refined this with a fine-grained taxonomy encompassing degree, orientation, and specific types of hallucination, alongside a Hallucination Vulnerability Index (HVI) for quantitative assessment. For Retrieval-Augmented Generation (RAG) systems, \cite{chen2023h04} developed the RGB benchmark to diagnose specific LLM capabilities crucial for RAG, such as Noise Robustness, Negative Rejection, and Information Integration, offering a granular view of how LLMs process external knowledge. \cite{chen2024c4k} introduced DiaHalu, a dialogue-level benchmark with a comprehensive taxonomy (e.g., Non-factual, Incoherence, Reasoning Error) to evaluate hallucinations in multi-turn conversations, highlighting the need for context-aware, fine-grained analysis in dynamic interactions. Furthermore, mitigation strategies like the Coarse-to-Fine Highlighting by \cite{lv2024k5x} implicitly rely on fine-grained identification of key lexical units within retrieved contexts to improve factual grounding, demonstrating the interplay between fine-grained analysis and intervention. Similarly, \cite{su2024gnz} developed DRAD, which uses Real-time Hallucination Detection (RHD) by analyzing the uncertainty of output entities to trigger targeted retrieval, showcasing fine-grained, real-time detection based on internal model signals.

In conclusion, the evolution of LLM evaluation has clearly moved towards more sophisticated, fine-grained, and rationale-based approaches. While significant progress has been made in identifying specific error locations, verifying reasoning steps through structured data, and leveraging LLMs for self-correction, challenges remain. The scalability of human annotation, the computational cost of multi-step verification, and the generalizability of detection methods across diverse hallucination types and domains are ongoing research areas. Future directions will likely focus on developing more efficient, robust, and universally applicable methods for fine-grained error diagnosis and transparent rationale verification, crucial for building truly trustworthy and reliable AI systems.