\subsection{Summary of Key Developments}

The intellectual journey in addressing hallucination within Large Language Models (LLMs) and their multimodal successors reflects a profound maturation in the pursuit of trustworthy artificial intelligence. This field has rapidly evolved from initial problem characterization to sophisticated management strategies, fundamentally shifting the paradigm from viewing hallucination as a solvable bug to acknowledging its inherent nature. This section provides a concise recap of these major milestones, highlighting the progression from foundational definitions and detection methods to diverse mitigation strategies, the crucial expansion into multimodal AI, and the emerging focus on advanced topics like adversarial robustness and safety-critical applications, underscoring the field's evolving approach to AI dependability across all modalities.

Early research meticulously focused on defining and characterizing hallucinations, primarily within text-based LLMs. Foundational work established critical distinctions between intrinsic errors (contradicting source input) and extrinsic errors (adding unsupportable, often false, information), particularly in tasks like abstractive summarization \cite{maynez2020h3q}. This initial understanding paved the way for comprehensive taxonomies that categorized hallucinations by their origin and nature, such as input-conflicting, context-conflicting, and fact-conflicting errors \cite{zhang2023k1j}. The challenge of fact-conflicting hallucinations, lacking immediate ground truth, underscored the need for more robust evaluation. This phase also saw the development of initial profiling frameworks and efforts to attribute hallucination causes through association analysis, laying the groundwork for understanding the mechanistic origins of these failures \cite{du2023qu7}.

As the understanding of hallucination deepened, detection methodologies became increasingly sophisticated, driven by the need for more granular, efficient, and black-box compatible evaluations. The field transitioned from relying on reference-dependent metrics to pioneering reference-free approaches that leveraged the internal consistency of LLM outputs \cite{manakul20236ex}. Further refinements included self-contained detection mechanisms employing metamorphic relations and prompt mutation to generate diverse test cases, enhancing detection accuracy and generalizability \cite{yang20251dw}. The increasing complexity of LLM applications also necessitated specialized benchmarks, moving towards fine-grained, token-level or sentence-level analysis, and the verification of LLM reasoning rationales using structured data \cite{oh2024xa3}. The imperative for reliability in high-stakes domains led to the development of domain-specific benchmarks, such as Med-HALT for medical contexts, revealing significant challenges even for state-of-the-art models in complex factual recall \cite{umapathi2023puv}. A notable advancement in verifiability was the development of benchmarks and metrics for enabling LLMs to generate text with explicit and verifiable citations, allowing for automatic evaluation of factual correctness and citation quality \cite{gao2023ht7}.

Mitigation efforts evolved along two complementary axes: external grounding and internal model interventions. External grounding techniques, notably Retrieval-Augmented Generation (RAG), became a cornerstone, enabling LLMs to access and integrate external, verifiable knowledge to reduce factual errors. The seminal ReAct paradigm further advanced this by demonstrating the power of interleaving an LLM's internal reasoning with external actions (e.g., API calls or tool use) to dynamically gather information and self-correct responses in interactive environments \cite{yao20229uz}. This approach was further refined by frameworks that dynamically interwove Chain-of-Thought reasoning with iterative knowledge retrieval \cite{trivedi2022qsf}. While highly effective, RAG's limitations, particularly in noise robustness and handling counterfactuals, spurred diagnostic benchmarks and the development of adaptive RAG frameworks that trigger retrieval only when hallucination is detected in real-time \cite{chen2023h04, su2024gnz}. The integration of Knowledge Graphs (KGs) also proved instrumental, allowing LLMs to reason over structured factual repositories and dynamically adapt heterogeneous knowledge sources for enhanced factual consistency \cite{wen2023t6v, sui20242u1}.

Complementing external grounding, internal strategies focused on steering model behavior during decoding or embedding hallucination resistance during training. Decoding-time techniques included contrastive decoding, which penalizes tokens inconsistent with a reference or subtly altered input, and innovative approaches that leverage "induced hallucinations" to guide models toward factuality \cite{zhang202396g}. Self-correction mechanisms, such as Chain-of-Verification (CoVe), empowered LLMs to internally plan and execute verification steps, iteratively refining responses based on consistency checks \cite{dhuliawala2023rqn}. Training-based approaches included robust instruction tuning with negative examples and preference optimization techniques like Direct Preference Optimization (DPO), which align model behavior with desired factual accuracy using AI-generated or human-annotated preference data \cite{liu2023882, xiao2024hv1}. More advanced interventions delved into manipulating internal model states, such as constraining specific attention heads responsible for "false premise hallucinations," demonstrating significant performance gains by targeting mechanistic origins \cite{yuan2024o7d}.

A pivotal intellectual shift in the field was the theoretical acknowledgment of hallucination's inherent nature. Groundbreaking theoretical proofs, notably employing diagonalization arguments, demonstrated that hallucination is an inevitable and unavoidable characteristic for *any computable LLM*, regardless of architectural advancements or training improvements \cite{xu2024n76}. This fundamental insight, reinforced by theoretical frameworks establishing the inherent impossibility of automated hallucination detection without expert-labeled negative examples \cite{karbasi2025j7n}, fundamentally reshaped the research paradigm. The focus moved decisively from the ambition of complete elimination to the pragmatic necessity of robust detection, effective mitigation, and responsible management, marking a significant maturation of the field's approach to AI dependability.

This theoretical grounding spurred the crucial expansion of research into multimodal AI, where hallucination presents unique and complex challenges due to the inherent "modality gap" and cross-modal inconsistencies \cite{lan20240yz, liu2024sn3}. Early work probed object hallucination in Vision-Language Pre-training (VLP) models \cite{dai20229aa}, leading to systematic empirical studies and specialized evaluation methods for various multimodal hallucination types, including object, relationship, temporal, and cross-modal driven errors \cite{li2023249, wu2024bxt, kuan20249pm, sungbin2024r2g, wang2024rta}. The increasing number and complexity of these benchmarks necessitated meta-evaluation to ensure their reliability and validity \cite{yan2024ux8}. Multimodal mitigation strategies also diversified, employing both training-based and inference-time approaches, such as robust instruction tuning with negative examples \cite{liu2023882}, visual grounding techniques \cite{yin2025s2b}, and advanced causal inference frameworks that mitigate modality prior-induced hallucinations by deciphering attention causality \cite{zhou2024lvp}. Recent work has also advanced adversarial generation of diverse visual hallucination instances to build more robust multimodal models \cite{huang20247wn}.

The field's maturation is further evidenced by the emergence of advanced topics that push beyond current paradigms. Research into adversarial hallucination actively probes model vulnerabilities to build more robust systems, revealing that LLMs are highly susceptible to fabricated details in prompts, especially in high-stakes domains like clinical decision support \cite{omar2025us3}. This necessitates a shift towards proactive robustness engineering. Concurrently, the development of specialized guardrails for safety-critical applications has become paramount, focusing on preventing "never event" errors and explicitly communicating uncertainty in domains like pharmacovigilance \cite{hakim2024d4u}. These developments, alongside the ongoing pursuit of unified theoretical frameworks and meta-evaluation of benchmarks, signify a comprehensive intellectual journey towards fundamentally dependable and trustworthy AI systems.