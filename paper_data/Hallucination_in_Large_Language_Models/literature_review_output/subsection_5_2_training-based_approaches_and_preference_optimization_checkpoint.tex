\subsection{Training-Based Approaches and Preference Optimization}

Embedding hallucination resistance directly into the learning phases of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) represents a proactive strategy to build robustness from the ground up, reducing reliance on post-hoc corrections. These training-based approaches encompass modifications to pre-training objectives, robust instruction tuning with carefully curated datasets, and sophisticated preference optimization techniques that align model behavior with desired factual accuracy.

Early efforts focused on refining pre-training objectives to enhance visual grounding. \cite{dai20229aa} systematically investigated object hallucination in Vision-Language Pre-training (VLP) models, revealing that optimizing for standard metrics like CIDEr could paradoxically increase hallucination. To counter this, they proposed ObjMLM (Object-Masked Language Modeling), a novel pre-training objective designed to improve token-level image-text alignment and controlled generation, thereby reducing object hallucination by up to 17.4\% without additional data. Building on the idea of fine-tuning, \cite{wang2023ubf} introduced `ReCaption`, a framework that mitigates fine-grained object hallucination by fine-tuning Large Vision-Language Models (LVLMs) with augmented data. This method leverages a two-stage prompting strategy with ChatGPT to rewrite original image captions, enriching the training data with diverse yet semantically consistent descriptions to improve the accuracy of object attributes and behaviors.

Beyond specific pre-training or fine-tuning data generation, some methods introduce generalizable training techniques. \cite{wu2024n00} proposed NoiseBoost, a simple yet effective method that injects Gaussian noise into projected visual tokens during Supervised Fine-tuning (SFT), Reinforcement Learning (RL), or Semi-Supervised Learning (SSL). This perturbation encourages MLLMs to distribute attention more evenly between visual and linguistic tokens, alleviating over-reliance on language priors and improving hallucination accuracy by 8.1\% in human evaluations of dense captions. This approach is notable for its generalizability across various MLLM training strategies and backbones without significant additional training costs.

More advanced strategies leverage preference optimization, often incorporating AI-generated or human-annotated feedback to align model behavior. \cite{xiao2024hv1} introduced Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO), a novel preference learning approach for Large Vision Language Models (LVLMs). Their method generates fine-grained, sentence-level AI feedback (including hallucination type, severity, and rationale) using proprietary models, then trains an open-source detection model to enable a "detect-then-rewrite" pipeline for cost-effective preference dataset construction. HSA-DPO significantly reduced hallucination rates on benchmarks like AMBER and Object HalBench, outperforming state-of-the-art mitigation models by explicitly prioritizing the mitigation of critical hallucinations. Addressing the computational expense and potential biases of using large models as judges for preference data, \cite{deng202405j} proposed an "efficient self-improvement" framework for Multimodal Large Language Models (MLLMs) that is "model-level judge-free." This approach generates controllable negative samples by blending conditional and unconditional decoding paths, then uses a lightweight CLIP model for robust preference data inversion and quality control before applying Direct Preference Optimization (DPO). This judge-free method achieves superior precision and recall in hallucination control with significantly lower computational demands, offering a scalable pathway for MLLM self-improvement.

In summary, training-based approaches have evolved from modifying foundational pre-training objectives to sophisticated fine-tuning and preference optimization techniques. While early work highlighted the need for better visual grounding \cite{dai20229aa}, subsequent research focused on enriching training data with fine-grained details \cite{wang2023ubf} or introducing generalizable regularization during training \cite{wu2024n00}. The advent of preference optimization, particularly with fine-grained AI feedback \cite{xiao2024hv1} and judge-free mechanisms \cite{deng202405j}, marks a significant step towards instilling intrinsic hallucination avoidance. However, challenges remain in ensuring the scalability and unbiased nature of synthetic data generation, managing the computational costs of iterative training, and preventing the introduction of new biases from reward models or lightweight verifiers. Future directions will likely involve developing more robust and efficient methods for generating high-quality preference data, exploring hybrid training paradigms, and integrating diverse signals to further enhance factual accuracy and trustworthiness.