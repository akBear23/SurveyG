\subsection{Defining Hallucination and Its Immediate Impact}

Hallucination in Large Language Models (LLMs) fundamentally challenges their reliability and trustworthiness, manifesting as the generation of factually inaccurate, nonsensical, or unfaithful information presented as truthful or coherent \cite{zhang2023k1j, ahmadi2024j88}. This phenomenon is a critical concern for the secure and accountable deployment of AI, as it directly undermines the dependability and credibility of LLMs across diverse applications. Establishing a clear conceptual foundation and common terminology is therefore paramount for understanding and addressing this pervasive issue.

The conceptualization of hallucination has evolved from task-specific observations to comprehensive frameworks tailored for general-purpose LLMs. Pioneering work in abstractive summarization by \cite{maynez2020h3q} provided an early systematic characterization, categorizing hallucinations into two primary forms based on their relationship to the source document: \textit{intrinsic hallucinations}, which misrepresent or contradict information explicitly present in the input, and \textit{extrinsic hallucinations}, which introduce information not inferable from the source but presented as fact. Their human evaluation revealed a significant prevalence of these errors, with over 70\% of summaries containing some form of hallucination, underscoring the inadequacy of traditional metrics for assessing faithfulness \cite{maynez2020h3q}. This foundational distinction between internal consistency and external factual grounding remains central to subsequent definitions.

As LLMs grew in scale and versatility, the scope of hallucination expanded beyond simple source document fidelity. \cite{zhang2023k1j} extended this taxonomy specifically for LLMs, recognizing that models often generate content without a single, explicit source document. They distinguished between \textit{input-conflicting} errors (similar to intrinsic, but generalized to user prompts), \textit{context-conflicting} errors (contradicting previously generated turns in a dialogue or a longer generated text), and \textit{fact-conflicting} errors (contradicting established world knowledge). This framework highlights the multifaceted nature of LLM hallucinations, where errors can arise from misinterpreting instructions, losing coherence over extended generations, or fabricating facts entirely. Further refining this understanding, \cite{rawte2023ao8} proposed a more granular framework, profiling hallucination by its \textit{degree} (mild, moderate, alarming), \textit{orientation} (Factual Mirage and Silver Lining, each with intrinsic/extrinsic sub-categories), and six specific \textit{types} (e.g., Numeric Nuisance, Generated Golem). While \cite{zhang2023k1j}'s categories offer a broad, practical classification for LLM behavior, \cite{rawte2023ao8}'s detailed taxonomy provides a finer-grained lens for diagnostic analysis, allowing researchers to pinpoint specific error patterns and quantify hallucination vulnerability with metrics like their proposed Hallucination Vulnerability Index (HVI). This progression from broad categories to more specific, measurable types reflects the field's increasing sophistication in dissecting the problem.

The immediate impact of hallucination is profound and far-reaching, directly affecting the utility and societal acceptance of LLMs. The generation of factually incorrect or nonsensical content erodes user trust and credibility, making LLMs unreliable sources of information \cite{ahmadi2024j88}. This is particularly critical in high-stakes domains such as healthcare, legal advice, or financial consulting, where misinformation can lead to severe consequences \cite{liu2024gxh}. For instance, fact-conflicting errors can propagate misinformation at an unprecedented scale, while context-conflicting errors can render dialogue systems incoherent and frustrating. Beyond textual outputs, the problem extends to multimodal LLMs, where hallucinations can manifest as descriptions that contradict visual or audio inputs, posing significant safety and reliability concerns in applications like autonomous driving or medical image analysis \cite{li2023249}. The pervasive nature of hallucination, therefore, necessitates an urgent focus on robust solutions to ensure the secure, dependable, and accountable integration of AI into critical societal functions.