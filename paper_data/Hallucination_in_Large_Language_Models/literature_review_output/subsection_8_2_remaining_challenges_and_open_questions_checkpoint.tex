\subsection{Remaining Challenges and Open Questions}

Despite significant advancements in understanding, detecting, and mitigating hallucinations in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), several persistent challenges and open questions continue to shape the research agenda. The theoretical proof that hallucination is an inherent and inevitable limitation for all computable LLMs, regardless of their architecture or training \cite{xu2024n76}, fundamentally shifts the paradigm from eradication to robust management, underscoring the enduring nature of this problem.

One primary challenge lies in the **scalability and fine-grained nature of hallucination evaluation**. Early efforts relied on costly human evaluation \cite{maynez2020h3q}, which is not scalable for the rapid development of LLMs. While methods like \textit{SelfCheckGPT} \cite{manakul20236ex} introduced zero-resource black-box detection, they still face computational costs and limitations in sampling quality. The need for fine-grained, token-level, and reference-free detection led to benchmarks like HADES \cite{liu2021mo6}, but these are often derived from specific text sources and may not capture the full spectrum of LLM errors. In the multimodal domain, initial object hallucination evaluations like POPE \cite{li2023249} focused on coarse-grained presence/absence, leading to subsequent work addressing more complex relationship hallucinations with R-Bench \cite{wu2024bxt} and cross-modal interactions with AVHBench \cite{sungbin2024r2g}. However, the quality of these benchmarks themselves has been questioned, with frameworks like HQM/HQH \cite{yan2024ux8} revealing issues like response bias and misalignment with human judgment. The sheer diversity of hallucination types, including multi-object \cite{chen2024vy7}, temporal \cite{li2024wyb}, and event hallucinations \cite{jiang2024792}, necessitates increasingly sophisticated and universal evaluation frameworks. Furthermore, the challenge of evaluating hallucinations in unconstrained, free-form generation \cite{kaul2024ta7} and dialogue-level contexts \cite{chen2024c4k} remains formidable, even for unified detection frameworks like UNIHD \cite{chen2024lc5}. Automated dataset generation tools like AutoHall \cite{cao2023ecl} offer scalability but often focus on specific hallucination types and rely on other LLMs for classification, potentially inheriting their biases.

Another critical area is the **generalizability of mitigation techniques across diverse tasks and domains**, coupled with the need for a **deeper understanding of complex causal mechanisms**. Initial retrieval-augmented generation (RAG) approaches like ReAct \cite{yao20229uz} and IRCoT \cite{trivedi2022qsf} improved grounding by interleaving reasoning with external tool use or retrieval. However, these methods often faced limitations in handling complex, multi-step scenarios, were sensitive to prompt engineering, or struggled with the quality and quantity of retrieved information, as highlighted by the ALCE benchmark for citation generation \cite{gao2023ht7} and the RGB benchmark's analysis of RAG's shortcomings in noise robustness and information integration \cite{chen2023h04}. Self-correction strategies, as surveyed by \cite{pan2023mwu, pan2024y3a}, such as Chain-of-Verification (CoVe) \cite{dhuliawala2023rqn}, empower LLMs to self-critique but can be computationally expensive or rely solely on the model's internal knowledge. Integrating Knowledge Graphs (KGs) offers structured grounding \cite{wen2023t6v, sui20242u1, dziri2021bw9, li2023v3v}, but faces challenges in KG quality, maintenance, and the effective integration of graphical structures into LLM reasoning. Dynamic RAG methods like DRAD \cite{su2024gnz} aim for efficiency by triggering retrieval only when hallucinations are detected, yet their effectiveness relies on the accuracy of real-time uncertainty correlation. In multimodal models, mitigation becomes even more complex. Training-based methods like robust instruction tuning with negative examples \cite{liu2023882} or fine-tuning with caption rewrites \cite{wang2023ubf} require extensive data. Inference-time methods like Woodpecker \cite{yin2023hx3} and Dentist \cite{chang2024u3t} leverage external expert models or LLM-as-judge paradigms, introducing dependencies. The phenomenon of "multimodal hallucination snowballing" \cite{zhong2024mfi}, where initial errors propagate, further complicates mitigation. Efforts to understand the causal mechanisms, such as CAUSAL MM \cite{zhou2024lvp} which intervenes on attention to balance modality priors, are emerging but often rely on assumptions about confounding factors. The challenge of balancing visual and linguistic priors without compromising content quality or inference speed remains, as seen in methods like ClearSight/VAF \cite{yin2025s2b}, MemVR \cite{zou2024dp7}, and ICT \cite{chen2024j0g}.

Finally, the **ethical implications of deploying systems with inherent hallucinatory tendencies** are paramount. The inevitability of hallucination \cite{xu2024n76} means that LLMs, even with the best mitigation, will occasionally produce untruthful content. This necessitates a focus on **truly transparent and accountable LLMs**. The ability to generate text with verifiable citations, as benchmarked by ALCE \cite{gao2023ht7}, is a step towards accountability, as is the generation of "mind maps" for reasoning transparency \cite{wen2023t6v}. However, the economic and societal impact of unreliable LLM outputs \cite{rejeleene2024okw} underscores the urgency of preventing and detecting misinformation \cite{liu2024gxh}. Furthermore, the vulnerability to adversarial attacks that exploit internal mechanisms like attention sinks to induce hallucinations \cite{wang2025jen} highlights the need for robust security. Future directions must emphasize the ongoing pursuit of **trustworthy AI and responsible innovation**. This includes developing more adaptive and context-aware AI that can dynamically assess its own uncertainty and seek external validation, as well as creating systems that are not only accurate but also explainable, allowing users to understand and verify their outputs. The goal is to build LLMs that are not just powerful, but also reliable, transparent, and ethically sound for deployment in high-stakes domains like medicine \cite{chen2024hfe, ji2023vhv}.