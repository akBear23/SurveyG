\subsection{Retrieval-Augmented Generation (RAG) and Knowledge Graphs}

Large Language Models (LLMs) are prone to generating plausible but factually incorrect information, a phenomenon known as hallucination, primarily due to their reliance on potentially outdated or incomplete internal knowledge acquired during pre-training \cite{maynez2020h3q, du2023qu7, rejeleene2024okw}. To mitigate this, Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs) have emerged as pivotal strategies, providing LLMs with dynamic access to external, up-to-date, and verifiable information \cite{gao20232zb, zhang20252at}. These techniques aim to ground LLM responses in factual evidence, thereby significantly enhancing factual consistency, credibility, and transparency.

The evolution of RAG, which augments LLMs with external knowledge retrieval, began by establishing foundational concepts for integrating external tools. Early paradigms demonstrated the power of allowing LLMs to interact with environments to gather information, thereby reducing hallucinations that arise from purely internal, ungrounded reasoning. Building on this, \cite{trivedi2022qsf} introduced Interleaving Retrieval with Chain-of-Thought (IRCoT), a dynamic RAG approach that leverages intermediate Chain-of-Thought steps as queries for iterative knowledge retrieval. This method significantly reduced factual errors in multi-step question answering by up to 50\% on specific benchmarks, compared to static retrieval, by allowing LLMs to refine their information needs dynamically. Further enhancing the ability to access current information, \cite{vu202337s} developed FRESH PROMPT, a few-shot in-context learning method that intelligently integrates diverse, up-to-date results from search engines (including "users also ask" questions) into LLM prompts. This approach demonstrated substantial improvements in factuality, achieving a 49\% increase in strict accuracy for models like GPT-4 on questions requiring current world knowledge, as evaluated on the FreshQA benchmark. More recently, \cite{zhang2024o58} critically assessed LLMs' ability to make "utility judgments" within RAG, finding that well-instructed LLMs can distinguish between relevance and utility of retrieved passages, and proposing a k-sampling listwise approach to reduce dependency on input sequence, thereby improving subsequent answer generation.

As RAG matured, research shifted towards more advanced, modular architectures that dynamically integrate knowledge and address the limitations of basic retrieval, such as noise, inefficiency, and context length. \cite{su2024gnz} proposed Dynamic Retrieval Augmentation based on hallucination Detection (DRAD), a framework that synchronizes retrieval with real-time hallucination detection. DRAD's Real-time Hallucination Detection (RHD) component identifies potential hallucinations by analyzing entity-level uncertainty (low probability, high entropy) in the LLM's output, triggering targeted retrieval only when necessary. This conditional retrieval mechanism makes the RAG process more efficient and precise. To combat the challenge of LLMs "getting lost in long contexts" from retrieved documents, \cite{lv2024k5x} introduced Coarse-to-Fine highlighTing (COFT). COFT dynamically identifies and emphasizes key information at varying granularities (word, sentence, paragraph) within lengthy retrieved contexts, leveraging external Knowledge Graphs (KGs) like Wikidata for entity extraction and a novel contextual weighting mechanism, leading to over 30\% improvement in F1 score on hallucination benchmarks by better guiding the LLM's attention. Expanding on the diversity of knowledge sources, \cite{li2023v3v} presented Chain-of-Knowledge (CoK), a modular RAG framework for dynamic knowledge adaptation over heterogeneous sources. CoK features an Adaptive Query Generator (AQG) capable of formulating queries in various languages (e.g., SPARQL for Wikidata, SQL for tables, natural language for unstructured text) and employs a progressive rationale correction mechanism to prevent error propagation across reasoning steps.

Beyond unstructured text retrieval, Knowledge Graphs (KGs) offer a structured, explicit, and verifiable source of information, proving invaluable for enhancing factual consistency. Early integration efforts, such as Neural Path Hunter by \cite{dziri2021bw9}, focused on reducing hallucination in dialogue systems. This generate-then-refine strategy employed a "Token-level Hallucination Critic" to identify erroneous entities and an "Entity Mention Retriever" to query a local k-hop subgraph of a KG for factual correction, resulting in a 39.98\% improvement in human-evaluated faithfulness. Moving towards deeper integration, \cite{wen2023t6v} introduced MindMap, which enables LLMs to comprehend and reason over structured graphical inputs from KGs. MindMap performs "Evidence Graph Mining" and "Evidence Graph Aggregation" to convert subgraphs into a natural language reasoning graph, allowing LLMs to build an internal "mind map" for synergistic inference and transparent reasoning.

However, integrating KGs presents unique challenges, including the knowledge acquisition bottleneck, inherent incompleteness, and the fidelity of verbalizing structured data. To address KG incompleteness and sparsity, \cite{liu2024kf2} proposed Logic Query of Thoughts (LGOT), which combines LLM reasoning with KG-based logic query reasoning to break down complex queries into subquestions, leveraging both sources to mitigate the limitations of each. For verifiable commonsense reasoning, particularly concerning long-tail entities, \cite{toroghi2024mxf} introduced Right for Right Reasons (R$^3$). R$^3$ axiomatically surfaces LLM's intrinsic commonsense knowledge while grounding every factual reasoning step on KG triples, significantly reducing hallucination and reasoning errors in commonsense KGQA. Furthermore, to combat noise and irrelevant data often introduced when retrieving from extensive KGs, \cite{li2024ncc} developed the Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. Amar retrieves knowledge across entities, relations, and subgraphs, using a self-alignment module to enhance retrieved text and a relevance gating module to filter irrelevant information, achieving state-of-the-art performance on benchmarks like WebQSP and CWQ. In abstractive summarization, \cite{dong20223yz} demonstrated how entity-linked KGs could mitigate hallucinations by providing external world knowledge, redefining faithfulness beyond mere document extractiveness through a generate-and-revise pipeline with a Fact Injected Language Model (FILM).

The synergy between RAG and KGs is increasingly explored to create more robust hybrid systems. KGs can guide the retrieval process, structure information, or serve as a verification layer for facts retrieved from unstructured text. For instance, as noted, COFT \cite{lv2024k5x} leverages KGs for entity extraction to improve highlighting in long contexts. \cite{chen2024qs5} explored hybrid RAG approaches, combining search engine and KG results with fine-tuning, demonstrating that a hybrid strategy achieved the highest score in the CRAG benchmark by allowing LLMs to say "I don't know" when uncertain, thereby reducing hallucination. Empirically, \cite{sui20242u1} investigated the trustworthiness of KG-augmented LLMs in open-ended question answering, introducing the OKGQA benchmark. Their findings confirmed that KG integration generally reduces factual errors, particularly for queries requiring deeper reasoning, and demonstrated robustness even when KGs were partially contaminated, using a prize-cost strategy for efficient knowledge extraction.

Despite these significant advancements, RAG and KG integration still face considerable challenges. Benchmarks like ALCE by \cite{gao2023ht7}, designed for evaluating LLMs' ability to generate text with verifiable citations, revealed that even advanced models like GPT-4 struggle with complete citation support and multi-document synthesis, with approximately 50\% of generations lacking full citation. Furthermore, the RGB benchmark by \cite{chen2023h04} provides a diagnostic view of RAG's current weaknesses. Its findings highlight LLMs' struggles with noise robustness (confusing similar but distinct information), negative rejection (failing to decline to answer when no relevant information is available), information integration from multiple documents, and counterfactual robustness (prioritizing incorrect retrieved information over correct internal knowledge even when warned). These challenges underscore that while RAG and KGs are powerful mitigation strategies, they do not eliminate hallucination entirely, a limitation theoretically supported by \cite{xu2024n76}, who proved that hallucination is an inherent and inevitable characteristic for all computable LLMs.

In conclusion, RAG and Knowledge Graphs are indispensable tools for grounding LLMs in external, verifiable knowledge, significantly reducing hallucinations and improving factual accuracy. However, ongoing research is crucial to address the complexities of dynamic context management, multi-source synthesis, robust error handling, and the inherent limitations of KGs themselves. Future directions must focus on developing more sophisticated hybrid mechanisms for LLMs to discern reliable information, integrate diverse knowledge sources seamlessly, and provide transparent, verifiable outputs, ultimately fostering more trustworthy and reliable AI applications.