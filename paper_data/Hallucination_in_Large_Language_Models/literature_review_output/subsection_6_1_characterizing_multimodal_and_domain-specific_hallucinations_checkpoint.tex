\subsection{Characterizing Multimodal and Domain-Specific Hallucinations}

Hallucinations in multimodal large language models (MLLMs) represent a complex and multifaceted challenge, distinct from those encountered in unimodal Large Language Models (LLMs). This distinction primarily stems from the inherent "modality gap" and the intricate interplay between diverse input types, such as vision, audio, and text \cite{liu2024sn3, bai2024tkm, sahoo2024hcb, lan20240yz}. MLLMs, including Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Audio-Visual Language Models (AV-LLMs), frequently generate outputs that are factually inconsistent with their non-textual inputs, necessitating a specialized understanding of their unique failure modes.

A foundational and extensively studied form of multimodal hallucination is \textit{object hallucination}, where models describe objects that are entirely absent from the visual input. Pioneering work by \cite{li2023249} systematically investigated this phenomenon in LVLMs, identifying object frequency and co-occurrence in training data as significant drivers. This initial characterization has since been refined, moving beyond simple object presence to more nuanced errors. For instance, \cite{chen2024vy7} extended the analysis to \textit{multi-object hallucination}, revealing the increased complexity and ambiguity when models struggle to simultaneously recognize and reason about multiple entities. Further, \cite{wang2023ubf} delved into \textit{fine-grained object hallucination}, where models misrepresent subtle object attributes or behaviors, highlighting a deeper deficiency in perceptual grounding beyond mere existence.

Beyond individual objects, MLLMs exhibit significant struggles in accurately interpreting relationships. \cite{wu2024bxt} introduced R-Bench to systematically evaluate \textit{relationship hallucinations} in LVLMs, uncovering a fundamental flaw in models' understanding of inter-object dynamics and their propensity to over-rely on common sense rather than visual evidence. This was further elaborated by \cite{zheng20246fk}, who categorized these into \textit{perceptive} (concrete, directly observable) and \textit{cognitive} (abstract, inferential) relation hallucinations with their Reefknot benchmark. Their findings critically demonstrated that MLLMs are often more susceptible to errors in perceiving concrete relationships, suggesting a weakness in direct visual interpretation rather than just high-level reasoning.

The integration of dynamic modalities like video and audio introduces novel categories of hallucinations related to temporal consistency. For video-language models, \cite{wang2024rta} characterized \textit{temporal hallucinations}, distinguishing between intrinsic (contradicting explicit video content) and extrinsic (unverifiable from the video) types, underscoring the challenge of maintaining factual accuracy across sequential frames. Similarly, \cite{li2024wyb} developed VIDHALLUC to specifically assess \textit{action, temporal sequence, and scene transition hallucinations} in video understanding, revealing models' difficulties in discerning visually similar yet semantically different video segments. In the audio domain, \cite{kuan20249pm} explored \textit{object hallucination} in LALMs, noting that while models can generate descriptive captions, their performance in discriminative tasks often falters, indicating a weakness in grounding query understanding to audio processing. The most intricate scenarios involve \textit{cross-modal driven hallucinations}, as characterized by \cite{sungbin2024r2g} with AVHBench. Here, models misinterpret information due to subtle relationships or an over-reliance on one modality, such as perceiving imaginary sounds from visual cues or fabricating visual events based on audio inputs, demonstrating a critical failure in multimodal fusion and consistency.

More intricate forms of hallucination arise from the entanglement of language and perception within complex reasoning tasks. \cite{jiang2024792} identified \textit{event hallucination}, where LVLMs invent fictional entities and construct entire narratives around them, a phenomenon observed to escalate with output length and complexity. \cite{guan2023z15} proposed Hallusionbench to diagnose entangled \textit{language hallucination} (over-reliance on language priors) and \textit{visual illusion} (misinterpreting visual inputs) in LVLMs. Their findings revealed a pronounced language bias in even state-of-the-art models, where linguistic patterns often override direct visual evidence. This problem is further compounded in interactive settings, where \cite{zhong2024mfi} identified \textit{multimodal hallucination snowballing}, demonstrating that an LVLM's own generated hallucinations can mislead subsequent responses, creating a cascade of errors. Moreover, \cite{cao2024o9a} introduced VisDiaHalBench to specifically diagnose hallucination in visual dialogue settings, highlighting how long-term misleading textual history can induce errors. As MLLMs become globally deployed, \cite{zhang2025pex} introduced CCHall to evaluate \textit{joint cross-lingual and cross-modal hallucinations}, demonstrating that models struggle significantly more when aligning visual content with text across diverse languages, indicating a complex interplay of linguistic and multimodal biases.

Understanding the root causes of these diverse hallucinations is paramount. A recurring theme in the literature is the \textit{over-reliance on language priors} and the \textit{modality imbalance} between powerful language model components and comparatively weaker visual or audio encoders \cite{dai20229aa, liu2023882, wu2024n00, zhou2024lvp, chen2024j0g, lan20240yz}. This imbalance often leads models to prioritize statistical patterns from text over direct perceptual evidence. For instance, the "language hallucination" observed by \cite{guan2023z15} directly stems from this bias, where models generate plausible but visually unfounded statements. Furthermore, issues like "visual amnesia" \cite{zou2024dp7}, where models lose track of visual information during autoregressive decoding, contribute to object and attribute misrepresentations. The internal attention mechanisms also play a critical role, with studies showing how modality priors can negatively impact output quality via attention, leading to biases that exacerbate hallucinations \cite{zhou2024lvp}. Adversarial attacks, such as those exploiting "attention sink" phenomena, further reveal how internal mechanisms can aggregate misleading information, inducing or amplifying hallucinations \cite{wang2025jen}. The inherent "toxicity in datasets" and "LLM hallucinations" inherited from the underlying language model components also contribute significantly to these multimodal errors \cite{lan20240yz}.

The criticality of hallucinations becomes particularly acute in \textit{domain-specific applications}. For instance, \cite{chen2024hfe} highlighted the severe risks of \textit{medical hallucinations} in LVLMs used for medical Visual Question Answering (VQA) or imaging report generation. They introduced Med-HallMark, a benchmark with a novel hierarchical categorization of hallucinations (e.g., Catastrophic, Critical, Attribute, Prompt-induced, Minor) based on their clinical severity, emphasizing the need for context-aware analysis and specialized detection methods beyond general-domain approaches. Similarly, \cite{gu2024eig} introduced MedVH, another benchmark for medical visual hallucination, revealing that domain-specific LVLMs, despite their promising performance on standard medical tasks, are often more susceptible to hallucinations than general models, raising significant concerns about their reliability in clinical settings. These findings underscore that the unique characteristics and high stakes of specialized domains necessitate tailored frameworks for characterizing and assessing hallucination.

In conclusion, the landscape of hallucination in multimodal and domain-specific models is vastly more intricate than in unimodal LLMs. The "modality gap" and the interplay of diverse inputs give rise to distinct phenomena such as object, relation, temporal, and cross-modal hallucinations, often compounded by issues like fine-grained attribute errors, event fabrication, and cross-lingual inconsistencies. These errors are frequently rooted in an over-reliance on language priors, modality imbalance, and challenges in maintaining visual or audio grounding during generation. While significant progress has been made in characterizing these diverse failure modes, the challenge of achieving truly factual and trustworthy multimodal AI systems, particularly in safety-critical domains, remains a paramount and active area of research. This detailed characterization provides a crucial foundation for the subsequent development of robust evaluation benchmarks.