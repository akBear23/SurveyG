\subsection{Adversarial Hallucination and Robustness}

The increasing deployment of Large Language Models (LLMs) and multimodal models (MLLMs) in sensitive applications necessitates a critical focus on their resilience against deliberate manipulation. This subsection delves into the emerging and crucial field of adversarial attacks specifically designed to induce hallucinations, examining how exploiting inherent vulnerabilities reveals fundamental weaknesses and drives the development of proactive robustness engineering. Understanding these techniques is paramount, not merely for creating new attacks, but for building robust AI systems capable of withstanding sophisticated, targeted manipulations.

\subsubsection{Adversarial Attack Methods and Exploitable Vulnerabilities}
Adversarial hallucination attacks aim to intentionally mislead LLMs and MLLMs into generating factually incorrect or unfaithful content. These attacks often exploit specific architectural weaknesses or biases learned during training. A prominent example in MLLMs is the "attention sink" phenomenon, identified by \cite{wang2025jen}. Their work proposes a novel attack that manipulates attention scores and hidden embeddings to trigger these sinks, leading to the generation of visually uninterpretable or misleading content. This attack significantly increases hallucinated sentences and words, revealing how instruction-tuning can inadvertently create "two-segment response" patterns with declining image-text relevance, exposing a fundamental weakness in model faithfulness.

Beyond architectural vulnerabilities, linguistic and data-driven biases can also be exploited. \cite{han202439z} reveal a "semantic shift bias" in LVLMs, where the presence of paragraph breaks (`\n\n`) in training data leads models to associate subsequent content with a semantic shift, increasing the likelihood of hallucination. Crucially, they demonstrate that strategically inserting `\n\n` can *induce* multimodal hallucinations, providing a novel and simple attack mechanism. This highlights how subtle structural elements can be leveraged to corrupt model output. Similarly, \cite{omar2025us3} demonstrate the high susceptibility of LLMs to adversarial hallucination attacks in clinical decision support. By embedding fabricated details (e.g., false lab results) into clinical prompts, they show that LLMs frequently elaborate on this false information, posing significant risks in high-stakes domains. This type of attack directly targets the LLM's knowledge retrieval and generation capabilities, forcing it to "hallucinate" details consistent with the fabricated premise.

Another form of adversarial manipulation, closely related to inducing misinformation, is explored by \cite{zhou2024b0u}. They identify that LLMs are "involuntary truth-tellers" and struggle to generate genuinely fallacious or deceptive reasoning. Exploiting this "fallacy failure," they propose a jailbreak attack where an LLM is prompted to generate a fallacious yet seemingly real procedure for harmful behavior. The model, unable to fabricate a truly fallacious solution, instead proposes a truthful (and thus harmful) one, bypassing safety mechanisms. While framed as a jailbreak, this method effectively induces the model to generate factually harmful content under a deceptive premise, akin to an adversarial hallucination of a "safe" procedure.

It is important to distinguish these direct adversarial attacks from diagnostic benchmarks that use adversarial-style inputs to probe model weaknesses. For instance, \cite{sungbin2024r2g} introduces AVHBench, which uses synthetic videos with swapped audio to create natural mismatches, intentionally inducing cross-modal hallucinations to evaluate an Audio-Visual LLM's discernment. Similarly, \cite{guan2023z15} utilizes human-edited images within their HALLUSION BENCH to challenge LVLMs' robustness against "language hallucination" and "visual illusion." While these tools are invaluable for revealing how models misinterpret subtle relationships or over-rely on one modality, they are primarily diagnostic rather than malicious attack vectors. However, understanding the vulnerabilities they expose, such as the "Multimodal Hallucination Snowballing" phenomenon identified by \cite{zhong2024mfi} where an LVLM's own previously generated hallucinations influence subsequent responses, is critical for anticipating potential adversarial exploitation.

\subsubsection{Proactive Robustness Engineering: Defenses Against Adversarial Hallucination}
Understanding these vulnerabilities and attack vectors is crucial for developing more robust AI systems, shifting the paradigm from purely reactive mitigation to proactive robustness engineering. This involves building strong defenses against sophisticated, targeted manipulations, ensuring models can withstand deliberate attempts to make them hallucinate. Proactive robustness techniques can be broadly categorized into training-based and inference-time interventions, often designed to strengthen grounding, balance modality priors, or enhance critical self-correction.

In the realm of \textbf{training-based robustness engineering}, methods aim to instill hallucination resistance directly into the model's learned parameters, making them inherently more resilient to adversarial prompts. \cite{wu2024n00} proposes NoiseBoost, a generalizable technique that injects Gaussian noise into projected visual tokens during supervised fine-tuning (SFT) or reinforcement learning (RL). This perturbation compels the MLLM to distribute attention more evenly between visual and linguistic tokens, thereby reducing over-reliance on language priors that adversaries might exploit to induce visual hallucinations. Addressing fine-grained hallucinations, \cite{wang2023ubf} introduces ReCaption, a framework that fine-tunes Large Vision-Language Models (LVLMs) using diverse rewritten captions generated by ChatGPT. This approach enhances fine-grained visual-text alignment, making models more robust to subtle attribute and behavior inaccuracies that an adversary might attempt to corrupt. Further advancing training-based defenses, \cite{deng202405j} presents an efficient "judge-free" self-improvement framework for MLLMs. This method generates controllable negative samples by blending conditional and unconditional decoding paths with a "hallucination ratio" and uses a lightweight CLIP-based verifier for Direct Preference Optimization (DPO), proactively training models to resist generating hallucinatory content even when subtly prompted towards it. More broadly, \cite{liu2024gxh} highlights the role of AI alignment training in reducing LLMs' propensity for misinformation and improving their ability to refuse malicious instructions, forming a foundational layer of defense against adversarial attempts to induce harmful hallucinations.

\textbf{Inference-time and post-hoc interventions} provide flexible, training-free solutions to enhance robustness in deployed models, often directly countering the vulnerabilities exploited by adversarial attacks. A direct countermeasure to the `\n\n`-induced hallucination attack by \cite{han202439z} is their proposed MiHO (Mitigating Hallucinations during Output) method. By adjusting the decoding strategy to reduce the logits of the `\n` token, MiHO effectively prevents its generation, thereby suppressing the semantic shift bias and reducing hallucination without retraining. This exemplifies a targeted defense against a specific adversarial mechanism.

Other inference-time strategies focus on strengthening visual grounding and balancing modality priors, directly countering attacks like the "attention sink." \cite{zou2024dp7} introduces MemVR (Memory-space Visual Retracing), a decoding paradigm that re-injects visual tokens into the MLLM's middle layers, dynamically activating when the model exhibits high uncertainty. This reinforces visual memory, mitigating the "amnesia" of visual information that can be exploited by adversaries to induce hallucinations. Similarly, \cite{zhou2024lvp} proposes CAUSAL MM, a causal inference framework that applies back-door adjustment and counterfactual reasoning to attention mechanisms. By treating modality priors as confounding factors, CAUSAL MM balances visual and language attention, making model outputs more aligned with multimodal inputs and less susceptible to prior-induced hallucinations that an adversary might leverage. \cite{park20247cm} presents ConVis, a novel contrastive decoding method that leverages a Text-to-Image (T2I) model to visualize potential hallucinations from the MLLM's initial caption. By comparing logit distributions from the original and T2I-reconstructed images, ConVis penalizes the generation of visualized hallucinations, offering a unique visual feedback loop for robustness against visually-grounded attacks. The Residual Visual Decoding (RVD) method by \cite{zhong2024mfi} also falls into this category, mitigating hallucination snowballing by integrating residual visual input to revise output distributions during generation, thereby defending against an adversary attempting to exploit this internal vulnerability.

Furthermore, critical reasoning and external validation at inference time are crucial. \cite{kim2024ozf} introduces Counterfactual Inception, a training-free method that prompts LMMs to generate and then explicitly avoid "counterfactual keywords" that deviate from visual content. This encourages a more critical self-correction process, making it harder for adversarial prompts to inject and propagate false information. \cite{qu2024pqc} proposes MVP (Multi-View Multi-Path Reasoning), a framework that maximizes an LVLM's innate capabilities by seeking multi-view information and employing certainty-driven decoding. By exploring multiple reasoning paths and selecting answers with high certainty, MVP significantly alleviates hallucinations without external tools or retraining, offering a robust approach against adversarial attempts to force a single, hallucinated conclusion.

In conclusion, the study of adversarial hallucination marks a crucial shift towards understanding and proactively defending against sophisticated manipulations of AI models. By exposing vulnerabilities like the attention sink, semantic shift biases, and the propensity to elaborate on fabricated details, researchers are developing a diverse arsenal of training-based and inference-time techniques to build more resilient systems. The ongoing challenge lies in creating defenses that are both effective against increasingly sophisticated attacks and efficient enough for real-world deployment, while ensuring that robustness measures do not inadvertently compromise other desirable model characteristics like fluency or creativity. Future work will likely focus on deeper causal understanding of model failures and adaptive, multi-layered defense strategies to ensure the credibility and trustworthiness of AI in complex, real-world scenarios.