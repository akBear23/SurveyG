\subsection{The Inevitability of Hallucination}

The pervasive phenomenon of hallucination in Large Language Models (LLMs), where models generate plausible but factually incorrect or unfaithful information, has long been approached as a complex engineering problem. Initial research focused on empirically characterizing these errors, developing detection mechanisms, and devising mitigation strategies, with the implicit goal of eventually eradicating them. For instance, early work by \cite{maynez2020h3q} systematically defined and categorized intrinsic and extrinsic hallucinations in abstractive summarization, highlighting their high prevalence and the inadequacy of traditional evaluation metrics. This foundational empirical understanding was extended to multi-modal contexts by \cite{li2023249}, who studied "object hallucination" in Large Vision-Language Models (LVLMs) and introduced the \texttt{POPE} evaluation method to address the instability of prior metrics. Such efforts aimed to precisely identify the problem's manifestations, laying the groundwork for targeted solutions.

The field subsequently witnessed a proliferation of sophisticated detection and mitigation techniques, reflecting a concerted effort to engineer solutions to this perceived problem. Methods like \texttt{SelfCheckGPT} \cite{manakul20236ex} emerged, offering zero-resource, black-box hallucination detection by leveraging the consistency of stochastically sampled LLM outputs. Researchers also delved into the root causes, with \cite{du2023qu7} quantifying and attributing hallucination to specific model capability deficiencies through association analysis, still within a problem-solving paradigm. Mitigation strategies became increasingly elaborate, ranging from retrieval-augmented generation (RAG) approaches like \texttt{ReAct} \cite{yao20229uz} and \texttt{IRCoT} \cite{trivedi2022qsf} that ground LLMs in external knowledge, to methods enabling verifiable generation with citations \cite{gao2023ht7}. Other techniques involved self-correction mechanisms such as Chain-of-Verification (\texttt{CoVe}) \cite{dhuliawala2023rqn}, knowledge graph prompting (\texttt{MindMap}) \cite{wen2023t6v}, and dynamic retrieval augmentation (\texttt{DRAD}) \cite{su2024gnz} that trigger retrieval based on real-time hallucination detection. Even novel decoding strategies like Induce-then-Contrast Decoding (\texttt{ICD}) \cite{zhang202396g} were developed, aiming to penalize induced hallucinations to enhance factuality. Comprehensive surveys by \cite{tonmoy20244e4}, \cite{zhang2023k1j}, \cite{ye2023yom}, \cite{rawte2023ao8}, and \cite{pan2024y3a} meticulously cataloged these diverse efforts, underscoring the community's dedication to overcoming hallucination as an engineering challenge. Benchmarks like \texttt{HaluEval 2.0} \cite{li2024qrj}, \texttt{HADES} \cite{liu2021mo6}, \texttt{AutoHall} \cite{cao2023ecl}, \texttt{ERBench} \cite{oh2024xa3}, \texttt{UHGEval} \cite{liang20236sh}, and \texttt{DiaHalu} \cite{chen2024c4k} were developed to rigorously evaluate progress in this fight.

However, a groundbreaking theoretical insight has fundamentally shifted this perspective, moving hallucination from a solvable engineering problem to an inherent, unavoidable characteristic of LLMs. \cite{xu2024n76} provided a formal proof demonstrating that hallucination is an *inevitable* limitation for any computable LLM, regardless of architectural advancements or training data improvements. Employing diagonalization arguments, akin to those used to prove the undecidability of the Halting Problem, their work formalizes LLMs as total computable functions and defines hallucination as inconsistencies with a computable ground truth function. The proof establishes that no computable LLM can perfectly learn all computable functions, implying that there will always exist inputs for which any given LLM will generate factually incorrect or nonsensical outputs. This means that complete elimination of hallucination is mathematically impossible, setting a fundamental theoretical ceiling on the problem.

This pivotal realization necessitates a profound paradigm shift in how the AI community approaches dependability and credibility. Instead of pursuing the elusive goal of eradication, the research agenda is re-framed towards robust detection, effective mitigation, and responsible deployment of LLMs. While techniques like \texttt{RGB} \cite{chen2023h04} continue to benchmark the limitations of even advanced mitigation strategies, confirming that LLMs still struggle with noise robustness, negative rejection, and information integration, the new theoretical understanding provides context. It suggests that these challenges are not merely temporary hurdles but reflections of an innate limitation. Future work must therefore focus on building systems that can reliably identify when an LLM is likely to hallucinate, actively correct or prevent such instances through external grounding \cite{sui20242u1, vu202337s, dziri2021bw9, li2023v3v, lv2024k5x}, and integrate self-reflection capabilities \cite{ji2023vhv} to manage, rather than eliminate, this inherent flaw. This paradigm shift acknowledges that LLMs are powerful but inherently fallible tools, demanding a new emphasis on designing trustworthy human-AI collaboration frameworks where external verification and critical assessment are paramount \cite{rejeleene2024okw, liu2024gxh, yang20251dw}.