\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 277 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction}

\section{Introduction}
\label{sec:introduction}

\subsection{Defining Hallucination and Its Immediate Impact}
\label{sec:1\_1\_defining\_hallucination\_\_and\_\_its\_immediate\_impact}

Hallucination in Large Language Models (LLMs) fundamentally challenges their reliability and trustworthiness, manifesting as the generation of factually inaccurate, nonsensical, or unfaithful information presented as truthful or coherent \cite{zhang2023k1j, ahmadi2024j88}. This phenomenon is a critical concern for the secure and accountable deployment of AI, as it directly undermines the dependability and credibility of LLMs across diverse applications. Establishing a clear conceptual foundation and common terminology is therefore paramount for understanding and addressing this pervasive issue.

The conceptualization of hallucination has evolved from task-specific observations to comprehensive frameworks tailored for general-purpose LLMs. Pioneering work in abstractive summarization by \cite{maynez2020h3q} provided an early systematic characterization, categorizing hallucinations into two primary forms based on their relationship to the source document: \textit{intrinsic hallucinations}, which misrepresent or contradict information explicitly present in the input, and \textit{extrinsic hallucinations}, which introduce information not inferable from the source but presented as fact. Their human evaluation revealed a significant prevalence of these errors, with over 70\\% of summaries containing some form of hallucination, underscoring the inadequacy of traditional metrics for assessing faithfulness \cite{maynez2020h3q}. This foundational distinction between internal consistency and external factual grounding remains central to subsequent definitions.

As LLMs grew in scale and versatility, the scope of hallucination expanded beyond simple source document fidelity. \cite{zhang2023k1j} extended this taxonomy specifically for LLMs, recognizing that models often generate content without a single, explicit source document. They distinguished between \textit{input-conflicting} errors (similar to intrinsic, but generalized to user prompts), \textit{context-conflicting} errors (contradicting previously generated turns in a dialogue or a longer generated text), and \textit{fact-conflicting} errors (contradicting established world knowledge). This framework highlights the multifaceted nature of LLM hallucinations, where errors can arise from misinterpreting instructions, losing coherence over extended generations, or fabricating facts entirely. Further refining this understanding, \cite{rawte2023ao8} proposed a more granular framework, profiling hallucination by its \textit{degree} (mild, moderate, alarming), \textit{orientation} (Factual Mirage and Silver Lining, each with intrinsic/extrinsic sub-categories), and six specific \textit{types} (e.g., Numeric Nuisance, Generated Golem). While \cite{zhang2023k1j}'s categories offer a broad, practical classification for LLM behavior, \cite{rawte2023ao8}'s detailed taxonomy provides a finer-grained lens for diagnostic analysis, allowing researchers to pinpoint specific error patterns and quantify hallucination vulnerability with metrics like their proposed Hallucination Vulnerability Index (HVI). This progression from broad categories to more specific, measurable types reflects the field's increasing sophistication in dissecting the problem.

The immediate impact of hallucination is profound and far-reaching, directly affecting the utility and societal acceptance of LLMs. The generation of factually incorrect or nonsensical content erodes user trust and credibility, making LLMs unreliable sources of information \cite{ahmadi2024j88}. This is particularly critical in high-stakes domains such as healthcare, legal advice, or financial consulting, where misinformation can lead to severe consequences \cite{liu2024gxh}. For instance, fact-conflicting errors can propagate misinformation at an unprecedented scale, while context-conflicting errors can render dialogue systems incoherent and frustrating. Beyond textual outputs, the problem extends to multimodal LLMs, where hallucinations can manifest as descriptions that contradict visual or audio inputs, posing significant safety and reliability concerns in applications like autonomous driving or medical image analysis \cite{li2023249}. The pervasive nature of hallucination, therefore, necessitates an urgent focus on robust solutions to ensure the secure, dependable, and accountable integration of AI into critical societal functions.
\subsection{Scope and Motivation of the Review}
\label{sec:1\_2\_scope\_\_and\_\_motivation\_of\_the\_review}

The proliferation of Large Language Models (LLMs) across diverse domains, from content generation to critical decision support, has underscored a fundamental challenge: hallucination. Defined as the generation of plausible yet factually incorrect, inconsistent, or unfaithful information, hallucination poses a significant barrier to the dependable, transparent, and safe deployment of these powerful AI systems \cite{zhang2023k1j}. This pervasive issue can lead to the dissemination of misinformation, erode user trust, and introduce substantial risks in sensitive applications, as highlighted by concerns in fields like medical education where incorrect information can lead to patient harm \cite{parente2024vlq}. Consequently, a comprehensive understanding and robust management of hallucination are paramount for the continued advancement and responsible integration of LLMs into society, ultimately contributing to the development of trustworthy artificial intelligence.

This literature review aims to provide a structured and critical synthesis of the evolving research landscape surrounding LLM hallucination. Our scope encompasses a holistic examination, tracing the intellectual trajectory from the foundational understanding of hallucination to advanced mitigation techniques and the emerging complexities in multimodal contexts. Specifically, we will delve into the historical characterization and evolving taxonomies of hallucination, exploring its root causes and the profound theoretical insights into its inherent inevitability. Subsequently, the review will scrutinize the diverse methodologies developed for benchmarking and detection, ranging from reference-free and consistency-based approaches to fine-grained, rationale-based evaluations and specialized diagnostic frameworks for Retrieval-Augmented Generation (RAG) systems. A significant portion of this analysis is dedicated to a critical appraisal of mitigation strategies, categorizing them into external grounding techniques (such as RAG and Knowledge Graphs) and internal model interventions (including decoding-time strategies, training-based approaches, and causal interventions). Finally, we will address the unique challenges and solutions pertinent to multimodal hallucination, alongside advanced topics like adversarial robustness, safety-critical applications, and the pursuit of unified theoretical frameworks.

The primary motivation for this comprehensive analysis stems from the urgent need to consolidate the rapidly expanding body of knowledge in this critical area. The field has witnessed an explosion of research, driven by the increasing scale and versatility of LLMs, which often exhibit subtle yet impactful errors that are difficult for both humans and models to detect \cite{zhang2023k1j}. This review seeks to synthesize the current state-of-the-art, identify critical gaps in existing approaches, and highlight promising future directions. By systematically mapping the progress in understanding, detecting, and mitigating hallucination, we aim to provide researchers and practitioners with a clear roadmap for developing more reliable and accountable AI systems. The imperative to ensure verifiability and accountability, for instance, has led to innovative approaches like enabling LLMs to generate text with explicit citations, crucial for building trustworthy information-seeking applications \cite{gao2023ht7}. Furthermore, the unique challenges posed by multimodal models, such as the "modality gap" and inherited LLM hallucination tendencies, necessitate dedicated attention \cite{lan20240yz}. This synthesis is crucial for moving beyond reactive problem-solving to proactive robustness engineering, fostering a deeper scientific understanding of AI reliability, and ensuring that LLMs can be deployed responsibly and ethically in an increasingly AI-driven world.

To achieve this, the review is structured as follows: Section 2 delves into the foundational understanding and theoretical limits of hallucination. Section 3 surveys the landscape of benchmarking and detection methodologies. Sections 4 and 5 then explore the two major families of mitigation strategies: external grounding and reasoning, and internal model interventions and training, respectively. Section 6 specifically addresses applications, specialized domains, and the complexities of multimodal hallucination. Finally, Section 7 discusses advanced topics and future directions, before Section 8 concludes with a summary of key developments and persistent challenges.


\label{sec:foundational_understanding_and_theoretical_limits}

\section{Foundational Understanding and Theoretical Limits}
\label{sec:foundational\_underst\_and\_ing\_\_and\_\_theoretical\_limits}

\subsection{Historical Characterization and Evolving Taxonomies}
\label{sec:2\_1\_historical\_characterization\_\_and\_\_evolving\_taxonomies}

The challenge of hallucination in natural language generation (NLG) has evolved from an observed anomaly to a systematically categorized and theoretically bounded problem. Initial empirical studies laid the groundwork for understanding and classifying these errors, providing the intellectual lineage for subsequent research and guiding early detection efforts across diverse tasks.

Pioneering work in abstractive summarization first systematically identified and distinguished different types of unfaithful content. \cite{maynez2020h3q} conducted a large-scale human evaluation, categorizing hallucinations into "intrinsic" (misrepresenting source information) and "extrinsic" (adding uninferable information). This study not only quantified the high prevalence of hallucinations (over 70\\% of summaries) but also highlighted the inadequacy of traditional metrics like ROUGE, advocating for semantically-aware evaluation. Building on this foundational understanding, researchers began to develop more comprehensive taxonomies to classify hallucinations by their source, type, and severity, particularly with the advent of Large Language Models (LLMs).

As LLMs became more prevalent, the scope of hallucination expanded beyond summarization. \cite{zhang2023k1j} introduced a broader taxonomy for LLMs, distinguishing between "input-conflicting" (diverging from user input), "context-conflicting" (inconsistent with prior generated text), and "fact-conflicting" (misaligning with world knowledge) hallucinations. This framework emphasized the unique challenges posed by LLMs' scale and versatility. Further deepening the understanding of hallucination sources, \cite{du2023qu7} proposed an association analysis framework that attributed hallucinations to specific model capability deficiencies, such as commonsense memorization, relational reasoning, and instruction following, offering a more nuanced, capability-based characterization.

The need for fine-grained classification to enable effective detection and mitigation led to increasingly detailed taxonomies. \cite{ye2023yom} provided a comprehensive review, organizing hallucinations by various text generation tasks (e.g., QA, dialogue, cross-modal systems) and analyzing their origins from data collection, knowledge gaps, and optimization processes. Expanding on this, \cite{rawte2023ao8} introduced a highly granular framework, profiling hallucination by its degree (mild, moderate, alarming), orientation (Factual Mirage, Silver Lining, each with intrinsic/extrinsic sub-categories), and six specific types (e.g., Numeric Nuisance, Geographic Erratum). This work also proposed the Hallucination Vulnerability Index (HVI) for quantitative assessment, pushing towards standardized measurement. Similarly, \cite{li2024qrj} developed HaluEval 2.0, a benchmark featuring a detailed taxonomy of factuality hallucinations including Entity-error, Relation-error, Incompleteness, Outdatedness, Overclaim, and Unverifiability, which is crucial for fine-grained detection and analysis.

The evolution of taxonomies also extended to the granularity of detection. \cite{liu2021mo6} pioneered the concept of token-level, reference-free hallucination detection for free-form text generation, introducing the HADES dataset. This marked a significant step towards real-time, fine-grained identification of errors, moving beyond coarse sentence or document-level assessments. Furthermore, as LLMs became multimodal, new forms of hallucination emerged, necessitating specialized characterization. \cite{li2023249} conducted the first systematic empirical study on "object hallucination" in Large Vision-Language Models (LVLMs), where generated descriptions contradict visual input, and proposed POPE as a more stable evaluation method. This was further refined by \cite{liu2023882}, who characterized LMM hallucination by the types of negative instructions (e.g., nonexistent object manipulation, knowledge manipulation) that models fail to follow.

Collectively, these studies illustrate a clear intellectual progression from initial empirical observation to sophisticated, multi-dimensional classification schemes. This systematic structuring of the problem space has been instrumental in guiding the development of targeted detection benchmarks, such as those by \cite{cao2023ecl} for automated dataset generation and \cite{oh2024xa3} for entity-relationship based verification, and laying the groundwork for effective mitigation strategies. While these evolving taxonomies provide powerful tools for analysis and intervention, the theoretical proof by \cite{xu2024n76} that hallucination is an inherent and inevitable limitation for all computable LLMs sets a fundamental ceiling on the problem, suggesting that complete elimination is impossible. This theoretical understanding reinforces the critical importance of robust detection and mitigation, which heavily rely on these sophisticated characterization frameworks.
\subsection{Root Causes and Mechanistic Insights}
\label{sec:2\_2\_root\_causes\_\_and\_\_mechanistic\_insights}

Understanding the underlying factors contributing to hallucination in Large Language Models (LLMs) is paramount for developing effective and targeted mitigation strategies, moving beyond superficial observations to delve into their mechanistic origins. Hallucinations are not merely random errors but stem from a complex interplay of issues spanning the entire LLM lifecycle, from data acquisition to inference.

A primary root cause lies in the \textbf{inherent biases and limitations of vast training data}. LLMs are trained on web-scale corpora that inevitably contain noisy, outdated, or even fabricated information \cite{zhang2023k1j, rejeleene2024okw}. This noisy data can directly lead to unfaithful generations, as demonstrated by \cite{adams202289x}, who addressed this by proposing reference revision to improve the quality of noisy training data for summarization. Empirical studies reveal that low-frequency knowledge in pre-training data correlates with a higher incidence of hallucinations, while specialized domain data can significantly alleviate domain-specific errors \cite{li2024qrj}. Furthermore, the very process of tokenization and the lack of data diversity can introduce information loss and biases, undermining the factual integrity of generated content \cite{rejeleene2024okw}. This can lead to phenomena like "knowledge overshadowing," where dominant or over-represented facts in the training data cause LLMs to over-generalize, potentially resulting in "amalgamated hallucinations" where disparate but frequently co-occurring facts are incorrectly combined.

Beyond data, \textbf{limitations in model architecture and inference-time errors} significantly contribute to hallucination. During the optimization process, LLMs can exhibit "stochastic parroting" due to maximum likelihood estimation, or suffer from "exposure bias" where errors compound over sequential token generation \cite{ye2023yom}. \cite{li2024qrj} empirically showed that certain decoding strategies (e.g., diversity-oriented decoding in professional domains) and even model quantization can elicit higher hallucination rates. A critical architectural challenge arises in processing long contexts; Retrieval-Augmented Language Models (RALMs) often "get lost in long contexts," where irrelevant information distracts the model and exacerbates hallucination \cite{lv2024k5x}. This suggests deficiencies in attention mechanisms or contextual filtering. Moreover, LLMs struggle with fundamental capabilities in Retrieval-Augmented Generation (RAG) scenarios, such as negative rejection (failing to abstain from answering when no relevant information is available) and counterfactual robustness (prioritizing incorrect retrieved information over their own correct internal knowledge, even when warned) \cite{chen2023h04}. This highlights a mechanistic flaw in how LLMs weigh and integrate information. The tendency for LLMs to repeat their own hallucinations when conditioned on prior incorrect generations, a form of error propagation, is a significant inference-time issue addressed by methods like Chain-of-Verification \cite{dhuliawala2023rqn}. Mechanistically, hallucinations can also be linked to the model's internal uncertainty; \cite{su2024gnz} found that entity-level hallucinations correlate with low predictive probability and high entropy, providing a real-time signal for detection. The very nature of maximum-likelihood training can lead LLMs to assign probabilities to non-factual information, making them prone to fabrication \cite{zhang202396g}.

\textbf{Deficiencies in knowledge representation and reasoning} also play a crucial role. \cite{du2023qu7} attributed hallucinations to specific model capability deficiencies, including commonsense memorization, relational reasoning, and instruction following, drawing parallels to cognitive psychology. LLMs often struggle to effectively integrate and reason over structured knowledge, treating Knowledge Graphs (KGs) as plain text rather than leveraging their inherent graphical structure for robust reasoning \cite{wen2023t6v, sui20242u1}. This leads to difficulties in multi-hop reasoning and maintaining factual consistency. The problem of error propagation, where mistakes in early reasoning steps cascade into subsequent generations, further compounds these issues \cite{li2023v3v}. In dialogue systems, hallucinations frequently manifest as the injection of erroneous entities, indicating a failure in precise knowledge grounding \cite{dziri2021bw9}. Furthermore, dialogue-level hallucinations extend beyond mere factual errors to include incoherence (conflicting with input, context, or self), irrelevance, overreliance on specific information, and general reasoning errors, pointing to complex mechanistic failures in maintaining conversational state and consistency \cite{chen2024c4k}. Attributing these diverse hallucinations to specific internal model behaviors or layers remains a significant challenge, often requiring sophisticated probing techniques.

Ultimately, a groundbreaking theoretical perspective suggests that hallucination is not merely an engineering bug but an \textbf{innate limitation of LLMs}. \cite{xu2024n76} provided a formal proof demonstrating the inevitability of hallucination for all computable LLMs, regardless of their architecture or training data. This fundamental insight shifts the paradigm from attempting to eliminate hallucination entirely to focusing on robust detection, quantification, and management strategies.

In conclusion, the root causes of hallucination are multifaceted, encompassing biases and limitations in training data, architectural constraints that hinder robust knowledge processing, and inference-time errors that propagate inaccuracies. These mechanistic insights, from the granular level of token generation uncertainty to the theoretical inevitability of errors, are indispensable for developing targeted, efficient, and truly impactful mitigation strategies that address the underlying mechanisms rather than just the symptoms. Future research must continue to unravel these complex interactions to build more trustworthy and reliable LLM systems.
\subsection{The Inevitability of Hallucination}
\label{sec:2\_3\_the\_inevitability\_of\_hallucination}

The pervasive phenomenon of hallucination in Large Language Models (LLMs), where models generate plausible but factually incorrect or unfaithful information, has long been approached as a complex engineering problem. Initial research focused on empirically characterizing these errors, developing detection mechanisms, and devising mitigation strategies, with the implicit goal of eventually eradicating them. For instance, early work by \cite{maynez2020h3q} systematically defined and categorized intrinsic and extrinsic hallucinations in abstractive summarization, highlighting their high prevalence and the inadequacy of traditional evaluation metrics. This foundational empirical understanding was extended to multi-modal contexts by \cite{li2023249}, who studied "object hallucination" in Large Vision-Language Models (LVLMs) and introduced the \texttt{POPE} evaluation method to address the instability of prior metrics. Such efforts aimed to precisely identify the problem's manifestations, laying the groundwork for targeted solutions.

The field subsequently witnessed a proliferation of sophisticated detection and mitigation techniques, reflecting a concerted effort to engineer solutions to this perceived problem. Methods like \texttt{SelfCheckGPT} \cite{manakul20236ex} emerged, offering zero-resource, black-box hallucination detection by leveraging the consistency of stochastically sampled LLM outputs. Researchers also delved into the root causes, with \cite{du2023qu7} quantifying and attributing hallucination to specific model capability deficiencies through association analysis, still within a problem-solving paradigm. Mitigation strategies became increasingly elaborate, ranging from retrieval-augmented generation (RAG) approaches like \texttt{ReAct} \cite{yao20229uz} and \texttt{IRCoT} \cite{trivedi2022qsf} that ground LLMs in external knowledge, to methods enabling verifiable generation with citations \cite{gao2023ht7}. Other techniques involved self-correction mechanisms such as Chain-of-Verification (\texttt{CoVe}) \cite{dhuliawala2023rqn}, knowledge graph prompting (\texttt{MindMap}) \cite{wen2023t6v}, and dynamic retrieval augmentation (\texttt{DRAD}) \cite{su2024gnz} that trigger retrieval based on real-time hallucination detection. Even novel decoding strategies like Induce-then-Contrast Decoding (\texttt{ICD}) \cite{zhang202396g} were developed, aiming to penalize induced hallucinations to enhance factuality. Comprehensive surveys by \cite{tonmoy20244e4}, \cite{zhang2023k1j}, \cite{ye2023yom}, \cite{rawte2023ao8}, and \cite{pan2024y3a} meticulously cataloged these diverse efforts, underscoring the community's dedication to overcoming hallucination as an engineering challenge. Benchmarks like \texttt{HaluEval 2.0} \cite{li2024qrj}, \texttt{HADES} \cite{liu2021mo6}, \texttt{AutoHall} \cite{cao2023ecl}, \texttt{ERBench} \cite{oh2024xa3}, \texttt{UHGEval} \cite{liang20236sh}, and \texttt{DiaHalu} \cite{chen2024c4k} were developed to rigorously evaluate progress in this fight.

However, a groundbreaking theoretical insight has fundamentally shifted this perspective, moving hallucination from a solvable engineering problem to an inherent, unavoidable characteristic of LLMs. \cite{xu2024n76} provided a formal proof demonstrating that hallucination is an \textit{inevitable} limitation for any computable LLM, regardless of architectural advancements or training data improvements. Employing diagonalization arguments, akin to those used to prove the undecidability of the Halting Problem, their work formalizes LLMs as total computable functions and defines hallucination as inconsistencies with a computable ground truth function. The proof establishes that no computable LLM can perfectly learn all computable functions, implying that there will always exist inputs for which any given LLM will generate factually incorrect or nonsensical outputs. This means that complete elimination of hallucination is mathematically impossible, setting a fundamental theoretical ceiling on the problem.

This pivotal realization necessitates a profound paradigm shift in how the AI community approaches dependability and credibility. Instead of pursuing the elusive goal of eradication, the research agenda is re-framed towards robust detection, effective mitigation, and responsible deployment of LLMs. While techniques like \texttt{RGB} \cite{chen2023h04} continue to benchmark the limitations of even advanced mitigation strategies, confirming that LLMs still struggle with noise robustness, negative rejection, and information integration, the new theoretical understanding provides context. It suggests that these challenges are not merely temporary hurdles but reflections of an innate limitation. Future work must therefore focus on building systems that can reliably identify when an LLM is likely to hallucinate, actively correct or prevent such instances through external grounding \cite{sui20242u1, vu202337s, dziri2021bw9, li2023v3v, lv2024k5x}, and integrate self-reflection capabilities \cite{ji2023vhv} to manage, rather than eliminate, this inherent flaw. This paradigm shift acknowledges that LLMs are powerful but inherently fallible tools, demanding a new emphasis on designing trustworthy human-AI collaboration frameworks where external verification and critical assessment are paramount \cite{rejeleene2024okw, liu2024gxh, yang20251dw}.


\label{sec:benchmarking_and_detection_methodologies}

\section{Benchmarking and Detection Methodologies}
\label{sec:benchmarking\_\_and\_\_detection\_methodologies}

\subsection{Reference-Free and Consistency-Based Detection}
\label{sec:3\_1\_reference-free\_\_and\_\_consistency-based\_detection}

Detecting hallucinations in Large Language Models (LLMs) without relying on external ground truth or human-annotated references is crucial for evaluating proprietary models and scaling detection efforts where human annotation is impractical. These methods offer efficient and flexible assessment of factual correctness and internal coherence, focusing on the model's self-consistency or its ability to self-verify.

Early efforts laid the groundwork for fine-grained, reference-free detection. \cite{liu2021mo6} introduced the first token-level, reference-free hallucination detection task, along with the HADES dataset, which was created by perturbing Wikipedia text and using an iterative model-in-the-loop annotation strategy. This pioneering work provided a benchmark for pinpointing hallucinations at a granular level without external human references, though its dataset was based on perturbed existing text rather than purely generative outputs.

Building on the need for black-box and zero-resource solutions, \cite{manakul20236ex} proposed \textit{SelfCheckGPT}, a seminal method that leverages the inherent stochasticity of LLMs. The core idea is that if an LLM genuinely "knows" a fact, multiple stochastically sampled responses to the same prompt will be consistent; conversely, hallucinated facts will likely lead to divergent or contradictory samples. \textit{SelfCheckGPT} offers five variants for measuring informational consistency, including BERTScore, Question Answering, n-gram overlap, Natural Language Inference (NLI), and LLM Prompting, making it applicable to proprietary models without access to internal probabilities or external knowledge bases. However, \textit{SelfCheckGPT} can sometimes struggle when LLMs tend to repeat their own hallucinations across samples, reinforcing incorrect information.

To address this limitation, \cite{yang20251dw} introduced \textit{MetaQA}, a self-contained hallucination detection approach that significantly enhances consistency-based methods through the use of metamorphic relations (MRs) and prompt mutation. \textit{MetaQA} generates diverse mutations (synonymous and antonymous) from an LLM's initial response and then uses the LLM itself as a "test oracle" to verify the factual consistency of these mutations. This innovative application of MRs allows for more robust exposure of factual inconsistencies, with \textit{MetaQA} empirically outperforming \textit{SelfCheckGPT} across various LLMs and datasets by effectively preventing the model from reinforcing its own errors.

Beyond consistency-based sampling, other reference-free approaches focus on automated dataset generation and self-contained verification mechanisms. \cite{cao2023ecl} developed \textit{AutoHall}, a pipeline for automatically generating model-specific hallucination datasets from existing fact-checking data, eliminating the need for costly manual annotation. \textit{AutoHall} also proposes a zero-resource, black-box detection method based on LLM-driven self-contradiction, where an LLM's initial response is compared against multiple independently generated references to identify inconsistencies. This approach offers a scalable solution for creating dynamic benchmarks that adapt to evolving LLMs.

Another category of "reference-free" methods leverages structured data or powerful LLMs as implicit verification oracles, removing the need for human ground truth annotation for every instance. \cite{oh2024xa3} introduced \textit{ERBench}, a novel benchmark construction framework that utilizes existing relational databases (RDBs) and their integrity constraints (e.g., Functional Dependencies, Foreign Key Constraints) to automatically generate complex, verifiable questions and rationales. This allows for automated verification of both the LLM's answer and its step-by-step reasoning against the database's inherent factual knowledge. Similarly, \cite{li2024qrj} proposed an LLM-based detection framework that uses advanced LLMs, such as GPT-4, to extract factual statements from a target LLM's responses and then judge their truthfulness, even considering interrelations between statements. While relying on a powerful external LLM, this method is "reference-free" from human annotation, offering a scalable way to evaluate factual accuracy.

In conclusion, reference-free and consistency-based detection methods represent a vital frontier in hallucination research, offering scalable and flexible solutions for evaluating LLMs, especially proprietary models. Techniques like \textit{SelfCheckGPT} and \textit{MetaQA} leverage the internal consistency of LLM outputs, while others like \textit{AutoHall} and \textit{ERBench} automate dataset generation and verification against structured knowledge. Despite their promise, these methods still face challenges related to computational cost, the quality of stochastic sampling, the inherent capabilities of the LLMs used for self-verification, and the potential for biases even in automated or LLM-as-a-judge systems. Future research will likely focus on enhancing the robustness and efficiency of these self-contained detection mechanisms, further reducing their reliance on external resources, and expanding their applicability to more complex and nuanced forms of hallucination.
\subsection{Fine-Grained and Rationale-Based Evaluation}
\label{sec:3\_2\_fine-grained\_\_and\_\_rationale-based\_evaluation}

Traditional evaluations of Large Language Model (LLM) outputs often rely on coarse-grained metrics that only assess overall answer correctness, failing to pinpoint the precise nature or location of factual errors and reasoning flaws. To foster greater confidence and enable targeted improvements, advanced evaluation methodologies have emerged, focusing on fine-grained analysis and the verifiability of an LLM's underlying reasoning process, or "rationales." These approaches move beyond simple correctness to provide a deeper, more granular understanding of LLM hallucinations, identifying errors at token or sentence levels and often leveraging structured data to automatically check logical steps.

Early efforts to characterize hallucination, such as the distinction between intrinsic and extrinsic types in abstractive summarization by \cite{maynez2020h3q}, laid the groundwork for more detailed analysis. A significant step towards fine-grained error localization was the introduction of HADES, the first token-level, reference-free hallucination detection benchmark for free-form text generation by \cite{liu2021mo6}. This benchmark, created through contextual perturbation and iterative human annotation, enabled the identification of hallucinated tokens without relying on a ground-truth reference, addressing a critical limitation for real-time applications. Building on this, \cite{dziri2021bw9} developed the Neural Path Hunter, which employs a token-level hallucination critic to identify and mask erroneous entity mentions in dialogue systems, subsequently correcting them by retrieving facts from a Knowledge Graph (KG).

The need to verify an LLM's reasoning process directly led to the development of rationale-based evaluation. \cite{gao2023ht7} introduced ALCE, a benchmark that enables LLMs to generate text with explicit citations, using NLI-based metrics to automatically evaluate whether generated statements are supported by their cited passages. This marked an important shift towards making LLM claims verifiable. Further advancing this, \cite{oh2024xa3} proposed ERBench, a novel benchmark that leverages existing relational databases (RDBs) to generate complex, automatically verifiable questions and, crucially, to check the LLM's \textit{rationales}. By utilizing database integrity constraints like Functional Dependencies (FDs) and Foreign Key Constraints (FKCs), ERBench can automatically verify if an LLM's step-by-step reasoning contains FD-inferred critical keywords, providing a robust mechanism to assess factual consistency and logical coherence. Similarly, \cite{wen2023t6v} introduced MindMap, a prompting pipeline that guides LLMs to reason over structured Knowledge Graph inputs and explicitly generate a "Graph of Thoughts" (or mind map), thereby making their reasoning pathways transparent and verifiable against KG facts. \cite{li2023v3v} extended this concept with Chain-of-Knowledge, a framework that dynamically adapts knowledge from heterogeneous sources (structured and unstructured) and performs progressive rationale correction, preventing error propagation in multi-step reasoning. The OKGQA benchmark by \cite{sui20242u1} further evaluates LLM+KG integration in open-ended question answering, employing metrics like FActScore and SAFE to measure factual precision and assess factual support, demonstrating how KGs can enhance trustworthiness.

A parallel development involves leveraging LLMs themselves as evaluators for fine-grained and rationale-based checks. \cite{du2023qu7} quantified and attributed hallucination by analyzing its association with specific model capability deficiencies, moving towards understanding the \textit{reasons} behind errors. \cite{cao2023ecl} introduced AutoHall, an automated dataset generation method, and a zero-resource hallucination detection technique based on LLM self-contradiction, where the model queries itself multiple times to check for consistency. This concept was significantly refined by \cite{dhuliawala2023rqn} with Chain-of-Verification (CoVe), a multi-step process where an LLM generates specific verification questions about its own initial response and then answers them independently to self-correct factual errors. This "factored" approach effectively reduces the LLM's tendency to repeat its own hallucinations. Extending this, \cite{ji2023vhv} proposed an iterative self-reflection methodology for medical generative question-answering, where the LLM systematically generates, scores, and refines both its background knowledge and answers for factuality and consistency, demonstrating strong performance in high-stakes domains. More recently, \cite{li2024qrj} presented HaluEval 2.0, a benchmark that uses powerful LLMs like GPT-4 to extract factual statements from responses and judge their truthfulness, considering interrelations between statements for a fine-grained assessment. \cite{yang20251dw} introduced MetaQA, a self-contained detection method that employs metamorphic relations and prompt mutation to expose factual inconsistencies by having the LLM verify transformed versions of its own output.

Beyond direct verification, comprehensive taxonomies and diagnostic benchmarks also contribute to fine-grained understanding. \cite{zhang2023k1j} and \cite{ye2023yom} provided extensive taxonomies of hallucination, categorizing error types and their underlying mechanisms. \cite{rawte2023ao8} further refined this with a fine-grained taxonomy encompassing degree, orientation, and specific types of hallucination, alongside a Hallucination Vulnerability Index (HVI) for quantitative assessment. For Retrieval-Augmented Generation (RAG) systems, \cite{chen2023h04} developed the RGB benchmark to diagnose specific LLM capabilities crucial for RAG, such as Noise Robustness, Negative Rejection, and Information Integration, offering a granular view of how LLMs process external knowledge. \cite{chen2024c4k} introduced DiaHalu, a dialogue-level benchmark with a comprehensive taxonomy (e.g., Non-factual, Incoherence, Reasoning Error) to evaluate hallucinations in multi-turn conversations, highlighting the need for context-aware, fine-grained analysis in dynamic interactions. Furthermore, mitigation strategies like the Coarse-to-Fine Highlighting by \cite{lv2024k5x} implicitly rely on fine-grained identification of key lexical units within retrieved contexts to improve factual grounding, demonstrating the interplay between fine-grained analysis and intervention. Similarly, \cite{su2024gnz} developed DRAD, which uses Real-time Hallucination Detection (RHD) by analyzing the uncertainty of output entities to trigger targeted retrieval, showcasing fine-grained, real-time detection based on internal model signals.

In conclusion, the evolution of LLM evaluation has clearly moved towards more sophisticated, fine-grained, and rationale-based approaches. While significant progress has been made in identifying specific error locations, verifying reasoning steps through structured data, and leveraging LLMs for self-correction, challenges remain. The scalability of human annotation, the computational cost of multi-step verification, and the generalizability of detection methods across diverse hallucination types and domains are ongoing research areas. Future directions will likely focus on developing more efficient, robust, and universally applicable methods for fine-grained error diagnosis and transparent rationale verification, crucial for building truly trustworthy and reliable AI systems.
\subsection{Benchmarking Retrieval-Augmented Generation (RAG)}
\label{sec:3\_3\_benchmarking\_retrieval-augmented\_generation\_(rag)}

The effective mitigation of Large Language Model (LLM) hallucination through Retrieval-Augmented Generation (RAG) systems critically depends on the development and application of rigorous, specialized evaluation benchmarks. While foundational work established the nature of hallucination and the necessity of external knowledge grounding \cite{maynez2020h3q}, the proliferation of RAG methods necessitated dedicated frameworks to systematically assess their performance, diagnose specific failure modes, and ensure the reliable utilization of external information. These benchmarks move beyond general LLM evaluation to scrutinize the complex interplay between retrieval and generation, directly addressing the challenges RAG introduces.

A pioneering effort in this domain is the \textbf{Retrieval-Augmented Generation Benchmark (RGB)} introduced by \cite{chen2023h04}. RGB is a multi-lingual corpus designed to systematically diagnose four fundamental RAG capabilities crucial for robust hallucination mitigation:
\begin{enumerate}
    \item \textbf{Noise Robustness}: The ability of an LLM to generate accurate answers even when irrelevant or distracting information is present in the retrieved context.
    \item \textbf{Negative Rejection}: The capacity of an LLM to correctly abstain from answering when no relevant information is available in the retrieved documents, preventing the generation of ungrounded content.
    \item \textbf{Information Integration}: The skill of synthesizing facts from multiple retrieved documents to formulate a comprehensive and accurate answer.
    \item \textbf{Counterfactual Robustness}: The resilience of an LLM to resist factually incorrect information present in the retrieved context, especially when it contradicts the model's internal knowledge.
\end{enumerate}
\cite{chen2023h04} constructed RGB instances using recent news to minimize bias from LLMs' pre-trained knowledge, leveraging LLMs for QA generation and search APIs for document retrieval. Their empirical analysis revealed significant shortcomings in current state-of-the-art LLMs across these dimensions. For instance, models frequently failed to reject answers when only noisy documents were provided (Negative Rejection) and often prioritized factually incorrect retrieved information over their own correct internal knowledge (Counterfactual Robustness), even when explicitly warned. This highlights that while RAG offers a promising mitigation strategy, the effective utilization of external knowledge remains a substantial challenge, with LLMs often struggling to discern utility and truthfulness within the provided context.

Complementing RGB's diagnostic focus on core RAG capabilities, other specialized benchmarks address critical aspects of RAG output quality, verifiability, and the often-overlooked retrieval step. \cite{gao2023ht7} developed \textbf{ALCE (Automatic LLMs' Citation Evaluation)}, which stands as the first reproducible benchmark for evaluating end-to-end RAG systems that not only generate answers but also provide verifiable citations. ALCE introduces novel NLI-based metrics for citation recall and precision, demonstrating that even advanced LLMs often lack complete citation support for their generated statements. This benchmark is crucial for assessing the trustworthiness and verifiability of RAG outputs, moving beyond mere factual correctness to accountability. While ALCE focuses on the \textit{attribution} of generated content, \cite{oh2024xa3} proposed \textbf{ERBench}, an Entity-Relationship based benchmark that leverages relational databases to construct complex, automatically verifiable questions. Crucially, ERBench assesses the correctness of an LLM's \textit{rationale} alongside its final answer, providing a fine-grained approach to pinpoint where RAG-augmented LLMs falter in their reasoning process given the retrieved context. This contrasts with ALCE by scrutinizing the internal logical steps rather than just the external citation links.

A critical limitation of many RAG benchmarks, including RGB, ALCE, and ERBench, is their primary focus on the \textit{generator's} ability to utilize \textit{provided} context, often assuming perfect or near-perfect retrieval. However, the success of RAG heavily relies on the quality of the retrieved passages themselves. Addressing this gap, \cite{zhang2024o58} conducted a comprehensive study on the capabilities of LLMs in \textbf{utility evaluation for open-domain question answering}. They introduced a benchmarking procedure and a collection of candidate passages with varying characteristics to scrutinize how LLMs judge the \textit{utility} of retrieved information in supporting question answering, distinguishing it from mere relevance. Their findings reveal that while well-instructed LLMs can differentiate between relevance and utility, they are highly susceptible to newly generated counterfactual passages. This work is vital as it directly evaluates the LLM's role in discerning the quality of the retrieval step, highlighting that a flawed retriever or an LLM's inability to correctly judge passage utility can independently lead to hallucinations, even if the generation component is otherwise robust.

Furthermore, for evaluating RAG in more realistic, unconstrained generation scenarios, \cite{liang20236sh} introduced \textbf{UHGEval} for Chinese LLMs, which prompts unconstrained continuations using "beginning text" from real news articles. Its novel \texttt{kwPrec} (keyword precision) metric, leveraging an LLM for factual keyword extraction, is particularly valuable for assessing the factual relevance of RAG outputs in open-ended contexts, where structured verification might be challenging.

Ultimately, these specialized benchmarks are indispensable for identifying specific failure modes within RAG systems, guiding improvements in both retrieval mechanisms and the LLM's ability to process and integrate external knowledge. They collectively push the field towards a more nuanced understanding of RAG performance, encompassing core capabilities, verifiability, rationale correctness, and the critical assessment of retrieval utility. However, current benchmarks often rely on synthetic data or specific QA formats, and may not fully capture the complexities of dynamic, multi-turn conversational RAG interactions or scenarios with conflicting information from multiple diverse sources. While these advancements provide powerful tools for evaluation, the theoretical understanding that hallucination is an inherent and inevitable limitation for all computable LLMs \cite{xu2024n76} underscores that RAG systems, despite their sophistication, can only mitigate, not entirely eliminate, factual errors. Future research must therefore focus on developing more dynamic, adaptive, and fine-grained benchmarks that can simulate complex, real-world RAG interactions, proactively identify subtle failure modes in the retriever-generator interplay, and rigorously assess the system's ability to handle conflicting or ambiguous retrieved information, pushing towards more dependable AI applications.


\label{sec:mitigation_strategies:_external_grounding_and_reasoning}

\section{Mitigation Strategies: External Grounding and Reasoning}
\label{sec:mitigation\_strategies:\_external\_grounding\_\_and\_\_reasoning}

\subsection{Retrieval-Augmented Generation (RAG) and Knowledge Graphs}
\label{sec:4\_1\_retrieval-augmented\_generation\_(rag)\_\_and\_\_knowledge\_graphs}

Large Language Models (LLMs) are prone to generating plausible but factually incorrect information, a phenomenon known as hallucination, primarily due to their reliance on potentially outdated or incomplete internal knowledge acquired during pre-training \cite{maynez2020h3q, du2023qu7, rejeleene2024okw}. To mitigate this, Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs) have emerged as pivotal strategies, providing LLMs with dynamic access to external, up-to-date, and verifiable information \cite{gao20232zb, zhang20252at}. These techniques aim to ground LLM responses in factual evidence, thereby significantly enhancing factual consistency, credibility, and transparency.

The evolution of RAG, which augments LLMs with external knowledge retrieval, began by establishing foundational concepts for integrating external tools. Early paradigms demonstrated the power of allowing LLMs to interact with environments to gather information, thereby reducing hallucinations that arise from purely internal, ungrounded reasoning. Building on this, \cite{trivedi2022qsf} introduced Interleaving Retrieval with Chain-of-Thought (IRCoT), a dynamic RAG approach that leverages intermediate Chain-of-Thought steps as queries for iterative knowledge retrieval. This method significantly reduced factual errors in multi-step question answering by up to 50\\% on specific benchmarks, compared to static retrieval, by allowing LLMs to refine their information needs dynamically. Further enhancing the ability to access current information, \cite{vu202337s} developed FRESH PROMPT, a few-shot in-context learning method that intelligently integrates diverse, up-to-date results from search engines (including "users also ask" questions) into LLM prompts. This approach demonstrated substantial improvements in factuality, achieving a 49\\% increase in strict accuracy for models like GPT-4 on questions requiring current world knowledge, as evaluated on the FreshQA benchmark. More recently, \cite{zhang2024o58} critically assessed LLMs' ability to make "utility judgments" within RAG, finding that well-instructed LLMs can distinguish between relevance and utility of retrieved passages, and proposing a k-sampling listwise approach to reduce dependency on input sequence, thereby improving subsequent answer generation.

As RAG matured, research shifted towards more advanced, modular architectures that dynamically integrate knowledge and address the limitations of basic retrieval, such as noise, inefficiency, and context length. \cite{su2024gnz} proposed Dynamic Retrieval Augmentation based on hallucination Detection (DRAD), a framework that synchronizes retrieval with real-time hallucination detection. DRAD's Real-time Hallucination Detection (RHD) component identifies potential hallucinations by analyzing entity-level uncertainty (low probability, high entropy) in the LLM's output, triggering targeted retrieval only when necessary. This conditional retrieval mechanism makes the RAG process more efficient and precise. To combat the challenge of LLMs "getting lost in long contexts" from retrieved documents, \cite{lv2024k5x} introduced Coarse-to-Fine highlighTing (COFT). COFT dynamically identifies and emphasizes key information at varying granularities (word, sentence, paragraph) within lengthy retrieved contexts, leveraging external Knowledge Graphs (KGs) like Wikidata for entity extraction and a novel contextual weighting mechanism, leading to over 30\\% improvement in F1 score on hallucination benchmarks by better guiding the LLM's attention. Expanding on the diversity of knowledge sources, \cite{li2023v3v} presented Chain-of-Knowledge (CoK), a modular RAG framework for dynamic knowledge adaptation over heterogeneous sources. CoK features an Adaptive Query Generator (AQG) capable of formulating queries in various languages (e.g., SPARQL for Wikidata, SQL for tables, natural language for unstructured text) and employs a progressive rationale correction mechanism to prevent error propagation across reasoning steps.

Beyond unstructured text retrieval, Knowledge Graphs (KGs) offer a structured, explicit, and verifiable source of information, proving invaluable for enhancing factual consistency. Early integration efforts, such as Neural Path Hunter by \cite{dziri2021bw9}, focused on reducing hallucination in dialogue systems. This generate-then-refine strategy employed a "Token-level Hallucination Critic" to identify erroneous entities and an "Entity Mention Retriever" to query a local k-hop subgraph of a KG for factual correction, resulting in a 39.98\\% improvement in human-evaluated faithfulness. Moving towards deeper integration, \cite{wen2023t6v} introduced MindMap, which enables LLMs to comprehend and reason over structured graphical inputs from KGs. MindMap performs "Evidence Graph Mining" and "Evidence Graph Aggregation" to convert subgraphs into a natural language reasoning graph, allowing LLMs to build an internal "mind map" for synergistic inference and transparent reasoning.

However, integrating KGs presents unique challenges, including the knowledge acquisition bottleneck, inherent incompleteness, and the fidelity of verbalizing structured data. To address KG incompleteness and sparsity, \cite{liu2024kf2} proposed Logic Query of Thoughts (LGOT), which combines LLM reasoning with KG-based logic query reasoning to break down complex queries into subquestions, leveraging both sources to mitigate the limitations of each. For verifiable commonsense reasoning, particularly concerning long-tail entities, \cite{toroghi2024mxf} introduced Right for Right Reasons (R$^3$). R$^3$ axiomatically surfaces LLM's intrinsic commonsense knowledge while grounding every factual reasoning step on KG triples, significantly reducing hallucination and reasoning errors in commonsense KGQA. Furthermore, to combat noise and irrelevant data often introduced when retrieving from extensive KGs, \cite{li2024ncc} developed the Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. Amar retrieves knowledge across entities, relations, and subgraphs, using a self-alignment module to enhance retrieved text and a relevance gating module to filter irrelevant information, achieving state-of-the-art performance on benchmarks like WebQSP and CWQ. In abstractive summarization, \cite{dong20223yz} demonstrated how entity-linked KGs could mitigate hallucinations by providing external world knowledge, redefining faithfulness beyond mere document extractiveness through a generate-and-revise pipeline with a Fact Injected Language Model (FILM).

The synergy between RAG and KGs is increasingly explored to create more robust hybrid systems. KGs can guide the retrieval process, structure information, or serve as a verification layer for facts retrieved from unstructured text. For instance, as noted, COFT \cite{lv2024k5x} leverages KGs for entity extraction to improve highlighting in long contexts. \cite{chen2024qs5} explored hybrid RAG approaches, combining search engine and KG results with fine-tuning, demonstrating that a hybrid strategy achieved the highest score in the CRAG benchmark by allowing LLMs to say "I don't know" when uncertain, thereby reducing hallucination. Empirically, \cite{sui20242u1} investigated the trustworthiness of KG-augmented LLMs in open-ended question answering, introducing the OKGQA benchmark. Their findings confirmed that KG integration generally reduces factual errors, particularly for queries requiring deeper reasoning, and demonstrated robustness even when KGs were partially contaminated, using a prize-cost strategy for efficient knowledge extraction.

Despite these significant advancements, RAG and KG integration still face considerable challenges. Benchmarks like ALCE by \cite{gao2023ht7}, designed for evaluating LLMs' ability to generate text with verifiable citations, revealed that even advanced models like GPT-4 struggle with complete citation support and multi-document synthesis, with approximately 50\\% of generations lacking full citation. Furthermore, the RGB benchmark by \cite{chen2023h04} provides a diagnostic view of RAG's current weaknesses. Its findings highlight LLMs' struggles with noise robustness (confusing similar but distinct information), negative rejection (failing to decline to answer when no relevant information is available), information integration from multiple documents, and counterfactual robustness (prioritizing incorrect retrieved information over correct internal knowledge even when warned). These challenges underscore that while RAG and KGs are powerful mitigation strategies, they do not eliminate hallucination entirely, a limitation theoretically supported by \cite{xu2024n76}, who proved that hallucination is an inherent and inevitable characteristic for all computable LLMs.

In conclusion, RAG and Knowledge Graphs are indispensable tools for grounding LLMs in external, verifiable knowledge, significantly reducing hallucinations and improving factual accuracy. However, ongoing research is crucial to address the complexities of dynamic context management, multi-source synthesis, robust error handling, and the inherent limitations of KGs themselves. Future directions must focus on developing more sophisticated hybrid mechanisms for LLMs to discern reliable information, integrate diverse knowledge sources seamlessly, and provide transparent, verifiable outputs, ultimately fostering more trustworthy and reliable AI applications.
\subsection{Synergizing Reasoning and Acting (ReAct)}
\label{sec:4\_2\_synergizing\_reasoning\_\_and\_\_acting\_(react)}

The pervasive challenge of ungrounded hallucinations in Large Language Models (LLMs) necessitates a fundamental shift from purely internal, potentially fallacious, reasoning to paradigms that dynamically intertwine an LLM's internal cognitive processes ("thoughts") with external actions (e.g., API calls, tool use). This synergistic approach aims to dynamically gather information, interact with environments, and verify facts, thereby directly addressing the generation of plausible but factually incorrect content. By grounding LLM behavior in external validation, these frameworks enhance robustness, factual accuracy, and interpretability in complex, interactive tasks, moving beyond the limitations of relying solely on internal knowledge \cite{yao20229uz}.

A seminal contribution in this direction is the \texttt{ReAct} framework, introduced by \cite{yao20229uz}. \texttt{ReAct} addresses the inherent limitations of reasoning-only approaches, such as Chain-of-Thought (CoT), which are susceptible to fact hallucination and error propagation due to their exclusive reliance on internal representations. By prompting LLMs to generate both verbal reasoning traces ("thoughts") and task-specific actions in an interleaved manner, \texttt{ReAct} enables a dynamic loop: "reason to act" (formulating plans based on current reasoning) and "act to reason" (incorporating observations from external interactions to refine subsequent reasoning). This grounding in external, verifiable information, such as through a Wikipedia API, significantly mitigates hallucination and enhances interpretability, as demonstrated across knowledge-intensive reasoning (e.g., HotpotQA, FEVER) and interactive decision-making benchmarks (e.g., ALFWorld, WebShop). The explicit thoughts provide a human-aligned and diagnosable decision-making process, allowing for inspection of factual correctness and reasoning paths \cite{yao20229uz}.

Building upon this principle of dynamic external grounding, \cite{trivedi2022qsf} further refined the interaction between reasoning and retrieval with \texttt{IRCoT} (Interleaved Retrieval with Chain-of-Thought Reasoning). While \texttt{ReAct} established the general synergy, \texttt{IRCoT} specifically leverages intermediate Chain-of-Thought steps as dynamic queries for iterative knowledge retrieval. This adaptive information-seeking process, where each reasoning step informs subsequent retrieval and newly retrieved facts ground further reasoning, significantly reduces factual errors and improves the accuracy of multi-step question answering, particularly in few-shot settings. This demonstrates a more fine-grained integration of external knowledge within the reasoning process, directly extending the \texttt{ReAct} paradigm's core mechanism.

The \texttt{ReAct} paradigm extends beyond simple API calls to encompass more complex tool use and agentic behaviors, proving critical for ensuring the reliability of LLM-generated content in practical applications. For instance, in automated code development, LLMs can generate code (an action) but require external verification to mitigate hallucination. \cite{alshahwan2024v64} describe Meta's \texttt{TestGen-LLM} tool, which uses LLMs to improve existing unit tests. Crucially, \texttt{TestGen-LLM} verifies its generated test classes against a set of filters to ensure measurable improvement and eliminate problems due to LLM hallucination, exemplifying the "act to reason" principle where external execution and verification ground the LLM's creative output. Similarly, a survey by \cite{ding20245e3} highlights that agent planning and iterative refinement, core components of the \texttt{ReAct} paradigm, are essential for hallucination mitigation in various code development tasks, from generation to testing and documentation.

Furthermore, the concept of LLM agents interacting within environments and managing internal states to avoid hallucination is explored in multi-agent contexts. \cite{li2023f7d} investigate LLM-based agents in cooperative text games, observing emergent collaborative behaviors but also systematic failures in planning optimization and "hallucination about the task state." They demonstrate that using explicit belief state representations can mitigate these issues, enhancing task performance and the accuracy of Theory of Mind inferences. This underscores the importance of grounding not just facts, but also the agent's understanding of its own state and the environment, a direct extension of \texttt{ReAct}'s interactive grounding principle.

Despite the significant advantages, the effectiveness of tool-augmented and agentic LLMs is not without specific challenges related to their synergistic nature. Diagnosing hallucinations in these complex systems requires specialized benchmarks that go beyond general factual accuracy. \cite{zhang2024h4a} introduced \texttt{ToolBeHonest}, a multi-level diagnostic benchmark specifically designed for tool-augmented LLMs. \texttt{ToolBeHonest} assesses hallucinations through both depth (solvability detection, solution planning, missing-tool analysis) and breadth (scenarios with missing, potential, or limited functionality tools). Their findings reveal that advanced models like Gemini-1.5-Pro and GPT-4o still face significant challenges, particularly in accurately assessing task solvability. This suggests that the LLM's internal reasoning about \textit{when and how} to use tools, and its ability to correctly interpret tool outputs, remains a critical bottleneck. The benchmark also indicates that verbose replies from open-weight models can degrade performance, while proprietary models excel with longer reasoning, highlighting the nuanced interplay between reasoning verbosity and tool-use reliability.

In summary, the \texttt{ReAct} paradigm and its successors represent a powerful shift towards more robust, factually grounded, and interpretable LLMs by dynamically integrating internal reasoning with external actions and observations. While these systems offer substantial improvements in mitigating ungrounded hallucinations through external validation and iterative refinement, challenges persist in the LLM's ability to optimally plan tool use, interpret complex environmental feedback, and maintain coherent internal states in interactive settings. Future research in this area will likely focus on enhancing the LLM's meta-reasoning capabilities regarding tool selection and application, improving robustness to noisy or ambiguous external information, and developing more sophisticated mechanisms for self-correction within these interactive loops.
\subsection{Self-Correction and Verification Mechanisms}
\label{sec:4\_3\_self-correction\_\_and\_\_verification\_mechanisms}

Large Language Models (LLMs) frequently generate outputs that, despite their fluency, can contain factual inaccuracies or logical inconsistencies, a phenomenon known as hallucination \cite{zhang2023k1j, ye2023yom}. To enhance the intrinsic dependability and logical coherence of LLM outputs, a critical area of research focuses on developing mechanisms that enable these models to critically evaluate their own generated content, identify potential errors, and perform revisions. Comprehensive surveys by \cite{pan2023mwu} and \cite{pan2024y3a} highlight automated self-correction as a pivotal strategy, categorizing diverse techniques, while \cite{tonmoy20244e4} provides a broader overview of mitigation strategies, including self-reflection and prompt engineering.

One major category of self-correction mechanisms relies on the LLM's internal reasoning and consistency checks to identify and rectify errors without direct external grounding. A prominent approach is \textit{Chain-of-Verification (CoVe)} \cite{dhuliawala2023rqn}, which enables LLMs to systematically self-critique factual claims through a multi-step process: generating a baseline response, planning specific verification questions, executing these verifications, and finally generating a revised, fact-checked response. The "factored" variant of CoVe is particularly notable, as it ensures verification questions are answered independently of the initial potentially hallucinated response, minimizing the risk of repeating errors. Building on such multi-step reasoning, \cite{ji2023vhv} proposes an iterative self-reflection methodology, particularly effective in high-stakes medical question-answering, where the LLM iteratively generates, scores, and refines both background knowledge and answers for factuality and consistency. For more complex reasoning tasks, \cite{nathani202338c} introduced Multi-Aspect Feedback (MAF), an iterative refinement framework that integrates multiple feedback modules (including frozen LMs) to address diverse error types within reasoning chains, such as logical inconsistencies and mathematical errors. A purely self-contained method, MetaQA \cite{yang20251dw}, leverages metamorphic relations and prompt mutation, allowing the LLM to act as its own "test oracle" by generating and verifying diverse mutations of its response to expose factual inconsistencies. These internal methods offer the advantage of not requiring external tools, making them broadly applicable and efficient when external resources are unavailable. However, their effectiveness is inherently limited by the LLM's internal knowledge and reasoning capabilities, and they can incur substantial computational costs due to multiple inference steps, especially for complex or long-form generations.

Beyond internal consistency, a crucial aspect of self-correction involves LLMs recognizing and acting upon their uncertainty, either by triggering further verification or by appropriately abstaining from answering. Researchers have explored quantifying LLM uncertainty through various internal signals, such as low predictive probability of output tokens, high semantic entropy \cite{su2024gnz}, or analyzing inference dynamics across model layers \cite{jiang20242kz, fadeeva2024lt8, ling2024hqv}. For instance, \cite{fadeeva2024lt8} proposes a token-level uncertainty quantification method, Claim Conditioned Probability (CCP), to fact-check atomic claims in LLM outputs, demonstrating strong improvements in hallucination detection by focusing on the uncertainty of a particular claim value. Building on this, \cite{su2024gnz} introduced Dynamic Retrieval Augmentation based on Hallucination Detection (DRAD), which uses Real-time Hallucination Detection (RHD) to identify potential hallucinations by analyzing the uncertainty of output entities. This allows for targeted and efficient self-correction by conditionally retrieving and incorporating external knowledge precisely when needed. More directly addressing the strategy of abstention, \cite{chen2024kgu} proposes CoKE, a method to teach LLMs to recognize and express their "knowledge boundary." CoKE probes LLMs' internal confidence to elicit explicit expressions of ignorance, enabling them to answer known questions while declining unknown ones, thereby reducing hallucinations caused by fabrication. Similarly, \cite{kang202378c}'s Real-time Verification and Rectification (EVER) framework, which performs step-wise validation during generation, explicitly handles extrinsic hallucinations by flagging unverified content with a "not sure" warning or by abstaining from answering, significantly enhancing trustworthiness. While these uncertainty-driven approaches offer a powerful mechanism for improving reliability and accountability, accurately quantifying and interpreting LLM uncertainty remains a complex challenge, particularly in diverse and open-ended generation tasks where the model's "knowledge" is implicit and dynamic.

While Section 4.1 discussed Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs) as primary external grounding strategies, these external resources can also be integral to an LLM's self-correction \textit{process}. Here, the focus shifts from merely retrieving information to how the LLM \textit{uses} that information to \textit{critique and revise its own output}. For example, \cite{wen2023t6v} introduces MindMap, a prompting pipeline where LLMs comprehend and reason over structured KGs, aggregating evidence from KGs and their implicit knowledge to build a "mind map" for transparent reasoning. This effectively enables self-evaluation and correction by grounding the LLM's thoughts in explicit facts. Similarly, Chain-of-Knowledge (CoK) \cite{li2023v3v} dynamically adapts knowledge from heterogeneous sources (structured and unstructured) and employs a progressive rationale correction mechanism, rectifying preceding reasoning steps before generating subsequent ones to prevent error propagation. In dialogue systems, \cite{dziri2021bw9}'s Neural Path Hunter uses a generate-then-refine strategy, where a hallucination critic identifies and masks erroneous entities in a generated response, which are then corrected by retrieving facts from a KG. The empirical study by \cite{sui20242u1} further reinforces the value of KGs in reducing hallucinations and improving trustworthiness in open-ended question answering, even with partially contaminated knowledge, by providing a verifiable external source for self-correction. These externally-guided self-correction methods often achieve superior factual accuracy by leveraging up-to-date and verifiable information, but they introduce dependencies on external infrastructure, increase computational overhead, and require robust mechanisms for integrating and reasoning over diverse knowledge sources, posing a trade-off with the self-contained nature of purely internal methods.

Despite these advancements, significant challenges remain in self-correction and verification. The computational cost of multi-step reasoning and iterative refinement can be substantial, especially for complex tasks or real-time applications. Generalizability across diverse hallucination types and domains, beyond factual errors, still requires further research, particularly for logical inconsistencies or creative generation. While mechanisms for self-correction and verification significantly improve LLM dependability, the inherent limitations of LLMs mean that complete elimination of hallucination might be theoretically impossible, as suggested by \cite{xu2024n76}. Future work must focus on developing more efficient, robust, and universally applicable self-correction strategies that seamlessly integrate internal reasoning with external knowledge, enhance the accuracy and interpretability of uncertainty quantification, and ensure greater accountability and trustworthiness in autonomous LLM operations.


\label{sec:mitigation_strategies:_internal_model_interventions_and_training}

\section{Mitigation Strategies: Internal Model Interventions and Training}
\label{sec:mitigation\_strategies:\_internal\_model\_interventions\_\_and\_\_training}

\subsection{Decoding-Time Strategies and Logit Manipulation}
\label{sec:5\_1\_decoding-time\_strategies\_\_and\_\_logit\_manipulation}

Decoding-time strategies represent a crucial and efficient paradigm for mitigating hallucinations in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). These techniques intervene directly during the token generation process, offering a training-free and flexible alternative to costly model retraining. The core principle involves dynamically shaping the model's output probability distribution (logits) to steer generation towards more factual and coherent content, often by leveraging internal uncertainty signals or external factual checks.

Traditional decoding methods, such as greedy decoding or beam search, prioritize high-probability sequences but can inadvertently amplify statistical biases or language priors, leading to hallucinations \cite{leng2023ohr}. To counter this, advanced decoding strategies primarily fall into two categories: \textbf{contrastive decoding}, which penalizes tokens inconsistent with a reference or a subtly altered input, and \textbf{direct logit steering}, which explicitly modifies token probabilities based on various signals.

\subsubsection{Contrastive Decoding for Factual Consistency}
Contrastive decoding methods operate by comparing the output distribution of the primary model (or a subtly altered version of the input) with a "negative" or "reference" signal to suppress hallucinated content. This typically involves multiple forward passes, where the difference in log probabilities guides the token selection.

A prominent approach, \textbf{Visual Contrastive Decoding (VCD)} \cite{leng2023ohr}, addresses object hallucinations in MLLMs by introducing visual uncertainty. VCD applies a Gaussian noise mask to the original image, creating a distorted input. It then contrasts the log probabilities from the original and distorted inputs, penalizing tokens that are highly probable under the distorted (hallucination-prone) input. This mechanism effectively calibrates the MLLM's over-reliance on language priors and statistical biases, demonstrating significant improvements on benchmarks like POPE and MME. To prevent VCD from promoting implausible tokens, an adaptive plausibility constraint is applied, pruning candidate tokens based on the confidence of the original output distribution.

Building on this, \textit{ConVis} \cite{park20247cm} introduces a novel training-free contrastive decoding method for MLLMs that leverages a Text-to-Image (T2I) model. By reconstructing an image from the MLLM's initial caption and using this T2I-generated image as a contrastive signal, \textit{ConVis} penalizes tokens corresponding to visually represented hallucinations. This innovative use of an external generative model as a "visual critic" achieves state-of-the-art performance on benchmarks like CHAIR and POPE.

Beyond visual modalities, contrastive decoding has been adapted for unimodal LLMs and specialized domains. \textit{Induce-then-Contrast Decoding (ICD)} \cite{zhang202396g} proposes creating a "factually weak LLM" by fine-tuning the original model on non-factual samples. During inference, the log probabilities of these induced hallucinations from the weak model are subtracted from the original model's predictions, thereby amplifying truthful outputs. ICD also incorporates an adaptive plausibility constraint to selectively penalize only potentially untruthful tokens, preserving overall generation quality. This method has shown remarkable improvements in factuality on TruthfulQA and FACTSCORE, enabling smaller models to rival larger ones. Similarly, \textit{ALternate Contrastive Decoding (ALCD)} \cite{xu2024t34} addresses hallucinations in medical information extraction (MIE) by separating identification and classification functions into sub-task models. During inference, ALCD alternately contrasts output distributions from these sub-task models, selectively enhancing specific capabilities while minimizing the influence of other inherent LLM abilities, further demonstrating the versatility of contrastive signals.

A more advanced form of contrastive decoding, \textbf{Hallucination-Induced Optimization (HIO)} \cite{chen20247jb}, refines the concept of a "negative" model. HIO involves a training stage to create an "Evil LVLM" by intentionally inducing hallucinations using a \textit{reversed} Bradley-Terry model. This "Evil LVLM" is trained to \textit{prioritize} hallucinatory content, amplifying the logits of incorrect tokens. During inference, logits from this "Evil LVLM" are contrasted with the original LVLM to precisely reduce hallucinations. HIO's theoretical analysis emphasizes the need for consistent logit differences between potential hallucinated and correct tokens, and its Amplification of Multiple Targeted Hallucination (AMTH) technique allows for simultaneous training against diverse hallucination types. While HIO involves a pre-training step for the "Evil LVLM," its core mechanism is a decoding-time contrast, making it a powerful extension of this paradigm.

A common trade-off in contrastive decoding is the potential for increased inference time due to multiple forward passes and the risk of degrading the quality or coherence of generated content by overly penalizing tokens. Adaptive constraints and careful tuning of contrastive strength (e.g., \texttt{} and \texttt{} parameters in VCD and ICD) are crucial to balance hallucination reduction with maintaining linguistic quality.

\subsubsection{Direct Logit Steering and Prompt-Derived Influence}
Beyond explicit contrastive signals, other decoding-time strategies directly manipulate the logit distribution or influence it through prompt-derived information without requiring deep architectural interventions.

\textit{OPERA} (Over-trust Penalty and Retrospection-Allocation) \cite{huang2023du3} is a novel MLLM decoding method applied during beam-search. It is grounded in the observation that MLLM hallucinations often stem from "over-trust" in a few "summary tokens" within the self-attention matrix, leading to neglect of visual information. OPERA introduces a penalty term on the model logits during beam-search decoding, derived from a column-wise metric on the self-attention map that indicates the "over-trust degree." Furthermore, its Retrospection-Allocation Strategy allows the decoding process to "roll back" and re-select better candidates if strong over-trust patterns are detected, dynamically correcting the generation path. This method effectively mitigates hallucinations without additional training, data, or external knowledge.

Similarly, \textit{MVP} (Multi-View Multi-Path Reasoning) \cite{qu2024pqc} enhances image comprehension through multi-view information seeking and guides decoding via a "certainty score" (based on logit differences) across multiple paths. This training-free and tool-free framework directly leverages logit-based certainty to alleviate hallucinations by addressing both incomplete visual understanding and low certainty during token generation.

Prompt-based steering methods, while not directly manipulating logits in a mathematical sense, influence the model's internal state such that subsequent token probabilities are altered. \textit{Counterfactual Inception} \cite{kim2024ozf} is a training-free method that prompts MLLMs to self-generate "counterfactual keywords" (deliberately deviating from visual content). These keywords are then filtered and incorporated into a prompt that instructs the MLLM to avoid them, thereby guiding the model towards more factual outputs by implicitly steering the logit space. \textit{Order Matters in Hallucination} \cite{xie20247zk} highlights how the sequence of reasoning in prompts can significantly impact an LLM's consistency and factual accuracy. By generating reasoning first and then the conclusion, models are less prone to fabricating answers, indicating that prompt structure can indirectly steer the internal generation process and thus the logit probabilities towards more reliable outcomes.

It is crucial to distinguish these methods from those that involve deeper manipulation of internal model states or causal interventions on attention mechanisms (covered in Subsection 5.3), or those that rely heavily on external tools and iterative self-correction (covered in Section 4.3). The techniques discussed here primarily focus on shaping the final token probabilities through contrastive signals or direct logit adjustments, often with minimal changes to the model's internal architecture or complex external interactions.

In conclusion, decoding-time strategies represent a vital and evolving area for mitigating hallucinations, offering efficient, training-free solutions. Approaches range from innovative contrastive decoding methods like VCD \cite{leng2023ohr}, ConVis \cite{park20247cm}, ICD \cite{zhang202396g}, ALCD \cite{xu2024t34}, and HIO \cite{chen20247jb}, which leverage diverse "negative" signals, to sophisticated logit-based steering techniques such as OPERA \cite{huang2023du3} and MVP \cite{qu2024pqc}, which directly adjust token probabilities based on internal observations or certainty. Furthermore, prompt-derived influences \cite{kim2024ozf, xie20247zk} offer flexible means of guiding model outputs. While significant progress has been made in improving factual consistency and inference efficiency, ongoing challenges include ensuring generalizability across diverse hallucination types, reducing reliance on external proprietary models, and developing more robust internal uncertainty quantification mechanisms to optimize logit adjustments without compromising overall generation quality. Future directions will likely focus on more sophisticated negative signal generation, adaptive logit scaling, and hybrid approaches that seamlessly integrate these decoding-time interventions with other mitigation paradigms.
\subsection{Training-Based Approaches and Preference Optimization}
\label{sec:5\_2\_training-based\_approaches\_\_and\_\_preference\_optimization}

Embedding hallucination resistance directly into the learning phases of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) represents a proactive strategy to build robustness from the ground up, reducing reliance on post-hoc corrections. These training-based approaches encompass modifications to pre-training objectives, robust instruction tuning with carefully curated datasets, and sophisticated preference optimization techniques that align model behavior with desired factual accuracy.

Early efforts focused on refining pre-training objectives to enhance visual grounding. \cite{dai20229aa} systematically investigated object hallucination in Vision-Language Pre-training (VLP) models, revealing that optimizing for standard metrics like CIDEr could paradoxically increase hallucination. To counter this, they proposed ObjMLM (Object-Masked Language Modeling), a novel pre-training objective designed to improve token-level image-text alignment and controlled generation, thereby reducing object hallucination by up to 17.4\\% without additional data. Building on the idea of fine-tuning, \cite{wang2023ubf} introduced \texttt{ReCaption}, a framework that mitigates fine-grained object hallucination by fine-tuning Large Vision-Language Models (LVLMs) with augmented data. This method leverages a two-stage prompting strategy with ChatGPT to rewrite original image captions, enriching the training data with diverse yet semantically consistent descriptions to improve the accuracy of object attributes and behaviors.

Beyond specific pre-training or fine-tuning data generation, some methods introduce generalizable training techniques. \cite{wu2024n00} proposed NoiseBoost, a simple yet effective method that injects Gaussian noise into projected visual tokens during Supervised Fine-tuning (SFT), Reinforcement Learning (RL), or Semi-Supervised Learning (SSL). This perturbation encourages MLLMs to distribute attention more evenly between visual and linguistic tokens, alleviating over-reliance on language priors and improving hallucination accuracy by 8.1\\% in human evaluations of dense captions. This approach is notable for its generalizability across various MLLM training strategies and backbones without significant additional training costs.

More advanced strategies leverage preference optimization, often incorporating AI-generated or human-annotated feedback to align model behavior. \cite{xiao2024hv1} introduced Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO), a novel preference learning approach for Large Vision Language Models (LVLMs). Their method generates fine-grained, sentence-level AI feedback (including hallucination type, severity, and rationale) using proprietary models, then trains an open-source detection model to enable a "detect-then-rewrite" pipeline for cost-effective preference dataset construction. HSA-DPO significantly reduced hallucination rates on benchmarks like AMBER and Object HalBench, outperforming state-of-the-art mitigation models by explicitly prioritizing the mitigation of critical hallucinations. Addressing the computational expense and potential biases of using large models as judges for preference data, \cite{deng202405j} proposed an "efficient self-improvement" framework for Multimodal Large Language Models (MLLMs) that is "model-level judge-free." This approach generates controllable negative samples by blending conditional and unconditional decoding paths, then uses a lightweight CLIP model for robust preference data inversion and quality control before applying Direct Preference Optimization (DPO). This judge-free method achieves superior precision and recall in hallucination control with significantly lower computational demands, offering a scalable pathway for MLLM self-improvement.

In summary, training-based approaches have evolved from modifying foundational pre-training objectives to sophisticated fine-tuning and preference optimization techniques. While early work highlighted the need for better visual grounding \cite{dai20229aa}, subsequent research focused on enriching training data with fine-grained details \cite{wang2023ubf} or introducing generalizable regularization during training \cite{wu2024n00}. The advent of preference optimization, particularly with fine-grained AI feedback \cite{xiao2024hv1} and judge-free mechanisms \cite{deng202405j}, marks a significant step towards instilling intrinsic hallucination avoidance. However, challenges remain in ensuring the scalability and unbiased nature of synthetic data generation, managing the computational costs of iterative training, and preventing the introduction of new biases from reward models or lightweight verifiers. Future directions will likely involve developing more robust and efficient methods for generating high-quality preference data, exploring hybrid training paradigms, and integrating diverse signals to further enhance factual accuracy and trustworthiness.
\subsection{Internal State Manipulation and Causal Interventions}
\label{sec:5\_3\_internal\_state\_manipulation\_\_and\_\_causal\_interventions}

Advanced mitigation techniques for Large Language Models (LLMs) are increasingly moving beyond superficial prompt engineering or costly retraining, focusing instead on directly manipulating the model's internal states, attention mechanisms, or learned parameters. These fine-grained interventions offer mechanistic insights into model failures, enabling more precise and targeted corrections that enhance factual accuracy and reduce spurious correlations. While many prominent examples of these techniques arise in Multimodal Large Language Models (MLLMs) due to the explicit need to balance information across modalities, the underlying principles of internal state manipulation are broadly applicable to unimodal LLMs as well.

One significant thrust in this area involves directly enhancing or modifying specific modality features within the model's processing pipeline, particularly in MLLMs to combat modality bias or "visual amnesia" \cite{lan20240yz}. For instance, \cite{yin2025s2b} introduced Visual Amplification Fusion (VAF), a training-free method that enhances attention to visual signals within the middle layers of MLLMs, where modality fusion predominantly occurs. VAF aims to counteract language bias by boosting visual feature processing without the inference speed penalties often associated with contrastive decoding methods. Building on this, \cite{zou2024dp7} proposed Memory-space Visual Retracing (MemVR), which dynamically re-injects visual tokens as supplementary evidence into the MLLM's Feed Forward Network (FFN) at a "middle trigger layer" when the model exhibits high uncertainty. This adaptive re-injection refreshes visual memory, effectively combating "visual amnesia" without significant inference overhead. Further refining targeted interventions, \cite{chen2024j0g} developed Image-Object Cross-Level Trusted Intervention (ICT), a plug-and-play method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information. ICT operates during the forward pass by identifying and intervening on specific attention heads responsible for encoding overall image information and fine-grained object details. While these methods offer efficient, training-free solutions, their interventions are often heuristic, relying on empirical observations of where and how visual information degrades. The challenge lies in ensuring these boosts do not inadvertently over-emphasize visual cues, potentially suppressing valid linguistic context or introducing new biases.

Moving beyond direct feature boosting, another line of research employs causal inference and targeted manipulation of attention mechanisms to understand and balance modality priors or correct specific hallucination types. \cite{zhou2024lvp} presented CAUSAL MM, a principled causal inference framework that applies structural causal modeling (SCM), back-door adjustment, and counterfactual reasoning at both visual and language attention levels. By treating modality priors as confounding factors, CAUSAL MM deciphers their causal impact on MLLM output, enabling a more balanced integration of visual and linguistic information to mitigate hallucinations. This represents a more theory-driven approach compared to purely heuristic interventions. Similarly, in unimodal LLMs, \cite{yuan2024o7d} analyzed "false premise hallucinations" and identified a small subset of attention heads ("false premise heads") that disturb knowledge extraction. Their method, FAITH (False premise Attention head constraIining for miTigating Hallucinations), constrains these specific attention heads during inference, demonstrating a significant performance increase by manipulating only about 1\\% of the heads. While CAUSAL MM offers a comprehensive causal framework, its application to complex neural networks can be computationally intensive and relies on strong assumptions about the underlying causal graph. FAITH, on the other hand, provides a more targeted, empirical intervention, but its generalizability across different models and hallucination types requires further investigation. Both, however, highlight the potential of fine-grained attention manipulation informed by mechanistic understanding.

Beyond real-time inference interventions, internal state manipulation can also be applied to directly edit or update the factual knowledge embedded within a model's parameters or representations. This approach, often termed "model editing," aims to precisely alter specific behaviors without affecting unrelated knowledge. For instance, \cite{xu2024f68} proposed MedLaSA (Layer-wise Scalable Adapter strategy) for medical model editing. MedLaSA leverages causal tracing to identify the association of knowledge in neurons across different layers, generating a corresponding scale set. It then incorporates scalable adapters into the dense layers of LLMs, adjusting adapter weights and ranks based on the specific knowledge. This allows for precise editing of semantically identical knowledge while minimizing impact on unrelated information, crucial for high-stakes domains like medicine. In a similar vein, \cite{zhang2024htn} introduced DAFNet (Dynamic Auxiliary Fusion Network) for sequential model editing, which addresses the challenge of continuously rectifying mistakes and preventing catastrophic forgetting. DAFNet enhances semantic interaction among factual knowledge by aggregating intra-editing attention flow into auto-regressive self-attention and leveraging multi-layer diagonal inter-editing attention flow to update sequence-level representations. These methods demonstrate how direct manipulation of internal parameters and attention flow can be used to refine or update factual knowledge, offering a powerful alternative to full retraining. However, challenges remain in ensuring the scalability of these editing operations, preventing unintended side effects on other knowledge, and maintaining consistency over sequential edits.

Mechanistic insights into model failures can also be gleaned from adversarial attacks that specifically manipulate internal states to induce hallucinations. \cite{wang2025jen} demonstrated a novel hallucination attack that exploits the "attention sink" phenomenon in MLLMs. By manipulating attention scores and hidden embeddings to induce these sinks, the attack triggers hallucinated content with minimal image-text relevance. Understanding how such internal vulnerabilities can be exploited to \textit{induce} hallucinations provides critical insights for developing more resilient internal state manipulation techniques, effectively turning offensive research into defensive strategies.

In conclusion, these advanced techniques represent a significant shift towards achieving fine-grained control and mechanistic understanding of LLM and MLLM behavior. By directly intervening in hidden states, attention mechanisms, or leveraging causal relationships and parameter modifications, researchers are developing powerful, often training-free, solutions to mitigate hallucinations. This allows for more precise and targeted corrections, moving beyond black-box approaches to address the root causes of factual inaccuracies. However, challenges remain in fully understanding the complex causal relationships within these models, ensuring generalizability across diverse tasks and architectures, and balancing the trade-offs between intervention strength and preserving beneficial model capabilities. Future work will likely focus on more adaptive and intelligent internal interventions, potentially informed by a deeper causal understanding of multimodal and unimodal reasoning, and ensuring that these interventions are robust and scalable.


\label{sec:applications,_specialized_domains,_and_multimodal_hallucination}

\section{Applications, Specialized Domains, and Multimodal Hallucination}
\label{sec:applications,\_specialized\_domains,\_\_and\_\_multimodal\_hallucination}

\subsection{Characterizing Multimodal and Domain-Specific Hallucinations}
\label{sec:6\_1\_characterizing\_multimodal\_\_and\_\_domain-specific\_hallucinations}

Hallucinations in multimodal large language models (MLLMs) represent a complex and multifaceted challenge, distinct from those encountered in unimodal Large Language Models (LLMs). This distinction primarily stems from the inherent "modality gap" and the intricate interplay between diverse input types, such as vision, audio, and text \cite{liu2024sn3, bai2024tkm, sahoo2024hcb, lan20240yz}. MLLMs, including Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Audio-Visual Language Models (AV-LLMs), frequently generate outputs that are factually inconsistent with their non-textual inputs, necessitating a specialized understanding of their unique failure modes.

A foundational and extensively studied form of multimodal hallucination is \textit{object hallucination}, where models describe objects that are entirely absent from the visual input. Pioneering work by \cite{li2023249} systematically investigated this phenomenon in LVLMs, identifying object frequency and co-occurrence in training data as significant drivers. This initial characterization has since been refined, moving beyond simple object presence to more nuanced errors. For instance, \cite{chen2024vy7} extended the analysis to \textit{multi-object hallucination}, revealing the increased complexity and ambiguity when models struggle to simultaneously recognize and reason about multiple entities. Further, \cite{wang2023ubf} delved into \textit{fine-grained object hallucination}, where models misrepresent subtle object attributes or behaviors, highlighting a deeper deficiency in perceptual grounding beyond mere existence.

Beyond individual objects, MLLMs exhibit significant struggles in accurately interpreting relationships. \cite{wu2024bxt} introduced R-Bench to systematically evaluate \textit{relationship hallucinations} in LVLMs, uncovering a fundamental flaw in models' understanding of inter-object dynamics and their propensity to over-rely on common sense rather than visual evidence. This was further elaborated by \cite{zheng20246fk}, who categorized these into \textit{perceptive} (concrete, directly observable) and \textit{cognitive} (abstract, inferential) relation hallucinations with their Reefknot benchmark. Their findings critically demonstrated that MLLMs are often more susceptible to errors in perceiving concrete relationships, suggesting a weakness in direct visual interpretation rather than just high-level reasoning.

The integration of dynamic modalities like video and audio introduces novel categories of hallucinations related to temporal consistency. For video-language models, \cite{wang2024rta} characterized \textit{temporal hallucinations}, distinguishing between intrinsic (contradicting explicit video content) and extrinsic (unverifiable from the video) types, underscoring the challenge of maintaining factual accuracy across sequential frames. Similarly, \cite{li2024wyb} developed VIDHALLUC to specifically assess \textit{action, temporal sequence, and scene transition hallucinations} in video understanding, revealing models' difficulties in discerning visually similar yet semantically different video segments. In the audio domain, \cite{kuan20249pm} explored \textit{object hallucination} in LALMs, noting that while models can generate descriptive captions, their performance in discriminative tasks often falters, indicating a weakness in grounding query understanding to audio processing. The most intricate scenarios involve \textit{cross-modal driven hallucinations}, as characterized by \cite{sungbin2024r2g} with AVHBench. Here, models misinterpret information due to subtle relationships or an over-reliance on one modality, such as perceiving imaginary sounds from visual cues or fabricating visual events based on audio inputs, demonstrating a critical failure in multimodal fusion and consistency.

More intricate forms of hallucination arise from the entanglement of language and perception within complex reasoning tasks. \cite{jiang2024792} identified \textit{event hallucination}, where LVLMs invent fictional entities and construct entire narratives around them, a phenomenon observed to escalate with output length and complexity. \cite{guan2023z15} proposed Hallusionbench to diagnose entangled \textit{language hallucination} (over-reliance on language priors) and \textit{visual illusion} (misinterpreting visual inputs) in LVLMs. Their findings revealed a pronounced language bias in even state-of-the-art models, where linguistic patterns often override direct visual evidence. This problem is further compounded in interactive settings, where \cite{zhong2024mfi} identified \textit{multimodal hallucination snowballing}, demonstrating that an LVLM's own generated hallucinations can mislead subsequent responses, creating a cascade of errors. Moreover, \cite{cao2024o9a} introduced VisDiaHalBench to specifically diagnose hallucination in visual dialogue settings, highlighting how long-term misleading textual history can induce errors. As MLLMs become globally deployed, \cite{zhang2025pex} introduced CCHall to evaluate \textit{joint cross-lingual and cross-modal hallucinations}, demonstrating that models struggle significantly more when aligning visual content with text across diverse languages, indicating a complex interplay of linguistic and multimodal biases.

Understanding the root causes of these diverse hallucinations is paramount. A recurring theme in the literature is the \textit{over-reliance on language priors} and the \textit{modality imbalance} between powerful language model components and comparatively weaker visual or audio encoders \cite{dai20229aa, liu2023882, wu2024n00, zhou2024lvp, chen2024j0g, lan20240yz}. This imbalance often leads models to prioritize statistical patterns from text over direct perceptual evidence. For instance, the "language hallucination" observed by \cite{guan2023z15} directly stems from this bias, where models generate plausible but visually unfounded statements. Furthermore, issues like "visual amnesia" \cite{zou2024dp7}, where models lose track of visual information during autoregressive decoding, contribute to object and attribute misrepresentations. The internal attention mechanisms also play a critical role, with studies showing how modality priors can negatively impact output quality via attention, leading to biases that exacerbate hallucinations \cite{zhou2024lvp}. Adversarial attacks, such as those exploiting "attention sink" phenomena, further reveal how internal mechanisms can aggregate misleading information, inducing or amplifying hallucinations \cite{wang2025jen}. The inherent "toxicity in datasets" and "LLM hallucinations" inherited from the underlying language model components also contribute significantly to these multimodal errors \cite{lan20240yz}.

The criticality of hallucinations becomes particularly acute in \textit{domain-specific applications}. For instance, \cite{chen2024hfe} highlighted the severe risks of \textit{medical hallucinations} in LVLMs used for medical Visual Question Answering (VQA) or imaging report generation. They introduced Med-HallMark, a benchmark with a novel hierarchical categorization of hallucinations (e.g., Catastrophic, Critical, Attribute, Prompt-induced, Minor) based on their clinical severity, emphasizing the need for context-aware analysis and specialized detection methods beyond general-domain approaches. Similarly, \cite{gu2024eig} introduced MedVH, another benchmark for medical visual hallucination, revealing that domain-specific LVLMs, despite their promising performance on standard medical tasks, are often more susceptible to hallucinations than general models, raising significant concerns about their reliability in clinical settings. These findings underscore that the unique characteristics and high stakes of specialized domains necessitate tailored frameworks for characterizing and assessing hallucination.

In conclusion, the landscape of hallucination in multimodal and domain-specific models is vastly more intricate than in unimodal LLMs. The "modality gap" and the interplay of diverse inputs give rise to distinct phenomena such as object, relation, temporal, and cross-modal hallucinations, often compounded by issues like fine-grained attribute errors, event fabrication, and cross-lingual inconsistencies. These errors are frequently rooted in an over-reliance on language priors, modality imbalance, and challenges in maintaining visual or audio grounding during generation. While significant progress has been made in characterizing these diverse failure modes, the challenge of achieving truly factual and trustworthy multimodal AI systems, particularly in safety-critical domains, remains a paramount and active area of research. This detailed characterization provides a crucial foundation for the subsequent development of robust evaluation benchmarks.
\subsection{Evaluation Benchmarks for Multimodal and Domain-Specific Hallucinations}
\label{sec:6\_2\_evaluation\_benchmarks\_for\_multimodal\_\_and\_\_domain-specific\_hallucinations}

The increasing sophistication and deployment of Multimodal Large Language Models (MLLMs) necessitate the development of specialized benchmarks and metrics to rigorously assess and quantify the diverse forms of hallucinations they exhibit. Moving beyond generic evaluations, the field has progressed towards fine-grained, context-aware, and domain-specific frameworks crucial for systematically identifying, categorizing, and quantifying these errors.

Initial efforts to evaluate hallucinations predominantly focused on object-level inconsistencies in static images. For instance, while not a new benchmark in itself, the POPE (Polling-based Object Probing Evaluation) framework became a common reference for assessing object existence. However, such methods often suffered from limitations like response bias or an inability to capture the nuances of free-form generation. To address this, \cite{kaul2024ta7} introduced \textbf{THRONE}, an object-based benchmark specifically designed for "Type I" hallucinations in the free-form generations of Large Vision-Language Models (LVLMs). THRONE leverages the semantic understanding of open-source LMs for Abstractive Question Answering (AQA) to accurately judge object existence within complex, unconstrained text, significantly reducing judgment errors compared to prior methods. Extending the scope to multiple objects, \cite{chen2024vy7} proposed \textbf{ROPE} (Recognition-based Object Probing Evaluation), an automated protocol for assessing multi-object hallucination. ROPE uniquely employs visual referring prompts (e.g., bounding boxes) to eliminate referential ambiguity and systematically investigates hallucination across various object class distributions within an image, revealing how LVLMs exploit shortcuts and spurious correlations.

As MLLMs expanded to other modalities, so did the need for tailored benchmarks. For audio-language models, \cite{kuan20249pm} investigated object hallucination, introducing discriminative and generative evaluation tasks along with novel metrics like \textbf{ECHO} (Evaluation of Caption Hallucination in audiO) and Cover to quantify hallucination and coverage in audio captioning. This work highlighted a critical weakness in LALMs regarding their understanding of discriminative queries.

Beyond simple object presence, researchers recognized the importance of evaluating more complex relationships between objects. \cite{wu2024bxt} introduced \textbf{R-Bench}, a novel benchmark specifically designed to evaluate and analyze relationship hallucinations in LVLMs. A key innovation of R-Bench is its meticulous construction using the \texttt{nocaps} validation set, preventing data leakage prevalent in earlier benchmarks that relied on pre-training datasets. Building on this, \cite{zheng20246fk} presented \textbf{Reefknot}, a comprehensive benchmark that systematically defines and categorizes relation hallucinations into \textit{perceptive} (concrete) and \textit{cognitive} (abstract) types. Reefknot utilizes real-world semantic triplets and offers diverse evaluation tasks (Yes/No, Multiple Choice, VQA), providing deeper insights into MLLMs' relational understanding, including the counter-intuitive finding that perceptive hallucinations are often more prevalent.

The dynamic nature of video content introduced new challenges, leading to benchmarks for temporal and event-based hallucinations. \cite{wang2024rta} developed \textbf{VideoHallucer}, the first comprehensive benchmark for hallucination detection in Large Video-Language Models (LVLMs). VideoHallucer categorizes hallucinations into \textit{intrinsic} (contradicting video content) and \textit{extrinsic} (not verifiable from video) and employs an adversarial binary VideoQA method for robust evaluation. Further specializing in temporal errors, \cite{li2024wyb} introduced \textbf{VidHalluc}, the largest benchmark for evaluating temporal hallucinations, including Action Hallucination (ACH), Temporal Sequence Hallucination (TSH), and Scene Transition Hallucination (STH). VidHalluc uses a semi-automated pipeline to identify adversarial video pairs that are visually different but semantically similar, exposing MLLMs' over-reliance on contextual scenes.

The complexity of multimodal interactions also spurred the creation of benchmarks for cross-modal driven hallucinations. \cite{sungbin2024r2g} presented \textbf{AVHBench}, the first cross-modal hallucination benchmark for Audio-Visual Large Language Models (AV-LLMs). AVHBench uniquely targets hallucinations arising from the \textit{interaction} between audio and visual modalities, such as perceiving imaginary sounds from visual cues or fake visual events from audio cues, using a semi-automatic annotation pipeline and synthetic videos. \cite{guan2023z15} introduced \textbf{Hallusionbench}, an advanced diagnostic suite for entangled language hallucination and visual illusion in LVLMs. This benchmark features a novel VQA structure with control groups and human-edited images to quantitatively analyze models' response tendencies and specific failure modes, revealing a pronounced language bias in even state-of-the-art models. Pushing the boundaries further, \cite{zhang2025pex} proposed \textbf{CCHall}, a novel benchmark for detecting joint cross-lingual and cross-modal hallucinations, a scenario highly relevant for global applications but largely unaddressed, demonstrating significantly worse performance in MLLMs when both complexities are combined.

To provide a more unified and fine-grained evaluation, \cite{jiang2024792} developed \textbf{Hal-Eval}, a universal and fine-grained hallucination evaluation framework. Hal-Eval introduces a novel category of "Event Hallucination," which involves inventing fictional entities and weaving entire narratives around them, and integrates both discriminative and generative evaluation methods through an automatic annotation pipeline. Similarly, \cite{chen2024lc5} presented \textbf{UNIHD} (Unified Multimodal Hallucination Detection) and its accompanying benchmark \textbf{MHaluBench}. UNIHD is a task-agnostic, tool-enhanced framework that unifies hallucination detection across image-to-text and text-to-image generation, covering modality-conflicting (object, attribute, scene-text) and fact-conflicting hallucinations with fine-grained, claim-level annotations.

Beyond general multimodal contexts, the critical implications of hallucinations in specific domains have led to specialized benchmarks. In the medical domain, \cite{chen2024hfe} introduced \textbf{Med-HallMark}, the first benchmark dedicated to detecting and evaluating medical hallucinations in LVLMs. Med-HallMark features multi-task support (Med-VQA, Imaging Report Generation), a novel hierarchical hallucination categorization based on clinical severity (e.g., Catastrophic, Critical), and a new evaluation metric, MediHall Score, to provide a nuanced assessment of clinical impact.

Finally, a crucial meta-perspective on benchmark quality was provided by \cite{yan2024ux8}, who introduced the \textbf{Hallucination benchmark Quality Measurement (HQM)} framework. HQM systematically assesses benchmark quality based on reliability (test-retest, parallel-forms) and validity (criterion validity, coverage of hallucination types), revealing significant issues like response bias and misalignment with human judgment in many existing benchmarks. They also proposed \textbf{HQH} (High-Quality Hallucination Benchmark), which demonstrates superior reliability and validity through a simplified binary hallucination detection metric and comprehensive coverage of eight hallucination types. This work underscores the importance of robust benchmark design for trustworthy progress. Furthermore, \cite{zhong2024mfi} investigated "Multimodal Hallucination Snowballing," proposing the \textbf{MMHalSnowball} framework to evaluate how previously generated hallucinations can mislead an LVLM's subsequent responses in conversational settings, highlighting the dynamic and cumulative nature of these errors.

In conclusion, the evolution of hallucination evaluation benchmarks for MLLMs reflects a growing understanding of the complexity and diversity of these errors. From basic object presence to intricate temporal, relational, cross-modal, and domain-specific inconsistencies, researchers are continuously developing more sophisticated tools. However, challenges persist in creating benchmarks that are truly comprehensive, scalable, free from inherent biases (e.g., from LLM-assisted generation or evaluation), and perfectly aligned with human judgment across all contexts. Future directions will likely involve more dynamic, interactive, and adaptive evaluation frameworks that can capture the evolving nature of MLLM capabilities and their failure modes, potentially integrating advanced causal inference techniques to pinpoint the root causes of hallucination more precisely.
\subsection{Multimodal Mitigation Strategies}
\label{sec:6\_3\_multimodal\_mitigation\_strategies}

Hallucinations in Large Vision-Language Models (LVLMs) and other multimodal architectures present a formidable challenge, manifesting as generated content factually inconsistent with visual inputs, misinterpretations of audio cues, or flawed cross-modal reasoning \cite{sahoo2024hcb, lan20240yz}. These errors, stemming from issues like the "modality gap," dataset biases, and inherited language model tendencies, severely undermine the dependability and credibility of multimodal AI systems \cite{lan20240yz}. Given the theoretical inevitability of hallucination in computable models \cite{xu2024n76}, mitigation strategies are crucial for managing, rather than eliminating, these inconsistencies. This subsection explores a range of techniques, from robust training paradigms to sophisticated inference-time interventions, all designed to enhance factual consistency by accounting for the complex interplay between modalities.

Training-based strategies aim to instill hallucination resistance directly into the model's learning process. Pioneering efforts in robust instruction tuning for LVLMs include \cite{liu2023882}'s LRV-Instruction, a dataset incorporating diverse \textit{negative instructions} (e.g., describing nonexistent objects) to explicitly train models to avoid visual hallucinations. This data-centric approach, coupled with the GAVIE evaluation framework, demonstrated the effectiveness of explicitly teaching LMMs what \textit{not} to generate. Building on the success of preference optimization, \cite{fu2024yqj} introduced Hallucination-targeted Direct Preference Optimization (HDPO) for MLLMs. HDPO constructs specific preference data to address three distinct causes of multimodal hallucinations: visual distracted hallucination (VDH), long-context hallucination (LCH), and multimodal conflict hallucination (MCH). Unlike general DPO methods, HDPO's targeted data generation, which includes amplifying irrelevant visual information or introducing conflicting text, allows for more precise and consistent improvements across diverse hallucination types. While effective, these instruction tuning and DPO methods demand substantial effort in curating high-quality, hallucination-specific preference data, which can be computationally expensive and may risk stifling generative creativity if not carefully balanced.

Beyond data augmentation, other training-based methods focus on internal model dynamics. \cite{wu2024n00} introduced NoiseBoost, a generalizable technique that injects noise into visual tokens during supervised fine-tuning (SFT), reinforcement learning (RL), or semi-supervised learning (SSL). This perturbation encourages MLLMs to distribute attention more evenly between visual and linguistic tokens, reducing over-reliance on language priors without incurring significant inference costs. This approach is particularly appealing for its efficiency, as it avoids the need for extensive negative data curation. Similarly, \cite{deng202405j} presented an efficient "judge-free" self-improvement framework for MLLMs. This method leverages controllable negative samples and lightweight CLIP-based verification for Direct Preference Optimization (DPO), bypassing the computational expense and potential biases associated with using large models as judges. The challenge with such internal perturbation methods lies in finding the optimal noise level or intervention point to improve robustness without degrading overall performance or introducing new biases.

Inference-time interventions offer "plug-and-play" solutions that do not require expensive retraining, making them highly adaptable. One category focuses on external grounding and post-remedy correction. \cite{yin2023hx3}'s Woodpecker is a training-free pipeline that leverages external expert models (e.g., object detectors, VQA models) to validate visual facts and correct MLLM responses. This approach, akin to active retrieval augmentation for multimodal contexts, enhances interpretability by explicitly adding visual grounding (e.g., bounding boxes) to corrected text. However, its effectiveness is inherently limited by the accuracy and coverage of the external expert models, potentially leading to cascading errors if the "experts" themselves are flawed, and can introduce additional inference latency.

Another significant area of inference-time intervention involves directly manipulating the model's internal states or attention mechanisms to enhance visual grounding. \cite{zou2024dp7}'s MemVR (Memory-space Visual Retracing) addresses "visual amnesia" by re-injecting visual tokens as supplementary evidence into intermediate layers of the MLLM, dynamically activated by high uncertainty. This "look-twice" mechanism efficiently refreshes visual memory, overcoming the forgetting of visual information that can lead to hallucinations. Similarly, \cite{yin2025s2b}'s ClearSight proposes Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals within MLLM middle layers during the forward pass, mitigating object hallucination without compromising content quality or inference speed. \cite{chen2024j0g} proposed ICT (Image-Object Cross-Level Trusted Intervention), a training-free method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information (overall image and fine-grained objects) by targeting specific attention heads. These methods offer fine-grained control but require deep architectural understanding and careful tuning to avoid indiscriminately suppressing beneficial language priors.

Contrastive decoding methods, which leverage subtle differences in input or model states to steer generation, have also been adapted for multimodal contexts. \cite{leng2023ohr} introduced Visual Contrastive Decoding (VCD), a training-free approach that mitigates object hallucinations by contrasting output distributions from original and \textit{distorted} visual inputs. By applying a Gaussian noise mask to create visual uncertainty, VCD penalizes tokens that are highly probable under the distorted input (which amplifies reliance on language priors), thereby calibrating the model's output. Building upon this, \cite{chen20247jb} proposed Hallucination-Induced Optimization (HIO), a more precise contrastive decoding strategy for LVLMs. HIO addresses the "uncontrollable nature of global visual uncertainty" in prior methods by training an "Evil LVLM" using a \textit{reversed} Bradley-Terry model to specifically \textit{amplify} targeted hallucinations. This "Evil LVLM" then provides a stronger, more focused contrastive signal during decoding, leading to superior hallucination reduction. Another novel approach, \cite{park20247cm}'s ConVis, employs a Text-to-Image (T2I) model to visualize potential hallucinations from an MLLM's initial caption. These visualized discrepancies then serve as a contrastive signal during decoding to penalize hallucinated tokens, offering a unique way to generate visual contrastive signals. While powerful, contrastive decoding methods can introduce additional computational overhead during inference and require careful balancing to prevent over-penalization that might stifle creativity or lead to overly cautious, less fluent outputs.

Beyond direct visual grounding, some methods leverage advanced reasoning or generative capabilities. \cite{kim2024ozf}'s Counterfactual Inception prompts LMMs to self-generate "counterfactual keywords" (e.g., non-existent objects) and then explicitly instructs the model to avoid them during response generation. This training-free method, guided by a Plausibility Verification Process (PVP) using CLIP scores, implants counterfactual thinking to reduce hallucinations by making the model explicitly aware of what \textit{not} to say. Furthermore, \cite{zhou2024lvp} offered CAUSAL MM, a causal inference framework that uses back-door adjustment and counterfactual reasoning to mitigate modality prior-induced hallucinations by deciphering attention causality. This provides a more principled and theoretically grounded way to balance visual and language influences, moving beyond heuristic adjustments.

Unified frameworks and multi-view reasoning approaches aim for more adaptive and comprehensive mitigation. \cite{chang2024u3t} introduced "Dentist," a unified hallucination mitigation framework that first classifies the query type (e.g., perception vs. reasoning) and then applies a tailored mitigation strategy within an iterative validation loop. For perception queries, it uses visual verification with sub-questions, while for reasoning queries, it employs Chain-of-Thought (CoT) prompting. This adaptive approach, which refines answers until semantic convergence, directly addresses the need for mitigation strategies that adapt based on query type. Complementing this, \cite{qu2024pqc}'s MVP (Multi-View Multi-Path Reasoning) is a training-free framework that enhances LVLM inference by thoroughly perceiving image information from "multi-view" dimensions (top-down, regular, bottom-up) and then employing "multi-path certainty-driven reasoning" during decoding. This approach explores multiple reasoning paths and aggregates certainty scores to select the most reliable answer, maximizing the innate capabilities of existing LVLMs. While promising for their adaptability, the complexity of these unified frameworks and the accuracy of query classification or certainty aggregation remain critical factors influencing their real-world performance.

In conclusion, the landscape of multimodal hallucination mitigation is characterized by a diverse array of strategies, each with distinct trade-offs. Training-based methods like LRV-Instruction \cite{liu2023882} and HDPO \cite{fu2024yqj} aim for intrinsic robustness but demand significant data curation and computational resources. Inference-time interventions, including external grounding (Woodpecker \cite{yin2023hx3}), internal state manipulation (MemVR \cite{zou2024dp7}, ClearSight \cite{yin2025s2b}), and contrastive decoding (VCD \cite{leng2023ohr}, HIO \cite{chen20247jb}, ConVis \cite{park20247cm}), offer flexible, often training-free, alternatives, but their effectiveness can depend on external tool quality, the intrusiveness of internal manipulations, or the computational overhead of generating contrastive signals. Unified frameworks like Dentist \cite{chang2024u3t} and multi-view reasoning approaches like MVP \cite{qu2024pqc} represent a move towards more adaptive and comprehensive solutions. Despite these advancements, the inherent complexity of multimodal understanding, coupled with the theoretical limits of hallucination, suggests that future research must continue to balance factual consistency with generative fluency, explore more sophisticated causal interventions, and develop methods that are robust to diverse and evolving hallucination types across various multimodal contexts, including audio and video, to ensure truly dependable and credible AI systems.


\label{sec:advanced_topics_and_future_directions}

\section{Advanced Topics and Future Directions}
\label{sec:advanced\_topics\_\_and\_\_future\_directions}

\subsection{Adversarial Hallucination and Robustness}
\label{sec:7\_1\_adversarial\_hallucination\_\_and\_\_robustness}

The increasing deployment of Large Language Models (LLMs) and multimodal models (MLLMs) in sensitive applications necessitates a critical focus on their resilience against deliberate manipulation. This subsection delves into the emerging and crucial field of adversarial attacks specifically designed to induce hallucinations, examining how exploiting inherent vulnerabilities reveals fundamental weaknesses and drives the development of proactive robustness engineering. Understanding these techniques is paramount, not merely for creating new attacks, but for building robust AI systems capable of withstanding sophisticated, targeted manipulations.

\subsubsection{Adversarial Attack Methods and Exploitable Vulnerabilities}
Adversarial hallucination attacks aim to intentionally mislead LLMs and MLLMs into generating factually incorrect or unfaithful content. These attacks often exploit specific architectural weaknesses or biases learned during training. A prominent example in MLLMs is the "attention sink" phenomenon, identified by \cite{wang2025jen}. Their work proposes a novel attack that manipulates attention scores and hidden embeddings to trigger these sinks, leading to the generation of visually uninterpretable or misleading content. This attack significantly increases hallucinated sentences and words, revealing how instruction-tuning can inadvertently create "two-segment response" patterns with declining image-text relevance, exposing a fundamental weakness in model faithfulness.

Beyond architectural vulnerabilities, linguistic and data-driven biases can also be exploited. \cite{han202439z} reveal a "semantic shift bias" in LVLMs, where the presence of paragraph breaks (\texttt{\n\n}) in training data leads models to associate subsequent content with a semantic shift, increasing the likelihood of hallucination. Crucially, they demonstrate that strategically inserting \texttt{\n\n} can \textit{induce} multimodal hallucinations, providing a novel and simple attack mechanism. This highlights how subtle structural elements can be leveraged to corrupt model output. Similarly, \cite{omar2025us3} demonstrate the high susceptibility of LLMs to adversarial hallucination attacks in clinical decision support. By embedding fabricated details (e.g., false lab results) into clinical prompts, they show that LLMs frequently elaborate on this false information, posing significant risks in high-stakes domains. This type of attack directly targets the LLM's knowledge retrieval and generation capabilities, forcing it to "hallucinate" details consistent with the fabricated premise.

Another form of adversarial manipulation, closely related to inducing misinformation, is explored by \cite{zhou2024b0u}. They identify that LLMs are "involuntary truth-tellers" and struggle to generate genuinely fallacious or deceptive reasoning. Exploiting this "fallacy failure," they propose a jailbreak attack where an LLM is prompted to generate a fallacious yet seemingly real procedure for harmful behavior. The model, unable to fabricate a truly fallacious solution, instead proposes a truthful (and thus harmful) one, bypassing safety mechanisms. While framed as a jailbreak, this method effectively induces the model to generate factually harmful content under a deceptive premise, akin to an adversarial hallucination of a "safe" procedure.

It is important to distinguish these direct adversarial attacks from diagnostic benchmarks that use adversarial-style inputs to probe model weaknesses. For instance, \cite{sungbin2024r2g} introduces AVHBench, which uses synthetic videos with swapped audio to create natural mismatches, intentionally inducing cross-modal hallucinations to evaluate an Audio-Visual LLM's discernment. Similarly, \cite{guan2023z15} utilizes human-edited images within their HALLUSION BENCH to challenge LVLMs' robustness against "language hallucination" and "visual illusion." While these tools are invaluable for revealing how models misinterpret subtle relationships or over-rely on one modality, they are primarily diagnostic rather than malicious attack vectors. However, understanding the vulnerabilities they expose, such as the "Multimodal Hallucination Snowballing" phenomenon identified by \cite{zhong2024mfi} where an LVLM's own previously generated hallucinations influence subsequent responses, is critical for anticipating potential adversarial exploitation.

\subsubsection{Proactive Robustness Engineering: Defenses Against Adversarial Hallucination}
Understanding these vulnerabilities and attack vectors is crucial for developing more robust AI systems, shifting the paradigm from purely reactive mitigation to proactive robustness engineering. This involves building strong defenses against sophisticated, targeted manipulations, ensuring models can withstand deliberate attempts to make them hallucinate. Proactive robustness techniques can be broadly categorized into training-based and inference-time interventions, often designed to strengthen grounding, balance modality priors, or enhance critical self-correction.

In the realm of \textbf{training-based robustness engineering}, methods aim to instill hallucination resistance directly into the model's learned parameters, making them inherently more resilient to adversarial prompts. \cite{wu2024n00} proposes NoiseBoost, a generalizable technique that injects Gaussian noise into projected visual tokens during supervised fine-tuning (SFT) or reinforcement learning (RL). This perturbation compels the MLLM to distribute attention more evenly between visual and linguistic tokens, thereby reducing over-reliance on language priors that adversaries might exploit to induce visual hallucinations. Addressing fine-grained hallucinations, \cite{wang2023ubf} introduces ReCaption, a framework that fine-tunes Large Vision-Language Models (LVLMs) using diverse rewritten captions generated by ChatGPT. This approach enhances fine-grained visual-text alignment, making models more robust to subtle attribute and behavior inaccuracies that an adversary might attempt to corrupt. Further advancing training-based defenses, \cite{deng202405j} presents an efficient "judge-free" self-improvement framework for MLLMs. This method generates controllable negative samples by blending conditional and unconditional decoding paths with a "hallucination ratio" and uses a lightweight CLIP-based verifier for Direct Preference Optimization (DPO), proactively training models to resist generating hallucinatory content even when subtly prompted towards it. More broadly, \cite{liu2024gxh} highlights the role of AI alignment training in reducing LLMs' propensity for misinformation and improving their ability to refuse malicious instructions, forming a foundational layer of defense against adversarial attempts to induce harmful hallucinations.

\textbf{Inference-time and post-hoc interventions} provide flexible, training-free solutions to enhance robustness in deployed models, often directly countering the vulnerabilities exploited by adversarial attacks. A direct countermeasure to the \texttt{\n\n}-induced hallucination attack by \cite{han202439z} is their proposed MiHO (Mitigating Hallucinations during Output) method. By adjusting the decoding strategy to reduce the logits of the \texttt{\n} token, MiHO effectively prevents its generation, thereby suppressing the semantic shift bias and reducing hallucination without retraining. This exemplifies a targeted defense against a specific adversarial mechanism.

Other inference-time strategies focus on strengthening visual grounding and balancing modality priors, directly countering attacks like the "attention sink." \cite{zou2024dp7} introduces MemVR (Memory-space Visual Retracing), a decoding paradigm that re-injects visual tokens into the MLLM's middle layers, dynamically activating when the model exhibits high uncertainty. This reinforces visual memory, mitigating the "amnesia" of visual information that can be exploited by adversaries to induce hallucinations. Similarly, \cite{zhou2024lvp} proposes CAUSAL MM, a causal inference framework that applies back-door adjustment and counterfactual reasoning to attention mechanisms. By treating modality priors as confounding factors, CAUSAL MM balances visual and language attention, making model outputs more aligned with multimodal inputs and less susceptible to prior-induced hallucinations that an adversary might leverage. \cite{park20247cm} presents ConVis, a novel contrastive decoding method that leverages a Text-to-Image (T2I) model to visualize potential hallucinations from the MLLM's initial caption. By comparing logit distributions from the original and T2I-reconstructed images, ConVis penalizes the generation of visualized hallucinations, offering a unique visual feedback loop for robustness against visually-grounded attacks. The Residual Visual Decoding (RVD) method by \cite{zhong2024mfi} also falls into this category, mitigating hallucination snowballing by integrating residual visual input to revise output distributions during generation, thereby defending against an adversary attempting to exploit this internal vulnerability.

Furthermore, critical reasoning and external validation at inference time are crucial. \cite{kim2024ozf} introduces Counterfactual Inception, a training-free method that prompts LMMs to generate and then explicitly avoid "counterfactual keywords" that deviate from visual content. This encourages a more critical self-correction process, making it harder for adversarial prompts to inject and propagate false information. \cite{qu2024pqc} proposes MVP (Multi-View Multi-Path Reasoning), a framework that maximizes an LVLM's innate capabilities by seeking multi-view information and employing certainty-driven decoding. By exploring multiple reasoning paths and selecting answers with high certainty, MVP significantly alleviates hallucinations without external tools or retraining, offering a robust approach against adversarial attempts to force a single, hallucinated conclusion.

In conclusion, the study of adversarial hallucination marks a crucial shift towards understanding and proactively defending against sophisticated manipulations of AI models. By exposing vulnerabilities like the attention sink, semantic shift biases, and the propensity to elaborate on fabricated details, researchers are developing a diverse arsenal of training-based and inference-time techniques to build more resilient systems. The ongoing challenge lies in creating defenses that are both effective against increasingly sophisticated attacks and efficient enough for real-world deployment, while ensuring that robustness measures do not inadvertently compromise other desirable model characteristics like fluency or creativity. Future work will likely focus on deeper causal understanding of model failures and adaptive, multi-layered defense strategies to ensure the credibility and trustworthiness of AI in complex, real-world scenarios.
\subsection{Safety-Critical Applications and Guardrails}
\label{sec:7\_2\_safety-critical\_applications\_\_and\_\_guardrails}

In domains where the consequences of AI errors are profound and potentially irreversible, such as medical diagnosis, legal counsel, financial advisories, or critical infrastructure management, the generation of erroneous or hallucinatory content by Large Language Models (LLMs) transitions from a mere inconvenience to a catastrophic risk. These high-stakes environments necessitate a paradigm shift from general factual accuracy to absolute error prevention, aiming to eliminate "never events"errors that are entirely preventable and unacceptable \cite{hakim2024d4u}. For example, in clinical settings, LLMs analyzing unstructured medical notes can exhibit significant hallucination, leading to inaccuracies in extracting critical patient information, which could directly impact diagnosis and treatment \cite{shah20242sx}. This underscores the profound ethical and practical imperative for robust safeguards. This subsection explores the development and implementation of specialized "guardrails" and application-specific mechanisms designed to ensure the highest levels of dependability, accountability, and credibility in AI systems deployed in these sensitive real-world settings.

Guardrails, in this context, represent explicit enforcement mechanisms designed to constrain LLM behavior and output within predefined safety and operational boundaries. Beyond merely detecting or mitigating hallucinations, they act as a critical layer of defense, preventing the generation of harmful, factually incorrect, or otherwise undesirable information before it reaches end-users \cite{gao20242nu}. While the term "guardrails" broadly encompasses mechanisms for preventing prompt injection, ensuring topical relevance, and upholding ethical guidelines, our focus here is primarily on \textbf{semantic guardrails} designed to prevent factual hallucinations in safety-critical applications. These guardrails specifically focus on the meaning and content of the LLM's output, ensuring strict alignment with domain-specific knowledge, safety protocols, and ethical considerations \cite{gao20242nu}.

A pioneering example of such a system is presented by \cite{hakim2024d4u}, who explicitly address the critical need for guardrails in medical safety-critical settings, specifically pharmacovigilance. Their work highlights how application-specific mechanisms can be engineered to prevent "never event" errors in processing Individual Case Safety Reports (ICSRs). They categorize guardrails into "hard" and "soft" types:
\begin{itemize}
    \item \textbf{Hard Semantic Guardrails}: These enforce strict, non-negotiable rules, actively preventing outputs that violate fundamental safety criteria. For instance, their "MISMATCH Guardrail" identifies and flags discrepancies in drug names or adverse event terms between source and generated text, leveraging custom drug dictionaries and medical ontologies (e.g., MedDRA). This mechanism ensures that critical information is neither hallucinated nor omitted, directly preventing errors that could lead to patient harm or regulatory non-compliance.
    \item \textbf{Soft Semantic Guardrails}: These mechanisms communicate uncertainty or flag outputs for human review when confidence is low or input data is anomalous. \cite{hakim2024d4u} introduce Document-wise Uncertainty Quantification (DL-UQ), which uses document embeddings to identify unusual input documents, signaling potential risks in the LLM's processing.
\end{itemize}
This distinction is crucial: hard guardrails aim for absolute prevention of specific, known error types, while soft guardrails manage residual risk by enabling informed human intervention.

Beyond explicit rule enforcement, guardrails are intrinsically linked to the LLM's ability to manage and communicate its inherent uncertainty. Building on the uncertainty quantification methods discussed in Section 4.3, the \textit{semantic entropy} proposed by \cite{tjandra2024umq} serves as a powerful signal for a guardrail system. This label-free abstention mechanism allows LLMs to self-identify high-risk outputs by quantifying the semantic diversity of potential continuations. A guardrail system can then leverage this signal to trigger an abstention response, preventing potentially harmful hallucinations by signaling when human intervention or further verification is required. This operationalizes uncertainty estimation as an active component of an error prevention strategy, acting as a soft guardrail.

Furthermore, ensuring the verifiability and accountability of AI-generated content is indispensable for safety-critical applications. While not a direct guardrail, the ability to trace information back to its source provides a critical layer of human-in-the-loop verification, often mandated in legal, medical, and financial contexts where accountability is non-negotiable. As discussed in Section 4.1, \cite{gao2023ht7} introduced a reproducible benchmark and automatic evaluation framework for enabling LLMs to generate text with verifiable citations. When integrated into a safety-critical workflow, this capability allows domain experts to audit the factual basis of AI claims, serving as an essential support system for outputs that have passed through guardrail checks. It complements guardrails by providing the necessary transparency for post-hoc analysis and ensuring trust in the system's overall credibility.

Implementing robust guardrails presents several significant challenges that extend beyond generic statements about completeness or overhead. A fundamental challenge stems from the inherent "unavoidable nature" of hallucination in LLMs, as discussed in Section 2.3 \cite{li2025qzg}. This theoretical limit implies that guardrails cannot completely eradicate all forms of hallucination but must instead manage and contain them. The brittleness of rule-based systems against the fluidity and ambiguity of natural language poses a continuous threat; subtle linguistic variations or adversarial prompt injections can bypass even well-designed guardrails, potentially inducing harmful outputs or hallucinations \cite{zhou2024b0u, gao20242nu}. Developing comprehensive rule sets for hard guardrails requires deep, often scarce, domain expertise and continuous, labor-intensive iteration, creating a scalability bottleneck. Moreover, formally verifying semantic constraints to guarantee their coverage and prevent adversarial bypasses remains an open research problem. In multimodal safety-critical applications, such as autonomous driving or medical image analysis, guardrails must also contend with cross-modal hallucinations arising from perturbed inputs or misinterpretations of visual information, adding another layer of complexity \cite{ding2024o88, li2023249}. The trade-off between strict error prevention and maintaining the LLM's utility and generative capabilities must be carefully managed, as overly restrictive guardrails can inadvertently limit the system's effectiveness.

Future research must focus on developing more adaptive, dynamic, and self-evolving guardrail systems that can infer rules from documentation or safety cases, reducing reliance on manual expert-driven creation. Advances in formal verification techniques are crucial to provide provable guarantees for guardrail effectiveness. Furthermore, integrating advanced uncertainty quantification and self-reflection mechanisms more deeply into guardrail architectures will enable more nuanced risk management and informed human intervention.

In conclusion, the deployment of AI in safety-critical applications necessitates a dedicated focus on guardrails as explicit enforcement mechanisms for absolute error prevention. The integration of application-specific semantic guardrails, as exemplified by \cite{hakim2024d4u}'s work in pharmacovigilance, alongside uncertainty-triggered abstention mechanisms leveraging signals like semantic entropy \cite{tjandra2024umq}, represents a significant advancement. These are complemented by supporting infrastructures like verifiable citation generation \cite{gao2023ht7} that enhance accountability. Addressing the inherent limitations of LLMs and the complexities of natural language, the development of robust, verifiable, and ethically sound guardrail architectures is paramount to building truly dependable AI systems capable of responsible deployment in the most sensitive real-world scenarios.
\subsection{Meta-Evaluation and Unified Theoretical Frameworks}
\label{sec:7\_3\_meta-evaluation\_\_and\_\_unified\_theoretical\_frameworks}

As the field of AI hallucination research matures, moving beyond initial problem identification and ad-hoc solutions, a critical dual focus has emerged: the rigorous meta-evaluation of existing benchmarks and the ongoing pursuit of a unified theoretical framework. This forward-looking perspective, building upon the diverse detection methodologies discussed in Section 3 and the mechanistic insights from Section 2.2, aims to ensure the scientific rigor of research, guide the development of future dependable AI systems, and establish a robust foundation for advancements in AI reliability.

The proliferation of diverse and sophisticated evaluation methodologies, while essential for progress, simultaneously necessitates a critical assessment of these tools themselves. True meta-evaluation involves scrutinizing the quality, reliability, and validity of hallucination benchmarks. For instance, while benchmarks like POPE \cite{wang2023zop} were foundational for object hallucination detection in Large Vision-Language Models (LVLMs), subsequent meta-analysis by \cite{wang2023zop} revealed significant prompt bias, demonstrating that these benchmarks could inadvertently exploit model judgment biases rather than accurately measuring real-world hallucination. This highlights a critical flaw in relying solely on primary evaluation metrics without a deeper understanding of their underlying mechanisms and potential vulnerabilities. Similarly, \cite{malin2024fin} provides a review of faithfulness metrics across various tasks, implicitly performing a meta-evaluation by correlating their effectiveness with human judgment, underscoring that LLM-based evaluators often achieve the highest correlation. This kind of comparative analysis is crucial for identifying robust and trustworthy evaluation paradigms. Furthermore, comprehensive surveys like those by \cite{bai2024tkm} and \cite{lan20240yz} contribute to meta-evaluation by systematically categorizing existing benchmarks and discussing their limitations, scope, and applicability. They reveal the fragmentation of evaluation efforts and the need for standardized, cross-comparable metrics that are less susceptible to dataset biases or prompt engineering. The challenge lies in developing meta-benchmarks or frameworks that can consistently compare the efficacy, scope, and validity of diverse hallucination detection methods, ensuring that the tools used to measure hallucination are themselves robust and reliable. Without such a meta-evaluative layer, the field risks optimizing for flawed metrics, leading to an illusion of progress rather than genuine advancement in mitigating hallucination.

Complementing the advancements and critiques in evaluation, the field is increasingly gravitating towards a unified theoretical understanding of hallucination, integrating empirical observations with fundamental mathematical insights. This conceptual shift is a direct response to the limitations of purely empirical taxonomies and mechanistic analyses (as seen in Section 2.2), seeking to uncover the underlying principles governing hallucination. A pivotal contribution in this direction is presented by \cite{li2025qzg}, who propose a unified theoretical framework for LLM hallucination, offering a formal mathematical definition and exploring its origins, such as undecidability principles, alongside empirical causes related to data and architecture. While this framework offers a powerful conceptual lens, its practical utility in guiding the development of specific, universally applicable mitigation techniques remains an underexplored but critical next step. Further deepening this theoretical grounding, \cite{karbasi2025j7n} rigorously investigate the (im)possibility of automated hallucination detection, proving that while detection is fundamentally impossible for most language collections when relying solely on correct examples, it becomes feasible with expert-labeled feedback (both positive and negative examples). This theoretical insight provides crucial guidance for the design of effective hallucination detectors, validating the importance of methods like Reinforcement Learning with Human Feedback (RLHF) and highlighting the inherent limitations of unsupervised detection.

Mechanistic insights also continue to integrate with theoretical frameworks. For instance, \cite{zhang2024qq9} identified "knowledge overshadowing" as a novel root cause of amalgamated hallucinations, where dominant conditions in training data lead to over-generalization. Their work provides a specific mechanistic insight into how factual inaccuracies can arise even with correct data, supported by a derived generalization bound, thereby integrating empirical observation with a mathematical explanation of its limits. Similarly, \cite{jiang20242kz} delves into the inference dynamics of LLMs, revealing how output token probabilities evolve across layers, distinguishing between correct and hallucinated cases. This fine-grained analysis of internal model states offers a mechanistic basis for understanding \textit{why} hallucinations occur even when models possess the correct knowledge, providing a pathway for more principled detection and mitigation. Furthermore, \cite{rejeleene2024okw} proposes a novel mathematical formulation for Information Quality (IQ) in LLMs, defining it as a weighted function of consistency, relevance, and accuracy. This framework, while conceptual, represents an important step towards formalizing the very qualities that hallucination undermines, offering a quantifiable target for theoretical and empirical improvements.

In conclusion, the dual focus on meta-evaluation and unified theoretical frameworks signifies a critical maturation in hallucination research. The field must now rigorously evaluate its evaluation tools, moving beyond simply applying benchmarks to critically assessing their biases, scope, and correlation with human judgment, as exemplified by critiques of existing LVLM benchmarks \cite{wang2023zop} and reviews of faithfulness metrics \cite{malin2024fin}. Concurrently, the pioneering work on unified theoretical frameworks \cite{li2025qzg}, the theoretical limits of detection \cite{karbasi2025j7n}, and the deepening mechanistic insights with mathematical grounding \cite{zhang2024qq9, jiang20242kz} are paving the way for a deeper scientific understanding of AI reliability. This integrated approach, combining robust evaluation of evaluation methodologies with fundamental theoretical insights, is essential for guiding the development of truly dependable, trustworthy, and scientifically grounded AI systems in the future, moving beyond reactive fixes to principled, proactive solutions.


\label{sec:conclusion}

\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Key Developments}
\label{sec:8\_1\_summary\_of\_key\_developments}

The intellectual journey in addressing hallucination within Large Language Models (LLMs) and their multimodal successors reflects a profound maturation in the pursuit of trustworthy artificial intelligence. This field has rapidly evolved from initial problem characterization to sophisticated management strategies, fundamentally shifting the paradigm from viewing hallucination as a solvable bug to acknowledging its inherent nature. This section provides a concise recap of these major milestones, highlighting the progression from foundational definitions and detection methods to diverse mitigation strategies, the crucial expansion into multimodal AI, and the emerging focus on advanced topics like adversarial robustness and safety-critical applications, underscoring the field's evolving approach to AI dependability across all modalities.

Early research meticulously focused on defining and characterizing hallucinations, primarily within text-based LLMs. Foundational work established critical distinctions between intrinsic errors (contradicting source input) and extrinsic errors (adding unsupportable, often false, information), particularly in tasks like abstractive summarization \cite{maynez2020h3q}. This initial understanding paved the way for comprehensive taxonomies that categorized hallucinations by their origin and nature, such as input-conflicting, context-conflicting, and fact-conflicting errors \cite{zhang2023k1j}. The challenge of fact-conflicting hallucinations, lacking immediate ground truth, underscored the need for more robust evaluation. This phase also saw the development of initial profiling frameworks and efforts to attribute hallucination causes through association analysis, laying the groundwork for understanding the mechanistic origins of these failures \cite{du2023qu7}.

As the understanding of hallucination deepened, detection methodologies became increasingly sophisticated, driven by the need for more granular, efficient, and black-box compatible evaluations. The field transitioned from relying on reference-dependent metrics to pioneering reference-free approaches that leveraged the internal consistency of LLM outputs \cite{manakul20236ex}. Further refinements included self-contained detection mechanisms employing metamorphic relations and prompt mutation to generate diverse test cases, enhancing detection accuracy and generalizability \cite{yang20251dw}. The increasing complexity of LLM applications also necessitated specialized benchmarks, moving towards fine-grained, token-level or sentence-level analysis, and the verification of LLM reasoning rationales using structured data \cite{oh2024xa3}. The imperative for reliability in high-stakes domains led to the development of domain-specific benchmarks, such as Med-HALT for medical contexts, revealing significant challenges even for state-of-the-art models in complex factual recall \cite{umapathi2023puv}. A notable advancement in verifiability was the development of benchmarks and metrics for enabling LLMs to generate text with explicit and verifiable citations, allowing for automatic evaluation of factual correctness and citation quality \cite{gao2023ht7}.

Mitigation efforts evolved along two complementary axes: external grounding and internal model interventions. External grounding techniques, notably Retrieval-Augmented Generation (RAG), became a cornerstone, enabling LLMs to access and integrate external, verifiable knowledge to reduce factual errors. The seminal ReAct paradigm further advanced this by demonstrating the power of interleaving an LLM's internal reasoning with external actions (e.g., API calls or tool use) to dynamically gather information and self-correct responses in interactive environments \cite{yao20229uz}. This approach was further refined by frameworks that dynamically interwove Chain-of-Thought reasoning with iterative knowledge retrieval \cite{trivedi2022qsf}. While highly effective, RAG's limitations, particularly in noise robustness and handling counterfactuals, spurred diagnostic benchmarks and the development of adaptive RAG frameworks that trigger retrieval only when hallucination is detected in real-time \cite{chen2023h04, su2024gnz}. The integration of Knowledge Graphs (KGs) also proved instrumental, allowing LLMs to reason over structured factual repositories and dynamically adapt heterogeneous knowledge sources for enhanced factual consistency \cite{wen2023t6v, sui20242u1}.

Complementing external grounding, internal strategies focused on steering model behavior during decoding or embedding hallucination resistance during training. Decoding-time techniques included contrastive decoding, which penalizes tokens inconsistent with a reference or subtly altered input, and innovative approaches that leverage "induced hallucinations" to guide models toward factuality \cite{zhang202396g}. Self-correction mechanisms, such as Chain-of-Verification (CoVe), empowered LLMs to internally plan and execute verification steps, iteratively refining responses based on consistency checks \cite{dhuliawala2023rqn}. Training-based approaches included robust instruction tuning with negative examples and preference optimization techniques like Direct Preference Optimization (DPO), which align model behavior with desired factual accuracy using AI-generated or human-annotated preference data \cite{liu2023882, xiao2024hv1}. More advanced interventions delved into manipulating internal model states, such as constraining specific attention heads responsible for "false premise hallucinations," demonstrating significant performance gains by targeting mechanistic origins \cite{yuan2024o7d}.

A pivotal intellectual shift in the field was the theoretical acknowledgment of hallucination's inherent nature. Groundbreaking theoretical proofs, notably employing diagonalization arguments, demonstrated that hallucination is an inevitable and unavoidable characteristic for \textit{any computable LLM}, regardless of architectural advancements or training improvements \cite{xu2024n76}. This fundamental insight, reinforced by theoretical frameworks establishing the inherent impossibility of automated hallucination detection without expert-labeled negative examples \cite{karbasi2025j7n}, fundamentally reshaped the research paradigm. The focus moved decisively from the ambition of complete elimination to the pragmatic necessity of robust detection, effective mitigation, and responsible management, marking a significant maturation of the field's approach to AI dependability.

This theoretical grounding spurred the crucial expansion of research into multimodal AI, where hallucination presents unique and complex challenges due to the inherent "modality gap" and cross-modal inconsistencies \cite{lan20240yz, liu2024sn3}. Early work probed object hallucination in Vision-Language Pre-training (VLP) models \cite{dai20229aa}, leading to systematic empirical studies and specialized evaluation methods for various multimodal hallucination types, including object, relationship, temporal, and cross-modal driven errors \cite{li2023249, wu2024bxt, kuan20249pm, sungbin2024r2g, wang2024rta}. The increasing number and complexity of these benchmarks necessitated meta-evaluation to ensure their reliability and validity \cite{yan2024ux8}. Multimodal mitigation strategies also diversified, employing both training-based and inference-time approaches, such as robust instruction tuning with negative examples \cite{liu2023882}, visual grounding techniques \cite{yin2025s2b}, and advanced causal inference frameworks that mitigate modality prior-induced hallucinations by deciphering attention causality \cite{zhou2024lvp}. Recent work has also advanced adversarial generation of diverse visual hallucination instances to build more robust multimodal models \cite{huang20247wn}.

The field's maturation is further evidenced by the emergence of advanced topics that push beyond current paradigms. Research into adversarial hallucination actively probes model vulnerabilities to build more robust systems, revealing that LLMs are highly susceptible to fabricated details in prompts, especially in high-stakes domains like clinical decision support \cite{omar2025us3}. This necessitates a shift towards proactive robustness engineering. Concurrently, the development of specialized guardrails for safety-critical applications has become paramount, focusing on preventing "never event" errors and explicitly communicating uncertainty in domains like pharmacovigilance \cite{hakim2024d4u}. These developments, alongside the ongoing pursuit of unified theoretical frameworks and meta-evaluation of benchmarks, signify a comprehensive intellectual journey towards fundamentally dependable and trustworthy AI systems.
\subsection{Remaining Challenges and Open Questions}
\label{sec:8\_2\_remaining\_challenges\_\_and\_\_open\_questions}

Despite significant advancements in understanding, detecting, and mitigating hallucinations in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), several persistent challenges and open questions continue to shape the research agenda. The theoretical proof that hallucination is an inherent and inevitable limitation for all computable LLMs, regardless of their architecture or training \cite{xu2024n76}, fundamentally shifts the paradigm from eradication to robust management, underscoring the enduring nature of this problem.

One primary challenge lies in the \textbf{scalability and fine-grained nature of hallucination evaluation}. Early efforts relied on costly human evaluation \cite{maynez2020h3q}, which is not scalable for the rapid development of LLMs. While methods like \textit{SelfCheckGPT} \cite{manakul20236ex} introduced zero-resource black-box detection, they still face computational costs and limitations in sampling quality. The need for fine-grained, token-level, and reference-free detection led to benchmarks like HADES \cite{liu2021mo6}, but these are often derived from specific text sources and may not capture the full spectrum of LLM errors. In the multimodal domain, initial object hallucination evaluations like POPE \cite{li2023249} focused on coarse-grained presence/absence, leading to subsequent work addressing more complex relationship hallucinations with R-Bench \cite{wu2024bxt} and cross-modal interactions with AVHBench \cite{sungbin2024r2g}. However, the quality of these benchmarks themselves has been questioned, with frameworks like HQM/HQH \cite{yan2024ux8} revealing issues like response bias and misalignment with human judgment. The sheer diversity of hallucination types, including multi-object \cite{chen2024vy7}, temporal \cite{li2024wyb}, and event hallucinations \cite{jiang2024792}, necessitates increasingly sophisticated and universal evaluation frameworks. Furthermore, the challenge of evaluating hallucinations in unconstrained, free-form generation \cite{kaul2024ta7} and dialogue-level contexts \cite{chen2024c4k} remains formidable, even for unified detection frameworks like UNIHD \cite{chen2024lc5}. Automated dataset generation tools like AutoHall \cite{cao2023ecl} offer scalability but often focus on specific hallucination types and rely on other LLMs for classification, potentially inheriting their biases.

Another critical area is the \textbf{generalizability of mitigation techniques across diverse tasks and domains}, coupled with the need for a \textbf{deeper understanding of complex causal mechanisms}. Initial retrieval-augmented generation (RAG) approaches like ReAct \cite{yao20229uz} and IRCoT \cite{trivedi2022qsf} improved grounding by interleaving reasoning with external tool use or retrieval. However, these methods often faced limitations in handling complex, multi-step scenarios, were sensitive to prompt engineering, or struggled with the quality and quantity of retrieved information, as highlighted by the ALCE benchmark for citation generation \cite{gao2023ht7} and the RGB benchmark's analysis of RAG's shortcomings in noise robustness and information integration \cite{chen2023h04}. Self-correction strategies, as surveyed by \cite{pan2023mwu, pan2024y3a}, such as Chain-of-Verification (CoVe) \cite{dhuliawala2023rqn}, empower LLMs to self-critique but can be computationally expensive or rely solely on the model's internal knowledge. Integrating Knowledge Graphs (KGs) offers structured grounding \cite{wen2023t6v, sui20242u1, dziri2021bw9, li2023v3v}, but faces challenges in KG quality, maintenance, and the effective integration of graphical structures into LLM reasoning. Dynamic RAG methods like DRAD \cite{su2024gnz} aim for efficiency by triggering retrieval only when hallucinations are detected, yet their effectiveness relies on the accuracy of real-time uncertainty correlation. In multimodal models, mitigation becomes even more complex. Training-based methods like robust instruction tuning with negative examples \cite{liu2023882} or fine-tuning with caption rewrites \cite{wang2023ubf} require extensive data. Inference-time methods like Woodpecker \cite{yin2023hx3} and Dentist \cite{chang2024u3t} leverage external expert models or LLM-as-judge paradigms, introducing dependencies. The phenomenon of "multimodal hallucination snowballing" \cite{zhong2024mfi}, where initial errors propagate, further complicates mitigation. Efforts to understand the causal mechanisms, such as CAUSAL MM \cite{zhou2024lvp} which intervenes on attention to balance modality priors, are emerging but often rely on assumptions about confounding factors. The challenge of balancing visual and linguistic priors without compromising content quality or inference speed remains, as seen in methods like ClearSight/VAF \cite{yin2025s2b}, MemVR \cite{zou2024dp7}, and ICT \cite{chen2024j0g}.

Finally, the \textbf{ethical implications of deploying systems with inherent hallucinatory tendencies} are paramount. The inevitability of hallucination \cite{xu2024n76} means that LLMs, even with the best mitigation, will occasionally produce untruthful content. This necessitates a focus on \textbf{truly transparent and accountable LLMs}. The ability to generate text with verifiable citations, as benchmarked by ALCE \cite{gao2023ht7}, is a step towards accountability, as is the generation of "mind maps" for reasoning transparency \cite{wen2023t6v}. However, the economic and societal impact of unreliable LLM outputs \cite{rejeleene2024okw} underscores the urgency of preventing and detecting misinformation \cite{liu2024gxh}. Furthermore, the vulnerability to adversarial attacks that exploit internal mechanisms like attention sinks to induce hallucinations \cite{wang2025jen} highlights the need for robust security. Future directions must emphasize the ongoing pursuit of \textbf{trustworthy AI and responsible innovation}. This includes developing more adaptive and context-aware AI that can dynamically assess its own uncertainty and seek external validation, as well as creating systems that are not only accurate but also explainable, allowing users to understand and verify their outputs. The goal is to build LLMs that are not just powerful, but also reliable, transparent, and ethically sound for deployment in high-stakes domains like medicine \cite{chen2024hfe, ji2023vhv}.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{277}

\bibitem{vu202337s}
Tu Vu, Mohit Iyyer, Xuezhi Wang, et al. (2023). \textit{FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chang2024u3t}
Yue Chang, Liqiang Jing, Xiaopeng Zhang, et al. (2024). \textit{A Unified Hallucination Mitigation Framework for Large Vision-Language Models}. Trans. Mach. Learn. Res..

\bibitem{wang2024vym}
Jiaqi Wang, Yifei Gao, and Jitao Sang (2024). \textit{VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding}. arXiv.org.

\bibitem{niu2024v97}
Mengjia Niu, Hao Li, Jie Shi, et al. (2024). \textit{Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval}. arXiv.org.

\bibitem{liu2024gxh}
Aiwei Liu, Qiang Sheng, and Xuming Hu (2024). \textit{Preventing and Detecting Misinformation Generated by Large Language Models}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2023v3v}
Xingxuan Li, Ruochen Zhao, Yew Ken Chia, et al. (2023). \textit{Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources}. International Conference on Learning Representations.

\bibitem{liang2024hoo}
Mengfei Liang, Archish Arun, Zekun Wu, et al. (2024). \textit{THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models}. arXiv.org.

\bibitem{zhang2023k1j}
Yue Zhang, Yafu Li, Leyang Cui, et al. (2023). \textit{Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}. Computational Linguistics.

\bibitem{zhou2024lvp}
Guanyu Zhou, Yibo Yan, Xin Zou, et al. (2024). \textit{Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality}. International Conference on Learning Representations.

\bibitem{bai2024tkm}
Zechen Bai, Pichao Wang, Tianjun Xiao, et al. (2024). \textit{Hallucination of Multimodal Large Language Models: A Survey}. arXiv.org.

\bibitem{yin2023hx3}
Shukang Yin, Chaoyou Fu, Sirui Zhao, et al. (2023). \textit{Woodpecker: Hallucination Correction for Multimodal Large Language Models}. Science China Information Sciences.

\bibitem{cao2023ecl}
Zouying Cao, Yifei Yang, and Hai Zhao (2023). \textit{AutoHall: Automated Hallucination Dataset Generation for Large Language Models}. arXiv.org.

\bibitem{wu2024bxt}
Ming-Kuan Wu, Jiayi Ji, Oucheng Huang, et al. (2024). \textit{Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models}. International Conference on Machine Learning.

\bibitem{ghosh2024tj5}
Bishwamittra Ghosh, Sarah Hasan, Naheed Anjum Arafat, et al. (2024). \textit{Logical Consistency of Large Language Models in Fact-checking}. International Conference on Learning Representations.

\bibitem{gao2023ht7}
Tianyu Gao, Howard Yen, Jiatong Yu, et al. (2023). \textit{Enabling Large Language Models to Generate Text with Citations}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yang20251dw}
Borui Yang, Md Afif Al Mamun, Jie M. Zhang, et al. (2025). \textit{Hallucination Detection in Large Language Models with Metamorphic Relations}. Proc. ACM Softw. Eng..

\bibitem{zhang2025pex}
Yongheng Zhang, Xu Liu, Ruoxi Zhou, et al. (2025). \textit{CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xing2024itg}
Shangyu Xing, Fei Zhao, Zhen Wu, et al. (2024). \textit{EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{leng2023ohr}
Sicong Leng, Hang Zhang, Guanzheng Chen, et al. (2023). \textit{Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding}. Computer Vision and Pattern Recognition.

\bibitem{kim2024ozf}
Junho Kim, Yeonju Kim, and Yonghyun Ro (2024). \textit{What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2024wyb}
Chaoyu Li, Eun Woo Im, and Pooyan Fazli (2024). \textit{VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding}. Computer Vision and Pattern Recognition.

\bibitem{ji20243j6}
Ziwei Ji, Yuzhe Gu, Wenwei Zhang, et al. (2024). \textit{ANAH: Analytical Annotation of Hallucinations in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{gao20232zb}
Yunfan Gao, Yun Xiong, Xinyu Gao, et al. (2023). \textit{Retrieval-Augmented Generation for Large Language Models: A Survey}. arXiv.org.

\bibitem{ji20227ii}
Ziwei Ji, Zihan Liu, Nayeon Lee, et al. (2022). \textit{RHO ($$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{adams202289x}
Griffin Adams, Han-Chin Shing, Q. Sun, et al. (2022). \textit{Learning to Revise References for Faithful Summarization}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{su2024gnz}
Weihang Su, Yichen Tang, Qingyao Ai, et al. (2024). \textit{Mitigating Entity-Level Hallucination in Large Language Models}. SIGIR-AP.

\bibitem{du2023qu7}
LI DU, Yequan Wang, Xingrun Xing, et al. (2023). \textit{Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis}. arXiv.org.

\bibitem{ji2023vhv}
Ziwei Ji, Tiezheng Yu, Yan Xu, et al. (2023). \textit{Towards Mitigating Hallucination in Large Language Models via Self-Reflection}. arXiv.org.

\bibitem{pan2023mwu}
Liangming Pan, Michael Stephen Saxon, Wenda Xu, et al. (2023). \textit{Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies}. arXiv.org.

\bibitem{kang20238j0}
Haoqiang Kang, and Xiao-Yang Liu (2023). \textit{Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination}. arXiv.org.

\bibitem{kang202378c}
Haoqiang Kang, Juntong Ni, and Huaxiu Yao (2023). \textit{Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification}. arXiv.org.

\bibitem{dong20223yz}
Yue Dong, J. Wieting, and Pat Verga (2022). \textit{Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{qu20240f7}
Xiaoye Qu, Mingyang Song, Wei Wei, et al. (2024). \textit{Mitigating Multilingual Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{mckenna2023pzc}
Nick McKenna, Tianyi Li, Liang Cheng, et al. (2023). \textit{Sources of Hallucination by Large Language Models on Inference Tasks}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{rejeleene2024okw}
Rick Rejeleene, Xiaowei Xu, and John R. Talburt (2024). \textit{Towards Trustable Language Models: Investigating Information Quality of Large Language Models}. arXiv.org.

\bibitem{liu2024p39}
Xinxin Liu (2024). \textit{A Survey of Hallucination Problems Based on Large Language Models}. Applied and Computational Engineering.

\bibitem{liu2024sn3}
Hanchao Liu, Wenyuan Xue, Yifei Chen, et al. (2024). \textit{A Survey on Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{chen2024hfe}
Jiawei Chen, Dingkang Yang, Tong Wu, et al. (2024). \textit{Detecting and Evaluating Medical Hallucinations in Large Vision Language Models}. arXiv.org.

\bibitem{li2024hdc}
Qing Li, Chenyang Lyu, Jiahui Geng, et al. (2024). \textit{Reference-free Hallucination Detection for Large Vision-Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yan2024ux8}
Bei Yan, Jie Zhang, Zheng Yuan, et al. (2024). \textit{Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models}. arXiv.org.

\bibitem{wang2024rta}
Yuxuan Wang, Yueqian Wang, Dongyan Zhao, et al. (2024). \textit{VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models}. arXiv.org.

\bibitem{xie2024l8a}
Yuxi Xie, Guanzhen Li, Xiao Xu, et al. (2024). \textit{V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{guan2023z15}
Tianrui Guan, Fuxiao Liu, Xiyang Wu, et al. (2023). \textit{Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models}. Computer Vision and Pattern Recognition.

\bibitem{liang20236sh}
Xun Liang, Shichao Song, Simin Niu, et al. (2023). \textit{UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{huang2023akj}
Lei Huang, Weijiang Yu, Weitao Ma, et al. (2023). \textit{A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}. ACM Trans. Inf. Syst..

\bibitem{lv2024k5x}
Qitan Lv, Jie Wang, Hanzhu Chen, et al. (2024). \textit{Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models}. International Conference on Machine Learning.

\bibitem{gu202414e}
Yuzhe Gu, Ziwei Ji, Wenwei Zhang, et al. (2024). \textit{ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models}. Neural Information Processing Systems.

\bibitem{huang20247wn}
Wen Huang, Hongbin Liu, Minxin Guo, et al. (2024). \textit{Visual Hallucinations of Multi-modal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{liu2023882}
Fuxiao Liu, Kevin Lin, Linjie Li, et al. (2023). \textit{Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning}. International Conference on Learning Representations.

\bibitem{ding2024o88}
Peng Ding, Jingyu Wu, Jun Kuang, et al. (2024). \textit{Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs}. ACM Multimedia.

\bibitem{rawte2023ao8}
Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, et al. (2023). \textit{The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{pan2024y3a}
Liangming Pan, Michael Stephen Saxon, Wenda Xu, et al. (2024). \textit{Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies}. Transactions of the Association for Computational Linguistics.

\bibitem{li2023rvf}
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, et al. (2023). \textit{HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhou2023zu6}
Yiyang Zhou, Chenhang Cui, Jaehong Yoon, et al. (2023). \textit{Analyzing and Mitigating Object Hallucination in Large Vision-Language Models}. International Conference on Learning Representations.

\bibitem{han202439z}
Zongbo Han, Zechen Bai, Haiyang Mei, et al. (2024). \textit{Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{wang2025jen}
Yining Wang, Mi Zhang, Junjie Sun, et al. (2025). \textit{Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink}. arXiv.org.

\bibitem{qu2024pqc}
Xiaoye Qu, Jiashuo Sun, Wei Wei, et al. (2024). \textit{Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning}. International Conference on Computational Linguistics.

\bibitem{dai20229aa}
Wenliang Dai, Zihan Liu, Ziwei Ji, et al. (2022). \textit{Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training}. Conference of the European Chapter of the Association for Computational Linguistics.

\bibitem{dziri2021bw9}
Nouha Dziri, Andrea Madotto, Osmar Zaiane, et al. (2021). \textit{Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{sungbin2024r2g}
Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, et al. (2024). \textit{AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models}. International Conference on Learning Representations.

\bibitem{hakim2024d4u}
Joe B Hakim, Jeffery L. Painter, D. Ramcharran, et al. (2024). \textit{The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem}. arXiv.org.

\bibitem{li2025qzg}
Chaozhuo Li, Pengbo Wang, Chenxu Wang, et al. (2025). \textit{Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models}. arXiv.org.

\bibitem{wang2023ubf}
Lei Wang, Jiabang He, Shenshen Li, et al. (2023). \textit{Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites}. Conference on Multimedia Modeling.

\bibitem{chen2024c4k}
Kedi Chen, Qin Chen, Jie Zhou, et al. (2024). \textit{DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{ding20244yr}
Hanxing Ding, Liang Pang, Zihao Wei, et al. (2024). \textit{Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models}. arXiv.org.

\bibitem{deng202405j}
Shijian Deng, Wentian Zhao, Yu-Jhe Li, et al. (2024). \textit{Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach}. arXiv.org.

\bibitem{chen2024j0g}
Junzhe Chen, Tianshu Zhang, Shiyu Huang, et al. (2024). \textit{ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models}. Computer Vision and Pattern Recognition.

\bibitem{chen20247jb}
Beitao Chen, Xinyu Lyu, Lianli Gao, et al. (2024). \textit{Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization}. Neural Information Processing Systems.

\bibitem{maynez2020h3q}
Joshua Maynez, Shashi Narayan, Bernd Bohnet, et al. (2020). \textit{On Faithfulness and Factuality in Abstractive Summarization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{qu20246yn}
Xiaoye Qu, Qiyuan Chen, Wei Wei, et al. (2024). \textit{Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation}. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP).

\bibitem{chen2023h04}
Jiawei Chen, Hongyu Lin, Xianpei Han, et al. (2023). \textit{Benchmarking Large Language Models in Retrieval-Augmented Generation}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2024wbi}
Yiyang Zhou, Chenhang Cui, Rafael Rafailov, et al. (2024). \textit{Aligning Modalities in Vision Large Language Models via Preference Fine-tuning}. arXiv.org.

\bibitem{jiang2024792}
Chaoya Jiang, Wei Ye, Mengfan Dong, et al. (2024). \textit{Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models}. ACM Multimedia.

\bibitem{tjandra2024umq}
Benedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, et al. (2024). \textit{Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy}. arXiv.org.

\bibitem{umapathi2023puv}
Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu (2023). \textit{Med-HALT: Medical Domain Hallucination Test for Large Language Models}. Conference on Computational Natural Language Learning.

\bibitem{zou2024dp7}
Xin Zou, Yizhou Wang, Yibo Yan, et al. (2024). \textit{Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models}. arXiv.org.

\bibitem{li2024osp}
Ningke Li, Yuekang Li, Yi Liu, et al. (2024). \textit{Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models}. Proc. ACM Program. Lang..

\bibitem{chuang20248ey}
Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, et al. (2024). \textit{Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2024qrj}
Junyi Li, Jie Chen, Ruiyang Ren, et al. (2024). \textit{The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{wang2023zop}
Junyan Wang, Yi Zhou, Guohai Xu, et al. (2023). \textit{Evaluation and Analysis of Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{xu2024n76}
Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli (2024). \textit{Hallucination is Inevitable: An Innate Limitation of Large Language Models}. arXiv.org.

\bibitem{liu2021mo6}
Tianyu Liu, Yizhe Zhang, C. Brockett, et al. (2021). \textit{A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{manakul20236ex}
Potsawee Manakul, Adian Liusie, and M. Gales (2023). \textit{SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhong2024mfi}
Weihong Zhong, Xiaocheng Feng, Liang Zhao, et al. (2024). \textit{Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2024qq9}
Yuji Zhang, Sha Li, Jiateng Liu, et al. (2024). \textit{Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models}. arXiv.org.

\bibitem{wu20241us}
J. Wu, Tsz Ting Chung, Kai Chen, et al. (2024). \textit{Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models}. Trans. Mach. Learn. Res..

\bibitem{tonmoy20244e4}
S. Tonmoy, S. M. M. Zaman, Vinija Jain, et al. (2024). \textit{A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models}. arXiv.org.

\bibitem{wu2024n00}
Kai Wu, Boyuan Jiang, Zhengkai Jiang, et al. (2024). \textit{NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models}. arXiv.org.

\bibitem{wen2023t6v}
Yilin Wen, Zifeng Wang, and Jimeng Sun (2023). \textit{MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{li2023249}
Yifan Li, Yifan Du, Kun Zhou, et al. (2023). \textit{Evaluating Object Hallucination in Large Vision-Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{lan20240yz}
Wei Lan, Wenyi Chen, Qingfeng Chen, et al. (2024). \textit{A Survey of Hallucination in Large Visual Language Models}. arXiv.org.

\bibitem{dhuliawala2023rqn}
S. Dhuliawala, M. Komeili, Jing Xu, et al. (2023). \textit{Chain-of-Verification Reduces Hallucination in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sui20242u1}
Yuan Sui, and Bryan Hooi (2024). \textit{Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xiao2024hv1}
Wenyi Xiao, Ziwei Huang, Leilei Gan, et al. (2024). \textit{Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback}. arXiv.org.

\bibitem{trivedi2022qsf}
H. Trivedi, Niranjan Balasubramanian, Tushar Khot, et al. (2022). \textit{Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{park20247cm}
Yeji Park, Deokyeong Lee, Junsuk Choe, et al. (2024). \textit{ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models}. AAAI Conference on Artificial Intelligence.

\bibitem{sridhar2022l1c}
A. Sridhar, and Erik M. Visser (2022). \textit{Improved Beam Search for Hallucination Mitigation in Abstractive Summarization}. arXiv.org.

\bibitem{su2024lem}
Weihang Su, Changyue Wang, Qingyao Ai, et al. (2024). \textit{Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{luo2023xyc}
Junyu Luo, Cao Xiao, and Fenglong Ma (2023). \textit{Zero-Resource Hallucination Prevention for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{wu2024h81}
Jun Wu, Q. Liu, Ding Wang, et al. (2024). \textit{Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chen2024vy7}
Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, et al. (2024). \textit{Multi-Object Hallucination in Vision-Language Models}. Neural Information Processing Systems.

\bibitem{zheng20246fk}
Kening Zheng, Junkai Chen, Yibo Yan, et al. (2024). \textit{Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chen2024lc5}
Xiang Chen, Chenxi Wang, Yida Xue, et al. (2024). \textit{Unified Hallucination Detection for Multimodal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sahoo2024hcb}
Pranab Sahoo, Prabhash Meharia, Akash Ghosh, et al. (2024). \textit{A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2024mmj}
Ruiyang Zhang, Hu Zhang, and Zhedong Zheng (2024). \textit{VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation}. arXiv.org.

\bibitem{qiu2024zyc}
Han Qiu, Jiaxing Huang, Peng Gao, et al. (2024). \textit{LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models}. arXiv.org.

\bibitem{oh2024xa3}
Jio Oh, Soyeon Kim, Junseok Seo, et al. (2024). \textit{ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models}. Neural Information Processing Systems.

\bibitem{zhang202396g}
Yue Zhang, Leyang Cui, Wei Bi, et al. (2023). \textit{Alleviating Hallucinations of Large Language Models through Induced Hallucinations}. North American Chapter of the Association for Computational Linguistics.

\bibitem{yin2025s2b}
Hao Yin, Guangzong Si, and Zilei Wang (2025). \textit{ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models}. Computer Vision and Pattern Recognition.

\bibitem{goyal2021onb}
Tanya Goyal, Jiacheng Xu, J. Li, et al. (2021). \textit{Training Dynamics for Text Summarization Models}. Findings.

\bibitem{kuan20249pm}
Chun-Yi Kuan, Wei-Ping Huang, and Hung-yi Lee (2024). \textit{Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models}. Interspeech.

\bibitem{kaul2024ta7}
Prannay Kaul, Zhizhong Li, Hao Yang, et al. (2024). \textit{THRONE: An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Language Models}. Computer Vision and Pattern Recognition.

\bibitem{yebin2024txh}
Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, et al. (2024). \textit{BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models}. European Conference on Computer Vision.

\bibitem{zhao2024ge8}
Linxi Zhao, Yihe Deng, Weitong Zhang, et al. (2024). \textit{Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance}. Unpublished manuscript.

\bibitem{ye2023yom}
Hongbin Ye, Tong Liu, Aijia Zhang, et al. (2023). \textit{Cognitive Mirage: A Review of Hallucinations in Large Language Models}. LKM@IJCAI.

\bibitem{zhang2023k5a}
Hanning Zhang, Shizhe Diao, Yong Lin, et al. (2023). \textit{R-Tuning: Instructing Large Language Models to Say I Dont Know}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2022ypy}
Yanyang Li, Jianqiao Zhao, M. Lyu, et al. (2022). \textit{Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{huang2023du3}
Qidong Huang, Xiao-wen Dong, Pan Zhang, et al. (2023). \textit{OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation}. Computer Vision and Pattern Recognition.

\bibitem{tang2024a1j}
Jianheng Tang, Qifan Zhang, Yuhan Li, et al. (2024). \textit{GraphArena: Evaluating and Exploring Large Language Models on Graph Computation}. International Conference on Learning Representations.

\bibitem{fu2024yqj}
Yuhan Fu, Ruobing Xie, Xingwu Sun, et al. (2024). \textit{Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{yao20229uz}
Shunyu Yao, Jeffrey Zhao, Dian Yu, et al. (2022). \textit{ReAct: Synergizing Reasoning and Acting in Language Models}. International Conference on Learning Representations.

\bibitem{kanthara2022kuj}
Shankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, et al. (2022). \textit{Chart-to-Text: A Large-Scale Benchmark for Chart Summarization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{kim2021obx}
Boseop Kim, Hyoungseok Kim, Sang-Woo Lee, et al. (2021). \textit{What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{xia20224cl}
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, et al. (2022). \textit{Training Trajectories of Language Models Across Scales}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{aharoni2022ioz}
Roee Aharoni, Shashi Narayan, Joshua Maynez, et al. (2022). \textit{mFACE: Multilingual Summarization with Factual Consistency Evaluation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2022p55}
Haopeng Zhang, Semih Yavuz, Wojciech Kryscinski, et al. (2022). \textit{Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control}. NAACL-HLT.

\bibitem{jiang2022reg}
Wenhui Jiang, Minwei Zhu, Yuming Fang, et al. (2022). \textit{Visual Cluster Grounding for Image Captioning}. IEEE Transactions on Image Processing.

\bibitem{wang2020vz6}
Hongmin Wang (2020). \textit{Revisiting Challenges in Data-to-Text Generation with Fact Grounding}. International Conference on Natural Language Generation.

\bibitem{chen2022gkm}
Sihao Chen, S. Buthpitiya, Alex Fabrikant, et al. (2022). \textit{PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{korbak202191w}
Tomasz Korbak, Hady ElSahar, Germn Kruszewski, et al. (2021). \textit{Controlling Conditional Language Models without Catastrophic Forgetting}. International Conference on Machine Learning.

\bibitem{raman20229ce}
K. Raman, Iftekhar Naim, Jiecao Chen, et al. (2022). \textit{Transforming Sequence Tagging Into A Seq2Seq Task}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{liu2021h6c}
Ling Liu, and Mans Hulden (2021). \textit{Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{norlund2021462}
Tobias Norlund, Lovisa Hagstrm, and Richard Johansson (2021). \textit{Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?}. BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP.

\bibitem{wu20206gt}
Tao Wu, E. Chio, Heng-Tze Cheng, et al. (2020). \textit{Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to Cold-Start Search Retrieval}. International Conference on Information and Knowledge Management.

\bibitem{liu2022hw7}
Yongtai Liu, Joshua Maynez, Gonalo Simes, et al. (2022). \textit{Data Augmentation for Low-Resource Dialogue Summarization}. NAACL-HLT.

\bibitem{jelinek2016205}
L. Jelinek, M. Hauschildt, C. Wittekind, et al. (2016). \textit{Efficacy of Metacognitive Training for Depression: A Randomized Controlled Trial}. Psychotherapy and Psychosomatics.

\bibitem{raunak2022r58}
Vikas Raunak, and Arul Menezes (2022). \textit{Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{saha20229lo}
Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, et al. (2022). \textit{MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{ham20213fx}
Soomin Ham, Kibaek Park, Yeongjun Jang, et al. (2021). \textit{KSL-Guide: A Large-scale Korean Sign Language Dataset Including Interrogative Sentences for Guiding the Deaf and Hard-of-Hearing}. IEEE International Conference on Automatic Face & Gesture Recognition.

\bibitem{jeong20180d2}
Eunji Jeong, Sungwoo Cho, Gyeong-In Yu, et al. (2018). \textit{JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs}. Symposium on Networked Systems Design and Implementation.

\bibitem{kedia2022c03}
Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee (2022). \textit{FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{gallace201080s}
A. Gallace, and A. Gallace (2010). \textit{Touch and the body: The role of the somatosensory cortex in tactile awareness}. Unpublished manuscript.

\bibitem{li20203k7}
Xintong Li, Aleksandre Maskharashvili, S. Stevens-Guille, et al. (2020). \textit{Leveraging Large Pretrained Models for WebNLG 2020}. WEBNLG.

\bibitem{zhou2011j8m}
Tom Chao Zhou, Chin-Yew Lin, Irwin King, et al. (2011). \textit{Learning to Suggest Questions in Online Forums}. AAAI Conference on Artificial Intelligence.

\bibitem{injae2016yq6}
Shin In-Jae, Byungkwen Song, and D. Eom (2016). \textit{Auto-Mapping and Configuration Method of IEC 61850 Information Model Based on OPC UA}. Unpublished manuscript.

\bibitem{jeong2019z3k}
Eunji Jeong, Sungwoo Cho, Gyeong-In Yu, et al. (2019). \textit{Speculative Symbolic Graph Execution of Imperative Deep Learning Programs}. ACM SIGOPS Operating Systems Review.

\bibitem{rajani20171n9}
Nazneen Rajani, Mihaela A. Bornea, and Ken Barker (2017). \textit{Stacking With Auxiliary Features for Entity Linking in the Medical Domain}. Workshop on Biomedical Natural Language Processing.

\bibitem{wang202379k}
Shuhe Wang, Xiaofei Sun, Xiaoya Li, et al. (2023). \textit{GPT-NER: Named Entity Recognition via Large Language Models}. North American Chapter of the Association for Computational Linguistics.

\bibitem{alshahwan2024v64}
N. Alshahwan, Jubin Chheda, Anastasia Finogenova, et al. (2024). \textit{Automated Unit Test Improvement using Large Language Models at Meta}. SIGSOFT FSE Companion.

\bibitem{zou2024ucl}
Wei Zou, Runpeng Geng, Binghui Wang, et al. (2024). \textit{PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models}. arXiv.org.

\bibitem{fadeeva2024lt8}
Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, et al. (2024). \textit{Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{nguyen2023obn}
Thuat Nguyen, C. Nguyen, Viet Dac Lai, et al. (2023). \textit{CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages}. International Conference on Language Resources and Evaluation.

\bibitem{zou2024c26}
Wei Zou, Runpeng Geng, Binghui Wang, et al. (2024). \textit{PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models}. Unpublished manuscript.

\bibitem{li2023f7d}
Huao Li, Yu Quan Chong, Simon Stepputtis, et al. (2023). \textit{Theory of Mind for Multi-Agent Collaboration via Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{wang2023hgw}
Yiming Wang, Zhuosheng Zhang, and Rui Wang (2023). \textit{Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chen2023gii}
Yuyan Chen, Qiang Fu, Yichen Yuan, et al. (2023). \textit{Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models}. International Conference on Information and Knowledge Management.

\bibitem{gilbert2024uu2}
S. Gilbert, J. Kather, and Aidan Hogan (2024). \textit{Augmented non-hallucinating large language models as medical information curators}. npj Digit. Medicine.

\bibitem{kim2024vgn}
Sunkyu Kim, Choong-kun Lee, and Seung-seob Kim (2024). \textit{Large Language Models: A Guide for Radiologists}. Korean Journal of Radiology.

\bibitem{wang2024sae}
Jianing Wang, Junda Wu, Yupeng Hou, et al. (2024). \textit{InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{huang20233v0}
Yuheng Huang, Jiayang Song, Zhijie Wang, et al. (2023). \textit{Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models}. arXiv.org.

\bibitem{zhao2024s3a}
Linxi Zhao, Yihe Deng, Weitong Zhang, et al. (2024). \textit{Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance}. arXiv.org.

\bibitem{leiser2024kfo}
Florian Leiser, S. Eckhardt, Valentin Leuthe, et al. (2024). \textit{HILL: A Hallucination Identifier for Large Language Models}. International Conference on Human Factors in Computing Systems.

\bibitem{malmqvist2024k7x}
Lars Malmqvist (2024). \textit{Sycophancy in Large Language Models: Causes and Mitigations}. arXiv.org.

\bibitem{lin2024gru}
Sheng-Chieh Lin, Luyu Gao, Barlas Ouz, et al. (2024). \textit{FLAME: Factuality-Aware Alignment for Large Language Models}. Neural Information Processing Systems.

\bibitem{li2023dw0}
Xingxuan Li, Ruochen Zhao, Yew Ken Chia, et al. (2023). \textit{Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases}. arXiv.org.

\bibitem{ma2023mka}
Fan Ma, Xiaojie Jin, Heng Wang, et al. (2023). \textit{Vista-llama: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens}. Computer Vision and Pattern Recognition.

\bibitem{song2024t8k}
Peiyang Song, Kaiyu Yang, and Anima Anandkumar (2024). \textit{Towards Large Language Models as Copilots for Theorem Proving in Lean}. arXiv.org.

\bibitem{jiang20242kz}
Che Jiang, Biqing Qi, Xiangyu Hong, et al. (2024). \textit{On Large Language Models Hallucination with Regard to Known Facts}. North American Chapter of the Association for Computational Linguistics.

\bibitem{song2024br2}
Peiyang Song, Kaiyu Yang, and Anima Anandkumar (2024). \textit{Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean}. NeuS.

\bibitem{liu2024ker}
Haochen Liu, Song Wang, Yaochen Zhu, et al. (2024). \textit{Knowledge Graph-Enhanced Large Language Models via Path Selection}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sun2024z6b}
Yuhong Sun, Zhangyue Yin, Qipeng Guo, et al. (2024). \textit{Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem}. International Conference on Language Resources and Evaluation.

\bibitem{ling2024hqv}
Chen Ling, Xujiang Zhao, Wei Cheng, et al. (2024). \textit{Uncertainty Quantification for In-Context Learning of Large Language Models}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2024jbb}
Moxin Li, Wenjie Wang, Fuli Feng, et al. (2024). \textit{Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{smith2023i8w}
Andrew L Smith, Felix Greaves, and T. Panch (2023). \textit{Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language Models}. PLOS Digital Health.

\bibitem{shah20242sx}
Savyasachi V. Shah (2024). \textit{Accuracy, Consistency, and Hallucination of Large Language Models When Analyzing Unstructured Clinical Notes in Electronic Medical Records.}. JAMA Network Open.

\bibitem{pan2024hm4}
Zhenyu Pan, Haozheng Luo, Manling Li, et al. (2024). \textit{Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models}. International Conference on Learning Representations.

\bibitem{zhang2024o58}
Hengran Zhang, Ruqing Zhang, J. Guo, et al. (2024). \textit{Are Large Language Models Good at Utility Judgments?}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{tang2024cxa}
Jianheng Tang, Qifan Zhang, Yuhan Li, et al. (2024). \textit{GraphArena: Benchmarking Large Language Models on Graph Computational Problems}. arXiv.org.

\bibitem{zhou2024d14}
Xiongtao Zhou, Jie He, Yuhua Ke, et al. (2024). \textit{An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhao2024g9c}
Ruilin Zhao, Feng Zhao, Long Wang, et al. (2024). \textit{KG-CoT: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering}. International Joint Conference on Artificial Intelligence.

\bibitem{pan2024uot}
Zhenyu Pan, Haozheng Luo, Manling Li, et al. (2024). \textit{Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action}. arXiv.org.

\bibitem{zhu2024hll}
Derui Zhu, Dingfan Chen, Qing Li, et al. (2024). \textit{PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics}. NAACL-HLT.

\bibitem{zhang20252at}
Wan Zhang, and Jing Zhang (2025). \textit{Hallucination Mitigation for Retrieval-Augmented Large Language Models: A Review}. Mathematics.

\bibitem{chen2024kgu}
Lida Chen, Zujie Liang, Xintao Wang, et al. (2024). \textit{Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals}. Proceedings of the 3rd Workshop on Towards Knowledgeable Foundation Models (KnowFM).

\bibitem{dernbach2024w0b}
Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, et al. (2024). \textit{GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding}. AAAI Spring Symposia.

\bibitem{wu202407f}
Jiageng Wu, Xian Wu, and Jie Yang (2024). \textit{Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds}. International Joint Conference on Artificial Intelligence.

\bibitem{ahn2024r1o}
Jaewoo Ahn, Taehyun Lee, Junyoung Lim, et al. (2024). \textit{TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{mou2024fsy}
Xinyi Mou, Zejun Li, Hanjia Lyu, et al. (2024). \textit{Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs}. The Web Conference.

\bibitem{xu2024f68}
Derong Xu, Ziheng Zhang, Zhihong Zhu, et al. (2024). \textit{Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models}. International Conference on Information and Knowledge Management.

\bibitem{hu2024fnt}
Xiangkun Hu, Dongyu Ru, Lin Qiu, et al. (2024). \textit{RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models}. arXiv.org.

\bibitem{mukherjee2024o5w}
Subhojyoti Mukherjee, Anusha Lalitha, Sailik Sengupta, et al. (2024). \textit{Multi-Objective Alignment of Large Language Models Through Hypervolume Maximization}. arXiv.org.

\bibitem{jiao2024l4e}
Qirui Jiao, Daoyuan Chen, Yilun Huang, et al. (2024). \textit{Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study}. arXiv.org.

\bibitem{ding20245e3}
Hao Ding, Ziwei Fan, Ingo Ghring, et al. (2024). \textit{Reasoning and Planning with Large Language Models in Code Development}. Knowledge Discovery and Data Mining.

\bibitem{wang2024swy}
Chengpeng Wang, Wuqi Zhang, Zian Su, et al. (2024). \textit{Sanitizing Large Language Models in Bug Detection with Data-Flow}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2024md6}
Zhuo Chen, Jiawei Liu, Haotan Liu, et al. (2024). \textit{Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models}. arXiv.org.

\bibitem{wang2023ynd}
Xiaohua Wang, Yuliang Yan, Longtao Huang, et al. (2023). \textit{Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yeo2024g7d}
Wei Jie Yeo, Teddy Ferdinan, Przemysaw Kazienko, et al. (2024). \textit{Self-training Large Language Models through Knowledge Detection}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{hu2024fld}
Sihao Hu, Tiansheng Huang, and Ling Liu (2024). \textit{PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models}. arXiv.org.

\bibitem{zhang2024ia4}
Zhenhong Zhang, Jiajing Chen, Weiyan Shi, et al. (2024). \textit{Contrastive Learning for Knowledge-Based Question Generation in Large Language Models}. 2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI).

\bibitem{yang2024iia}
Dingkang Yang, Dongling Xiao, Jinjie Wei, et al. (2024). \textit{Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators}. AAAI Conference on Artificial Intelligence.

\bibitem{yu2023ine}
Xiaodong Yu, Hao Cheng, Xiaodong Liu, et al. (2023). \textit{ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks}. NAACL-HLT.

\bibitem{gu2024eig}
Zishan Gu, Changchang Yin, Fenglin Liu, et al. (2024). \textit{MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context}. arXiv.org.

\bibitem{liu2024kf2}
Lihui Liu, Zihao Wang, Ruizhong Qiu, et al. (2024). \textit{Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs}. arXiv.org.

\bibitem{woo2024dtm}
B. Woo, Tom Huynh, Arthur Tang, et al. (2024). \textit{Transforming nursing with large language models: from concept to practice.}. European Journal of Cardiovascular Nursing.

\bibitem{zhao20246wi}
Xiutian Zhao, Ke Wang, and Wei Peng (2024). \textit{Measuring the Inconsistency of Large Language Models in Preferential Ranking}. KNOWLLM.

\bibitem{qian2024mj9}
Xinying Qian, Ying Zhang, Yu Zhao, et al. (2024). \textit{TimeR^4 : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{butler20242xs}
J. Butler, James Puleo, Michael Harrington, et al. (2024). \textit{From technical to understandable: Artificial Intelligence Large Language Models improve the readability of knee radiology reports.}. Knee Surgery, Sports Traumatology, Arthroscopy.

\bibitem{sahoo202420w}
N. R. Sahoo, Ashita Saxena, Kishan Maharaj, et al. (2024). \textit{Addressing Bias and Hallucination in Large Language Models}. International Conference on Language Resources and Evaluation.

\bibitem{zuo20242i0}
Kaiwen Zuo, and Yirui Jiang (2024). \textit{MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models}. arXiv.org.

\bibitem{parente2024vlq}
D. J. Parente (2024). \textit{Generative Artificial Intelligence and Large Language Models in Primary Care Medical Education.}. Family Medicine.

\bibitem{zhao2024h5n}
Haiyan Zhao, Fan Yang, Himabindu Lakkaraju, et al. (2024). \textit{Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability}. arXiv.org.

\bibitem{zhou2024b0u}
Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, et al. (2024). \textit{Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{adewumi2024lv9}
Tosin P. Adewumi, Nudrat Habib, Lama Alkhaled, et al. (2024). \textit{On the Limitations of Large Language Models (LLMs): False Attribution}. arXiv.org.

\bibitem{toroghi2024mxf}
Armin Toroghi, Willis Guo, Mohammad Mahdi Torabi pour, et al. (2024). \textit{Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{taveekitworachai2024aql}
Pittawat Taveekitworachai, Febri Abdullah, and R. Thawonmas (2024). \textit{Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{wan2024mh1}
Fanqi Wan, Xinting Huang, Leyang Cui, et al. (2024). \textit{Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment}. arXiv.org.

\bibitem{alsadat2024i78}
Shayan Meshkat Alsadat, Jean-Raphael Gaglione, D. Neider, et al. (2024). \textit{Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine}. American Control Conference.

\bibitem{cao2024o9a}
Qingxing Cao, Junhao Cheng, Xiaodan Liang, et al. (2024). \textit{VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{benkirane202494i}
Kenza Benkirane, Laura Gongas, Shahar Pelles, et al. (2024). \textit{Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{jin2024jpw}
Lifeng Jin, Baolin Peng, Linfeng Song, et al. (2024). \textit{Collaborative decoding of critical tokens for boosting factuality of large language models}. arXiv.org.

\bibitem{mu2024f3b}
Yida Mu, Peizhen Bai, Kalina Bontcheva, et al. (2024). \textit{Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling}. arXiv.org.

\bibitem{yu2024pp9}
Jun Yu, Yunxiang Zhang, Zerui Zhang, et al. (2024). \textit{RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector}. ACM Multimedia.

\bibitem{amirizaniani2024cad}
Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, et al. (2024). \textit{LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop}. Unpublished manuscript.

\bibitem{ling2024qto}
Chen Ling, Xujiang Zhao, Wei Cheng, et al. (2024). \textit{Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models}. arXiv.org.

\bibitem{sarmah2023cuq}
Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, et al. (2023). \textit{Towards reducing hallucination in extracting information from financial reports using Large Language Models}. International Conference on AI-ML-Systems.

\bibitem{nathani202338c}
Deepak Nathani, David Wang, Liangming Pan, et al. (2023). \textit{MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chataigner2024cr0}
Cl'ea Chataigner, Afaf Tak, and G. Farnadi (2024). \textit{Multilingual Hallucination Gaps in Large Language Models}. arXiv.org.

\bibitem{liu2025xwv}
Q. Liu, Xinlong Chen, Yue Ding, et al. (2025). \textit{Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models}. arXiv.org.

\bibitem{song2024v5n}
Jongyoon Song, Sangwon Yu, and Sungroh Yoon (2024). \textit{Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination}. arXiv.org.

\bibitem{barkley202472d}
Liam Barkley, and Brink van der Merwe (2024). \textit{Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models}. arXiv.org.

\bibitem{huang2024c9t}
Chao-Wei Huang, and Yun-Nung Chen (2024). \textit{FactAlign: Long-form Factuality Alignment of Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{malin2024fin}
B. Malin, Tatiana Kalganova, and Nikoloas Boulgouris (2024). \textit{A review of faithfulness metrics for hallucination assessment in Large Language Models}. IEEE Journal on Selected Topics in Signal Processing.

\bibitem{yuan2024o7d}
Hongbang Yuan, Pengfei Cao, Zhuoran Jin, et al. (2024). \textit{Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{pandit20257jx}
Shrey Pandit, Jiawei Xu, Junyuan Hong, et al. (2025). \textit{MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models}. arXiv.org.

\bibitem{bellinileite2023y38}
Samuel C. Bellini-Leite (2023). \textit{Dual Process Theory for Large Language Models: An overview of using Psychology to address hallucination and reliability issues}. Adaptive Behavior.

\bibitem{omar2025us3}
M. Omar, V. Sorin, J. Collins, et al. (2025). \textit{Large Language Models Are Highly Vulnerable to Adversarial Hallucination Attacks in Clinical Decision Support: A Multi-Model Assurance Analysis}. medRxiv.

\bibitem{lee2024i72}
Yi-Lun Lee, Yi-Hsuan Tsai, and Wei-Chen Chiu (2024). \textit{Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models}. arXiv.org.

\bibitem{li2023irg}
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, et al. (2023). \textit{HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}. Unpublished manuscript.

\bibitem{zhang2023pb6}
Chen Zhang (2023). \textit{User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination}. arXiv.org.

\bibitem{irulandi2023xlg}
Muneeswaran Irulandi, Shreya Saxena, Siva Prasad, et al. (2023). \textit{Minimizing Factual Inconsistency and Hallucination in Large Language Models}. arXiv.org.

\bibitem{li2024ncc}
Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, et al. (2024). \textit{Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation}. arXiv.org.

\bibitem{guo2024tlu}
Hongyi Guo, Zhihan Liu, Yufeng Zhang, et al. (2024). \textit{Can Large Language Models Play Games? A Case Study of A Self-Play Approach}. arXiv.org.

\bibitem{xie20247zk}
Zikai Xie (2024). \textit{Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models}. arXiv.org.

\bibitem{amirizaniani2024493}
Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, et al. (2024). \textit{Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop}. arXiv.org.

\bibitem{yang2025n54}
Tianyun Yang, Ziniu Li, Juan Cao, et al. (2025). \textit{Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention}. International Conference on Learning Representations.

\bibitem{agarwal202418c}
Vibhor Agarwal, Yulong Pei, Salwa Alamir, et al. (2024). \textit{CodeMirage: Hallucinations in Code Generated by Large Language Models}. arXiv.org.

\bibitem{rrv2024gw0}
Aswin Rrv, Nemika Tyagi, Md Nayem Uddin, et al. (2024). \textit{Chaos with Keywords: Exposing Large Language Models Sycophantic Hallucination to Misleading Keywords and Evaluating Defense Strategies}. Unpublished manuscript.

\bibitem{li2024hl9}
Mingchen Li, Zaifu Zhan, Han Yang, et al. (2024). \textit{Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness}. arXiv.org.

\bibitem{wang2024t4o}
Shirui Wang, Bohan Xie, Ling Ding, et al. (2024). \textit{SeCor: Aligning Semantic and Collaborative Representations by Large Language Models for Next-Point-of-Interest Recommendations}. ACM Conference on Recommender Systems.

\bibitem{hegselmann20249q4}
S. Hegselmann, Zejiang Shen, Florian Gierse, et al. (2024). \textit{A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models}. ACM Conference on Health, Inference, and Learning.

\bibitem{gao2024ncr}
Jun Gao, Huan Zhao, Wei Wang, et al. (2024). \textit{EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models}. arXiv.org.

\bibitem{tsai2024klg}
Yao-Hung Tsai, Walter Talbott, and Jian Zhang (2024). \textit{Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning}. arXiv.org.

\bibitem{chen2024qs5}
Xinxi Chen, Li Wang, Wei Wu, et al. (2024). \textit{Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know", and Reducing Hallucination in RAG}. arXiv.org.

\bibitem{liu2025juo}
MingShan Liu, Shi Bo, and Jialing Fang (2025). \textit{Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection}. arXiv.org.

\bibitem{zhang2024htn}
Taolin Zhang, Qizhou Chen, Dongyang Li, et al. (2024). \textit{DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{guo2024hgn}
Yuhang Guo, and Zhiyu Wan (2024). \textit{Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks}. IEEE International Conference on Healthcare Informatics.

\bibitem{luo2024uh8}
Weiqing Luo, Chonggang Song, Lingling Yi, et al. (2024). \textit{KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation}. arXiv.org.

\bibitem{hamid2024pwn}
Oussama H. Hamid (2024). \textit{Beyond Probabilities: Unveiling the Delicate Dance of Large Language Models (LLMs) and AI-Hallucination}. Conference on Cognitive and Computational Aspects of Situation Management.

\bibitem{zheng20240qd}
Xinxin Zheng, Feihu Che, Jinyang Wu, et al. (2024). \textit{KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering}. arXiv.org.

\bibitem{rawte2024bu6}
Vipula Rawte, Aman Chadha, Amit P. Sheth, et al. (2024). \textit{Tutorial Proposal: Hallucination in Large Language Models}. International Conference on Language Resources and Evaluation.

\bibitem{yin2024iau}
Zhibo Yin (2024). \textit{A review of methods for alleviating hallucination issues in large language models}. Applied and Computational Engineering.

\bibitem{tang2025mfi}
Zilu Tang, Rajen Chatterjee, and Sarthak Garg (2025). \textit{Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization}. North American Chapter of the Association for Computational Linguistics.

\bibitem{das2024jdt}
Souvik Das, Lifeng Jin, Linfeng Song, et al. (2024). \textit{Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models}. International Conference on Computational Linguistics.

\bibitem{zhang2024h4a}
Yuxiang Zhang, Jing Chen, Junjie Wang, et al. (2024). \textit{ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{xu2024t34}
Derong Xu, Ziheng Zhang, Zhihong Zhu, et al. (2024). \textit{Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2025p1z}
Hongjie Zhang, Hourui Deng, Jie Ou, et al. (2025). \textit{Mitigating spatial hallucination in large language models for path planning via prompt engineering}. Scientific Reports.

\bibitem{ahmadi2024j88}
Ali Ahmadi (2024). \textit{Unravelling the Mysteries of Hallucination in Large Language Models: Strategies for Precision in Artificial Intelligence Language Generation}. Asian Journal of Computer Science and Technology.

\bibitem{abdelghafour2024efh}
M. Abdelghafour, Mohammed Mabrouk, and Zaki Taha (2024). \textit{Hallucination Mitigation Techniques in Large Language Models}. International Journal of Intelligent Computing and Information Sciences.

\bibitem{zhou20253zv}
Xiaoling Zhou, Mingjie Zhang, Zhemg Lee, et al. (2025). \textit{HaDeMiF: Hallucination Detection and Mitigation in Large Language Models}. International Conference on Learning Representations.

\bibitem{karbasi2025j7n}
Amin Karbasi, Omar Montasser, John Sous, et al. (2025). \textit{(Im)possibility of Automated Hallucination Detection in Large Language Models}. arXiv.org.

\bibitem{mubarak2024lx6}
Hamdy Mubarak, Hend Suliman Al-Khalifa, and Khaloud Suliman Alkhalefah (2024). \textit{Halwasa: Quantify and Analyze Hallucinations in Large Language Models: Arabic as a Case Study}. International Conference on Language Resources and Evaluation.

\bibitem{zhang2024sbu}
Wenbo Zhang, Zihang Xu, and Hengrui Cai (2024). \textit{Recognizing Limits: Investigating Infeasibility in Large Language Models}. Unpublished manuscript.

\bibitem{omar2025cc3}
Mahmud Omar, Vera Sorin, Jeremy D. Collins, et al. (2025). \textit{Multi-model assurance analysis showing large language models are highly vulnerable to adversarial hallucination attacks during clinical decision support}. Communications Medicine.

\bibitem{gao20242nu}
Zhengjie Gao, Xuanzi Liu, Yuanshuai Lan, et al. (2024). \textit{A Brief Survey on Safety of Large Language Models}. Journal of computer & information technology.

\bibitem{pester20242zt}
Andreas Pester, Ahmed Tammaa, Christian Gtl, et al. (2024). \textit{Conversational Agents, Virtual Worlds, and Beyond: A Review of Large Language Models Enabling Immersive Learning}. IEEE Global Engineering Education Conference.

\bibitem{wu202415r}
Kangxi Wu, Liang Pang, Huawei Shen, et al. (2024). \textit{Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{tu2024v40}
Yahan Tu, Rui Hu, and Jitao Sang (2024). \textit{ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models}. Computer Vision and Pattern Recognition.

\end{thebibliography}

\end{document}