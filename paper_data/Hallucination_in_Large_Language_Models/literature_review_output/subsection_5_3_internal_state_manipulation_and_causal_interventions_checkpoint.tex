\subsection*{Internal State Manipulation and Causal Interventions}

Advanced mitigation techniques for Large Language Models (LLMs) are increasingly moving beyond superficial prompt engineering or costly retraining, focusing instead on directly manipulating the model's internal states, attention mechanisms, or learned parameters. These fine-grained interventions offer mechanistic insights into model failures, enabling more precise and targeted corrections that enhance factual accuracy and reduce spurious correlations. While many prominent examples of these techniques arise in Multimodal Large Language Models (MLLMs) due to the explicit need to balance information across modalities, the underlying principles of internal state manipulation are broadly applicable to unimodal LLMs as well.

One significant thrust in this area involves directly enhancing or modifying specific modality features within the model's processing pipeline, particularly in MLLMs to combat modality bias or "visual amnesia" \cite{lan20240yz}. For instance, \cite{yin2025s2b} introduced Visual Amplification Fusion (VAF), a training-free method that enhances attention to visual signals within the middle layers of MLLMs, where modality fusion predominantly occurs. VAF aims to counteract language bias by boosting visual feature processing without the inference speed penalties often associated with contrastive decoding methods. Building on this, \cite{zou2024dp7} proposed Memory-space Visual Retracing (MemVR), which dynamically re-injects visual tokens as supplementary evidence into the MLLM's Feed Forward Network (FFN) at a "middle trigger layer" when the model exhibits high uncertainty. This adaptive re-injection refreshes visual memory, effectively combating "visual amnesia" without significant inference overhead. Further refining targeted interventions, \cite{chen2024j0g} developed Image-Object Cross-Level Trusted Intervention (ICT), a plug-and-play method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information. ICT operates during the forward pass by identifying and intervening on specific attention heads responsible for encoding overall image information and fine-grained object details. While these methods offer efficient, training-free solutions, their interventions are often heuristic, relying on empirical observations of where and how visual information degrades. The challenge lies in ensuring these boosts do not inadvertently over-emphasize visual cues, potentially suppressing valid linguistic context or introducing new biases.

Moving beyond direct feature boosting, another line of research employs causal inference and targeted manipulation of attention mechanisms to understand and balance modality priors or correct specific hallucination types. \cite{zhou2024lvp} presented CAUSAL MM, a principled causal inference framework that applies structural causal modeling (SCM), back-door adjustment, and counterfactual reasoning at both visual and language attention levels. By treating modality priors as confounding factors, CAUSAL MM deciphers their causal impact on MLLM output, enabling a more balanced integration of visual and linguistic information to mitigate hallucinations. This represents a more theory-driven approach compared to purely heuristic interventions. Similarly, in unimodal LLMs, \cite{yuan2024o7d} analyzed "false premise hallucinations" and identified a small subset of attention heads ("false premise heads") that disturb knowledge extraction. Their method, FAITH (False premise Attention head constraIining for miTigating Hallucinations), constrains these specific attention heads during inference, demonstrating a significant performance increase by manipulating only about 1\% of the heads. While CAUSAL MM offers a comprehensive causal framework, its application to complex neural networks can be computationally intensive and relies on strong assumptions about the underlying causal graph. FAITH, on the other hand, provides a more targeted, empirical intervention, but its generalizability across different models and hallucination types requires further investigation. Both, however, highlight the potential of fine-grained attention manipulation informed by mechanistic understanding.

Beyond real-time inference interventions, internal state manipulation can also be applied to directly edit or update the factual knowledge embedded within a model's parameters or representations. This approach, often termed "model editing," aims to precisely alter specific behaviors without affecting unrelated knowledge. For instance, \cite{xu2024f68} proposed MedLaSA (Layer-wise Scalable Adapter strategy) for medical model editing. MedLaSA leverages causal tracing to identify the association of knowledge in neurons across different layers, generating a corresponding scale set. It then incorporates scalable adapters into the dense layers of LLMs, adjusting adapter weights and ranks based on the specific knowledge. This allows for precise editing of semantically identical knowledge while minimizing impact on unrelated information, crucial for high-stakes domains like medicine. In a similar vein, \cite{zhang2024htn} introduced DAFNet (Dynamic Auxiliary Fusion Network) for sequential model editing, which addresses the challenge of continuously rectifying mistakes and preventing catastrophic forgetting. DAFNet enhances semantic interaction among factual knowledge by aggregating intra-editing attention flow into auto-regressive self-attention and leveraging multi-layer diagonal inter-editing attention flow to update sequence-level representations. These methods demonstrate how direct manipulation of internal parameters and attention flow can be used to refine or update factual knowledge, offering a powerful alternative to full retraining. However, challenges remain in ensuring the scalability of these editing operations, preventing unintended side effects on other knowledge, and maintaining consistency over sequential edits.

Mechanistic insights into model failures can also be gleaned from adversarial attacks that specifically manipulate internal states to induce hallucinations. \cite{wang2025jen} demonstrated a novel hallucination attack that exploits the "attention sink" phenomenon in MLLMs. By manipulating attention scores and hidden embeddings to induce these sinks, the attack triggers hallucinated content with minimal image-text relevance. Understanding how such internal vulnerabilities can be exploited to *induce* hallucinations provides critical insights for developing more resilient internal state manipulation techniques, effectively turning offensive research into defensive strategies.

In conclusion, these advanced techniques represent a significant shift towards achieving fine-grained control and mechanistic understanding of LLM and MLLM behavior. By directly intervening in hidden states, attention mechanisms, or leveraging causal relationships and parameter modifications, researchers are developing powerful, often training-free, solutions to mitigate hallucinations. This allows for more precise and targeted corrections, moving beyond black-box approaches to address the root causes of factual inaccuracies. However, challenges remain in fully understanding the complex causal relationships within these models, ensuring generalizability across diverse tasks and architectures, and balancing the trade-offs between intervention strength and preserving beneficial model capabilities. Future work will likely focus on more adaptive and intelligent internal interventions, potentially informed by a deeper causal understanding of multimodal and unimodal reasoning, and ensuring that these interventions are robust and scalable.