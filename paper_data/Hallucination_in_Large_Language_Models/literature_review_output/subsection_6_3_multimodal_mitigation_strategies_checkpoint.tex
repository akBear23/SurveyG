\subsection{Multimodal Mitigation Strategies}
Hallucinations in Large Vision-Language Models (LVLMs) and other multimodal architectures present a formidable challenge, manifesting as generated content factually inconsistent with visual inputs, misinterpretations of audio cues, or flawed cross-modal reasoning \cite{sahoo2024hcb, lan20240yz}. These errors, stemming from issues like the "modality gap," dataset biases, and inherited language model tendencies, severely undermine the dependability and credibility of multimodal AI systems \cite{lan20240yz}. Given the theoretical inevitability of hallucination in computable models \cite{xu2024n76}, mitigation strategies are crucial for managing, rather than eliminating, these inconsistencies. This subsection explores a range of techniques, from robust training paradigms to sophisticated inference-time interventions, all designed to enhance factual consistency by accounting for the complex interplay between modalities.

Training-based strategies aim to instill hallucination resistance directly into the model's learning process. Pioneering efforts in robust instruction tuning for LVLMs include \cite{liu2023882}'s LRV-Instruction, a dataset incorporating diverse *negative instructions* (e.g., describing nonexistent objects) to explicitly train models to avoid visual hallucinations. This data-centric approach, coupled with the GAVIE evaluation framework, demonstrated the effectiveness of explicitly teaching LMMs what *not* to generate. Building on the success of preference optimization, \cite{fu2024yqj} introduced Hallucination-targeted Direct Preference Optimization (HDPO) for MLLMs. HDPO constructs specific preference data to address three distinct causes of multimodal hallucinations: visual distracted hallucination (VDH), long-context hallucination (LCH), and multimodal conflict hallucination (MCH). Unlike general DPO methods, HDPO's targeted data generation, which includes amplifying irrelevant visual information or introducing conflicting text, allows for more precise and consistent improvements across diverse hallucination types. While effective, these instruction tuning and DPO methods demand substantial effort in curating high-quality, hallucination-specific preference data, which can be computationally expensive and may risk stifling generative creativity if not carefully balanced.

Beyond data augmentation, other training-based methods focus on internal model dynamics. \cite{wu2024n00} introduced NoiseBoost, a generalizable technique that injects noise into visual tokens during supervised fine-tuning (SFT), reinforcement learning (RL), or semi-supervised learning (SSL). This perturbation encourages MLLMs to distribute attention more evenly between visual and linguistic tokens, reducing over-reliance on language priors without incurring significant inference costs. This approach is particularly appealing for its efficiency, as it avoids the need for extensive negative data curation. Similarly, \cite{deng202405j} presented an efficient "judge-free" self-improvement framework for MLLMs. This method leverages controllable negative samples and lightweight CLIP-based verification for Direct Preference Optimization (DPO), bypassing the computational expense and potential biases associated with using large models as judges. The challenge with such internal perturbation methods lies in finding the optimal noise level or intervention point to improve robustness without degrading overall performance or introducing new biases.

Inference-time interventions offer "plug-and-play" solutions that do not require expensive retraining, making them highly adaptable. One category focuses on external grounding and post-remedy correction. \cite{yin2023hx3}'s Woodpecker is a training-free pipeline that leverages external expert models (e.g., object detectors, VQA models) to validate visual facts and correct MLLM responses. This approach, akin to active retrieval augmentation for multimodal contexts, enhances interpretability by explicitly adding visual grounding (e.g., bounding boxes) to corrected text. However, its effectiveness is inherently limited by the accuracy and coverage of the external expert models, potentially leading to cascading errors if the "experts" themselves are flawed, and can introduce additional inference latency.

Another significant area of inference-time intervention involves directly manipulating the model's internal states or attention mechanisms to enhance visual grounding. \cite{zou2024dp7}'s MemVR (Memory-space Visual Retracing) addresses "visual amnesia" by re-injecting visual tokens as supplementary evidence into intermediate layers of the MLLM, dynamically activated by high uncertainty. This "look-twice" mechanism efficiently refreshes visual memory, overcoming the forgetting of visual information that can lead to hallucinations. Similarly, \cite{yin2025s2b}'s ClearSight proposes Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals within MLLM middle layers during the forward pass, mitigating object hallucination without compromising content quality or inference speed. \cite{chen2024j0g} proposed ICT (Image-Object Cross-Level Trusted Intervention), a training-free method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information (overall image and fine-grained objects) by targeting specific attention heads. These methods offer fine-grained control but require deep architectural understanding and careful tuning to avoid indiscriminately suppressing beneficial language priors.

Contrastive decoding methods, which leverage subtle differences in input or model states to steer generation, have also been adapted for multimodal contexts. \cite{leng2023ohr} introduced Visual Contrastive Decoding (VCD), a training-free approach that mitigates object hallucinations by contrasting output distributions from original and *distorted* visual inputs. By applying a Gaussian noise mask to create visual uncertainty, VCD penalizes tokens that are highly probable under the distorted input (which amplifies reliance on language priors), thereby calibrating the model's output. Building upon this, \cite{chen20247jb} proposed Hallucination-Induced Optimization (HIO), a more precise contrastive decoding strategy for LVLMs. HIO addresses the "uncontrollable nature of global visual uncertainty" in prior methods by training an "Evil LVLM" using a *reversed* Bradley-Terry model to specifically *amplify* targeted hallucinations. This "Evil LVLM" then provides a stronger, more focused contrastive signal during decoding, leading to superior hallucination reduction. Another novel approach, \cite{park20247cm}'s ConVis, employs a Text-to-Image (T2I) model to visualize potential hallucinations from an MLLM's initial caption. These visualized discrepancies then serve as a contrastive signal during decoding to penalize hallucinated tokens, offering a unique way to generate visual contrastive signals. While powerful, contrastive decoding methods can introduce additional computational overhead during inference and require careful balancing to prevent over-penalization that might stifle creativity or lead to overly cautious, less fluent outputs.

Beyond direct visual grounding, some methods leverage advanced reasoning or generative capabilities. \cite{kim2024ozf}'s Counterfactual Inception prompts LMMs to self-generate "counterfactual keywords" (e.g., non-existent objects) and then explicitly instructs the model to avoid them during response generation. This training-free method, guided by a Plausibility Verification Process (PVP) using CLIP scores, implants counterfactual thinking to reduce hallucinations by making the model explicitly aware of what *not* to say. Furthermore, \cite{zhou2024lvp} offered CAUSAL MM, a causal inference framework that uses back-door adjustment and counterfactual reasoning to mitigate modality prior-induced hallucinations by deciphering attention causality. This provides a more principled and theoretically grounded way to balance visual and language influences, moving beyond heuristic adjustments.

Unified frameworks and multi-view reasoning approaches aim for more adaptive and comprehensive mitigation. \cite{chang2024u3t} introduced "Dentist," a unified hallucination mitigation framework that first classifies the query type (e.g., perception vs. reasoning) and then applies a tailored mitigation strategy within an iterative validation loop. For perception queries, it uses visual verification with sub-questions, while for reasoning queries, it employs Chain-of-Thought (CoT) prompting. This adaptive approach, which refines answers until semantic convergence, directly addresses the need for mitigation strategies that adapt based on query type. Complementing this, \cite{qu2024pqc}'s MVP (Multi-View Multi-Path Reasoning) is a training-free framework that enhances LVLM inference by thoroughly perceiving image information from "multi-view" dimensions (top-down, regular, bottom-up) and then employing "multi-path certainty-driven reasoning" during decoding. This approach explores multiple reasoning paths and aggregates certainty scores to select the most reliable answer, maximizing the innate capabilities of existing LVLMs. While promising for their adaptability, the complexity of these unified frameworks and the accuracy of query classification or certainty aggregation remain critical factors influencing their real-world performance.

In conclusion, the landscape of multimodal hallucination mitigation is characterized by a diverse array of strategies, each with distinct trade-offs. Training-based methods like LRV-Instruction \cite{liu2023882} and HDPO \cite{fu2024yqj} aim for intrinsic robustness but demand significant data curation and computational resources. Inference-time interventions, including external grounding (Woodpecker \cite{yin2023hx3}), internal state manipulation (MemVR \cite{zou2024dp7}, ClearSight \cite{yin2025s2b}), and contrastive decoding (VCD \cite{leng2023ohr}, HIO \cite{chen20247jb}, ConVis \cite{park20247cm}), offer flexible, often training-free, alternatives, but their effectiveness can depend on external tool quality, the intrusiveness of internal manipulations, or the computational overhead of generating contrastive signals. Unified frameworks like Dentist \cite{chang2024u3t} and multi-view reasoning approaches like MVP \cite{qu2024pqc} represent a move towards more adaptive and comprehensive solutions. Despite these advancements, the inherent complexity of multimodal understanding, coupled with the theoretical limits of hallucination, suggests that future research must continue to balance factual consistency with generative fluency, explore more sophisticated causal interventions, and develop methods that are robust to diverse and evolving hallucination types across various multimodal contexts, including audio and video, to ensure truly dependable and credible AI systems.