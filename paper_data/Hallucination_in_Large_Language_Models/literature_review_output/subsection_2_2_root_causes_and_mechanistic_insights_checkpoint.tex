\subsection*{Root Causes and Mechanistic Insights}

Understanding the underlying factors contributing to hallucination in Large Language Models (LLMs) is paramount for developing effective and targeted mitigation strategies, moving beyond superficial observations to delve into their mechanistic origins. Hallucinations are not merely random errors but stem from a complex interplay of issues spanning the entire LLM lifecycle, from data acquisition to inference.

A primary root cause lies in the **inherent biases and limitations of vast training data**. LLMs are trained on web-scale corpora that inevitably contain noisy, outdated, or even fabricated information \cite{zhang2023k1j, rejeleene2024okw}. This noisy data can directly lead to unfaithful generations, as demonstrated by \cite{adams202289x}, who addressed this by proposing reference revision to improve the quality of noisy training data for summarization. Empirical studies reveal that low-frequency knowledge in pre-training data correlates with a higher incidence of hallucinations, while specialized domain data can significantly alleviate domain-specific errors \cite{li2024qrj}. Furthermore, the very process of tokenization and the lack of data diversity can introduce information loss and biases, undermining the factual integrity of generated content \cite{rejeleene2024okw}. This can lead to phenomena like "knowledge overshadowing," where dominant or over-represented facts in the training data cause LLMs to over-generalize, potentially resulting in "amalgamated hallucinations" where disparate but frequently co-occurring facts are incorrectly combined.

Beyond data, **limitations in model architecture and inference-time errors** significantly contribute to hallucination. During the optimization process, LLMs can exhibit "stochastic parroting" due to maximum likelihood estimation, or suffer from "exposure bias" where errors compound over sequential token generation \cite{ye2023yom}. \cite{li2024qrj} empirically showed that certain decoding strategies (e.g., diversity-oriented decoding in professional domains) and even model quantization can elicit higher hallucination rates. A critical architectural challenge arises in processing long contexts; Retrieval-Augmented Language Models (RALMs) often "get lost in long contexts," where irrelevant information distracts the model and exacerbates hallucination \cite{lv2024k5x}. This suggests deficiencies in attention mechanisms or contextual filtering. Moreover, LLMs struggle with fundamental capabilities in Retrieval-Augmented Generation (RAG) scenarios, such as negative rejection (failing to abstain from answering when no relevant information is available) and counterfactual robustness (prioritizing incorrect retrieved information over their own correct internal knowledge, even when warned) \cite{chen2023h04}. This highlights a mechanistic flaw in how LLMs weigh and integrate information. The tendency for LLMs to repeat their own hallucinations when conditioned on prior incorrect generations, a form of error propagation, is a significant inference-time issue addressed by methods like Chain-of-Verification \cite{dhuliawala2023rqn}. Mechanistically, hallucinations can also be linked to the model's internal uncertainty; \cite{su2024gnz} found that entity-level hallucinations correlate with low predictive probability and high entropy, providing a real-time signal for detection. The very nature of maximum-likelihood training can lead LLMs to assign probabilities to non-factual information, making them prone to fabrication \cite{zhang202396g}.

**Deficiencies in knowledge representation and reasoning** also play a crucial role. \cite{du2023qu7} attributed hallucinations to specific model capability deficiencies, including commonsense memorization, relational reasoning, and instruction following, drawing parallels to cognitive psychology. LLMs often struggle to effectively integrate and reason over structured knowledge, treating Knowledge Graphs (KGs) as plain text rather than leveraging their inherent graphical structure for robust reasoning \cite{wen2023t6v, sui20242u1}. This leads to difficulties in multi-hop reasoning and maintaining factual consistency. The problem of error propagation, where mistakes in early reasoning steps cascade into subsequent generations, further compounds these issues \cite{li2023v3v}. In dialogue systems, hallucinations frequently manifest as the injection of erroneous entities, indicating a failure in precise knowledge grounding \cite{dziri2021bw9}. Furthermore, dialogue-level hallucinations extend beyond mere factual errors to include incoherence (conflicting with input, context, or self), irrelevance, overreliance on specific information, and general reasoning errors, pointing to complex mechanistic failures in maintaining conversational state and consistency \cite{chen2024c4k}. Attributing these diverse hallucinations to specific internal model behaviors or layers remains a significant challenge, often requiring sophisticated probing techniques.

Ultimately, a groundbreaking theoretical perspective suggests that hallucination is not merely an engineering bug but an **innate limitation of LLMs**. \cite{xu2024n76} provided a formal proof demonstrating the inevitability of hallucination for all computable LLMs, regardless of their architecture or training data. This fundamental insight shifts the paradigm from attempting to eliminate hallucination entirely to focusing on robust detection, quantification, and management strategies.

In conclusion, the root causes of hallucination are multifaceted, encompassing biases and limitations in training data, architectural constraints that hinder robust knowledge processing, and inference-time errors that propagate inaccuracies. These mechanistic insights, from the granular level of token generation uncertainty to the theoretical inevitability of errors, are indispensable for developing targeted, efficient, and truly impactful mitigation strategies that address the underlying mechanisms rather than just the symptoms. Future research must continue to unravel these complex interactions to build more trustworthy and reliable LLM systems.