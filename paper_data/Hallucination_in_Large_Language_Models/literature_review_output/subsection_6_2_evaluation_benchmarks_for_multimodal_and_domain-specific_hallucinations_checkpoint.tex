\subsection{Evaluation Benchmarks for Multimodal and Domain-Specific Hallucinations}

The increasing sophistication and deployment of Multimodal Large Language Models (MLLMs) necessitate the development of specialized benchmarks and metrics to rigorously assess and quantify the diverse forms of hallucinations they exhibit. Moving beyond generic evaluations, the field has progressed towards fine-grained, context-aware, and domain-specific frameworks crucial for systematically identifying, categorizing, and quantifying these errors.

Initial efforts to evaluate hallucinations predominantly focused on object-level inconsistencies in static images. For instance, while not a new benchmark in itself, the POPE (Polling-based Object Probing Evaluation) framework became a common reference for assessing object existence. However, such methods often suffered from limitations like response bias or an inability to capture the nuances of free-form generation. To address this, \cite{kaul2024ta7} introduced \textbf{THRONE}, an object-based benchmark specifically designed for "Type I" hallucinations in the free-form generations of Large Vision-Language Models (LVLMs). THRONE leverages the semantic understanding of open-source LMs for Abstractive Question Answering (AQA) to accurately judge object existence within complex, unconstrained text, significantly reducing judgment errors compared to prior methods. Extending the scope to multiple objects, \cite{chen2024vy7} proposed \textbf{ROPE} (Recognition-based Object Probing Evaluation), an automated protocol for assessing multi-object hallucination. ROPE uniquely employs visual referring prompts (e.g., bounding boxes) to eliminate referential ambiguity and systematically investigates hallucination across various object class distributions within an image, revealing how LVLMs exploit shortcuts and spurious correlations.

As MLLMs expanded to other modalities, so did the need for tailored benchmarks. For audio-language models, \cite{kuan20249pm} investigated object hallucination, introducing discriminative and generative evaluation tasks along with novel metrics like \textbf{ECHO} (Evaluation of Caption Hallucination in audiO) and Cover to quantify hallucination and coverage in audio captioning. This work highlighted a critical weakness in LALMs regarding their understanding of discriminative queries.

Beyond simple object presence, researchers recognized the importance of evaluating more complex relationships between objects. \cite{wu2024bxt} introduced \textbf{R-Bench}, a novel benchmark specifically designed to evaluate and analyze relationship hallucinations in LVLMs. A key innovation of R-Bench is its meticulous construction using the `nocaps` validation set, preventing data leakage prevalent in earlier benchmarks that relied on pre-training datasets. Building on this, \cite{zheng20246fk} presented \textbf{Reefknot}, a comprehensive benchmark that systematically defines and categorizes relation hallucinations into *perceptive* (concrete) and *cognitive* (abstract) types. Reefknot utilizes real-world semantic triplets and offers diverse evaluation tasks (Yes/No, Multiple Choice, VQA), providing deeper insights into MLLMs' relational understanding, including the counter-intuitive finding that perceptive hallucinations are often more prevalent.

The dynamic nature of video content introduced new challenges, leading to benchmarks for temporal and event-based hallucinations. \cite{wang2024rta} developed \textbf{VideoHallucer}, the first comprehensive benchmark for hallucination detection in Large Video-Language Models (LVLMs). VideoHallucer categorizes hallucinations into *intrinsic* (contradicting video content) and *extrinsic* (not verifiable from video) and employs an adversarial binary VideoQA method for robust evaluation. Further specializing in temporal errors, \cite{li2024wyb} introduced \textbf{VidHalluc}, the largest benchmark for evaluating temporal hallucinations, including Action Hallucination (ACH), Temporal Sequence Hallucination (TSH), and Scene Transition Hallucination (STH). VidHalluc uses a semi-automated pipeline to identify adversarial video pairs that are visually different but semantically similar, exposing MLLMs' over-reliance on contextual scenes.

The complexity of multimodal interactions also spurred the creation of benchmarks for cross-modal driven hallucinations. \cite{sungbin2024r2g} presented \textbf{AVHBench}, the first cross-modal hallucination benchmark for Audio-Visual Large Language Models (AV-LLMs). AVHBench uniquely targets hallucinations arising from the *interaction* between audio and visual modalities, such as perceiving imaginary sounds from visual cues or fake visual events from audio cues, using a semi-automatic annotation pipeline and synthetic videos. \cite{guan2023z15} introduced \textbf{Hallusionbench}, an advanced diagnostic suite for entangled language hallucination and visual illusion in LVLMs. This benchmark features a novel VQA structure with control groups and human-edited images to quantitatively analyze models' response tendencies and specific failure modes, revealing a pronounced language bias in even state-of-the-art models. Pushing the boundaries further, \cite{zhang2025pex} proposed \textbf{CCHall}, a novel benchmark for detecting joint cross-lingual and cross-modal hallucinations, a scenario highly relevant for global applications but largely unaddressed, demonstrating significantly worse performance in MLLMs when both complexities are combined.

To provide a more unified and fine-grained evaluation, \cite{jiang2024792} developed \textbf{Hal-Eval}, a universal and fine-grained hallucination evaluation framework. Hal-Eval introduces a novel category of "Event Hallucination," which involves inventing fictional entities and weaving entire narratives around them, and integrates both discriminative and generative evaluation methods through an automatic annotation pipeline. Similarly, \cite{chen2024lc5} presented \textbf{UNIHD} (Unified Multimodal Hallucination Detection) and its accompanying benchmark \textbf{MHaluBench}. UNIHD is a task-agnostic, tool-enhanced framework that unifies hallucination detection across image-to-text and text-to-image generation, covering modality-conflicting (object, attribute, scene-text) and fact-conflicting hallucinations with fine-grained, claim-level annotations.

Beyond general multimodal contexts, the critical implications of hallucinations in specific domains have led to specialized benchmarks. In the medical domain, \cite{chen2024hfe} introduced \textbf{Med-HallMark}, the first benchmark dedicated to detecting and evaluating medical hallucinations in LVLMs. Med-HallMark features multi-task support (Med-VQA, Imaging Report Generation), a novel hierarchical hallucination categorization based on clinical severity (e.g., Catastrophic, Critical), and a new evaluation metric, MediHall Score, to provide a nuanced assessment of clinical impact.

Finally, a crucial meta-perspective on benchmark quality was provided by \cite{yan2024ux8}, who introduced the \textbf{Hallucination benchmark Quality Measurement (HQM)} framework. HQM systematically assesses benchmark quality based on reliability (test-retest, parallel-forms) and validity (criterion validity, coverage of hallucination types), revealing significant issues like response bias and misalignment with human judgment in many existing benchmarks. They also proposed \textbf{HQH} (High-Quality Hallucination Benchmark), which demonstrates superior reliability and validity through a simplified binary hallucination detection metric and comprehensive coverage of eight hallucination types. This work underscores the importance of robust benchmark design for trustworthy progress. Furthermore, \cite{zhong2024mfi} investigated "Multimodal Hallucination Snowballing," proposing the \textbf{MMHalSnowball} framework to evaluate how previously generated hallucinations can mislead an LVLM's subsequent responses in conversational settings, highlighting the dynamic and cumulative nature of these errors.

In conclusion, the evolution of hallucination evaluation benchmarks for MLLMs reflects a growing understanding of the complexity and diversity of these errors. From basic object presence to intricate temporal, relational, cross-modal, and domain-specific inconsistencies, researchers are continuously developing more sophisticated tools. However, challenges persist in creating benchmarks that are truly comprehensive, scalable, free from inherent biases (e.g., from LLM-assisted generation or evaluation), and perfectly aligned with human judgment across all contexts. Future directions will likely involve more dynamic, interactive, and adaptive evaluation frameworks that can capture the evolving nature of MLLM capabilities and their failure modes, potentially integrating advanced causal inference techniques to pinpoint the root causes of hallucination more precisely.