\subsection{Reference-Free and Consistency-Based Detection}

Detecting hallucinations in Large Language Models (LLMs) without relying on external ground truth or human-annotated references is crucial for evaluating proprietary models and scaling detection efforts where human annotation is impractical. These methods offer efficient and flexible assessment of factual correctness and internal coherence, focusing on the model's self-consistency or its ability to self-verify.

Early efforts laid the groundwork for fine-grained, reference-free detection. \cite{liu2021mo6} introduced the first token-level, reference-free hallucination detection task, along with the HADES dataset, which was created by perturbing Wikipedia text and using an iterative model-in-the-loop annotation strategy. This pioneering work provided a benchmark for pinpointing hallucinations at a granular level without external human references, though its dataset was based on perturbed existing text rather than purely generative outputs.

Building on the need for black-box and zero-resource solutions, \cite{manakul20236ex} proposed \textit{SelfCheckGPT}, a seminal method that leverages the inherent stochasticity of LLMs. The core idea is that if an LLM genuinely "knows" a fact, multiple stochastically sampled responses to the same prompt will be consistent; conversely, hallucinated facts will likely lead to divergent or contradictory samples. \textit{SelfCheckGPT} offers five variants for measuring informational consistency, including BERTScore, Question Answering, n-gram overlap, Natural Language Inference (NLI), and LLM Prompting, making it applicable to proprietary models without access to internal probabilities or external knowledge bases. However, \textit{SelfCheckGPT} can sometimes struggle when LLMs tend to repeat their own hallucinations across samples, reinforcing incorrect information.

To address this limitation, \cite{yang20251dw} introduced \textit{MetaQA}, a self-contained hallucination detection approach that significantly enhances consistency-based methods through the use of metamorphic relations (MRs) and prompt mutation. \textit{MetaQA} generates diverse mutations (synonymous and antonymous) from an LLM's initial response and then uses the LLM itself as a "test oracle" to verify the factual consistency of these mutations. This innovative application of MRs allows for more robust exposure of factual inconsistencies, with \textit{MetaQA} empirically outperforming \textit{SelfCheckGPT} across various LLMs and datasets by effectively preventing the model from reinforcing its own errors.

Beyond consistency-based sampling, other reference-free approaches focus on automated dataset generation and self-contained verification mechanisms. \cite{cao2023ecl} developed \textit{AutoHall}, a pipeline for automatically generating model-specific hallucination datasets from existing fact-checking data, eliminating the need for costly manual annotation. \textit{AutoHall} also proposes a zero-resource, black-box detection method based on LLM-driven self-contradiction, where an LLM's initial response is compared against multiple independently generated references to identify inconsistencies. This approach offers a scalable solution for creating dynamic benchmarks that adapt to evolving LLMs.

Another category of "reference-free" methods leverages structured data or powerful LLMs as implicit verification oracles, removing the need for human ground truth annotation for every instance. \cite{oh2024xa3} introduced \textit{ERBench}, a novel benchmark construction framework that utilizes existing relational databases (RDBs) and their integrity constraints (e.g., Functional Dependencies, Foreign Key Constraints) to automatically generate complex, verifiable questions and rationales. This allows for automated verification of both the LLM's answer and its step-by-step reasoning against the database's inherent factual knowledge. Similarly, \cite{li2024qrj} proposed an LLM-based detection framework that uses advanced LLMs, such as GPT-4, to extract factual statements from a target LLM's responses and then judge their truthfulness, even considering interrelations between statements. While relying on a powerful external LLM, this method is "reference-free" from human annotation, offering a scalable way to evaluate factual accuracy.

In conclusion, reference-free and consistency-based detection methods represent a vital frontier in hallucination research, offering scalable and flexible solutions for evaluating LLMs, especially proprietary models. Techniques like \textit{SelfCheckGPT} and \textit{MetaQA} leverage the internal consistency of LLM outputs, while others like \textit{AutoHall} and \textit{ERBench} automate dataset generation and verification against structured knowledge. Despite their promise, these methods still face challenges related to computational cost, the quality of stochastic sampling, the inherent capabilities of the LLMs used for self-verification, and the potential for biases even in automated or LLM-as-a-judge systems. Future research will likely focus on enhancing the robustness and efficiency of these self-contained detection mechanisms, further reducing their reliance on external resources, and expanding their applicability to more complex and nuanced forms of hallucination.