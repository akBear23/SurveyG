\subsection{Scope and Motivation of the Review}
The proliferation of Large Language Models (LLMs) across diverse domains, from content generation to critical decision support, has underscored a fundamental challenge: hallucination. Defined as the generation of plausible yet factually incorrect, inconsistent, or unfaithful information, hallucination poses a significant barrier to the dependable, transparent, and safe deployment of these powerful AI systems \cite{zhang2023k1j}. This pervasive issue can lead to the dissemination of misinformation, erode user trust, and introduce substantial risks in sensitive applications, as highlighted by concerns in fields like medical education where incorrect information can lead to patient harm \cite{parente2024vlq}. Consequently, a comprehensive understanding and robust management of hallucination are paramount for the continued advancement and responsible integration of LLMs into society, ultimately contributing to the development of trustworthy artificial intelligence.

This literature review aims to provide a structured and critical synthesis of the evolving research landscape surrounding LLM hallucination. Our scope encompasses a holistic examination, tracing the intellectual trajectory from the foundational understanding of hallucination to advanced mitigation techniques and the emerging complexities in multimodal contexts. Specifically, we will delve into the historical characterization and evolving taxonomies of hallucination, exploring its root causes and the profound theoretical insights into its inherent inevitability. Subsequently, the review will scrutinize the diverse methodologies developed for benchmarking and detection, ranging from reference-free and consistency-based approaches to fine-grained, rationale-based evaluations and specialized diagnostic frameworks for Retrieval-Augmented Generation (RAG) systems. A significant portion of this analysis is dedicated to a critical appraisal of mitigation strategies, categorizing them into external grounding techniques (such as RAG and Knowledge Graphs) and internal model interventions (including decoding-time strategies, training-based approaches, and causal interventions). Finally, we will address the unique challenges and solutions pertinent to multimodal hallucination, alongside advanced topics like adversarial robustness, safety-critical applications, and the pursuit of unified theoretical frameworks.

The primary motivation for this comprehensive analysis stems from the urgent need to consolidate the rapidly expanding body of knowledge in this critical area. The field has witnessed an explosion of research, driven by the increasing scale and versatility of LLMs, which often exhibit subtle yet impactful errors that are difficult for both humans and models to detect \cite{zhang2023k1j}. This review seeks to synthesize the current state-of-the-art, identify critical gaps in existing approaches, and highlight promising future directions. By systematically mapping the progress in understanding, detecting, and mitigating hallucination, we aim to provide researchers and practitioners with a clear roadmap for developing more reliable and accountable AI systems. The imperative to ensure verifiability and accountability, for instance, has led to innovative approaches like enabling LLMs to generate text with explicit citations, crucial for building trustworthy information-seeking applications \cite{gao2023ht7}. Furthermore, the unique challenges posed by multimodal models, such as the "modality gap" and inherited LLM hallucination tendencies, necessitate dedicated attention \cite{lan20240yz}. This synthesis is crucial for moving beyond reactive problem-solving to proactive robustness engineering, fostering a deeper scientific understanding of AI reliability, and ensuring that LLMs can be deployed responsibly and ethically in an increasingly AI-driven world.

To achieve this, the review is structured as follows: Section 2 delves into the foundational understanding and theoretical limits of hallucination. Section 3 surveys the landscape of benchmarking and detection methodologies. Sections 4 and 5 then explore the two major families of mitigation strategies: external grounding and reasoning, and internal model interventions and training, respectively. Section 6 specifically addresses applications, specialized domains, and the complexities of multimodal hallucination. Finally, Section 7 discusses advanced topics and future directions, before Section 8 concludes with a summary of key developments and persistent challenges.