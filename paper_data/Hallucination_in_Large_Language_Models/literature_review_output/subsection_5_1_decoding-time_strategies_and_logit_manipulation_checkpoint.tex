\subsection{Decoding-Time Strategies and Logit Manipulation}

Decoding-time strategies represent a crucial and efficient paradigm for mitigating hallucinations in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). These techniques intervene directly during the token generation process, offering a training-free and flexible alternative to costly model retraining. The core principle involves dynamically shaping the model's output probability distribution (logits) to steer generation towards more factual and coherent content, often by leveraging internal uncertainty signals or external factual checks.

Traditional decoding methods, such as greedy decoding or beam search, prioritize high-probability sequences but can inadvertently amplify statistical biases or language priors, leading to hallucinations \cite{leng2023ohr}. To counter this, advanced decoding strategies primarily fall into two categories: **contrastive decoding**, which penalizes tokens inconsistent with a reference or a subtly altered input, and **direct logit steering**, which explicitly modifies token probabilities based on various signals.

\subsubsection{Contrastive Decoding for Factual Consistency}
Contrastive decoding methods operate by comparing the output distribution of the primary model (or a subtly altered version of the input) with a "negative" or "reference" signal to suppress hallucinated content. This typically involves multiple forward passes, where the difference in log probabilities guides the token selection.

A prominent approach, **Visual Contrastive Decoding (VCD)** \cite{leng2023ohr}, addresses object hallucinations in MLLMs by introducing visual uncertainty. VCD applies a Gaussian noise mask to the original image, creating a distorted input. It then contrasts the log probabilities from the original and distorted inputs, penalizing tokens that are highly probable under the distorted (hallucination-prone) input. This mechanism effectively calibrates the MLLM's over-reliance on language priors and statistical biases, demonstrating significant improvements on benchmarks like POPE and MME. To prevent VCD from promoting implausible tokens, an adaptive plausibility constraint is applied, pruning candidate tokens based on the confidence of the original output distribution.

Building on this, \textit{ConVis} \cite{park20247cm} introduces a novel training-free contrastive decoding method for MLLMs that leverages a Text-to-Image (T2I) model. By reconstructing an image from the MLLM's initial caption and using this T2I-generated image as a contrastive signal, \textit{ConVis} penalizes tokens corresponding to visually represented hallucinations. This innovative use of an external generative model as a "visual critic" achieves state-of-the-art performance on benchmarks like CHAIR and POPE.

Beyond visual modalities, contrastive decoding has been adapted for unimodal LLMs and specialized domains. \textit{Induce-then-Contrast Decoding (ICD)} \cite{zhang202396g} proposes creating a "factually weak LLM" by fine-tuning the original model on non-factual samples. During inference, the log probabilities of these induced hallucinations from the weak model are subtracted from the original model's predictions, thereby amplifying truthful outputs. ICD also incorporates an adaptive plausibility constraint to selectively penalize only potentially untruthful tokens, preserving overall generation quality. This method has shown remarkable improvements in factuality on TruthfulQA and FACTSCORE, enabling smaller models to rival larger ones. Similarly, \textit{ALternate Contrastive Decoding (ALCD)} \cite{xu2024t34} addresses hallucinations in medical information extraction (MIE) by separating identification and classification functions into sub-task models. During inference, ALCD alternately contrasts output distributions from these sub-task models, selectively enhancing specific capabilities while minimizing the influence of other inherent LLM abilities, further demonstrating the versatility of contrastive signals.

A more advanced form of contrastive decoding, **Hallucination-Induced Optimization (HIO)** \cite{chen20247jb}, refines the concept of a "negative" model. HIO involves a training stage to create an "Evil LVLM" by intentionally inducing hallucinations using a *reversed* Bradley-Terry model. This "Evil LVLM" is trained to *prioritize* hallucinatory content, amplifying the logits of incorrect tokens. During inference, logits from this "Evil LVLM" are contrasted with the original LVLM to precisely reduce hallucinations. HIO's theoretical analysis emphasizes the need for consistent logit differences between potential hallucinated and correct tokens, and its Amplification of Multiple Targeted Hallucination (AMTH) technique allows for simultaneous training against diverse hallucination types. While HIO involves a pre-training step for the "Evil LVLM," its core mechanism is a decoding-time contrast, making it a powerful extension of this paradigm.

A common trade-off in contrastive decoding is the potential for increased inference time due to multiple forward passes and the risk of degrading the quality or coherence of generated content by overly penalizing tokens. Adaptive constraints and careful tuning of contrastive strength (e.g., `α` and `β` parameters in VCD and ICD) are crucial to balance hallucination reduction with maintaining linguistic quality.

\subsubsection{Direct Logit Steering and Prompt-Derived Influence}
Beyond explicit contrastive signals, other decoding-time strategies directly manipulate the logit distribution or influence it through prompt-derived information without requiring deep architectural interventions.

\textit{OPERA} (Over-trust Penalty and Retrospection-Allocation) \cite{huang2023du3} is a novel MLLM decoding method applied during beam-search. It is grounded in the observation that MLLM hallucinations often stem from "over-trust" in a few "summary tokens" within the self-attention matrix, leading to neglect of visual information. OPERA introduces a penalty term on the model logits during beam-search decoding, derived from a column-wise metric on the self-attention map that indicates the "over-trust degree." Furthermore, its Retrospection-Allocation Strategy allows the decoding process to "roll back" and re-select better candidates if strong over-trust patterns are detected, dynamically correcting the generation path. This method effectively mitigates hallucinations without additional training, data, or external knowledge.

Similarly, \textit{MVP} (Multi-View Multi-Path Reasoning) \cite{qu2024pqc} enhances image comprehension through multi-view information seeking and guides decoding via a "certainty score" (based on logit differences) across multiple paths. This training-free and tool-free framework directly leverages logit-based certainty to alleviate hallucinations by addressing both incomplete visual understanding and low certainty during token generation.

Prompt-based steering methods, while not directly manipulating logits in a mathematical sense, influence the model's internal state such that subsequent token probabilities are altered. \textit{Counterfactual Inception} \cite{kim2024ozf} is a training-free method that prompts MLLMs to self-generate "counterfactual keywords" (deliberately deviating from visual content). These keywords are then filtered and incorporated into a prompt that instructs the MLLM to avoid them, thereby guiding the model towards more factual outputs by implicitly steering the logit space. \textit{Order Matters in Hallucination} \cite{xie20247zk} highlights how the sequence of reasoning in prompts can significantly impact an LLM's consistency and factual accuracy. By generating reasoning first and then the conclusion, models are less prone to fabricating answers, indicating that prompt structure can indirectly steer the internal generation process and thus the logit probabilities towards more reliable outcomes.

It is crucial to distinguish these methods from those that involve deeper manipulation of internal model states or causal interventions on attention mechanisms (covered in Subsection 5.3), or those that rely heavily on external tools and iterative self-correction (covered in Section 4.3). The techniques discussed here primarily focus on shaping the final token probabilities through contrastive signals or direct logit adjustments, often with minimal changes to the model's internal architecture or complex external interactions.

In conclusion, decoding-time strategies represent a vital and evolving area for mitigating hallucinations, offering efficient, training-free solutions. Approaches range from innovative contrastive decoding methods like VCD \cite{leng2023ohr}, ConVis \cite{park20247cm}, ICD \cite{zhang202396g}, ALCD \cite{xu2024t34}, and HIO \cite{chen20247jb}, which leverage diverse "negative" signals, to sophisticated logit-based steering techniques such as OPERA \cite{huang2023du3} and MVP \cite{qu2024pqc}, which directly adjust token probabilities based on internal observations or certainty. Furthermore, prompt-derived influences \cite{kim2024ozf, xie20247zk} offer flexible means of guiding model outputs. While significant progress has been made in improving factual consistency and inference efficiency, ongoing challenges include ensuring generalizability across diverse hallucination types, reducing reliance on external proprietary models, and developing more robust internal uncertainty quantification mechanisms to optimize logit adjustments without compromising overall generation quality. Future directions will likely focus on more sophisticated negative signal generation, adaptive logit scaling, and hybrid approaches that seamlessly integrate these decoding-time interventions with other mitigation paradigms.