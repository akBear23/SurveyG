\subsection{Systematic Taxonomies and Categorization of Hallucinations}

The accurate identification and mitigation of hallucinations in Large Language Models (LLMs) necessitate a systematic understanding and categorization of this multifaceted problem. Early efforts to formalize hallucination primarily focused on abstractive summarization, where \textcite{maynez2020h3q} pioneered a foundational distinction between \textit{intrinsic} and \textit{extrinsic} hallucinations. Intrinsic hallucinations misrepresent information present in the source document, while extrinsic hallucinations introduce information not directly inferable from the input. Their work, based on extensive human evaluation, highlighted the prevalence of these issues and advocated for semantic-aware evaluation metrics beyond traditional surface-level scores.

As LLMs evolved into more general-purpose agents, the need for more fine-grained and LLM-specific taxonomies became apparent. \textcite{liu2021mo6} contributed to this empirical understanding by introducing HADES, a token-level, reference-free hallucination detection benchmark. Their analysis of the dataset characterized various hallucination types, including domain-specific knowledge errors, commonsense knowledge conflicts, and incoherence, providing early insights into the diverse manifestations of LLM errors. Building on such observations, \textcite{zhang2023k1j} proposed a comprehensive LLM-centric taxonomy, categorizing hallucinations based on their relation to the input: \textit{input-conflicting}, \textit{context-conflicting}, and \textit{fact-conflicting}. This framework emphasized the growing challenge of fact-conflicting hallucinations, where generated content contradicts established world knowledge, often without any explicit input or context to ground it.

Further refining these classifications, \textcite{rawte2023ao8} introduced an extensive framework that profiles hallucination across multiple dimensions. Their taxonomy includes degrees of severity (mild, moderate, alarming), orientations (Factual Mirage and Silver Lining, each with intrinsic and extrinsic sub-categories), and six specific types such as Acronym Ambiguity, Numeric Nuisance, and Geographic Erratum. This detailed categorization was instrumental in developing their Hallucination Vulnerability Index (HVI), a quantitative metric for assessing LLM susceptibility to hallucination. Complementing these descriptive taxonomies, \textcite{du2023qu7} offered a capability-based perspective, attributing hallucinations to deficiencies in fundamental model capabilities like commonsense memorization, relational reasoning, and instruction following. This mechanistic view provides insights into the underlying causes rather than just the observable phenomena.

The evolving understanding of hallucination also led to more specialized classifications. \textcite{li2024qrj} focused specifically on \textit{factuality hallucination}, proposing a detailed taxonomy that includes Entity-error, Relation-error, Incompleteness, Outdatedness, Overclaim, and Unverifiability. This fine-grained breakdown of factual errors, coupled with their HaluEval 2.0 benchmark, enabled a more precise analysis of LLM factual accuracy. Similarly, for conversational AI, \textcite{chen2024c4k} developed DiaHalu, a dialogue-level hallucination benchmark, and introduced a comprehensive taxonomy for dialogue contexts. This included subtypes such as Non-factual, Incoherence (further divided into input-conflicting, context-conflicting, and self-conflicting), Irrelevance, Overreliance, and Reasoning Error, reflecting the unique complexities of multi-turn interactions.

The problem's complexity extends significantly into multimodal contexts. \textcite{li2023249} conducted the first systematic empirical study of "object hallucination" in Large Vision-Language Models (LVLMs), where models describe objects inconsistent with or absent from the target images. Their work, which introduced the POPE evaluation method, highlighted that object frequency and co-occurrence in training data are key drivers of this specific type of hallucination. Building on this, \textcite{liu2023882} implicitly categorized multimodal hallucinations through their robust instruction tuning dataset, LRV-Instruction, which included negative instructions designed to address nonexistent object manipulation, existent object manipulation with inconsistent attributes, and knowledge manipulation. These efforts illustrate how hallucination manifests differently across modalities and generation tasks, requiring tailored categorization and evaluation.

Finally, review papers like \textcite{ye2023yom} synthesize these diverse categorizations, offering a broad taxonomy of hallucinations observed across various text generation tasks (e.g., machine translation, question answering, dialogue, summarization, knowledge graph generation, and cross-modal systems). They also provide a systematic analysis of hallucination origins, linking them to factors such as data collection, knowledge gaps, and the optimization process. This comprehensive overview underscores the dynamic nature of hallucination and the continuous need for refined taxonomies. The dimensions of information quality, such as consistency, relevance, and accuracy, as formalized by \textcite{rejeleene2024okw} in their IQ model, also implicitly contribute to categorization by defining the attributes that hallucinated content violates.

In conclusion, the systematic categorization of LLM hallucinations has evolved from initial broad distinctions like intrinsic versus extrinsic to highly granular, task-specific, and modality-specific taxonomies. This progression reflects an deepening understanding of the problem's diverse manifestations and underlying causes. However, the rapid advancement of LLM capabilities and the emergence of new generation paradigms necessitate ongoing research into dynamic and comprehensive categorization frameworks that can adapt to novel forms of hallucination and provide actionable insights for mitigation. A universally applicable, yet sufficiently granular, taxonomy remains a critical area for future research to ensure the trustworthiness and reliability of LLM applications.