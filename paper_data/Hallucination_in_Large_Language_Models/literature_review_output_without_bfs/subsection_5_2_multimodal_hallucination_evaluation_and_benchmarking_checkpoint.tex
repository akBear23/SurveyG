\subsection{Multimodal Hallucination Evaluation and Benchmarking}

The pervasive issue of hallucination in Multimodal Large Language Models (MMLMs), where generated content deviates from factual multimodal inputs, necessitates robust and specialized evaluation frameworks. Systematically measuring these complex errors is crucial for diagnosing and addressing the underlying causes, moving beyond subjective assessments to quantitative diagnostics. The field has evolved from basic object presence checks to sophisticated analyses of temporal, relational, and cross-modal inconsistencies.

Early efforts to quantify hallucinations primarily focused on object-level inconsistencies. \cite{dai20229aa} conducted a systematic study of object hallucination in Vision-Language Pre-trained (VLP) models, revealing that optimizing for standard metrics could paradoxically increase hallucination and introducing ObjMLM to mitigate this. Building on this, \cite{kaul2024ta7} distinguished between Type I (free-form) and Type II (fixed-format) object hallucinations, introducing THRONE, an object-based benchmark for free-form generations, which highlighted that existing benchmarks like POPE \cite{dai20229aa} often underestimate hallucinations due to sampling biases. Furthering the complexity, \cite{chen2024vy7} addressed *multi-object hallucination*, proposing the Recognition-based Object Probing Evaluation (ROPE) protocol. ROPE utilizes visual referring prompts to eliminate ambiguity and systematically analyzes hallucination across various object class distributions, revealing how models exploit shortcuts and spurious correlations.

As MMLMs advanced, the scope of hallucination evaluation expanded beyond simple object presence to more intricate visual reasoning. \cite{wu2024bxt} introduced R-Bench, a novel benchmark specifically designed to evaluate and analyze *relationship hallucinations* in Large Vision-Language Models (LVLMs), a critical gap previously overlooked by object-focused benchmarks. Extending this, \cite{zheng20246fk} developed Reefknot, a comprehensive benchmark for *relation hallucination* that categorizes errors into perceptive and cognitive types and proposes a "Detect-then-Calibrate" mitigation strategy based on token-level confidence. This work underscored that MMLMs struggle significantly with relational understanding, often exhibiting a bias towards 'Yes' answers.

The advent of MMLMs capable of processing diverse modalities necessitated benchmarks tailored to cross-modal interactions. \cite{sungbin2024r2g} introduced AVHBench, the first comprehensive benchmark for *cross-modal hallucinations* in Audio-Visual Large Language Models (AV-LLMs). AVHBench evaluates both audio-driven video hallucination and video-driven audio hallucination, utilizing a semi-automatic annotation pipeline and synthetic videos to create challenging scenarios. Similarly, \cite{kuan20249pm} specifically investigated *object hallucination in Large Audio-Language Models*, proposing ECHO and Cover metrics for generative tasks and highlighting LALMs' struggle with discriminative queries. For video-language models, \cite{wang2024rta} presented VideoHallucer, a benchmark evaluating *intrinsic and extrinsic hallucinations* related to dynamic video content and temporal reasoning, and introduced the Self-PEP framework for mitigation. Complementing this, \cite{li2024wyb} developed VidHalluc, the largest benchmark for evaluating *temporal hallucinations*, focusing on action, temporal sequence, and scene transition inconsistencies, and proposed DINO-HEAL for training-free mitigation. These benchmarks collectively emphasize the unique challenges of evaluating dynamic and multi-sensory information.

Beyond static or single-turn evaluations, researchers also began to investigate how hallucinations propagate in interactive settings. \cite{zhong2024mfi} identified and systematically investigated "Multimodal Hallucination Snowballing," where an LVLM's own generated hallucinations influence subsequent responses. Their MMHalSnowball framework and Residual Visual Decoding (RVD) mitigation method highlighted the need for models to maintain direct access to visual information to prevent error accumulation.

The growing complexity and deployment of MMLMs in high-stakes domains also spurred the development of domain-specific and unified evaluation approaches. \cite{chen2024hfe} introduced Med-HallMark, the first benchmark for detecting and evaluating *medical hallucinations* in LVLMs, emphasizing the critical need for clinically relevant, fine-grained assessment and proposing the MediHall Score and Detector. To unify disparate evaluation efforts, \cite{chen2024lc5} proposed UNIHD (Unified Multimodal Hallucination Detection), a task-agnostic, tool-enhanced framework with the MHaluBench meta-evaluation benchmark, covering various hallucination categories and multimodal tasks. Further refining the granularity, \cite{jiang2024792} presented Hal-Eval, a universal and fine-grained framework that introduced "Event Hallucination" as a new category, involving fictional narratives around non-existent entities. Addressing global applicability, \cite{zhang2025pex} developed CCHall, a novel benchmark for detecting *joint cross-lingual and cross-modal hallucinations*, revealing significantly worse performance in these compounded scenarios.

Crucially, the proliferation of benchmarks led to a critical meta-evaluation of their quality. \cite{yan2024ux8} introduced the Hallucination benchmark Quality Measurement (HQM) framework, a psychometrics-inspired approach to assess benchmark reliability and validity. Their analysis revealed significant issues in existing benchmarks, such as response bias in closed-ended tasks and misalignment with human judgment in open-ended evaluations, leading to the proposal of the High-Quality Hallucination Benchmark (HQH). Similarly, \cite{guan2023z15} developed Hallusionbench, an advanced diagnostic suite for "entangled language hallucination and visual illusion," utilizing human-edited images and control groups to systematically dissect complex failure modes. These works underscore the critical importance of tailored evaluation metrics and diagnostic suites that capture nuanced errors from cross-modal interactions, while also highlighting the persistent challenges of creating unbiased, human-aligned, and truly reliable assessments.

In conclusion, the evaluation of multimodal hallucinations has progressed significantly, moving from simple object presence to complex relational, temporal, and cross-modal interactions. However, despite the development of numerous specialized benchmarks and diagnostic suites, significant challenges remain. The meta-evaluation studies reveal that many existing benchmarks suffer from reliability and validity issues, including response biases and misalignment with human judgment. The dynamic nature of MMLMs, coupled with the inherent complexity of cross-modal reasoning, necessitates continuous innovation in evaluation methodologies. Future directions must prioritize the creation of robust, unbiased, and human-aligned diagnostic tools that not only quantify hallucinations but also provide fine-grained insights into their root causes, enabling more targeted and effective mitigation strategies.