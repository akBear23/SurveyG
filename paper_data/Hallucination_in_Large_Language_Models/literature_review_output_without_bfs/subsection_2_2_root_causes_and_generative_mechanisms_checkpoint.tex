\subsection{Root Causes and Generative Mechanisms}

Large Language Model (LLM) hallucination, characterized by the generation of plausible but factually incorrect or unfaithful content, arises from a complex interplay of inherent model limitations, data imperfections, and the probabilistic nature of their generative process. A comprehensive understanding of these underlying factors is paramount for developing effective detection and mitigation strategies \cite{ye2023yom, zhang2023k1j, liu2024p39}.

A primary category of generative mechanisms stems from **data-centric pathologies** embedded throughout the LLM lifecycle, from pre-training to fine-tuning. The vast, web-scale datasets used for pre-training often contain fabricated, outdated, or biased information, making it challenging to eliminate sources of factual inconsistencies \cite{zhang2023k1j, yin2024iau}. \textcite{rejeleene2024okw} highlight how suboptimal tokenization, lack of data diversity, and inherent biases within pre-training data directly compromise information quality and trustworthiness. Furthermore, noisy or imperfect reference summaries in training datasets, particularly in low-resource or specialized domains like clinical notes, have been shown to directly contribute to unfaithful generation \cite{adams202289x}. Another significant data-related issue is the "knowledge cutoff," where LLMs struggle with rapidly evolving world knowledge, leading to the generation of outdated or false information. This limitation is empirically demonstrated by \textcite{vu202337s} through their dynamic FRESH QA benchmark, underscoring the static nature of parametric knowledge acquired during training.

Beyond data quality, **model architecture and knowledge representation flaws** contribute significantly to hallucination. LLMs, despite their scale, possess inherent limitations in capacity and their ability to perfectly represent and recall all knowledge. \textcite{du2023qu7} provide a quantitative attribution framework, linking hallucination to specific model capability deficiencies, such as commonsense memorization, relational reasoning, and instruction following. Their analysis reveals that lower frequency of pre-training knowledge, increased complexity in reasoning tasks, and conflicts between pre-training and fine-tuning objectives significantly correlate with higher hallucination rates. The black-box nature of LLMs and their often vague knowledge boundaries further complicate the identification and rectification of these internal inconsistencies \cite{zhang2023k1j}. A critical challenge arises from the conflict between a model's parametric knowledge (learned during pre-training) and contextual knowledge (provided in the prompt or retrieved documents). Even when augmented with retrieval systems, LLMs can struggle to effectively process long and potentially noisy contexts. \textcite{lv2024k5x} demonstrate that irrelevant information within retrieved documents can distract LLMs, leading to knowledge hallucination, even for models designed for long inputs. This difficulty in discerning relevant from irrelevant information is further highlighted by \textcite{chen2023h04}, whose RAG benchmark shows that LLMs often lack robustness to noise, fail to reject negative information, and struggle with integrating facts from multiple documents, frequently prioritizing incorrect retrieved information over their own correct internal knowledge. \textcite{song2024v5n} identify a specific "false negative problem" where LLMs exhibit a bias towards negative judgments when assessing statements against context, leading to input-conflicting hallucinations and overconfidence in false responses.

Finally, **training and inference procedure artifacts** are crucial generative mechanisms. The inherent probabilistic nature of next-token prediction means LLMs can assign probabilities to non-factual information, leading to factual inaccuracies. This uncertainty in output entities, characterized by low predictive probability and high entropy, is leveraged by \textcite{su2024gnz} for real-time hallucination detection, underscoring the direct link between probabilistic generation and factual inaccuracies. The standard maximum-likelihood estimation (MLE) training objective, while promoting fluency, can inadvertently encourage the generation of plausible but incorrect information by prioritizing token sequences that are statistically probable over those that are factually accurate \cite{liu2024p39, li2025qzg}. This "exposure bias" during training, where models are only exposed to ground truth sequences, can lead to compounding errors during free-form generation \cite{liu2024p39}. Decoding strategies further modulate the manifestation of hallucinations. While not a root cause itself, the choice of decoding algorithm (e.g., greedy, beam search, nucleus sampling) can amplify or mitigate the risk of hallucination by influencing the model's exploration of the token probability space. The inherent difficulty for LLMs to distinguish between plausible but incorrect information and verifiable facts is a persistent challenge, often leading to a "snowballing" effect where LLMs maintain coherence with earlier incorrect statements, propagating errors through multi-step reasoning \cite{ye2023yom}. The very necessity for post-hoc verification frameworks, as explored in mitigation strategies, underscores this fundamental generative mechanism: the model's inability to perform concurrent fact-checking during generation. Even when augmented with structured knowledge graphs (KGs), LLMs can still exhibit hallucinations, especially when KGs are perturbed or contain errors, as empirically studied by \textcite{sui20242u1}, highlighting the ongoing challenge of robust knowledge integration and the model's internal processing of such information. The complexity of multi-turn interactions also introduces unique hallucination patterns, including incoherence and reasoning errors, as explored by \textcite{chen2024c4k} in their dialogue-level benchmark, pointing to the difficulty in maintaining consistent internal states over extended conversations.

In summary, LLM hallucinations are not attributable to a single factor but emerge from a confluence of issues: imperfect training data, limitations in model capacity and knowledge representation, and the probabilistic, fluency-oriented nature of their training and inference processes. These mechanisms collectively highlight the challenge in achieving complete factual consistency, laying the groundwork for the necessity of robust detection and mitigation efforts.