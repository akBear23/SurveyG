\subsection{Chain-of-Thought and Self-Correction Mechanisms}

Enhancing Large Language Models' (LLMs) internal reasoning capabilities and enabling robust self-verification are crucial steps towards mitigating hallucination and improving factual accuracy. This research direction focuses on empowering LLMs to deliberate, plan, and critique their own outputs, moving beyond mere generation to more grounded and trustworthy responses primarily through internal cognitive processes.

\subsubsection{Linear Reasoning and Action Paradigms}
A foundational approach to structured reasoning is Chain-of-Thought (CoT) prompting, which encourages LLMs to articulate their intermediate reasoning steps. Building upon this, \cite{yao20229uz} introduced \textit{ReAct} (Reasoning and Acting), a paradigm that synergizes verbal reasoning with external actions. ReAct prompts LLMs to generate interleaved "thoughts" (internal reasoning) and "actions" (interactions with external environments or tools, such as a search API). While ReAct involves external actions, its core contribution to self-correction lies in how these actions are used to \textit{verify} and \textit{refine} the internal reasoning. The dynamic interplay allows the LLM to "reason to act" by formulating plans and strategies, and simultaneously "act to reason" by gathering external information to refine its understanding or correct factual inaccuracies. This mechanism effectively grounds LLM reasoning in verifiable external information, mitigating hallucination and error propagation inherent in purely internal, unverified CoT.

\subsubsection{Post-hoc Verification and Iterative Refinement}
Beyond interleaved reasoning and action, other strategies focus on explicit internal verification and iterative refinement. \cite{dhuliawala2023rqn} proposed \textit{Chain-of-Verification} (CoVe), a systematic approach that prompts LLMs to check their own generated facts. CoVe involves a multi-step process: first, the LLM generates a baseline response; second, it formulates a list of specific verification questions based on that response; third, it answers these questions; and finally, it revises its initial response. A critical innovation is the "factored" variant of CoVe, which executes verification questions independently, without conditioning on the potentially hallucinated baseline response or other verification answers. This design significantly reduces the likelihood of the model repeating its own mistakes, demonstrating a consistent decrease in hallucinations and improved factual precision.

Generalizing this concept, \cite{ji2023vhv} developed an iterative \textit{self-reflection methodology} for mitigating hallucination, particularly in high-stakes domains like medical generative question-answering. This approach establishes a dynamic feedback loop where the LLM iteratively generates, scores, and refines both its background knowledge and its answers until satisfactory levels of factuality, consistency, and entailment are achieved. This introspective process allows LLMs to actively identify and correct shortcomings, leading to demonstrably more reliable outputs. The broader landscape of such automated correction strategies is comprehensively surveyed by \cite{pan2023mwu} and \cite{pan2024y3a}, which categorize techniques based on the correction target, feedback source, timing, and application method, highlighting the growing interest in autonomous error rectification to reduce reliance on costly human feedback.

\subsubsection{Advanced Non-Linear Reasoning Structures}
While linear CoT and ReAct enhance sequential reasoning, more complex problems often benefit from exploring multiple reasoning paths. \cite{yao2023tot} introduced \textit{Tree-of-Thoughts} (ToT), which generalizes CoT by enabling LLMs to explore diverse reasoning steps, evaluate intermediate thoughts, and backtrack when necessary. Instead of a single linear chain, ToT constructs a tree of possible thoughts, allowing the model to explicitly consider multiple paths, prune unpromising ones, and self-correct by selecting the most coherent and factually consistent reasoning trajectory. This multi-path exploration inherently provides a more robust self-correction mechanism compared to linear CoT, as it allows the model to identify and discard erroneous intermediate steps before committing to a final answer.

Further extending this, \cite{besta2023got} proposed \textit{Graph-of-Thoughts} (GoT), which moves beyond the tree structure to a more flexible graph representation. GoT allows for arbitrary connections between thoughts, enabling more complex reasoning patterns like cycles, parallel processing, and dynamic merging of ideas. This flexibility facilitates richer internal deliberation, where LLMs can iteratively refine and interconnect thoughts, leading to more sophisticated self-correction capabilities. By explicitly representing and manipulating a graph of ideas, GoT offers enhanced transparency and control over the reasoning process, making it easier to diagnose and correct errors compared to opaque end-to-end generation. The ability to explore, evaluate, and refine multiple interconnected thoughts significantly reduces the propagation of initial errors, a common source of hallucination in simpler reasoning paradigms.

\subsubsection{Decoding-Level Interventions}
Beyond explicit reasoning steps, self-correction can also be integrated at the decoding level. \cite{zhang202396g} introduced \textit{Induce-then-Contrast Decoding (ICD)} to alleviate hallucinations by influencing token generation probabilities. ICD constructs a "factually weak LLM" by inducing hallucinations and then, during generation, subtracts the log probabilities of this weak model from the original model's predictions. This contrastive approach effectively penalizes untruthful tokens, guiding the model towards more factual outputs without requiring extensive retraining. This method represents a more implicit form of self-correction, where the model's internal "critic" is encoded within the decoding process itself.

Despite these significant advancements, challenges remain. The computational cost of multi-step reasoning, especially for iterative self-correction and multi-path exploration (e.g., ToT, GoT), can be substantial, limiting their application in real-time or resource-constrained scenarios. The robustness of internal "critics" or self-evaluation mechanisms is still dependent on the LLM's inherent capabilities and can sometimes perpetuate errors if the model's internal knowledge is flawed or its self-assessment is inaccurate. Future work will likely focus on developing more efficient and sophisticated internal critics, seamlessly integrating external tools with internal deliberation while maintaining a clear focus on internal reasoning, and ensuring generalizability of these self-correction mechanisms across diverse hallucination types and domains.