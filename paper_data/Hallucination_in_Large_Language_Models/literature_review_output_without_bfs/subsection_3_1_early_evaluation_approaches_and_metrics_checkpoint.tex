\subsection{Early Evaluation Approaches and Metrics}

The initial efforts to assess the factuality and faithfulness of text generation, particularly in abstractive summarization, quickly exposed the profound limitations of traditional surface-level metrics. While widely adopted metrics like ROUGE and BERTScore proved effective in quantifying lexical overlap, fluency, and content similarity, they consistently failed to capture semantic accuracy or subtle factual errors \cite{maynez2020h3q}. This critical inadequacy meant that models could generate fluent but entirely fabricated content, yet still score highly on these metrics, leading to a significant gap in reliably understanding model trustworthiness and reliability. This realization spurred a crucial shift towards developing more robust evaluation methodologies that prioritized human judgment and deeper semantic analysis.

A seminal contribution in this evolving landscape was made by \cite{maynez2020h3q}, who introduced a systematic human evaluation framework to rigorously characterize and quantify hallucinations in abstractive summaries. Their meticulous annotation scheme categorized hallucinations into "intrinsic" (content inconsistent with the source document) and "extrinsic" (plausible but uninferable information not present in the source). This work empirically demonstrated that a substantial proportion of summaries (over 70\% in single-sentence examples) contained such errors, and, critically, that traditional metrics correlated poorly with human judgments of faithfulness. Maynez et al. (2020) further proposed that textual entailment measures could serve as a more semantically-aware automatic proxy for faithfulness, laying foundational groundwork for moving beyond superficial text similarity.

Building on the need for more granular semantic evaluation, \cite{chen2022gkm} introduced PropSegmEnt, a large-scale corpus designed for proposition-level segmentation and entailment recognition. This work highlighted that even a single sentence often contains multiple propositions, each carrying distinct truth values, and argued for the necessity of evaluating entailment at this fine-grained level. By enabling the recognition of textual entailment for individual propositions, PropSegmEnt offered a more precise mechanism for detecting factual inconsistencies than sentence- or paragraph-level Natural Language Inference (NLI), directly addressing the limitations of coarser semantic checks. Extending these semantic evaluation principles to diverse linguistic contexts, \cite{aharoni2022ioz} explored factual consistency evaluation in multilingual summarization. Their mFACE approach leveraged multilingual NLI models to mitigate hallucinations, demonstrating the applicability of semantically-aware evaluation beyond English and highlighting early efforts to ensure factual consistency across different languages.

The inherent complexity of reliably measuring hallucination, especially in black-box models lacking accessible internal probabilities or requiring expensive human annotation, necessitated the development of early automated and reference-free detection techniques. \cite{liu2021mo6} addressed the challenge of reference-free evaluation by proposing a token-level hallucination detection task and introducing HADES, an annotated dataset created through contextual perturbation and iterative model-in-the-loop annotation. This represented an early step towards fine-grained, automatic detection without relying on ground-truth references, a common bottleneck in real-world free-form generation. Further advancing black-box evaluation, \cite{manakul20236ex} introduced SelfCheckGPT. This novel zero-resource method leverages the consistency of stochastically sampled LLM outputs, operating on the premise that genuinely known facts yield consistent samples, whereas hallucinations result in divergent or contradictory responses. By employing various consistency measures, including BERTScore and NLI, SelfCheckGPT provided a practical baseline for assessing factuality in proprietary models, thus tackling the initial challenges of measuring this complex phenomenon without internal model access or external knowledge bases.

As the field progressed, there was also a continued refinement of human evaluation methodologies. \cite{wang2023hgw} introduced expert-aligned evaluation for summarization, annotating new test sets based on the "Lasswell Communication Model" to focus on more fine-grained news elements. This approach aimed to address issues of factual hallucination and information redundancy in existing datasets like CNN/DailyMail and BBC XSum, demonstrating an ongoing effort to improve the quality and diagnostic power of human annotations beyond initial broad categorizations.

In conclusion, these early evaluation approaches marked a pivotal transition in understanding and measuring hallucination in text generation. They decisively moved beyond the limitations of traditional lexical overlap metrics, establishing systematic human evaluation as the gold standard for assessing faithfulness and factuality, particularly in abstractive summarization \cite{maynez2020h3q}. Concurrently, researchers began exploring more semantically-aware automatic methods, such as proposition-level entailment \cite{chen2022gkm} and multilingual NLI-based evaluations \cite{aharoni2022ioz}, to capture nuanced factual errors. The emergence of reference-free and black-box detection techniques like HADES \cite{liu2021mo6} and SelfCheckGPT \cite{manakul20236ex} addressed critical scalability and accessibility challenges, providing initial tools for practical assessment. While these pioneering efforts laid essential groundwork, they also highlighted the persistent difficulty in achieving comprehensive, scalable, and fully automated diagnostic evaluation, thereby motivating the development of more advanced benchmarks and sophisticated detection methodologies in subsequent research.