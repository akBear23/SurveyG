\subsection{Summary of Key Developments and Trajectory}

Research into Large Language Model (LLM) hallucination has undergone a significant intellectual evolution, progressing from initial problem definition and empirical detection to sophisticated mitigation strategies, and ultimately, to a theoretical understanding of its inherent inevitability. This trajectory reflects a shift in the community's perspective from aiming for complete eradication to focusing on effective management and robust deployment.

The foundational understanding of hallucination began with early efforts to define and categorize the problem. \cite{maynez2020h3q} pioneered a systematic human evaluation, distinguishing between intrinsic (contradicting source) and extrinsic (adding unsupportable facts) hallucinations in abstractive summarization, highlighting the inadequacy of traditional metrics. Building on this, comprehensive surveys like \cite{zhang2023k1j} and \cite{ye2023yom} expanded these taxonomies to encompass input-conflicting, context-conflicting, and fact-conflicting hallucinations, emphasizing the unique challenges posed by LLMs' scale and versatility. \cite{rawte2023ao8} further refined this with a fine-grained categorization based on degree, orientation, and specific types, while \cite{du2023qu7} introduced association analysis to quantify and attribute hallucinations to specific model capability deficiencies. The economic and trustworthiness implications of poor information quality were underscored by \cite{rejeleene2024okw}. This foundational work culminated in the profound theoretical insight from \cite{xu2024n76}, which, using a diagonalization argument, provided a formal proof that hallucination is an inherent and inevitable limitation for all computable LLMs. This theoretical grounding fundamentally shifted the paradigm, suggesting that complete elimination is impossible and redirecting research towards effective management.

With the problem characterized and its inevitability understood, research intensified on robust detection mechanisms. Early efforts, such as \cite{liu2021mo6}'s HADES dataset, focused on token-level, reference-free hallucination detection for free-form text generation, providing fine-grained signals. Addressing the challenge of black-box models, \cite{manakul20236ex} introduced SelfCheckGPT, a zero-resource method that leverages the consistency of stochastically sampled LLM outputs to identify non-factual statements. However, SelfCheckGPT sometimes struggled with LLMs generating identical hallucinated samples, a limitation addressed by \cite{yang20251dw}'s MetaQA, which utilized metamorphic relations and prompt mutation for more robust self-contained detection. The creation of specialized benchmarks became crucial for rigorous evaluation. \cite{cao2023ecl} developed AutoHall for automated, model-specific hallucination dataset generation, reducing manual annotation costs. For Retrieval-Augmented Generation (RAG) systems, \cite{chen2023h04} introduced the RGB benchmark to diagnose specific RAG capabilities like noise robustness and negative rejection, revealing persistent challenges. Dialogue-level hallucinations, a complex multi-turn phenomenon, were addressed by \cite{chen2024c4k}'s DiaHalu benchmark, while \cite{vu202337s} introduced FreshLLMs for evaluating LLMs on rapidly changing world knowledge with search engine augmentation. More recently, \cite{oh2024xa3}'s ERBench leveraged relational databases for automatically verifiable rationales, and \cite{liang20236sh}'s UHGEval provided an unconstrained generation benchmark for Chinese LLMs. The quality of these benchmarks themselves became a focus, with \cite{yan2024ux8} proposing a framework to evaluate their reliability and validity. A comprehensive empirical study by \cite{li2024qrj} further analyzed factuality hallucination across the entire LLM lifecycle, from pre-training to inference.

Concurrently with detection, diverse mitigation strategies emerged, broadly categorized into external grounding and internal self-correction. For text-based LLMs, external grounding became a prominent approach. \cite{yao20229uz} introduced ReAct, a seminal paradigm interleaving verbal reasoning ("thoughts") with task-specific actions (e.g., API calls) to ground LLM responses in external environments. This was further refined by \cite{trivedi2022qsf}'s IRCoT, which dynamically used intermediate Chain-of-Thought steps as queries for iterative knowledge retrieval, significantly reducing factual errors. To enhance trustworthiness, \cite{gao2023ht7} developed ALCE, a benchmark and evaluation framework for enabling LLMs to generate text with verifiable citations. Knowledge graph integration also gained traction, with \cite{wen2023t6v}'s MindMap enabling LLMs to reason over structured graphical inputs, and \cite{sui20242u1}'s OKGQA benchmark assessing KG-augmented LLMs in open-ended QA. Earlier, \cite{dziri2021bw9}'s Neural Path Hunter used KGs for post-hoc entity correction in dialogue systems, and \cite{li2023v3v}'s Chain-of-Knowledge dynamically adapted knowledge from heterogeneous sources. Internal self-correction mechanisms also advanced, with \cite{dhuliawala2023rqn}'s Chain-of-Verification enabling LLMs to self-critique and correct factual claims by generating independent verification questions. \cite{ji2023vhv} further explored iterative self-reflection for high-stakes domains like medical QA. Novel decoding strategies also emerged, such as \cite{zhang202396g}'s Induce-then-Contrast Decoding, which leveraged "induced hallucinations" to improve factuality without retraining. More dynamic RAG approaches, like \cite{su2024gnz}'s DRAD, integrated real-time hallucination detection to conditionally trigger retrieval, while \cite{lv2024k5x}'s COFT used coarse-to-fine highlighting to reduce hallucination in long contexts. Comprehensive surveys like \cite{tonmoy20244e4}, \cite{pan2023mwu}, \cite{pan2024y3a}, and \cite{liu2024gxh} have categorized these diverse mitigation techniques, from prompt engineering to model development.

As LLMs evolved into multimodal models, new and complex forms of hallucination emerged, necessitating tailored approaches. \cite{li2023249} conducted the first systematic empirical study of "object hallucination" in Large Vision-Language Models (LVLMs), proposing POPE as a more stable evaluation method and identifying object frequency as a key driver. This built on earlier work by \cite{dai20229aa} that probed object hallucination in VLP models and proposed ObjMLM loss. The field rapidly developed specialized benchmarks to characterize these multimodal failures: \cite{kuan20249pm} introduced ECHO/Cover for object hallucination in audio-language models, \cite{wu2024bxt}'s R-Bench focused on relationship hallucinations, and \cite{sungbin2024r2g}'s AVHBench addressed cross-modal hallucinations in audio-visual LLMs. Medical hallucinations were specifically targeted by \cite{chen2024hfe}'s Med-HallMark, while \cite{zheng20246fk}'s Reefknot provided a comprehensive benchmark for relation hallucinations. More complex forms include entangled language hallucination and visual illusion (\cite{guan2023z15}'s Hallusionbench), multi-object hallucination (\cite{chen2024vy7}'s ROPE), temporal hallucinations in video (\cite{li2024wyb}'s VidHalluc), and joint cross-lingual and cross-modal hallucinations (\cite{zhang2025pex}'s CCHall). \cite{jiang2024792}'s Hal-Eval introduced "event hallucination" for a universal, fine-grained evaluation. Unified detection frameworks like \cite{chen2024lc5}'s UNIHD emerged to cover various tasks and hallucination types. Mitigation strategies for multimodal models also diversified. Training-based approaches included \cite{liu2023882}'s LRV-Instruction dataset with negative instructions for robust instruction tuning, \cite{wang2023ubf}'s ReCaption for fine-grained hallucination mitigation via caption rewrites, \cite{wu2024n00}'s NoiseBoost for alleviating hallucination with noise perturbation, and \cite{xiao2024hv1}'s fine-grained AI feedback for severity-aware DPO. \cite{deng202405j} proposed a model-level judge-free self-improvement framework to reduce computational costs. Inference-time and post-hoc methods also proliferated: \cite{yin2023hx3}'s Woodpecker offered a training-free pipeline for post-generation correction, \cite{chang2024u3t}'s "Dentist" framework adaptively applied visual verification or Chain-of-Thought based on query type, and \cite{yin2025s2b}'s ClearSight enhanced visual signals to mitigate object hallucination. Other methods intervened in attention mechanisms or hidden states, such as \cite{zou2024dp7}'s Memory-space Visual Retracing, \cite{zhou2024lvp}'s CAUSAL MM for deciphering attention causality, and \cite{chen2024j0g}'s ICT for cross-level trusted intervention. Cognitive-inspired approaches like \cite{kim2024ozf}'s Counterfactual Inception prompted models to think counterfactually, while \cite{qu2024pqc}'s MVP used multi-view multi-path reasoning. The complex phenomenon of multimodal hallucination snowballing was investigated and mitigated by \cite{zhong2024mfi}. Diagnostic tools like \cite{wang2025jen}'s hallucination attack, which exploits attention sinks, also emerged to expose vulnerabilities. These developments are comprehensively surveyed by \cite{liu2024sn3}.

The overall trajectory of LLM hallucination research reflects a mature understanding of its complexity. While initial efforts focused on defining and detecting the problem, the theoretical proof of its inevitability by \cite{xu2024n76} marked a pivotal shift. The field has since moved towards sophisticated management strategies, including external grounding through retrieval and knowledge graphs, internal self-correction via reasoning and verification, and specialized multimodal approaches. Despite significant progress in detection and mitigation, challenges remain in achieving complete robustness, particularly in handling dynamic, multi-modal, and conversational contexts. Future directions will likely focus on more adaptive, real-time, and resource-efficient solutions that can seamlessly integrate diverse knowledge sources and self-correction mechanisms, continually refining the balance between model creativity and factual fidelity for trustworthy AI deployment.