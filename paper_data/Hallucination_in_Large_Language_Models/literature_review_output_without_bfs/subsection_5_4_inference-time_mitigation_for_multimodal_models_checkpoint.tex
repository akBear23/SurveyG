\subsection{Inference-Time Mitigation for Multimodal Models}

Inference-time mitigation strategies for multimodal models focus on correcting or preventing hallucinations during the generation phase or as post-processing steps, crucially without altering the core model parameters. These methods offer a flexible and often computationally efficient alternative to extensive retraining, aiming to enhance visual grounding and balance modality priors efficiently during generation.

Early approaches to post-hoc correction, such as \textit{Woodpecker} \cite{yin2023hx3}, established a foundational pipeline for hallucination correction. This framework leverages external expert models, including an open-set object detector (Grounding DINO) for object existence and counts, and a pre-trained VQA model (BLIP-2-FlanT5 XXL) for attribute verification, alongside a Large Language Model (LLM) like GPT-3.5 for key concept extraction, question formulation, and final response correction. While effective in boosting factual accuracy, \textit{Woodpecker}'s fixed, multi-stage pipeline and reliance on multiple external models presented limitations in adaptability to diverse query types.

Addressing this, \textit{Dentist} \cite{chang2024u3t} introduced a unified, adaptive framework that first classifies the query type (e.g., perception or reasoning) and then applies a tailored mitigation strategy within an iterative validation loop. For perception queries, it employs visual verification with sub-questions, while for reasoning queries, it uses Chain-of-Thought (CoT) prompting, all orchestrated by an external LLM (ChatGPT). This adaptive approach improves upon fixed pipelines by dynamically responding to the nature of the user's request, although it still depends heavily on the external LLM for classification and iterative refinement.

Moving beyond external verification, several methods focus on intervening directly within the model's internal mechanisms during inference to improve visual grounding and balance modality priors. \textit{ClearSight} \cite{yin2025s2b} proposes Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals specifically within the model's middle layers, where modality fusion predominantly occurs. This method argues that language bias stems from insufficient visual attention, and by amplifying visual features, it mitigates object hallucinations without the quality degradation or inference slowdown often associated with contrastive decoding. Similarly, \textit{CAUSAL MM} \cite{zhou2024lvp} employs a principled causal inference framework, applying structural causal modeling and counterfactual reasoning to attention mechanisms. By treating modality priors as confounding factors, it systematically balances visual and language attention to produce outputs more aligned with multimodal inputs, offering a deeper, more theoretical approach to modality balance.

Further refining internal interventions, \textit{ICT} (Image-Object Cross-Level Trusted Intervention) \cite{chen2024j0g} operates during the forward pass to mitigate object hallucination. It calculates an "intervention direction" to shift the model's focus towards both overall image information and fine-grained object details by identifying and adjusting specific attention heads using trusted and untrusted data pairs. This method avoids additional inference latency and aims to preserve beneficial language priors, a key distinction from some contrastive decoding techniques. Complementing these, \textit{MemVR} (Memory-Space Visual Retracing) \cite{zou2024dp7} draws inspiration from human cognition's "look-twice" mechanism. It re-injects visual tokens as supplementary evidence into the model's Feed Forward Networks (FFNs) at intermediate layers, dynamically activating this process when the model exhibits high uncertainty. This aims to counteract the model's "amnesia" about visual information, enhancing factual alignment efficiently.

Other inference-time strategies involve sophisticated decoding and prompting techniques. \textit{Counterfactual Inception} \cite{kim2024ozf} introduces a training-free method that prompts Large Multi-modal Models (LMMs) to self-generate "counterfactual keywords" (e.g., non-existent objects or attributes) and then explicitly instructs the model to avoid these in its response. A Plausibility Verification Process (PVP) using CLIP scores filters these keywords, ensuring they are meaningful yet sufficiently counterfactual to guide the model's reasoning. This method leverages the LMM's own generative capabilities for self-correction. In a different vein, \textit{ConVis} \cite{park20247cm} employs a Text-to-Image (T2I) model for "hallucination visualization" during contrastive decoding. It generates an image from the MLLM's initial caption, and then uses the discrepancies between this T2I-generated image and the original input to penalize hallucinated tokens during the final decoding step, offering a novel visual feedback loop.

Finally, the \textit{MVP} (Multi-View Multi-Path Reasoning) framework \cite{qu2024pqc} provides a training-free and tool-free solution (within the LVLM itself) to alleviate hallucinations. It enhances visual understanding by generating multi-view captions (top-down, regular, bottom-up) from the LVLM, enriching the global visual context. Subsequently, it employs a "certainty-driven reasoning" mechanism that quantifies the certainty of answer tokens across multiple decoding paths and information views, selecting the most reliable output. This approach maximizes the innate capabilities of existing LVLMs without external tools or additional training.

Despite the advancements, inference-time mitigation still faces challenges. A common limitation is the reliance on external LLMs or specialized tools, which can introduce dependencies, computational overhead, or new biases. Furthermore, the effectiveness of internal interventions is often tied to specific model architectures or internal behaviors (e.g., attention patterns, uncertainty scores). Future directions will likely focus on developing more robust, truly architecture-agnostic solutions that can efficiently balance modality priors and improve visual grounding without compromising other aspects of generation, such as fluency and diversity.