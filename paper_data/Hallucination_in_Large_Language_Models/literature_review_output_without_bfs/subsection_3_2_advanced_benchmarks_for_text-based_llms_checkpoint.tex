\subsection*{Advanced Benchmarks for Text-based LLMs}

The escalating prevalence of hallucination in Large Language Models (LLMs) has necessitated a paradigm shift from rudimentary accuracy metrics to sophisticated, diagnostic benchmarks capable of providing comprehensive and nuanced evaluations across diverse generation scenarios. Early efforts, such as the intrinsic and extrinsic categories identified by \cite{maynez2020h3q} in abstractive summarization, underscored the limitations of traditional, reference-dependent metrics, propelling the development of more semantically aware and fine-grained evaluation frameworks. As \cite{zhang2023k1j} highlights in their comprehensive survey, the unique challenges of LLM hallucination, particularly fact-conflicting types, demand benchmarks that can probe deeper into factual consistency and reasoning processes.

A significant trajectory in advanced benchmark design focuses on **enhancing factual grounding and verifiability**. Building on the need for verifiable outputs, \cite{gao2023ht7} introduced ALCE (Automatic LLMs’ Citation Evaluation), a reproducible benchmark for assessing LLMs' ability to generate text with verifiable citations. ALCE employs novel NLI-based metrics for citation recall and precision, demonstrating a strong correlation with human judgments and offering a scalable method to evaluate whether generated statements are fully supported by their cited passages. Complementing this, \cite{oh2024xa3} proposed ERBench, an Entity-Relationship based benchmark that leverages existing relational databases (RDBs) and their integrity constraints (Functional Dependencies and Foreign Key Constraints). Unlike ALCE, which focuses on natural language inference for citation support, ERBench automatically constructs complex multi-hop questions and, crucially, verifies not only the LLM's answer but also its underlying rationale by checking for critical keywords inferred from the database schema. This provides a highly automated, extensible, and fine-grained method for evaluating factual hallucination and reasoning processes against structured knowledge. Further extending the assessment of reasoning, \cite{ghosh2024tj5} introduced novel datasets (FreebaseLFC, NELLLFC, WikiLFC) and quantitative measures to evaluate the *logical consistency* of LLMs in fact-checking tasks, particularly for propositional logic queries derived from Knowledge Graphs. This work moves beyond simple factual accuracy to rigorously test an LLM's adherence to logical rules, a critical aspect of trustworthy reasoning.

The rise of Retrieval-Augmented Generation (RAG) as a primary mitigation strategy has spurred the creation of **diagnostic benchmarks specifically for RAG capabilities**. \cite{chen2023h04} developed RGB (Retrieval-Augmented Generation Benchmark), a multi-lingual corpus designed to diagnose four fundamental RAG abilities: Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness. By constructing instances from *latest news information* and systematically introducing noise or conflicting facts, RGB provides a granular diagnostic tool to understand how LLMs interact with and process external knowledge, revealing their struggles with rejecting irrelevant information or synthesizing facts from multiple documents. Extending this diagnostic approach, \cite{zhang2024o58} introduced a benchmarking procedure to evaluate LLMs' ability to make *utility judgments*—that is, to identify passages with genuine utility in supporting question answering within RAG contexts, distinguishing utility from mere relevance. This highlights a nuanced aspect of RAG performance often overlooked by simpler benchmarks.

Beyond static factuality, benchmarks are evolving to address **dynamic, evolving, and context-specific knowledge**. Recognising that LLMs struggle with rapidly changing information, \cite{vu202337s} introduced FRESH QA, a dynamic benchmark focusing on current, fast-changing, and false-premise knowledge. This benchmark employs a rigorous two-mode (RELAXED and STRICT) human evaluation protocol to capture nuanced aspects of factuality and hallucination in an ever-changing information landscape. For conversational AI, \cite{chen2024c4k} developed DiaHalu, the first dedicated dialogue-level hallucination evaluation benchmark. DiaHalu features a comprehensive taxonomy of hallucination (including non-factual, incoherence, irrelevance, overreliance, and reasoning errors) across multiple dialogue domains, simulating natural human-machine interactions to capture complex, multi-turn hallucination patterns. Addressing language-specific challenges and the bias of constrained generation, \cite{liang20236sh} proposed UHGEval, an Unconstrained Hallucination Generation benchmark for Chinese LLMs, which uses multiple LLMs for generating unconstrained text continuations and introduces a novel `kwPrec` metric for factual relevance, offering a more realistic assessment of hallucination in real-world usage. Furthermore, \cite{xie20247zk} introduced a novel benchmark method that assesses LLM consistency by comparing responses generated through different *reasoning orders*, revealing instances where LLMs fabricate answers and subsequently generate justifications. This diagnostic approach sheds light on the internal reasoning processes and their impact on factual output.

While many advanced benchmarks still rely on human annotation, there is a growing trend towards **leveraging LLMs themselves for automated evaluation**. For instance, \cite{li2024qrj} presented HaluEval 2.0, a comprehensive benchmark for factuality hallucination, alongside a simple yet effective LLM-based framework for automatic detection. However, this approach raises critical questions about potential circularity or inherent biases when using LLMs to evaluate other LLMs. As theoretically explored by \cite{karbasi2025j7n}, automated hallucination detection, akin to language identification, is fundamentally challenging for most language collections if the detector is trained solely on correct examples. Their work highlights that the use of expert-labeled *negative examples* is crucial for making automated detection feasible, providing a theoretical underpinning for the need for diverse and robust benchmark construction that includes explicit instances of hallucination.

In conclusion, the evolution of benchmarks for text-based LLMs demonstrates a clear progression towards more comprehensive, diagnostic, and context-aware evaluations. From citation-based verification and structured knowledge grounding to specialized RAG diagnostics and dynamic conversational contexts, these benchmarks offer increasingly fine-grained insights into LLM reliability. However, challenges persist, particularly concerning the scalability of creating diverse, high-quality negative examples, the potential biases of LLM-as-evaluators, and the continuous need for benchmarks that can adapt to the dynamic nature of knowledge and the evolving capabilities and failure modes of LLMs. Future research must continue to focus on creating benchmarks that are not only scalable and fine-grained but also robust to these inherent complexities, ultimately fostering more trustworthy and reliable AI systems.