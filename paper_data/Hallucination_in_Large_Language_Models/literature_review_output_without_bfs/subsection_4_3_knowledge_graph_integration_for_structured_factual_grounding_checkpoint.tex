\subsection{Knowledge Graph Integration for Structured Factual Grounding}

Large Language Models (LLMs) frequently generate plausible but factually incorrect information, a phenomenon known as hallucination, which severely undermines their trustworthiness and utility \cite{zhang2023k1j, du2023qu7, ye2023yom}. To mitigate this, Knowledge Graphs (KGs) offer a powerful solution by providing structured, verifiable factual knowledge that can explicitly ground LLM generations. This approach leverages the explicit relational information within KGs to enhance the factual consistency and reliability of LLM outputs, particularly in complex tasks like open-ended question answering and dialogue systems.

One prominent strategy for integrating KGs involves sophisticated prompting techniques that enable LLMs to reason over structured graphical inputs. MindMap \cite{wen2023t6v} introduces a novel pipeline that allows LLMs to comprehend and aggregate evidence sub-graphs from KGs, converting them into a natural language reasoning graph. This method goes beyond simple retrieval-augmentation by prompting LLMs to build an internal "mind map" for synergistic inference, combining explicit KG facts with their implicit knowledge to generate more transparent and factually grounded answers. This addresses the limitation of earlier KG retrieval methods that often treated structured knowledge as plain text, making responses harder to validate and more prone to hallucination.

Complementing sophisticated prompting, dynamic knowledge adapting over heterogeneous sources further strengthens factual grounding. Chain-of-Knowledge (CoK) \cite{li2023v3v} proposes a three-stage framework that dynamically incorporates grounding information from diverse knowledge sources, including both unstructured (e.g., Wikipedia) and structured (e.g., Wikidata, tables) data. A key innovation is its Adaptive Query Generator (AQG), which can generate queries tailored to each source's specific language (e.g., SPARQL for KGs, SQL for tables), and a progressive rationale correction mechanism that prevents error propagation by sequentially rectifying reasoning steps. This approach overcomes the limitations of fixed knowledge sources and enhances factual accuracy across various knowledge-intensive tasks.

The efficacy of KG integration is particularly evident in complex applications like open-ended question answering. An empirical study by \cite{sui20242u1} introduces OKGQA, a benchmark specifically designed for evaluating LLMs augmented with KGs in open-ended, real-world QA scenarios, and OKGQA-P to assess robustness under perturbed KGs. Their proposed RAG-like framework, featuring "Graph-guided retrieval" with a prize-cost strategy for efficient subgraph extraction, demonstrates that integrating KG information significantly reduces factual errors, even when the KG itself contains imperfections. This highlights the practical utility of KGs in making LLMs more trustworthy for nuanced, paragraph-long answers.

Similarly, in dialogue systems, KGs play a pivotal role in mitigating hallucinations. NEURAL PATH HUNTER (NPH) \cite{dziri2021bw9} employs a generate-then-refine strategy to amend generated responses using a KG. It features a token-level hallucination critic that identifies and masks erroneous entity mentions, followed by an entity mention retriever that queries a local k-hop subgraph of the KG to retrieve factually correct entities. This method effectively addresses the "source-reference divergence problem" in dialogue systems, where training on reference data alone often fails to guarantee faithfulness to auxiliary knowledge, providing a concrete mechanism for real-time factual correction in conversational AI.

These KG integration strategies fall under the broader category of "external knowledge grounding" for hallucination mitigation, as comprehensively surveyed by \cite{tonmoy20244e4} and \cite{pan2024y3a}. They offer a crucial counterpoint to methods relying solely on internal model mechanisms, such as self-correction \cite{dhuliawala2023rqn, ji2023vhv} or induced hallucinations \cite{zhang202396g}, by providing an explicit, verifiable source of truth. The inherent inevitability of hallucination in LLMs, as theoretically proven by \cite{xu2024n76}, underscores the critical need for such robust external grounding mechanisms. Furthermore, benchmarks like RGB \cite{chen2023h04} reveal that LLMs often struggle with noise robustness, negative rejection, and information integration in Retrieval-Augmented Generation (RAG) settings, emphasizing the value of structured, high-quality knowledge from KGs.

Despite the significant advancements, challenges remain. The scalability and maintenance of KGs, especially for rapidly evolving or highly specialized domains, continue to be a concern. Optimal fusion strategies that seamlessly blend the implicit knowledge and reasoning capabilities of LLMs with the explicit, structured facts from KGs are still an active area of research. Future work will likely focus on developing more adaptive and dynamic KG construction and integration techniques, alongside robust evaluation benchmarks that can precisely measure the impact of KG grounding on various types of hallucinations across diverse applications.