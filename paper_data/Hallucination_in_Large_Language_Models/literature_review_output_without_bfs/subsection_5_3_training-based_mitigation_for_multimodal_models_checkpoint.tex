\subsection{Training-Based Mitigation for Multimodal Models}

Proactive strategies to reduce multimodal hallucinations focus on modifying the model's training process, aiming to instill robustness directly into its learned parameters. This approach seeks to build more factually consistent Multimodal Large Language Models (MLLMs) from the ground up, rather than relying solely on post-hoc corrections.

An early investigation into foundational training objectives by \cite{dai20229aa} revealed that standard Vision-Language Pre-training (VLP) can inadvertently increase object hallucination, particularly when optimizing for metrics like CIDEr. To counter this, they proposed ObjMLM (Object-Masked Language Modeling), a novel pre-training objective that enhances token-level image-text alignment and controlled generation to reduce object hallucination by up to 17.4\%. Building on the idea of improving visual grounding through data, \cite{wang2023ubf} tackled fine-grained hallucinations (incorrect attributes or behaviors) by fine-tuning Large Vision-Language Models (LVLMs) with carefully rewritten captions. Their ReCaption framework leverages ChatGPT to generate diverse, high-quality captions, enriching the training data to improve fine-grained visual-text alignment and reduce hallucination.

Moving towards more efficient and scalable data generation for training, \cite{deng202405j} introduced a "judge-free" self-improvement framework for MLLMs. This approach generates controllable negative samples by blending conditional and unconditional decoding paths, and then uses a lightweight CLIP model for objective, average sentence-level verification and preference data inversion for Direct Preference Optimization (DPO). This method significantly reduces the computational costs and biases associated with relying on MLLMs as judges for feedback. Further advancing preference learning, \cite{xiao2024hv1} developed Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO), which incorporates fine-grained AI feedback (including hallucination type, severity, and rationale) into the DPO objective. Their "detect-then-rewrite" pipeline for data generation enables severity-aware mitigation, prioritizing the reduction of critical hallucinations and achieving state-of-the-art performance in reducing hallucination rates.

Complementing these data-centric and preference-learning approaches, \cite{wu2024n00} proposed NoiseBoost, a generalizable technique that injects Gaussian noise into projected visual tokens during various training stages, including Supervised Fine-tuning (SFT), Reinforcement Learning (RL), and Semi-Supervised Learning (SSL). This noise perturbation increases the "hardship" in visual understanding, compelling the MLLM to distribute its attention more evenly between visual and linguistic tokens, thereby reducing over-reliance on language priors and improving hallucination resistance with negligible additional training costs.

Collectively, these training-based mitigation strategies demonstrate a clear progression from modifying fundamental pre-training objectives to sophisticated fine-tuning with curated data, efficient self-improvement via DPO, advanced Reinforcement Learning from AI Feedback (RLAIF), and novel regularization techniques like noise perturbation. While these methods aim to embed robustness directly into the model's parameters, they often face challenges related to the scalability of high-quality data, the computational expense of extensive retraining, and the potential for new biases introduced by synthetic data or reward models. Future research must continue to explore more efficient and robust ways to integrate factual consistency and visual grounding into the very fabric of MLLM learning.