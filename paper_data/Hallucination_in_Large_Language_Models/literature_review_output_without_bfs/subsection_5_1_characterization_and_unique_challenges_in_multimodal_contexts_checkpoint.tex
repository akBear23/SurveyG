\subsection{Characterization and Unique Challenges in Multimodal Contexts}

Hallucination in Multimodal Large Language Models (MMLMs) presents a critical challenge, manifesting as inconsistencies between generated textual content and diverse sensory inputs such as visual, audio, or video data. Unlike their unimodal counterparts, MMLMs face the complex task of accurately grounding language in heterogeneous modalities and maintaining cross-modal consistency, leading to distinct forms of factual errors.

Early investigations into visual hallucination in Vision-Language Pre-trained (VLP) models, such as that by \cite{dai20229aa}, revealed that even models optimized for standard metrics could exhibit high rates of object hallucination, where non-existent objects are described. Building on this, \cite{li2023249} provided the first systematic empirical study of "object hallucination" in Large Vision-Language Models (LVLMs), introducing the Polling-based Object Probing Evaluation (POPE) to robustly assess the generation of non-existent objects. Their work crucially identified object frequency and co-occurrence in training data as significant drivers of this phenomenon. Extending this, \cite{chen2024vy7} further characterized the more complex problem of "multi-object hallucination," demonstrating that LVLMs struggle significantly more when recognizing and reasoning about multiple objects simultaneously, often inventing objects or being misled by spurious correlations.

Beyond individual objects, MMLMs also grapple with misrepresenting the interactions between entities. \cite{wu2024bxt} systematically evaluated and analyzed "relationship hallucinations" in LVLMs, where models inaccurately describe inter-object relationships within an image. They introduced R-Bench, a novel benchmark designed to prevent data leakage and provide fine-grained evaluation of these more complex inconsistencies. Further refining this understanding, \cite{zheng20246fk} proposed Reefknot, a comprehensive benchmark that categorizes relation hallucinations into "perceptive" (concrete spatial relations) and "cognitive" (abstract interactions), revealing that MLLMs are surprisingly more susceptible to perceptive errors.

The temporal dimension introduces further complexities, particularly for video-language models. \cite{li2024wyb} characterized "temporal hallucinations" in MMLMs for video understanding, where models inaccurately narrate actions, temporal sequences, or scene transitions. Their VIDHALLUC benchmark, the largest of its kind, specifically targets these dynamic inconsistencies by curating adversarial video pairs. Complementing this, \cite{wang2024rta} introduced VideoHallucer, a comprehensive benchmark for evaluating both "intrinsic" (contradicting video content) and "extrinsic" (unverifiable from video) hallucinations in Large Video-Language Models, including temporal aspects, further highlighting the challenges of dynamic grounding.

The problem extends beyond the visual domain. \cite{kuan20249pm} investigated "object hallucination" in Large Audio-Language Models (LALMs), demonstrating that these models also generate or affirm the presence of non-existent objects in audio inputs. Their work, introducing ECHO and Cover metrics, revealed a critical discrepancy where LALMs perform well on generative tasks (audio captioning) but struggle with discriminative queries, indicating a weakness in understanding the *nature* of discriminative questions rather than a fundamental inability to process audio. This highlights the unique challenge of grounding language in auditory inputs and the model's struggle with precise factual verification across modalities.

The inherent difficulty in maintaining cross-modal consistency leads to more entangled forms of hallucination. \cite{sungbin2024r2g} introduced AVHBench, the first benchmark for "cross-modal hallucinations" in Audio-Visual Large Language Models, where misinterpretations arise from the interaction between audio and visual signals, such as perceiving imaginary sounds from visual cues or fake visual events from audio cues. This underscores how MMLMs can over-rely on one modality or misinterpret subtle inter-modal relationships. \cite{guan2023z15} further explored this entanglement with Hallusionbench, a diagnostic suite for "language hallucination and visual illusion," revealing that LVLMs often exhibit a pronounced language bias, where parametric knowledge conflicts with visual context, leading to overconfident yet erroneous assertions.

Moreover, these inconsistencies can propagate in interactive settings. \cite{zhong2024mfi} identified "multimodal hallucination snowballing," demonstrating that initial hallucinations can influence an LVLM's subsequent generations, leading to further incorrect responses even when ground visual information is available. This phenomenon poses a significant challenge for conversational MMLMs. The problem is exacerbated in specialized domains, as shown by \cite{chen2024hfe}, who characterized "medical hallucinations" in LVLMs, highlighting the critical need for fine-grained, clinically relevant evaluation due to the severe consequences of factual errors in healthcare. The ultimate challenge in cross-modal consistency is presented by \cite{zhang2025pex}, who introduced CCHall to evaluate "joint cross-lingual and cross-modal hallucinations," where MLLMs must align visual content with text across multiple languages, leading to significantly increased hallucination rates in these complex, real-world scenarios.

The pervasive nature and diverse manifestations of hallucination in multimodal contexts underscore the fundamental challenge of robustly grounding language in rich, heterogeneous sensory inputs. While significant progress has been made in characterizing specific hallucination types across modalities, the field continues to grapple with the inherent difficulty of achieving perfect cross-modal consistency and preventing the propagation of subtle inconsistencies. The meta-evaluation by \cite{yan2024ux8} further highlights that even the benchmarks designed to characterize these problems often suffer from reliability and validity issues, indicating that the very tools for understanding these challenges require continuous refinement. Therefore, future research must focus on more holistic and dynamic evaluation frameworks that account for the intricate interplay of modalities and the cascading effects of errors, moving towards MMLMs that are not only capable but also consistently trustworthy.