\subsection*{Initial Recognition and Challenges in Defining Hallucination}

The advent of Large Language Models (LLMs) marked a significant leap in natural language generation, demonstrating unprecedented fluency and coherence. However, this impressive capability was quickly accompanied by a pervasive and problematic tendency: the generation of plausible-sounding but factually incorrect, inconsistent, or nonsensical information. This phenomenon, colloquially termed "hallucination," rapidly emerged as a critical challenge, fundamentally undermining the reliability and trustworthiness of LLM outputs across nascent applications. Early efforts to precisely define this elusive behavior were fraught with inherent ambiguity and context-dependency, complicating initial attempts to systematically categorize factual inconsistencies.

Initial observations and informal characterizations of hallucination often materialized in tasks requiring adherence to source material, such as abstractive summarization. Models would frequently generate content that, while grammatically sound, lacked support from the provided input document. A seminal contribution by \cite{maynez2020h3q} provided one of the first systematic human evaluations and categorizations, offering a foundational understanding of the problem. They meticulously distinguished between "intrinsic hallucinations," which misrepresent information explicitly present in the source document, and "extrinsic hallucinations," which introduce information not directly inferable from the source. Their findings were stark: over 70\% of single-sentence summaries contained hallucinations, predominantly extrinsic and often erroneous. This early work was instrumental in conceptualizing hallucination as a deviation from factual accuracy or faithfulness to the source, highlighting the inadequacy of traditional metrics like ROUGE, which primarily measure lexical overlap rather than semantic or factual correctness.

However, Maynez et al.'s work also inadvertently illuminated the profound challenges in establishing a universally precise definition. The authors noted the nuanced case of "factual extrinsic hallucinations"â€”instances where models added correct information not present in the source. While technically a deviation from faithfulness to the input, such additions could, in some contexts, be perceived as improving summary quality by providing beneficial background. This observation underscored a core definitional dilemma: is any deviation from the source a hallucination, even if factually correct and potentially helpful? This ambiguity highlighted the context-dependency of what constitutes a "hallucination," blurring the lines between harmful fabrication and potentially benevolent information enrichment.

The difficulty in defining hallucination was further exacerbated as LLMs evolved beyond strictly source-grounded tasks like summarization to more free-form, open-ended, and interactive generation. In these broader contexts, where no single source document dictates truth, the very concept of "faithfulness" became harder to apply. Without a clear ground truth reference, distinguishing between a model's creative inference, general world knowledge, or outright fabrication became a significant conceptual hurdle. The anthropomorphic term "hallucination" itself, while evocative, also presented a challenge, as it implies a cognitive process of perception without external stimulus, which does not accurately reflect the probabilistic token generation mechanisms of LLMs. This semantic debate further complicated the establishment of a neutral, universally accepted definition, moving beyond a simple factual error to encompass broader issues of coherence, consistency, and adherence to implicit user intent.

Consequently, the initial recognition of hallucination was not merely about identifying factual errors but grappling with the fundamental conceptual challenges inherent in defining truth, faithfulness, and consistency in machine-generated text. The early systematic investigations, exemplified by \cite{maynez2020h3q}, laid crucial groundwork by providing empirical evidence and initial categorizations. Yet, they simultaneously revealed the deep complexities arising from the nuanced nature of factual accuracy, the context-dependent utility of generated content, and the expanding capabilities of LLMs. These definitional struggles underscored the imperative for more rigorous taxonomies, systematic evaluation frameworks, and a deeper theoretical understanding of hallucination's origins, which would become the focus of subsequent research efforts.