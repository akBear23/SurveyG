{
    "1b387e3fbec0447c8bf2dcee21f6db59cdddf698.pdf": {
        "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
        "authors": [
            "Junyi Li",
            "Jie Chen",
            "Ruiyang Ren",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "Jian-yun Nie",
            "Ji-Rong Wen"
        ],
        "published_date": "2024",
        "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the LLM hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucination, focused on the the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet effective detection method for LLM hallucination. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucination. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs. Our code and data can be accessed at https://github.com/RUCAIBox/HaluEval-2.0.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1b387e3fbec0447c8bf2dcee21f6db59cdddf698.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "576023f7cc3da5a36ac0cfda402af859cc90be10.pdf": {
        "title": "Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models",
        "authors": [
            "Chun-Yi Kuan",
            "Wei-Ping Huang",
            "Hung-yi Lee"
        ],
        "published_date": "2024",
        "abstract": "Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/576023f7cc3da5a36ac0cfda402af859cc90be10.pdf",
        "venue": "Interspeech",
        "citationCount": 0,
        "score": 0
    },
    "27d55a944b5c02b8c10eb250773d8eb082e06476.pdf": {
        "title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback",
        "authors": [
            "Wenyi Xiao",
            "Ziwei Huang",
            "Leilei Gan",
            "Wanggui He",
            "Haoyuan Li",
            "Zhelun Yu",
            "Hao Jiang",
            "Fei Wu",
            "Linchao Zhu"
        ],
        "published_date": "2024",
        "abstract": "The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts, significantly restricting the usages of LVLMs. Most previous work detects and mitigates hallucination at the coarse-grained level or requires expensive annotation (e.g., labeling by proprietary models or human experts). To address these issues, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a hallucination detection model which can perform sentence-level hallucination detection, covering primary hallucination types (i.e., object, attribute, and relationship). Then, we propose a detect-then-rewrite pipeline to automatically construct preference dataset for training hallucination mitigating model. Furthermore, we propose differentiating the severity of hallucinations, and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating hallucination in LVLMs by incorporating the severity of hallucinations into preference learning. Extensive experiments demonstrate the effectiveness of our method.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/27d55a944b5c02b8c10eb250773d8eb082e06476.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "dbeeca8466e0c177ec67c60d529899232415ca87.pdf": {
        "title": "On Faithfulness and Factuality in Abstractive Summarization",
        "authors": [
            "Joshua Maynez",
            "Shashi Narayan",
            "Bernd Bohnet",
            "Ryan T. McDonald"
        ],
        "published_date": "2020",
        "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/dbeeca8466e0c177ec67c60d529899232415ca87.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "5e060f23914aff74d8c7b6973df44e5af8d97db5.pdf": {
        "title": "Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis",
        "authors": [
            "LI DU",
            "Yequan Wang",
            "Xingrun Xing",
            "Yiqun Ya",
            "Xiang Li",
            "Xin Jiang",
            "Xuezhi Fang"
        ],
        "published_date": "2023",
        "abstract": "Although demonstrating superb performance on various NLP tasks, large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs. To measure the level of hallucination of LLMs, previous works first categorize the hallucination according to the phenomenon similarity, then quantify the proportion that model outputs contain hallucinatory contents. However, such hallucination rates could easily be distorted by confounders. Moreover, such hallucination rates could not reflect the reasons for the hallucination, as similar hallucinatory phenomena may originate from different sources. To address these issues, we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of LLMs with a set of risk factors. In this way, we are able to observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor, meanwhile excluding the confounding effect of other factors. Additionally, by recognizing the risk factors according to a taxonomy of model capability, we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5e060f23914aff74d8c7b6973df44e5af8d97db5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "5272acad9e4201e93dabe3fd99bd7ead9b1a544d.pdf": {
        "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
        "authors": [
            "S. Tonmoy",
            "S. M. M. Zaman",
            "Vinija Jain",
            "Anku Rani",
            "Vipula Rawte",
            "Aman Chadha",
            "Amitava Das"
        ],
        "published_date": "2024",
        "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5272acad9e4201e93dabe3fd99bd7ead9b1a544d.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "2126e045f81b831da34c185e2b51a49194bf4aa4.pdf": {
        "title": "NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models",
        "authors": [
            "Kai Wu",
            "Boyuan Jiang",
            "Zhengkai Jiang",
            "Qingdong He",
            "Donghao Luo",
            "Shengzhi Wang",
            "Qingwen Liu",
            "Chengjie Wang"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) contribute a powerful mechanism to understanding visual information building on large language models. However, MLLMs are notorious for suffering from hallucinations, especially when generating lengthy, detailed descriptions for images. Our analysis reveals that hallucinations stem from the inherent summarization mechanism of large language models, leading to excessive dependence on linguistic tokens while neglecting vision information. In this paper, we propose NoiseBoost, a broadly applicable and simple method for alleviating hallucinations for MLLMs through the integration of noise feature perturbations. Noise perturbation acts as a regularizer, facilitating a balanced distribution of attention weights among visual and linguistic tokens. Despite its simplicity, NoiseBoost consistently enhances the performance of MLLMs across common training strategies, including supervised fine-tuning and reinforcement learning. Further, NoiseBoost pioneerly enables semi-supervised learning for MLLMs, unleashing the power of unlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves dense caption accuracy by 8.1% with human evaluation and achieves comparable results with 50% of the data by mining unlabeled data. Code and models are available at https://kaiwu5.github.io/noiseboost.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/2126e045f81b831da34c185e2b51a49194bf4aa4.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "7b181a867f243d83ed0731201b69a82e038feea3.pdf": {
        "title": "Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models",
        "authors": [
            "Ming-Kuan Wu",
            "Jiayi Ji",
            "Oucheng Huang",
            "Jiale Li",
            "Yuhang Wu",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "published_date": "2024",
        "abstract": "The issue of hallucinations is a prevalent concern in existing Large Vision-Language Models (LVLMs). Previous efforts have primarily focused on investigating object hallucinations, which can be easily alleviated by introducing object detectors. However, these efforts neglect hallucinations in inter-object relationships, which is essential for visual comprehension. In this work, we introduce R-Bench, a novel benchmark for evaluating Vision Relationship Hallucination. R-Bench features image-level questions that focus on the existence of relationships and instance-level questions that assess local visual comprehension. We identify three types of relationship co-occurrences that lead to hallucinations: relationship-relationship, subject-relationship, and relationship-object. The visual instruction tuning dataset's long-tail distribution significantly impacts LVLMs' understanding of visual relationships. Furthermore, our analysis reveals that current LVLMs tend to disregard visual content and overly rely on the common sense knowledge of Large Language Models. They also struggle with reasoning about spatial relationships based on contextual information.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/7b181a867f243d83ed0731201b69a82e038feea3.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0
    },
    "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365.pdf": {
        "title": "Enabling Large Language Models to Generate Text with Citations",
        "authors": [
            "Tianyu Gao",
            "Howard Yen",
            "Jiatong Yu",
            "Danqi Chen"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "9b05e1dfd158c307b74298df3d4608b93d2060a7.pdf": {
        "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models",
        "authors": [
            "Kim Sung-Bin",
            "Oh Hyun-Bin",
            "JungMok Lee",
            "Arda Senocak",
            "Joon Son Chung",
            "Tae-Hyun Oh"
        ],
        "published_date": "2024",
        "abstract": "Following the success of Large Language Models (LLMs), expanding their boundaries to new modalities represents a significant paradigm shift in multimodal understanding. Human perception is inherently multimodal, relying not only on text but also on auditory and visual cues for a complete understanding of the world. In recognition of this fact, audio-visual LLMs have recently emerged. Despite promising developments, the lack of dedicated benchmarks poses challenges for understanding and evaluating models. In this work, we show that audio-visual LLMs struggle to discern subtle relationships between audio and visual signals, leading to hallucinations and highlighting the need for reliable benchmarks. To address this, we introduce AVHBench, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs. Our benchmark includes tests for assessing hallucinations, as well as the cross-modal matching and reasoning abilities of these models. Our results reveal that most existing audio-visual LLMs struggle with hallucinations caused by cross-interactions between modalities, due to their limited capacity to perceive complex multimodal signals and their relationships. Additionally, we demonstrate that simple training with our AVHBench improves robustness of audio-visual LLMs against hallucinations. Dataset: https://github.com/kaist-ami/AVHBench",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/9b05e1dfd158c307b74298df3d4608b93d2060a7.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "5667f64b23cf48c94ff7413122bc56e5aad7e6a2.pdf": {
        "title": "Preventing and Detecting Misinformation Generated by Large Language Models",
        "authors": [
            "Aiwei Liu",
            "Qiang Sheng",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "As large language models (LLMs) become increasingly capable and widely deployed, the risk of them generating misinformation poses a critical challenge. Misinformation from LLMs can take various forms, from factual errors due to hallucination to intentionally deceptive content, and can have severe consequences in high-stakes domains.This tutorial covers comprehensive strategies to prevent and detect misinformation generated by LLMs. We first introduce the types of misinformation LLMs can produce and their root causes. We then explore two broad categories: Preventing misinformation generation: a) AI alignment training techniques to reduce LLMs' propensity for misinformation and refuse malicious instructions during model training. b) Training-free mitigation methods like prompt guardrails, retrieval-augmented generation (RAG), and decoding strategies to curb misinformation at inference time. Detecting misinformation after generation, including a) using LLMs themselves to detect misinformation through embedded knowledge or retrieval-enhanced judgments, and b) distinguishing LLM-generated text from human-written text through black-box approaches (e.g., classifiers, probability analysis) and white-box approaches (e.g., watermarking). We also discuss the challenges and limitations of detecting LLM-generated misinformation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5667f64b23cf48c94ff7413122bc56e5aad7e6a2.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0
    },
    "425d16205b28ce175c8429965a964d19b6f390c1.pdf": {
        "title": "Hallucination Detection in Large Language Models with Metamorphic Relations",
        "authors": [
            "Borui Yang",
            "Md Afif Al Mamun",
            "Jie M. Zhang",
            "Gias Uddin"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) are prone to hallucinations, e.g., factually incorrect information, in their responses. These hallucinations present challenges for LLM-based applications that demand high factual accuracy. Existing hallucination detection methods primarily depend on external resources, which can suffer from issues such as low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability. There are also methods depending on output probabilities, which are often inaccessible for closed-source LLMs like GPT models. This paper presents MetaQA, a self-contained hallucination detection approach that leverages metamorphic relation and prompt mutation. Unlike existing methods, MetaQA operates without any external resources and is compatible with both open-source and closed-source LLMs.\n \n \n \nMetaQA is based on the hypothesis that if an LLM\u2019s response is a hallucination, the designed metamorphic relations will be violated. We compare MetaQA with the state-of-the-art zero-resource hallucination detection method, SelfCheckGPT, across multiple datasets, and on two open-source and two closed-source LLMs. Our results reveal that MetaQA outperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and 0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an average F1-score of 0.435, compared to SelfCheckGPT\u2019s F1-score of 0.205, representing an improvement rate of 112.2%. MetaQA also demonstrates superiority across all different categories of questions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/425d16205b28ce175c8429965a964d19b6f390c1.pdf",
        "venue": "Proc. ACM Softw. Eng.",
        "citationCount": 0,
        "score": 0
    },
    "1c7ef42897ad2dced83ab1d58d8fbd4539f87ddc.pdf": {
        "title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models",
        "authors": [
            "Qitan Lv",
            "Jie Wang",
            "Hanzhu Chen",
            "Bin Li",
            "Yongdong Zhang",
            "Feng Wu"
        ],
        "published_date": "2024",
        "abstract": "Generation of plausible but incorrect factual information, often termed hallucination, has attracted significant research interest. Retrieval-augmented language model (RALM) -- which enhances models with up-to-date knowledge -- emerges as a promising method to reduce hallucination. However, existing RALMs may instead exacerbate hallucination when retrieving lengthy contexts. To address this challenge, we propose COFT, a novel \\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on different granularity-level key texts, thereby avoiding getting lost in lengthy contexts. Specifically, COFT consists of three components: \\textit{recaller}, \\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a knowledge graph to extract potential key entities in a given context. Second, \\textit{scorer} measures the importance of each entity by calculating its contextual weight. Finally, \\textit{selector} selects high contextual weight entities with a dynamic threshold algorithm and highlights the corresponding paragraphs, sentences, or words in a coarse-to-fine manner. Extensive experiments on the knowledge hallucination benchmark demonstrate the effectiveness of COFT, leading to a superior performance over $30\\%$ in the F1 score metric. Moreover, COFT also exhibits remarkable versatility across various long-form tasks, such as reading comprehension and question answering.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1c7ef42897ad2dced83ab1d58d8fbd4539f87ddc.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0
    },
    "088a42203bc9a67e14b1bfd5c1fd25a03c126c08.pdf": {
        "title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models",
        "authors": [
            "Yue Chang",
            "Liqiang Jing",
            "Xiaopeng Zhang",
            "Yue Zhang"
        ],
        "published_date": "2024",
        "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current studies either focus on the process of model inference or the results of model generation, but the solutions they design sometimes do not deal appropriately with various types of queries and the hallucinations of the generations about these queries. To accurately deal with various hallucinations, we present a unified framework, Dentist, for hallucination mitigation. The core step is to first classify the queries, then perform different processes of hallucination mitigation based on the classification result, just like a dentist first observes the teeth and then makes a plan. In a simple deployment, Dentist can classify queries as perception or reasoning and easily mitigate potential hallucinations in answers which has been demonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8% improvement in accuracy on Image Quality, a Coarse Perception visual question answering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/088a42203bc9a67e14b1bfd5c1fd25a03c126c08.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 0,
        "score": 0
    },
    "ecc51ce52ca524be17616a9c0dc8a051a2996ad7.pdf": {
        "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models",
        "authors": [
            "Hao Yin",
            "Guangzong Si",
            "Zilei Wang"
        ],
        "published_date": "2025",
        "abstract": "Contrastive decoding strategies are widely used to mitigate object hallucinations in multimodal large language models (MLLMs). By reducing over-reliance on language priors, these strategies ensure that generated content remains closely grounded in visual inputs, producing contextually accurate outputs. Since contrastive decoding requires no additional training or external tools, it offers both computational efficiency and versatility, making it highly attractive. However, these methods present two main limitations: (1) bluntly suppressing language priors can compromise coherence and accuracy of generated content, and (2) processing contrastive inputs adds computational load, significantly slowing inference speed. To address these challenges, we propose Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals within the model\u2019s middle layers, where modality fusion predominantly occurs. This approach enables more effective capture of visual features, reducing the model\u2019s bias toward language modality. Experimental results demonstrate that VAF significantly reduces hallucinations across various MLLMs without affecting inference speed, while maintaining coherence and accuracy in generated outputs. The code is available at https://github.com/ustc-hyin/ClearSight.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/ecc51ce52ca524be17616a9c0dc8a051a2996ad7.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "c4d3c2516d24bd1c0eff93ea047147f3afd586ca.pdf": {
        "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models",
        "authors": [
            "Xin Zou",
            "Yizhou Wang",
            "Yibo Yan",
            "Sirui Huang",
            "Kening Zheng",
            "Junkai Chen",
            "Chang Tang",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Despite their impressive capabilities, multimodal large language models (MLLMs) are prone to hallucinations, i.e., the generated content that is nonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in MLLMs often stem from the sensitivity of text decoder to visual tokens, leading to a phenomenon akin to\"amnesia\"about visual information. To address this issue, we propose MemVR, a novel decoding paradigm inspired by common cognition: when the memory of an image seen the moment before is forgotten, people will look at it again for factual answers. Following this principle, we treat visual tokens as supplementary evidence, re-injecting them into the MLLM through Feed Forward Network (FFN) as\"key-value memory\"at the middle trigger layer. This\"look-twice\"mechanism occurs when the model exhibits high uncertainty during inference, effectively enhancing factual alignment. Comprehensive experimental evaluations demonstrate that MemVR significantly mitigates hallucination across various MLLMs and excels in general benchmarks without incurring additional time overhead. The implementation is available from https://github.com/1zhou-Wang/MemVR",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c4d3c2516d24bd1c0eff93ea047147f3afd586ca.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "c910c8f715d8231ed824caff13952d6946de1e59.pdf": {
        "title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models",
        "authors": [
            "Jiawei Chen",
            "Dingkang Yang",
            "Tong Wu",
            "Yue Jiang",
            "Xiaolu Hou",
            "Mingcheng Li",
            "Shunli Wang",
            "Dongling Xiao",
            "Ke Li",
            "Lihua Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations-a significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MediHallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MediHallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work will be released soon.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c910c8f715d8231ed824caff13952d6946de1e59.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "933d1d4f18e721160ddbf8dab25c33f8e3d2cec7.pdf": {
        "title": "Learning to Revise References for Faithful Summarization",
        "authors": [
            "Griffin Adams",
            "Han-Chin Shing",
            "Q. Sun",
            "C. Winestock",
            "K. McKeown",
            "No\u00e9mie Elhadad"
        ],
        "published_date": "2022",
        "abstract": "In real-world scenarios with naturally occurring datasets, reference summaries are noisy and may contain information that cannot be inferred from the source text. On large news corpora, removing low quality samples has been shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora, filtering is detrimental to performance. To improve reference quality while retaining all data, we propose a new approach: to selectively re-write unsupported reference sentences to better reflect source data. We automatically generate a synthetic dataset of positive and negative revisions by corrupting supported sentences and learn to revise reference sentences with contrastive learning. The intensity of revisions is treated as a controllable attribute so that, at inference, diverse candidates can be over-generated-then-rescored to balance faithfulness and abstraction. To test our methods, we extract noisy references from publicly available MIMIC-III discharge summaries for the task of hospital-course summarization, and vary the data on which models are trained. According to metrics and human evaluation, models trained on revised clinical references are much more faithful, informative, and fluent than models trained on original or filtered data.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/933d1d4f18e721160ddbf8dab25c33f8e3d2cec7.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "968bd4cf71c66bb153527778836e54c85ee6162c.pdf": {
        "title": "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models",
        "authors": [
            "Weihong Zhong",
            "Xiaocheng Feng",
            "Liang Zhao",
            "Qiming Li",
            "Lei Huang",
            "Yuxuan Gu",
            "Weitao Ma",
            "Yuan Xu",
            "Bing Qin"
        ],
        "published_date": "2024",
        "abstract": "Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least $31\\%$, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not have supported without distractions. We term this phenomenon Multimodal Hallucination Snowballing. To mitigate this, we further propose a training-free method called Residual Visual Decoding, where we revise the output distribution of LVLMs with the one derived from the residual visual input, providing models with direct access to the visual information. Experiments show that our method can mitigate more than $24\\%$ of the snowballed multimodal hallucination while maintaining capabilities.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/968bd4cf71c66bb153527778836e54c85ee6162c.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "bb3cc013c462ff2bf3dc5be90f731ebf34996f86.pdf": {
        "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models",
        "authors": [
            "Zouying Cao",
            "Yifei Yang",
            "Hai Zhao"
        ],
        "published_date": "2023",
        "abstract": "While Large language models (LLMs) have garnered widespread applications across various domains due to their powerful language understanding and generation capabilities, the detection of non-factual or hallucinatory content generated by LLMs remains scarce. Currently, one significant challenge in hallucination detection is the laborious task of time-consuming and expensive manual annotation of the hallucinatory generation. To address this issue, this paper first introduces a method for automatically constructing model-specific hallucination datasets based on existing fact-checking datasets called AutoHall. Furthermore, we propose a zero-resource and black-box hallucination detection method based on self-contradiction. We conduct experiments towards prevalent open-/closed-source LLMs, achieving superior hallucination detection performance compared to extant baselines. Moreover, our experiments reveal variations in hallucination proportions and types among different models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/bb3cc013c462ff2bf3dc5be90f731ebf34996f86.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "2635c1aeee582dacb865f00d1289b443c3d96d02.pdf": {
        "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
        "authors": [
            "Guanyu Zhou",
            "Yibo Yan",
            "Xin Zou",
            "Kun Wang",
            "Aiwei Liu",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in both industry and academia, but often suffer from biases introduced by visual and language priors, which can lead to multimodal hallucination. These biases arise from the visual encoder and the Large Language Model (LLM) backbone, affecting the attention mechanism responsible for aligning multimodal inputs. Existing decoding-based mitigation methods focus on statistical correlations and overlook the causal relationships between attention mechanisms and model output, limiting their effectiveness in addressing these biases. To tackle this issue, we propose a causal inference framework termed CausalMM that applies structural causal modeling to MLLMs, treating modality priors as a confounder between attention mechanisms and output. Specifically, by employing backdoor adjustment and counterfactual reasoning at both the visual and language attention levels, our method mitigates the negative effects of modality priors and enhances the alignment of MLLM's inputs and outputs, with a maximum score improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME Benchmark compared to conventional methods. Extensive experiments validate the effectiveness of our approach while being a plug-and-play solution. Our code is available at: https://github.com/The-Martyr/CausalMM",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/2635c1aeee582dacb865f00d1289b443c3d96d02.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "4f83d9c391d782d358c2bf0d7ffc6150924dae01.pdf": {
        "title": "Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models",
        "authors": [
            "Kening Zheng",
            "Junkai Chen",
            "Yibo Yan",
            "Xin Zou",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Hallucination issues continue to affect multimodal large language models (MLLMs), with existing research mainly addressing object-level or attribute-level hallucinations, neglecting the more complex relation hallucinations that require advanced reasoning. Current benchmarks for relation hallucinations lack detailed evaluation and effective mitigation, and their datasets often suffer from biases due to systematic annotation processes. To address these challenges, we introduce Reefknot, a comprehensive benchmark targeting relation hallucinations, comprising over 20,000 real-world samples. We provide a systematic definition of relation hallucinations, integrating perceptive and cognitive perspectives, and construct a relation-based corpus using the Visual Genome scene graph dataset. Our comparative evaluation reveals significant limitations in current MLLMs' ability to handle relation hallucinations. Additionally, we propose a novel confidence-based mitigation strategy, which reduces the hallucination rate by an average of 9.75% across three datasets, including Reefknot. Our work offers valuable insights for achieving trustworthy multimodal intelligence.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4f83d9c391d782d358c2bf0d7ffc6150924dae01.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "4b0b56be0ae9479d2bd5c2f0943db1906343c10f.pdf": {
        "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
        "authors": [
            "S. Dhuliawala",
            "M. Komeili",
            "Jing Xu",
            "R. Raileanu",
            "Xian Li",
            "Asli Celikyilmaz",
            "Jason Weston"
        ],
        "published_date": "2023",
        "abstract": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4b0b56be0ae9479d2bd5c2f0943db1906343c10f.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "ca261cb681b082e90ca6c7a9d325b4265ed1dc28.pdf": {
        "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
        "authors": [
            "Yilin Wen",
            "Zifeng Wang",
            "Jimeng Sun"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/ca261cb681b082e90ca6c7a9d325b4265ed1dc28.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04.pdf": {
        "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies",
        "authors": [
            "Liangming Pan",
            "Michael Stephen Saxon",
            "Wenda Xu",
            "Deepak Nathani",
            "Xinyi Wang",
            "William Yang Wang"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "45ffc7928a358ff6567d8420b58d509fc3b7dbd1.pdf": {
        "title": "Mitigating Entity-Level Hallucination in Large Language Models",
        "authors": [
            "Weihang Su",
            "Yichen Tang",
            "Qingyao Ai",
            "Changyue Wang",
            "Zhijing Wu",
            "Yiqun Liu"
        ],
        "published_date": "2024",
        "abstract": "The emergence of Large Language Models (LLMs) has revolutionized how users access information, shifting from traditional search engines to direct question-and-answer interactions with LLMs. However, the widespread adoption of LLMs has revealed a significant challenge known as hallucination, wherein LLMs generate coherent yet factually inaccurate responses. This hallucination phenomenon has led to users' distrust in information retrieval systems based on LLMs. To tackle this challenge, this paper proposes Dynamic Retrieval Augmentation based on hallucination Detection (DRAD) as a novel method to detect and mitigate hallucinations in LLMs. DRAD improves upon traditional retrieval augmentation by dynamically adapting the retrieval process based on real-time hallucination detection. It features two main components: Real-time Hallucination Detection (RHD) for identifying potential hallucinations without external models, and Self-correction based on External Knowledge (SEK) for correcting these errors using external knowledge. Experiment results show that DRAD demonstrates superior performance in both detecting and mitigating hallucinations in LLMs. All of our code and data are open-sourced at https://github.com/oneal2000/EntityHallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/45ffc7928a358ff6567d8420b58d509fc3b7dbd1.pdf",
        "venue": "SIGIR-AP",
        "citationCount": 0,
        "score": 0
    },
    "d00735241af700d21762d2f3ca00d920241a15a4.pdf": {
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
        "authors": [
            "Yue Zhang",
            "Yafu Li",
            "Leyang Cui",
            "Deng Cai",
            "Lemao Liu",
            "Tingchen Fu",
            "Xinting Huang",
            "Enbo Zhao",
            "Yu Zhang",
            "Yulong Chen",
            "Longyue Wang",
            "A. Luu",
            "Wei Bi",
            "Freda Shi",
            "Shuming Shi"
        ],
        "published_date": "2023",
        "abstract": "\n While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/d00735241af700d21762d2f3ca00d920241a15a4.pdf",
        "venue": "Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e.pdf": {
        "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations",
        "authors": [
            "Yue Zhang",
            "Leyang Cui",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published_date": "2023",
        "abstract": "Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as ``hallucination''. In this work, we propose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various model sizes and families. For example, when equipped with ICD, Llama2-7B-Chat and Mistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on TruthfulQA, respectively.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "fca2da71f3dce2f757aef39e561a572f68106603.pdf": {
        "title": "Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models",
        "authors": [
            "Bei Yan",
            "Jie Zhang",
            "Zheng Yuan",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "published_date": "2024",
        "abstract": "Despite the rapid progress and outstanding performance of Large Vision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the issue of hallucination, i.e., LVLMs tend to generate responses that are inconsistent with the corresponding visual inputs. To evaluate the degree of hallucination in LVLMs, previous works have proposed a series of benchmarks featuring different types of tasks and evaluation metrics. However, we find that the quality of the existing hallucination benchmarks varies, with some suffering from problems, e.g., inconsistent evaluation results under repeated tests, and misalignment with human evaluation. To this end, we propose a Hallucination benchmark Quality Measurement framework (HQM), which leverages various indicators to assess the reliability and validity of existing hallucination benchmarks separately. Specifically, for reliability we explore test-retest reliability and parallel-forms reliability, while for validity we examine criterion validity and coverage of hallucination types. Furthermore, based on the results of our quality measurement, we construct a High-Quality Hallucination Benchmark (HQH) for LVLMs, which demonstrates superior reliability and validity under our HQM framework. We conduct an extensive evaluation of over 10 representative LVLMs, including GPT-4o and Gemini-1.5-Pro, to provide an in-depth analysis of the hallucination issues in existing models. Our benchmark is publicly available at https://github.com/HQHBench/HQHBench.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/fca2da71f3dce2f757aef39e561a572f68106603.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "89fccb4b70d0a072d9c874dddfab0afb3676d1b8.pdf": {
        "title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering",
        "authors": [
            "Yuan Sui",
            "Bryan Hooi"
        ],
        "published_date": "2024",
        "abstract": "Recent works integrating Knowledge Graphs (KGs) have shown promising improvements in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing benchmarks primarily focus on closed-ended tasks, leaving a gap in evaluating performance on more complex, real-world scenarios. This limitation also hinders a thorough assessment of KGs' potential to reduce hallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark specifically designed to evaluate LLMs augmented with KGs in open-ended, real-world question answering settings. OKGQA reflects practical complexities through diverse question types and incorporates metrics to quantify both hallucination rates and reasoning improvements in LLM+KG models. To consider the scenarios in which KGs may contain varying levels of errors, we propose a benchmark variant, OKGQA-P, to assess model performance when the semantics and structure of KGs are deliberately perturbed and contaminated. In this paper, we aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on method design. We believe this study can facilitate a more complete performance comparison and encourages continuous improvement in integrating KGs with LLMs to mitigate hallucination, and make LLMs more trustworthy. Code and data are released at https://github.com/Y-Sui/OKGQA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/89fccb4b70d0a072d9c874dddfab0afb3676d1b8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf": {
        "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
        "authors": [
            "Jiawei Chen",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "published_date": "2023",
        "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0
    },
    "0b395ed1c8b284e551172b728e83cf257e33729a.pdf": {
        "title": "Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
        "authors": [
            "Tianrui Guan",
            "Fuxiao Liu",
            "Xiyang Wu",
            "Ruiqi Xian",
            "Zongxia Li",
            "Xiaoyu Liu",
            "Xijun Wang",
            "Lichang Chen",
            "Furong Huang",
            "Yaser Yacoob",
            "Dinesh Manocha",
            "Tianyi Zhou"
        ],
        "published_date": "2023",
        "abstract": "We introduce \u201cHALLUSIONBENCH11\u201cHallusion\u201d is a portmanteau of \u201challucination\u201d and \u201cillusion.\u201d,\u201d a comprehensive benchmark designed for the evaluation of image-context rea-soning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpre-tation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on Hallusion-bench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only high-lights the observed failure modes, including language hal-lucination and visual illusion but also deepens an under-standing of these pitfalls. Our comprehensive case studies within Hallusionbench shed light on the challenges of hallucination and illusion in LVLMs. Based on these in-sights, we suggest potential pathways for their future im-provement. The benchmark and codebase can be accessed at https://github.com/tianyi-labIHallusionBench.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/0b395ed1c8b284e551172b728e83cf257e33729a.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "5cd671efa2af8456c615c5faf54d1be4950f3819.pdf": {
        "title": "Hallucination is Inevitable: An Innate Limitation of Large Language Models",
        "authors": [
            "Ziwei Xu",
            "Sanjay Jain",
            "Mohan Kankanhalli"
        ],
        "published_date": "2024",
        "abstract": "Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5cd671efa2af8456c615c5faf54d1be4950f3819.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "4661b7e8f6bb4f0cc1d4a767a92534f1def344b8.pdf": {
        "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models",
        "authors": [
            "Yongheng Zhang",
            "Xu Liu",
            "Ruoxi Zhou",
            "Qiguang Chen",
            "Hao Fei",
            "Wenpeng Lu",
            "Libo Qin"
        ],
        "published_date": "2025",
        "abstract": "Investigating hallucination issues in large language models (LLMs) within cross-lingual and cross-modal scenarios can greatly advance the large-scale deployment in real-world applications. Nevertheless, the current studies are limited to a single scenario, either cross-lingual or cross-modal, leaving a gap in the exploration of hallucinations in the joint cross-lingual and cross-modal scenarios. Motivated by this, we introduce a novel joint Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this gap. Specifically, CCHall simultaneously incorporates both cross-lingual and cross-modal hallucination scenarios, which can be used to assess the cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a comprehensive evaluation on CCHall, exploring both mainstream open-source and closed-source LLMs. The experimental results highlight that current LLMs still struggle with CCHall. We hope CCHall can serve as a valuable resource to assess LLMs in joint cross-lingual and cross-modal scenarios.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4661b7e8f6bb4f0cc1d4a767a92534f1def344b8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "19e909f88b8b9b0635bd6e441094e1738c3bba9a.pdf": {
        "title": "Unified Hallucination Detection for Multimodal Large Language Models",
        "authors": [
            "Xiang Chen",
            "Chenxi Wang",
            "Yida Xue",
            "Ningyu Zhang",
            "Xiaoyan Yang",
            "Qian Li",
            "Yue Shen",
            "Lei Liang",
            "Jinjie Gu",
            "Huajun Chen"
        ],
        "published_date": "2024",
        "abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/19e909f88b8b9b0635bd6e441094e1738c3bba9a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "05839a68bd05880beef2f171cee7aab960bb6d2f.pdf": {
        "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models",
        "authors": [
            "Chaoya Jiang",
            "Wei Ye",
            "Mengfan Dong",
            "Hongrui Jia",
            "Haiyang Xu",
            "Mingshi Yan",
            "Ji Zhang",
            "Shikun Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision-Language Models (LVLMs) exhibit remarkable capabilities but struggle with ''hallucinations''-inconsistencies between images and their descriptions. Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity. In this paper, we introduce a refined taxonomy of hallucinations, featuring a new category: Event Hallucination. We then utilize advanced LLMs to generate and filter fine-grained hallucinatory data consisting of various types of hallucinations, with a particular focus on event hallucinations, laying the groundwork for integrating discriminative and generative evaluation methods within our universal evaluation framework. The proposed benchmark distinctively assesses LVLMs' ability to tackle a broad spectrum of hallucinations, making it a reliable and comprehensive tool for gauging LVLMs' efficacy in handling hallucinations. We will release our code and data.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/05839a68bd05880beef2f171cee7aab960bb6d2f.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 0,
        "score": 0
    },
    "b877f5076c617a948081e12e08809e6c6b84b468.pdf": {
        "title": "ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models",
        "authors": [
            "Jio Oh",
            "Soyeon Kim",
            "Junseok Seo",
            "Jindong Wang",
            "Ruochen Xu",
            "Xing Xie",
            "Steven Euijong Whang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have achieved unprecedented performances in various applications, yet evaluating them is still challenging. Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity. We contend that utilizing existing relational databases based on the entity-relationship (ER) model is a promising approach for constructing benchmarks as they contain structured knowledge that can be used to question LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational databases have integrity constraints that can be used to better construct complex in-depth questions and verify answers: (1) functional dependencies can be used to pinpoint critical keywords that an LLM must know to properly answer a given question containing certain attribute values; and (2) foreign key constraints can be used to join relations and construct multi-hop questions, which can be arbitrarily long and used to debug intermediate answers. We thus propose ERBench, which uses these integrity constraints to convert any database into an LLM benchmark. ERBench supports continuous evaluation as databases change, multimodal questions, and various prompt engineering techniques. In our experiments, we construct LLM benchmarks using databases of multiple domains and make an extensive comparison of contemporary LLMs. We show how ERBench can properly evaluate any LLM by not only checking for answer correctness, but also effectively verifying the rationales by looking for the right keywords.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/b877f5076c617a948081e12e08809e6c6b84b468.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "7cfbd36c0043098589cbaf18dca2b41d8dc24abe.pdf": {
        "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
        "authors": [
            "Wenliang Dai",
            "Zihan Liu",
            "Ziwei Ji",
            "Dan Su",
            "Pascale Fung"
        ],
        "published_date": "2022",
        "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate non-existent visual objects when generating text based on visual information. In this paper, we systematically study the object hallucination problem from three aspects. First, we examine recent state-of-the-art VLP models, showing that they still hallucinate frequently and models achieving better scores on standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate how different types of image encoding in VLP influence hallucination, including region-based, grid-based, and patch-based. Surprisingly, we find that patch-based features perform the best and smaller patch resolution yields a non-trivial reduction in object hallucination. Third, we decouple various VLP objectives and demonstrate that token-level image-text alignment and controlled generation are crucial to reducing hallucination. Based on that, we propose a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination. Results show that it reduces object hallucination by up to 17.4% when tested on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation).",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/7cfbd36c0043098589cbaf18dca2b41d8dc24abe.pdf",
        "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "1146d40d3d01427a008a20530269667b8989750c.pdf": {
        "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation",
        "authors": [
            "Xun Liang",
            "Shichao Song",
            "Simin Niu",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Bo Tang",
            "Zhaohui Wy",
            "Dawei He",
            "Peng Cheng",
            "Zhonghao Wang",
            "Haiying Deng"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations in text generation is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, designed to compile outputs produced with minimal restrictions by LLMs. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also executed extensive experiments, evaluating prominent Chinese language models and the GPT series models to derive professional performance insights regarding hallucination challenges.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1146d40d3d01427a008a20530269667b8989750c.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "58ee9e1c426166a5451a1ce13e1186f7d6baacfd.pdf": {
        "title": "VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models",
        "authors": [
            "Yuxuan Wang",
            "Yueqian Wang",
            "Dongyan Zhao",
            "Cihang Xie",
            "Zilong Zheng"
        ],
        "published_date": "2024",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have extended their capabilities to video understanding. Yet, these models are often plagued by\"hallucinations\", where irrelevant or nonsensical content is generated, deviating from the actual video context. This work introduces VideoHallucer, the first comprehensive benchmark for hallucination detection in large video-language models (LVLMs). VideoHallucer categorizes hallucinations into two main types: intrinsic and extrinsic, offering further subcategories for detailed analysis, including object-relation, temporal, semantic detail, extrinsic factual, and extrinsic non-factual hallucinations. We adopt an adversarial binary VideoQA method for comprehensive evaluation, where pairs of basic and hallucinated questions are crafted strategically. By evaluating eleven LVLMs on VideoHallucer, we reveal that i) the majority of current models exhibit significant issues with hallucinations; ii) while scaling datasets and parameters improves models' ability to detect basic visual cues and counterfactuals, it provides limited benefit for detecting extrinsic factual hallucinations; iii) existing models are more adept at detecting facts than identifying hallucinations. As a byproduct, these analyses further instruct the development of our self-PEP framework, achieving an average of 5.38% improvement in hallucination resistance across all model architectures.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/58ee9e1c426166a5451a1ce13e1186f7d6baacfd.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "15aaf20d02a1e26be9106e66d065fd1ca5600e29.pdf": {
        "title": "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models",
        "authors": [
            "Junho Kim",
            "Yeonju Kim",
            "Yonghyun Ro"
        ],
        "published_date": "2024",
        "abstract": "This paper presents a way of enhancing the reliability of Large Multi-modal Models (LMMs) in addressing hallucination, where the models generate cross-modal inconsistent responses. Without additional training, we propose Counterfactual Inception, a novel method that implants counterfactual thinking into LMMs using self-generated counterfactual keywords. Our method is grounded in the concept of counterfactual thinking, a cognitive process where human considers alternative realities, enabling more extensive context exploration. Bridging the human cognition mechanism into LMMs, we aim for the models to engage with and generate responses that span a wider contextual scene understanding, mitigating hallucinatory outputs. We further introduce Plausibility Verification Process (PVP), a simple yet robust keyword constraint that effectively filters out sub-optimal keywords to enable the consistent triggering of counterfactual thinking in the model responses. Comprehensive analyses across various LMMs, including both open-source and proprietary models, corroborate that counterfactual thinking significantly reduces hallucination and helps to broaden contextual understanding based on true visual clues.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/15aaf20d02a1e26be9106e66d065fd1ca5600e29.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "be177300487b6d0f25e6cade9a31900454b13281.pdf": {
        "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
        "authors": [
            "Tu Vu",
            "Mohit Iyyer",
            "Xuezhi Wang",
            "Noah Constant",
            "Jerry Wei",
            "Jason Wei",
            "C. Tar",
            "Yun-Hsuan Sung",
            "Denny Zhou",
            "Quoc Le",
            "Thang Luong"
        ],
        "published_date": "2023",
        "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/be177300487b6d0f25e6cade9a31900454b13281.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "889feabe31ba0d24c093ac94d54a06eecb87e3f4.pdf": {
        "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
        "authors": [
            "Nouha Dziri",
            "Andrea Madotto",
            "Osmar Zaiane",
            "A. Bose"
        ],
        "published_date": "2021",
        "abstract": "Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/889feabe31ba0d24c093ac94d54a06eecb87e3f4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "39d8486475173357619647061dda377f4c38853e.pdf": {
        "title": "Multi-Object Hallucination in Vision-Language Models",
        "authors": [
            "Xuweiyi Chen",
            "Ziqiao Ma",
            "Xuejun Zhang",
            "Sihan Xu",
            "Shengyi Qian",
            "Jianing Yang",
            "D. Fouhey",
            "Joyce Chai"
        ],
        "published_date": "2024",
        "abstract": "Large vision language models (LVLMs) often suffer from object hallucination, producing objects not present in the given images. While current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities, this work systematically investigates multi-object hallucination, examining how models misperceive (e.g., invent nonexistent objects or become distracted) when tasked with focusing on multiple objects simultaneously. We introduce Recognition-based Object Probing Evaluation (ROPE), an automated evaluation protocol that considers the distribution of object classes within a single image during testing and uses visual referring prompts to eliminate ambiguity. With comprehensive empirical studies and analysis of potential factors leading to multi-object hallucination, we found that (1). LVLMs suffer more hallucinations when focusing on multiple objects compared to a single object. (2). The tested object class distribution affects hallucination behaviors, indicating that LVLMs may follow shortcuts and spurious correlations. (3). Hallucinatory behaviors are influenced by data-specific factors, salience and frequency, and model intrinsic behaviors. We hope to enable LVLMs to recognize and reason about multiple objects that often occur in realistic visual scenes, provide insights, and quantify our progress towards mitigating the issues.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/39d8486475173357619647061dda377f4c38853e.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "24a48ef14c8eb4e571e3f4ae9b37936060a3fb06.pdf": {
        "title": "VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding",
        "authors": [
            "Chaoyu Li",
            "Eun Woo Im",
            "Pooyan Fazli"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) have recently shown significant advancements in video understanding, excelling in content reasoning and instruction-following tasks. However, hallucination, where models generate inaccurate or misleading content, remains underexplored in the video domain. Building on the observation that MLLM visual encoders often fail to distinguish visually different yet semantically similar video pairs, we introduce VIDHALLUC, the largest benchmark designed to examine hallucinations in MLLMs for video understanding. It consists of 5,002 videos, paired to highlight cases prone to hallucinations. VIDHALLUC assesses hallucinations across three critical dimensions: (1) action, (2) temporal sequence, and (3) scene transition. Comprehensive testing shows that most MLLMs are vulnerable to hallucinations across these dimensions. Furthermore, we propose DINO-HEAL, a training-free method that reduces hallucinations by incorporating spatial saliency from DINOv2 to reweight visual features during inference. Our results show that DINO-HEAL consistently improves performance on VIDHALLUC, achieving an average improvement of 3.02% in mitigating hallucinations across all tasks. Both the VIDHALLUC benchmark and DINO-HEAL code are available at https://peoplerobots.github.io/vidhalluc.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/24a48ef14c8eb4e571e3f4ae9b37936060a3fb06.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "99832586d55f540f603637e458a292406a0ed75d.pdf": {
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "authors": [
            "Shunyu Yao",
            "Jeffrey Zhao",
            "Dian Yu",
            "Nan Du",
            "Izhak Shafran",
            "Karthik Narasimhan",
            "Yuan Cao"
        ],
        "published_date": "2022",
        "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/99832586d55f540f603637e458a292406a0ed75d.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "a2f44fc0f0c24fd4ab848f01a770a68dfa114f62.pdf": {
        "title": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach",
        "authors": [
            "Shijian Deng",
            "Wentian Zhao",
            "Yu-Jhe Li",
            "Kun Wan",
            "Daniel Miranda",
            "Ajinkya Kale",
            "Yapeng Tian"
        ],
        "published_date": "2024",
        "abstract": "Self-improvement in multimodal large language models (MLLMs) is crucial for enhancing their reliability and robustness. However, current methods often rely heavily on MLLMs themselves as judges, leading to high computational costs and potential pitfalls like reward hacking and model collapse. This paper introduces a novel, model-level judge-free self-improvement framework. Our approach employs a controlled feedback mechanism while eliminating the need for MLLMs in the verification loop. We generate preference learning pairs using a controllable hallucination mechanism and optimize data quality by leveraging lightweight, contrastive language-image encoders to evaluate and reverse pairs when necessary. Evaluations across public benchmarks and our newly introduced IC dataset designed to challenge hallucination control demonstrate that our model outperforms conventional techniques. We achieve superior precision and recall with significantly lower computational demands. This method offers an efficient pathway to scalable self-improvement in MLLMs, balancing performance gains with reduced resource requirements.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/a2f44fc0f0c24fd4ab848f01a770a68dfa114f62.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "c680e5d34b713f8b63ad68149973d5b2b485dd07.pdf": {
        "title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Junzhe Chen",
            "Tianshu Zhang",
            "Shiyu Huang",
            "Yuwei Niu",
            "Linfeng Zhang",
            "Lijie Wen",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Despite the recent breakthroughs achieved by Large Vision Language Models (LVLMs) in understanding and responding to complex visual-textual contexts, their inherent hallucination tendencies limit their practical application in real-world scenarios that demand high levels of precision. Existing methods typically either fine-tune the LVLMs using additional data, which incurs extra costs in manual annotation and computational resources or perform comparisons at the decoding stage, which may eliminate useful language priors for reasoning while introducing inference time overhead. Therefore, we propose ICT, a lightweight, training-free method that calculates an intervention direction to shift the model\u2019s focus towards different levels of visual information, enhancing its attention to high-level and fine-grained visual details. During the forward pass stage, the intervention is applied to the attention heads that encode the overall image information and the fine-grained object details, effectively mitigating the phenomenon of overly language priors, and thereby alleviating hallucinations. Extensive experiments demonstrate that ICT achieves strong performance with a small amount of data and generalizes well across different datasets and models. Our codes are publicly available at:https://github.com/THU-BPM/ICT/.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c680e5d34b713f8b63ad68149973d5b2b485dd07.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "c7714dc70eb508a0b1859b7b1a5af552439b973f.pdf": {
        "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
        "authors": [
            "Yining Wang",
            "Mi Zhang",
            "Junjie Sun",
            "Chenyue Wang",
            "Min Yang",
            "Hui Xue",
            "Jialing Tao",
            "Ranjie Duan",
            "Jiexi Liu"
        ],
        "published_date": "2025",
        "abstract": "Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content. In this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process. We propose a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications. Distinguished from previous adversarial methods that rely on fixed patterns, our approach generates dynamic, effective, and highly transferable visual adversarial inputs, without sacrificing the quality of model responses. Comprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our attack in compromising black-box MLLMs even with extensive mitigating mechanisms, as well as the promising results against cutting-edge commercial APIs, such as GPT-4o and Gemini 1.5. Our code is available at https://huggingface.co/RachelHGF/Mirage-in-the-Eyes.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c7714dc70eb508a0b1859b7b1a5af552439b973f.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "fc4c380102d6f72657d1ab54dffd6be536bb01c7.pdf": {
        "title": "A Survey on Hallucination in Large Vision-Language Models",
        "authors": [
            "Hanchao Liu",
            "Wenyuan Xue",
            "Yifei Chen",
            "Dapeng Chen",
            "Xiutian Zhao",
            "Ke Wang",
            "Liping Hou",
            "Rong-Zhi Li",
            "Wei Peng"
        ],
        "published_date": "2024",
        "abstract": "Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/fc4c380102d6f72657d1ab54dffd6be536bb01c7.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "a7f4deb9a1452374330f202bc8d36966a0f254e8.pdf": {
        "title": "THRONE: An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Language Models",
        "authors": [
            "Prannay Kaul",
            "Zhizhong Li",
            "Hao Yang",
            "Yonatan Dukler",
            "Ashwin Swaminathan",
            "C. Taylor",
            "Stefano Soatto"
        ],
        "published_date": "2024",
        "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term \u201cType I hallucinations\u201d. Instead, they focus on hallucinations responding to very specific question formats-typically a multiple-choice response regarding a particular object or attribute-which we term \u201cType II hallucinations\u201d. Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of halluci-nations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/a7f4deb9a1452374330f202bc8d36966a0f254e8.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "e468ed6b824e60f45ba9a20b034e4090c6630751.pdf": {
        "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
        "authors": [
            "Xingxuan Li",
            "Ruochen Zhao",
            "Yew Ken Chia",
            "Bosheng Ding",
            "Shafiq R. Joty",
            "Soujanya Poria",
            "Lidong Bing"
        ],
        "published_date": "2023",
        "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e468ed6b824e60f45ba9a20b034e4090c6630751.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "807f336176070bd3f95b82a16f125ee99b7d2c80.pdf": {
        "title": "Woodpecker: Hallucination Correction for Multimodal Large Language Models",
        "authors": [
            "Shukang Yin",
            "Chaoyou Fu",
            "Sirui Zhao",
            "Tong Xu",
            "Hao Wang",
            "Dianbo Sui",
            "Yunhang Shen",
            "Ke Li",
            "Xingguo Sun",
            "Enhong Chen"
        ],
        "published_date": "2023",
        "abstract": "Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/807f336176070bd3f95b82a16f125ee99b7d2c80.pdf",
        "venue": "Science China Information Sciences",
        "citationCount": 0,
        "score": 0
    },
    "83d81e31f5c32f6989d98be1133adfc08db094ce.pdf": {
        "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Kedi Chen",
            "Qin Chen",
            "Jie Zhou",
            "Yishen He",
            "Liang He"
        ],
        "published_date": "2024",
        "abstract": "Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-machine interaction scenarios. Finally, professional scholars annotate all the samples in the dataset. DiaHalu covers four common multi-turn dialogue domains and five hallucination subtypes, extended from factuality and faithfulness hallucination. Experiments through some well-known LLMs and detection methods on the dataset show that DiaHalu is a challenging benchmark, holding significant value for further research.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/83d81e31f5c32f6989d98be1133adfc08db094ce.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "396305230ddcf915b19a19683a89e34d76321a33.pdf": {
        "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models",
        "authors": [
            "Hongbin Ye",
            "Tong Liu",
            "Aijia Zhang",
            "Wei Hua",
            "Weiqiang Jia"
        ],
        "published_date": "2023",
        "abstract": "As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/396305230ddcf915b19a19683a89e34d76321a33.pdf",
        "venue": "LKM@IJCAI",
        "citationCount": 0,
        "score": 0
    },
    "c6bf48f25e0a65d64d658b47326de5922ea7dd44.pdf": {
        "title": "A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation",
        "authors": [
            "Tianyu Liu",
            "Yizhe Zhang",
            "C. Brockett",
            "Yi Mao",
            "Zhifang Sui",
            "Weizhu Chen",
            "W. Dolan"
        ],
        "published_date": "2021",
        "abstract": "Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time. As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HaDeS (HAllucination DEtection dataSet). To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowd-sourced annotations. To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. We conduct comprehensive data analyses and create multiple baseline models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c6bf48f25e0a65d64d658b47326de5922ea7dd44.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "99bfe503743c5ec8e16e50ab8438159cdb533a89.pdf": {
        "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
        "authors": [
            "Vipula Rawte",
            "Swagata Chakraborty",
            "Agnibh Pathak",
            "Anubhav Sarkar",
            "S.M. Towhidul Islam Tonmoy",
            "Islam Tonmoy",
            "Aman Chadha",
            "Amit P. Sheth",
            "Amitava Das",
            "Paris",
            "A. Sridhar",
            "Erik Visser",
            "Improved",
            "Jianlin Su",
            "Yu Lu",
            "Shengfeng Pan",
            "Ahmed Murtadha",
            "Bo Wen",
            "Yunfeng Liu",
            "Roformer",
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Tianyi Zhang",
            "Yann Dubois",
            "Xuechen Li",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori Hashimoto",
            "Stanford",
            "Hugo Touvron",
            "Thibaut Lavril",
            "Gautier Izacard",
            "Xavier Martinet",
            "M. Lachaux",
            "Timoth\u00e9e Lacroix",
            "Baptiste Rozi\u00e8re",
            "Naman Goyal",
            "Eric Hambro",
            "Faisal Azhar",
            "Aur'elien Rodriguez",
            "Armand Joulin",
            "Thomas Wolf",
            "Lysandre Debut",
            "Victor Sanh",
            "Julien Chaumond",
            "Clement Delangue",
            "Anthony Moi",
            "Pierric Cistac",
            "Tim Rault",
            "R\u00e9mi Louf",
            "Morgan Funtow-icz",
            "Joe Davison",
            "Sam Shleifer",
            "Patrick von Platen",
            "Clara Ma",
            "Yacine Jernite",
            "J. Plu",
            "Canwen Xu",
            "Teven Le Scao",
            "Sylvain Gugger",
            "Mariama Drame",
            "Quentin Lhoest",
            "Susan Zhang",
            "Stephen Roller",
            "Mikel Artetxe",
            "Moya Chen",
            "Shuohui Chen",
            "Christopher De-wan",
            "Mona T. Diab",
            "Xi Xian Li",
            "Todor Victoria Lin",
            "Myle Ott",
            "Kurt Shuster",
            "Punit Daniel Simig",
            "S. Koura",
            "Anjali Sridhar",
            "Tianlu Wang",
            "Luke Zettlemoyer. 2022",
            "Daniel M. Ziegler",
            "Nisan Stiennon",
            "Jeffrey Wu",
            "Tom B. Brown",
            "Alec Radford",
            "Dario Amodei",
            "Paul F. Chris-tiano"
        ],
        "published_date": "2023",
        "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/99bfe503743c5ec8e16e50ab8438159cdb533a89.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "c946888e2f81b1db84ba4addf2a11e87f0568fe9.pdf": {
        "title": "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies",
        "authors": [
            "Liangming Pan",
            "Michael Stephen Saxon",
            "Wenda Xu",
            "Deepak Nathani",
            "Xinyi Wang",
            "W. Wang"
        ],
        "published_date": "2024",
        "abstract": "While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback\u2014either produced by the LLM itself (self-correction) or some external system\u2014are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c946888e2f81b1db84ba4addf2a11e87f0568fe9.pdf",
        "venue": "Transactions of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "cd2e04598909158494e556823d9de8baa692cee2.pdf": {
        "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
        "authors": [
            "Ziwei Ji",
            "Tiezheng Yu",
            "Yan Xu",
            "Nayeon Lee",
            "Etsuko Ishii",
            "Pascale Fung"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/cd2e04598909158494e556823d9de8baa692cee2.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "0422493dc3a70816bb5d327c4c67094f64a78c98.pdf": {
        "title": "Towards Trustable Language Models: Investigating Information Quality of Large Language Models",
        "authors": [
            "Rick Rejeleene",
            "Xiaowei Xu",
            "John R. Talburt"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data. Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality. Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM. Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information. Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity. In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/0422493dc3a70816bb5d327c4c67094f64a78c98.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f.pdf": {
        "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
        "authors": [
            "Lei Wang",
            "Jiabang He",
            "Shenshen Li",
            "Ning Liu",
            "Ee-Peng Lim"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f.pdf",
        "venue": "Conference on Multimedia Modeling",
        "citationCount": 0,
        "score": 0
    },
    "97d24f9f0d81007d57cc43e61bf2b0c9081fe184.pdf": {
        "title": "R-Tuning: Instructing Large Language Models to Say \u2018I Don\u2019t Know\u2019",
        "authors": [
            "Hanning Zhang",
            "Shizhe Diao",
            "Yong Lin",
            "Y. Fung",
            "Qing Lian",
            "Xingyao Wang",
            "Yangyi Chen",
            "Heng Ji",
            "Tong Zhang"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a model\u2019s ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing. Our code is available at https://github.com/shizhediao/R-Tuning",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/97d24f9f0d81007d57cc43e61bf2b0c9081fe184.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "411b725522e2747e890ba5acfbf43d22f759c00a.pdf": {
        "title": "Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models",
        "authors": [
            "Weihang Su",
            "Changyue Wang",
            "Qingyao Ai",
            "Hu Yiran",
            "Zhijing Wu",
            "Yujia Zhou",
            "Yiqun Liu"
        ],
        "published_date": "2024",
        "abstract": "Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/411b725522e2747e890ba5acfbf43d22f759c00a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "492e526ca2416a734f286da0efcfeda4672ea77f.pdf": {
        "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models",
        "authors": [
            "Ningke Li",
            "Yuekang Li",
            "Yi Liu",
            "Ling Shi",
            "Kailong Wang",
            "Haoyu Wang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations \u2014 coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations. To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth. Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7% to 59.8%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations. To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/492e526ca2416a734f286da0efcfeda4672ea77f.pdf",
        "venue": "Proc. ACM Program. Lang.",
        "citationCount": 0,
        "score": 0
    },
    "b169426b9181adee0e7d6616fc12fc12611d9901.pdf": {
        "title": "The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem",
        "authors": [
            "Joe B Hakim",
            "Jeffery L. Painter",
            "D. Ramcharran",
            "V. Kara",
            "Greg Powell",
            "Paulina Sobczak",
            "Chiho Sato",
            "Andrew Bate",
            "Andrew Beam"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are useful tools with the capacity for performing specific types of knowledge work at an effective scale. However, LLM deployments in high-risk and safety-critical domains pose unique challenges, notably the issue of ``hallucination,'' where LLMs can generate fabricated information. This is particularly concerning in settings such as drug safety, where inaccuracies could lead to patient harm. To mitigate these risks, we have developed and demonstrated a proof of concept suite of guardrails specifically designed to mitigate certain types of hallucinations and errors for drug safety, and potentially applicable to other medical safety-critical contexts. These guardrails include mechanisms to detect anomalous documents to prevent the ingestion of inappropriate data, identify incorrect drug names or adverse event terms, and convey uncertainty in generated content. We integrated these guardrails with an LLM fine-tuned for a text-to-text task, which involves converting both structured and unstructured data within adverse event reports into natural language. This method was applied to translate individual case safety reports, demonstrating effective application in a pharmacovigilance processing task. Our guardrail framework offers a set of tools with broad applicability across various domains, ensuring LLMs can be safely used in high-risk situations by eliminating the occurrence of key errors, including the generation of incorrect pharmacovigilance-related terms, thus adhering to stringent regulatory and quality standards in medical safety-critical environments.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/b169426b9181adee0e7d6616fc12fc12611d9901.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "682ff66a5ec0248f7e4a17a684b2d1e328e57f70.pdf": {
        "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
        "authors": [
            "Yuzhe Gu",
            "Ziwei Ji",
            "Wenwei Zhang",
            "Chengqi Lyu",
            "Dahua Lin",
            "Kai Chen"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) exhibit hallucinations in long-form question-answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators. To facilitate the scalable oversight of LLM hallucinations, this paper introduces an iterative self-training framework that simultaneously and progressively scales up the hallucination annotation dataset and improves the accuracy of the hallucination annotator. Based on the Expectation Maximization (EM) algorithm, in each iteration, the framework first applies a hallucination annotation pipeline to annotate a scaled dataset and then trains a more accurate hallucination annotator on the dataset. This new hallucination annotator is adopted in the hallucination annotation pipeline used for the next iteration. Extensive experimental results demonstrate that the finally obtained hallucination annotator with only 7B parameters surpasses the performance of GPT-4 and obtains new state-of-the-art hallucination detection results on HaluEval and HalluQA by zero-shot inference. Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference (NLI) metric increasing from 25% to 37% on HaluEval.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/682ff66a5ec0248f7e4a17a684b2d1e328e57f70.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8.pdf": {
        "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
        "authors": [
            "Lei Huang",
            "Weijiang Yu",
            "Weitao Ma",
            "Weihong Zhong",
            "Zhangyin Feng",
            "Haotian Wang",
            "Qianglong Chen",
            "Weihua Peng",
            "Xiaocheng Feng",
            "Bing Qin",
            "Ting Liu"
        ],
        "published_date": "2023",
        "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1e909e2a8cdacdcdff125ebcc566f37cb869a1c8.pdf",
        "venue": "ACM Trans. Inf. Syst.",
        "citationCount": 0,
        "score": 0
    },
    "6d26836a4cee8f90c6fa4d5751d5f10e0f720301.pdf": {
        "title": "Improved Beam Search for Hallucination Mitigation in Abstractive Summarization",
        "authors": [
            "A. Sridhar",
            "Erik M. Visser"
        ],
        "published_date": "2022",
        "abstract": "Advancement in large pretrained language models has significantly improved their performance for conditional language generation tasks including summarization albeit with hallucinations. To reduce hallucinations, conventional methods proposed improving beam search or using a fact checker as a postprocessing step. In this paper, we investigate the use of the Natural Language Inference (NLI) entailment metric to detect and prevent hallucinations in summary generation. We propose an NLI-assisted beam re-ranking mechanism by computing entailment probability scores between the input context and summarization model-generated beams during saliency-enhanced greedy decoding. Moreover, a diversity metric is introduced to compare its effectiveness against vanilla beam search. Our proposed algorithm significantly outperforms vanilla beam decoding on XSum and CNN/DM datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/6d26836a4cee8f90c6fa4d5751d5f10e0f720301.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "b10482ab3dd1d340c3c926d92c3e617c24ee3949.pdf": {
        "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
        "authors": [
            "Haoqiang Kang",
            "Juntong Ni",
            "Huaxiu Yao"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the\"snowballing\"issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/b10482ab3dd1d340c3c926d92c3e617c24ee3949.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "e0384ba36555232c587d4a80d527895a095a9001.pdf": {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "J. Nie",
            "Ji-rong Wen"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e0384ba36555232c587d4a80d527895a095a9001.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "143a05fb36be8198d7675b594c0656b5652da3cb.pdf": {
        "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps",
        "authors": [
            "Yung-Sung Chuang",
            "Linlu Qiu",
            "Cheng-Yu Hsieh",
            "Ranjay Krishna",
            "Yoon Kim",
            "James Glass"
        ],
        "published_date": "2024",
        "abstract": "When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such **contextual hallucinations**. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these _lookback ratio_ features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector\u2014**Lookback Lens**\u2014is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/143a05fb36be8198d7675b594c0656b5652da3cb.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "705ffeccfde95c3b0723f197c4565f7d3f0451a1.pdf": {
        "title": "Zero-Resource Hallucination Prevention for Large Language Models",
        "authors": [
            "Junyu Luo",
            "Cao Xiao",
            "Fenglong Ma"
        ],
        "published_date": "2023",
        "abstract": "The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of\"hallucination,\"which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from responding to unfamiliar topics, thus reducing hallucinations. We validate SELF-FAMILIARITY across four different large language models, demonstrating consistently superior performance compared to existing techniques. Our findings propose a significant shift towards preemptive strategies for hallucination mitigation in LLM assistants, promising improvements in reliability, applicability, and interpretability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/705ffeccfde95c3b0723f197c4565f7d3f0451a1.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "57f0d904629955d16bb2b80a5d427e6b1efa6562.pdf": {
        "title": "ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models",
        "authors": [
            "Yeji Park",
            "Deokyeong Lee",
            "Junsuk Choe",
            "Buru Chang"
        ],
        "published_date": "2024",
        "abstract": "Hallucinations in Multimodal Large Language Models (MLLMs) where generated responses fail to accurately reflect the given image pose a significant challenge to their reliability. To address this, we introduce ConVis, a novel training-free contrastive decoding method. ConVis leverages a text-to-image (T2I) generation model to semantically reconstruct the given image from hallucinated captions. By comparing the contrasting probability distributions produced by the original and reconstructed images, ConVis enables MLLMs to capture visual contrastive signals that penalize hallucination generation. Notably, this method operates purely within the decoding process, eliminating the need for additional data or model updates. Our extensive experiments on five popular benchmarks demonstrate that ConVis effectively reduces hallucinations across various MLLMs, highlighting its potential to enhance model reliability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/57f0d904629955d16bb2b80a5d427e6b1efa6562.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0
    },
    "e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0.pdf": {
        "title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning",
        "authors": [
            "Xiaoye Qu",
            "Jiashuo Sun",
            "Wei Wei",
            "Yu Cheng"
        ],
        "published_date": "2024",
        "abstract": "Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multi-modal context comprehension. However, they still suffer from hallucination problems referring to generating inconsistent outputs with the image content. To mitigate hallucinations, previous studies mainly focus on retraining LVLMs with custom datasets. Although effective, they inherently come with additional computational costs. In this paper, we propose a training-free framework, \\textbf{MVP}, that aims to reduce hallucinations by making the most of the innate capabilities of the LVLMs via \\textbf{M}ulti-\\textbf{V}iew Multi-\\textbf{P}ath Reasoning. Specifically, we first devise a multi-view information-seeking strategy to thoroughly perceive the comprehensive information in the image, which enriches the general global information captured by the original vision encoder in LVLMs. Furthermore, during the answer decoding, we observe that the occurrence of hallucinations has a strong correlation with the certainty of the answer tokens. Thus, we propose multi-path reasoning for each information view to quantify and aggregate the certainty scores for each potential answer among multiple decoding paths and finally decide the output answer. By fully grasping the information in the image and carefully considering the certainty of the potential answers when decoding, our MVP can effectively reduce hallucinations in LVLMs.The extensive experiments verify that our proposed MVP significantly mitigates the hallucination problem across four well-known LVLMs. The source code is available at: \\url{https://github.com/GasolSun36/MVP}.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b.pdf": {
        "title": "Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy",
        "authors": [
            "Benedict Aaron Tjandra",
            "Muhammed Razzak",
            "Jannik Kossen",
            "Kunal Handa",
            "Yarin Gal"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are known to hallucinate, whereby they generate plausible but inaccurate text. This phenomenon poses significant risks in critical applications, such as medicine or law, necessitating robust hallucination mitigation strategies. While recent works have proposed fine-tuning methods to teach LLMs to abstain from answering questions beyond their knowledge or capabilities, these methods rely on the existence of ground-truth labels or are limited to short-form responses. To address these limitations, we propose fine-tuning using semantic entropy, an uncertainty measure derived from introspection into the model which does not require external labels. We demonstrate that our approach matches or outperforms models fine-tuned using prior work and achieves strong performance for both short and long-form generations on a range of datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "c7a7104df3db13737a865ede2be8146990fa4026.pdf": {
        "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
        "authors": [
            "Fuxiao Liu",
            "Kevin Lin",
            "Linjie Li",
            "Jianfeng Wang",
            "Y. Yacoob",
            "Lijuan Wang"
        ],
        "published_date": "2023",
        "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data are available at https://github.com/FuxiaoLiu/LRV-Instruction.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c7a7104df3db13737a865ede2be8146990fa4026.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4.pdf": {
        "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
        "authors": [
            "Nick McKenna",
            "Tianyi Li",
            "Liang Cheng",
            "Mohammad Javad Hosseini",
            "Mark Johnson",
            "Mark Steedman"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences: we show that, regardless of the premise, models falsely label NLI test samples as entailing when the hypothesis is attested in training data, and that entities are used as ``indices'' to access the memorized data. Second, statistical patterns of usage learned at the level of corpora: we further show a similar effect when the premise predicate is less frequent than that of the hypothesis in the training data, a bias following from previous studies. We demonstrate that LLMs perform significantly worse on NLI test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future LLM evaluation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf": {
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "Yunfan Gao",
            "Yun Xiong",
            "Xinyu Gao",
            "Kangxiang Jia",
            "Jinliu Pan",
            "Yuxi Bi",
            "Yi Dai",
            "Jiawei Sun",
            "Qianyu Guo",
            "Meng Wang",
            "Haofen Wang"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "328eb183007bf4aefbf42437b42a15db375803e3.pdf": {
        "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
        "authors": [
            "Sicong Leng",
            "Hang Zhang",
            "Guanzheng Chen",
            "Xin Li",
            "Shijian Lu",
            "Chunyan Miao",
            "Li Bing"
        ],
        "published_date": "2023",
        "abstract": "Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/328eb183007bf4aefbf42437b42a15db375803e3.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "4e53b481beabba42aac027e5a8c69fed26ab4062.pdf": {
        "title": "RHO ($\u03c1$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
        "authors": [
            "Ziwei Ji",
            "Zihan Liu",
            "Nayeon Lee",
            "Tiezheng Yu",
            "Bryan Wilie",
            "Mini Zeng",
            "Pascale Fung"
        ],
        "published_date": "2022",
        "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4e53b481beabba42aac027e5a8c69fed26ab4062.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "f6d4c76b21539aadc2ca8d813fe631be7149231e.pdf": {
        "title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models",
        "authors": [
            "J. Wu",
            "Tsz Ting Chung",
            "Kai Chen",
            "Dit-Yan Yeung"
        ],
        "published_date": "2024",
        "abstract": "Despite the outstanding performance in vision-language reasoning, Large Vision-Language Models (LVLMs) might generate hallucinated contents that do not exist in the given image. Most existing LVLM hallucination benchmarks are constrained to evaluate the object-related hallucinations. However, the potential hallucination on the relations between two objects, i.e., relation hallucination, still lacks investigation. To remedy that, we design a unified framework to measure the object and relation hallucination in LVLMs simultaneously. The core idea of our framework is to evaluate hallucinations via (object, relation, object) triplets extracted from LVLMs'responses, making it easily generalizable to different vision-language tasks. Based on our framework, we further introduce Tri-HE, a novel Triplet-level Hallucination Evaluation benchmark which can be used to study both object and relation hallucination at the same time. With comprehensive evaluations on Tri-HE, we observe that the relation hallucination issue is even more serious than object hallucination among existing LVLMs, highlighting a previously neglected problem towards reliable LVLMs. Moreover, based on our findings, we design a simple training-free approach that effectively mitigates hallucinations for LVLMs. Our dataset and code for the reproduction of our experiments are available publicly at https://github.com/wujunjie1998/Tri-HE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/f6d4c76b21539aadc2ca8d813fe631be7149231e.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 0,
        "score": 0
    },
    "49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c.pdf": {
        "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation",
        "authors": [
            "Qidong Huang",
            "Xiao-wen Dong",
            "Pan Zhang",
            "Bin Wang",
            "Conghui He",
            "Jiaqi Wang",
            "Dahua Lin",
            "Weiming Zhang",
            "Neng H. Yu"
        ],
        "published_date": "2023",
        "abstract": "Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment. Existing methods mitigate this issue with either training with specific designed data or inferencing with external knowledge from other sources, incurring inevitable additional costs. In this paper, we present OPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a Retrospection-Allocation strategy, serving as a nearly free lunch to alleviate the hallucination issue without additional data, knowledge, or training. Our approach begins with an interesting observation that, most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens, but not all the previous tokens. Such partial overtrust inclination results in the neglecting of image tokens and describes the image content with hallucination. Based on the observation, OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue, along with a rollback strategy that retrospects the presence of summary tokens in the previously generated tokens, and re-allocate the token selection if necessary. With extensive experiments, OPERA shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality. Our code is at: https://github.com/shikiw/OPERA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0
    },
    "01f3b1809035a593b9dd6fb0b2cabdc8e216542f.pdf": {
        "title": "Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models",
        "authors": [
            "Yuji Zhang",
            "Sha Li",
            "Jiateng Liu",
            "Pengfei Yu",
            "Y. Fung",
            "Jing Li",
            "Manling Li",
            "Heng Ji"
        ],
        "published_date": "2024",
        "abstract": "Hallucination is often regarded as a major impediment for using large language models (LLMs), especially for knowledge-intensive tasks. Even when the training corpus consists solely of true statements, language models still generate hallucinations in the form of amalgamations of multiple facts. We coin this phenomenon as ``knowledge overshadowing'': when we query knowledge from a language model with multiple conditions, some conditions overshadow others, leading to hallucinated outputs. This phenomenon partially stems from training data imbalance, which we verify on both pretrained models and fine-tuned models, over a wide range of LM model families and sizes.From a theoretical point of view, knowledge overshadowing can be interpreted as over-generalization of the dominant conditions (patterns). We show that the hallucination rate grows with both the imbalance ratio (between the popular and unpopular condition) and the length of dominant condition description, consistent with our derived generalization bound. Finally, we propose to utilize overshadowing conditions as a signal to catch hallucination before it is produced, along with a training-free self-contrastive decoding method to alleviate hallucination during inference. Our proposed approach showcases up to 82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control, with different models and datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/01f3b1809035a593b9dd6fb0b2cabdc8e216542f.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "0f7a6c557e376d8c77d684bcda0daee74fc29acf.pdf": {
        "title": "Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation",
        "authors": [
            "Yanyang Li",
            "Jianqiao Zhao",
            "M. Lyu",
            "Liwei Wang"
        ],
        "published_date": "2022",
        "abstract": "Recent advances in large-scale pre-training provide large models with the potential to learn knowledge from the raw text. It is thus natural to ask whether it is possible to leverage these large models as knowledge bases for downstream tasks. In this work, we answer the aforementioned question in unsupervised knowledge-grounded conversation. We explore various methods that best elicit knowledge from large models. Our human study indicates that, though hallucinations exist, large models post the unique advantage of being able to output common sense and summarize facts that cannot be directly retrieved from the search engine. To better exploit such generated knowledge in dialogue generation, we treat the generated knowledge as a noisy knowledge source and propose the posterior-based reweighing as well as the noisy training strategy. Empirical results on two benchmarks show advantages over the state-of-the-art methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/0f7a6c557e376d8c77d684bcda0daee74fc29acf.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "d6da914d0c8021df6622857aba23b794fc7e6a40.pdf": {
        "title": "A Survey of Hallucination in Large Visual Language Models",
        "authors": [
            "Wei Lan",
            "Wenyi Chen",
            "Qingfeng Chen",
            "Shirui Pan",
            "Huiyu Zhou",
            "Yi Pan"
        ],
        "published_date": "2024",
        "abstract": "The Large Visual Language Models (LVLMs) enhances user interaction and enriches user experience by integrating visual modality on the basis of the Large Language Models (LLMs). It has demonstrated their powerful information processing and generation capabilities. However, the existence of hallucinations has limited the potential and practical effectiveness of LVLM in various fields. Although lots of work has been devoted to the issue of hallucination mitigation and correction, there are few reviews to summary this issue. In this survey, we first introduce the background of LVLMs and hallucinations. Then, the structure of LVLMs and main causes of hallucination generation are introduced. Further, we summary recent works on hallucination correction and mitigation. In addition, the available hallucination evaluation benchmarks for LVLMs are presented from judgmental and generative perspectives. Finally, we suggest some future research directions to enhance the dependability and utility of LVLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/d6da914d0c8021df6622857aba23b794fc7e6a40.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "5838b56f2c7ca3dd946428dae07bdc26a9265c67.pdf": {
        "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
        "authors": [
            "Haoqiang Kang",
            "Xiao-Yang Liu"
        ],
        "published_date": "2023",
        "abstract": "The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs), especially when applied to fields such as finance, education, and law. Despite the growing concerns, there has been a lack of empirical investigation. In this paper, we provide an empirical examination of LLMs' hallucination behaviors in financial tasks. First, we empirically investigate LLM model's ability of explaining financial concepts and terminologies. Second, we assess LLM models' capacity of querying historical stock prices. Third, to alleviate the hallucination issue, we evaluate the efficacy of four practical methods, including few-shot learning, Decoding by Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method and the prompt-based tool learning method for a function to generate a query command. Finally, our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks. Therefore, there is an urgent need to call for research efforts in mitigating LLMs' hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5838b56f2c7ca3dd946428dae07bdc26a9265c67.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "3c3f5af1aee19bf0093c40f35a120744d099723e.pdf": {
        "title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models",
        "authors": [
            "Ziwei Ji",
            "Yuzhe Gu",
            "Wenwei Zhang",
            "Chengqi Lyu",
            "Dahua Lin",
            "Kai Chen"
        ],
        "published_date": "2024",
        "abstract": "Reducing the `$\\textit{hallucination}$' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community. Thus, we present $\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical $\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative Question Answering. Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of ~12k sentence-level annotations for ~4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/3c3f5af1aee19bf0093c40f35a120744d099723e.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "dd6b124606e3696dcddc93c889a824feaa322117.pdf": {
        "title": "Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization",
        "authors": [
            "Yuhan Fu",
            "Ruobing Xie",
            "Xingwu Sun",
            "Zhanhui Kang",
            "Xirong Li"
        ],
        "published_date": "2024",
        "abstract": "Multimodal Large Language Models (MLLMs) are known to hallucinate, which limits their practical applications. Recent works have attempted to apply Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but have shown inconsistent improvements in mitigating hallucinations. To address this issue more effectively, we introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes. Specifically, we develop three types of preference pair data targeting the following causes of MLLM hallucinations: (1) insufficient visual capabilities, (2) long context generation, and (3) multimodal conflicts. Experimental results demonstrate that our method achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art (SOTA) methods and highlighting the potential of our approach. Ablation studies and in-depth analyses further confirm the effectiveness of our method and suggest the potential for further improvements through scaling up.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/dd6b124606e3696dcddc93c889a824feaa322117.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "94c81ec4364d63fe67f98098547d0d09f063931d.pdf": {
        "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models",
        "authors": [
            "Mengfei Liang",
            "Archish Arun",
            "Zekun Wu",
            "Cristian Munoz",
            "Jonathan Lutch",
            "Emre Kazim",
            "A. Koshiyama",
            "Philip C. Treleaven"
        ],
        "published_date": "2024",
        "abstract": "Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. It automates test set creation from any corpus, ensuring high data quality, diversity, and cost-efficiency through techniques like batch processing, weighted sampling, and counterfactual validation. THaMES assesses a model's ability to detect and reduce hallucinations across various tasks, including text generation and binary classification, applying optimal mitigation strategies like In-Context Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base of academic papers, political news, and Wikipedia reveal that commercial models like GPT-4o benefit more from RAG than ICL, while open-weight models like Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT significantly enhances the performance of Llama-3.1-8B-Instruct in both evaluation tasks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/94c81ec4364d63fe67f98098547d0d09f063931d.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5.pdf": {
        "title": "Skip \\n: A Simple Method to Reduce Hallucination in Large Vision-Language Models",
        "authors": [
            "Zongbo Han",
            "Zechen Bai",
            "Haiyang Mei",
            "Qianli Xu",
            "Changqing Zhang",
            "Mike Zheng Shou"
        ],
        "published_date": "2024",
        "abstract": "Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks (\\n\\n), where the content before and after '\\n\\n' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '\\n\\n' should be obviously different from the preceding contents with less hallucinatory descriptions, thereby increasing the probability of hallucinatory descriptions subsequent to the '\\n\\n'. We have validated this hypothesis on multiple publicly available LVLMs. Besides, we find that deliberately inserting '\\n\\n' at the generated description can induce more hallucinations. A simple method is proposed to effectively mitigate the hallucination of LVLMs by skipping the output of '\\n'.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "431a4e7e89863b038069335baa80c3e489538214.pdf": {
        "title": "VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation",
        "authors": [
            "Ruiyang Zhang",
            "Hu Zhang",
            "Zhedong Zheng"
        ],
        "published_date": "2024",
        "abstract": "Given the higher information load processed by large vision-language models (LVLMs) compared to single-modal LLMs, detecting LVLM hallucinations requires more human and time expense, and thus rise a wider safety concerns. In this paper, we introduce VL-Uncertainty, the first uncertainty-based framework for detecting hallucinations in LVLMs. Different from most existing methods that require ground-truth or pseudo annotations, VL-Uncertainty utilizes uncertainty as an intrinsic metric. We measure uncertainty by analyzing the prediction variance across semantically equivalent but perturbed prompts, including visual and textual data. When LVLMs are highly confident, they provide consistent responses to semantically equivalent queries. However, when uncertain, the responses of the target LVLM become more random. Considering semantically similar answers with different wordings, we cluster LVLM responses based on their semantic content and then calculate the cluster distribution entropy as the uncertainty measure to detect hallucination. Our extensive experiments on 10 LVLMs across four benchmarks, covering both free-form and multi-choice tasks, show that VL-Uncertainty significantly outperforms strong baseline methods in hallucination detection.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/431a4e7e89863b038069335baa80c3e489538214.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "45ed6263e02d219f0542ac743b9c9f837154a58d.pdf": {
        "title": "Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval",
        "authors": [
            "Mengjia Niu",
            "Hao Li",
            "Jie Shi",
            "Hamed Haddadi",
            "Fan Mo"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains, although their susceptibility to hallucination poses significant challenges for their deployment in critical areas such as healthcare. To address this issue, retrieving relevant facts from knowledge graphs (KGs) is considered a promising method. Existing KG-augmented approaches tend to be resource-intensive, requiring multiple rounds of retrieval and verification for each factoid, which impedes their application in real-world scenarios. In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR) to augment the factuality of LLMs' responses with less retrieval efforts in the medical field. Our approach leverages the attribution of next-token predictive probability distributions across different tokens, and various model layers to primarily identify tokens with a high potential for hallucination, reducing verification rounds by refining knowledge triples associated with these tokens. Moreover, we rectify inaccurate content using retrieved knowledge in the post-processing stage, which improves the truthfulness of generated responses. Experimental results on a medical dataset demonstrate that our approach can enhance the factual capability of LLMs across various foundational models as evidenced by the highest scores on truthfulness.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/45ed6263e02d219f0542ac743b9c9f837154a58d.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "ae1e48a74cb2f313e8e99c82f0aa4487b0805002.pdf": {
        "title": "Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization",
        "authors": [
            "Yue Dong",
            "J. Wieting",
            "Pat Verga"
        ],
        "published_date": "2022",
        "abstract": "Despite recent advances in abstractive summarization, current summarization systems still suffer from content hallucinations where models generate text that is either irrelevant or contradictory to the source document. However, prior work has been predicated on the assumption that any generated facts not appearing explicitly in the source are undesired hallucinations. Methods have been proposed to address this scenario by ultimately improving `faithfulness' to the source document, but in reality, there is a large portion of entities in the gold reference targets that are not directly in the source. In this work, we show that these entities are not aberrations, but they instead require utilizing external world knowledge to infer reasoning paths from entities in the source. We show that by utilizing an external knowledge base, we can improve the faithfulness of summaries without simply making them more extractive, and additionally, we show that external knowledge bases linked from the source can benefit the factuality of generated summaries.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/ae1e48a74cb2f313e8e99c82f0aa4487b0805002.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "1cc347c97a8f9d30edc809e4f207d64c7b8247b4.pdf": {
        "title": "GraphArena: Evaluating and Exploring Large Language Models on Graph Computation",
        "authors": [
            "Jianheng Tang",
            "Qifan Zhang",
            "Yuhan Li",
            "Nuo Chen",
            "Jia Li"
        ],
        "published_date": "2024",
        "abstract": "The ``arms race'' of Large Language Models (LLMs) demands new benchmarks to examine their progresses. In this paper, we introduce GraphArena, a benchmarking tool designed to evaluate LLMs on real-world graph computational problems. It offers a suite of four polynomial-time tasks (e.g., Shortest Distance) and six NP-complete challenges (e.g., Traveling Salesman Problem). GraphArena features a rigorous evaluation framework that classifies LLM outputs as correct, suboptimal (feasible but not optimal), hallucinatory (properly formatted but infeasible), or missing. Evaluation of over 10 LLMs reveals that even top-performing LLMs struggle with larger, more complex graph problems and exhibit hallucination issues. We further explore four potential solutions to address this issue and improve LLMs on graph computation, including chain-of-thought prompting, instruction tuning, code writing, and scaling test-time compute, each demonstrating unique strengths and limitations. GraphArena complements the existing LLM benchmarks and is open-sourced at https://github.com/squareRoot3/GraphArena.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1cc347c97a8f9d30edc809e4f207d64c7b8247b4.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "bb1083425517bdac8d9a6438fcf5032543acb20e.pdf": {
        "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models",
        "authors": [
            "Junyan Wang",
            "Yi Zhou",
            "Guohai Xu",
            "Pengcheng Shi",
            "Chenlin Zhao",
            "Haiyang Xu",
            "Qinghao Ye",
            "Mingshi Yan",
            "Ji Zhang",
            "Jihua Zhu",
            "Jitao Sang",
            "Haoyu Tang"
        ],
        "published_date": "2023",
        "abstract": "Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation hallucination data will be made public soon.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/bb1083425517bdac8d9a6438fcf5032543acb20e.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "7bcd5c0b17560ee560aec903ea42487a1a54e5d9.pdf": {
        "title": "BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models",
        "authors": [
            "Moon Ye-Bin",
            "Nam Hyeon-Woo",
            "Wonseok Choi",
            "Tae-Hyun Oh"
        ],
        "published_date": "2024",
        "abstract": "Vision language models (VLMs) perceive the world through a combination of a visual encoder and a large language model (LLM). The visual encoder, pre-trained on large-scale vision-text datasets, provides zero-shot generalization to visual data, and the LLM endows its high reasoning ability to VLMs. It leads VLMs to achieve high performance on wide benchmarks without fine-tuning, exhibiting zero or few-shot capability. However, recent studies show that VLMs are vulnerable to hallucination. This undesirable behavior degrades reliability and credibility, thereby making users unable to fully trust the output from VLMs. To enhance trustworthiness and better tackle the hallucination of VLMs, we curate a new evaluation dataset, called the BEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID). Unlike prior works that focus only on constructing questions and answers, the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes. This allows us to clearly assess whether VLMs correctly understand a given scene by observing the ability to perceive changes. We also visualize image-wise object relationship by virtue of our two-axis view: vision and text. Upon evaluating VLMs with our dataset, we observed that our metrics reveal different aspects of VLM hallucination that have not been reported before. Project page: \\url{https://beafbench.github.io/}",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/7bcd5c0b17560ee560aec903ea42487a1a54e5d9.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 0,
        "score": 0
    },
    "25243632a6159c19db280e2f0064aa59562a518a.pdf": {
        "title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models",
        "authors": [
            "Hanxing Ding",
            "Liang Pang",
            "Zihao Wei",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published_date": "2024",
        "abstract": "Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations. While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations. A careful and balanced integration of the parametric knowledge within LLMs with external information is crucial to alleviate hallucinations. In this study, we present Rowen, a novel approach that enhances LLMs with a selective retrieval augmentation process tailored to address hallucinated outputs. This process is governed by a multilingual semantic-aware detection module, which evaluates the consistency of the perturbed responses across various languages for the same queries. Upon detecting inconsistencies indicative of hallucinations, Rowen activates the retrieval of external information to rectify the model outputs. Rowen adeptly harmonizes the intrinsic parameters in LLMs with external knowledge sources, effectively mitigating hallucinations by ensuring a balanced integration of internal reasoning and external evidence. Through a comprehensive empirical analysis, we demonstrate that Rowen surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/25243632a6159c19db280e2f0064aa59562a518a.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "6947893915861e8c30bc6b010eb1faf0d82f0a19.pdf": {
        "title": "Logical Consistency of Large Language Models in Fact-checking",
        "authors": [
            "Bishwamittra Ghosh",
            "Sarah Hasan",
            "Naheed Anjum Arafat",
            "Arijit Khan"
        ],
        "published_date": "2024",
        "abstract": "In recent years, large language models (LLMs) have demonstrated significant success in performing varied natural language tasks such as language translation, question-answering, summarizing, fact-checking, etc. Despite LLMs' impressive ability to generate human-like texts, LLMs are infamous for their inconsistent responses - a meaning-preserving change in the input query results in an inconsistent response and attributes to vulnerabilities of LLMs such as hallucination. Consequently, existing research focuses on simple paraphrasing-based consistency assessment of LLMs, and ignores complex queries that necessitate an even better understanding of logical reasoning by an LLM. Our work therefore addresses the logical inconsistency of LLMs under complex logical queries with primitive logical operators, e.g., negation, conjunction, and disjunction. As a test bed, we consider retrieval-augmented LLMs on a fact-checking task involving propositional logic queries from knowledge graphs (KGs). Our contributions are threefold. Benchmark: We introduce three logical fact-checking datasets over KGs for community development towards logically consistent LLMs. Assessment: We propose consistency measures of LLMs on propositional logic queries and demonstrate that existing LLMs lack logical consistency, especially on complex queries. Improvement: We employ supervised fine-tuning to improve the logical consistency of LLMs on the complex fact-checking task with KG contexts. We have made our source code and benchmarks available.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/6947893915861e8c30bc6b010eb1faf0d82f0a19.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "f8a642fbb51e0b0ae4774781309545d15d6d9b11.pdf": {
        "title": "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning",
        "authors": [
            "Yiyang Zhou",
            "Chenhang Cui",
            "Rafael Rafailov",
            "Chelsea Finn",
            "Huaxiu Yao"
        ],
        "published_date": "2024",
        "abstract": "Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucinations into the correct answer. Second, we distort the image to trigger the inherent hallucination behavior of the VLLM. This is an automated approach, which does not rely on human data generation or require a perfect expert, which makes it easily scalable. Finally, both of these generation strategies are integrated into an RLHF pipeline via Direct Preference Optimization. In experiments across broad benchmarks, we show that we can not only reduce hallucinations, but improve model performance across standard benchmarks, outperforming prior approaches. Our data and code are available at https://github.com/YiyangZhou/POVID.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/f8a642fbb51e0b0ae4774781309545d15d6d9b11.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "3b0792f6d7f6aa6aadd316e73943116afef2979b.pdf": {
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
        "authors": [
            "Logesh Kumar Umapathi",
            "Ankit Pal",
            "Malaikannan Sankarasubbu"
        ],
        "published_date": "2023",
        "abstract": "This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs\u2019 problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/3b0792f6d7f6aa6aadd316e73943116afef2979b.pdf",
        "venue": "Conference on Computational Natural Language Learning",
        "citationCount": 0,
        "score": 0
    },
    "7c1707db9aafd209aa93db3251e7ebd593d55876.pdf": {
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "authors": [
            "Potsawee Manakul",
            "Adian Liusie",
            "M. Gales"
        ],
        "published_date": "2023",
        "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/7c1707db9aafd209aa93db3251e7ebd593d55876.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "88e52de2320e06c7556795be43b38c85a9800e5a.pdf": {
        "title": "Mitigating Multilingual Hallucination in Large Vision-Language Models",
        "authors": [
            "Xiaoye Qu",
            "Mingyang Song",
            "Wei Wei",
            "Jianfeng Dong",
            "Yu Cheng"
        ],
        "published_date": "2024",
        "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable capabilities across a wide range of tasks, they suffer from hallucination problems, where models generate plausible yet incorrect answers given the input image-query pair. This hallucination phenomenon is even more severe when querying the image in non-English languages, while existing methods for mitigating hallucinations in LVLMs only consider the English scenarios. In this paper, we make the first attempt to mitigate this important multilingual hallucination in LVLMs. With thorough experiment analysis, we found that multilingual hallucination in LVLMs is a systemic problem that could arise from deficiencies in multilingual capabilities or inadequate multimodal abilities. To this end, we propose a two-stage Multilingual Hallucination Removal (MHR) framework for LVLMs, aiming to improve resistance to hallucination for both high-resource and low-resource languages. Instead of relying on the intricate manual annotations of multilingual resources, we fully leverage the inherent capabilities of the LVLM and propose a novel cross-lingual alignment method, which generates multiple responses for each image-query input and then identifies the hallucination-aware pairs for each language. These data pairs are finally used for direct preference optimization to prompt the LVLMs to favor non-hallucinating responses. Experimental results show that our MHR achieves a substantial reduction in hallucination generation for LVLMs. Notably, on our extended multilingual POPE benchmark, our framework delivers an average increase of 19.0% in accuracy across 13 different languages. Our code and model weights are available at https://github.com/ssmisya/MHR",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/88e52de2320e06c7556795be43b38c85a9800e5a.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "8ff45750057cc9452ae09aef6b9dfee3bd84b083.pdf": {
        "title": "Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance",
        "authors": [
            "Linxi Zhao",
            "Yihe Deng",
            "Weitong Zhang",
            "Quanquan Gu"
        ],
        "published_date": "2024",
        "abstract": "The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs to rectify the outputs of LVLMs. However, these approaches require either costly training or fine-tuning, or API access to proprietary LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE (MARINE), a framework that is both training-free and API-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across 5 popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations. We release our code at https://github.com/Linxi-ZHAO/MARINE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/8ff45750057cc9452ae09aef6b9dfee3bd84b083.pdf",
        "venue": "",
        "citationCount": 0,
        "score": 0
    },
    "4d608203639087e0fe3c5d2b7a374941dd182cb7.pdf": {
        "title": "Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models",
        "authors": [
            "Chaozhuo Li",
            "Pengbo Wang",
            "Chenxu Wang",
            "Litian Zhang",
            "Zheng Liu",
            "Qiwei Ye",
            "Yuanbo Xu",
            "Feiran Huang",
            "Xi Zhang",
            "Philip S. Yu"
        ],
        "published_date": "2025",
        "abstract": "Edgar Allan Poe noted,\"Truth often lurks in the shadow of error,\"highlighting the deep complexity intrinsic to the interplay between truth and falsehood, notably under conditions of cognitive and informational asymmetry. This dynamic is strikingly evident in large language models (LLMs). Despite their impressive linguistic generation capabilities, LLMs sometimes produce information that appears factually accurate but is, in reality, fabricated, an issue often referred to as'hallucinations'. The prevalence of these hallucinations can mislead users, affecting their judgments and decisions. In sectors such as finance, law, and healthcare, such misinformation risks causing substantial economic losses, legal disputes, and health risks, with wide-ranging consequences.In our research, we have methodically categorized, analyzed the causes, detection methods, and solutions related to LLM hallucinations. Our efforts have particularly focused on understanding the roots of hallucinations and evaluating the efficacy of current strategies in revealing the underlying logic, thereby paving the way for the development of innovative and potent approaches. By examining why certain measures are effective against hallucinations, our study aims to foster a comprehensive approach to tackling this issue within the domain of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4d608203639087e0fe3c5d2b7a374941dd182cb7.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "db646f0eb37bb97fda3a89f94c81e507f9421ba9.pdf": {
        "title": "Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization",
        "authors": [
            "Beitao Chen",
            "Xinyu Lyu",
            "Lianli Gao",
            "Jingkuan Song",
            "Hengtao Shen"
        ],
        "published_date": "2024",
        "abstract": "Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnect between the generated text and the corresponding images. Almost all current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones. However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations. To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs. Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/db646f0eb37bb97fda3a89f94c81e507f9421ba9.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "c14010990c9d75a6e836e1c86d42f405a5d3d0a6.pdf": {
        "title": "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models",
        "authors": [
            "Pranab Sahoo",
            "Prabhash Meharia",
            "Akash Ghosh",
            "Sriparna Saha",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "published_date": "2024",
        "abstract": "The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount. This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities. By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners. Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research in this pivotal area.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c14010990c9d75a6e836e1c86d42f405a5d3d0a6.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92.pdf": {
        "title": "Reference-free Hallucination Detection for Large Vision-Language Models",
        "authors": [
            "Qing Li",
            "Chenyang Lyu",
            "Jiahui Geng",
            "Derui Zhu",
            "Maxim Panov",
            "Fakhri Karray"
        ],
        "published_date": "2024",
        "abstract": "Large vision-language models (LVLMs) have made significant progress in recent years. While LVLMs exhibit excellent ability in language understanding, question answering, and conversations of visual inputs, they are prone to producing hallucinations. While several methods are proposed to evaluate the hallucinations in LVLMs, most are reference-based and depend on external tools, which complicates their practical application. To assess the viability of alternative methods, it is critical to understand whether the reference-free approaches, which do not rely on any external tools, can efficiently detect hallucinations. Therefore, we initiate an exploratory study to demonstrate the effectiveness of different reference-free solutions in detecting hallucinations in LVLMs. In particular, we conduct an extensive study on three kinds of techniques: uncertainty-based, consistency-based, and supervised uncertainty quantification methods on four representative LVLMs across two different tasks. The empirical results show that the reference-free approaches are capable of effectively detecting non-factual responses in LVLMs, with the supervised uncertainty quantification method outperforming the others, achieving the best performance across different settings.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "2feb4d83da1892db3934fcf406c8beb6cd10ded1.pdf": {
        "title": "VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding",
        "authors": [
            "Jiaqi Wang",
            "Yifei Gao",
            "Jitao Sang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in multimodal task reasoning. However, they often generate responses that appear plausible yet do not accurately reflect the visual content, a phenomenon known as hallucination. Recent approaches have introduced training-free methods to mitigate hallucinations by adjusting the decoding strategy during the inference stage, typically attributing hallucinations to the language model itself. Our analysis, however, reveals that distortions in the visual encoding process significantly affect the model's reasoning capabilities. Specifically, earlier visual layers may retain key features but gradually distort as the information propagates toward the output layer. Building on these insights, we propose a novel hallucination-mitigation method from the visual encoding perspective: \\textbf{V}isu\\textbf{a}l \\textbf{L}ayer Fus\\textbf{i}on Contrastive \\textbf{D}ecoding (\\textbf{VaLiD}). This method utilizes uncertainty to guide the visual layer selection, correcting distortions in the visual encoding process and thereby enhancing the reliability of the generated content. Experimental results demonstrate the effectiveness of VaLiD in mitigating hallucinations across various benchmarks, achieving state-of-the-art performance when compared to baseline methods. Codes are available at \\href{https://github.com/RicardoLuL/VaLiD_LVLMs_hallucinations}{Github}.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/2feb4d83da1892db3934fcf406c8beb6cd10ded1.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "9e2037d7d2f8222a7be86d2471eda895c8040ff5.pdf": {
        "title": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models",
        "authors": [
            "Han Qiu",
            "Jiaxing Huang",
            "Peng Gao",
            "Qi Qin",
            "Xiaoqin Zhang",
            "Ling Shao",
            "Shijian Lu"
        ],
        "published_date": "2024",
        "abstract": "Hallucination, a phenomenon where multimodal large language models~(MLLMs) tend to generate textual responses that are plausible but unaligned with the image, has become one major hurdle in various MLLM-related applications. Several benchmarks have been created to gauge the hallucination levels of MLLMs, by either raising discriminative questions about the existence of objects or introducing LLM evaluators to score the generated text from MLLMs. However, the discriminative data largely involve simple questions that are not aligned with real-world text, while the generative data involve LLM evaluators that are computationally intensive and unstable due to their inherent randomness. We propose LongHalQA, an LLM-free hallucination benchmark that comprises 6K long and complex hallucination text. LongHalQA is featured by GPT4V-generated hallucinatory data that are well aligned with real-world scenarios, including object/image descriptions and multi-round conversations with 14/130 words and 189 words, respectively, on average. It introduces two new tasks, hallucination discrimination and hallucination completion, unifying both discriminative and generative evaluations in a single multiple-choice-question form and leading to more reliable and efficient evaluations without the need for LLM evaluators. Further, we propose an advanced pipeline that greatly facilitates the construction of future hallucination benchmarks with long and complex questions and descriptions. Extensive experiments over multiple recent MLLMs reveal various new challenges when they are handling hallucinations with long and complex textual data. Dataset and evaluation code are available at https://github.com/hanqiu-hq/LongHalQA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/9e2037d7d2f8222a7be86d2471eda895c8040ff5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05.pdf": {
        "title": "Hallucination of Multimodal Large Language Models: A Survey",
        "authors": [
            "Zechen Bai",
            "Pichao Wang",
            "Tianjun Xiao",
            "Tong He",
            "Zongbo Han",
            "Zheng Zhang",
            "Mike Zheng Shou"
        ],
        "published_date": "2024",
        "abstract": "This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "80248c8c7cbb5bb1d2a508001108f3f15bb60430.pdf": {
        "title": "Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models",
        "authors": [
            "Jun Wu",
            "Q. Liu",
            "Ding Wang",
            "Jinghao Zhang",
            "Shu Wu",
            "Liang Wang",
            "Tien-Ping Tan"
        ],
        "published_date": "2024",
        "abstract": "Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/80248c8c7cbb5bb1d2a508001108f3f15bb60430.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "206400aba5f12f734cdd2e4ab48ef6014ea60773.pdf": {
        "title": "Evaluating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Yifan Li",
            "Yifan Du",
            "Kun Zhou",
            "Jinpeng Wang",
            "Wayne Xin Zhao",
            "Ji-rong Wen"
        ],
        "published_date": "2023",
        "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/206400aba5f12f734cdd2e4ab48ef6014ea60773.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "bf54792cf01761a2c51ac3410287797fff665cd4.pdf": {
        "title": "EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models",
        "authors": [
            "Shangyu Xing",
            "Fei Zhao",
            "Zhen Wu",
            "Tuo An",
            "Weihao Chen",
            "Chunhui Li",
            "Jianbing Zhang",
            "Xinyu Dai"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we propose an efficient fine-grained unlearning framework (EFUF), which performs gradient ascent utilizing three tailored losses to eliminate hallucinations without paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/bf54792cf01761a2c51ac3410287797fff665cd4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5.pdf": {
        "title": "Training Dynamics for Text Summarization Models",
        "authors": [
            "Tanya Goyal",
            "Jiacheng Xu",
            "J. Li",
            "Greg Durrett"
        ],
        "published_date": "2021",
        "abstract": "Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on summarization. Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process. We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/23f1d4b46bc7c8f357a5a89144d5d32af7be13a5.pdf",
        "venue": "Findings",
        "citationCount": 0,
        "score": 0
    },
    "03e2f5cded1b1d92dc8e693e0e93ad466f6cc352.pdf": {
        "title": "Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs",
        "authors": [
            "Peng Ding",
            "Jingyu Wu",
            "Jun Kuang",
            "Dan Ma",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Shi Chen",
            "Jiajun Chen",
            "Shujian Huang"
        ],
        "published_date": "2024",
        "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable performance on various visual-language understanding and generation tasks. However, MLLMs occasionally generate content inconsistent with the given images, which is known as \"hallucination\". Prior works primarily center on evaluating hallucination using standard, unperturbed benchmarks, which overlook the prevalent occurrence of perturbed inputs in real-world scenarios-such as image cropping or blurring-that are critical for a comprehensive assessment of MLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI, the first benchmark designed to evaluate Hallucination in MLLMs within Perturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios, containing 1,260 perturbed images from 11 object types. Each image is accompanied by detailed annotations, which include fine-grained hallucination types, such as existence, attribute, and relation. We equip these annotations with a rich set of questions, making Hallu-PI suitable for both discriminative and generative tasks. Extensive experiments on 12 mainstream MLLMs, such as GPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant hallucinations on Hallu-PI, which is not observed in unperturbed scenarios. Furthermore, our research reveals a severe bias in MLLMs' ability to handle different types of hallucinations. We also design two baselines specifically for perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope that our study will bring researchers' attention to the limitations of MLLMs when dealing with perturbed inputs, and spur further investigations to address this issue. Our code and datasets are publicly available at https://github.com/NJUNLP/Hallu-PI.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/03e2f5cded1b1d92dc8e693e0e93ad466f6cc352.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 0,
        "score": 0
    },
    "93c525267e93c78309a5b28a3eb0780704125744.pdf": {
        "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Yiyang Zhou",
            "Chenhang Cui",
            "Jaehong Yoon",
            "Linjun Zhang",
            "Zhun Deng",
            "Chelsea Finn",
            "Mohit Bansal",
            "Huaxiu Yao"
        ],
        "published_date": "2023",
        "abstract": "Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top. Our data and code are available at https://github.com/YiyangZhou/LURE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/93c525267e93c78309a5b28a3eb0780704125744.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2.pdf": {
        "title": "Visual Hallucinations of Multi-modal Large Language Models",
        "authors": [
            "Wen Huang",
            "Hongbin Liu",
            "Minxin Guo",
            "N. Gong"
        ],
        "published_date": "2024",
        "abstract": "Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "171807aeeb88f0c7983bc6cc960b5605441d7121.pdf": {
        "title": "Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation",
        "authors": [
            "Xiaoye Qu",
            "Qiyuan Chen",
            "Wei Wei",
            "Jiashuo Sun",
            "Jianfeng Dong"
        ],
        "published_date": "2024",
        "abstract": "Despite the remarkable ability of large vision-language models (LVLMs) in image comprehension, these models frequently generate plausible yet factually incorrect responses, a phenomenon known as hallucination. Recently, in large language models (LLMs), augmenting LLMs by retrieving information from external knowledge resources has been proven as a promising solution to mitigate hallucinations. However, the retrieval augmentation in LVLM significantly lags behind the widespread applications of LVLM. Moreover, when transferred to augmenting LVLMs, sometimes the hallucination degree of the model is even exacerbated. Motivated by the research gap and counter-intuitive phenomenon, we introduce a novel framework, the Active Retrieval-Augmented large vision-language model (ARA), specifically designed to address hallucinations by incorporating three critical dimensions: (i) dissecting the retrieval targets based on the inherent hierarchical structures of images. (ii) pinpointing the most effective retrieval methods and filtering out the reliable retrieval results. (iii) timing the retrieval process to coincide with episodes of low certainty, while circumventing unnecessary retrieval during periods of high certainty. To assess the capability of our proposed ARA model in reducing hallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and mPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by utilizing fitting retrieval mechanisms and timing the retrieval judiciously, we can effectively mitigate the hallucination problem. We hope that this study can provide deeper insights into how to adapt the retrieval augmentation to LVLMs for reducing hallucinations with more effective retrieval and minimal retrieval occurrences.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/171807aeeb88f0c7983bc6cc960b5605441d7121.pdf",
        "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
        "citationCount": 0,
        "score": 0
    },
    "f208ea909fa7f54fea82def9a92fd81dfc758c39.pdf": {
        "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
        "authors": [
            "H. Trivedi",
            "Niranjan Balasubramanian",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "published_date": "2022",
        "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/f208ea909fa7f54fea82def9a92fd81dfc758c39.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d.pdf": {
        "title": "A Survey of Hallucination Problems Based on Large Language Models",
        "authors": [
            "Xinxin Liu"
        ],
        "published_date": "2024",
        "abstract": "Abstract. Large language models (LLM) have made significant achievements in the field of natural language processing, but the generated text often contains content that is inconsistent with the real world or user input, known as hallucinations. This article investigates the current situation of hallucinations in LLM, including the definition, types, causes, and solutions of hallucinations. Illusions are divided into different types such as factual and faithful, mainly caused by factors such as training data defects, low utilization of facts, and randomness in the decoding process. The phenomenon of hallucinations poses a threat to the reliability of LLM, especially in fields such as healthcare, finance, and law, which may lead to serious consequences. To address this issue, this article investigates methods such as managing training datasets, knowledge editing, and enhancing retrieval generation. Future research should classify and evaluate illusions more finely, explore multimodal strategies, enhance model stability, and integrate human intelligence and artificial intelligence to jointly address challenges, promoting the continuous progress of LLM.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d.pdf",
        "venue": "Applied and Computational Engineering",
        "citationCount": 0,
        "score": 0
    },
    "73020a07af4cfc42286e299097a0e35d2fe71a6c.pdf": {
        "title": "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization",
        "authors": [
            "Yuxi Xie",
            "Guanzhen Li",
            "Xiao Xu",
            "Min-Yen Kan"
        ],
        "published_date": "2024",
        "abstract": "Large vision-language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content. Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs. We tackle this issue of hallucination by mitigating such over-reliance through preference learning. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time. To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples. Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks. Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context. Our code is publicly available at https://github.com/YuxiXie/V-DPO.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/73020a07af4cfc42286e299097a0e35d2fe71a6c.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    }
}