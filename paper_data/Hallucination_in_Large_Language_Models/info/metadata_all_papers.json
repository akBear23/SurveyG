{
    "99832586d55f540f603637e458a292406a0ed75d.pdf": {
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "authors": [
            "Shunyu Yao",
            "Jeffrey Zhao",
            "Dian Yu",
            "Nan Du",
            "Izhak Shafran",
            "Karthik Narasimhan",
            "Yuan Cao"
        ],
        "published_date": "2022",
        "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/99832586d55f540f603637e458a292406a0ed75d.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 3859,
        "score": 1286.3333333333333,
        "summary": "Here's a focused summary of the paper \"ReAct: Synergizing Reasoning and Acting in Language Models\" by `\\cite{yao20229uz}` for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) have shown impressive capabilities in both reasoning (e.g., chain-of-thought prompting) and acting (e.g., action plan generation), but these abilities have primarily been studied and applied in isolation. This separation limits their effectiveness in complex tasks requiring both.\n    *   **Importance and Challenge**:\n        *   **Reasoning-only (CoT)**: Suffers from issues like fact hallucination and error propagation because it relies solely on internal representations and is not grounded in the external world, preventing reactive reasoning or knowledge updates `\\cite{yao20229uz}`.\n        *   **Acting-only**: Lacks the ability for LLMs to reason abstractly about high-level goals, maintain a working memory, or handle exceptions, leading to less robust decision-making in interactive environments `\\cite{yao20229uz}`.\n        *   **Human-like Intelligence**: Humans seamlessly combine verbal reasoning (inner speech) with task-oriented actions for self-regulation, strategization, and robust decision-making, even under uncertainty. Emulating this synergy in LLMs is a key challenge for general task solving `\\cite{yao20229uz}`.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `\\cite{yao20229uz}` positions ReAct as a paradigm that bridges the gap between Chain-of-Thought (CoT) reasoning and action generation in LLMs.\n    *   **Limitations of Previous Solutions**:\n        *   **Chain-of-Thought (CoT)**: While effective for multi-step reasoning, CoT is a \"static black box\" that uses internal representations, making it ungrounded, prone to hallucination, and unable to update its knowledge based on external feedback `\\cite{yao20229uz}`.\n        *   **Action-only approaches**: These focus on predicting domain-specific actions from textual observations but typically do not employ LLMs for abstract high-level reasoning or maintaining a working memory to support acting `\\cite{yao20229uz}`. Some limited forms of verbal reasoning exist but are not synergistic.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{yao20229uz}` introduces ReAct, a general paradigm that prompts LLMs to generate both verbal reasoning traces (\"thoughts\") and task-specific actions in an *interleaved manner*.\n    *   **Novelty**:\n        *   **Augmented Action Space**: ReAct augments the agent's action space to include language (`^A = A \\cup L`), where `L` is the space of language for \"thoughts.\" Thoughts do not affect the external environment but compose useful information by reasoning over the current context and updating it to support future reasoning or acting `\\cite{yao20229uz}`.\n        *   **Dynamic Synergy**: This interleaved generation allows for dynamic reasoning to create, maintain, and adjust high-level plans for acting (\"reason to act\"), while simultaneously interacting with external environments (e.g., Wikipedia API) to incorporate additional information into reasoning (\"act to reason\") `\\cite{yao20229uz}`.\n        *   **Few-shot Prompting**: ReAct primarily operates by prompting a frozen LLM (PaLM-540B) with few-shot in-context examples, demonstrating strong generalization without extensive training `\\cite{yao20229uz}`.\n        *   **Flexible Thought Occurrence**: For reasoning-intensive tasks, thoughts and actions alternate densely. For decision-making tasks with many actions, thoughts can appear sparsely and asynchronously, allowing the model to decide when reasoning is needed `\\cite{yao20229uz}`.\n\n*   **Key Technical Contributions**\n    *   **Novel Paradigm**: Introduction of ReAct, a prompt-based paradigm to synergize reasoning and acting in language models for general task solving `\\cite{yao20229uz}`.\n    *   **Extensive Empirical Validation**: Demonstrates the advantage of ReAct in a few-shot learning setup across diverse benchmarks over prior approaches that perform either reasoning or action generation in isolation `\\cite{yao20229uz}`.\n    *   **Systematic Ablations and Analysis**: Provides insights into the importance of acting in reasoning tasks and reasoning in interactive tasks `\\cite{yao20229uz}`.\n    *   **Combined Internal and External Knowledge**: Proposes and evaluates methods (e.g., `CoT-SC!ReAct`, `ReAct!CoT-SC`) to combine ReAct's external grounding with CoT-SC's internal reasoning for enhanced performance `\\cite{yao20229uz}`.\n    *   **Interpretability and Controllability**: The interleaved thoughts provide an interpretable, human-aligned, and diagnosable decision-making process, allowing humans to inspect reasoning and factual correctness `\\cite{yao20229uz}`.\n\n*   **Experimental Validation**\n    *   **Tasks**: Evaluated on four diverse benchmarks:\n        *   **Knowledge-intensive Reasoning**: HotpotQA (multi-hop QA) and FEVER (fact verification), using a simple Wikipedia API for external interaction `\\cite{yao20229uz}`.\n        *   **Interactive Decision Making**: ALFWorld (text-based games) and WebShop (webpage navigation) `\\cite{yao20229uz}`.\n    *   **Baselines**: Compared against Standard prompting, Chain-of-Thought (CoT), CoT with Self-Consistency (CoT-SC), and Act-only prompting `\\cite{yao20229uz}`. For interactive tasks, also compared against imitation and reinforcement learning methods.\n    *   **Key Performance Metrics & Results**:\n        *   **HotpotQA & FEVER**: ReAct, with access to a Wikipedia API, outperforms vanilla action generation models and is competitive with CoT. The best approach is a combination of ReAct and CoT-SC (`CoT-SC!ReAct` or `ReAct!CoT-SC`), achieving 35.1 EM on HotpotQA and 64.6 Acc on FEVER, demonstrating the synergy of internal and external knowledge `\\cite{yao20229uz}`.\n        *   **ALFWorld & WebShop**: Few-shot (one or two examples) ReAct prompting significantly outperforms imitation and reinforcement learning methods trained with 10^3-10^5 task instances, with absolute success rate improvements of 34% and 10% respectively `\\cite{yao20229uz}`.\n        *   **Interpretability**: ReAct generates human-like task-solving trajectories that are more interpretable and trustworthy than baselines without reasoning traces `\\cite{yao20229uz}`.\n\n*   **Limitations & Scope**\n    *   **Prompting Setup Limitations**: The current prompting setup has limited support for complex reasoning and acting behaviors `\\cite{yao20229uz}`.\n    *   **Data Scarcity**: Manual annotation of reasoning traces and actions at scale is challenging, suggesting a need for bootstrapping or other data generation methods `\\cite{yao20229uz}`.\n    *   **Future Scaling**: The paper notes that scaling up ReAct with additional training data (e.g., through finetuning) and combining it with complementary paradigms like reinforcement learning could further unlock its potential `\\cite{yao20229uz}`.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `\\cite{yao20229uz}` significantly advances the state-of-the-art in LLM capabilities by demonstrating that synergizing reasoning and acting leads to more robust, grounded, and performant agents for diverse tasks, especially in few-shot settings.\n    *   **Improved Robustness and Grounding**: ReAct overcomes prevalent issues of hallucination and error propagation in CoT by grounding reasoning in external information obtained through actions `\\cite{yao20229uz}`.\n    *   **Enhanced Interpretability**: The explicit reasoning traces make LLM decision-making processes more transparent, interpretable, and diagnosable for humans `\\cite{yao20229uz}`.\n    *   **Potential Impact on Future Research**: ReAct opens new avenues for research into more general, human-aligned, and controllable AI agents, suggesting that combining LLMs with interactive environments and explicit reasoning is a powerful direction for developing more capable autonomous systems `\\cite{yao20229uz}`.",
        "keywords": [
            "ReAct paradigm",
            "synergizing reasoning and acting",
            "Large Language Models (LLMs)",
            "Chain-of-Thought (CoT)",
            "interleaved generation",
            "few-shot prompting",
            "external environment grounding",
            "augmented action space",
            "knowledge-intensive reasoning",
            "interactive decision making",
            "hallucination mitigation",
            "enhanced interpretability",
            "robustness",
            "general task solving"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"in this paper, we explore the use of llms to generate both reasoning traces and task-speci\ufb01c actions in an interleaved manner...\"** - this indicates the proposal of a new approach or method.\n2.  **\"we apply our approach, named react...\"** - clearly states they have developed and are presenting a new system/method called react.\n3.  **\"...demonstrate its effectiveness over state-of-the-art baselines...\"** - this shows empirical evaluation, which is common for technical papers validating their proposed solution.\n4.  **\"react overcomes prevalent issues... by interacting with a simple wikipedia api, and generating human-like task-solving trajectories...\"** - describes the technical mechanism and solution.\n\nthe paper introduces a novel approach/system (\"react\") and then demonstrates its effectiveness through experiments. while it includes strong empirical evidence, its primary contribution is the *development and presentation of a new method*.\n\ntherefore, this paper is best classified as **technical**."
    },
    "dbeeca8466e0c177ec67c60d529899232415ca87.pdf": {
        "title": "On Faithfulness and Factuality in Abstractive Summarization",
        "authors": [
            "Joshua Maynez",
            "Shashi Narayan",
            "Bernd Bohnet",
            "Ryan T. McDonald"
        ],
        "published_date": "2020",
        "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/dbeeca8466e0c177ec67c60d529899232415ca87.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 1136,
        "score": 227.20000000000002
    },
    "f208ea909fa7f54fea82def9a92fd81dfc758c39.pdf": {
        "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
        "authors": [
            "H. Trivedi",
            "Niranjan Balasubramanian",
            "Tushar Khot",
            "Ashish Sabharwal"
        ],
        "published_date": "2022",
        "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f208ea909fa7f54fea82def9a92fd81dfc758c39.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 577,
        "score": 192.33333333333331
    },
    "ca2f63950685a97e5ab6b8e6b2db78a8995e94a2.pdf": {
        "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization",
        "authors": [
            "Shankar Kanthara",
            "Rixie Tiffany Ko Leong",
            "Xiang Lin",
            "Ahmed Masry",
            "Megh Thakkar",
            "Enamul Hoque",
            "Shafiq R. Joty"
        ],
        "published_date": "2022",
        "abstract": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ca2f63950685a97e5ab6b8e6b2db78a8995e94a2.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 176,
        "score": 58.666666666666664,
        "summary": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
        "keywords": []
    },
    "c6bf48f25e0a65d64d658b47326de5922ea7dd44.pdf": {
        "title": "A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation",
        "authors": [
            "Tianyu Liu",
            "Yizhe Zhang",
            "C. Brockett",
            "Yi Mao",
            "Zhifang Sui",
            "Weizhu Chen",
            "W. Dolan"
        ],
        "published_date": "2021",
        "abstract": "Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time. As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HaDeS (HAllucination DEtection dataSet). To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowd-sourced annotations. To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. We conduct comprehensive data analyses and create multiple baseline models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c6bf48f25e0a65d64d658b47326de5922ea7dd44.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 163,
        "score": 40.75
    },
    "889feabe31ba0d24c093ac94d54a06eecb87e3f4.pdf": {
        "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
        "authors": [
            "Nouha Dziri",
            "Andrea Madotto",
            "Osmar Zaiane",
            "A. Bose"
        ],
        "published_date": "2021",
        "abstract": "Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/889feabe31ba0d24c093ac94d54a06eecb87e3f4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 145,
        "score": 36.25
    },
    "a6d8d04962f84ae6225e72723869a002b9fc8036.pdf": {
        "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers",
        "authors": [
            "Boseop Kim",
            "Hyoungseok Kim",
            "Sang-Woo Lee",
            "Gichang Lee",
            "Donghyun Kwak",
            "D. Jeon",
            "Sunghyun Park",
            "Sungju Kim",
            "Seonhoon Kim",
            "D. Seo",
            "Heungsub Lee",
            "Minyoung Jeong",
            "Sungjae Lee",
            "Minsub Kim",
            "SukHyun Ko",
            "Seokhun Kim",
            "Taeyong Park",
            "Jinuk Kim",
            "Soyoung Kang",
            "Nahyeon Ryu",
            "Kang Min Yoo",
            "Minsuk Chang",
            "Soobin Suh",
            "Sookyo In",
            "Jinseong Park",
            "Kyungduk Kim",
            "Hiun Kim",
            "Jisu Jeong",
            "Y. Yeo",
            "Dong-hyun Ham",
            "Dongju Park",
            "Min Young Lee",
            "Jaewook Kang",
            "Inho Kang",
            "Jung-Woo Ha",
            "W. Park",
            "Nako Sung"
        ],
        "published_date": "2021",
        "abstract": "GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/a6d8d04962f84ae6225e72723869a002b9fc8036.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 124,
        "score": 31.0,
        "summary": "GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",
        "keywords": []
    },
    "7cfbd36c0043098589cbaf18dca2b41d8dc24abe.pdf": {
        "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
        "authors": [
            "Wenliang Dai",
            "Zihan Liu",
            "Ziwei Ji",
            "Dan Su",
            "Pascale Fung"
        ],
        "published_date": "2022",
        "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate non-existent visual objects when generating text based on visual information. In this paper, we systematically study the object hallucination problem from three aspects. First, we examine recent state-of-the-art VLP models, showing that they still hallucinate frequently and models achieving better scores on standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate how different types of image encoding in VLP influence hallucination, including region-based, grid-based, and patch-based. Surprisingly, we find that patch-based features perform the best and smaller patch resolution yields a non-trivial reduction in object hallucination. Third, we decouple various VLP objectives and demonstrate that token-level image-text alignment and controlled generation are crucial to reducing hallucination. Based on that, we propose a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination. Results show that it reduces object hallucination by up to 17.4% when tested on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation).",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7cfbd36c0043098589cbaf18dca2b41d8dc24abe.pdf",
        "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
        "citationCount": 72,
        "score": 24.0
    },
    "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e.pdf": {
        "title": "Training Trajectories of Language Models Across Scales",
        "authors": [
            "Mengzhou Xia",
            "Mikel Artetxe",
            "Chunting Zhou",
            "Xi Victoria Lin",
            "Ramakanth Pasunuru",
            "Danqi Chen",
            "Luke Zettlemoyer",
            "Ves Stoyanov"
        ],
        "published_date": "2022",
        "abstract": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)\u2014from 125M to 175B parameters\u2014on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 67,
        "score": 22.333333333333332,
        "summary": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)\u2014from 125M to 175B parameters\u2014on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
        "keywords": []
    },
    "4e53b481beabba42aac027e5a8c69fed26ab4062.pdf": {
        "title": "RHO ($\u03c1$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
        "authors": [
            "Ziwei Ji",
            "Zihan Liu",
            "Nayeon Lee",
            "Tiezheng Yu",
            "Bryan Wilie",
            "Mini Zeng",
            "Pascale Fung"
        ],
        "published_date": "2022",
        "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4e53b481beabba42aac027e5a8c69fed26ab4062.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 59,
        "score": 19.666666666666664
    },
    "c4f26bc007343c59bedd1423250a3b453d3d2d22.pdf": {
        "title": "mFACE: Multilingual Summarization with Factual Consistency Evaluation",
        "authors": [
            "Roee Aharoni",
            "Shashi Narayan",
            "Joshua Maynez",
            "Jonathan Herzig",
            "Elizabeth Clark",
            "Mirella Lapata"
        ],
        "published_date": "2022",
        "abstract": "Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c4f26bc007343c59bedd1423250a3b453d3d2d22.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 48,
        "score": 16.0,
        "summary": "Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation.",
        "keywords": []
    },
    "ebb4826b717798918bebed4dfac28c917e44577d.pdf": {
        "title": "Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control",
        "authors": [
            "Haopeng Zhang",
            "Semih Yavuz",
            "Wojciech Kryscinski",
            "Kazuma Hashimoto",
            "Yingbo Zhou"
        ],
        "published_date": "2022",
        "abstract": "Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ebb4826b717798918bebed4dfac28c917e44577d.pdf",
        "venue": "NAACL-HLT",
        "citationCount": 36,
        "score": 12.0,
        "summary": "Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles.",
        "keywords": []
    },
    "35872510c095b1189105e9f902f04f51bd0a88e3.pdf": {
        "title": "Visual Cluster Grounding for Image Captioning",
        "authors": [
            "Wenhui Jiang",
            "Minwei Zhu",
            "Yuming Fang",
            "Guangming Shi",
            "Xiaowei Zhao",
            "Yang Liu"
        ],
        "published_date": "2022",
        "abstract": "Attention mechanisms have been extensively adopted in vision and language tasks such as image captioning. It encourages a captioning model to dynamically ground appropriate image regions when generating words or phrases, and it is critical to alleviate the problems of object hallucinations and language bias. However, current studies show that the grounding accuracy of existing captioners is still far from satisfactory. Recently, much effort is devoted to improving the grounding accuracy by linking the words to the full content of objects in images. However, due to the noisy grounding annotations and large variations of object appearance, such strict word-object alignment regularization may not be optimal for improving captioning performance. In this paper, to improve the performance of both grounding and captioning, we propose a novel grounding model which implicitly links the words to the evidence in the image. The proposed model encourages the captioner to dynamically focus on informative regions of the objects, which could be either discriminative parts or full object content. With slacked constraints, the proposed captioning model can capture correct linguistic characteristics and visual relevance, and then generate more grounded image captions. In addition, we propose a novel quantitative metric for evaluating the correctness of the soft attention mechanism by considering the overall contribution of all object proposals when generating certain words. The proposed grounding model can be seamlessly plugged into most attention-based architectures without introducing inference complexity. We conduct extensive experiments on Flickr30k (Young et al., 2014) and MS COCO datasets (Lin et al., 2014), demonstrating that the proposed method consistently improves image captioning in both grounding and captioning. Besides, the proposed attention evaluation metric shows better consistency with the captioning performance.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/35872510c095b1189105e9f902f04f51bd0a88e3.pdf",
        "venue": "IEEE Transactions on Image Processing",
        "citationCount": 34,
        "score": 11.333333333333332,
        "summary": "Attention mechanisms have been extensively adopted in vision and language tasks such as image captioning. It encourages a captioning model to dynamically ground appropriate image regions when generating words or phrases, and it is critical to alleviate the problems of object hallucinations and language bias. However, current studies show that the grounding accuracy of existing captioners is still far from satisfactory. Recently, much effort is devoted to improving the grounding accuracy by linking the words to the full content of objects in images. However, due to the noisy grounding annotations and large variations of object appearance, such strict word-object alignment regularization may not be optimal for improving captioning performance. In this paper, to improve the performance of both grounding and captioning, we propose a novel grounding model which implicitly links the words to the evidence in the image. The proposed model encourages the captioner to dynamically focus on informative regions of the objects, which could be either discriminative parts or full object content. With slacked constraints, the proposed captioning model can capture correct linguistic characteristics and visual relevance, and then generate more grounded image captions. In addition, we propose a novel quantitative metric for evaluating the correctness of the soft attention mechanism by considering the overall contribution of all object proposals when generating certain words. The proposed grounding model can be seamlessly plugged into most attention-based architectures without introducing inference complexity. We conduct extensive experiments on Flickr30k (Young et al., 2014) and MS COCO datasets (Lin et al., 2014), demonstrating that the proposed method consistently improves image captioning in both grounding and captioning. Besides, the proposed attention evaluation metric shows better consistency with the captioning performance.",
        "keywords": []
    },
    "933d1d4f18e721160ddbf8dab25c33f8e3d2cec7.pdf": {
        "title": "Learning to Revise References for Faithful Summarization",
        "authors": [
            "Griffin Adams",
            "Han-Chin Shing",
            "Q. Sun",
            "C. Winestock",
            "K. McKeown",
            "No\u00e9mie Elhadad"
        ],
        "published_date": "2022",
        "abstract": "In real-world scenarios with naturally occurring datasets, reference summaries are noisy and may contain information that cannot be inferred from the source text. On large news corpora, removing low quality samples has been shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora, filtering is detrimental to performance. To improve reference quality while retaining all data, we propose a new approach: to selectively re-write unsupported reference sentences to better reflect source data. We automatically generate a synthetic dataset of positive and negative revisions by corrupting supported sentences and learn to revise reference sentences with contrastive learning. The intensity of revisions is treated as a controllable attribute so that, at inference, diverse candidates can be over-generated-then-rescored to balance faithfulness and abstraction. To test our methods, we extract noisy references from publicly available MIMIC-III discharge summaries for the task of hospital-course summarization, and vary the data on which models are trained. According to metrics and human evaluation, models trained on revised clinical references are much more faithful, informative, and fluent than models trained on original or filtered data.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/933d1d4f18e721160ddbf8dab25c33f8e3d2cec7.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 34,
        "score": 11.333333333333332
    },
    "c2c3220b9faf95db43d90a5ed42fa824b4b3d2f0.pdf": {
        "title": "Revisiting Challenges in Data-to-Text Generation with Fact Grounding",
        "authors": [
            "Hongmin Wang"
        ],
        "published_date": "2020",
        "abstract": "Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60% of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50% more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c2c3220b9faf95db43d90a5ed42fa824b4b3d2f0.pdf",
        "venue": "International Conference on Natural Language Generation",
        "citationCount": 51,
        "score": 10.200000000000001,
        "summary": "Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60% of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50% more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.",
        "keywords": []
    },
    "72da4a646a31d72bcae90b916e120cd7df5f9dae.pdf": {
        "title": "PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition",
        "authors": [
            "Sihao Chen",
            "S. Buthpitiya",
            "Alex Fabrikant",
            "D. Roth",
            "Tal Schuster"
        ],
        "published_date": "2022",
        "abstract": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/72da4a646a31d72bcae90b916e120cd7df5f9dae.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 28,
        "score": 9.333333333333332,
        "summary": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels.",
        "keywords": []
    },
    "ae1e48a74cb2f313e8e99c82f0aa4487b0805002.pdf": {
        "title": "Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization",
        "authors": [
            "Yue Dong",
            "J. Wieting",
            "Pat Verga"
        ],
        "published_date": "2022",
        "abstract": "Despite recent advances in abstractive summarization, current summarization systems still suffer from content hallucinations where models generate text that is either irrelevant or contradictory to the source document. However, prior work has been predicated on the assumption that any generated facts not appearing explicitly in the source are undesired hallucinations. Methods have been proposed to address this scenario by ultimately improving `faithfulness' to the source document, but in reality, there is a large portion of entities in the gold reference targets that are not directly in the source. In this work, we show that these entities are not aberrations, but they instead require utilizing external world knowledge to infer reasoning paths from entities in the source. We show that by utilizing an external knowledge base, we can improve the faithfulness of summaries without simply making them more extractive, and additionally, we show that external knowledge bases linked from the source can benefit the factuality of generated summaries.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ae1e48a74cb2f313e8e99c82f0aa4487b0805002.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 28,
        "score": 9.333333333333332
    },
    "ecb5a6fe2f5261e4e717ece1e82c464c63cb4862.pdf": {
        "title": "Controlling Conditional Language Models without Catastrophic Forgetting",
        "authors": [
            "Tomasz Korbak",
            "Hady ElSahar",
            "Germ\u00e1n Kruszewski",
            "Marc Dymetman"
        ],
        "published_date": "2021",
        "abstract": "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ecb5a6fe2f5261e4e717ece1e82c464c63cb4862.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 36,
        "score": 9.0,
        "summary": "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.",
        "keywords": []
    },
    "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5.pdf": {
        "title": "Training Dynamics for Text Summarization Models",
        "authors": [
            "Tanya Goyal",
            "Jiacheng Xu",
            "J. Li",
            "Greg Durrett"
        ],
        "published_date": "2021",
        "abstract": "Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on summarization. Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process. We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/23f1d4b46bc7c8f357a5a89144d5d32af7be13a5.pdf",
        "venue": "Findings",
        "citationCount": 32,
        "score": 8.0
    },
    "f35dbee22c1572d149b7c1e20d69672cae931451.pdf": {
        "title": "Transforming Sequence Tagging Into A Seq2Seq Task",
        "authors": [
            "K. Raman",
            "Iftekhar Naim",
            "Jiecao Chen",
            "Kazuma Hashimoto",
            "Kiran Yalasangi",
            "Krishna Srinivasan"
        ],
        "published_date": "2022",
        "abstract": "Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled understanding of the trade-offs associated with these formats (such as the effect on model accuracy, sequence length, multilingual generalization, hallucination). In this paper, we rigorously study different formats one could use for casting input text sentences and their output labels into the input and target (i.e., output) of a Seq2Seq model. Along the way, we introduce a new format, which we show to to be both simpler and more effective. Additionally the new format demonstrates significant gains in the multilingual settings \u2013 both zero-shot transfer learning and joint training. Lastly, we find that the new format is more robust and almost completely devoid of hallucination \u2013 an issue we find common in existing formats. With well over a 1000 experiments studying 14 different formats, over 7 diverse public benchmarks \u2013 including 3 multilingual datasets spanning 7 languages \u2013 we believe our findings provide a strong empirical basis in understanding how we should tackle sequence tagging tasks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f35dbee22c1572d149b7c1e20d69672cae931451.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 23,
        "score": 7.666666666666666,
        "summary": "Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled understanding of the trade-offs associated with these formats (such as the effect on model accuracy, sequence length, multilingual generalization, hallucination). In this paper, we rigorously study different formats one could use for casting input text sentences and their output labels into the input and target (i.e., output) of a Seq2Seq model. Along the way, we introduce a new format, which we show to to be both simpler and more effective. Additionally the new format demonstrates significant gains in the multilingual settings \u2013 both zero-shot transfer learning and joint training. Lastly, we find that the new format is more robust and almost completely devoid of hallucination \u2013 an issue we find common in existing formats. With well over a 1000 experiments studying 14 different formats, over 7 diverse public benchmarks \u2013 including 3 multilingual datasets spanning 7 languages \u2013 we believe our findings provide a strong empirical basis in understanding how we should tackle sequence tagging tasks.",
        "keywords": []
    },
    "fda76a1411e16722ebd2d8278c3143ca4363da6b.pdf": {
        "title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models",
        "authors": [
            "Ling Liu",
            "Mans Hulden"
        ],
        "published_date": "2021",
        "abstract": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata\u2014i.e. under \u201cwug test\u201d-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fda76a1411e16722ebd2d8278c3143ca4363da6b.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 29,
        "score": 7.25,
        "summary": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmata\u2014i.e. under \u201cwug test\u201d-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",
        "keywords": []
    },
    "3e075efc541c7d2b357199655e11f084686e8575.pdf": {
        "title": "Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?",
        "authors": [
            "Tobias Norlund",
            "Lovisa Hagstr\u00f6m",
            "Richard Johansson"
        ],
        "published_date": "2021",
        "abstract": "Large language models are known to suffer from the hallucination problem in that they are prone to output statements that are false or inconsistent, indicating a lack of knowledge. A proposed solution to this is to provide the model with additional data modalities that complements the knowledge obtained through text. We investigate the use of visual data to complement the knowledge of large language models by proposing a method for evaluating visual knowledge transfer to text for uni- or multimodal language models. The method is based on two steps, 1) a novel task querying for knowledge of memory colors, i.e. typical colors of well-known objects, and 2) filtering of model training data to clearly separate knowledge contributions. Additionally, we introduce a model architecture that involves a visual imagination step and evaluate it with our proposed method. We find that our method can successfully be used to measure visual knowledge transfer capabilities in models and that our novel model architecture shows promising results for leveraging multimodal knowledge in a unimodal setting.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3e075efc541c7d2b357199655e11f084686e8575.pdf",
        "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
        "citationCount": 25,
        "score": 6.25,
        "summary": "Large language models are known to suffer from the hallucination problem in that they are prone to output statements that are false or inconsistent, indicating a lack of knowledge. A proposed solution to this is to provide the model with additional data modalities that complements the knowledge obtained through text. We investigate the use of visual data to complement the knowledge of large language models by proposing a method for evaluating visual knowledge transfer to text for uni- or multimodal language models. The method is based on two steps, 1) a novel task querying for knowledge of memory colors, i.e. typical colors of well-known objects, and 2) filtering of model training data to clearly separate knowledge contributions. Additionally, we introduce a model architecture that involves a visual imagination step and evaluate it with our proposed method. We find that our method can successfully be used to measure visual knowledge transfer capabilities in models and that our novel model architecture shows promising results for leveraging multimodal knowledge in a unimodal setting.",
        "keywords": []
    },
    "0f7a6c557e376d8c77d684bcda0daee74fc29acf.pdf": {
        "title": "Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation",
        "authors": [
            "Yanyang Li",
            "Jianqiao Zhao",
            "M. Lyu",
            "Liwei Wang"
        ],
        "published_date": "2022",
        "abstract": "Recent advances in large-scale pre-training provide large models with the potential to learn knowledge from the raw text. It is thus natural to ask whether it is possible to leverage these large models as knowledge bases for downstream tasks. In this work, we answer the aforementioned question in unsupervised knowledge-grounded conversation. We explore various methods that best elicit knowledge from large models. Our human study indicates that, though hallucinations exist, large models post the unique advantage of being able to output common sense and summarize facts that cannot be directly retrieved from the search engine. To better exploit such generated knowledge in dialogue generation, we treat the generated knowledge as a noisy knowledge source and propose the posterior-based reweighing as well as the noisy training strategy. Empirical results on two benchmarks show advantages over the state-of-the-art methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/0f7a6c557e376d8c77d684bcda0daee74fc29acf.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 18,
        "score": 6.0
    },
    "334bf07262320eb895a22973c948b4111e782daa.pdf": {
        "title": "Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to Cold-Start Search Retrieval",
        "authors": [
            "Tao Wu",
            "E. Chio",
            "Heng-Tze Cheng",
            "Yu Du",
            "Steffen Rendle",
            "D. Kuzmin",
            "Ritesh Agarwal",
            "Li Zhang",
            "John R. Anderson",
            "Sarvjeet Singh",
            "Tushar Chandra",
            "Ed H. Chi",
            "Wen Li",
            "Ankit Kumar",
            "Xiang Ma",
            "A. Soares",
            "Nitin Jindal",
            "Pei Cao"
        ],
        "published_date": "2020",
        "abstract": "Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/334bf07262320eb895a22973c948b4111e782daa.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 27,
        "score": 5.4,
        "summary": "Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.",
        "keywords": []
    },
    "5b824faaea855eb1700b16d7a67ee58a8f75e7d4.pdf": {
        "title": "Data Augmentation for Low-Resource Dialogue Summarization",
        "authors": [
            "Yongtai Liu",
            "Joshua Maynez",
            "Gon\u00e7alo Sim\u00f5es",
            "Shashi Narayan"
        ],
        "published_date": "2022",
        "abstract": "We present DADS , a novel D ata A ugmentation 001 technique for low-resource D ialogue 002 S ummarization. Our method generates 003 synthetic examples by replacing sections of 004 text from both the input dialogue and summary 005 while preserving the augmented summary 006 to correspond to a viable summary for the 007 augmented dialogue. We utilize pretrained 008 language models that produce highly likely 009 dialogue alternatives while still being free to 010 generate diverse alternatives. We applied our 011 data augmentation method to the SAMSum 012 dataset in low resource scenarios, mimicking 013 real world problems such as chat, thread, and 014 meeting summarization where large scale 015 supervised datasets with human-written sum-016 maries are scarce. Through both automatic 017 and human evaluations, we show that DADS 018 shows strong improvements for low resource 019 scenarios while generating topically diverse 020 summaries without introducing additional 021 hallucinations to the summaries. 022",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5b824faaea855eb1700b16d7a67ee58a8f75e7d4.pdf",
        "venue": "NAACL-HLT",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "We present DADS , a novel D ata A ugmentation 001 technique for low-resource D ialogue 002 S ummarization. Our method generates 003 synthetic examples by replacing sections of 004 text from both the input dialogue and summary 005 while preserving the augmented summary 006 to correspond to a viable summary for the 007 augmented dialogue. We utilize pretrained 008 language models that produce highly likely 009 dialogue alternatives while still being free to 010 generate diverse alternatives. We applied our 011 data augmentation method to the SAMSum 012 dataset in low resource scenarios, mimicking 013 real world problems such as chat, thread, and 014 meeting summarization where large scale 015 supervised datasets with human-written sum-016 maries are scarce. Through both automatic 017 and human evaluations, we show that DADS 018 shows strong improvements for low resource 019 scenarios while generating topically diverse 020 summaries without introducing additional 021 hallucinations to the summaries. 022",
        "keywords": []
    },
    "c8373577bcce9b8811672ddeb372a5b780397117.pdf": {
        "title": "Efficacy of Metacognitive Training for Depression: A Randomized Controlled Trial",
        "authors": [
            "L. Jelinek",
            "M. Hauschildt",
            "C. Wittekind",
            "Brooke C. Schneider",
            "L. Kriston",
            "S. Moritz"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c8373577bcce9b8811672ddeb372a5b780397117.pdf",
        "venue": "Psychotherapy and Psychosomatics",
        "citationCount": 47,
        "score": 5.222222222222222,
        "summary": "",
        "keywords": []
    },
    "6d26836a4cee8f90c6fa4d5751d5f10e0f720301.pdf": {
        "title": "Improved Beam Search for Hallucination Mitigation in Abstractive Summarization",
        "authors": [
            "A. Sridhar",
            "Erik M. Visser"
        ],
        "published_date": "2022",
        "abstract": "Advancement in large pretrained language models has significantly improved their performance for conditional language generation tasks including summarization albeit with hallucinations. To reduce hallucinations, conventional methods proposed improving beam search or using a fact checker as a postprocessing step. In this paper, we investigate the use of the Natural Language Inference (NLI) entailment metric to detect and prevent hallucinations in summary generation. We propose an NLI-assisted beam re-ranking mechanism by computing entailment probability scores between the input context and summarization model-generated beams during saliency-enhanced greedy decoding. Moreover, a diversity metric is introduced to compare its effectiveness against vanilla beam search. Our proposed algorithm significantly outperforms vanilla beam decoding on XSum and CNN/DM datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6d26836a4cee8f90c6fa4d5751d5f10e0f720301.pdf",
        "venue": "arXiv.org",
        "citationCount": 15,
        "score": 5.0
    },
    "6ab78343ab82fa9d7baa68027f9f7e8cd9863737.pdf": {
        "title": "Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks",
        "authors": [
            "Vikas Raunak",
            "Arul Menezes"
        ],
        "published_date": "2022",
        "abstract": "Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous studies of memorization in constrained NLG tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. In this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in NMT. We demonstrate that extractive memorization poses a serious threat to NMT reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. Based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. Finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. We have released the code to reproduce our results at https://github.com/vyraun/Finding-Memo.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6ab78343ab82fa9d7baa68027f9f7e8cd9863737.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous studies of memorization in constrained NLG tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. In this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in NMT. We demonstrate that extractive memorization poses a serious threat to NMT reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. Based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. Finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. We have released the code to reproduce our results at https://github.com/vyraun/Finding-Memo.",
        "keywords": []
    },
    "5791c2b41dd23310c53d6738a4c0d587107c2dc8.pdf": {
        "title": "MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation",
        "authors": [
            "Swarnadeep Saha",
            "Xinyan Velocity Yu",
            "Mohit Bansal",
            "Ramakanth Pasunuru",
            "Asli Celikyilmaz"
        ],
        "published_date": "2022",
        "abstract": "Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5791c2b41dd23310c53d6738a4c0d587107c2dc8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.",
        "keywords": []
    },
    "fbe549b5b30f54a4f76cb4bf4c6b8a9fb8e24657.pdf": {
        "title": "KSL-Guide: A Large-scale Korean Sign Language Dataset Including Interrogative Sentences for Guiding the Deaf and Hard-of-Hearing",
        "authors": [
            "Soomin Ham",
            "Kibaek Park",
            "Yeongjun Jang",
            "Youngtaek Oh",
            "Seokmin Yun",
            "Sukwon Yoon",
            "Chang Jo Kim",
            "Han-Mu Park",
            "In-So Kweon"
        ],
        "published_date": "2021",
        "abstract": "Many advancements in computer vision and machine learning have shown potential for significantly improving the lives of people with disabilities. In particular, recent research has demonstrated that deep neural network models could be used to bridge the gap between the deaf who use sign language and hearing people. The major impediment to advancing such models is the lack of high-quality and large-scale training data. Moreover, previously released sign language datasets include few or no interrogative sentences compared to declarative sentences. In this paper, we introduce a new publicly available large-scale Korean Sign Language (KSL) dataset-KSL-Guide-that includes both declarative sentences and comparable interrogative sentences, which are required for a model to achieve high performance in real-world interactive tasks deployed on service applications. Our dataset contains a total of 121K sign language video samples featuring sentences and words spoken by native KSL speakers with extensive annotations (e.g., gloss, translation, keypoints, and timestamps). We exploit a multi-camera system to produce 3D human pose keypoints as well as 2D keypoints from multi-view RGB. Our experiments quantitatively demonstrate that the inclusion of interrogative sentences in training for sign language recognition and translation tasks greatly improves their performance. Furthermore, we empirically show the qualitative results by developing a prototype application using our dataset, providing an interactive guide service that helps to lower the communication barrier between sign language speakers and hearing people.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fbe549b5b30f54a4f76cb4bf4c6b8a9fb8e24657.pdf",
        "venue": "IEEE International Conference on Automatic Face & Gesture Recognition",
        "citationCount": 15,
        "score": 3.75,
        "summary": "Many advancements in computer vision and machine learning have shown potential for significantly improving the lives of people with disabilities. In particular, recent research has demonstrated that deep neural network models could be used to bridge the gap between the deaf who use sign language and hearing people. The major impediment to advancing such models is the lack of high-quality and large-scale training data. Moreover, previously released sign language datasets include few or no interrogative sentences compared to declarative sentences. In this paper, we introduce a new publicly available large-scale Korean Sign Language (KSL) dataset-KSL-Guide-that includes both declarative sentences and comparable interrogative sentences, which are required for a model to achieve high performance in real-world interactive tasks deployed on service applications. Our dataset contains a total of 121K sign language video samples featuring sentences and words spoken by native KSL speakers with extensive annotations (e.g., gloss, translation, keypoints, and timestamps). We exploit a multi-camera system to produce 3D human pose keypoints as well as 2D keypoints from multi-view RGB. Our experiments quantitatively demonstrate that the inclusion of interrogative sentences in training for sign language recognition and translation tasks greatly improves their performance. Furthermore, we empirically show the qualitative results by developing a prototype application using our dataset, providing an interactive guide service that helps to lower the communication barrier between sign language speakers and hearing people.",
        "keywords": []
    },
    "a996fb4b9f3d4315138f8773b1e995f8386b11eb.pdf": {
        "title": "JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs",
        "authors": [
            "Eunji Jeong",
            "Sungwoo Cho",
            "Gyeong-In Yu",
            "Joo Seong Jeong",
            "Dongjin Shin",
            "Byung-Gon Chun"
        ],
        "published_date": "2018",
        "abstract": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. \nThis paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, the de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Experiments demonstrate that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/a996fb4b9f3d4315138f8773b1e995f8386b11eb.pdf",
        "venue": "Symposium on Networked Systems Design and Implementation",
        "citationCount": 25,
        "score": 3.571428571428571,
        "summary": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. \nThis paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, the de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Experiments demonstrate that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
        "keywords": []
    },
    "471a49220cea2069e8b8a76821b1d2434204a732.pdf": {
        "title": "FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering",
        "authors": [
            "Akhil Kedia",
            "Mohd Abbas Zaidi",
            "Haejun Lee"
        ],
        "published_date": "2022",
        "abstract": "Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/471a49220cea2069e8b8a76821b1d2434204a732.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",
        "keywords": []
    },
    "3b7cdcbe19fdbda17f7cce1eb1b6c8f5a8a60e3b.pdf": {
        "title": "Touch and the body: The role of the somatosensory cortex in tactile awareness",
        "authors": [
            "A. Gallace",
            "A. Gallace"
        ],
        "published_date": "2010",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3b7cdcbe19fdbda17f7cce1eb1b6c8f5a8a60e3b.pdf",
        "venue": "",
        "citationCount": 49,
        "score": 3.2666666666666666,
        "summary": "",
        "keywords": []
    },
    "6aed9dffe3246efd9d19b2873994ba112e2ad422.pdf": {
        "title": "Leveraging Large Pretrained Models for WebNLG 2020",
        "authors": [
            "Xintong Li",
            "Aleksandre Maskharashvili",
            "S. Stevens-Guille",
            "Michael White"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6aed9dffe3246efd9d19b2873994ba112e2ad422.pdf",
        "venue": "WEBNLG",
        "citationCount": 15,
        "score": 3.0,
        "summary": "",
        "keywords": []
    },
    "b32cf25dffbdbe6d838fc7b8781c126c8fea7d3c.pdf": {
        "title": "Learning to Suggest Questions in Online Forums",
        "authors": [
            "Tom Chao Zhou",
            "Chin-Yew Lin",
            "Irwin King",
            "Michael R. Lyu",
            "Young-In Song",
            "Yunbo Cao"
        ],
        "published_date": "2011",
        "abstract": "\n \n Online forums contain interactive and semantically related discussions on various questions. Extracted question-answer archive is invaluable knowledge, which can be used to improve Question Answering services. In this paper, we address the problem of Question Suggestion, which targets at suggesting questions that are semantically related to a queried question. Existing bag-of-words approaches suffer from the shortcoming that they could not bridge the lexical chasm between semantically related questions. Therefore, we present a new framework to suggest questions, and propose the Topicenhanced Translation-based Language Model (TopicTRLM) which fuses both the lexical and latent semantic knowledge. Extensive experiments have been conducted with a large real world data set. Experimental results indicate our approach is very effective and outperforms other popular methods in several metrics.\n \n",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b32cf25dffbdbe6d838fc7b8781c126c8fea7d3c.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 28,
        "score": 2.0,
        "summary": "\n \n Online forums contain interactive and semantically related discussions on various questions. Extracted question-answer archive is invaluable knowledge, which can be used to improve Question Answering services. In this paper, we address the problem of Question Suggestion, which targets at suggesting questions that are semantically related to a queried question. Existing bag-of-words approaches suffer from the shortcoming that they could not bridge the lexical chasm between semantically related questions. Therefore, we present a new framework to suggest questions, and propose the Topicenhanced Translation-based Language Model (TopicTRLM) which fuses both the lexical and latent semantic knowledge. Extensive experiments have been conducted with a large real world data set. Experimental results indicate our approach is very effective and outperforms other popular methods in several metrics.\n \n",
        "keywords": []
    },
    "20013690616f6e781c05feaa08a2247a97640a87.pdf": {
        "title": "Auto-Mapping and Configuration Method of IEC 61850 Information Model Based on OPC UA",
        "authors": [
            "Shin In-Jae",
            "Byungkwen Song",
            "D. Eom"
        ],
        "published_date": "2016",
        "abstract": "The open-platform communication (OPC) unified architecture (UA) (IEC62541) is introduced as a key technology for realizing a variety of smart grid (SG) use cases enabling relevant automation and control tasks. The OPC UA can expand interoperability between power systems. The top-level SG management platform needs independent middleware to transparently manage the power information technology (IT) systems, including the IEC 61850. To expand interoperability between the power system for a large number of stakeholders and various standards, this paper focuses on the IEC 61850 for the digital substation. In this paper, we propose the interconnection method to integrate communication with OPC UA and convert OPC UA AddressSpace using system configuration description language (SCL) of IEC 61850. We implemented the mapping process for the verification of the interconnection method. The interconnection method in this paper can expand interoperability between power systems for OPC UA integration for various data structures in the smart grid.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/20013690616f6e781c05feaa08a2247a97640a87.pdf",
        "venue": "",
        "citationCount": 17,
        "score": 1.8888888888888888,
        "summary": "The open-platform communication (OPC) unified architecture (UA) (IEC62541) is introduced as a key technology for realizing a variety of smart grid (SG) use cases enabling relevant automation and control tasks. The OPC UA can expand interoperability between power systems. The top-level SG management platform needs independent middleware to transparently manage the power information technology (IT) systems, including the IEC 61850. To expand interoperability between the power system for a large number of stakeholders and various standards, this paper focuses on the IEC 61850 for the digital substation. In this paper, we propose the interconnection method to integrate communication with OPC UA and convert OPC UA AddressSpace using system configuration description language (SCL) of IEC 61850. We implemented the mapping process for the verification of the interconnection method. The interconnection method in this paper can expand interoperability between power systems for OPC UA integration for various data structures in the smart grid.",
        "keywords": []
    },
    "e416b5fb1ab75c0770cf7cbd6976f5444b0ee89f.pdf": {
        "title": "Speculative Symbolic Graph Execution of Imperative Deep Learning Programs",
        "authors": [
            "Eunji Jeong",
            "Sungwoo Cho",
            "Gyeong-In Yu",
            "Joo Seong Jeong",
            "Dongjin Shin",
            "Taebum Kim",
            "Byung-Gon Chun"
        ],
        "published_date": "2019",
        "abstract": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. This paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, a de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Our experiments show that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e416b5fb1ab75c0770cf7cbd6976f5444b0ee89f.pdf",
        "venue": "ACM SIGOPS Operating Systems Review",
        "citationCount": 11,
        "score": 1.8333333333333333,
        "summary": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. This paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, a de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Our experiments show that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
        "keywords": []
    },
    "b7d484357f5f84c065ef7fbc77d0460b9795964d.pdf": {
        "title": "Stacking With Auxiliary Features for Entity Linking in the Medical Domain",
        "authors": [
            "Nazneen Rajani",
            "Mihaela A. Bornea",
            "Ken Barker"
        ],
        "published_date": "2017",
        "abstract": "Linking spans of natural language text to concepts in a structured source is an important task for many problems. It allows intelligent systems to leverage rich knowledge available in those sources (such as concept properties and relations) to enhance the semantics of the mentions of these concepts in text. In the medical domain, it is common to link text spans to medical concepts in large, curated knowledge repositories such as the Unified Medical Language System. Different approaches have different strengths: some are precision-oriented, some recall-oriented; some better at considering context but more prone to hallucination. The variety of techniques suggests that ensembling could outperform component technologies at this task. In this paper, we describe our process for building a Stacking ensemble using additional, auxiliary features for Entity Linking in the medical domain. We report experiments that show that naive ensembling does not always outperform component Entity Linking systems, that stacking usually outperforms naive ensembling, and that auxiliary features added to the stacker further improve its performance on three distinct datasets. Our best model produces state-of-the-art results on several medical datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b7d484357f5f84c065ef7fbc77d0460b9795964d.pdf",
        "venue": "Workshop on Biomedical Natural Language Processing",
        "citationCount": 14,
        "score": 1.75,
        "summary": "Linking spans of natural language text to concepts in a structured source is an important task for many problems. It allows intelligent systems to leverage rich knowledge available in those sources (such as concept properties and relations) to enhance the semantics of the mentions of these concepts in text. In the medical domain, it is common to link text spans to medical concepts in large, curated knowledge repositories such as the Unified Medical Language System. Different approaches have different strengths: some are precision-oriented, some recall-oriented; some better at considering context but more prone to hallucination. The variety of techniques suggests that ensembling could outperform component technologies at this task. In this paper, we describe our process for building a Stacking ensemble using additional, auxiliary features for Entity Linking in the medical domain. We report experiments that show that naive ensembling does not always outperform component Entity Linking systems, that stacking usually outperforms naive ensembling, and that auxiliary features added to the stacker further improve its performance on three distinct datasets. Our best model produces state-of-the-art results on several medical datasets.",
        "keywords": []
    },
    "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf": {
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "Yunfan Gao",
            "Yun Xiong",
            "Xinyu Gao",
            "Kangxiang Jia",
            "Jinliu Pan",
            "Yuxi Bi",
            "Yi Dai",
            "Jiawei Sun",
            "Qianyu Guo",
            "Meng Wang",
            "Haofen Wang"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf",
        "venue": "arXiv.org",
        "citationCount": 2344,
        "score": 1172.0
    },
    "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8.pdf": {
        "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
        "authors": [
            "Lei Huang",
            "Weijiang Yu",
            "Weitao Ma",
            "Weihong Zhong",
            "Zhangyin Feng",
            "Haotian Wang",
            "Qianglong Chen",
            "Weihua Peng",
            "Xiaocheng Feng",
            "Bing Qin",
            "Ting Liu"
        ],
        "published_date": "2023",
        "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1e909e2a8cdacdcdff125ebcc566f37cb869a1c8.pdf",
        "venue": "ACM Trans. Inf. Syst.",
        "citationCount": 1315,
        "score": 657.5
    },
    "206400aba5f12f734cdd2e4ab48ef6014ea60773.pdf": {
        "title": "Evaluating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Yifan Li",
            "Yifan Du",
            "Kun Zhou",
            "Jinpeng Wang",
            "Wayne Xin Zhao",
            "Ji-rong Wen"
        ],
        "published_date": "2023",
        "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/206400aba5f12f734cdd2e4ab48ef6014ea60773.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 1021,
        "score": 510.5
    },
    "5cd671efa2af8456c615c5faf54d1be4950f3819.pdf": {
        "title": "Hallucination is Inevitable: An Innate Limitation of Large Language Models",
        "authors": [
            "Ziwei Xu",
            "Sanjay Jain",
            "Mohan Kankanhalli"
        ],
        "published_date": "2024",
        "abstract": "Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5cd671efa2af8456c615c5faf54d1be4950f3819.pdf",
        "venue": "arXiv.org",
        "citationCount": 340,
        "score": 340.0
    },
    "d00735241af700d21762d2f3ca00d920241a15a4.pdf": {
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
        "authors": [
            "Yue Zhang",
            "Yafu Li",
            "Leyang Cui",
            "Deng Cai",
            "Lemao Liu",
            "Tingchen Fu",
            "Xinting Huang",
            "Enbo Zhao",
            "Yu Zhang",
            "Yulong Chen",
            "Longyue Wang",
            "A. Luu",
            "Wei Bi",
            "Freda Shi",
            "Shuming Shi"
        ],
        "published_date": "2023",
        "abstract": "\n While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d00735241af700d21762d2f3ca00d920241a15a4.pdf",
        "venue": "Computational Linguistics",
        "citationCount": 672,
        "score": 336.0
    },
    "7c1707db9aafd209aa93db3251e7ebd593d55876.pdf": {
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "authors": [
            "Potsawee Manakul",
            "Adian Liusie",
            "M. Gales"
        ],
        "published_date": "2023",
        "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7c1707db9aafd209aa93db3251e7ebd593d55876.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 535,
        "score": 267.5
    },
    "c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05.pdf": {
        "title": "Hallucination of Multimodal Large Language Models: A Survey",
        "authors": [
            "Zechen Bai",
            "Pichao Wang",
            "Tianjun Xiao",
            "Tong He",
            "Zongbo Han",
            "Zheng Zhang",
            "Mike Zheng Shou"
        ],
        "published_date": "2024",
        "abstract": "This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05.pdf",
        "venue": "arXiv.org",
        "citationCount": 267,
        "score": 267.0
    },
    "5272acad9e4201e93dabe3fd99bd7ead9b1a544d.pdf": {
        "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
        "authors": [
            "S. Tonmoy",
            "S. M. M. Zaman",
            "Vinija Jain",
            "Anku Rani",
            "Vipula Rawte",
            "Aman Chadha",
            "Amitava Das"
        ],
        "published_date": "2024",
        "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5272acad9e4201e93dabe3fd99bd7ead9b1a544d.pdf",
        "venue": "arXiv.org",
        "citationCount": 259,
        "score": 259.0
    },
    "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365.pdf": {
        "title": "Enabling Large Language Models to Generate Text with Citations",
        "authors": [
            "Tianyu Gao",
            "Howard Yen",
            "Jiatong Yu",
            "Danqi Chen"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 416,
        "score": 208.0
    },
    "fc4c380102d6f72657d1ab54dffd6be536bb01c7.pdf": {
        "title": "A Survey on Hallucination in Large Vision-Language Models",
        "authors": [
            "Hanchao Liu",
            "Wenyuan Xue",
            "Yifei Chen",
            "Dapeng Chen",
            "Xiutian Zhao",
            "Ke Wang",
            "Liping Hou",
            "Rong-Zhi Li",
            "Wei Peng"
        ],
        "published_date": "2024",
        "abstract": "Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fc4c380102d6f72657d1ab54dffd6be536bb01c7.pdf",
        "venue": "arXiv.org",
        "citationCount": 193,
        "score": 193.0
    },
    "28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf": {
        "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
        "authors": [
            "Jiawei Chen",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "published_date": "2023",
        "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 380,
        "score": 190.0
    },
    "c7a7104df3db13737a865ede2be8146990fa4026.pdf": {
        "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
        "authors": [
            "Fuxiao Liu",
            "Kevin Lin",
            "Linjie Li",
            "Jianfeng Wang",
            "Y. Yacoob",
            "Lijuan Wang"
        ],
        "published_date": "2023",
        "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data are available at https://github.com/FuxiaoLiu/LRV-Instruction.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c7a7104df3db13737a865ede2be8146990fa4026.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 337,
        "score": 168.5
    },
    "328eb183007bf4aefbf42437b42a15db375803e3.pdf": {
        "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
        "authors": [
            "Sicong Leng",
            "Hang Zhang",
            "Guanzheng Chen",
            "Xin Li",
            "Shijian Lu",
            "Chunyan Miao",
            "Li Bing"
        ],
        "published_date": "2023",
        "abstract": "Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/328eb183007bf4aefbf42437b42a15db375803e3.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 327,
        "score": 163.5
    },
    "e0384ba36555232c587d4a80d527895a095a9001.pdf": {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "J. Nie",
            "Ji-rong Wen"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e0384ba36555232c587d4a80d527895a095a9001.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 293,
        "score": 146.5
    },
    "f8a642fbb51e0b0ae4774781309545d15d6d9b11.pdf": {
        "title": "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning",
        "authors": [
            "Yiyang Zhou",
            "Chenhang Cui",
            "Rafael Rafailov",
            "Chelsea Finn",
            "Huaxiu Yao"
        ],
        "published_date": "2024",
        "abstract": "Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucinations into the correct answer. Second, we distort the image to trigger the inherent hallucination behavior of the VLLM. This is an automated approach, which does not rely on human data generation or require a perfect expert, which makes it easily scalable. Finally, both of these generation strategies are integrated into an RLHF pipeline via Direct Preference Optimization. In experiments across broad benchmarks, we show that we can not only reduce hallucinations, but improve model performance across standard benchmarks, outperforming prior approaches. Our data and code are available at https://github.com/YiyangZhou/POVID.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f8a642fbb51e0b0ae4774781309545d15d6d9b11.pdf",
        "venue": "arXiv.org",
        "citationCount": 145,
        "score": 145.0
    },
    "49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c.pdf": {
        "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation",
        "authors": [
            "Qidong Huang",
            "Xiao-wen Dong",
            "Pan Zhang",
            "Bin Wang",
            "Conghui He",
            "Jiaqi Wang",
            "Dahua Lin",
            "Weiming Zhang",
            "Neng H. Yu"
        ],
        "published_date": "2023",
        "abstract": "Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment. Existing methods mitigate this issue with either training with specific designed data or inferencing with external knowledge from other sources, incurring inevitable additional costs. In this paper, we present OPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a Retrospection-Allocation strategy, serving as a nearly free lunch to alleviate the hallucination issue without additional data, knowledge, or training. Our approach begins with an interesting observation that, most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens, but not all the previous tokens. Such partial overtrust inclination results in the neglecting of image tokens and describes the image content with hallucination. Based on the observation, OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue, along with a rollback strategy that retrospects the presence of summary tokens in the previously generated tokens, and re-allocate the token selection if necessary. With extensive experiments, OPERA shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality. Our code is at: https://github.com/shikiw/OPERA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 280,
        "score": 140.0
    },
    "0b395ed1c8b284e551172b728e83cf257e33729a.pdf": {
        "title": "Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
        "authors": [
            "Tianrui Guan",
            "Fuxiao Liu",
            "Xiyang Wu",
            "Ruiqi Xian",
            "Zongxia Li",
            "Xiaoyu Liu",
            "Xijun Wang",
            "Lichang Chen",
            "Furong Huang",
            "Yaser Yacoob",
            "Dinesh Manocha",
            "Tianyi Zhou"
        ],
        "published_date": "2023",
        "abstract": "We introduce \u201cHALLUSIONBENCH11\u201cHallusion\u201d is a portmanteau of \u201challucination\u201d and \u201cillusion.\u201d,\u201d a comprehensive benchmark designed for the evaluation of image-context rea-soning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpre-tation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on Hallusion-bench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only high-lights the observed failure modes, including language hal-lucination and visual illusion but also deepens an under-standing of these pitfalls. Our comprehensive case studies within Hallusionbench shed light on the challenges of hallucination and illusion in LVLMs. Based on these in-sights, we suggest potential pathways for their future im-provement. The benchmark and codebase can be accessed at https://github.com/tianyi-labIHallusionBench.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/0b395ed1c8b284e551172b728e83cf257e33729a.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 272,
        "score": 136.0
    },
    "7c9f69848d28e0a7cbb00942ee83dab9773c23e4.pdf": {
        "title": "GPT-NER: Named Entity Recognition via Large Language Models",
        "authors": [
            "Shuhe Wang",
            "Xiaofei Sun",
            "Xiaoya Li",
            "Rongbin Ouyang",
            "Fei Wu",
            "Tianwei Zhang",
            "Jiwei Li",
            "Guoyin Wang"
        ],
        "published_date": "2023",
        "abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7c9f69848d28e0a7cbb00942ee83dab9773c23e4.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 248,
        "score": 124.0,
        "summary": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
        "keywords": []
    },
    "be177300487b6d0f25e6cade9a31900454b13281.pdf": {
        "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
        "authors": [
            "Tu Vu",
            "Mohit Iyyer",
            "Xuezhi Wang",
            "Noah Constant",
            "Jerry Wei",
            "Jason Wei",
            "C. Tar",
            "Yun-Hsuan Sung",
            "Denny Zhou",
            "Quoc Le",
            "Thang Luong"
        ],
        "published_date": "2023",
        "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/be177300487b6d0f25e6cade9a31900454b13281.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 246,
        "score": 123.0
    },
    "4b0b56be0ae9479d2bd5c2f0943db1906343c10f.pdf": {
        "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
        "authors": [
            "S. Dhuliawala",
            "M. Komeili",
            "Jing Xu",
            "R. Raileanu",
            "Xian Li",
            "Asli Celikyilmaz",
            "Jason Weston"
        ],
        "published_date": "2023",
        "abstract": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4b0b56be0ae9479d2bd5c2f0943db1906343c10f.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 244,
        "score": 122.0
    },
    "ce4e101950554c41d3b35f5b297722abd1ce6403.pdf": {
        "title": "Automated Unit Test Improvement using Large Language Models at Meta",
        "authors": [
            "N. Alshahwan",
            "Jubin Chheda",
            "Anastasia Finogenova",
            "Beliz Gokkaya",
            "Mark Harman",
            "Inna Harper",
            "Alexandru Marginean",
            "Shubho Sengupta",
            "Eddy Wang"
        ],
        "published_date": "2024",
        "abstract": "This paper describes Meta\u2019s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM\u2019s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta\u2019s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ce4e101950554c41d3b35f5b297722abd1ce6403.pdf",
        "venue": "SIGSOFT FSE Companion",
        "citationCount": 120,
        "score": 120.0,
        "summary": "This paper describes Meta\u2019s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLM\u2019s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Meta\u2019s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.",
        "keywords": []
    },
    "ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04.pdf": {
        "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies",
        "authors": [
            "Liangming Pan",
            "Michael Stephen Saxon",
            "Wenda Xu",
            "Deepak Nathani",
            "Xinyi Wang",
            "William Yang Wang"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04.pdf",
        "venue": "arXiv.org",
        "citationCount": 235,
        "score": 117.5
    },
    "93c525267e93c78309a5b28a3eb0780704125744.pdf": {
        "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Yiyang Zhou",
            "Chenhang Cui",
            "Jaehong Yoon",
            "Linjun Zhang",
            "Zhun Deng",
            "Chelsea Finn",
            "Mohit Bansal",
            "Huaxiu Yao"
        ],
        "published_date": "2023",
        "abstract": "Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top. Our data and code are available at https://github.com/YiyangZhou/LURE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/93c525267e93c78309a5b28a3eb0780704125744.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 220,
        "score": 110.0
    },
    "2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4.pdf": {
        "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
        "authors": [
            "Nick McKenna",
            "Tianyi Li",
            "Liang Cheng",
            "Mohammad Javad Hosseini",
            "Mark Johnson",
            "Mark Steedman"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences: we show that, regardless of the premise, models falsely label NLI test samples as entailing when the hypothesis is attested in training data, and that entities are used as ``indices'' to access the memorized data. Second, statistical patterns of usage learned at the level of corpora: we further show a similar effect when the premise predicate is less frequent than that of the hypothesis in the training data, a bias following from previous studies. We demonstrate that LLMs perform significantly worse on NLI test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future LLM evaluation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 220,
        "score": 110.0
    },
    "c946888e2f81b1db84ba4addf2a11e87f0568fe9.pdf": {
        "title": "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies",
        "authors": [
            "Liangming Pan",
            "Michael Stephen Saxon",
            "Wenda Xu",
            "Deepak Nathani",
            "Xinyi Wang",
            "W. Wang"
        ],
        "published_date": "2024",
        "abstract": "While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback\u2014either produced by the LLM itself (self-correction) or some external system\u2014are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c946888e2f81b1db84ba4addf2a11e87f0568fe9.pdf",
        "venue": "Transactions of the Association for Computational Linguistics",
        "citationCount": 100,
        "score": 100.0
    },
    "3b0792f6d7f6aa6aadd316e73943116afef2979b.pdf": {
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
        "authors": [
            "Logesh Kumar Umapathi",
            "Ankit Pal",
            "Malaikannan Sankarasubbu"
        ],
        "published_date": "2023",
        "abstract": "This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs\u2019 problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3b0792f6d7f6aa6aadd316e73943116afef2979b.pdf",
        "venue": "Conference on Computational Natural Language Learning",
        "citationCount": 164,
        "score": 82.0
    },
    "807f336176070bd3f95b82a16f125ee99b7d2c80.pdf": {
        "title": "Woodpecker: Hallucination Correction for Multimodal Large Language Models",
        "authors": [
            "Shukang Yin",
            "Chaoyou Fu",
            "Sirui Zhao",
            "Tong Xu",
            "Hao Wang",
            "Dianbo Sui",
            "Yunhang Shen",
            "Ke Li",
            "Xingguo Sun",
            "Enhong Chen"
        ],
        "published_date": "2023",
        "abstract": "Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/807f336176070bd3f95b82a16f125ee99b7d2c80.pdf",
        "venue": "Science China Information Sciences",
        "citationCount": 162,
        "score": 81.0
    },
    "1b387e3fbec0447c8bf2dcee21f6db59cdddf698.pdf": {
        "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
        "authors": [
            "Junyi Li",
            "Jie Chen",
            "Ruiyang Ren",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "Jian-yun Nie",
            "Ji-Rong Wen"
        ],
        "published_date": "2024",
        "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the LLM hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucination, focused on the the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet effective detection method for LLM hallucination. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucination. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs. Our code and data can be accessed at https://github.com/RUCAIBox/HaluEval-2.0.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1b387e3fbec0447c8bf2dcee21f6db59cdddf698.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 77,
        "score": 77.0
    },
    "5c204b2421d05b83d3c96a6c515cc03143073935.pdf": {
        "title": "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Wei Zou",
            "Runpeng Geng",
            "Binghui Wang",
            "Jinyuan Jia"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5c204b2421d05b83d3c96a6c515cc03143073935.pdf",
        "venue": "arXiv.org",
        "citationCount": 76,
        "score": 76.0,
        "summary": "",
        "keywords": []
    },
    "99bfe503743c5ec8e16e50ab8438159cdb533a89.pdf": {
        "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
        "authors": [
            "Vipula Rawte",
            "Swagata Chakraborty",
            "Agnibh Pathak",
            "Anubhav Sarkar",
            "S.M. Towhidul Islam Tonmoy",
            "Islam Tonmoy",
            "Aman Chadha",
            "Amit P. Sheth",
            "Amitava Das",
            "Paris",
            "A. Sridhar",
            "Erik Visser",
            "Improved",
            "Jianlin Su",
            "Yu Lu",
            "Shengfeng Pan",
            "Ahmed Murtadha",
            "Bo Wen",
            "Yunfeng Liu",
            "Roformer",
            "Rohan Taori",
            "Ishaan Gulrajani",
            "Tianyi Zhang",
            "Yann Dubois",
            "Xuechen Li",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori Hashimoto",
            "Stanford",
            "Hugo Touvron",
            "Thibaut Lavril",
            "Gautier Izacard",
            "Xavier Martinet",
            "M. Lachaux",
            "Timoth\u00e9e Lacroix",
            "Baptiste Rozi\u00e8re",
            "Naman Goyal",
            "Eric Hambro",
            "Faisal Azhar",
            "Aur'elien Rodriguez",
            "Armand Joulin",
            "Thomas Wolf",
            "Lysandre Debut",
            "Victor Sanh",
            "Julien Chaumond",
            "Clement Delangue",
            "Anthony Moi",
            "Pierric Cistac",
            "Tim Rault",
            "R\u00e9mi Louf",
            "Morgan Funtow-icz",
            "Joe Davison",
            "Sam Shleifer",
            "Patrick von Platen",
            "Clara Ma",
            "Yacine Jernite",
            "J. Plu",
            "Canwen Xu",
            "Teven Le Scao",
            "Sylvain Gugger",
            "Mariama Drame",
            "Quentin Lhoest",
            "Susan Zhang",
            "Stephen Roller",
            "Mikel Artetxe",
            "Moya Chen",
            "Shuohui Chen",
            "Christopher De-wan",
            "Mona T. Diab",
            "Xi Xian Li",
            "Todor Victoria Lin",
            "Myle Ott",
            "Kurt Shuster",
            "Punit Daniel Simig",
            "S. Koura",
            "Anjali Sridhar",
            "Tianlu Wang",
            "Luke Zettlemoyer. 2022",
            "Daniel M. Ziegler",
            "Nisan Stiennon",
            "Jeffrey Wu",
            "Tom B. Brown",
            "Alec Radford",
            "Dario Amodei",
            "Paul F. Chris-tiano"
        ],
        "published_date": "2023",
        "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/99bfe503743c5ec8e16e50ab8438159cdb533a89.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 149,
        "score": 74.5
    },
    "8c5acaafe43e710d55b08c63d567550ad26ec437.pdf": {
        "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
        "authors": [
            "Ekaterina Fadeeva",
            "Aleksandr Rubashevskii",
            "Artem Shelmanov",
            "Sergey Petrakov",
            "Haonan Li",
            "Hamdy Mubarak",
            "Evgenii Tsymbalov",
            "Gleb Kuzmin",
            "Alexander Panchenko",
            "Timothy Baldwin",
            "Preslav Nakov",
            "Maxim Panov"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8c5acaafe43e710d55b08c63d567550ad26ec437.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 73,
        "score": 73.0,
        "summary": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.",
        "keywords": []
    },
    "143a05fb36be8198d7675b594c0656b5652da3cb.pdf": {
        "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps",
        "authors": [
            "Yung-Sung Chuang",
            "Linlu Qiu",
            "Cheng-Yu Hsieh",
            "Ranjay Krishna",
            "Yoon Kim",
            "James Glass"
        ],
        "published_date": "2024",
        "abstract": "When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such **contextual hallucinations**. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these _lookback ratio_ features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector\u2014**Lookback Lens**\u2014is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/143a05fb36be8198d7675b594c0656b5652da3cb.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 66,
        "score": 66.0
    },
    "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b.pdf": {
        "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",
        "authors": [
            "Thuat Nguyen",
            "C. Nguyen",
            "Viet Dac Lai",
            "Hieu Man",
            "Nghia Trung Ngo",
            "Franck Dernoncourt",
            "Ryan A. Rossi",
            "Thien Huu Nguyen"
        ],
        "published_date": "2023",
        "abstract": "Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1ebcf1884390c28f24b3adaf5a7aba5b9453b48b.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 129,
        "score": 64.5,
        "summary": "Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.",
        "keywords": []
    },
    "e468ed6b824e60f45ba9a20b034e4090c6630751.pdf": {
        "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
        "authors": [
            "Xingxuan Li",
            "Ruochen Zhao",
            "Yew Ken Chia",
            "Bosheng Ding",
            "Shafiq R. Joty",
            "Soujanya Poria",
            "Lidong Bing"
        ],
        "published_date": "2023",
        "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e468ed6b824e60f45ba9a20b034e4090c6630751.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 120,
        "score": 60.0
    },
    "c14010990c9d75a6e836e1c86d42f405a5d3d0a6.pdf": {
        "title": "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models",
        "authors": [
            "Pranab Sahoo",
            "Prabhash Meharia",
            "Akash Ghosh",
            "Sriparna Saha",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "published_date": "2024",
        "abstract": "The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount. This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities. By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners. Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research in this pivotal area.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c14010990c9d75a6e836e1c86d42f405a5d3d0a6.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 59,
        "score": 59.0
    },
    "19e909f88b8b9b0635bd6e441094e1738c3bba9a.pdf": {
        "title": "Unified Hallucination Detection for Multimodal Large Language Models",
        "authors": [
            "Xiang Chen",
            "Chenxi Wang",
            "Yida Xue",
            "Ningyu Zhang",
            "Xiaoyan Yang",
            "Qian Li",
            "Yue Shen",
            "Lei Liang",
            "Jinjie Gu",
            "Huajun Chen"
        ],
        "published_date": "2024",
        "abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/19e909f88b8b9b0635bd6e441094e1738c3bba9a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 53,
        "score": 53.0
    },
    "f4e06256ab07727ff4e0465deea83fcf45012354.pdf": {
        "title": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Wei Zou",
            "Runpeng Geng",
            "Binghui Wang",
            "Jinyuan Jia"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f4e06256ab07727ff4e0465deea83fcf45012354.pdf",
        "venue": "",
        "citationCount": 53,
        "score": 53.0,
        "summary": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.",
        "keywords": []
    },
    "e17c58d7a48b6b811df023484161a3b9c03e0d6b.pdf": {
        "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
        "authors": [
            "Huao Li",
            "Yu Quan Chong",
            "Simon Stepputtis",
            "Joseph Campbell",
            "Dana Hughes",
            "Michael Lewis",
            "Katia P. Sycara"
        ],
        "published_date": "2023",
        "abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e17c58d7a48b6b811df023484161a3b9c03e0d6b.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 103,
        "score": 51.5,
        "summary": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
        "keywords": []
    },
    "2338d7c9ab07e6d0f4160335dce0e6e6a87c4749.pdf": {
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "authors": [
            "Yiming Wang",
            "Zhuosheng Zhang",
            "Rui Wang"
        ],
        "published_date": "2023",
        "abstract": "Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the \u201cLasswell Communication Model\u201d proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs\u2019 zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2338d7c9ab07e6d0f4160335dce0e6e6a87c4749.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 101,
        "score": 50.5,
        "summary": "Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the \u201cLasswell Communication Model\u201d proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs\u2019 zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
        "keywords": []
    },
    "ca261cb681b082e90ca6c7a9d325b4265ed1dc28.pdf": {
        "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
        "authors": [
            "Yilin Wen",
            "Zifeng Wang",
            "Jimeng Sun"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ca261cb681b082e90ca6c7a9d325b4265ed1dc28.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 96,
        "score": 48.0
    },
    "4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2.pdf": {
        "title": "Visual Hallucinations of Multi-modal Large Language Models",
        "authors": [
            "Wen Huang",
            "Hongbin Liu",
            "Minxin Guo",
            "N. Gong"
        ],
        "published_date": "2024",
        "abstract": "Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 48,
        "score": 48.0
    },
    "fac468032e0c38ea10dfb95ba6cdeac51a473050.pdf": {
        "title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models",
        "authors": [
            "Yuyan Chen",
            "Qiang Fu",
            "Yichen Yuan",
            "Zhihao Wen",
            "Ge Fan",
            "Dayiheng Liu",
            "Dongmei Zhang",
            "Zhixu Li",
            "Yanghua Xiao"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have gained widespread adoption in various natural language processing tasks, including question answering and dialogue systems. However, a major drawback of LLMs is the issue of hallucination, where they generate unfaithful or inconsistent content that deviates from the input source, leading to severe consequences. In this paper, we propose a robust discriminator named RelD to effectively detect hallucination in LLMs' generated answers. RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics. Our experimental results demonstrate that the proposed RelD successfully detects hallucination in the answers generated by diverse LLMs. Moreover, it performs well in distinguishing hallucination in LLMs' generated answers from both in-distribution and out-of-distribution datasets. Additionally, we also conduct a thorough analysis of the types of hallucinations that occur and present valuable insights. This research significantly contributes to the detection of reliable answers generated by LLMs and holds noteworthy implications for mitigating hallucination in the future work.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fac468032e0c38ea10dfb95ba6cdeac51a473050.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 92,
        "score": 46.0,
        "summary": "Large language models (LLMs) have gained widespread adoption in various natural language processing tasks, including question answering and dialogue systems. However, a major drawback of LLMs is the issue of hallucination, where they generate unfaithful or inconsistent content that deviates from the input source, leading to severe consequences. In this paper, we propose a robust discriminator named RelD to effectively detect hallucination in LLMs' generated answers. RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics. Our experimental results demonstrate that the proposed RelD successfully detects hallucination in the answers generated by diverse LLMs. Moreover, it performs well in distinguishing hallucination in LLMs' generated answers from both in-distribution and out-of-distribution datasets. Additionally, we also conduct a thorough analysis of the types of hallucinations that occur and present valuable insights. This research significantly contributes to the detection of reliable answers generated by LLMs and holds noteworthy implications for mitigating hallucination in the future work.",
        "keywords": []
    },
    "3e7b421f9df1bd34d9cb42ece2760269f0314f05.pdf": {
        "title": "Augmented non-hallucinating large language models as medical information curators",
        "authors": [
            "S. Gilbert",
            "J. Kather",
            "Aidan Hogan"
        ],
        "published_date": "2024",
        "abstract": "Reliably processing and interlinking medical information has been recognized as a critical foundation to the digital transformation of medical workflows, and despite the development of medical ontologies, the optimization of these has been a major bottleneck to digital medicine. The advent of large language models has brought great excitement, and maybe a solution to the medicines\u2019 \u2018communication problem\u2019 is in sight, but how can the known weaknesses of these models, such as hallucination and non-determinism, be tempered? Retrieval Augmented Generation, particularly through knowledge graphs, is an automated approach that can deliver structured reasoning and a model of truth alongside LLMs, relevant to information structuring and therefore also to decision support.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3e7b421f9df1bd34d9cb42ece2760269f0314f05.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 46,
        "score": 46.0,
        "summary": "Reliably processing and interlinking medical information has been recognized as a critical foundation to the digital transformation of medical workflows, and despite the development of medical ontologies, the optimization of these has been a major bottleneck to digital medicine. The advent of large language models has brought great excitement, and maybe a solution to the medicines\u2019 \u2018communication problem\u2019 is in sight, but how can the known weaknesses of these models, such as hallucination and non-determinism, be tempered? Retrieval Augmented Generation, particularly through knowledge graphs, is an automated approach that can deliver structured reasoning and a model of truth alongside LLMs, relevant to information structuring and therefore also to decision support.",
        "keywords": []
    },
    "25a9c0946925cc860c4600dba91b313cdbe7c8a8.pdf": {
        "title": "Large Language Models: A Guide for Radiologists",
        "authors": [
            "Sunkyu Kim",
            "Choong-kun Lee",
            "Seung-seob Kim"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have revolutionized the global landscape of technology beyond natural language processing. Owing to their extensive pre-training on vast datasets, contemporary LLMs can handle tasks ranging from general functionalities to domain-specific areas, such as radiology, without additional fine-tuning. General-purpose chatbots based on LLMs can optimize the efficiency of radiologists in terms of their professional work and research endeavors. Importantly, these LLMs are on a trajectory of rapid evolution, wherein challenges such as \u201challucination,\u201d high training cost, and efficiency issues are addressed, along with the inclusion of multimodal inputs. In this review, we aim to offer conceptual knowledge and actionable guidance to radiologists interested in utilizing LLMs through a succinct overview of the topic and a summary of radiology-specific aspects, from the beginning to potential future directions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/25a9c0946925cc860c4600dba91b313cdbe7c8a8.pdf",
        "venue": "Korean Journal of Radiology",
        "citationCount": 46,
        "score": 46.0,
        "summary": "Large language models (LLMs) have revolutionized the global landscape of technology beyond natural language processing. Owing to their extensive pre-training on vast datasets, contemporary LLMs can handle tasks ranging from general functionalities to domain-specific areas, such as radiology, without additional fine-tuning. General-purpose chatbots based on LLMs can optimize the efficiency of radiologists in terms of their professional work and research endeavors. Importantly, these LLMs are on a trajectory of rapid evolution, wherein challenges such as \u201challucination,\u201d high training cost, and efficiency issues are addressed, along with the inclusion of multimodal inputs. In this review, we aim to offer conceptual knowledge and actionable guidance to radiologists interested in utilizing LLMs through a succinct overview of the topic and a summary of radiology-specific aspects, from the beginning to potential future directions.",
        "keywords": []
    },
    "396305230ddcf915b19a19683a89e34d76321a33.pdf": {
        "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models",
        "authors": [
            "Hongbin Ye",
            "Tong Liu",
            "Aijia Zhang",
            "Wei Hua",
            "Weiqiang Jia"
        ],
        "published_date": "2023",
        "abstract": "As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/396305230ddcf915b19a19683a89e34d76321a33.pdf",
        "venue": "LKM@IJCAI",
        "citationCount": 90,
        "score": 45.0
    },
    "27d55a944b5c02b8c10eb250773d8eb082e06476.pdf": {
        "title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback",
        "authors": [
            "Wenyi Xiao",
            "Ziwei Huang",
            "Leilei Gan",
            "Wanggui He",
            "Haoyuan Li",
            "Zhelun Yu",
            "Hao Jiang",
            "Fei Wu",
            "Linchao Zhu"
        ],
        "published_date": "2024",
        "abstract": "The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts, significantly restricting the usages of LVLMs. Most previous work detects and mitigates hallucination at the coarse-grained level or requires expensive annotation (e.g., labeling by proprietary models or human experts). To address these issues, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a hallucination detection model which can perform sentence-level hallucination detection, covering primary hallucination types (i.e., object, attribute, and relationship). Then, we propose a detect-then-rewrite pipeline to automatically construct preference dataset for training hallucination mitigating model. Furthermore, we propose differentiating the severity of hallucinations, and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating hallucination in LVLMs by incorporating the severity of hallucinations into preference learning. Extensive experiments demonstrate the effectiveness of our method.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/27d55a944b5c02b8c10eb250773d8eb082e06476.pdf",
        "venue": "arXiv.org",
        "citationCount": 43,
        "score": 43.0
    },
    "58ee9e1c426166a5451a1ce13e1186f7d6baacfd.pdf": {
        "title": "VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models",
        "authors": [
            "Yuxuan Wang",
            "Yueqian Wang",
            "Dongyan Zhao",
            "Cihang Xie",
            "Zilong Zheng"
        ],
        "published_date": "2024",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have extended their capabilities to video understanding. Yet, these models are often plagued by\"hallucinations\", where irrelevant or nonsensical content is generated, deviating from the actual video context. This work introduces VideoHallucer, the first comprehensive benchmark for hallucination detection in large video-language models (LVLMs). VideoHallucer categorizes hallucinations into two main types: intrinsic and extrinsic, offering further subcategories for detailed analysis, including object-relation, temporal, semantic detail, extrinsic factual, and extrinsic non-factual hallucinations. We adopt an adversarial binary VideoQA method for comprehensive evaluation, where pairs of basic and hallucinated questions are crafted strategically. By evaluating eleven LVLMs on VideoHallucer, we reveal that i) the majority of current models exhibit significant issues with hallucinations; ii) while scaling datasets and parameters improves models' ability to detect basic visual cues and counterfactuals, it provides limited benefit for detecting extrinsic factual hallucinations; iii) existing models are more adept at detecting facts than identifying hallucinations. As a byproduct, these analyses further instruct the development of our self-PEP framework, achieving an average of 5.38% improvement in hallucination resistance across all model architectures.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/58ee9e1c426166a5451a1ce13e1186f7d6baacfd.pdf",
        "venue": "arXiv.org",
        "citationCount": 43,
        "score": 43.0
    },
    "d299a6b26e9ee23d0337a1d1a896fc1c847f5a46.pdf": {
        "title": "InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment",
        "authors": [
            "Jianing Wang",
            "Junda Wu",
            "Yupeng Hou",
            "Yao Liu",
            "Ming Gao",
            "Julian McAuley"
        ],
        "published_date": "2024",
        "abstract": "Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\\% and 38\\%, respectively.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d299a6b26e9ee23d0337a1d1a896fc1c847f5a46.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 42,
        "score": 42.0,
        "summary": "Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\\% and 38\\%, respectively.",
        "keywords": []
    },
    "ea0d41514a41f8273f13b3b277e7fcbbc65a8549.pdf": {
        "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
        "authors": [
            "Yuheng Huang",
            "Jiayang Song",
            "Zhijie Wang",
            "Huaming Chen",
            "Lei Ma"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ea0d41514a41f8273f13b3b277e7fcbbc65a8549.pdf",
        "venue": "arXiv.org",
        "citationCount": 83,
        "score": 41.5,
        "summary": "",
        "keywords": []
    },
    "411b725522e2747e890ba5acfbf43d22f759c00a.pdf": {
        "title": "Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models",
        "authors": [
            "Weihang Su",
            "Changyue Wang",
            "Qingyao Ai",
            "Hu Yiran",
            "Zhijing Wu",
            "Yujia Zhou",
            "Yiqun Liu"
        ],
        "published_date": "2024",
        "abstract": "Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/411b725522e2747e890ba5acfbf43d22f759c00a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 41,
        "score": 41.0
    },
    "bb1083425517bdac8d9a6438fcf5032543acb20e.pdf": {
        "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models",
        "authors": [
            "Junyan Wang",
            "Yi Zhou",
            "Guohai Xu",
            "Pengcheng Shi",
            "Chenlin Zhao",
            "Haiyang Xu",
            "Qinghao Ye",
            "Mingshi Yan",
            "Ji Zhang",
            "Jihua Zhu",
            "Jitao Sang",
            "Haoyu Tang"
        ],
        "published_date": "2023",
        "abstract": "Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation hallucination data will be made public soon.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/bb1083425517bdac8d9a6438fcf5032543acb20e.pdf",
        "venue": "arXiv.org",
        "citationCount": 81,
        "score": 40.5
    },
    "cd2e04598909158494e556823d9de8baa692cee2.pdf": {
        "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
        "authors": [
            "Ziwei Ji",
            "Tiezheng Yu",
            "Yan Xu",
            "Nayeon Lee",
            "Etsuko Ishii",
            "Pascale Fung"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/cd2e04598909158494e556823d9de8baa692cee2.pdf",
        "venue": "arXiv.org",
        "citationCount": 80,
        "score": 40.0
    },
    "a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824.pdf": {
        "title": "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance",
        "authors": [
            "Linxi Zhao",
            "Yihe Deng",
            "Weitong Zhang",
            "Quanquan Gu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824.pdf",
        "venue": "arXiv.org",
        "citationCount": 37,
        "score": 37.0,
        "summary": "",
        "keywords": []
    },
    "24b6b70e1b1525535155cc9fa66dfd9d5d42d6b5.pdf": {
        "title": "HILL: A Hallucination Identifier for Large Language Models",
        "authors": [
            "Florian Leiser",
            "S. Eckhardt",
            "Valentin Leuthe",
            "Merlin Knaeble",
            "Alexander Maedche",
            "Gerhard Schwabe",
            "A. Sunyaev"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILL\u2019s interface design by surveying 17 participants. Further, we investigated HILL\u2019s functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/24b6b70e1b1525535155cc9fa66dfd9d5d42d6b5.pdf",
        "venue": "International Conference on Human Factors in Computing Systems",
        "citationCount": 35,
        "score": 35.0,
        "summary": "Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILL\u2019s interface design by surveying 17 participants. Further, we investigated HILL\u2019s functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts.",
        "keywords": []
    },
    "73020a07af4cfc42286e299097a0e35d2fe71a6c.pdf": {
        "title": "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization",
        "authors": [
            "Yuxi Xie",
            "Guanzhen Li",
            "Xiao Xu",
            "Min-Yen Kan"
        ],
        "published_date": "2024",
        "abstract": "Large vision-language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content. Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs. We tackle this issue of hallucination by mitigating such over-reliance through preference learning. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time. To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples. Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks. Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context. Our code is publicly available at https://github.com/YuxiXie/V-DPO.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/73020a07af4cfc42286e299097a0e35d2fe71a6c.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 33,
        "score": 33.0
    },
    "6a9a9120d746a3c29902548bd1d93d6ea034c5d7.pdf": {
        "title": "Sycophancy in Large Language Models: Causes and Mitigations",
        "authors": [
            "Lars Malmqvist"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. However, their tendency to exhibit sycophantic behavior - excessively agreeing with or flattering users - poses significant risks to their reliability and ethical deployment. This paper provides a technical survey of sycophancy in LLMs, analyzing its causes, impacts, and potential mitigation strategies. We review recent work on measuring and quantifying sycophantic tendencies, examine the relationship between sycophancy and other challenges like hallucination and bias, and evaluate promising techniques for reducing sycophancy while maintaining model performance. Key approaches explored include improved training data, novel fine-tuning methods, post-deployment control mechanisms, and decoding strategies. We also discuss the broader implications of sycophancy for AI alignment and propose directions for future research. Our analysis suggests that mitigating sycophancy is crucial for developing more robust, reliable, and ethically-aligned language models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6a9a9120d746a3c29902548bd1d93d6ea034c5d7.pdf",
        "venue": "arXiv.org",
        "citationCount": 33,
        "score": 33.0,
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. However, their tendency to exhibit sycophantic behavior - excessively agreeing with or flattering users - poses significant risks to their reliability and ethical deployment. This paper provides a technical survey of sycophancy in LLMs, analyzing its causes, impacts, and potential mitigation strategies. We review recent work on measuring and quantifying sycophantic tendencies, examine the relationship between sycophancy and other challenges like hallucination and bias, and evaluate promising techniques for reducing sycophancy while maintaining model performance. Key approaches explored include improved training data, novel fine-tuning methods, post-deployment control mechanisms, and decoding strategies. We also discuss the broader implications of sycophancy for AI alignment and propose directions for future research. Our analysis suggests that mitigating sycophancy is crucial for developing more robust, reliable, and ethically-aligned language models.",
        "keywords": []
    },
    "3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e.pdf": {
        "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations",
        "authors": [
            "Yue Zhang",
            "Leyang Cui",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published_date": "2023",
        "abstract": "Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as ``hallucination''. In this work, we propose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various model sizes and families. For example, when equipped with ICD, Llama2-7B-Chat and Mistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on TruthfulQA, respectively.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 64,
        "score": 32.0
    },
    "39d8486475173357619647061dda377f4c38853e.pdf": {
        "title": "Multi-Object Hallucination in Vision-Language Models",
        "authors": [
            "Xuweiyi Chen",
            "Ziqiao Ma",
            "Xuejun Zhang",
            "Sihan Xu",
            "Shengyi Qian",
            "Jianing Yang",
            "D. Fouhey",
            "Joyce Chai"
        ],
        "published_date": "2024",
        "abstract": "Large vision language models (LVLMs) often suffer from object hallucination, producing objects not present in the given images. While current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities, this work systematically investigates multi-object hallucination, examining how models misperceive (e.g., invent nonexistent objects or become distracted) when tasked with focusing on multiple objects simultaneously. We introduce Recognition-based Object Probing Evaluation (ROPE), an automated evaluation protocol that considers the distribution of object classes within a single image during testing and uses visual referring prompts to eliminate ambiguity. With comprehensive empirical studies and analysis of potential factors leading to multi-object hallucination, we found that (1). LVLMs suffer more hallucinations when focusing on multiple objects compared to a single object. (2). The tested object class distribution affects hallucination behaviors, indicating that LVLMs may follow shortcuts and spurious correlations. (3). Hallucinatory behaviors are influenced by data-specific factors, salience and frequency, and model intrinsic behaviors. We hope to enable LVLMs to recognize and reason about multiple objects that often occur in realistic visual scenes, provide insights, and quantify our progress towards mitigating the issues.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/39d8486475173357619647061dda377f4c38853e.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 31,
        "score": 31.0
    },
    "95a52dd5adf6eb8d918cdfbf6189aab4eaa8e607.pdf": {
        "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
        "authors": [
            "Sheng-Chieh Lin",
            "Luyu Gao",
            "Barlas O\u011fuz",
            "Wenhan Xiong",
            "Jimmy Lin",
            "Wen-tau Yih",
            "Xilun Chen"
        ],
        "published_date": "2024",
        "abstract": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/95a52dd5adf6eb8d918cdfbf6189aab4eaa8e607.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 31,
        "score": 31.0,
        "summary": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
        "keywords": []
    },
    "80fd20e175f83a699258b8780cf365418d1538b0.pdf": {
        "title": "Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases",
        "authors": [
            "Xingxuan Li",
            "Ruochen Zhao",
            "Yew Ken Chia",
            "Bosheng Ding",
            "Lidong Bing",
            "Shafiq R. Joty",
            "Soujanya Poria"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/80fd20e175f83a699258b8780cf365418d1538b0.pdf",
        "venue": "arXiv.org",
        "citationCount": 62,
        "score": 31.0,
        "summary": "",
        "keywords": []
    },
    "97d24f9f0d81007d57cc43e61bf2b0c9081fe184.pdf": {
        "title": "R-Tuning: Instructing Large Language Models to Say \u2018I Don\u2019t Know\u2019",
        "authors": [
            "Hanning Zhang",
            "Shizhe Diao",
            "Yong Lin",
            "Y. Fung",
            "Qing Lian",
            "Xingyao Wang",
            "Yangyi Chen",
            "Heng Ji",
            "Tong Zhang"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a model\u2019s ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing. Our code is available at https://github.com/shizhediao/R-Tuning",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/97d24f9f0d81007d57cc43e61bf2b0c9081fe184.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 62,
        "score": 31.0
    },
    "81bd66d960503106ef969830568016da4f93754a.pdf": {
        "title": "Vista-llama: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens",
        "authors": [
            "Fan Ma",
            "Xiaojie Jin",
            "Heng Wang",
            "Yuchen Xian",
            "Jiashi Feng",
            "Yi Yang"
        ],
        "published_date": "2023",
        "abstract": "Recent advances in large video-language models have displayed promising outcomes in video comprehension. Current approaches straightforwardly convert video into language tokens and employ large language models for multi-modal tasks. However, this method often leads to the generation of irrelevant content, commonly known as \u201challucination\u201d, as the length of the text increases and the impact of the video diminishes. To address this problem, we propose Vista-llama, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length. Vista-llama omits relative position encoding when determining attention weights between visual and text tokens, retaining the position encoding for text and text tokens. This amplifies the effect of visual tokens on text generation, especially when the relative distance is longer between visual and text tokens. The proposed attention mechanism significantly reduces the chance of producing irrelevant text related to the video content. Furthermore, we present a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame. This approach not only captures the temporal relationship within the video, but also allows less visual tokens to encompass the entire video. Our approach significantly outperforms various previous methods (e.g., Video-ChatGPT, MovieChat) on four challenging open-ended video question answering benchmarks. We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot MSRVTT-QA, setting a new state-of-the-art performance. This project is available at https://jinxxian.github.iolVista-LLaMA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/81bd66d960503106ef969830568016da4f93754a.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 62,
        "score": 31.0,
        "summary": "Recent advances in large video-language models have displayed promising outcomes in video comprehension. Current approaches straightforwardly convert video into language tokens and employ large language models for multi-modal tasks. However, this method often leads to the generation of irrelevant content, commonly known as \u201challucination\u201d, as the length of the text increases and the impact of the video diminishes. To address this problem, we propose Vista-llama, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length. Vista-llama omits relative position encoding when determining attention weights between visual and text tokens, retaining the position encoding for text and text tokens. This amplifies the effect of visual tokens on text generation, especially when the relative distance is longer between visual and text tokens. The proposed attention mechanism significantly reduces the chance of producing irrelevant text related to the video content. Furthermore, we present a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame. This approach not only captures the temporal relationship within the video, but also allows less visual tokens to encompass the entire video. Our approach significantly outperforms various previous methods (e.g., Video-ChatGPT, MovieChat) on four challenging open-ended video question answering benchmarks. We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot MSRVTT-QA, setting a new state-of-the-art performance. This project is available at https://jinxxian.github.iolVista-LLaMA.",
        "keywords": []
    },
    "91ca6535fc8fb03efe0ecbe424ce5354ed129b0c.pdf": {
        "title": "Towards Large Language Models as Copilots for Theorem Proving in Lean",
        "authors": [
            "Peiyang Song",
            "Kaiyu Yang",
            "Anima Anandkumar"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/91ca6535fc8fb03efe0ecbe424ce5354ed129b0c.pdf",
        "venue": "arXiv.org",
        "citationCount": 30,
        "score": 30.0,
        "summary": "",
        "keywords": []
    },
    "d48c56dbce88580736c037797666060cb3b03bf7.pdf": {
        "title": "On Large Language Models\u2019 Hallucination with Regard to Known Facts",
        "authors": [
            "Che Jiang",
            "Biqing Qi",
            "Xiangyu Hong",
            "Dayuan Fu",
            "Yang Cheng",
            "Fandong Meng",
            "Mo Yu",
            "Bowen Zhou",
            "Jie Zhou"
        ],
        "published_date": "2024",
        "abstract": "Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen.Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space.We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token\u2019s information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model.Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88% success rate. Our study shed light on understanding the reasons for LLMs\u2019 hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d48c56dbce88580736c037797666060cb3b03bf7.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 29,
        "score": 29.0,
        "summary": "Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen.Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space.We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token\u2019s information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model.Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88% success rate. Our study shed light on understanding the reasons for LLMs\u2019 hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.",
        "keywords": []
    },
    "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f.pdf": {
        "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
        "authors": [
            "Lei Wang",
            "Jiabang He",
            "Shenshen Li",
            "Ning Liu",
            "Ee-Peng Lim"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f.pdf",
        "venue": "Conference on Multimedia Modeling",
        "citationCount": 58,
        "score": 29.0
    },
    "80248c8c7cbb5bb1d2a508001108f3f15bb60430.pdf": {
        "title": "Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models",
        "authors": [
            "Jun Wu",
            "Q. Liu",
            "Ding Wang",
            "Jinghao Zhang",
            "Shu Wu",
            "Liang Wang",
            "Tien-Ping Tan"
        ],
        "published_date": "2024",
        "abstract": "Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/80248c8c7cbb5bb1d2a508001108f3f15bb60430.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 29,
        "score": 29.0
    },
    "668341051f3c9c087e42e393c610792df3e45992.pdf": {
        "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean",
        "authors": [
            "Peiyang Song",
            "Kaiyu Yang",
            "Anima Anandkumar"
        ],
        "published_date": "2024",
        "abstract": "Neural theorem proving combines large language models (LLMs) with proof assistants such as Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for hallucination. With existing neural theorem provers pretrained on a fixed collection of data and offering valuable suggestions at times, it is challenging for them to continually prove novel theorems in a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs as copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework for running LLM inference natively in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can use our pretrained models or bring their own ones that run either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete proof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared to existing rule-based proof automation in Lean (aesop). When assisting humans, Lean Copilot requires only 2.08 manually-entered proof steps on average (3.86 required by aesop); when automating the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/668341051f3c9c087e42e393c610792df3e45992.pdf",
        "venue": "NeuS",
        "citationCount": 28,
        "score": 28.0,
        "summary": "Neural theorem proving combines large language models (LLMs) with proof assistants such as Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for hallucination. With existing neural theorem provers pretrained on a fixed collection of data and offering valuable suggestions at times, it is challenging for them to continually prove novel theorems in a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs as copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework for running LLM inference natively in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can use our pretrained models or bring their own ones that run either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete proof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared to existing rule-based proof automation in Lean (aesop). When assisting humans, Lean Copilot requires only 2.08 manually-entered proof steps on average (3.86 required by aesop); when automating the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research.",
        "keywords": []
    },
    "3dea23e10eeff848f7352b17bbc1fdce38112acc.pdf": {
        "title": "Knowledge Graph-Enhanced Large Language Models via Path Selection",
        "authors": [
            "Haochen Liu",
            "Song Wang",
            "Yaochen Zhu",
            "Yushun Dong",
            "Jundong Li"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3dea23e10eeff848f7352b17bbc1fdce38112acc.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 27,
        "score": 27.0,
        "summary": "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.",
        "keywords": []
    },
    "31968a970f14beab3cbadf9f6ad45c1a51f4ea95.pdf": {
        "title": "Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem",
        "authors": [
            "Yuhong Sun",
            "Zhangyue Yin",
            "Qipeng Guo",
            "Jiawen Wu",
            "Xipeng Qiu",
            "Hui Zhao"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model\u2019s ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at https://github.com/Yuki-Asuuna/UMWP.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/31968a970f14beab3cbadf9f6ad45c1a51f4ea95.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 26,
        "score": 26.0,
        "summary": "Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the model\u2019s ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at https://github.com/Yuki-Asuuna/UMWP.",
        "keywords": []
    },
    "25243632a6159c19db280e2f0064aa59562a518a.pdf": {
        "title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models",
        "authors": [
            "Hanxing Ding",
            "Liang Pang",
            "Zihao Wei",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published_date": "2024",
        "abstract": "Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations. While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations. A careful and balanced integration of the parametric knowledge within LLMs with external information is crucial to alleviate hallucinations. In this study, we present Rowen, a novel approach that enhances LLMs with a selective retrieval augmentation process tailored to address hallucinated outputs. This process is governed by a multilingual semantic-aware detection module, which evaluates the consistency of the perturbed responses across various languages for the same queries. Upon detecting inconsistencies indicative of hallucinations, Rowen activates the retrieval of external information to rectify the model outputs. Rowen adeptly harmonizes the intrinsic parameters in LLMs with external knowledge sources, effectively mitigating hallucinations by ensuring a balanced integration of internal reasoning and external evidence. Through a comprehensive empirical analysis, we demonstrate that Rowen surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/25243632a6159c19db280e2f0064aa59562a518a.pdf",
        "venue": "arXiv.org",
        "citationCount": 26,
        "score": 26.0
    },
    "be8c90bca14d59f180f40a41126b7cd8c29c5d4e.pdf": {
        "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Wei Cheng",
            "Yanchi Liu",
            "Yiyou Sun",
            "Xuchao Zhang",
            "Mika Oishi",
            "Takao Osaki",
            "Katsushi Matsuda",
            "Jie Ji",
            "Guangji Bai",
            "Liang Zhao",
            "Haifeng Chen"
        ],
        "published_date": "2024",
        "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/be8c90bca14d59f180f40a41126b7cd8c29c5d4e.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 26,
        "score": 26.0,
        "summary": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
        "keywords": []
    },
    "aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6.pdf": {
        "title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection",
        "authors": [
            "Moxin Li",
            "Wenjie Wang",
            "Fuli Feng",
            "Fengbin Zhu",
            "Qifan Wang",
            "Tat-Seng Chua"
        ],
        "published_date": "2024",
        "abstract": "Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection approaches only retrospectively evaluate answers generated by LLM, typically leading to the over-trust in incorrectly generated answers. To tackle this limitation, we propose a novel self-detection paradigm that considers the comprehensive answer space beyond LLM-generated answers. It thoroughly compares the trustworthiness of multiple candidate answers to mitigate the over-trust in LLM-generated incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self-detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 25,
        "score": 25.0,
        "summary": "Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection approaches only retrospectively evaluate answers generated by LLM, typically leading to the over-trust in incorrectly generated answers. To tackle this limitation, we propose a novel self-detection paradigm that considers the comprehensive answer space beyond LLM-generated answers. It thoroughly compares the trustworthiness of multiple candidate answers to mitigate the over-trust in LLM-generated incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self-detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.",
        "keywords": []
    },
    "694b753385820ea675f2ca80dcdb4c91fc05962a.pdf": {
        "title": "Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language Models",
        "authors": [
            "Andrew L Smith",
            "Felix Greaves",
            "T. Panch"
        ],
        "published_date": "2023",
        "abstract": "Language Models (LLMs) and their capabilities become an increasingly prominent aspect of our workflows and our lives",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/694b753385820ea675f2ca80dcdb4c91fc05962a.pdf",
        "venue": "PLOS Digital Health",
        "citationCount": 49,
        "score": 24.5,
        "summary": "Language Models (LLMs) and their capabilities become an increasingly prominent aspect of our workflows and our lives",
        "keywords": []
    },
    "171807aeeb88f0c7983bc6cc960b5605441d7121.pdf": {
        "title": "Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation",
        "authors": [
            "Xiaoye Qu",
            "Qiyuan Chen",
            "Wei Wei",
            "Jiashuo Sun",
            "Jianfeng Dong"
        ],
        "published_date": "2024",
        "abstract": "Despite the remarkable ability of large vision-language models (LVLMs) in image comprehension, these models frequently generate plausible yet factually incorrect responses, a phenomenon known as hallucination. Recently, in large language models (LLMs), augmenting LLMs by retrieving information from external knowledge resources has been proven as a promising solution to mitigate hallucinations. However, the retrieval augmentation in LVLM significantly lags behind the widespread applications of LVLM. Moreover, when transferred to augmenting LVLMs, sometimes the hallucination degree of the model is even exacerbated. Motivated by the research gap and counter-intuitive phenomenon, we introduce a novel framework, the Active Retrieval-Augmented large vision-language model (ARA), specifically designed to address hallucinations by incorporating three critical dimensions: (i) dissecting the retrieval targets based on the inherent hierarchical structures of images. (ii) pinpointing the most effective retrieval methods and filtering out the reliable retrieval results. (iii) timing the retrieval process to coincide with episodes of low certainty, while circumventing unnecessary retrieval during periods of high certainty. To assess the capability of our proposed ARA model in reducing hallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and mPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by utilizing fitting retrieval mechanisms and timing the retrieval judiciously, we can effectively mitigate the hallucination problem. We hope that this study can provide deeper insights into how to adapt the retrieval augmentation to LVLMs for reducing hallucinations with more effective retrieval and minimal retrieval occurrences.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/171807aeeb88f0c7983bc6cc960b5605441d7121.pdf",
        "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
        "citationCount": 24,
        "score": 24.0
    },
    "6a2e0927914ef03f25b99c2666f2275e0c950e5d.pdf": {
        "title": "Accuracy, Consistency, and Hallucination of Large Language Models When Analyzing Unstructured Clinical Notes in Electronic Medical Records.",
        "authors": [
            "Savyasachi V. Shah"
        ],
        "published_date": "2024",
        "abstract": "Researchers have been exploring advanced artificial intelligence (AI) techniques such as large language models (LLMs) to enhance data extraction from clinical notes in electronic medical records (EMRs), a critical need given the vast amounts of unstructured data in clinical notes. Evolving LLMs, their handling of details in prompting, and hallucinations in replication warrant a comparison with conventional approaches to understand the tradeoff between efficiency and accuracy. One such study by Burford et al 1 investigates the utility and efficacy of an LLM, ChatGPT-4 (OpenAI), in extracting helmet status from clinical narratives of patients involved in micromobility-related accidents. 1 This study by Burford et al 1 leverages data from the US Consumer Product Safety Commission (CPSC) National Electronic Injury Surveillance System (NEISS) spanning 2019 to 2022, including 54729 emergency department (ED) visits among patients with a micromobility accident. The primary objective was to compare the LLM\u2019s performance with the text string\u2013search approach in identifying whether patients were wearing helmets at the time of their accidents. Three different levels of prompting detail for the LLM (low, intermediate, and high) were employed, and the agreementwiththetextstring\u2013searchapproachwasmeasuredusingCohen\u03bateststatistics.Thetest-retestreliabilityofthehigh-detailpromptwasmeasuredacrossnewchatsessionson5differentdays using Fleiss \u03ba statistics. Performance statistics were calculated for a criterion standard review in a small random sample of 400 records comparing results from the high-detail prompt and text string\u2013 search approach to classifications of helmet status generated by researchers reading the clinical notes. Burford and colleagues 1 found moderate agreement (Cohen \u03ba = 0.74 [95% CI, 0.73-0.75]) for the low-detail prompt and weak agreement (Cohen \u03ba = 0.53 [95% CI, 0.52-0.54]) for the intermediate-detail prompt compared with the text string\u2013search approach. The high-detail prompt, which included comprehensive researcher-generated",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6a2e0927914ef03f25b99c2666f2275e0c950e5d.pdf",
        "venue": "JAMA Network Open",
        "citationCount": 24,
        "score": 24.0,
        "summary": "Researchers have been exploring advanced artificial intelligence (AI) techniques such as large language models (LLMs) to enhance data extraction from clinical notes in electronic medical records (EMRs), a critical need given the vast amounts of unstructured data in clinical notes. Evolving LLMs, their handling of details in prompting, and hallucinations in replication warrant a comparison with conventional approaches to understand the tradeoff between efficiency and accuracy. One such study by Burford et al 1 investigates the utility and efficacy of an LLM, ChatGPT-4 (OpenAI), in extracting helmet status from clinical narratives of patients involved in micromobility-related accidents. 1 This study by Burford et al 1 leverages data from the US Consumer Product Safety Commission (CPSC) National Electronic Injury Surveillance System (NEISS) spanning 2019 to 2022, including 54729 emergency department (ED) visits among patients with a micromobility accident. The primary objective was to compare the LLM\u2019s performance with the text string\u2013search approach in identifying whether patients were wearing helmets at the time of their accidents. Three different levels of prompting detail for the LLM (low, intermediate, and high) were employed, and the agreementwiththetextstring\u2013searchapproachwasmeasuredusingCohen\u03bateststatistics.Thetest-retestreliabilityofthehigh-detailpromptwasmeasuredacrossnewchatsessionson5differentdays using Fleiss \u03ba statistics. Performance statistics were calculated for a criterion standard review in a small random sample of 400 records comparing results from the high-detail prompt and text string\u2013 search approach to classifications of helmet status generated by researchers reading the clinical notes. Burford and colleagues 1 found moderate agreement (Cohen \u03ba = 0.74 [95% CI, 0.73-0.75]) for the low-detail prompt and weak agreement (Cohen \u03ba = 0.53 [95% CI, 0.52-0.54]) for the intermediate-detail prompt compared with the text string\u2013search approach. The high-detail prompt, which included comprehensive researcher-generated",
        "keywords": []
    },
    "a7f4deb9a1452374330f202bc8d36966a0f254e8.pdf": {
        "title": "THRONE: An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Language Models",
        "authors": [
            "Prannay Kaul",
            "Zhizhong Li",
            "Hao Yang",
            "Yonatan Dukler",
            "Ashwin Swaminathan",
            "C. Taylor",
            "Stefano Soatto"
        ],
        "published_date": "2024",
        "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term \u201cType I hallucinations\u201d. Instead, they focus on hallucinations responding to very specific question formats-typically a multiple-choice response regarding a particular object or attribute-which we term \u201cType II hallucinations\u201d. Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of halluci-nations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/a7f4deb9a1452374330f202bc8d36966a0f254e8.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 24,
        "score": 24.0
    },
    "05839a68bd05880beef2f171cee7aab960bb6d2f.pdf": {
        "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models",
        "authors": [
            "Chaoya Jiang",
            "Wei Ye",
            "Mengfan Dong",
            "Hongrui Jia",
            "Haiyang Xu",
            "Mingshi Yan",
            "Ji Zhang",
            "Shikun Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision-Language Models (LVLMs) exhibit remarkable capabilities but struggle with ''hallucinations''-inconsistencies between images and their descriptions. Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity. In this paper, we introduce a refined taxonomy of hallucinations, featuring a new category: Event Hallucination. We then utilize advanced LLMs to generate and filter fine-grained hallucinatory data consisting of various types of hallucinations, with a particular focus on event hallucinations, laying the groundwork for integrating discriminative and generative evaluation methods within our universal evaluation framework. The proposed benchmark distinctively assesses LVLMs' ability to tackle a broad spectrum of hallucinations, making it a reliable and comprehensive tool for gauging LVLMs' efficacy in handling hallucinations. We will release our code and data.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/05839a68bd05880beef2f171cee7aab960bb6d2f.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 23,
        "score": 23.0
    },
    "f19e6c955b05d61aeb1cbc7580dc3723d31398ea.pdf": {
        "title": "Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models",
        "authors": [
            "Zhenyu Pan",
            "Haozheng Luo",
            "Manling Li",
            "Han Liu"
        ],
        "published_date": "2024",
        "abstract": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f19e6c955b05d61aeb1cbc7580dc3723d31398ea.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 23,
        "score": 23.0,
        "summary": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.",
        "keywords": []
    },
    "431a4e7e89863b038069335baa80c3e489538214.pdf": {
        "title": "VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation",
        "authors": [
            "Ruiyang Zhang",
            "Hu Zhang",
            "Zhedong Zheng"
        ],
        "published_date": "2024",
        "abstract": "Given the higher information load processed by large vision-language models (LVLMs) compared to single-modal LLMs, detecting LVLM hallucinations requires more human and time expense, and thus rise a wider safety concerns. In this paper, we introduce VL-Uncertainty, the first uncertainty-based framework for detecting hallucinations in LVLMs. Different from most existing methods that require ground-truth or pseudo annotations, VL-Uncertainty utilizes uncertainty as an intrinsic metric. We measure uncertainty by analyzing the prediction variance across semantically equivalent but perturbed prompts, including visual and textual data. When LVLMs are highly confident, they provide consistent responses to semantically equivalent queries. However, when uncertain, the responses of the target LVLM become more random. Considering semantically similar answers with different wordings, we cluster LVLM responses based on their semantic content and then calculate the cluster distribution entropy as the uncertainty measure to detect hallucination. Our extensive experiments on 10 LVLMs across four benchmarks, covering both free-form and multi-choice tasks, show that VL-Uncertainty significantly outperforms strong baseline methods in hallucination detection.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/431a4e7e89863b038069335baa80c3e489538214.pdf",
        "venue": "arXiv.org",
        "citationCount": 21,
        "score": 21.0
    },
    "3c2130e219528df234658b94810de33fe7b077dc.pdf": {
        "title": "Are Large Language Models Good at Utility Judgments?",
        "authors": [
            "Hengran Zhang",
            "Ruqing Zhang",
            "J. Guo",
            "M. D. Rijke",
            "Yixing Fan",
            "Xueqi Cheng"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain question answering (QA). Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at https://github.com/ict-bigdatalab/utility_judgments.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3c2130e219528df234658b94810de33fe7b077dc.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 20,
        "score": 20.0,
        "summary": "Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain question answering (QA). Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at https://github.com/ict-bigdatalab/utility_judgments.",
        "keywords": []
    },
    "5667f64b23cf48c94ff7413122bc56e5aad7e6a2.pdf": {
        "title": "Preventing and Detecting Misinformation Generated by Large Language Models",
        "authors": [
            "Aiwei Liu",
            "Qiang Sheng",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "As large language models (LLMs) become increasingly capable and widely deployed, the risk of them generating misinformation poses a critical challenge. Misinformation from LLMs can take various forms, from factual errors due to hallucination to intentionally deceptive content, and can have severe consequences in high-stakes domains.This tutorial covers comprehensive strategies to prevent and detect misinformation generated by LLMs. We first introduce the types of misinformation LLMs can produce and their root causes. We then explore two broad categories: Preventing misinformation generation: a) AI alignment training techniques to reduce LLMs' propensity for misinformation and refuse malicious instructions during model training. b) Training-free mitigation methods like prompt guardrails, retrieval-augmented generation (RAG), and decoding strategies to curb misinformation at inference time. Detecting misinformation after generation, including a) using LLMs themselves to detect misinformation through embedded knowledge or retrieval-enhanced judgments, and b) distinguishing LLM-generated text from human-written text through black-box approaches (e.g., classifiers, probability analysis) and white-box approaches (e.g., watermarking). We also discuss the challenges and limitations of detecting LLM-generated misinformation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5667f64b23cf48c94ff7413122bc56e5aad7e6a2.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 20,
        "score": 20.0
    },
    "37f6249b8381c955d732755714c1e700ba62b988.pdf": {
        "title": "GraphArena: Benchmarking Large Language Models on Graph Computational Problems",
        "authors": [
            "Jianheng Tang",
            "Qifan Zhang",
            "Yuhan Li",
            "Jia Li"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/37f6249b8381c955d732755714c1e700ba62b988.pdf",
        "venue": "arXiv.org",
        "citationCount": 20,
        "score": 20.0,
        "summary": "",
        "keywords": []
    },
    "f123b838e000e11f08cb0d7c63e01934b38d3092.pdf": {
        "title": "An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models",
        "authors": [
            "Xiongtao Zhou",
            "Jie He",
            "Yuhua Ke",
            "Guangyao Zhu",
            "V'ictor Guti'errez-Basulto",
            "Jeff Z. Pan"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters. To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs. Code and data are available at https://github.com/alenai97/PEFT-MLLM.git.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f123b838e000e11f08cb0d7c63e01934b38d3092.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 20,
        "score": 20.0,
        "summary": "Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters. To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs. Code and data are available at https://github.com/alenai97/PEFT-MLLM.git.",
        "keywords": []
    },
    "b10482ab3dd1d340c3c926d92c3e617c24ee3949.pdf": {
        "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
        "authors": [
            "Haoqiang Kang",
            "Juntong Ni",
            "Huaxiu Yao"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the\"snowballing\"issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b10482ab3dd1d340c3c926d92c3e617c24ee3949.pdf",
        "venue": "arXiv.org",
        "citationCount": 38,
        "score": 19.0
    },
    "e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0.pdf": {
        "title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning",
        "authors": [
            "Xiaoye Qu",
            "Jiashuo Sun",
            "Wei Wei",
            "Yu Cheng"
        ],
        "published_date": "2024",
        "abstract": "Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multi-modal context comprehension. However, they still suffer from hallucination problems referring to generating inconsistent outputs with the image content. To mitigate hallucinations, previous studies mainly focus on retraining LVLMs with custom datasets. Although effective, they inherently come with additional computational costs. In this paper, we propose a training-free framework, \\textbf{MVP}, that aims to reduce hallucinations by making the most of the innate capabilities of the LVLMs via \\textbf{M}ulti-\\textbf{V}iew Multi-\\textbf{P}ath Reasoning. Specifically, we first devise a multi-view information-seeking strategy to thoroughly perceive the comprehensive information in the image, which enriches the general global information captured by the original vision encoder in LVLMs. Furthermore, during the answer decoding, we observe that the occurrence of hallucinations has a strong correlation with the certainty of the answer tokens. Thus, we propose multi-path reasoning for each information view to quantify and aggregate the certainty scores for each potential answer among multiple decoding paths and finally decide the output answer. By fully grasping the information in the image and carefully considering the certainty of the potential answers when decoding, our MVP can effectively reduce hallucinations in LVLMs.The extensive experiments verify that our proposed MVP significantly mitigates the hallucination problem across four well-known LVLMs. The source code is available at: \\url{https://github.com/GasolSun36/MVP}.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 19,
        "score": 19.0
    },
    "5838b56f2c7ca3dd946428dae07bdc26a9265c67.pdf": {
        "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
        "authors": [
            "Haoqiang Kang",
            "Xiao-Yang Liu"
        ],
        "published_date": "2023",
        "abstract": "The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs), especially when applied to fields such as finance, education, and law. Despite the growing concerns, there has been a lack of empirical investigation. In this paper, we provide an empirical examination of LLMs' hallucination behaviors in financial tasks. First, we empirically investigate LLM model's ability of explaining financial concepts and terminologies. Second, we assess LLM models' capacity of querying historical stock prices. Third, to alleviate the hallucination issue, we evaluate the efficacy of four practical methods, including few-shot learning, Decoding by Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method and the prompt-based tool learning method for a function to generate a query command. Finally, our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks. Therefore, there is an urgent need to call for research efforts in mitigating LLMs' hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5838b56f2c7ca3dd946428dae07bdc26a9265c67.pdf",
        "venue": "arXiv.org",
        "citationCount": 38,
        "score": 19.0
    },
    "4c0f029efd5371eed087d0794fb0df71238600cc.pdf": {
        "title": "KG-CoT: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering",
        "authors": [
            "Ruilin Zhao",
            "Feng Zhao",
            "Long Wang",
            "Xianzhi Wang",
            "Guandong Xu"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) encounter challenges such as hallucination and factual errors in knowledge-intensive tasks. One the one hand, LLMs sometimes struggle to generate reliable answers based on the black-box parametric knowledge, due to the lack of responsible knowledge. Moreover, fragmented knowledge facts extracted by knowledge retrievers fail to provide explicit and coherent reasoning paths for improving LLM reasoning. To address these challenges, we propose KG-CoT, a novel knowledge-augmented paradigm that leverages a small-scale step-by-step graph reasoning model to reason over knowledge graphs (KGs) and utilizes a reasoning path generation method to generate chains of reasoning with high confidence for large-scale LLMs. Extensive experiments demonstrate that our KG-CoT significantly improves the performance of LLMs on knowledge-intensive question answering tasks, such as multi-hop, single-hop, and open-domain question answering benchmarks, without fine-tuning LLMs. KG-CoT outperforms the CoT prompting as well as prior retrieval-augmented and knowledge base question answering baselines. Moreover, KG-CoT can reduce the number of API calls and cost and generalize to various LLM backbones in a lightweight plug-and-play manner.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4c0f029efd5371eed087d0794fb0df71238600cc.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 19,
        "score": 19.0,
        "summary": "Large language models (LLMs) encounter challenges such as hallucination and factual errors in knowledge-intensive tasks. One the one hand, LLMs sometimes struggle to generate reliable answers based on the black-box parametric knowledge, due to the lack of responsible knowledge. Moreover, fragmented knowledge facts extracted by knowledge retrievers fail to provide explicit and coherent reasoning paths for improving LLM reasoning. To address these challenges, we propose KG-CoT, a novel knowledge-augmented paradigm that leverages a small-scale step-by-step graph reasoning model to reason over knowledge graphs (KGs) and utilizes a reasoning path generation method to generate chains of reasoning with high confidence for large-scale LLMs. Extensive experiments demonstrate that our KG-CoT significantly improves the performance of LLMs on knowledge-intensive question answering tasks, such as multi-hop, single-hop, and open-domain question answering benchmarks, without fine-tuning LLMs. KG-CoT outperforms the CoT prompting as well as prior retrieval-augmented and knowledge base question answering baselines. Moreover, KG-CoT can reduce the number of API calls and cost and generalize to various LLM backbones in a lightweight plug-and-play manner.",
        "keywords": []
    },
    "1cc347c97a8f9d30edc809e4f207d64c7b8247b4.pdf": {
        "title": "GraphArena: Evaluating and Exploring Large Language Models on Graph Computation",
        "authors": [
            "Jianheng Tang",
            "Qifan Zhang",
            "Yuhan Li",
            "Nuo Chen",
            "Jia Li"
        ],
        "published_date": "2024",
        "abstract": "The ``arms race'' of Large Language Models (LLMs) demands new benchmarks to examine their progresses. In this paper, we introduce GraphArena, a benchmarking tool designed to evaluate LLMs on real-world graph computational problems. It offers a suite of four polynomial-time tasks (e.g., Shortest Distance) and six NP-complete challenges (e.g., Traveling Salesman Problem). GraphArena features a rigorous evaluation framework that classifies LLM outputs as correct, suboptimal (feasible but not optimal), hallucinatory (properly formatted but infeasible), or missing. Evaluation of over 10 LLMs reveals that even top-performing LLMs struggle with larger, more complex graph problems and exhibit hallucination issues. We further explore four potential solutions to address this issue and improve LLMs on graph computation, including chain-of-thought prompting, instruction tuning, code writing, and scaling test-time compute, each demonstrating unique strengths and limitations. GraphArena complements the existing LLM benchmarks and is open-sourced at https://github.com/squareRoot3/GraphArena.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1cc347c97a8f9d30edc809e4f207d64c7b8247b4.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 19,
        "score": 19.0
    },
    "01f3b1809035a593b9dd6fb0b2cabdc8e216542f.pdf": {
        "title": "Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models",
        "authors": [
            "Yuji Zhang",
            "Sha Li",
            "Jiateng Liu",
            "Pengfei Yu",
            "Y. Fung",
            "Jing Li",
            "Manling Li",
            "Heng Ji"
        ],
        "published_date": "2024",
        "abstract": "Hallucination is often regarded as a major impediment for using large language models (LLMs), especially for knowledge-intensive tasks. Even when the training corpus consists solely of true statements, language models still generate hallucinations in the form of amalgamations of multiple facts. We coin this phenomenon as ``knowledge overshadowing'': when we query knowledge from a language model with multiple conditions, some conditions overshadow others, leading to hallucinated outputs. This phenomenon partially stems from training data imbalance, which we verify on both pretrained models and fine-tuned models, over a wide range of LM model families and sizes.From a theoretical point of view, knowledge overshadowing can be interpreted as over-generalization of the dominant conditions (patterns). We show that the hallucination rate grows with both the imbalance ratio (between the popular and unpopular condition) and the length of dominant condition description, consistent with our derived generalization bound. Finally, we propose to utilize overshadowing conditions as a signal to catch hallucination before it is produced, along with a training-free self-contrastive decoding method to alleviate hallucination during inference. Our proposed approach showcases up to 82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control, with different models and datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/01f3b1809035a593b9dd6fb0b2cabdc8e216542f.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 18.0
    },
    "2635c1aeee582dacb865f00d1289b443c3d96d02.pdf": {
        "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
        "authors": [
            "Guanyu Zhou",
            "Yibo Yan",
            "Xin Zou",
            "Kun Wang",
            "Aiwei Liu",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in both industry and academia, but often suffer from biases introduced by visual and language priors, which can lead to multimodal hallucination. These biases arise from the visual encoder and the Large Language Model (LLM) backbone, affecting the attention mechanism responsible for aligning multimodal inputs. Existing decoding-based mitigation methods focus on statistical correlations and overlook the causal relationships between attention mechanisms and model output, limiting their effectiveness in addressing these biases. To tackle this issue, we propose a causal inference framework termed CausalMM that applies structural causal modeling to MLLMs, treating modality priors as a confounder between attention mechanisms and output. Specifically, by employing backdoor adjustment and counterfactual reasoning at both the visual and language attention levels, our method mitigates the negative effects of modality priors and enhances the alignment of MLLM's inputs and outputs, with a maximum score improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME Benchmark compared to conventional methods. Extensive experiments validate the effectiveness of our approach while being a plug-and-play solution. Our code is available at: https://github.com/The-Martyr/CausalMM",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2635c1aeee582dacb865f00d1289b443c3d96d02.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 18,
        "score": 18.0
    },
    "cdfab7e389f94263ce99b7c0025090971df40a01.pdf": {
        "title": "Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action",
        "authors": [
            "Zhenyu Pan",
            "Haozheng Luo",
            "Manling Li",
            "Han Liu"
        ],
        "published_date": "2024",
        "abstract": "We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA). Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval. Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever. Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions. Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations. Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/cdfab7e389f94263ce99b7c0025090971df40a01.pdf",
        "venue": "arXiv.org",
        "citationCount": 17,
        "score": 17.0,
        "summary": "We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA). Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval. Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever. Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions. Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations. Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions.",
        "keywords": []
    },
    "c4d3c2516d24bd1c0eff93ea047147f3afd586ca.pdf": {
        "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models",
        "authors": [
            "Xin Zou",
            "Yizhou Wang",
            "Yibo Yan",
            "Sirui Huang",
            "Kening Zheng",
            "Junkai Chen",
            "Chang Tang",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Despite their impressive capabilities, multimodal large language models (MLLMs) are prone to hallucinations, i.e., the generated content that is nonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in MLLMs often stem from the sensitivity of text decoder to visual tokens, leading to a phenomenon akin to\"amnesia\"about visual information. To address this issue, we propose MemVR, a novel decoding paradigm inspired by common cognition: when the memory of an image seen the moment before is forgotten, people will look at it again for factual answers. Following this principle, we treat visual tokens as supplementary evidence, re-injecting them into the MLLM through Feed Forward Network (FFN) as\"key-value memory\"at the middle trigger layer. This\"look-twice\"mechanism occurs when the model exhibits high uncertainty during inference, effectively enhancing factual alignment. Comprehensive experimental evaluations demonstrate that MemVR significantly mitigates hallucination across various MLLMs and excels in general benchmarks without incurring additional time overhead. The implementation is available from https://github.com/1zhou-Wang/MemVR",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c4d3c2516d24bd1c0eff93ea047147f3afd586ca.pdf",
        "venue": "arXiv.org",
        "citationCount": 17,
        "score": 17.0
    },
    "db646f0eb37bb97fda3a89f94c81e507f9421ba9.pdf": {
        "title": "Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization",
        "authors": [
            "Beitao Chen",
            "Xinyu Lyu",
            "Lianli Gao",
            "Jingkuan Song",
            "Hengtao Shen"
        ],
        "published_date": "2024",
        "abstract": "Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnect between the generated text and the corresponding images. Almost all current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones. However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations. To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs. Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/db646f0eb37bb97fda3a89f94c81e507f9421ba9.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 17,
        "score": 17.0
    },
    "4c3447dce6798b894313bb3ff2735ef139cbf071.pdf": {
        "title": "PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics",
        "authors": [
            "Derui Zhu",
            "Dingfan Chen",
            "Qing Li",
            "Zongxiong Chen",
            "Lei Ma",
            "Jens Grossklags",
            "Mario Fritz"
        ],
        "published_date": "2024",
        "abstract": "Despite tremendous advancements in large language models (LLMs) over recent years, a notably urgent challenge for their practical deployment is the phenomenon of hallucination, where the model fabricates facts and produces non-factual statements. In response, we propose PoLLMgraph, a Polygraph for LLMs, as an effective model-based white-box detection and forecasting approach. PoLLMgraph distinctly differs from the large body of existing research that concentrates on addressing such challenges through black-box evaluations. In particular, we demonstrate that hallucination can be effectively detected by analyzing the LLM's internal state transition dynamics during generation via tractable probabilistic models. Experimental results on various open-source LLMs confirm the efficacy of PoLLMgraph, outperforming state-of-the-art methods by a considerable margin, evidenced by over 20% improvement in AUC-ROC on common benchmarking datasets like TruthfulQA. Our work paves a new way for model-based white-box analysis of LLMs, motivating the research community to further explore, understand, and refine the intricate dynamics of LLM behaviors.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4c3447dce6798b894313bb3ff2735ef139cbf071.pdf",
        "venue": "NAACL-HLT",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Despite tremendous advancements in large language models (LLMs) over recent years, a notably urgent challenge for their practical deployment is the phenomenon of hallucination, where the model fabricates facts and produces non-factual statements. In response, we propose PoLLMgraph, a Polygraph for LLMs, as an effective model-based white-box detection and forecasting approach. PoLLMgraph distinctly differs from the large body of existing research that concentrates on addressing such challenges through black-box evaluations. In particular, we demonstrate that hallucination can be effectively detected by analyzing the LLM's internal state transition dynamics during generation via tractable probabilistic models. Experimental results on various open-source LLMs confirm the efficacy of PoLLMgraph, outperforming state-of-the-art methods by a considerable margin, evidenced by over 20% improvement in AUC-ROC on common benchmarking datasets like TruthfulQA. Our work paves a new way for model-based white-box analysis of LLMs, motivating the research community to further explore, understand, and refine the intricate dynamics of LLM behaviors.",
        "keywords": []
    },
    "968bd4cf71c66bb153527778836e54c85ee6162c.pdf": {
        "title": "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models",
        "authors": [
            "Weihong Zhong",
            "Xiaocheng Feng",
            "Liang Zhao",
            "Qiming Li",
            "Lei Huang",
            "Yuxuan Gu",
            "Weitao Ma",
            "Yuan Xu",
            "Bing Qin"
        ],
        "published_date": "2024",
        "abstract": "Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least $31\\%$, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not have supported without distractions. We term this phenomenon Multimodal Hallucination Snowballing. To mitigate this, we further propose a training-free method called Residual Visual Decoding, where we revise the output distribution of LVLMs with the one derived from the residual visual input, providing models with direct access to the visual information. Experiments show that our method can mitigate more than $24\\%$ of the snowballed multimodal hallucination while maintaining capabilities.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/968bd4cf71c66bb153527778836e54c85ee6162c.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 16,
        "score": 16.0
    },
    "1f49b4586cc71cca59151e7a7bbfd500574c2fee.pdf": {
        "title": "Hallucination Mitigation for Retrieval-Augmented Large Language Models: A Review",
        "authors": [
            "Wan Zhang",
            "Jing Zhang"
        ],
        "published_date": "2025",
        "abstract": "Retrieval-augmented generation (RAG) leverages the strengths of information retrieval and generative models to enhance the handling of real-time and domain-specific knowledge. Despite its advantages, limitations within RAG components may cause hallucinations, or more precisely termed confabulations in generated outputs, driving extensive research to address these limitations and mitigate hallucinations. This review focuses on hallucination in retrieval-augmented large language models (LLMs). We first examine the causes of hallucinations from different sub-tasks in the retrieval and generation phases. Then, we provide a comprehensive overview of corresponding hallucination mitigation techniques, offering a targeted and complete framework for addressing hallucinations in retrieval-augmented LLMs. We also investigate methods to reduce the impact of hallucination through detection and correction. Finally, we discuss promising future research directions for mitigating hallucinations in retrieval-augmented LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1f49b4586cc71cca59151e7a7bbfd500574c2fee.pdf",
        "venue": "Mathematics",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Retrieval-augmented generation (RAG) leverages the strengths of information retrieval and generative models to enhance the handling of real-time and domain-specific knowledge. Despite its advantages, limitations within RAG components may cause hallucinations, or more precisely termed confabulations in generated outputs, driving extensive research to address these limitations and mitigate hallucinations. This review focuses on hallucination in retrieval-augmented large language models (LLMs). We first examine the causes of hallucinations from different sub-tasks in the retrieval and generation phases. Then, we provide a comprehensive overview of corresponding hallucination mitigation techniques, offering a targeted and complete framework for addressing hallucinations in retrieval-augmented LLMs. We also investigate methods to reduce the impact of hallucination through detection and correction. Finally, we discuss promising future research directions for mitigating hallucinations in retrieval-augmented LLMs.",
        "keywords": []
    },
    "4150d370a29258ac88552561cdd2b10f7862bfd7.pdf": {
        "title": "Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals",
        "authors": [
            "Lida Chen",
            "Zujie Liang",
            "Xintao Wang",
            "Jiaqing Liang",
            "Yanghua Xiao",
            "Feng Wei",
            "Jinglei Chen",
            "Zhenghong Hao",
            "Bing Han",
            "Wei Wang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have achieved great success, but their occasional content fabrication, or hallucination, limits their practical application. Hallucination arises because LLMs struggle to admit ignorance due to inadequate training on knowledge boundaries. We call it a limitation of LLMs that they can not accurately express their knowledge boundary, answering questions they know while admitting ignorance to questions they do not know. In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know. We propose CoKE, which first probes LLMs' knowledge boundary via internal confidence given a set of questions, and then leverages the probing results to elicit the expression of the knowledge boundary. Extensive experiments show CoKE helps LLMs express knowledge boundaries, answering known questions while declining unknown ones, significantly improving in-domain and out-of-domain performance.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4150d370a29258ac88552561cdd2b10f7862bfd7.pdf",
        "venue": "Proceedings of the 3rd Workshop on Towards Knowledgeable Foundation Models (KnowFM)",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Large language models (LLMs) have achieved great success, but their occasional content fabrication, or hallucination, limits their practical application. Hallucination arises because LLMs struggle to admit ignorance due to inadequate training on knowledge boundaries. We call it a limitation of LLMs that they can not accurately express their knowledge boundary, answering questions they know while admitting ignorance to questions they do not know. In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know. We propose CoKE, which first probes LLMs' knowledge boundary via internal confidence given a set of questions, and then leverages the probing results to elicit the expression of the knowledge boundary. Extensive experiments show CoKE helps LLMs express knowledge boundaries, answering known questions while declining unknown ones, significantly improving in-domain and out-of-domain performance.",
        "keywords": []
    },
    "de02ba19fb957ae30de7f09904ae3d983c3b50e7.pdf": {
        "title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
        "authors": [
            "Stefan Dernbach",
            "Khushbu Agarwal",
            "Alejandro Zuniga",
            "Michael Henry",
            "Sutanay Choudhury"
        ],
        "published_date": "2024",
        "abstract": "Integrating large language models with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a model to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no \u2013 such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the models\u2019 capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/de02ba19fb957ae30de7f09904ae3d983c3b50e7.pdf",
        "venue": "AAAI Spring Symposia",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Integrating large language models with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a model to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no \u2013 such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the models\u2019 capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.",
        "keywords": []
    },
    "75a381667ef536d02d99063eb3568e410d7ce909.pdf": {
        "title": "Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds",
        "authors": [
            "Jiageng Wu",
            "Xian Wu",
            "Jie Yang"
        ],
        "published_date": "2024",
        "abstract": "Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients\u2019 diseases, and selecting appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision pathways of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), to enhance LLMs reasoning with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets validate that ICP significantly improves the clinical reasoning ability of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/75a381667ef536d02d99063eb3568e410d7ce909.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patients\u2019 diseases, and selecting appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision pathways of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), to enhance LLMs reasoning with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets validate that ICP significantly improves the clinical reasoning ability of LLMs.",
        "keywords": []
    },
    "492e526ca2416a734f286da0efcfeda4672ea77f.pdf": {
        "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models",
        "authors": [
            "Ningke Li",
            "Yuekang Li",
            "Yi Liu",
            "Ling Shi",
            "Kailong Wang",
            "Haoyu Wang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations \u2014 coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations. To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth. Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7% to 59.8%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations. To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/492e526ca2416a734f286da0efcfeda4672ea77f.pdf",
        "venue": "Proc. ACM Program. Lang.",
        "citationCount": 15,
        "score": 15.0
    },
    "d409053ed94ec725d72c812e7c8bd71b87278b96.pdf": {
        "title": "TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models",
        "authors": [
            "Jaewoo Ahn",
            "Taehyun Lee",
            "Junyoung Lim",
            "Jin-Hwa Kim",
            "Sangdoo Yun",
            "Hwaran Lee",
            "Gunhee Kim"
        ],
        "published_date": "2024",
        "abstract": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d409053ed94ec725d72c812e7c8bd71b87278b96.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 15,
        "score": 15.0,
        "summary": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.",
        "keywords": []
    },
    "24a48ef14c8eb4e571e3f4ae9b37936060a3fb06.pdf": {
        "title": "VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding",
        "authors": [
            "Chaoyu Li",
            "Eun Woo Im",
            "Pooyan Fazli"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) have recently shown significant advancements in video understanding, excelling in content reasoning and instruction-following tasks. However, hallucination, where models generate inaccurate or misleading content, remains underexplored in the video domain. Building on the observation that MLLM visual encoders often fail to distinguish visually different yet semantically similar video pairs, we introduce VIDHALLUC, the largest benchmark designed to examine hallucinations in MLLMs for video understanding. It consists of 5,002 videos, paired to highlight cases prone to hallucinations. VIDHALLUC assesses hallucinations across three critical dimensions: (1) action, (2) temporal sequence, and (3) scene transition. Comprehensive testing shows that most MLLMs are vulnerable to hallucinations across these dimensions. Furthermore, we propose DINO-HEAL, a training-free method that reduces hallucinations by incorporating spatial saliency from DINOv2 to reweight visual features during inference. Our results show that DINO-HEAL consistently improves performance on VIDHALLUC, achieving an average improvement of 3.02% in mitigating hallucinations across all tasks. Both the VIDHALLUC benchmark and DINO-HEAL code are available at https://peoplerobots.github.io/vidhalluc.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/24a48ef14c8eb4e571e3f4ae9b37936060a3fb06.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 15,
        "score": 15.0
    },
    "379e3ee9a6a92817bd0812848b409eafb9bf9550.pdf": {
        "title": "Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs",
        "authors": [
            "Xinyi Mou",
            "Zejun Li",
            "Hanjia Lyu",
            "Jiebo Luo",
            "Zhongyu Wei"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have revolutionized solutions for general natural language processing (NLP) tasks. However, deploying these models in specific domains still faces challenges like hallucination. While existing knowledge graph retrieval-based approaches offer partial solutions, they cannot be well adapted to the political domain. On one hand, existing generic knowledge graphs lack vital political context, hindering deductions for practical tasks. On the other hand, the nature of political questions often renders the direct facts elusive, necessitating deeper aggregation and comprehension of retrieved evidence. To address these challenges, we propose a Political Experts through Knowledge Graph Integration (PEG) framework. PEG entails the creation and utilization of a multi-view political knowledge graph (MVPKG), which integrates U.S. legislative, election, and diplomatic data, as well as conceptual knowledge from Wikidata. With MVPKG as its foundation, PEG enhances existing methods through knowledge acquisition, aggregation, and injection. This process begins with refining evidence through semantic filtering, followed by its aggregation into global knowledge via implicit or explicit methods. The integrated knowledge is then utilized by LLMs through prompts. Experiments on three real-world datasets across diverse LLMs confirm PEG's superiority in tackling political modeling tasks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/379e3ee9a6a92817bd0812848b409eafb9bf9550.pdf",
        "venue": "The Web Conference",
        "citationCount": 14,
        "score": 14.0,
        "summary": "Large Language Models (LLMs) have revolutionized solutions for general natural language processing (NLP) tasks. However, deploying these models in specific domains still faces challenges like hallucination. While existing knowledge graph retrieval-based approaches offer partial solutions, they cannot be well adapted to the political domain. On one hand, existing generic knowledge graphs lack vital political context, hindering deductions for practical tasks. On the other hand, the nature of political questions often renders the direct facts elusive, necessitating deeper aggregation and comprehension of retrieved evidence. To address these challenges, we propose a Political Experts through Knowledge Graph Integration (PEG) framework. PEG entails the creation and utilization of a multi-view political knowledge graph (MVPKG), which integrates U.S. legislative, election, and diplomatic data, as well as conceptual knowledge from Wikidata. With MVPKG as its foundation, PEG enhances existing methods through knowledge acquisition, aggregation, and injection. This process begins with refining evidence through semantic filtering, followed by its aggregation into global knowledge via implicit or explicit methods. The integrated knowledge is then utilized by LLMs through prompts. Experiments on three real-world datasets across diverse LLMs confirm PEG's superiority in tackling political modeling tasks.",
        "keywords": []
    },
    "b0633ccf235e467c35b963ad012f6b8c54aba19f.pdf": {
        "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
        "authors": [
            "Derong Xu",
            "Ziheng Zhang",
            "Zhihong Zhu",
            "Zhenxi Lin",
            "Qidong Liu",
            "Xian Wu",
            "Tong Xu",
            "Xiangyu Zhao",
            "Yefeng Zheng",
            "Enhong Chen"
        ],
        "published_date": "2024",
        "abstract": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b0633ccf235e467c35b963ad012f6b8c54aba19f.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 14,
        "score": 14.0,
        "summary": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.",
        "keywords": []
    },
    "89fccb4b70d0a072d9c874dddfab0afb3676d1b8.pdf": {
        "title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering",
        "authors": [
            "Yuan Sui",
            "Bryan Hooi"
        ],
        "published_date": "2024",
        "abstract": "Recent works integrating Knowledge Graphs (KGs) have shown promising improvements in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing benchmarks primarily focus on closed-ended tasks, leaving a gap in evaluating performance on more complex, real-world scenarios. This limitation also hinders a thorough assessment of KGs' potential to reduce hallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark specifically designed to evaluate LLMs augmented with KGs in open-ended, real-world question answering settings. OKGQA reflects practical complexities through diverse question types and incorporates metrics to quantify both hallucination rates and reasoning improvements in LLM+KG models. To consider the scenarios in which KGs may contain varying levels of errors, we propose a benchmark variant, OKGQA-P, to assess model performance when the semantics and structure of KGs are deliberately perturbed and contaminated. In this paper, we aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on method design. We believe this study can facilitate a more complete performance comparison and encourages continuous improvement in integrating KGs with LLMs to mitigate hallucination, and make LLMs more trustworthy. Code and data are released at https://github.com/Y-Sui/OKGQA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/89fccb4b70d0a072d9c874dddfab0afb3676d1b8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 14,
        "score": 14.0
    },
    "45ffc7928a358ff6567d8420b58d509fc3b7dbd1.pdf": {
        "title": "Mitigating Entity-Level Hallucination in Large Language Models",
        "authors": [
            "Weihang Su",
            "Yichen Tang",
            "Qingyao Ai",
            "Changyue Wang",
            "Zhijing Wu",
            "Yiqun Liu"
        ],
        "published_date": "2024",
        "abstract": "The emergence of Large Language Models (LLMs) has revolutionized how users access information, shifting from traditional search engines to direct question-and-answer interactions with LLMs. However, the widespread adoption of LLMs has revealed a significant challenge known as hallucination, wherein LLMs generate coherent yet factually inaccurate responses. This hallucination phenomenon has led to users' distrust in information retrieval systems based on LLMs. To tackle this challenge, this paper proposes Dynamic Retrieval Augmentation based on hallucination Detection (DRAD) as a novel method to detect and mitigate hallucinations in LLMs. DRAD improves upon traditional retrieval augmentation by dynamically adapting the retrieval process based on real-time hallucination detection. It features two main components: Real-time Hallucination Detection (RHD) for identifying potential hallucinations without external models, and Self-correction based on External Knowledge (SEK) for correcting these errors using external knowledge. Experiment results show that DRAD demonstrates superior performance in both detecting and mitigating hallucinations in LLMs. All of our code and data are open-sourced at https://github.com/oneal2000/EntityHallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/45ffc7928a358ff6567d8420b58d509fc3b7dbd1.pdf",
        "venue": "SIGIR-AP",
        "citationCount": 13,
        "score": 13.0
    },
    "03e2f5cded1b1d92dc8e693e0e93ad466f6cc352.pdf": {
        "title": "Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs",
        "authors": [
            "Peng Ding",
            "Jingyu Wu",
            "Jun Kuang",
            "Dan Ma",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Shi Chen",
            "Jiajun Chen",
            "Shujian Huang"
        ],
        "published_date": "2024",
        "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable performance on various visual-language understanding and generation tasks. However, MLLMs occasionally generate content inconsistent with the given images, which is known as \"hallucination\". Prior works primarily center on evaluating hallucination using standard, unperturbed benchmarks, which overlook the prevalent occurrence of perturbed inputs in real-world scenarios-such as image cropping or blurring-that are critical for a comprehensive assessment of MLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI, the first benchmark designed to evaluate Hallucination in MLLMs within Perturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios, containing 1,260 perturbed images from 11 object types. Each image is accompanied by detailed annotations, which include fine-grained hallucination types, such as existence, attribute, and relation. We equip these annotations with a rich set of questions, making Hallu-PI suitable for both discriminative and generative tasks. Extensive experiments on 12 mainstream MLLMs, such as GPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant hallucinations on Hallu-PI, which is not observed in unperturbed scenarios. Furthermore, our research reveals a severe bias in MLLMs' ability to handle different types of hallucinations. We also design two baselines specifically for perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope that our study will bring researchers' attention to the limitations of MLLMs when dealing with perturbed inputs, and spur further investigations to address this issue. Our code and datasets are publicly available at https://github.com/NJUNLP/Hallu-PI.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/03e2f5cded1b1d92dc8e693e0e93ad466f6cc352.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 13,
        "score": 13.0
    },
    "45ed6263e02d219f0542ac743b9c9f837154a58d.pdf": {
        "title": "Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval",
        "authors": [
            "Mengjia Niu",
            "Hao Li",
            "Jie Shi",
            "Hamed Haddadi",
            "Fan Mo"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains, although their susceptibility to hallucination poses significant challenges for their deployment in critical areas such as healthcare. To address this issue, retrieving relevant facts from knowledge graphs (KGs) is considered a promising method. Existing KG-augmented approaches tend to be resource-intensive, requiring multiple rounds of retrieval and verification for each factoid, which impedes their application in real-world scenarios. In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR) to augment the factuality of LLMs' responses with less retrieval efforts in the medical field. Our approach leverages the attribution of next-token predictive probability distributions across different tokens, and various model layers to primarily identify tokens with a high potential for hallucination, reducing verification rounds by refining knowledge triples associated with these tokens. Moreover, we rectify inaccurate content using retrieved knowledge in the post-processing stage, which improves the truthfulness of generated responses. Experimental results on a medical dataset demonstrate that our approach can enhance the factual capability of LLMs across various foundational models as evidenced by the highest scores on truthfulness.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/45ed6263e02d219f0542ac743b9c9f837154a58d.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0
    },
    "35a544ca04325006f7f2e2369d3e0aaa8ba36a07.pdf": {
        "title": "RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models",
        "authors": [
            "Xiangkun Hu",
            "Dongyu Ru",
            "Lin Qiu",
            "Qipeng Guo",
            "Tianhang Zhang",
            "Yang Xu",
            "Yun Luo",
            "Pengfei Liu",
            "Yue Zhang",
            "Zheng Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments. This work is open sourced at https://github.com/amazon-science/RefChecker",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/35a544ca04325006f7f2e2369d3e0aaa8ba36a07.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments. This work is open sourced at https://github.com/amazon-science/RefChecker",
        "keywords": []
    },
    "1146d40d3d01427a008a20530269667b8989750c.pdf": {
        "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation",
        "authors": [
            "Xun Liang",
            "Shichao Song",
            "Simin Niu",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Bo Tang",
            "Zhaohui Wy",
            "Dawei He",
            "Peng Cheng",
            "Zhonghao Wang",
            "Haiying Deng"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations in text generation is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, designed to compile outputs produced with minimal restrictions by LLMs. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also executed extensive experiments, evaluating prominent Chinese language models and the GPT series models to derive professional performance insights regarding hallucination challenges.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1146d40d3d01427a008a20530269667b8989750c.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 26,
        "score": 13.0
    },
    "165fdad3949b7abdb985cb8834c26c7baa7bd40f.pdf": {
        "title": "Multi-Objective Alignment of Large Language Models Through Hypervolume Maximization",
        "authors": [
            "Subhojyoti Mukherjee",
            "Anusha Lalitha",
            "Sailik Sengupta",
            "Aniket Deshmukh",
            "B. Kveton"
        ],
        "published_date": "2024",
        "abstract": "Multi-objective alignment from human feedback (MOAHF) in large language models (LLMs) is a challenging problem as human preferences are complex, multifaceted, and often conflicting. Recent works on MOAHF considered a-priori multi-objective optimization (MOO), where human preferences are known at training or inference time. In contrast, when human preferences are unknown or difficult to quantify, a natural approach is to cover the Pareto front by multiple diverse solutions. We propose an algorithm HaM for learning diverse LLM policies that maximizes their hypervolume. This is the first application of a-posteriori MOO to MOAHF. HaM is computationally and space efficient, and empirically superior across objectives such as harmlessness, helpfulness, humor, faithfulness, and hallucination, on various datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/165fdad3949b7abdb985cb8834c26c7baa7bd40f.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Multi-objective alignment from human feedback (MOAHF) in large language models (LLMs) is a challenging problem as human preferences are complex, multifaceted, and often conflicting. Recent works on MOAHF considered a-priori multi-objective optimization (MOO), where human preferences are known at training or inference time. In contrast, when human preferences are unknown or difficult to quantify, a natural approach is to cover the Pareto front by multiple diverse solutions. We propose an algorithm HaM for learning diverse LLM policies that maximizes their hypervolume. This is the first application of a-posteriori MOO to MOAHF. HaM is computationally and space efficient, and empirically superior across objectives such as harmlessness, helpfulness, humor, faithfulness, and hallucination, on various datasets.",
        "keywords": []
    },
    "8c8527f7615d53cbc21b9c3536486540f1c75000.pdf": {
        "title": "Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study",
        "authors": [
            "Qirui Jiao",
            "Daoyuan Chen",
            "Yilun Huang",
            "Yaliang Li",
            "Ying Shen"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8c8527f7615d53cbc21b9c3536486540f1c75000.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0,
        "summary": "",
        "keywords": []
    },
    "e131a11ad907023f655f22a4fae2b2a6f2db96f7.pdf": {
        "title": "Reasoning and Planning with Large Language Models in Code Development",
        "authors": [
            "Hao Ding",
            "Ziwei Fan",
            "Ingo G\u00fchring",
            "Gaurav Gupta",
            "Wooseok Ha",
            "Jun Huan",
            "Linbo Liu",
            "Behrooz Omidvar-Tehrani",
            "Shiqi Wang",
            "Hao Zhou"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e131a11ad907023f655f22a4fae2b2a6f2db96f7.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.",
        "keywords": []
    },
    "ffeec58ed1fc045c55512e20b30fce951913a3f0.pdf": {
        "title": "Sanitizing Large Language Models in Bug Detection with Data-Flow",
        "authors": [
            "Chengpeng Wang",
            "Wuqi Zhang",
            "Zian Su",
            "Xiangzhe Xu",
            "Xiangyu Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition . Specifically, we dissect data-flow paths into basic properties upon concise code snip-pets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks, and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively. LLMSAN is open-sourced at https: //github.com/chengpeng-wang/LLMSAN .",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ffeec58ed1fc045c55512e20b30fce951913a3f0.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition . Specifically, we dissect data-flow paths into basic properties upon concise code snip-pets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks, and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively. LLMSAN is open-sourced at https: //github.com/chengpeng-wang/LLMSAN .",
        "keywords": []
    },
    "89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf": {
        "title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Zhuo Chen",
            "Jiawei Liu",
            "Haotan Liu",
            "Qikai Cheng",
            "Fan Zhang",
            "Wei Lu",
            "Xiaozhong Liu"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
        "keywords": []
    },
    "bca44a53d9becd158ee0abf34c4255375fdc7327.pdf": {
        "title": "Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation",
        "authors": [
            "Xiaohua Wang",
            "Yuliang Yan",
            "Longtao Huang",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published_date": "2023",
        "abstract": ",",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/bca44a53d9becd158ee0abf34c4255375fdc7327.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 25,
        "score": 12.5,
        "summary": ",",
        "keywords": []
    },
    "d5ebd84b996491d8ffadefd05a32f8f25085935d.pdf": {
        "title": "Self-training Large Language Models through Knowledge Detection",
        "authors": [
            "Wei Jie Yeo",
            "Teddy Ferdinan",
            "Przemys\u0142aw Kazienko",
            "Ranjan Satapathy",
            "Erik Cambria"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) often necessitate extensive labeled datasets and training compute to achieve impressive performance across downstream tasks. This paper explores a self-training paradigm, where the LLM autonomously curates its own labels and selectively trains on unknown data samples identified through a reference-free consistency method. Empirical evaluations demonstrate significant improvements in reducing hallucination in generation across multiple subjects. Furthermore, the selective training framework mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing a critical limitation in training LLMs. Our findings suggest that such an approach can substantially reduce the dependency on large labeled datasets, paving the way for more scalable and cost-effective language model training.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d5ebd84b996491d8ffadefd05a32f8f25085935d.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Large language models (LLMs) often necessitate extensive labeled datasets and training compute to achieve impressive performance across downstream tasks. This paper explores a self-training paradigm, where the LLM autonomously curates its own labels and selectively trains on unknown data samples identified through a reference-free consistency method. Empirical evaluations demonstrate significant improvements in reducing hallucination in generation across multiple subjects. Furthermore, the selective training framework mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing a critical limitation in training LLMs. Our findings suggest that such an approach can substantially reduce the dependency on large labeled datasets, paving the way for more scalable and cost-effective language model training.",
        "keywords": []
    },
    "5cb8fc293567f4f2930712a3bf7dec97b4dd1776.pdf": {
        "title": "PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models",
        "authors": [
            "Sihao Hu",
            "Tiansheng Huang",
            "Ling Liu"
        ],
        "published_date": "2024",
        "abstract": "We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon's human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: https://github.com/git-disl/PokeLLMon.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5cb8fc293567f4f2930712a3bf7dec97b4dd1776.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 12.0,
        "summary": "We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon's human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: https://github.com/git-disl/PokeLLMon.",
        "keywords": []
    },
    "576023f7cc3da5a36ac0cfda402af859cc90be10.pdf": {
        "title": "Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models",
        "authors": [
            "Chun-Yi Kuan",
            "Wei-Ping Huang",
            "Hung-yi Lee"
        ],
        "published_date": "2024",
        "abstract": "Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/576023f7cc3da5a36ac0cfda402af859cc90be10.pdf",
        "venue": "Interspeech",
        "citationCount": 11,
        "score": 11.0
    },
    "9b05e1dfd158c307b74298df3d4608b93d2060a7.pdf": {
        "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models",
        "authors": [
            "Kim Sung-Bin",
            "Oh Hyun-Bin",
            "JungMok Lee",
            "Arda Senocak",
            "Joon Son Chung",
            "Tae-Hyun Oh"
        ],
        "published_date": "2024",
        "abstract": "Following the success of Large Language Models (LLMs), expanding their boundaries to new modalities represents a significant paradigm shift in multimodal understanding. Human perception is inherently multimodal, relying not only on text but also on auditory and visual cues for a complete understanding of the world. In recognition of this fact, audio-visual LLMs have recently emerged. Despite promising developments, the lack of dedicated benchmarks poses challenges for understanding and evaluating models. In this work, we show that audio-visual LLMs struggle to discern subtle relationships between audio and visual signals, leading to hallucinations and highlighting the need for reliable benchmarks. To address this, we introduce AVHBench, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs. Our benchmark includes tests for assessing hallucinations, as well as the cross-modal matching and reasoning abilities of these models. Our results reveal that most existing audio-visual LLMs struggle with hallucinations caused by cross-interactions between modalities, due to their limited capacity to perceive complex multimodal signals and their relationships. Additionally, we demonstrate that simple training with our AVHBench improves robustness of audio-visual LLMs against hallucinations. Dataset: https://github.com/kaist-ami/AVHBench",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/9b05e1dfd158c307b74298df3d4608b93d2060a7.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 11,
        "score": 11.0
    },
    "388e6dbb4b4486e01d4f040684560135a9e1ef71.pdf": {
        "title": "Contrastive Learning for Knowledge-Based Question Generation in Large Language Models",
        "authors": [
            "Zhenhong Zhang",
            "Jiajing Chen",
            "Weiyan Shi",
            "Lingjie Yi",
            "Chihang Wang",
            "Qian Yu"
        ],
        "published_date": "2024",
        "abstract": "With the rapid development of artificial intelligence technology, especially the increasingly widespread application of question-and-answer systems, high-quality question generation has become a key component in supporting the development of these systems. This article focuses on knowledge-based question generation technology, which aims to enable computers to simulate the human questioning process based on understanding specific texts or knowledge bases. In light of the issues of hallucination and knowledge gaps present in large-scale language models when applied to knowledge-intensive tasks, this paper proposes an enhanced question generation method that incorporates contrastive learning. This method utilizes multiple models to jointly mine domain knowledge and uses contrastive learning to guide the model in reducing noise and hallucinations in generation. Experimental results show that by designing prompts containing contrasting examples, the model's performance in question generation improves considerably, particularly when contrasting instructions and examples are used simultaneously, leading to the highest quality of generated questions and improved accuracy. These results demonstrate that the method proposed in this study, which combines contrasting context and chain-of-thought prompts, can effectively improve both the quality and the practicality of question generation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/388e6dbb4b4486e01d4f040684560135a9e1ef71.pdf",
        "venue": "2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI)",
        "citationCount": 11,
        "score": 11.0,
        "summary": "With the rapid development of artificial intelligence technology, especially the increasingly widespread application of question-and-answer systems, high-quality question generation has become a key component in supporting the development of these systems. This article focuses on knowledge-based question generation technology, which aims to enable computers to simulate the human questioning process based on understanding specific texts or knowledge bases. In light of the issues of hallucination and knowledge gaps present in large-scale language models when applied to knowledge-intensive tasks, this paper proposes an enhanced question generation method that incorporates contrastive learning. This method utilizes multiple models to jointly mine domain knowledge and uses contrastive learning to guide the model in reducing noise and hallucinations in generation. Experimental results show that by designing prompts containing contrasting examples, the model's performance in question generation improves considerably, particularly when contrasting instructions and examples are used simultaneously, leading to the highest quality of generated questions and improved accuracy. These results demonstrate that the method proposed in this study, which combines contrasting context and chain-of-thought prompts, can effectively improve both the quality and the practicality of question generation.",
        "keywords": []
    },
    "e384f513114d7e6c20d007b4a3ad13fa58cf83dd.pdf": {
        "title": "Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators",
        "authors": [
            "Dingkang Yang",
            "Dongling Xiao",
            "Jinjie Wei",
            "Mingcheng Li",
            "Zhaoyu Chen",
            "Ke Li",
            "Lihua Zhang"
        ],
        "published_date": "2024",
        "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, i.e., unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic representations, which compromise the internal factual knowledge of target LLMs. In addition, hallucinations typically exhibit multifaceted patterns in downstream tasks, limiting the model's holistic performance across tasks. In this paper, we propose a Comparator-driven Decoding-Time (CDT) framework to alleviate the response hallucination. Firstly, we construct hallucinatory and truthful comparators with multi-task fine-tuning samples. In this case, we present an instruction prototype-guided mixture of experts strategy to enhance the ability of the corresponding comparators to capture different hallucination or truthfulness patterns in distinct task instructions. CDT constrains next-token predictions to factuality-robust distributions by contrasting the logit differences between the target LLMs and these comparators. Systematic experiments on multiple downstream tasks show that our framework can significantly improve the model performance and response factuality.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e384f513114d7e6c20d007b4a3ad13fa58cf83dd.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, i.e., unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic representations, which compromise the internal factual knowledge of target LLMs. In addition, hallucinations typically exhibit multifaceted patterns in downstream tasks, limiting the model's holistic performance across tasks. In this paper, we propose a Comparator-driven Decoding-Time (CDT) framework to alleviate the response hallucination. Firstly, we construct hallucinatory and truthful comparators with multi-task fine-tuning samples. In this case, we present an instruction prototype-guided mixture of experts strategy to enhance the ability of the corresponding comparators to capture different hallucination or truthfulness patterns in distinct task instructions. CDT constrains next-token predictions to factuality-robust distributions by contrasting the logit differences between the target LLMs and these comparators. Systematic experiments on multiple downstream tasks show that our framework can significantly improve the model performance and response factuality.",
        "keywords": []
    },
    "4f83d9c391d782d358c2bf0d7ffc6150924dae01.pdf": {
        "title": "Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models",
        "authors": [
            "Kening Zheng",
            "Junkai Chen",
            "Yibo Yan",
            "Xin Zou",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Hallucination issues continue to affect multimodal large language models (MLLMs), with existing research mainly addressing object-level or attribute-level hallucinations, neglecting the more complex relation hallucinations that require advanced reasoning. Current benchmarks for relation hallucinations lack detailed evaluation and effective mitigation, and their datasets often suffer from biases due to systematic annotation processes. To address these challenges, we introduce Reefknot, a comprehensive benchmark targeting relation hallucinations, comprising over 20,000 real-world samples. We provide a systematic definition of relation hallucinations, integrating perceptive and cognitive perspectives, and construct a relation-based corpus using the Visual Genome scene graph dataset. Our comparative evaluation reveals significant limitations in current MLLMs' ability to handle relation hallucinations. Additionally, we propose a novel confidence-based mitigation strategy, which reduces the hallucination rate by an average of 9.75% across three datasets, including Reefknot. Our work offers valuable insights for achieving trustworthy multimodal intelligence.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4f83d9c391d782d358c2bf0d7ffc6150924dae01.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 11,
        "score": 11.0
    },
    "56ff9de0931bd1accb9d4e3f109afcbf31f7df25.pdf": {
        "title": "ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks",
        "authors": [
            "Xiaodong Yu",
            "Hao Cheng",
            "Xiaodong Liu",
            "Dan Roth",
            "Jianfeng Gao"
        ],
        "published_date": "2023",
        "abstract": "Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs' reliability in using new evidence for answering. We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection of LLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/56ff9de0931bd1accb9d4e3f109afcbf31f7df25.pdf",
        "venue": "NAACL-HLT",
        "citationCount": 22,
        "score": 11.0,
        "summary": "Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs' reliability in using new evidence for answering. We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection of LLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.",
        "keywords": []
    },
    "705ffeccfde95c3b0723f197c4565f7d3f0451a1.pdf": {
        "title": "Zero-Resource Hallucination Prevention for Large Language Models",
        "authors": [
            "Junyu Luo",
            "Cao Xiao",
            "Fenglong Ma"
        ],
        "published_date": "2023",
        "abstract": "The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of\"hallucination,\"which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from responding to unfamiliar topics, thus reducing hallucinations. We validate SELF-FAMILIARITY across four different large language models, demonstrating consistently superior performance compared to existing techniques. Our findings propose a significant shift towards preemptive strategies for hallucination mitigation in LLM assistants, promising improvements in reliability, applicability, and interpretability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/705ffeccfde95c3b0723f197c4565f7d3f0451a1.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 22,
        "score": 11.0
    },
    "bff123171a5b7bddd699a72daed96b4c56742069.pdf": {
        "title": "MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context",
        "authors": [
            "Zishan Gu",
            "Changchang Yin",
            "Fenglin Liu",
            "Ping Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMs fine-tuning and training. Despite their advancements, there has been scant research on the robustness of these models against hallucination when fine-tuned on smaller datasets. In this study, we introduce a new benchmark dataset, the Medical Visual Hallucination Test (MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH comprises five tasks to evaluate hallucinations in LVLMs within the medical context, which includes tasks for comprehensive understanding of textual and visual input, as well as long textual response generation. Our extensive experiments with both general and medical LVLMs reveal that, although medical LVLMs demonstrate promising performance on standard medical tasks, they are particularly susceptible to hallucinations, often more so than the general models, raising significant concerns about the reliability of these domain-specific models. For medical LVLMs to be truly valuable in real-world applications, they must not only accurately integrate medical knowledge but also maintain robust reasoning abilities to prevent hallucination. Our work paves the way for future evaluations of these studies.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/bff123171a5b7bddd699a72daed96b4c56742069.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMs fine-tuning and training. Despite their advancements, there has been scant research on the robustness of these models against hallucination when fine-tuned on smaller datasets. In this study, we introduce a new benchmark dataset, the Medical Visual Hallucination Test (MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH comprises five tasks to evaluate hallucinations in LVLMs within the medical context, which includes tasks for comprehensive understanding of textual and visual input, as well as long textual response generation. Our extensive experiments with both general and medical LVLMs reveal that, although medical LVLMs demonstrate promising performance on standard medical tasks, they are particularly susceptible to hallucinations, often more so than the general models, raising significant concerns about the reliability of these domain-specific models. For medical LVLMs to be truly valuable in real-world applications, they must not only accurately integrate medical knowledge but also maintain robust reasoning abilities to prevent hallucination. Our work paves the way for future evaluations of these studies.",
        "keywords": []
    },
    "4582779668ab801f29db457790cd291767510035.pdf": {
        "title": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs",
        "authors": [
            "Lihui Liu",
            "Zihao Wang",
            "Ruizhong Qiu",
            "Yikun Ban",
            "Hanghang Tong"
        ],
        "published_date": "2024",
        "abstract": "Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps. On the other hand, knowledge graph (KG) based question answering methods are capable of accurately identifying the correct answers with the help of knowledge graph, yet its accuracy could quickly deteriorate when the knowledge graph itself is sparse and incomplete. It remains a critical challenge on how to integrate knowledge graph reasoning with LLMs in a mutually beneficial way so as to mitigate both the hallucination problem of LLMs as well as the incompleteness issue of knowledge graphs. In this paper, we propose 'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs with knowledge graph based logic query reasoning. LGOT seamlessly combines knowledge graph reasoning and LLMs, effectively breaking down complex logic queries into easy to answer subquestions. Through the utilization of both knowledge graph reasoning and LLMs, it successfully derives answers for each subquestion. By aggregating these results and selecting the highest quality candidate answers for each step, LGOT achieves accurate results to complex questions. Our experimental findings demonstrate substantial performance enhancements, with up to 20% improvement over ChatGPT.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4582779668ab801f29db457790cd291767510035.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps. On the other hand, knowledge graph (KG) based question answering methods are capable of accurately identifying the correct answers with the help of knowledge graph, yet its accuracy could quickly deteriorate when the knowledge graph itself is sparse and incomplete. It remains a critical challenge on how to integrate knowledge graph reasoning with LLMs in a mutually beneficial way so as to mitigate both the hallucination problem of LLMs as well as the incompleteness issue of knowledge graphs. In this paper, we propose 'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs with knowledge graph based logic query reasoning. LGOT seamlessly combines knowledge graph reasoning and LLMs, effectively breaking down complex logic queries into easy to answer subquestions. Through the utilization of both knowledge graph reasoning and LLMs, it successfully derives answers for each subquestion. By aggregating these results and selecting the highest quality candidate answers for each step, LGOT achieves accurate results to complex questions. Our experimental findings demonstrate substantial performance enhancements, with up to 20% improvement over ChatGPT.",
        "keywords": []
    },
    "e7d5f2da48b9141e18de9ea57ed8a4cebb6a09cd.pdf": {
        "title": "Transforming nursing with large language models: from concept to practice.",
        "authors": [
            "B. Woo",
            "Tom Huynh",
            "Arthur Tang",
            "Nhat Bui",
            "Giang Nguyen",
            "W. Tam"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) such as ChatGPT have emerged as potential game-changers in nursing, aiding in patient education, diagnostic assistance, treatment recommendations, and administrative task efficiency. While these advancements signal promising strides in healthcare, integrated LLMs are not without challenges, particularly artificial intelligence hallucination and data privacy concerns. Methodologies such as prompt engineering, temperature adjustments, model fine-tuning, and local deployment are proposed to refine the accuracy of LLMs and ensure data security. While LLMs offer transformative potential, it is imperative to acknowledge that they cannot substitute the intricate expertise of human professionals in the clinical field, advocating for a synergistic approach in patient care.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e7d5f2da48b9141e18de9ea57ed8a4cebb6a09cd.pdf",
        "venue": "European Journal of Cardiovascular Nursing",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Large language models (LLMs) such as ChatGPT have emerged as potential game-changers in nursing, aiding in patient education, diagnostic assistance, treatment recommendations, and administrative task efficiency. While these advancements signal promising strides in healthcare, integrated LLMs are not without challenges, particularly artificial intelligence hallucination and data privacy concerns. Methodologies such as prompt engineering, temperature adjustments, model fine-tuning, and local deployment are proposed to refine the accuracy of LLMs and ensure data security. While LLMs offer transformative potential, it is imperative to acknowledge that they cannot substitute the intricate expertise of human professionals in the clinical field, advocating for a synergistic approach in patient care.",
        "keywords": []
    },
    "36ffd74db97f98f121a6c2954bf98c93dad5d2ce.pdf": {
        "title": "Measuring the Inconsistency of Large Language Models in Preferential Ranking",
        "authors": [
            "Xiutian Zhao",
            "Ke Wang",
            "Wei Peng"
        ],
        "published_date": "2024",
        "abstract": "Despite large language models\u2019 (LLMs\u2019) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/36ffd74db97f98f121a6c2954bf98c93dad5d2ce.pdf",
        "venue": "KNOWLLM",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Despite large language models\u2019 (LLMs\u2019) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.",
        "keywords": []
    },
    "4cf85f9436bb8b5b1c68715f44d6b67254413ef8.pdf": {
        "title": "TimeR^4 : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering",
        "authors": [
            "Xinying Qian",
            "Ying Zhang",
            "Yu Zhao",
            "Baohang Zhou",
            "Xuhui Sui",
            "Li Zhang",
            "Kehui Song"
        ],
        "published_date": "2024",
        "abstract": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMs\u2019 temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR^4.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8% and 22.5% on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4cf85f9436bb8b5b1c68715f44d6b67254413ef8.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMs\u2019 temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR^4.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8% and 22.5% on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4.",
        "keywords": []
    },
    "6947893915861e8c30bc6b010eb1faf0d82f0a19.pdf": {
        "title": "Logical Consistency of Large Language Models in Fact-checking",
        "authors": [
            "Bishwamittra Ghosh",
            "Sarah Hasan",
            "Naheed Anjum Arafat",
            "Arijit Khan"
        ],
        "published_date": "2024",
        "abstract": "In recent years, large language models (LLMs) have demonstrated significant success in performing varied natural language tasks such as language translation, question-answering, summarizing, fact-checking, etc. Despite LLMs' impressive ability to generate human-like texts, LLMs are infamous for their inconsistent responses - a meaning-preserving change in the input query results in an inconsistent response and attributes to vulnerabilities of LLMs such as hallucination. Consequently, existing research focuses on simple paraphrasing-based consistency assessment of LLMs, and ignores complex queries that necessitate an even better understanding of logical reasoning by an LLM. Our work therefore addresses the logical inconsistency of LLMs under complex logical queries with primitive logical operators, e.g., negation, conjunction, and disjunction. As a test bed, we consider retrieval-augmented LLMs on a fact-checking task involving propositional logic queries from knowledge graphs (KGs). Our contributions are threefold. Benchmark: We introduce three logical fact-checking datasets over KGs for community development towards logically consistent LLMs. Assessment: We propose consistency measures of LLMs on propositional logic queries and demonstrate that existing LLMs lack logical consistency, especially on complex queries. Improvement: We employ supervised fine-tuning to improve the logical consistency of LLMs on the complex fact-checking task with KG contexts. We have made our source code and benchmarks available.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6947893915861e8c30bc6b010eb1faf0d82f0a19.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 11,
        "score": 11.0
    },
    "2b277717203bb5354e7d41e79a35c59e34fa6778.pdf": {
        "title": "From technical to understandable: Artificial Intelligence Large Language Models improve the readability of knee radiology reports.",
        "authors": [
            "J. Butler",
            "James Puleo",
            "Michael Harrington",
            "J. Dahmen",
            "Andrew J. Rosenbaum",
            "G. Kerkhoffs",
            "J. Kennedy"
        ],
        "published_date": "2024",
        "abstract": "PURPOSE\nThe purpose of this study was to evaluate the effectiveness of an Artificial Intelligence-Large Language Model (AI-LLM) at improving the readability of knee radiology reports.\n\n\nMETHODS\nReports of 100 knee X-rays, 100 knee computed tomography (CT) scans and 100 knee magnetic resonance imaging (MRI) scans were retrieved. The following prompt command was inserted into the AI-LLM: 'Explain this radiology report to a patient in layman's terms in the second person:[Report Text]'. The Flesch-Kincaid reading level (FKRL) score, Flesch reading ease (FRE) score and report length were calculated for the original radiology report and the AI-LLM generated report. Any 'hallucination' or inaccurate text produced by the AI-LLM-generated report was documented.\n\n\nRESULTS\nStatistically significant improvements in mean FKRL scores in the AI-LLM generated X-ray report (12.7\u2009\u00b1\u20091.0-7.2\u2009\u00b1\u20090.6), CT report (13.4\u2009\u00b1\u20091.0-7.5\u2009\u00b1\u20090.5) and MRI report (13.5\u2009\u00b1\u20090.9-7.5\u2009\u00b1\u20090.6) were observed. Statistically significant improvements in mean FRE scores in the AI-LLM generated X-ray report (39.5\u2009\u00b1\u20097.5-76.8\u2009\u00b1\u20095.1), CT report (27.3\u2009\u00b1\u20095.9-73.1\u2009\u00b1\u20095.6) and MRI report (26.8\u2009\u00b1\u20096.4-73.4\u2009\u00b1\u20095.0) were observed. Superior FKRL scores and FRE scores were observed in the AI-LLM-generated X-ray report compared to the AI-LLM-generated CT report and MRI report, p\u2009<\u20090.001. The hallucination rates in the AI-LLM generated X-ray report, CT report and MRI report were 2%, 5% and 5%, respectively.\n\n\nCONCLUSIONS\nThis study highlights the promising use of AI-LLMs as an innovative, patient-centred strategy to improve the readability of knee radiology reports. The clinical relevance of this study is that an AI-LLM-generated knee radiology report may enhance patients' understanding of their imaging reports, potentially reducing the responder burden placed on the ordering physicians. However, due to the 'hallucinations' produced by the AI-LLM-generated report, the ordering physician must always engage in a collaborative discussion with the patient regarding both reports and the corresponding images.\n\n\nLEVEL OF EVIDENCE\nLevel IV.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2b277717203bb5354e7d41e79a35c59e34fa6778.pdf",
        "venue": "Knee Surgery, Sports Traumatology, Arthroscopy",
        "citationCount": 11,
        "score": 11.0,
        "summary": "PURPOSE\nThe purpose of this study was to evaluate the effectiveness of an Artificial Intelligence-Large Language Model (AI-LLM) at improving the readability of knee radiology reports.\n\n\nMETHODS\nReports of 100 knee X-rays, 100 knee computed tomography (CT) scans and 100 knee magnetic resonance imaging (MRI) scans were retrieved. The following prompt command was inserted into the AI-LLM: 'Explain this radiology report to a patient in layman's terms in the second person:[Report Text]'. The Flesch-Kincaid reading level (FKRL) score, Flesch reading ease (FRE) score and report length were calculated for the original radiology report and the AI-LLM generated report. Any 'hallucination' or inaccurate text produced by the AI-LLM-generated report was documented.\n\n\nRESULTS\nStatistically significant improvements in mean FKRL scores in the AI-LLM generated X-ray report (12.7\u2009\u00b1\u20091.0-7.2\u2009\u00b1\u20090.6), CT report (13.4\u2009\u00b1\u20091.0-7.5\u2009\u00b1\u20090.5) and MRI report (13.5\u2009\u00b1\u20090.9-7.5\u2009\u00b1\u20090.6) were observed. Statistically significant improvements in mean FRE scores in the AI-LLM generated X-ray report (39.5\u2009\u00b1\u20097.5-76.8\u2009\u00b1\u20095.1), CT report (27.3\u2009\u00b1\u20095.9-73.1\u2009\u00b1\u20095.6) and MRI report (26.8\u2009\u00b1\u20096.4-73.4\u2009\u00b1\u20095.0) were observed. Superior FKRL scores and FRE scores were observed in the AI-LLM-generated X-ray report compared to the AI-LLM-generated CT report and MRI report, p\u2009<\u20090.001. The hallucination rates in the AI-LLM generated X-ray report, CT report and MRI report were 2%, 5% and 5%, respectively.\n\n\nCONCLUSIONS\nThis study highlights the promising use of AI-LLMs as an innovative, patient-centred strategy to improve the readability of knee radiology reports. The clinical relevance of this study is that an AI-LLM-generated knee radiology report may enhance patients' understanding of their imaging reports, potentially reducing the responder burden placed on the ordering physicians. However, due to the 'hallucinations' produced by the AI-LLM-generated report, the ordering physician must always engage in a collaborative discussion with the patient regarding both reports and the corresponding images.\n\n\nLEVEL OF EVIDENCE\nLevel IV.",
        "keywords": []
    },
    "dd20cfadf992986b5d71b3a44b5a8660f0d68671.pdf": {
        "title": "Addressing Bias and Hallucination in Large Language Models",
        "authors": [
            "N. R. Sahoo",
            "Ashita Saxena",
            "Kishan Maharaj",
            "Arif Ahmad",
            "Abhijit Mishra",
            "Pushpak Bhattacharyya"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/dd20cfadf992986b5d71b3a44b5a8660f0d68671.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 10,
        "score": 10.0,
        "summary": "",
        "keywords": []
    },
    "7112d0d36b960b590f55569bc294b2190d288860.pdf": {
        "title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models",
        "authors": [
            "Kaiwen Zuo",
            "Yirui Jiang"
        ],
        "published_date": "2024",
        "abstract": "Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations -- generating medically implausible or inaccurate information -- presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs' reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7112d0d36b960b590f55569bc294b2190d288860.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations -- generating medically implausible or inaccurate information -- presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs' reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.",
        "keywords": []
    },
    "5be8fd7aa6564ea1884626c16bcac36508f79ff1.pdf": {
        "title": "Generative Artificial Intelligence and Large Language Models in Primary Care Medical Education.",
        "authors": [
            "D. J. Parente"
        ],
        "published_date": "2024",
        "abstract": "Generative artificial intelligence and large language models are the continuation of a technological revolution in information processing that began with the invention of the transistor in 1947. These technologies, driven by transformer architectures for artificial neural networks, are poised to broadly influence society. It is already apparent that these technologies will be adapted to drive innovation in education. Medical education is a high-risk activity: Information that is incorrectly taught to a student may go unrecognized for years until a relevant clinical situation appears in which that error can lead to patient harm. In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education-hallucination, bias, cost, and security-and suggest some approaches to confronting these problems. Additionally, I identify the potential applications of generative artificial intelligence to medical education, including personalized instruction, simulation, feedback, evaluation, augmentation of qualitative research, and performance of critical assessment of the existing scientific literature.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5be8fd7aa6564ea1884626c16bcac36508f79ff1.pdf",
        "venue": "Family Medicine",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Generative artificial intelligence and large language models are the continuation of a technological revolution in information processing that began with the invention of the transistor in 1947. These technologies, driven by transformer architectures for artificial neural networks, are poised to broadly influence society. It is already apparent that these technologies will be adapted to drive innovation in education. Medical education is a high-risk activity: Information that is incorrectly taught to a student may go unrecognized for years until a relevant clinical situation appears in which that error can lead to patient harm. In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education-hallucination, bias, cost, and security-and suggest some approaches to confronting these problems. Additionally, I identify the potential applications of generative artificial intelligence to medical education, including personalized instruction, simulation, feedback, evaluation, augmentation of qualitative research, and performance of critical assessment of the existing scientific literature.",
        "keywords": []
    },
    "8a9b43946dc10f91ce8c5971a1f247fbacda7a42.pdf": {
        "title": "Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability",
        "authors": [
            "Haiyan Zhao",
            "Fan Yang",
            "Himabindu Lakkaraju",
            "Mengnan Du"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8a9b43946dc10f91ce8c5971a1f247fbacda7a42.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0,
        "summary": "",
        "keywords": []
    },
    "e615f33365ea1d439507fc477588528ffb0764a8.pdf": {
        "title": "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks",
        "authors": [
            "Yue Zhou",
            "Henry Peng Zou",
            "Barbara Di Eugenio",
            "Yang Zhang"
        ],
        "published_date": "2024",
        "abstract": "We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e615f33365ea1d439507fc477588528ffb0764a8.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 10,
        "score": 10.0,
        "summary": "We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.",
        "keywords": []
    },
    "108bc0498629f4710b44076fe0c6270954494097.pdf": {
        "title": "On the Limitations of Large Language Models (LLMs): False Attribution",
        "authors": [
            "Tosin P. Adewumi",
            "Nudrat Habib",
            "Lama Alkhaled",
            "Elisa Barney"
        ],
        "published_date": "2024",
        "abstract": "In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/108bc0498629f4710b44076fe0c6270954494097.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0,
        "summary": "In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.",
        "keywords": []
    },
    "b169426b9181adee0e7d6616fc12fc12611d9901.pdf": {
        "title": "The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem",
        "authors": [
            "Joe B Hakim",
            "Jeffery L. Painter",
            "D. Ramcharran",
            "V. Kara",
            "Greg Powell",
            "Paulina Sobczak",
            "Chiho Sato",
            "Andrew Bate",
            "Andrew Beam"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are useful tools with the capacity for performing specific types of knowledge work at an effective scale. However, LLM deployments in high-risk and safety-critical domains pose unique challenges, notably the issue of ``hallucination,'' where LLMs can generate fabricated information. This is particularly concerning in settings such as drug safety, where inaccuracies could lead to patient harm. To mitigate these risks, we have developed and demonstrated a proof of concept suite of guardrails specifically designed to mitigate certain types of hallucinations and errors for drug safety, and potentially applicable to other medical safety-critical contexts. These guardrails include mechanisms to detect anomalous documents to prevent the ingestion of inappropriate data, identify incorrect drug names or adverse event terms, and convey uncertainty in generated content. We integrated these guardrails with an LLM fine-tuned for a text-to-text task, which involves converting both structured and unstructured data within adverse event reports into natural language. This method was applied to translate individual case safety reports, demonstrating effective application in a pharmacovigilance processing task. Our guardrail framework offers a set of tools with broad applicability across various domains, ensuring LLMs can be safely used in high-risk situations by eliminating the occurrence of key errors, including the generation of incorrect pharmacovigilance-related terms, thus adhering to stringent regulatory and quality standards in medical safety-critical environments.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b169426b9181adee0e7d6616fc12fc12611d9901.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0
    },
    "59395cf4f9346ef4ccb37499a3a7e52c2978fc61.pdf": {
        "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
        "authors": [
            "Armin Toroghi",
            "Willis Guo",
            "Mohammad Mahdi Torabi pour",
            "Scott Sanner"
        ],
        "published_date": "2024",
        "abstract": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *\u201cIn which city was Silvio Berlusconi\u2019s first wife born?\u201d*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *\u201cDo I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?\u201d* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R^3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasks\u2014question answering, claim verification, and preference matching\u2014our findings showcase R^3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/59395cf4f9346ef4ccb37499a3a7e52c2978fc61.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *\u201cIn which city was Silvio Berlusconi\u2019s first wife born?\u201d*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *\u201cDo I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?\u201d* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R^3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasks\u2014question answering, claim verification, and preference matching\u2014our findings showcase R^3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",
        "keywords": []
    },
    "425d16205b28ce175c8429965a964d19b6f390c1.pdf": {
        "title": "Hallucination Detection in Large Language Models with Metamorphic Relations",
        "authors": [
            "Borui Yang",
            "Md Afif Al Mamun",
            "Jie M. Zhang",
            "Gias Uddin"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) are prone to hallucinations, e.g., factually incorrect information, in their responses. These hallucinations present challenges for LLM-based applications that demand high factual accuracy. Existing hallucination detection methods primarily depend on external resources, which can suffer from issues such as low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability. There are also methods depending on output probabilities, which are often inaccessible for closed-source LLMs like GPT models. This paper presents MetaQA, a self-contained hallucination detection approach that leverages metamorphic relation and prompt mutation. Unlike existing methods, MetaQA operates without any external resources and is compatible with both open-source and closed-source LLMs.\n \n \n \nMetaQA is based on the hypothesis that if an LLM\u2019s response is a hallucination, the designed metamorphic relations will be violated. We compare MetaQA with the state-of-the-art zero-resource hallucination detection method, SelfCheckGPT, across multiple datasets, and on two open-source and two closed-source LLMs. Our results reveal that MetaQA outperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and 0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an average F1-score of 0.435, compared to SelfCheckGPT\u2019s F1-score of 0.205, representing an improvement rate of 112.2%. MetaQA also demonstrates superiority across all different categories of questions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/425d16205b28ce175c8429965a964d19b6f390c1.pdf",
        "venue": "Proc. ACM Softw. Eng.",
        "citationCount": 9,
        "score": 9.0
    },
    "655f92355d7930919a01125bd7a35c812498b1a9.pdf": {
        "title": "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination",
        "authors": [
            "Pittawat Taveekitworachai",
            "Febri Abdullah",
            "R. Thawonmas"
        ],
        "published_date": "2024",
        "abstract": "This paper presents a series of investigations into an interesting phenomenon where we observe performance increases in large language models (LLMs) when providing a prompt that causes and exploits hallucination. We propose null-shot prompting, a counter-intuitive approach where we intentionally instruct LLMs to look at and utilize information from a null section. We investigate null-shot prompting on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and reading comprehension. We observe a substantial increase in performance in arithmetic reasoning tasks for various models, with up to a 44.62% increase compared to a baseline in one model. Therefore, we investigate deeper into this task by utilizing a more challenging mathematics problem-solving benchmark. We observe that LLMs benefit from hallucination in null-shot prompting in this task and discuss the mathematical topics that benefit the most from introducing hallucination in the prompt. We continue our investigation by evaluating hallucination detection abilities of the LLMs when using null-shot prompting. We find surprising results where hallucination in prompts can improve hallucination detection abilities of many LLMs. We also examine the effects of introducing both reasoning, which is known to mitigate hallucination, and hallucination simultaneously in the prompt and observe another surprising turn for the mathematics problem-solving benchmark with many performance improvements. We hope this paper will spark more interest, investigations, and discussions on how hallucination in prompts LLMs and even bolsters them in certain cases.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/655f92355d7930919a01125bd7a35c812498b1a9.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 9,
        "score": 9.0,
        "summary": "This paper presents a series of investigations into an interesting phenomenon where we observe performance increases in large language models (LLMs) when providing a prompt that causes and exploits hallucination. We propose null-shot prompting, a counter-intuitive approach where we intentionally instruct LLMs to look at and utilize information from a null section. We investigate null-shot prompting on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and reading comprehension. We observe a substantial increase in performance in arithmetic reasoning tasks for various models, with up to a 44.62% increase compared to a baseline in one model. Therefore, we investigate deeper into this task by utilizing a more challenging mathematics problem-solving benchmark. We observe that LLMs benefit from hallucination in null-shot prompting in this task and discuss the mathematical topics that benefit the most from introducing hallucination in the prompt. We continue our investigation by evaluating hallucination detection abilities of the LLMs when using null-shot prompting. We find surprising results where hallucination in prompts can improve hallucination detection abilities of many LLMs. We also examine the effects of introducing both reasoning, which is known to mitigate hallucination, and hallucination simultaneously in the prompt and observe another surprising turn for the mathematics problem-solving benchmark with many performance improvements. We hope this paper will spark more interest, investigations, and discussions on how hallucination in prompts LLMs and even bolsters them in certain cases.",
        "keywords": []
    },
    "d0fe343fbdecaf4cc477d70e8701f9a6935b13d0.pdf": {
        "title": "Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment",
        "authors": [
            "Fanqi Wan",
            "Xinting Huang",
            "Leyang Cui",
            "Xiaojun Quan",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d0fe343fbdecaf4cc477d70e8701f9a6935b13d0.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0,
        "summary": "",
        "keywords": []
    },
    "10269639a0462a0e9790bd0524f5a092325d8d51.pdf": {
        "title": "Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine",
        "authors": [
            "Shayan Meshkat Alsadat",
            "Jean-Raphael Gaglione",
            "D. Neider",
            "U. Topcu",
            "Zhe Xu"
        ],
        "published_date": "2024",
        "abstract": "We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses large language models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning (RL) algorithm directly with the high-level knowledge that requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. Moreover, we demonstrate LARM-RM robustness to LLM hallucination and show the theoretical guarantee of our algorithm to converge to an optimal policy. We show that LARL-RM speeds up the convergence by implementing our method in two case studies and compare it to other RL methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/10269639a0462a0e9790bd0524f5a092325d8d51.pdf",
        "venue": "American Control Conference",
        "citationCount": 9,
        "score": 9.0,
        "summary": "We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses large language models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning (RL) algorithm directly with the high-level knowledge that requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. Moreover, we demonstrate LARM-RM robustness to LLM hallucination and show the theoretical guarantee of our algorithm to converge to an optimal policy. We show that LARL-RM speeds up the convergence by implementing our method in two case studies and compare it to other RL methods.",
        "keywords": []
    },
    "dd6b124606e3696dcddc93c889a824feaa322117.pdf": {
        "title": "Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization",
        "authors": [
            "Yuhan Fu",
            "Ruobing Xie",
            "Xingwu Sun",
            "Zhanhui Kang",
            "Xirong Li"
        ],
        "published_date": "2024",
        "abstract": "Multimodal Large Language Models (MLLMs) are known to hallucinate, which limits their practical applications. Recent works have attempted to apply Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but have shown inconsistent improvements in mitigating hallucinations. To address this issue more effectively, we introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes. Specifically, we develop three types of preference pair data targeting the following causes of MLLM hallucinations: (1) insufficient visual capabilities, (2) long context generation, and (3) multimodal conflicts. Experimental results demonstrate that our method achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art (SOTA) methods and highlighting the potential of our approach. Ablation studies and in-depth analyses further confirm the effectiveness of our method and suggest the potential for further improvements through scaling up.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/dd6b124606e3696dcddc93c889a824feaa322117.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 9,
        "score": 9.0
    },
    "b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5.pdf": {
        "title": "Skip \\n: A Simple Method to Reduce Hallucination in Large Vision-Language Models",
        "authors": [
            "Zongbo Han",
            "Zechen Bai",
            "Haiyang Mei",
            "Qianli Xu",
            "Changqing Zhang",
            "Mike Zheng Shou"
        ],
        "published_date": "2024",
        "abstract": "Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks (\\n\\n), where the content before and after '\\n\\n' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '\\n\\n' should be obviously different from the preceding contents with less hallucinatory descriptions, thereby increasing the probability of hallucinatory descriptions subsequent to the '\\n\\n'. We have validated this hypothesis on multiple publicly available LVLMs. Besides, we find that deliberately inserting '\\n\\n' at the generated description can induce more hallucinations. A simple method is proposed to effectively mitigate the hallucination of LVLMs by skipping the output of '\\n'.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "c680e5d34b713f8b63ad68149973d5b2b485dd07.pdf": {
        "title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Junzhe Chen",
            "Tianshu Zhang",
            "Shiyu Huang",
            "Yuwei Niu",
            "Linfeng Zhang",
            "Lijie Wen",
            "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Despite the recent breakthroughs achieved by Large Vision Language Models (LVLMs) in understanding and responding to complex visual-textual contexts, their inherent hallucination tendencies limit their practical application in real-world scenarios that demand high levels of precision. Existing methods typically either fine-tune the LVLMs using additional data, which incurs extra costs in manual annotation and computational resources or perform comparisons at the decoding stage, which may eliminate useful language priors for reasoning while introducing inference time overhead. Therefore, we propose ICT, a lightweight, training-free method that calculates an intervention direction to shift the model\u2019s focus towards different levels of visual information, enhancing its attention to high-level and fine-grained visual details. During the forward pass stage, the intervention is applied to the attention heads that encode the overall image information and the fine-grained object details, effectively mitigating the phenomenon of overly language priors, and thereby alleviating hallucinations. Extensive experiments demonstrate that ICT achieves strong performance with a small amount of data and generalizes well across different datasets and models. Our codes are publicly available at:https://github.com/THU-BPM/ICT/.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c680e5d34b713f8b63ad68149973d5b2b485dd07.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 8,
        "score": 8.0
    },
    "7e38ac6f71408383939ec05f60b0bd85759a4c4e.pdf": {
        "title": "VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models",
        "authors": [
            "Qingxing Cao",
            "Junhao Cheng",
            "Xiaodan Liang",
            "Liang Lin"
        ],
        "published_date": "2024",
        "abstract": "Despite the signi\ufb01cant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hal-lucination problem, where the LVLMs\u2019 response contains descriptions of non-existent objects. Although various benchmarks have been proposed to investigate this problem, they mostly focus on single-turn evaluation and overlook the hallucination raised by textual inputs. To investigate the hallucination problem of LVLMs when given long-term misleading textual history, we propose a novel visual dialogue hallucination evaluation benchmark VisDiaHalBench. The benchmark consists of samples with \ufb01ve-turn questions about an edited image and its original version. Vis-DiaHalBench differs from previous hallucination benchmarks in the following three points:",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7e38ac6f71408383939ec05f60b0bd85759a4c4e.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Despite the signi\ufb01cant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hal-lucination problem, where the LVLMs\u2019 response contains descriptions of non-existent objects. Although various benchmarks have been proposed to investigate this problem, they mostly focus on single-turn evaluation and overlook the hallucination raised by textual inputs. To investigate the hallucination problem of LVLMs when given long-term misleading textual history, we propose a novel visual dialogue hallucination evaluation benchmark VisDiaHalBench. The benchmark consists of samples with \ufb01ve-turn questions about an edited image and its original version. Vis-DiaHalBench differs from previous hallucination benchmarks in the following three points:",
        "keywords": []
    },
    "1541bc9e588bfcd4bf365c868fa2f11461896980.pdf": {
        "title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models",
        "authors": [
            "Kenza Benkirane",
            "Laura Gongas",
            "Shahar Pelles",
            "Naomi Fuchs",
            "Joshua Darmon",
            "Pontus Stenetorp",
            "David Ifeoluwa Adelani",
            "Eduardo S\u00e1nchez",
            "Meta"
        ],
        "published_date": "2024",
        "abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates sentence-level hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1541bc9e588bfcd4bf365c868fa2f11461896980.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates sentence-level hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.",
        "keywords": []
    },
    "2126e045f81b831da34c185e2b51a49194bf4aa4.pdf": {
        "title": "NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models",
        "authors": [
            "Kai Wu",
            "Boyuan Jiang",
            "Zhengkai Jiang",
            "Qingdong He",
            "Donghao Luo",
            "Shengzhi Wang",
            "Qingwen Liu",
            "Chengjie Wang"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) contribute a powerful mechanism to understanding visual information building on large language models. However, MLLMs are notorious for suffering from hallucinations, especially when generating lengthy, detailed descriptions for images. Our analysis reveals that hallucinations stem from the inherent summarization mechanism of large language models, leading to excessive dependence on linguistic tokens while neglecting vision information. In this paper, we propose NoiseBoost, a broadly applicable and simple method for alleviating hallucinations for MLLMs through the integration of noise feature perturbations. Noise perturbation acts as a regularizer, facilitating a balanced distribution of attention weights among visual and linguistic tokens. Despite its simplicity, NoiseBoost consistently enhances the performance of MLLMs across common training strategies, including supervised fine-tuning and reinforcement learning. Further, NoiseBoost pioneerly enables semi-supervised learning for MLLMs, unleashing the power of unlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves dense caption accuracy by 8.1% with human evaluation and achieves comparable results with 50% of the data by mining unlabeled data. Code and models are available at https://kaiwu5.github.io/noiseboost.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2126e045f81b831da34c185e2b51a49194bf4aa4.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "682ff66a5ec0248f7e4a17a684b2d1e328e57f70.pdf": {
        "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
        "authors": [
            "Yuzhe Gu",
            "Ziwei Ji",
            "Wenwei Zhang",
            "Chengqi Lyu",
            "Dahua Lin",
            "Kai Chen"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) exhibit hallucinations in long-form question-answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators. To facilitate the scalable oversight of LLM hallucinations, this paper introduces an iterative self-training framework that simultaneously and progressively scales up the hallucination annotation dataset and improves the accuracy of the hallucination annotator. Based on the Expectation Maximization (EM) algorithm, in each iteration, the framework first applies a hallucination annotation pipeline to annotate a scaled dataset and then trains a more accurate hallucination annotator on the dataset. This new hallucination annotator is adopted in the hallucination annotation pipeline used for the next iteration. Extensive experimental results demonstrate that the finally obtained hallucination annotator with only 7B parameters surpasses the performance of GPT-4 and obtains new state-of-the-art hallucination detection results on HaluEval and HalluQA by zero-shot inference. Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference (NLI) metric increasing from 25% to 37% on HaluEval.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/682ff66a5ec0248f7e4a17a684b2d1e328e57f70.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 8,
        "score": 8.0
    },
    "fca2da71f3dce2f757aef39e561a572f68106603.pdf": {
        "title": "Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models",
        "authors": [
            "Bei Yan",
            "Jie Zhang",
            "Zheng Yuan",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "published_date": "2024",
        "abstract": "Despite the rapid progress and outstanding performance of Large Vision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the issue of hallucination, i.e., LVLMs tend to generate responses that are inconsistent with the corresponding visual inputs. To evaluate the degree of hallucination in LVLMs, previous works have proposed a series of benchmarks featuring different types of tasks and evaluation metrics. However, we find that the quality of the existing hallucination benchmarks varies, with some suffering from problems, e.g., inconsistent evaluation results under repeated tests, and misalignment with human evaluation. To this end, we propose a Hallucination benchmark Quality Measurement framework (HQM), which leverages various indicators to assess the reliability and validity of existing hallucination benchmarks separately. Specifically, for reliability we explore test-retest reliability and parallel-forms reliability, while for validity we examine criterion validity and coverage of hallucination types. Furthermore, based on the results of our quality measurement, we construct a High-Quality Hallucination Benchmark (HQH) for LVLMs, which demonstrates superior reliability and validity under our HQM framework. We conduct an extensive evaluation of over 10 representative LVLMs, including GPT-4o and Gemini-1.5-Pro, to provide an in-depth analysis of the hallucination issues in existing models. Our benchmark is publicly available at https://github.com/HQHBench/HQHBench.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fca2da71f3dce2f757aef39e561a572f68106603.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "bf54792cf01761a2c51ac3410287797fff665cd4.pdf": {
        "title": "EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models",
        "authors": [
            "Shangyu Xing",
            "Fei Zhao",
            "Zhen Wu",
            "Tuo An",
            "Weihao Chen",
            "Chunhui Li",
            "Jianbing Zhang",
            "Xinyu Dai"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we propose an efficient fine-grained unlearning framework (EFUF), which performs gradient ascent utilizing three tailored losses to eliminate hallucinations without paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/bf54792cf01761a2c51ac3410287797fff665cd4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 8,
        "score": 8.0
    },
    "f05e64c2a096e3762939dfdb7f475724c04a46bd.pdf": {
        "title": "Collaborative decoding of critical tokens for boosting factuality of large language models",
        "authors": [
            "Lifeng Jin",
            "Baolin Peng",
            "Linfeng Song",
            "Haitao Mi",
            "Ye Tian",
            "Dong Yu"
        ],
        "published_date": "2024",
        "abstract": "The most common training pipeline for large language models includes pretraining, finetuning and aligning phases, with their respective resulting models, such as the pretrained model and the finetuned model. Finetuned and aligned models show improved abilities of instruction following and safe generation, however their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hallucination significantly, showcasing the importance of the collaborative decoding framework.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f05e64c2a096e3762939dfdb7f475724c04a46bd.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0,
        "summary": "The most common training pipeline for large language models includes pretraining, finetuning and aligning phases, with their respective resulting models, such as the pretrained model and the finetuned model. Finetuned and aligned models show improved abilities of instruction following and safe generation, however their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hallucination significantly, showcasing the importance of the collaborative decoding framework.",
        "keywords": []
    },
    "5e060f23914aff74d8c7b6973df44e5af8d97db5.pdf": {
        "title": "Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis",
        "authors": [
            "LI DU",
            "Yequan Wang",
            "Xingrun Xing",
            "Yiqun Ya",
            "Xiang Li",
            "Xin Jiang",
            "Xuezhi Fang"
        ],
        "published_date": "2023",
        "abstract": "Although demonstrating superb performance on various NLP tasks, large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs. To measure the level of hallucination of LLMs, previous works first categorize the hallucination according to the phenomenon similarity, then quantify the proportion that model outputs contain hallucinatory contents. However, such hallucination rates could easily be distorted by confounders. Moreover, such hallucination rates could not reflect the reasons for the hallucination, as similar hallucinatory phenomena may originate from different sources. To address these issues, we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of LLMs with a set of risk factors. In this way, we are able to observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor, meanwhile excluding the confounding effect of other factors. Additionally, by recognizing the risk factors according to a taxonomy of model capability, we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5e060f23914aff74d8c7b6973df44e5af8d97db5.pdf",
        "venue": "arXiv.org",
        "citationCount": 15,
        "score": 7.5
    },
    "46e542884db4fc4df605eb28473cff79aec54c99.pdf": {
        "title": "Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling",
        "authors": [
            "Yida Mu",
            "Peizhen Bai",
            "Kalina Bontcheva",
            "Xingyi Song"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/46e542884db4fc4df605eb28473cff79aec54c99.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics.",
        "keywords": []
    },
    "1c7ef42897ad2dced83ab1d58d8fbd4539f87ddc.pdf": {
        "title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models",
        "authors": [
            "Qitan Lv",
            "Jie Wang",
            "Hanzhu Chen",
            "Bin Li",
            "Yongdong Zhang",
            "Feng Wu"
        ],
        "published_date": "2024",
        "abstract": "Generation of plausible but incorrect factual information, often termed hallucination, has attracted significant research interest. Retrieval-augmented language model (RALM) -- which enhances models with up-to-date knowledge -- emerges as a promising method to reduce hallucination. However, existing RALMs may instead exacerbate hallucination when retrieving lengthy contexts. To address this challenge, we propose COFT, a novel \\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on different granularity-level key texts, thereby avoiding getting lost in lengthy contexts. Specifically, COFT consists of three components: \\textit{recaller}, \\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a knowledge graph to extract potential key entities in a given context. Second, \\textit{scorer} measures the importance of each entity by calculating its contextual weight. Finally, \\textit{selector} selects high contextual weight entities with a dynamic threshold algorithm and highlights the corresponding paragraphs, sentences, or words in a coarse-to-fine manner. Extensive experiments on the knowledge hallucination benchmark demonstrate the effectiveness of COFT, leading to a superior performance over $30\\%$ in the F1 score metric. Moreover, COFT also exhibits remarkable versatility across various long-form tasks, such as reading comprehension and question answering.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1c7ef42897ad2dced83ab1d58d8fbd4539f87ddc.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 7,
        "score": 7.0
    },
    "d6da914d0c8021df6622857aba23b794fc7e6a40.pdf": {
        "title": "A Survey of Hallucination in Large Visual Language Models",
        "authors": [
            "Wei Lan",
            "Wenyi Chen",
            "Qingfeng Chen",
            "Shirui Pan",
            "Huiyu Zhou",
            "Yi Pan"
        ],
        "published_date": "2024",
        "abstract": "The Large Visual Language Models (LVLMs) enhances user interaction and enriches user experience by integrating visual modality on the basis of the Large Language Models (LLMs). It has demonstrated their powerful information processing and generation capabilities. However, the existence of hallucinations has limited the potential and practical effectiveness of LVLM in various fields. Although lots of work has been devoted to the issue of hallucination mitigation and correction, there are few reviews to summary this issue. In this survey, we first introduce the background of LVLMs and hallucinations. Then, the structure of LVLMs and main causes of hallucination generation are introduced. Further, we summary recent works on hallucination correction and mitigation. In addition, the available hallucination evaluation benchmarks for LVLMs are presented from judgmental and generative perspectives. Finally, we suggest some future research directions to enhance the dependability and utility of LVLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d6da914d0c8021df6622857aba23b794fc7e6a40.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0
    },
    "57f0d904629955d16bb2b80a5d427e6b1efa6562.pdf": {
        "title": "ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models",
        "authors": [
            "Yeji Park",
            "Deokyeong Lee",
            "Junsuk Choe",
            "Buru Chang"
        ],
        "published_date": "2024",
        "abstract": "Hallucinations in Multimodal Large Language Models (MLLMs) where generated responses fail to accurately reflect the given image pose a significant challenge to their reliability. To address this, we introduce ConVis, a novel training-free contrastive decoding method. ConVis leverages a text-to-image (T2I) generation model to semantically reconstruct the given image from hallucinated captions. By comparing the contrasting probability distributions produced by the original and reconstructed images, ConVis enables MLLMs to capture visual contrastive signals that penalize hallucination generation. Notably, this method operates purely within the decoding process, eliminating the need for additional data or model updates. Our extensive experiments on five popular benchmarks demonstrate that ConVis effectively reduces hallucinations across various MLLMs, highlighting its potential to enhance model reliability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/57f0d904629955d16bb2b80a5d427e6b1efa6562.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 7,
        "score": 7.0
    },
    "ff06bee3d898b3dc3a0364f2bfe506591d7e6d52.pdf": {
        "title": "RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector",
        "authors": [
            "Jun Yu",
            "Yunxiang Zhang",
            "Zerui Zhang",
            "Zhao Yang",
            "Gongpeng Zhao",
            "Fengzhao Sun",
            "Fanrui Zhang",
            "Qingsong Liu",
            "Jianqing Sun",
            "Jiaen Liang",
            "Yaohui Zhang"
        ],
        "published_date": "2024",
        "abstract": "Visual Spatial Description (VSD) is an emerging image-to-text task which aims at generating descriptions of the spatial relationships between given objects in an image. In this paper, we apply Retrieval-Augmented Generation (RAG) technology in guiding Multimodal Large Language Models (MLLMs) for the task of VSD, complemented by an Adaptive Hallucination Corrector, and further fine-tuning them to bolster semantic understanding and overall model efficacy. We found that our approach demonstrated higher accuracy and fewer hallucination errors in both spatial relationship classification and visual language description tasks within the VSD task, achieving state-of-the-art results.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ff06bee3d898b3dc3a0364f2bfe506591d7e6d52.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Visual Spatial Description (VSD) is an emerging image-to-text task which aims at generating descriptions of the spatial relationships between given objects in an image. In this paper, we apply Retrieval-Augmented Generation (RAG) technology in guiding Multimodal Large Language Models (MLLMs) for the task of VSD, complemented by an Adaptive Hallucination Corrector, and further fine-tuning them to bolster semantic understanding and overall model efficacy. We found that our approach demonstrated higher accuracy and fewer hallucination errors in both spatial relationship classification and visual language description tasks within the VSD task, achieving state-of-the-art results.",
        "keywords": []
    },
    "0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92.pdf": {
        "title": "Reference-free Hallucination Detection for Large Vision-Language Models",
        "authors": [
            "Qing Li",
            "Chenyang Lyu",
            "Jiahui Geng",
            "Derui Zhu",
            "Maxim Panov",
            "Fakhri Karray"
        ],
        "published_date": "2024",
        "abstract": "Large vision-language models (LVLMs) have made significant progress in recent years. While LVLMs exhibit excellent ability in language understanding, question answering, and conversations of visual inputs, they are prone to producing hallucinations. While several methods are proposed to evaluate the hallucinations in LVLMs, most are reference-based and depend on external tools, which complicates their practical application. To assess the viability of alternative methods, it is critical to understand whether the reference-free approaches, which do not rely on any external tools, can efficiently detect hallucinations. Therefore, we initiate an exploratory study to demonstrate the effectiveness of different reference-free solutions in detecting hallucinations in LVLMs. In particular, we conduct an extensive study on three kinds of techniques: uncertainty-based, consistency-based, and supervised uncertainty quantification methods on four representative LVLMs across two different tasks. The empirical results show that the reference-free approaches are capable of effectively detecting non-factual responses in LVLMs, with the supervised uncertainty quantification method outperforming the others, achieving the best performance across different settings.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 7,
        "score": 7.0
    },
    "7641749cae1ad30779bfb46948fd47922bcc296a.pdf": {
        "title": "LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop",
        "authors": [
            "Maryam Amirizaniani",
            "Jihan Yao",
            "Adrian Lavergne",
            "Elizabeth Snell Okada",
            "Aman Chadha",
            "Tanya Roosta",
            "Chirag Shah"
        ],
        "published_date": "2024",
        "abstract": "As Large Language Models (LLMs) become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples of such issues include: bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is often warranted, such a process is neither easy nor accessible for most. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose the LLMAuditor framework which is an automatic, and scalable solution, where one uses a different LLM along with human-in-the-loop (HIL). This approach offers verifiability and transparency, while avoiding circular reliance on the same LLM, and increasing scientific rigor and generalizability. Specifically, LLMAuditor includes two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes. A case study using questions from the TruthfulQA dataset demonstrates that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM. This process is enhanced by our structured prompt template with HIL, which not only boosts the reliability of our approach in auditing but also yields the delivery of less hallucinated results. The novelty of our research stems from the development of a comprehensive, general-purpose framework that includes a HIL verified prompt template for auditing responses generated by LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7641749cae1ad30779bfb46948fd47922bcc296a.pdf",
        "venue": "",
        "citationCount": 7,
        "score": 7.0,
        "summary": "As Large Language Models (LLMs) become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples of such issues include: bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is often warranted, such a process is neither easy nor accessible for most. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose the LLMAuditor framework which is an automatic, and scalable solution, where one uses a different LLM along with human-in-the-loop (HIL). This approach offers verifiability and transparency, while avoiding circular reliance on the same LLM, and increasing scientific rigor and generalizability. Specifically, LLMAuditor includes two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes. A case study using questions from the TruthfulQA dataset demonstrates that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM. This process is enhanced by our structured prompt template with HIL, which not only boosts the reliability of our approach in auditing but also yields the delivery of less hallucinated results. The novelty of our research stems from the development of a comprehensive, general-purpose framework that includes a HIL verified prompt template for auditing responses generated by LLMs.",
        "keywords": []
    },
    "7adb88771376c2a31688e3b0395b0550a35b824d.pdf": {
        "title": "Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Wei Cheng",
            "Yanchi Liu",
            "Yiyou Sun",
            "Xuchao Zhang",
            "Mika Oishi",
            "Takao Osaki",
            "Katsushi Matsuda",
            "Jie Ji",
            "Guangji Bai",
            "Liang Zhao",
            "Haifeng Chen"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7adb88771376c2a31688e3b0395b0550a35b824d.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "",
        "keywords": []
    },
    "f41977c497c96c1da2e9e945315e9be6d6ad472e.pdf": {
        "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
        "authors": [
            "Bhaskarjit Sarmah",
            "Dhagash Mehta",
            "Stefano Pasquali",
            "Tianjie Zhu"
        ],
        "published_date": "2023",
        "abstract": "For a financial analyst, the question and answer (Q&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy\u2014transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q&A systems, and empirically demonstrate superiority of our method.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f41977c497c96c1da2e9e945315e9be6d6ad472e.pdf",
        "venue": "International Conference on AI-ML-Systems",
        "citationCount": 13,
        "score": 6.5,
        "summary": "For a financial analyst, the question and answer (Q&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy\u2014transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q&A systems, and empirically demonstrate superiority of our method.",
        "keywords": []
    },
    "20eecb9ead20ffe49a66588a9662336eefb20a54.pdf": {
        "title": "MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models",
        "authors": [
            "Deepak Nathani",
            "David Wang",
            "Liangming Pan",
            "W. Wang"
        ],
        "published_date": "2023",
        "abstract": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/20eecb9ead20ffe49a66588a9662336eefb20a54.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 13,
        "score": 6.5,
        "summary": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.",
        "keywords": []
    },
    "ff0450c78e5fabaa8aab61a368f267bd83753a64.pdf": {
        "title": "Multilingual Hallucination Gaps in Large Language Models",
        "authors": [
            "Cl'ea Chataigner",
            "Afaf Ta\u00efk",
            "G. Farnadi"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are increasingly used as alternatives to traditional search engines given their capacity to generate text that resembles human language. However, this shift is concerning, as LLMs often generate hallucinations, misleading or false information that appears highly credible. In this study, we explore the phenomenon of hallucinations across multiple languages in freeform text generation, focusing on what we call multilingual hallucination gaps. These gaps reflect differences in the frequency of hallucinated answers depending on the prompt and language used. To quantify such hallucinations, we used the FactScore metric and extended its framework to a multilingual setting. We conducted experiments using LLMs from the LLaMA, Qwen, and Aya families, generating biographies in 19 languages and comparing the results to Wikipedia pages. Our results reveal variations in hallucination rates, especially between high and low resource languages, raising important questions about LLM multilingual performance and the challenges in evaluating hallucinations in multilingual freeform text generation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ff0450c78e5fabaa8aab61a368f267bd83753a64.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large language models (LLMs) are increasingly used as alternatives to traditional search engines given their capacity to generate text that resembles human language. However, this shift is concerning, as LLMs often generate hallucinations, misleading or false information that appears highly credible. In this study, we explore the phenomenon of hallucinations across multiple languages in freeform text generation, focusing on what we call multilingual hallucination gaps. These gaps reflect differences in the frequency of hallucinated answers depending on the prompt and language used. To quantify such hallucinations, we used the FactScore metric and extended its framework to a multilingual setting. We conducted experiments using LLMs from the LLaMA, Qwen, and Aya families, generating biographies in 19 languages and comparing the results to Wikipedia pages. Our results reveal variations in hallucination rates, especially between high and low resource languages, raising important questions about LLM multilingual performance and the challenges in evaluating hallucinations in multilingual freeform text generation.",
        "keywords": []
    },
    "e33fceb7cfb825ae3c530de0bf093769169039fc.pdf": {
        "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models",
        "authors": [
            "Q. Liu",
            "Xinlong Chen",
            "Yue Ding",
            "Shizhen Xu",
            "Shu Wu",
            "Liang Wang"
        ],
        "published_date": "2025",
        "abstract": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational overhead, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e33fceb7cfb825ae3c530de0bf093769169039fc.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational overhead, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.",
        "keywords": []
    },
    "6518d3209cec0a1ad277e8aaf153242b3a4233d9.pdf": {
        "title": "Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination",
        "authors": [
            "Jongyoon Song",
            "Sangwon Yu",
            "Sungroh Yoon"
        ],
        "published_date": "2024",
        "abstract": "In this paper, we identify a new category of bias that induces input-conflicting hallucinations, where large language models (LLMs) generate responses inconsistent with the content of the input context. This issue we have termed the false negative problem refers to the phenomenon where LLMs are predisposed to return negative judgments when assessing the correctness of a statement given the context. In experiments involving pairs of statements that contain the same information but have contradictory factual directions, we observe that LLMs exhibit a bias toward false negatives. Specifically, the model presents greater overconfidence when responding with False. Furthermore, we analyze the relationship between the false negative problem and context and query rewriting and observe that both effectively tackle false negatives in LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6518d3209cec0a1ad277e8aaf153242b3a4233d9.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "In this paper, we identify a new category of bias that induces input-conflicting hallucinations, where large language models (LLMs) generate responses inconsistent with the content of the input context. This issue we have termed the false negative problem refers to the phenomenon where LLMs are predisposed to return negative judgments when assessing the correctness of a statement given the context. In experiments involving pairs of statements that contain the same information but have contradictory factual directions, we observe that LLMs exhibit a bias toward false negatives. Specifically, the model presents greater overconfidence when responding with False. Furthermore, we analyze the relationship between the false negative problem and context and query rewriting and observe that both effectively tackle false negatives in LLMs.",
        "keywords": []
    },
    "d45865c981161ad711adf18b0492e959771554e4.pdf": {
        "title": "Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models",
        "authors": [
            "Liam Barkley",
            "Brink van der Merwe"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d45865c981161ad711adf18b0492e959771554e4.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.",
        "keywords": []
    },
    "e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d.pdf": {
        "title": "A Survey of Hallucination Problems Based on Large Language Models",
        "authors": [
            "Xinxin Liu"
        ],
        "published_date": "2024",
        "abstract": "Abstract. Large language models (LLM) have made significant achievements in the field of natural language processing, but the generated text often contains content that is inconsistent with the real world or user input, known as hallucinations. This article investigates the current situation of hallucinations in LLM, including the definition, types, causes, and solutions of hallucinations. Illusions are divided into different types such as factual and faithful, mainly caused by factors such as training data defects, low utilization of facts, and randomness in the decoding process. The phenomenon of hallucinations poses a threat to the reliability of LLM, especially in fields such as healthcare, finance, and law, which may lead to serious consequences. To address this issue, this article investigates methods such as managing training datasets, knowledge editing, and enhancing retrieval generation. Future research should classify and evaluate illusions more finely, explore multimodal strategies, enhance model stability, and integrate human intelligence and artificial intelligence to jointly address challenges, promoting the continuous progress of LLM.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d.pdf",
        "venue": "Applied and Computational Engineering",
        "citationCount": 6,
        "score": 6.0
    },
    "f6d4c76b21539aadc2ca8d813fe631be7149231e.pdf": {
        "title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models",
        "authors": [
            "J. Wu",
            "Tsz Ting Chung",
            "Kai Chen",
            "Dit-Yan Yeung"
        ],
        "published_date": "2024",
        "abstract": "Despite the outstanding performance in vision-language reasoning, Large Vision-Language Models (LVLMs) might generate hallucinated contents that do not exist in the given image. Most existing LVLM hallucination benchmarks are constrained to evaluate the object-related hallucinations. However, the potential hallucination on the relations between two objects, i.e., relation hallucination, still lacks investigation. To remedy that, we design a unified framework to measure the object and relation hallucination in LVLMs simultaneously. The core idea of our framework is to evaluate hallucinations via (object, relation, object) triplets extracted from LVLMs'responses, making it easily generalizable to different vision-language tasks. Based on our framework, we further introduce Tri-HE, a novel Triplet-level Hallucination Evaluation benchmark which can be used to study both object and relation hallucination at the same time. With comprehensive evaluations on Tri-HE, we observe that the relation hallucination issue is even more serious than object hallucination among existing LVLMs, highlighting a previously neglected problem towards reliable LVLMs. Moreover, based on our findings, we design a simple training-free approach that effectively mitigates hallucinations for LVLMs. Our dataset and code for the reproduction of our experiments are available publicly at https://github.com/wujunjie1998/Tri-HE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f6d4c76b21539aadc2ca8d813fe631be7149231e.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 6,
        "score": 6.0
    },
    "2feb4d83da1892db3934fcf406c8beb6cd10ded1.pdf": {
        "title": "VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding",
        "authors": [
            "Jiaqi Wang",
            "Yifei Gao",
            "Jitao Sang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in multimodal task reasoning. However, they often generate responses that appear plausible yet do not accurately reflect the visual content, a phenomenon known as hallucination. Recent approaches have introduced training-free methods to mitigate hallucinations by adjusting the decoding strategy during the inference stage, typically attributing hallucinations to the language model itself. Our analysis, however, reveals that distortions in the visual encoding process significantly affect the model's reasoning capabilities. Specifically, earlier visual layers may retain key features but gradually distort as the information propagates toward the output layer. Building on these insights, we propose a novel hallucination-mitigation method from the visual encoding perspective: \\textbf{V}isu\\textbf{a}l \\textbf{L}ayer Fus\\textbf{i}on Contrastive \\textbf{D}ecoding (\\textbf{VaLiD}). This method utilizes uncertainty to guide the visual layer selection, correcting distortions in the visual encoding process and thereby enhancing the reliability of the generated content. Experimental results demonstrate the effectiveness of VaLiD in mitigating hallucinations across various benchmarks, achieving state-of-the-art performance when compared to baseline methods. Codes are available at \\href{https://github.com/RicardoLuL/VaLiD_LVLMs_hallucinations}{Github}.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2feb4d83da1892db3934fcf406c8beb6cd10ded1.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0
    },
    "4d710532fff7aec8187f68fb2ca90079c40e7004.pdf": {
        "title": "FactAlign: Long-form Factuality Alignment of Large Language Models",
        "authors": [
            "Chao-Wei Huang",
            "Yun-Nung Chen"
        ],
        "published_date": "2024",
        "abstract": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4d710532fff7aec8187f68fb2ca90079c40e7004.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign",
        "keywords": []
    },
    "003463160918704684f812ae8d7b6920d2d15e31.pdf": {
        "title": "A review of faithfulness metrics for hallucination assessment in Large Language Models",
        "authors": [
            "B. Malin",
            "Tatiana Kalganova",
            "Nikoloas Boulgouris"
        ],
        "published_date": "2024",
        "abstract": "This review examines the means with which faithfulness has been evaluated across open-ended summarization, question-answering and machine translation tasks. We find that the use of LLMs as a faithfulness evaluator is commonly the metric that is most highly correlated with human judgement. The means with which other studies have mitigated hallucinations is discussed, with both retrieval augmented generation (RAG) and prompting framework approaches having been linked with superior faithfulness, whilst other recommendations for mitigation are provided. Research into faithfulness is integral to the continued widespread use of LLMs, as unfaithful responses can pose major risks to many areas whereby LLMs would otherwise be suitable. Furthermore, evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which can help in advancing the trust that can be placed within LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/003463160918704684f812ae8d7b6920d2d15e31.pdf",
        "venue": "IEEE Journal on Selected Topics in Signal Processing",
        "citationCount": 5,
        "score": 5.0,
        "summary": "This review examines the means with which faithfulness has been evaluated across open-ended summarization, question-answering and machine translation tasks. We find that the use of LLMs as a faithfulness evaluator is commonly the metric that is most highly correlated with human judgement. The means with which other studies have mitigated hallucinations is discussed, with both retrieval augmented generation (RAG) and prompting framework approaches having been linked with superior faithfulness, whilst other recommendations for mitigation are provided. Research into faithfulness is integral to the continued widespread use of LLMs, as unfaithful responses can pose major risks to many areas whereby LLMs would otherwise be suitable. Furthermore, evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which can help in advancing the trust that can be placed within LLMs.",
        "keywords": []
    },
    "3c3f5af1aee19bf0093c40f35a120744d099723e.pdf": {
        "title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models",
        "authors": [
            "Ziwei Ji",
            "Yuzhe Gu",
            "Wenwei Zhang",
            "Chengqi Lyu",
            "Dahua Lin",
            "Kai Chen"
        ],
        "published_date": "2024",
        "abstract": "Reducing the `$\\textit{hallucination}$' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community. Thus, we present $\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical $\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative Question Answering. Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of ~12k sentence-level annotations for ~4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3c3f5af1aee19bf0093c40f35a120744d099723e.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 5,
        "score": 5.0
    },
    "f0a79fe7765ab253480a0be6d29c889eac19eb3c.pdf": {
        "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models",
        "authors": [
            "Hongbang Yuan",
            "Pengfei Cao",
            "Zhuoran Jin",
            "Yubo Chen",
            "Daojian Zeng",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose FAITH (False premise Attention head constraIining for miTigating Hallucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f0a79fe7765ab253480a0be6d29c889eac19eb3c.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose FAITH (False premise Attention head constraIining for miTigating Hallucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance.",
        "keywords": []
    },
    "9e2037d7d2f8222a7be86d2471eda895c8040ff5.pdf": {
        "title": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models",
        "authors": [
            "Han Qiu",
            "Jiaxing Huang",
            "Peng Gao",
            "Qi Qin",
            "Xiaoqin Zhang",
            "Ling Shao",
            "Shijian Lu"
        ],
        "published_date": "2024",
        "abstract": "Hallucination, a phenomenon where multimodal large language models~(MLLMs) tend to generate textual responses that are plausible but unaligned with the image, has become one major hurdle in various MLLM-related applications. Several benchmarks have been created to gauge the hallucination levels of MLLMs, by either raising discriminative questions about the existence of objects or introducing LLM evaluators to score the generated text from MLLMs. However, the discriminative data largely involve simple questions that are not aligned with real-world text, while the generative data involve LLM evaluators that are computationally intensive and unstable due to their inherent randomness. We propose LongHalQA, an LLM-free hallucination benchmark that comprises 6K long and complex hallucination text. LongHalQA is featured by GPT4V-generated hallucinatory data that are well aligned with real-world scenarios, including object/image descriptions and multi-round conversations with 14/130 words and 189 words, respectively, on average. It introduces two new tasks, hallucination discrimination and hallucination completion, unifying both discriminative and generative evaluations in a single multiple-choice-question form and leading to more reliable and efficient evaluations without the need for LLM evaluators. Further, we propose an advanced pipeline that greatly facilitates the construction of future hallucination benchmarks with long and complex questions and descriptions. Extensive experiments over multiple recent MLLMs reveal various new challenges when they are handling hallucinations with long and complex textual data. Dataset and evaluation code are available at https://github.com/hanqiu-hq/LongHalQA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/9e2037d7d2f8222a7be86d2471eda895c8040ff5.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0
    },
    "910d26adcd83c4ec36f365198f8b2224b14ad6c9.pdf": {
        "title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models",
        "authors": [
            "Shrey Pandit",
            "Jiawei Xu",
            "Junyuan Hong",
            "Zhangyang Wang",
            "Tianlong Chen",
            "Kaidi Xu",
            "Ying Ding"
        ],
        "published_date": "2025",
        "abstract": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting\"hard\"category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a\"not sure\"category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/910d26adcd83c4ec36f365198f8b2224b14ad6c9.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting\"hard\"category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a\"not sure\"category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.",
        "keywords": []
    },
    "b5f56f466c06d10100d8d1aac9e1f979c527b1cf.pdf": {
        "title": "Dual Process Theory for Large Language Models: An overview of using Psychology to address hallucination and reliability issues",
        "authors": [
            "Samuel C. Bellini-Leite"
        ],
        "published_date": "2023",
        "abstract": "State-of-the-art Large Language Models have recently exhibited extraordinary linguistic abilities which have surprisingly extended to reasoning. However, responses that are unreliable, false, or invented are still a frequent issue. It has been argued that scaling up strategies, as in increasing model size or hardware power, might not be enough to resolve the issue. Recent research has implemented Type 2 strategies (such as Chain-of-Thought and Tree-of-Thought), as strategies that mimic Type 2 reasoning, from Dual Process Theory, to interact with Large Language Models for improved results. The current paper reviews these strategies in light of the Predicting and Reflecting Framework for understanding Dual Process Theory and suggests what Psychology, drawing from research in executive functions, thinking disposition and creativity, can further contribute to possible implementations that address hallucination and reliability issues.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b5f56f466c06d10100d8d1aac9e1f979c527b1cf.pdf",
        "venue": "Adaptive Behavior",
        "citationCount": 10,
        "score": 5.0,
        "summary": "State-of-the-art Large Language Models have recently exhibited extraordinary linguistic abilities which have surprisingly extended to reasoning. However, responses that are unreliable, false, or invented are still a frequent issue. It has been argued that scaling up strategies, as in increasing model size or hardware power, might not be enough to resolve the issue. Recent research has implemented Type 2 strategies (such as Chain-of-Thought and Tree-of-Thought), as strategies that mimic Type 2 reasoning, from Dual Process Theory, to interact with Large Language Models for improved results. The current paper reviews these strategies in light of the Predicting and Reflecting Framework for understanding Dual Process Theory and suggests what Psychology, drawing from research in executive functions, thinking disposition and creativity, can further contribute to possible implementations that address hallucination and reliability issues.",
        "keywords": []
    },
    "6b44d0ac2b6d6deeb7f35ef4a7ad77b12b646b9e.pdf": {
        "title": "Large Language Models Are Highly Vulnerable to Adversarial Hallucination Attacks in Clinical Decision Support: A Multi-Model Assurance Analysis",
        "authors": [
            "M. Omar",
            "V. Sorin",
            "J. Collins",
            "D. Reich",
            "R. Freeman",
            "N. Gavin",
            "A. Charney",
            "L. Stump",
            "N. L. Bragazzi",
            "G. Nadkarni",
            "E. Klang"
        ],
        "published_date": "2025",
        "abstract": "Background: Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as hallucinations). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods: We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versions - differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a hallucination. Results: Hallucination rates ranged from 50% to 82% across models and prompting methods. Prompt-based mitigation lowered overall hallucinations (mean across all models) from 66% to 44% (p<0.001). For the best overall performing model, GPT 4o, rates declined from 53% to 23% (p<0.001). Temperature adjustments offered no significant improvement. Short vignettes showed slightly higher odds of hallucination. Conclusions: LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6b44d0ac2b6d6deeb7f35ef4a7ad77b12b646b9e.pdf",
        "venue": "medRxiv",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Background: Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as hallucinations). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods: We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versions - differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a hallucination. Results: Hallucination rates ranged from 50% to 82% across models and prompting methods. Prompt-based mitigation lowered overall hallucinations (mean across all models) from 66% to 44% (p<0.001). For the best overall performing model, GPT 4o, rates declined from 53% to 23% (p<0.001). Temperature adjustments offered no significant improvement. Short vignettes showed slightly higher odds of hallucination. Conclusions: LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
        "keywords": []
    },
    "088a42203bc9a67e14b1bfd5c1fd25a03c126c08.pdf": {
        "title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models",
        "authors": [
            "Yue Chang",
            "Liqiang Jing",
            "Xiaopeng Zhang",
            "Yue Zhang"
        ],
        "published_date": "2024",
        "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current studies either focus on the process of model inference or the results of model generation, but the solutions they design sometimes do not deal appropriately with various types of queries and the hallucinations of the generations about these queries. To accurately deal with various hallucinations, we present a unified framework, Dentist, for hallucination mitigation. The core step is to first classify the queries, then perform different processes of hallucination mitigation based on the classification result, just like a dentist first observes the teeth and then makes a plan. In a simple deployment, Dentist can classify queries as perception or reasoning and easily mitigate potential hallucinations in answers which has been demonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8% improvement in accuracy on Image Quality, a Coarse Perception visual question answering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/088a42203bc9a67e14b1bfd5c1fd25a03c126c08.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 5,
        "score": 5.0
    },
    "9516ad22fbd81875b160c4471ff3f747c4543da1.pdf": {
        "title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models",
        "authors": [
            "Yi-Lun Lee",
            "Yi-Hsuan Tsai",
            "Wei-Chen Chiu"
        ],
        "published_date": "2024",
        "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To address this, recent approaches apply contrastive decoding to calibrate the model's response via contrasting output distributions with original and visually distorted samples, demonstrating promising hallucination mitigation in a training-free manner. However, the potential of changing information in visual inputs is not well-explored, so a deeper investigation into the behaviors of visual contrastive decoding is of great interest. In this paper, we first explore various methods for contrastive decoding to change visual contents, including image downsampling and editing. Downsampling images reduces the detailed textual information while editing yields new contents in images, providing new aspects as visual contrastive samples. To further study benefits by using different contrastive samples, we analyze probability-level metrics, including entropy and distribution distance. Interestingly, the effect of these samples in mitigating hallucinations varies a lot across LVLMs and benchmarks. Based on our analysis, we propose a simple yet effective method to combine contrastive samples, offering a practical solution for applying contrastive decoding across various scenarios. Extensive experiments are conducted to validate the proposed fusion method among different benchmarks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/9516ad22fbd81875b160c4471ff3f747c4543da1.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0,
        "summary": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To address this, recent approaches apply contrastive decoding to calibrate the model's response via contrasting output distributions with original and visually distorted samples, demonstrating promising hallucination mitigation in a training-free manner. However, the potential of changing information in visual inputs is not well-explored, so a deeper investigation into the behaviors of visual contrastive decoding is of great interest. In this paper, we first explore various methods for contrastive decoding to change visual contents, including image downsampling and editing. Downsampling images reduces the detailed textual information while editing yields new contents in images, providing new aspects as visual contrastive samples. To further study benefits by using different contrastive samples, we analyze probability-level metrics, including entropy and distribution distance. Interestingly, the effect of these samples in mitigating hallucinations varies a lot across LVLMs and benchmarks. Based on our analysis, we propose a simple yet effective method to combine contrastive samples, offering a practical solution for applying contrastive decoding across various scenarios. Extensive experiments are conducted to validate the proposed fusion method among different benchmarks.",
        "keywords": []
    },
    "bb3cc013c462ff2bf3dc5be90f731ebf34996f86.pdf": {
        "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models",
        "authors": [
            "Zouying Cao",
            "Yifei Yang",
            "Hai Zhao"
        ],
        "published_date": "2023",
        "abstract": "While Large language models (LLMs) have garnered widespread applications across various domains due to their powerful language understanding and generation capabilities, the detection of non-factual or hallucinatory content generated by LLMs remains scarce. Currently, one significant challenge in hallucination detection is the laborious task of time-consuming and expensive manual annotation of the hallucinatory generation. To address this issue, this paper first introduces a method for automatically constructing model-specific hallucination datasets based on existing fact-checking datasets called AutoHall. Furthermore, we propose a zero-resource and black-box hallucination detection method based on self-contradiction. We conduct experiments towards prevalent open-/closed-source LLMs, achieving superior hallucination detection performance compared to extant baselines. Moreover, our experiments reveal variations in hallucination proportions and types among different models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/bb3cc013c462ff2bf3dc5be90f731ebf34996f86.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 4.5
    },
    "cb7fa7ee3df826628c113ba0c6db1205751d89a3.pdf": {
        "title": "HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "J. Nie",
            "Ji-rong Wen"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/cb7fa7ee3df826628c113ba0c6db1205751d89a3.pdf",
        "venue": "",
        "citationCount": 8,
        "score": 4.0,
        "summary": "",
        "keywords": []
    },
    "13fd528a587196ff6429bfbe1d11d2f89a4036f5.pdf": {
        "title": "User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination",
        "authors": [
            "Chen Zhang"
        ],
        "published_date": "2023",
        "abstract": "In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/13fd528a587196ff6429bfbe1d11d2f89a4036f5.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 3.5,
        "summary": "In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.",
        "keywords": []
    },
    "f63fdbbdf9005245d960ac1912cf4d0805e274a8.pdf": {
        "title": "Minimizing Factual Inconsistency and Hallucination in Large Language Models",
        "authors": [
            "Muneeswaran Irulandi",
            "Shreya Saxena",
            "Siva Prasad",
            "M. V. S. Prakash",
            "Advaith Shankar",
            "V. Varun",
            "Vishal Vaddina",
            "Saisubramaniam Gopalakrishnan"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or\"hallucinations,\"which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f63fdbbdf9005245d960ac1912cf4d0805e274a8.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 3.0,
        "summary": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or\"hallucinations,\"which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models.",
        "keywords": []
    },
    "c910c8f715d8231ed824caff13952d6946de1e59.pdf": {
        "title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models",
        "authors": [
            "Jiawei Chen",
            "Dingkang Yang",
            "Tong Wu",
            "Yue Jiang",
            "Xiaolu Hou",
            "Mingcheng Li",
            "Shunli Wang",
            "Dongling Xiao",
            "Ke Li",
            "Lihua Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations-a significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MediHallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MediHallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work will be released soon.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c910c8f715d8231ed824caff13952d6946de1e59.pdf",
        "venue": "arXiv.org",
        "citationCount": 32,
        "score": 32.0
    },
    "7b181a867f243d83ed0731201b69a82e038feea3.pdf": {
        "title": "Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models",
        "authors": [
            "Ming-Kuan Wu",
            "Jiayi Ji",
            "Oucheng Huang",
            "Jiale Li",
            "Yuhang Wu",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "published_date": "2024",
        "abstract": "The issue of hallucinations is a prevalent concern in existing Large Vision-Language Models (LVLMs). Previous efforts have primarily focused on investigating object hallucinations, which can be easily alleviated by introducing object detectors. However, these efforts neglect hallucinations in inter-object relationships, which is essential for visual comprehension. In this work, we introduce R-Bench, a novel benchmark for evaluating Vision Relationship Hallucination. R-Bench features image-level questions that focus on the existence of relationships and instance-level questions that assess local visual comprehension. We identify three types of relationship co-occurrences that lead to hallucinations: relationship-relationship, subject-relationship, and relationship-object. The visual instruction tuning dataset's long-tail distribution significantly impacts LVLMs' understanding of visual relationships. Furthermore, our analysis reveals that current LVLMs tend to disregard visual content and overly rely on the common sense knowledge of Large Language Models. They also struggle with reasoning about spatial relationships based on contextual information.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7b181a867f243d83ed0731201b69a82e038feea3.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 14,
        "score": 14.0
    },
    "3b1baabcbfd19e2f292863c522de41083814856a.pdf": {
        "title": "Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation",
        "authors": [
            "Derong Xu Xinhang Li",
            "Ziheng Zhang",
            "Zhenxi Lin",
            "Zhihong Zhu",
            "Zhi Zheng",
            "Xian Wu",
            "Xiangyu Zhao",
            "Tong Xu",
            "Enhong Chen"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The Amar framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMs' output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy over its best competitor and a 6.6\\% improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of Amar in improving the reasoning of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3b1baabcbfd19e2f292863c522de41083814856a.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The Amar framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMs' output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy over its best competitor and a 6.6\\% improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of Amar in improving the reasoning of LLMs.",
        "keywords": []
    },
    "fa0d056dd585eeffb4333cb55807d357808f8440.pdf": {
        "title": "Can Large Language Models Play Games? A Case Study of A Self-Play Approach",
        "authors": [
            "Hongyi Guo",
            "Zhihan Liu",
            "Yufeng Zhang",
            "Zhaoran Wang"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theoretically prove that the suboptimality of the estimated value in our proposed method scales with $\\tilde{\\mathcal O}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} + \\epsilon_\\mathrm{critic}\\Bigr)$, where \\(N\\) is the number of simulations, $|\\tilde {\\mathcal A}|$ is the cardinality of the pruned action space by LLM, and $\\epsilon_\\mathrm{pruner}$ and $\\epsilon_\\mathrm{critic}$ quantify the errors incurred by adopting LLMs as action space pruner and value function proxy, respectively. Our experiments in chess and go demonstrate the capability of our method to address challenges beyond the scope of MCTS and improve the performance of the directly application of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fa0d056dd585eeffb4333cb55807d357808f8440.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theoretically prove that the suboptimality of the estimated value in our proposed method scales with $\\tilde{\\mathcal O}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} + \\epsilon_\\mathrm{critic}\\Bigr)$, where \\(N\\) is the number of simulations, $|\\tilde {\\mathcal A}|$ is the cardinality of the pruned action space by LLM, and $\\epsilon_\\mathrm{pruner}$ and $\\epsilon_\\mathrm{critic}$ quantify the errors incurred by adopting LLMs as action space pruner and value function proxy, respectively. Our experiments in chess and go demonstrate the capability of our method to address challenges beyond the scope of MCTS and improve the performance of the directly application of LLMs.",
        "keywords": []
    },
    "0797f2d1366da1f3441ea7d33b2109d7f27d1ad7.pdf": {
        "title": "Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models",
        "authors": [
            "Zikai Xie"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the\"hallucination problem\", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that\"9.11$>$9.9\". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/0797f2d1366da1f3441ea7d33b2109d7f27d1ad7.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the\"hallucination problem\", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that\"9.11$>$9.9\". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.",
        "keywords": []
    },
    "fbca0c2ec5425bbd8dc4898d684c909a58dab1de.pdf": {
        "title": "Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop",
        "authors": [
            "Maryam Amirizaniani",
            "Jihan Yao",
            "Adrian Lavergne",
            "Elizabeth Snell Okada",
            "Aman Chadha",
            "Tanya Roosta",
            "Chirag Shah"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fbca0c2ec5425bbd8dc4898d684c909a58dab1de.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0,
        "summary": "",
        "keywords": []
    },
    "4dc1fbde861d1e97daa0677ac9cfae92b2832589.pdf": {
        "title": "Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention",
        "authors": [
            "Tianyun Yang",
            "Ziniu Li",
            "Juan Cao",
            "Chang Xu"
        ],
        "published_date": "2025",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4dc1fbde861d1e97daa0677ac9cfae92b2832589.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 8,
        "score": 8.0,
        "summary": "",
        "keywords": []
    },
    "0422493dc3a70816bb5d327c4c67094f64a78c98.pdf": {
        "title": "Towards Trustable Language Models: Investigating Information Quality of Large Language Models",
        "authors": [
            "Rick Rejeleene",
            "Xiaowei Xu",
            "John R. Talburt"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data. Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality. Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM. Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information. Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity. In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/0422493dc3a70816bb5d327c4c67094f64a78c98.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "f363c38f2a5d3ed8697a72d3dd014d228bfda91a.pdf": {
        "title": "CodeMirage: Hallucinations in Code Generated by Large Language Models",
        "authors": [
            "Vibhor Agarwal",
            "Yulong Pei",
            "Salwa Alamir",
            "Xiaomo Liu"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have shown promising potentials in program generation and no-code automation. However, LLMs are prone to generate hallucinations, i.e., they generate text which sounds plausible but is incorrect. Although there has been a recent surge in research on LLM hallucinations for text generation, similar hallucination phenomenon can happen in code generation. Sometimes the generated code can have syntactical or logical errors as well as more advanced issues like security vulnerabilities, memory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in code generation and development in general, it becomes imperative to investigate hallucinations in code generation. To the best of our knowledge, this is the first attempt at studying hallucinations in the code generated by LLMs. We start by introducing the code hallucination definition and a comprehensive taxonomy of code hallucination types. We propose the first benchmark CodeMirage dataset for code hallucinations. The benchmark contains 1,137 GPT-3.5 generated hallucinated code snippets for Python programming problems from two base datasets - HumanEval and MBPP. We then propose the methodology for code hallucination detection and experiment with open source LLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using one-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and gives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset. Towards the end, we discuss various mitigation strategies for code hallucinations and conclude our work.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f363c38f2a5d3ed8697a72d3dd014d228bfda91a.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Large Language Models (LLMs) have shown promising potentials in program generation and no-code automation. However, LLMs are prone to generate hallucinations, i.e., they generate text which sounds plausible but is incorrect. Although there has been a recent surge in research on LLM hallucinations for text generation, similar hallucination phenomenon can happen in code generation. Sometimes the generated code can have syntactical or logical errors as well as more advanced issues like security vulnerabilities, memory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in code generation and development in general, it becomes imperative to investigate hallucinations in code generation. To the best of our knowledge, this is the first attempt at studying hallucinations in the code generated by LLMs. We start by introducing the code hallucination definition and a comprehensive taxonomy of code hallucination types. We propose the first benchmark CodeMirage dataset for code hallucinations. The benchmark contains 1,137 GPT-3.5 generated hallucinated code snippets for Python programming problems from two base datasets - HumanEval and MBPP. We then propose the methodology for code hallucination detection and experiment with open source LLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using one-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and gives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset. Towards the end, we discuss various mitigation strategies for code hallucinations and conclude our work.",
        "keywords": []
    },
    "15aaf20d02a1e26be9106e66d065fd1ca5600e29.pdf": {
        "title": "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models",
        "authors": [
            "Junho Kim",
            "Yeonju Kim",
            "Yonghyun Ro"
        ],
        "published_date": "2024",
        "abstract": "This paper presents a way of enhancing the reliability of Large Multi-modal Models (LMMs) in addressing hallucination, where the models generate cross-modal inconsistent responses. Without additional training, we propose Counterfactual Inception, a novel method that implants counterfactual thinking into LMMs using self-generated counterfactual keywords. Our method is grounded in the concept of counterfactual thinking, a cognitive process where human considers alternative realities, enabling more extensive context exploration. Bridging the human cognition mechanism into LMMs, we aim for the models to engage with and generate responses that span a wider contextual scene understanding, mitigating hallucinatory outputs. We further introduce Plausibility Verification Process (PVP), a simple yet robust keyword constraint that effectively filters out sub-optimal keywords to enable the consistent triggering of counterfactual thinking in the model responses. Comprehensive analyses across various LMMs, including both open-source and proprietary models, corroborate that counterfactual thinking significantly reduces hallucination and helps to broaden contextual understanding based on true visual clues.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/15aaf20d02a1e26be9106e66d065fd1ca5600e29.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 8,
        "score": 8.0
    },
    "43210579b1ff7707afbd5d1ed045cc56ba52e938.pdf": {
        "title": "Chaos with Keywords: Exposing Large Language Models Sycophantic Hallucination to Misleading Keywords and Evaluating Defense Strategies",
        "authors": [
            "Aswin Rrv",
            "Nemika Tyagi",
            "Md Nayem Uddin",
            "Neeraj Varshney",
            "Chitta Baral"
        ],
        "published_date": "2024",
        "abstract": "This study explores the sycophantic tendencies of Large Language Models (LLMs), where these models tend to provide answers that match what users want to hear, even if they are not entirely correct. The motivation behind this exploration stems from the common behavior observed in individuals searching the internet for facts with partial or misleading knowledge. Similar to using web search engines, users may recall fragments of misleading keywords and submit them to an LLM, hoping for a comprehensive response. Our empirical analysis of several LLMs shows the potential danger of these models amplifying misinformation when presented with misleading keywords. Additionally, we thoroughly assess four existing hallucination mitigation strategies to reduce LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of these strategies for generating factually correct statements. Furthermore, our analyses delve into knowledge-probing experiments on factual keywords and different categories of sycophancy mitigation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/43210579b1ff7707afbd5d1ed045cc56ba52e938.pdf",
        "venue": "",
        "citationCount": 7,
        "score": 7.0,
        "summary": "This study explores the sycophantic tendencies of Large Language Models (LLMs), where these models tend to provide answers that match what users want to hear, even if they are not entirely correct. The motivation behind this exploration stems from the common behavior observed in individuals searching the internet for facts with partial or misleading knowledge. Similar to using web search engines, users may recall fragments of misleading keywords and submit them to an LLM, hoping for a comprehensive response. Our empirical analysis of several LLMs shows the potential danger of these models amplifying misinformation when presented with misleading keywords. Additionally, we thoroughly assess four existing hallucination mitigation strategies to reduce LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of these strategies for generating factually correct statements. Furthermore, our analyses delve into knowledge-probing experiments on factual keywords and different categories of sycophancy mitigation.",
        "keywords": []
    },
    "b877f5076c617a948081e12e08809e6c6b84b468.pdf": {
        "title": "ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models",
        "authors": [
            "Jio Oh",
            "Soyeon Kim",
            "Junseok Seo",
            "Jindong Wang",
            "Ruochen Xu",
            "Xing Xie",
            "Steven Euijong Whang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have achieved unprecedented performances in various applications, yet evaluating them is still challenging. Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity. We contend that utilizing existing relational databases based on the entity-relationship (ER) model is a promising approach for constructing benchmarks as they contain structured knowledge that can be used to question LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational databases have integrity constraints that can be used to better construct complex in-depth questions and verify answers: (1) functional dependencies can be used to pinpoint critical keywords that an LLM must know to properly answer a given question containing certain attribute values; and (2) foreign key constraints can be used to join relations and construct multi-hop questions, which can be arbitrarily long and used to debug intermediate answers. We thus propose ERBench, which uses these integrity constraints to convert any database into an LLM benchmark. ERBench supports continuous evaluation as databases change, multimodal questions, and various prompt engineering techniques. In our experiments, we construct LLM benchmarks using databases of multiple domains and make an extensive comparison of contemporary LLMs. We show how ERBench can properly evaluate any LLM by not only checking for answer correctness, but also effectively verifying the rationales by looking for the right keywords.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b877f5076c617a948081e12e08809e6c6b84b468.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 7,
        "score": 7.0
    },
    "1154343478e423fecb12501cf02208499cd57846.pdf": {
        "title": "Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness",
        "authors": [
            "Mingchen Li",
            "Zaifu Zhan",
            "Han Yang",
            "Yongkang Xiao",
            "Jiatan Huang",
            "Rui Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1154343478e423fecb12501cf02208499cd57846.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.",
        "keywords": []
    },
    "691f111348f3b19163e62a208de9803280205ed8.pdf": {
        "title": "SeCor: Aligning Semantic and Collaborative Representations by Large Language Models for Next-Point-of-Interest Recommendations",
        "authors": [
            "Shirui Wang",
            "Bohan Xie",
            "Ling Ding",
            "Xiaoying Gao",
            "Jianting Chen",
            "Yang Xiang"
        ],
        "published_date": "2024",
        "abstract": "The widespread adoption of location-based applications has created a growing demand for point-of-interest (POI) recommendation, which aims to predict a user\u2019s next POI based on their historical check-in data and current location. However, existing methods often struggle to capture the intricate relationships within check-in data. This is largely due to their limitations in representing temporal and spatial information and underutilizing rich semantic features. While large language models (LLMs) offer powerful semantic comprehension to solve them, they are limited by hallucination and the inability to incorporate global collaborative information. To address these issues, we propose a novel method SeCor, which treats POI recommendation as a multi-modal task and integrates semantic and collaborative representations to form an efficient hybrid encoding. SeCor first employs a basic collaborative filtering model to mine interaction features. These embeddings, as one modal information, are fed into LLM to align with semantic representation, leading to efficient hybrid embeddings. To mitigate the hallucination, SeCor recommends based on the hybrid embeddings rather than directly using the LLM\u2019s output text. Extensive experiments on three public real-world datasets show that SeCor outperforms all baselines, achieving improved recommendation performance by effectively integrating collaborative and semantic information through LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/691f111348f3b19163e62a208de9803280205ed8.pdf",
        "venue": "ACM Conference on Recommender Systems",
        "citationCount": 7,
        "score": 7.0,
        "summary": "The widespread adoption of location-based applications has created a growing demand for point-of-interest (POI) recommendation, which aims to predict a user\u2019s next POI based on their historical check-in data and current location. However, existing methods often struggle to capture the intricate relationships within check-in data. This is largely due to their limitations in representing temporal and spatial information and underutilizing rich semantic features. While large language models (LLMs) offer powerful semantic comprehension to solve them, they are limited by hallucination and the inability to incorporate global collaborative information. To address these issues, we propose a novel method SeCor, which treats POI recommendation as a multi-modal task and integrates semantic and collaborative representations to form an efficient hybrid encoding. SeCor first employs a basic collaborative filtering model to mine interaction features. These embeddings, as one modal information, are fed into LLM to align with semantic representation, leading to efficient hybrid embeddings. To mitigate the hallucination, SeCor recommends based on the hybrid embeddings rather than directly using the LLM\u2019s output text. Extensive experiments on three public real-world datasets show that SeCor outperforms all baselines, achieving improved recommendation performance by effectively integrating collaborative and semantic information through LLMs.",
        "keywords": []
    },
    "c0082580c4b9e5c6c96cf06f1be67c0cbbafb753.pdf": {
        "title": "A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models",
        "authors": [
            "S. Hegselmann",
            "Zejiang Shen",
            "Florian Gierse",
            "Monica Agrawal",
            "David Sontag",
            "Xiaoyi Jiang"
        ],
        "published_date": "2024",
        "abstract": "Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we release (i) a rigorous labeling protocol for errors in medical texts and (ii) a publicly available dataset of annotated hallucinations in 100 doctor-written and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. We observe a similar effect on GPT-4 (0.70 to 0.40), when the few-shot examples are hallucination-free. We also conduct a qualitative evaluation using hallucination-free and improved training data. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test GPT-4 for automatic hallucination detection, which clearly outperforms common baselines.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c0082580c4b9e5c6c96cf06f1be67c0cbbafb753.pdf",
        "venue": "ACM Conference on Health, Inference, and Learning",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we release (i) a rigorous labeling protocol for errors in medical texts and (ii) a publicly available dataset of annotated hallucinations in 100 doctor-written and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. We observe a similar effect on GPT-4 (0.70 to 0.40), when the few-shot examples are hallucination-free. We also conduct a qualitative evaluation using hallucination-free and improved training data. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test GPT-4 for automatic hallucination detection, which clearly outperforms common baselines.",
        "keywords": []
    },
    "ded732209b0ba8a6704cc62ab8197a898b57f833.pdf": {
        "title": "EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models",
        "authors": [
            "Jun Gao",
            "Huan Zhao",
            "Wei Wang",
            "Changlong Yu",
            "Ruifeng Xu"
        ],
        "published_date": "2024",
        "abstract": "In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing model size leads to higher accuracy, maintaining the ability to generalize is essential to avoid overfitting.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ded732209b0ba8a6704cc62ab8197a898b57f833.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing model size leads to higher accuracy, maintaining the ability to generalize is essential to avoid overfitting.",
        "keywords": []
    },
    "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d.pdf": {
        "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
        "authors": [
            "Yao-Hung Tsai",
            "Walter Talbott",
            "Jian Zhang"
        ],
        "published_date": "2024",
        "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6d3ae6d6b312b659b3a14ae3f3e86a36db63200d.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
        "keywords": []
    },
    "7bcd5c0b17560ee560aec903ea42487a1a54e5d9.pdf": {
        "title": "BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models",
        "authors": [
            "Moon Ye-Bin",
            "Nam Hyeon-Woo",
            "Wonseok Choi",
            "Tae-Hyun Oh"
        ],
        "published_date": "2024",
        "abstract": "Vision language models (VLMs) perceive the world through a combination of a visual encoder and a large language model (LLM). The visual encoder, pre-trained on large-scale vision-text datasets, provides zero-shot generalization to visual data, and the LLM endows its high reasoning ability to VLMs. It leads VLMs to achieve high performance on wide benchmarks without fine-tuning, exhibiting zero or few-shot capability. However, recent studies show that VLMs are vulnerable to hallucination. This undesirable behavior degrades reliability and credibility, thereby making users unable to fully trust the output from VLMs. To enhance trustworthiness and better tackle the hallucination of VLMs, we curate a new evaluation dataset, called the BEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID). Unlike prior works that focus only on constructing questions and answers, the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes. This allows us to clearly assess whether VLMs correctly understand a given scene by observing the ability to perceive changes. We also visualize image-wise object relationship by virtue of our two-axis view: vision and text. Upon evaluating VLMs with our dataset, we observed that our metrics reveal different aspects of VLM hallucination that have not been reported before. Project page: \\url{https://beafbench.github.io/}",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7bcd5c0b17560ee560aec903ea42487a1a54e5d9.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 7,
        "score": 7.0
    },
    "e059a20ae41aa32837030fd6e4392b8217243a2c.pdf": {
        "title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG",
        "authors": [
            "Xinxi Chen",
            "Li Wang",
            "Wei Wu",
            "Qizhi Tang",
            "Yiyao Liu"
        ],
        "published_date": "2024",
        "abstract": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e059a20ae41aa32837030fd6e4392b8217243a2c.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
        "keywords": []
    },
    "55b193ab7967fb20a8a05f878ac85bf48c9fc615.pdf": {
        "title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
        "authors": [
            "MingShan Liu",
            "Shi Bo",
            "Jialing Fang"
        ],
        "published_date": "2025",
        "abstract": "Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/55b193ab7967fb20a8a05f878ac85bf48c9fc615.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.",
        "keywords": []
    },
    "ea32e8511cde4a1b852d8c003e0ec64bdf64b0d8.pdf": {
        "title": "DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models",
        "authors": [
            "Taolin Zhang",
            "Qizhou Chen",
            "Dongyang Li",
            "Chengyu Wang",
            "Xiaofeng He",
            "Longtao Huang",
            "Hui Xue",
            "Junyuan Huang"
        ],
        "published_date": "2024",
        "abstract": "Recently, while large language models (LLMs) have demonstrated impressive results, they still suffer from hallucination, i.e., the generation of false information. Model editing is the task of fixing factual mistakes in LLMs; yet, most previous works treat it as a one-time task, paying little attention to ever-emerging mistakes generated by LLMs. We address the task of sequential model editing (SME) that aims to rectify mistakes continuously. A Dynamic Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic interaction among the factual knowledge within the entire sequence, preventing catastrophic forgetting during the editing process of multiple knowledge triples. Specifically, (1) for semantic fusion within a relation triple, we aggregate the intra-editing attention flow into auto-regressive self-attention with token-level granularity in LLMs. We further leverage multi-layer diagonal inter-editing attention flow to update the weighted representations of the entire sequence-level granularity. (2) Considering that auxiliary parameters are required to store the knowledge for sequential editing, we construct a new dataset named \\textbf{DAFSet}, fulfilling recent, popular, long-tail and robust properties to enhance the generality of sequential editing. Experiments show DAFNet significantly outperforms strong baselines in single-turn and sequential editing. The usage of DAFSet also consistently improves the performance of other auxiliary network-based methods in various scenarios",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ea32e8511cde4a1b852d8c003e0ec64bdf64b0d8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Recently, while large language models (LLMs) have demonstrated impressive results, they still suffer from hallucination, i.e., the generation of false information. Model editing is the task of fixing factual mistakes in LLMs; yet, most previous works treat it as a one-time task, paying little attention to ever-emerging mistakes generated by LLMs. We address the task of sequential model editing (SME) that aims to rectify mistakes continuously. A Dynamic Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic interaction among the factual knowledge within the entire sequence, preventing catastrophic forgetting during the editing process of multiple knowledge triples. Specifically, (1) for semantic fusion within a relation triple, we aggregate the intra-editing attention flow into auto-regressive self-attention with token-level granularity in LLMs. We further leverage multi-layer diagonal inter-editing attention flow to update the weighted representations of the entire sequence-level granularity. (2) Considering that auxiliary parameters are required to store the knowledge for sequential editing, we construct a new dataset named \\textbf{DAFSet}, fulfilling recent, popular, long-tail and robust properties to enhance the generality of sequential editing. Experiments show DAFNet significantly outperforms strong baselines in single-turn and sequential editing. The usage of DAFSet also consistently improves the performance of other auxiliary network-based methods in various scenarios",
        "keywords": []
    },
    "b7aa5af5bf96ee003543d5ff7dcfc3d9a46d43bb.pdf": {
        "title": "Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks",
        "authors": [
            "Yuhang Guo",
            "Zhiyu Wan"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b7aa5af5bf96ee003543d5ff7dcfc3d9a46d43bb.pdf",
        "venue": "IEEE International Conference on Healthcare Informatics",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application.",
        "keywords": []
    },
    "a2f44fc0f0c24fd4ab848f01a770a68dfa114f62.pdf": {
        "title": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach",
        "authors": [
            "Shijian Deng",
            "Wentian Zhao",
            "Yu-Jhe Li",
            "Kun Wan",
            "Daniel Miranda",
            "Ajinkya Kale",
            "Yapeng Tian"
        ],
        "published_date": "2024",
        "abstract": "Self-improvement in multimodal large language models (MLLMs) is crucial for enhancing their reliability and robustness. However, current methods often rely heavily on MLLMs themselves as judges, leading to high computational costs and potential pitfalls like reward hacking and model collapse. This paper introduces a novel, model-level judge-free self-improvement framework. Our approach employs a controlled feedback mechanism while eliminating the need for MLLMs in the verification loop. We generate preference learning pairs using a controllable hallucination mechanism and optimize data quality by leveraging lightweight, contrastive language-image encoders to evaluate and reverse pairs when necessary. Evaluations across public benchmarks and our newly introduced IC dataset designed to challenge hallucination control demonstrate that our model outperforms conventional techniques. We achieve superior precision and recall with significantly lower computational demands. This method offers an efficient pathway to scalable self-improvement in MLLMs, balancing performance gains with reduced resource requirements.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/a2f44fc0f0c24fd4ab848f01a770a68dfa114f62.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0
    },
    "8767dcddfb856db4bfa1e150470fc99f51f43835.pdf": {
        "title": "KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation",
        "authors": [
            "Weiqing Luo",
            "Chonggang Song",
            "Lingling Yi",
            "Gong Cheng"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8767dcddfb856db4bfa1e150470fc99f51f43835.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "",
        "keywords": []
    },
    "6bf4e95e63df023c81458ec60a8324788535a2f4.pdf": {
        "title": "Beyond Probabilities: Unveiling the Delicate Dance of Large Language Models (LLMs) and AI-Hallucination",
        "authors": [
            "Oussama H. Hamid"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs), like OpenAI\u2019s ChatGPT and Google\u2019s Gemini, operate as probabilistic models, leveraging their ability to generalise and discern intricate patterns within data. By assigning probabilities to different tokens based on patterns learned during extensive training on large datasets, these models can generate a wide range of contextually appropriate responses, spanning from textual scripts to auditory and visual outputs (both static and moving images). However, the inherent probabilistic nature of LLMs introduces a notable challenge, leading to the phenomenon known in the field of artificial intelligence as \u2018AI-hallucination,\u2019 where the model may produce responses that sound plausible but are factually incorrect or nonsensical. Despite being perceived as a drawback, we posit in this paper that AI-hallucinations can be reframed as a distinctive feature of LLMs rather than a mere limitation. Our argument stems from the understanding that attempts to mitigate the harms caused by AI-hallucinations might inadvertently lead to increased model rigidity. This delicate balance between minimising harm and preserving the model\u2019s flexibility is a central theme in our discussion. Furthermore, we revisit the concept of \u2018context,\u2019 contending that a complete definition goes beyond the mere description of circumstances, environment, or surrounding facts. We assert that context is enriched by a conscious embodiment, involving the choice or refusal of action (considering all associate ethical implications) among a set of available options.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6bf4e95e63df023c81458ec60a8324788535a2f4.pdf",
        "venue": "Conference on Cognitive and Computational Aspects of Situation Management",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Large language models (LLMs), like OpenAI\u2019s ChatGPT and Google\u2019s Gemini, operate as probabilistic models, leveraging their ability to generalise and discern intricate patterns within data. By assigning probabilities to different tokens based on patterns learned during extensive training on large datasets, these models can generate a wide range of contextually appropriate responses, spanning from textual scripts to auditory and visual outputs (both static and moving images). However, the inherent probabilistic nature of LLMs introduces a notable challenge, leading to the phenomenon known in the field of artificial intelligence as \u2018AI-hallucination,\u2019 where the model may produce responses that sound plausible but are factually incorrect or nonsensical. Despite being perceived as a drawback, we posit in this paper that AI-hallucinations can be reframed as a distinctive feature of LLMs rather than a mere limitation. Our argument stems from the understanding that attempts to mitigate the harms caused by AI-hallucinations might inadvertently lead to increased model rigidity. This delicate balance between minimising harm and preserving the model\u2019s flexibility is a central theme in our discussion. Furthermore, we revisit the concept of \u2018context,\u2019 contending that a complete definition goes beyond the mere description of circumstances, environment, or surrounding facts. We assert that context is enriched by a conscious embodiment, involving the choice or refusal of action (considering all associate ethical implications) among a set of available options.",
        "keywords": []
    },
    "8705389fd69be2a0cecd2242287a08e8280f2c52.pdf": {
        "title": "KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering",
        "authors": [
            "Xinxin Zheng",
            "Feihu Che",
            "Jinyang Wu",
            "Shuai Zhang",
            "Shuai Nie",
            "Kang Liu",
            "Jianhua Tao"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as TriviaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8705389fd69be2a0cecd2242287a08e8280f2c52.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as TriviaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results.",
        "keywords": []
    },
    "21967f943a189a9171f9f880182894acff5b87a4.pdf": {
        "title": "Tutorial Proposal: Hallucination in Large Language Models",
        "authors": [
            "Vipula Rawte",
            "Aman Chadha",
            "Amit P. Sheth",
            "Amitava Das"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/21967f943a189a9171f9f880182894acff5b87a4.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 4,
        "score": 4.0,
        "summary": "",
        "keywords": []
    },
    "5e5336d0284e94cd835c40f931e0d379e21b464d.pdf": {
        "title": "A review of methods for alleviating hallucination issues in large language models",
        "authors": [
            "Zhibo Yin"
        ],
        "published_date": "2024",
        "abstract": "Large language models have demonstrated impressive language processing capabilities in recent years, exhibiting unparalleled excellence in the field of natural language processing. However, the generated text sometimes contains hallucinations, which is the text that contradicts the knowledge in the real world, the context, and the user input. This problem is mainly due to the inherent limitations of the method itself in aspects such as data quality, the model training process, and the model generation process. The issue of hallucinations has always been closely monitored by the academic community. It is widely recognized that its potential consequences should not be underestimated. This paper systematically summarizes the research on the causes of hallucinations in large language models, and introduces mainstream classification methods as well as current measures to address the issue of hallucinations. To be more specific, the article divides the causes of hallucinations into two categories: 1. hallucinations come from the training process and 2. hallucinations come from the generation process. Also, 4 typical types of causes for the former and 5 typical types of causes for the latter are provided. Simultaneously, a detailed discussion of 16 methods to mitigate hallucinations that arise in the generation process is offered. Finally, this paper also discusses inherent flaws that may exist in large language models, aiming to help people gain a more comprehensive understanding and research into hallucinations and large language models. In general, the text details about the hallucinations that exist in the large language model. Meanwhile, according to the previous research, it is pointed out that it is difficult for the large language model based on autoregressive method for token prediction to avoid the hallucinations completely.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5e5336d0284e94cd835c40f931e0d379e21b464d.pdf",
        "venue": "Applied and Computational Engineering",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Large language models have demonstrated impressive language processing capabilities in recent years, exhibiting unparalleled excellence in the field of natural language processing. However, the generated text sometimes contains hallucinations, which is the text that contradicts the knowledge in the real world, the context, and the user input. This problem is mainly due to the inherent limitations of the method itself in aspects such as data quality, the model training process, and the model generation process. The issue of hallucinations has always been closely monitored by the academic community. It is widely recognized that its potential consequences should not be underestimated. This paper systematically summarizes the research on the causes of hallucinations in large language models, and introduces mainstream classification methods as well as current measures to address the issue of hallucinations. To be more specific, the article divides the causes of hallucinations into two categories: 1. hallucinations come from the training process and 2. hallucinations come from the generation process. Also, 4 typical types of causes for the former and 5 typical types of causes for the latter are provided. Simultaneously, a detailed discussion of 16 methods to mitigate hallucinations that arise in the generation process is offered. Finally, this paper also discusses inherent flaws that may exist in large language models, aiming to help people gain a more comprehensive understanding and research into hallucinations and large language models. In general, the text details about the hallucinations that exist in the large language model. Meanwhile, according to the previous research, it is pointed out that it is difficult for the large language model based on autoregressive method for token prediction to avoid the hallucinations completely.",
        "keywords": []
    },
    "1d1af14aa70f86b013e616cfd07fa8a164652d84.pdf": {
        "title": "Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization",
        "authors": [
            "Zilu Tang",
            "Rajen Chatterjee",
            "Sarthak Garg"
        ],
        "published_date": "2025",
        "abstract": "Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user's trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve post-hoc mitigation - detecting hallucinated translations and re-translating them. While effective, this approach introduces additional complexity in deploying extra tools in production and also increases latency. To address these limitations, we propose a method that intrinsically learns to mitigate hallucinations during the model training phase. Specifically, we introduce a data creation framework to generate hallucination focused preference datasets. Fine-tuning LLMs on these preference datasets reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality. In a zero-shot setting our approach reduces hallucinations by 89% on an average across three unseen target languages.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1d1af14aa70f86b013e616cfd07fa8a164652d84.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user's trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve post-hoc mitigation - detecting hallucinated translations and re-translating them. While effective, this approach introduces additional complexity in deploying extra tools in production and also increases latency. To address these limitations, we propose a method that intrinsically learns to mitigate hallucinations during the model training phase. Specifically, we introduce a data creation framework to generate hallucination focused preference datasets. Fine-tuning LLMs on these preference datasets reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality. In a zero-shot setting our approach reduces hallucinations by 89% on an average across three unseen target languages.",
        "keywords": []
    },
    "6073b9a4856d726c270f03ebee54ea7658f16ec1.pdf": {
        "title": "Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models",
        "authors": [
            "Souvik Das",
            "Lifeng Jin",
            "Linfeng Song",
            "Haitao Mi",
            "Baolin Peng",
            "Dong Yu"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination -- generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality during inference by leveraging LLMs' hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting early-exit distributions from a lower layer with the final layer to exploit information related to factuality within the model forward procedure. However, such methods often assume the final layer is the most reliable and the lower layer selection process depends on it. In this work, we first propose extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins. Analyses show different kinds of prompts respond to different selection strategies.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6073b9a4856d726c270f03ebee54ea7658f16ec1.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination -- generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality during inference by leveraging LLMs' hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting early-exit distributions from a lower layer with the final layer to exploit information related to factuality within the model forward procedure. However, such methods often assume the final layer is the most reliable and the lower layer selection process depends on it. In this work, we first propose extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins. Analyses show different kinds of prompts respond to different selection strategies.",
        "keywords": []
    },
    "4c4a3328153e85749c690e68acc13b42a7225e50.pdf": {
        "title": "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models",
        "authors": [
            "Yuxiang Zhang",
            "Jing Chen",
            "Junjie Wang",
            "Yaxin Liu",
            "Cheng Yang",
            "Chufan Shi",
            "Xinyu Zhu",
            "Zihao Lin",
            "Hanwen Wan",
            "Yujiu Yang",
            "Tetsuya Sakai",
            "Tian Feng",
            "Hayato Yamana"
        ],
        "published_date": "2024",
        "abstract": "Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLM\u2019s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4c4a3328153e85749c690e68acc13b42a7225e50.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLM\u2019s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.",
        "keywords": []
    },
    "83d81e31f5c32f6989d98be1133adfc08db094ce.pdf": {
        "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Kedi Chen",
            "Qin Chen",
            "Jie Zhou",
            "Yishen He",
            "Liang He"
        ],
        "published_date": "2024",
        "abstract": "Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-machine interaction scenarios. Finally, professional scholars annotate all the samples in the dataset. DiaHalu covers four common multi-turn dialogue domains and five hallucination subtypes, extended from factuality and faithfulness hallucination. Experiments through some well-known LLMs and detection methods on the dataset show that DiaHalu is a challenging benchmark, holding significant value for further research.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/83d81e31f5c32f6989d98be1133adfc08db094ce.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 4,
        "score": 4.0
    },
    "4661b7e8f6bb4f0cc1d4a767a92534f1def344b8.pdf": {
        "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models",
        "authors": [
            "Yongheng Zhang",
            "Xu Liu",
            "Ruoxi Zhou",
            "Qiguang Chen",
            "Hao Fei",
            "Wenpeng Lu",
            "Libo Qin"
        ],
        "published_date": "2025",
        "abstract": "Investigating hallucination issues in large language models (LLMs) within cross-lingual and cross-modal scenarios can greatly advance the large-scale deployment in real-world applications. Nevertheless, the current studies are limited to a single scenario, either cross-lingual or cross-modal, leaving a gap in the exploration of hallucinations in the joint cross-lingual and cross-modal scenarios. Motivated by this, we introduce a novel joint Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this gap. Specifically, CCHall simultaneously incorporates both cross-lingual and cross-modal hallucination scenarios, which can be used to assess the cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a comprehensive evaluation on CCHall, exploring both mainstream open-source and closed-source LLMs. The experimental results highlight that current LLMs still struggle with CCHall. We hope CCHall can serve as a valuable resource to assess LLMs in joint cross-lingual and cross-modal scenarios.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4661b7e8f6bb4f0cc1d4a767a92534f1def344b8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 4,
        "score": 4.0
    },
    "c7714dc70eb508a0b1859b7b1a5af552439b973f.pdf": {
        "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
        "authors": [
            "Yining Wang",
            "Mi Zhang",
            "Junjie Sun",
            "Chenyue Wang",
            "Min Yang",
            "Hui Xue",
            "Jialing Tao",
            "Ranjie Duan",
            "Jiexi Liu"
        ],
        "published_date": "2025",
        "abstract": "Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content. In this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process. We propose a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications. Distinguished from previous adversarial methods that rely on fixed patterns, our approach generates dynamic, effective, and highly transferable visual adversarial inputs, without sacrificing the quality of model responses. Comprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our attack in compromising black-box MLLMs even with extensive mitigating mechanisms, as well as the promising results against cutting-edge commercial APIs, such as GPT-4o and Gemini 1.5. Our code is available at https://huggingface.co/RachelHGF/Mirage-in-the-Eyes.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c7714dc70eb508a0b1859b7b1a5af552439b973f.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0
    },
    "3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b.pdf": {
        "title": "Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy",
        "authors": [
            "Benedict Aaron Tjandra",
            "Muhammed Razzak",
            "Jannik Kossen",
            "Kunal Handa",
            "Yarin Gal"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are known to hallucinate, whereby they generate plausible but inaccurate text. This phenomenon poses significant risks in critical applications, such as medicine or law, necessitating robust hallucination mitigation strategies. While recent works have proposed fine-tuning methods to teach LLMs to abstain from answering questions beyond their knowledge or capabilities, these methods rely on the existence of ground-truth labels or are limited to short-form responses. To address these limitations, we propose fine-tuning using semantic entropy, an uncertainty measure derived from introspection into the model which does not require external labels. We demonstrate that our approach matches or outperforms models fine-tuned using prior work and achieves strong performance for both short and long-form generations on a range of datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0
    },
    "849b3727dbb41c37f92a338ac5860b764a5b94f4.pdf": {
        "title": "Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding",
        "authors": [
            "Derong Xu",
            "Ziheng Zhang",
            "Zhihong Zhu",
            "Zhenxi Lin",
            "Qidong Liu",
            "Xian Wu",
            "Tong Xu",
            "Xiangyu Zhao",
            "Yefeng Zheng",
            "Enhong Chen"
        ],
        "published_date": "2024",
        "abstract": "The impressive capabilities of large language models (LLMs) have attracted extensive interests of applying LLMs to medical field. However, the complex nature of clinical environments presents significant hallucination challenges for LLMs, hindering their widespread adoption. In this paper, we address these hallucination issues in the context of Medical Information Extraction (MIE) tasks by introducing ALternate Contrastive Decoding (ALCD). We begin by redefining MIE tasks as an identify-and-classify process. We then separate the identification and classification functions of LLMs by selectively masking the optimization of tokens during fine-tuning. During the inference stage, we alternately contrast output distributions derived from sub-task models. This approach aims to selectively enhance the identification and classification capabilities while minimizing the influence of other inherent abilities in LLMs. Additionally, we propose an alternate adaptive constraint strategy to more effectively adjust the scale and scope of contrastive tokens. Through comprehensive experiments on two different backbones and six diverse medical information extraction tasks, ALCD demonstrates significant improvements in resolving hallucination issues compared to conventional decoding methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/849b3727dbb41c37f92a338ac5860b764a5b94f4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 4,
        "score": 4.0,
        "summary": "The impressive capabilities of large language models (LLMs) have attracted extensive interests of applying LLMs to medical field. However, the complex nature of clinical environments presents significant hallucination challenges for LLMs, hindering their widespread adoption. In this paper, we address these hallucination issues in the context of Medical Information Extraction (MIE) tasks by introducing ALternate Contrastive Decoding (ALCD). We begin by redefining MIE tasks as an identify-and-classify process. We then separate the identification and classification functions of LLMs by selectively masking the optimization of tokens during fine-tuning. During the inference stage, we alternately contrast output distributions derived from sub-task models. This approach aims to selectively enhance the identification and classification capabilities while minimizing the influence of other inherent abilities in LLMs. Additionally, we propose an alternate adaptive constraint strategy to more effectively adjust the scale and scope of contrastive tokens. Through comprehensive experiments on two different backbones and six diverse medical information extraction tasks, ALCD demonstrates significant improvements in resolving hallucination issues compared to conventional decoding methods.",
        "keywords": []
    },
    "55adbe4a6511a9c036024c6e2a637782d53289f4.pdf": {
        "title": "Mitigating spatial hallucination in large language models for path planning via prompt engineering",
        "authors": [
            "Hongjie Zhang",
            "Hourui Deng",
            "Jie Ou",
            "Chaosheng Feng"
        ],
        "published_date": "2025",
        "abstract": "Spatial reasoning in Large Language Models (LLMs) serves as a foundation for embodied intelligence. However, even in simple maze environments, LLMs often struggle to plan correct paths due to hallucination issues. To address this, we propose S2ERS, an LLM-based technique that integrates entity and relation extraction with the on-policy reinforcement learning algorithm Sarsa for optimal path planning. We introduce three key improvements: (1) To tackle the hallucination of spatial, we extract a graph structure of entities and relations from the text-based maze description, aiding LLMs in accurately comprehending spatial relationships. (2) To prevent LLMs from getting trapped in dead ends due to context inconsistency hallucination by long-term reasoning, we insert the state-action value function Q into the prompts, guiding the LLM\u2019s path planning. (3) To reduce the token consumption of LLMs, we utilize multi-step reasoning, dynamically inserting local Q-tables into the prompt to assist the LLM in outputting multiple steps of actions at once. Our comprehensive experimental evaluation, conducted using closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B, demonstrates that S2ERS significantly mitigates the spatial hallucination issues in LLMs, and improves the success rate and optimal rate by approximately 29% and 19%, respectively, in comparison to the SOTA CoT methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/55adbe4a6511a9c036024c6e2a637782d53289f4.pdf",
        "venue": "Scientific Reports",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Spatial reasoning in Large Language Models (LLMs) serves as a foundation for embodied intelligence. However, even in simple maze environments, LLMs often struggle to plan correct paths due to hallucination issues. To address this, we propose S2ERS, an LLM-based technique that integrates entity and relation extraction with the on-policy reinforcement learning algorithm Sarsa for optimal path planning. We introduce three key improvements: (1) To tackle the hallucination of spatial, we extract a graph structure of entities and relations from the text-based maze description, aiding LLMs in accurately comprehending spatial relationships. (2) To prevent LLMs from getting trapped in dead ends due to context inconsistency hallucination by long-term reasoning, we insert the state-action value function Q into the prompts, guiding the LLM\u2019s path planning. (3) To reduce the token consumption of LLMs, we utilize multi-step reasoning, dynamically inserting local Q-tables into the prompt to assist the LLM in outputting multiple steps of actions at once. Our comprehensive experimental evaluation, conducted using closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B, demonstrates that S2ERS significantly mitigates the spatial hallucination issues in LLMs, and improves the success rate and optimal rate by approximately 29% and 19%, respectively, in comparison to the SOTA CoT methods.",
        "keywords": []
    },
    "934635a82a338ddea6e0c0059663250eee259e8f.pdf": {
        "title": "Unravelling the Mysteries of Hallucination in Large Language Models: Strategies for Precision in Artificial Intelligence Language Generation",
        "authors": [
            "Ali Ahmadi"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools in Artificial Intelligence, showcasing remarkable linguistic mastery. However, amidst their expansive capabilities, a nuanced challenge arises: the phenomenon of hallucination. Hallucination introduces unpredictability and creativity into LLM-generated content, raising concerns about its implications. This paper seeks to illuminate the complex ramifications of hallucination in LLMs by examining its subtleties. The goal is to evaluate current efforts to mitigate hallucinations and improve the clarity of language generation. We delve into the intriguing world of AI with a focused examination of hallucinations in LLMs, exploring various strategies and methods aimed at reducing their effects and enhancing the accuracy of language generation. The analysis highlights the potential consequences for various applications and underscores the significant impact of hallucinations on LLM-generated content. Current solutions to this issue are discussed, showcasing advancements in the reliability and clarity of language generation. In conclusion, the pursuit of accuracy in LLMs faces captivating challenges posed by hallucinations. By exploring the complexities of this phenomenon and investigating mitigation strategies, we aim to bring greater consistency and clarity to the vast world of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/934635a82a338ddea6e0c0059663250eee259e8f.pdf",
        "venue": "Asian Journal of Computer Science and Technology",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Large Language Models (LLMs) have emerged as powerful tools in Artificial Intelligence, showcasing remarkable linguistic mastery. However, amidst their expansive capabilities, a nuanced challenge arises: the phenomenon of hallucination. Hallucination introduces unpredictability and creativity into LLM-generated content, raising concerns about its implications. This paper seeks to illuminate the complex ramifications of hallucination in LLMs by examining its subtleties. The goal is to evaluate current efforts to mitigate hallucinations and improve the clarity of language generation. We delve into the intriguing world of AI with a focused examination of hallucinations in LLMs, exploring various strategies and methods aimed at reducing their effects and enhancing the accuracy of language generation. The analysis highlights the potential consequences for various applications and underscores the significant impact of hallucinations on LLM-generated content. Current solutions to this issue are discussed, showcasing advancements in the reliability and clarity of language generation. In conclusion, the pursuit of accuracy in LLMs faces captivating challenges posed by hallucinations. By exploring the complexities of this phenomenon and investigating mitigation strategies, we aim to bring greater consistency and clarity to the vast world of LLMs.",
        "keywords": []
    },
    "4d608203639087e0fe3c5d2b7a374941dd182cb7.pdf": {
        "title": "Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models",
        "authors": [
            "Chaozhuo Li",
            "Pengbo Wang",
            "Chenxu Wang",
            "Litian Zhang",
            "Zheng Liu",
            "Qiwei Ye",
            "Yuanbo Xu",
            "Feiran Huang",
            "Xi Zhang",
            "Philip S. Yu"
        ],
        "published_date": "2025",
        "abstract": "Edgar Allan Poe noted,\"Truth often lurks in the shadow of error,\"highlighting the deep complexity intrinsic to the interplay between truth and falsehood, notably under conditions of cognitive and informational asymmetry. This dynamic is strikingly evident in large language models (LLMs). Despite their impressive linguistic generation capabilities, LLMs sometimes produce information that appears factually accurate but is, in reality, fabricated, an issue often referred to as'hallucinations'. The prevalence of these hallucinations can mislead users, affecting their judgments and decisions. In sectors such as finance, law, and healthcare, such misinformation risks causing substantial economic losses, legal disputes, and health risks, with wide-ranging consequences.In our research, we have methodically categorized, analyzed the causes, detection methods, and solutions related to LLM hallucinations. Our efforts have particularly focused on understanding the roots of hallucinations and evaluating the efficacy of current strategies in revealing the underlying logic, thereby paving the way for the development of innovative and potent approaches. By examining why certain measures are effective against hallucinations, our study aims to foster a comprehensive approach to tackling this issue within the domain of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4d608203639087e0fe3c5d2b7a374941dd182cb7.pdf",
        "venue": "arXiv.org",
        "citationCount": 3,
        "score": 3.0
    },
    "94c81ec4364d63fe67f98098547d0d09f063931d.pdf": {
        "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models",
        "authors": [
            "Mengfei Liang",
            "Archish Arun",
            "Zekun Wu",
            "Cristian Munoz",
            "Jonathan Lutch",
            "Emre Kazim",
            "A. Koshiyama",
            "Philip C. Treleaven"
        ],
        "published_date": "2024",
        "abstract": "Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. It automates test set creation from any corpus, ensuring high data quality, diversity, and cost-efficiency through techniques like batch processing, weighted sampling, and counterfactual validation. THaMES assesses a model's ability to detect and reduce hallucinations across various tasks, including text generation and binary classification, applying optimal mitigation strategies like In-Context Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base of academic papers, political news, and Wikipedia reveal that commercial models like GPT-4o benefit more from RAG than ICL, while open-weight models like Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT significantly enhances the performance of Llama-3.1-8B-Instruct in both evaluation tasks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/94c81ec4364d63fe67f98098547d0d09f063931d.pdf",
        "venue": "arXiv.org",
        "citationCount": 3,
        "score": 3.0
    },
    "2777cf13955e7683e0ec5446fdff7cdf7dd57d91.pdf": {
        "title": "Hallucination Mitigation Techniques in Large Language Models",
        "authors": [
            "M. Abdelghafour",
            "Mohammed Mabrouk",
            "Zaki Taha"
        ],
        "published_date": "2024",
        "abstract": ": Large language models (LLMs) have demonstrated impressive natural language understanding and generation capabilities, enabling advancements in diverse fields such as customer support, healthcare, and content creation. However, a significant challenge with LLMs is their tendency to produce factually inaccurate or nonsensical information, commonly known as hallucination. Hallucinations not only compromise the reliability of these models but can also lead to serious ethical and practical issues, particularly in high-stakes applications. This survey comprehensively reviews recent advancements in hallucination mitigation strategies for LLMs. We explore retrieval-augmented models, which enhance factual grounding by integrating external knowledge sources; human feedback mechanisms, such as reinforcement learning, which improve accuracy by aligning model responses with human evaluations; knowledge augmentation techniques that embed structured knowledge bases for enhanced consistency; and controlled generation, which restricts output to ensure alignment with factual constraints. Additionally, we examine the challenges of integrating these techniques and the limitations of current methods, including scalability, resource intensity, and dependency on quality data. Finally, we discuss future research directions to improve factual reliability in LLMs and explore hybrid solutions to create accurate and adaptable models for a wider range of real-world applications.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2777cf13955e7683e0ec5446fdff7cdf7dd57d91.pdf",
        "venue": "International Journal of Intelligent Computing and Information Sciences",
        "citationCount": 3,
        "score": 3.0,
        "summary": ": Large language models (LLMs) have demonstrated impressive natural language understanding and generation capabilities, enabling advancements in diverse fields such as customer support, healthcare, and content creation. However, a significant challenge with LLMs is their tendency to produce factually inaccurate or nonsensical information, commonly known as hallucination. Hallucinations not only compromise the reliability of these models but can also lead to serious ethical and practical issues, particularly in high-stakes applications. This survey comprehensively reviews recent advancements in hallucination mitigation strategies for LLMs. We explore retrieval-augmented models, which enhance factual grounding by integrating external knowledge sources; human feedback mechanisms, such as reinforcement learning, which improve accuracy by aligning model responses with human evaluations; knowledge augmentation techniques that embed structured knowledge bases for enhanced consistency; and controlled generation, which restricts output to ensure alignment with factual constraints. Additionally, we examine the challenges of integrating these techniques and the limitations of current methods, including scalability, resource intensity, and dependency on quality data. Finally, we discuss future research directions to improve factual reliability in LLMs and explore hybrid solutions to create accurate and adaptable models for a wider range of real-world applications.",
        "keywords": []
    },
    "f7a47a7de7289d6fd69e3bf226f7cdaffa670a3f.pdf": {
        "title": "HaDeMiF: Hallucination Detection and Mitigation in Large Language Models",
        "authors": [
            "Xiaoling Zhou",
            "Mingjie Zhang",
            "Zhemg Lee",
            "Wei Ye",
            "Shikun Zhang"
        ],
        "published_date": "2025",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f7a47a7de7289d6fd69e3bf226f7cdaffa670a3f.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
    },
    "d6442ff9d10310071108f44734b00d182b6e2c28.pdf": {
        "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models",
        "authors": [
            "Amin Karbasi",
            "Omar Montasser",
            "John Sous",
            "Grigoris Velegkas"
        ],
        "published_date": "2025",
        "abstract": "Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations. First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language. Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections. These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d6442ff9d10310071108f44734b00d182b6e2c28.pdf",
        "venue": "arXiv.org",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations. First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language. Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections. These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.",
        "keywords": []
    },
    "88e52de2320e06c7556795be43b38c85a9800e5a.pdf": {
        "title": "Mitigating Multilingual Hallucination in Large Vision-Language Models",
        "authors": [
            "Xiaoye Qu",
            "Mingyang Song",
            "Wei Wei",
            "Jianfeng Dong",
            "Yu Cheng"
        ],
        "published_date": "2024",
        "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable capabilities across a wide range of tasks, they suffer from hallucination problems, where models generate plausible yet incorrect answers given the input image-query pair. This hallucination phenomenon is even more severe when querying the image in non-English languages, while existing methods for mitigating hallucinations in LVLMs only consider the English scenarios. In this paper, we make the first attempt to mitigate this important multilingual hallucination in LVLMs. With thorough experiment analysis, we found that multilingual hallucination in LVLMs is a systemic problem that could arise from deficiencies in multilingual capabilities or inadequate multimodal abilities. To this end, we propose a two-stage Multilingual Hallucination Removal (MHR) framework for LVLMs, aiming to improve resistance to hallucination for both high-resource and low-resource languages. Instead of relying on the intricate manual annotations of multilingual resources, we fully leverage the inherent capabilities of the LVLM and propose a novel cross-lingual alignment method, which generates multiple responses for each image-query input and then identifies the hallucination-aware pairs for each language. These data pairs are finally used for direct preference optimization to prompt the LVLMs to favor non-hallucinating responses. Experimental results show that our MHR achieves a substantial reduction in hallucination generation for LVLMs. Notably, on our extended multilingual POPE benchmark, our framework delivers an average increase of 19.0% in accuracy across 13 different languages. Our code and model weights are available at https://github.com/ssmisya/MHR",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/88e52de2320e06c7556795be43b38c85a9800e5a.pdf",
        "venue": "arXiv.org",
        "citationCount": 3,
        "score": 3.0
    },
    "8ff45750057cc9452ae09aef6b9dfee3bd84b083.pdf": {
        "title": "Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance",
        "authors": [
            "Linxi Zhao",
            "Yihe Deng",
            "Weitong Zhang",
            "Quanquan Gu"
        ],
        "published_date": "2024",
        "abstract": "The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs to rectify the outputs of LVLMs. However, these approaches require either costly training or fine-tuning, or API access to proprietary LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE (MARINE), a framework that is both training-free and API-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across 5 popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations. We release our code at https://github.com/Linxi-ZHAO/MARINE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8ff45750057cc9452ae09aef6b9dfee3bd84b083.pdf",
        "venue": "",
        "citationCount": 3,
        "score": 3.0
    },
    "0e3d306997e4830e668f750bddc3ee28487ce59a.pdf": {
        "title": "Halwasa: Quantify and Analyze Hallucinations in Large Language Models: Arabic as a Case Study",
        "authors": [
            "Hamdy Mubarak",
            "Hend Suliman Al-Khalifa",
            "Khaloud Suliman Alkhalefah"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/0e3d306997e4830e668f750bddc3ee28487ce59a.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
    },
    "962cdae24cebb49c8870525fbf229554203aa5d2.pdf": {
        "title": "Recognizing Limits: Investigating Infeasibility in Large Language Models",
        "authors": [
            "Wenbo Zhang",
            "Zihang Xu",
            "Hengrui Cai"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses. This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the requests surpassing their capabilities. We conceptualize four main categories of infeasible tasks for LLMs, which cover a broad spectrum of hallucination-related challenges identified in prior literature. We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to evaluate multiple LLMs'abilities to decline infeasible tasks. Furthermore, we explore the potential of increasing LLMs'refusal capabilities with fine-tuning. Our experiments validate the effectiveness of the trained models, suggesting promising directions for improving the performance of LLMs in real-world applications.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/962cdae24cebb49c8870525fbf229554203aa5d2.pdf",
        "venue": "",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses. This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the requests surpassing their capabilities. We conceptualize four main categories of infeasible tasks for LLMs, which cover a broad spectrum of hallucination-related challenges identified in prior literature. We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to evaluate multiple LLMs'abilities to decline infeasible tasks. Furthermore, we explore the potential of increasing LLMs'refusal capabilities with fine-tuning. Our experiments validate the effectiveness of the trained models, suggesting promising directions for improving the performance of LLMs in real-world applications.",
        "keywords": []
    },
    "1c3eee136c5fa85ad97ef62f353557f776059f3e.pdf": {
        "title": "Multi-model assurance analysis showing large language models are highly vulnerable to adversarial hallucination attacks during clinical decision support",
        "authors": [
            "Mahmud Omar",
            "Vera Sorin",
            "Jeremy D. Collins",
            "David Reich",
            "Robert Freeman",
            "Nicholas Gavin",
            "Alexander W. Charney",
            "L. Stump",
            "N. L. Bragazzi",
            "G. Nadkarni",
            "Eyal Klang"
        ],
        "published_date": "2025",
        "abstract": "Background Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as \u201challucinations\u201d). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versions\u2014differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a \u201challucination\u201d. Results Hallucination rates range from 50 % to 82 % across models and prompting methods. Prompt-based mitigation lowers the overall hallucination rate (mean across all models) from 66 % to 44 % (p\u2009<\u20090.001). For the best-performing model, GPT-4o, rates decline from 53 % to 23 % (p\u2009<\u20090.001). Temperature adjustments offer no significant improvement. Short vignettes show slightly higher odds of hallucination. Conclusions LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1c3eee136c5fa85ad97ef62f353557f776059f3e.pdf",
        "venue": "Communications Medicine",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Background Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as \u201challucinations\u201d). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versions\u2014differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a \u201challucination\u201d. Results Hallucination rates range from 50 % to 82 % across models and prompting methods. Prompt-based mitigation lowers the overall hallucination rate (mean across all models) from 66 % to 44 % (p\u2009<\u20090.001). For the best-performing model, GPT-4o, rates decline from 53 % to 23 % (p\u2009<\u20090.001). Temperature adjustments offer no significant improvement. Short vignettes show slightly higher odds of hallucination. Conclusions LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
        "keywords": []
    },
    "ecc51ce52ca524be17616a9c0dc8a051a2996ad7.pdf": {
        "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models",
        "authors": [
            "Hao Yin",
            "Guangzong Si",
            "Zilei Wang"
        ],
        "published_date": "2025",
        "abstract": "Contrastive decoding strategies are widely used to mitigate object hallucinations in multimodal large language models (MLLMs). By reducing over-reliance on language priors, these strategies ensure that generated content remains closely grounded in visual inputs, producing contextually accurate outputs. Since contrastive decoding requires no additional training or external tools, it offers both computational efficiency and versatility, making it highly attractive. However, these methods present two main limitations: (1) bluntly suppressing language priors can compromise coherence and accuracy of generated content, and (2) processing contrastive inputs adds computational load, significantly slowing inference speed. To address these challenges, we propose Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals within the model\u2019s middle layers, where modality fusion predominantly occurs. This approach enables more effective capture of visual features, reducing the model\u2019s bias toward language modality. Experimental results demonstrate that VAF significantly reduces hallucinations across various MLLMs without affecting inference speed, while maintaining coherence and accuracy in generated outputs. The code is available at https://github.com/ustc-hyin/ClearSight.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ecc51ce52ca524be17616a9c0dc8a051a2996ad7.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 3,
        "score": 3.0
    },
    "b29b7aaae0554e864409dfd1afadf9e1564a2616.pdf": {
        "title": "A Brief Survey on Safety of Large Language Models",
        "authors": [
            "Zhengjie Gao",
            "Xuanzi Liu",
            "Yuanshuai Lan",
            "Zheng Yang"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and have been widely adopted in various applications such as machine translation, chatbots, text summarization, and so on. However, the use of LLMs has raised concerns about their potential safety and security risks. In this survey, we explore the safety implications of LLMs, including ethical considerations, hallucination, and prompt injection. We also discuss current research efforts to mitigate these risks and identify areas for future research. Our survey provides a comprehensive overview of the safety concerns related to LLMs, which can help researchers and practitioners in the NLP community develop more safe and ethical applications of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b29b7aaae0554e864409dfd1afadf9e1564a2616.pdf",
        "venue": "Journal of computer & information technology",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and have been widely adopted in various applications such as machine translation, chatbots, text summarization, and so on. However, the use of LLMs has raised concerns about their potential safety and security risks. In this survey, we explore the safety implications of LLMs, including ethical considerations, hallucination, and prompt injection. We also discuss current research efforts to mitigate these risks and identify areas for future research. Our survey provides a comprehensive overview of the safety concerns related to LLMs, which can help researchers and practitioners in the NLP community develop more safe and ethical applications of LLMs.",
        "keywords": []
    },
    "6becd0d29f013a7aec01d453727cb1680c01979f.pdf": {
        "title": "Conversational Agents, Virtual Worlds, and Beyond: A Review of Large Language Models Enabling Immersive Learning",
        "authors": [
            "Andreas Pester",
            "Ahmed Tammaa",
            "Christian G\u00fctl",
            "Alexander Steinmaurer",
            "S. A. El-Seoud"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models represent a significant breakthrough in Natural Language Processing research and opened a wide range of application domains. This paper demonstrates the successful integration of Large Language Models into immersive learning environments. The review highlights how this emerging technology aligns with pedagogical principles, enhancing the effectiveness of current educational systems. It also reflects recent advancements in integrating Large Language Models, including fine-tuning, hallucination reduction, fact-checking, and human evaluation of generated results.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6becd0d29f013a7aec01d453727cb1680c01979f.pdf",
        "venue": "IEEE Global Engineering Education Conference",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Large Language Models represent a significant breakthrough in Natural Language Processing research and opened a wide range of application domains. This paper demonstrates the successful integration of Large Language Models into immersive learning environments. The review highlights how this emerging technology aligns with pedagogical principles, enhancing the effectiveness of current educational systems. It also reflects recent advancements in integrating Large Language Models, including fine-tuning, hallucination reduction, fact-checking, and human evaluation of generated results.",
        "keywords": []
    },
    "dbcd51388bc622e7725782177c09cf8b5c1daf5d.pdf": {
        "title": "Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration",
        "authors": [
            "Kangxi Wu",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published_date": "2024",
        "abstract": "The black-box nature of large language models (LLMs) poses challenges in interpreting results, impacting issues such as data intellectual property protection and hallucination tracing. Training data attribution (TDA) methods are considered effective solutions to address these challenges.Most recent TDA methods rely on influence functions, assuming the model achieves minimized empirical risk. However, achieving this criterion is difficult, and sourcing accuracy can be compromised by fitting errors during model training. In this paper, we introduce a novel TDA method called Debias and Denoise Attribution (DDA), which enhances influence functions by addressing fitting errors. Specifically, the debias strategy seeks to improve the performance of influence functions by eliminating the knowledge bias present in the base model before fine-tuning, while the denoise strategy aims to reduce discrepancies in influence scores arising from varying degrees of fitting during the training process through smoothing techniques.Experimental results demonstrate that our method significantly outperforms existing approaches, achieving an averaged AUC of 91.64%. Moreover, DDA exhibits strong generality and scalability across various sources and different-scale models like LLaMA2, QWEN2, and Mistral.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/dbcd51388bc622e7725782177c09cf8b5c1daf5d.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 3,
        "score": 3.0,
        "summary": "The black-box nature of large language models (LLMs) poses challenges in interpreting results, impacting issues such as data intellectual property protection and hallucination tracing. Training data attribution (TDA) methods are considered effective solutions to address these challenges.Most recent TDA methods rely on influence functions, assuming the model achieves minimized empirical risk. However, achieving this criterion is difficult, and sourcing accuracy can be compromised by fitting errors during model training. In this paper, we introduce a novel TDA method called Debias and Denoise Attribution (DDA), which enhances influence functions by addressing fitting errors. Specifically, the debias strategy seeks to improve the performance of influence functions by eliminating the knowledge bias present in the base model before fine-tuning, while the denoise strategy aims to reduce discrepancies in influence scores arising from varying degrees of fitting during the training process through smoothing techniques.Experimental results demonstrate that our method significantly outperforms existing approaches, achieving an averaged AUC of 91.64%. Moreover, DDA exhibits strong generality and scalability across various sources and different-scale models like LLaMA2, QWEN2, and Mistral.",
        "keywords": []
    },
    "ac7cc880f897626ee2d2d5a5c40180d551f4e0f8.pdf": {
        "title": "ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models",
        "authors": [
            "Yahan Tu",
            "Rui Hu",
            "Jitao Sang"
        ],
        "published_date": "2024",
        "abstract": "Hallucination poses a persistent challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are generally static, which may overlook the potential risk of data contamination. To address this issue, we propose ODE, an openset, dynamic protocol designed to evaluate object hallucinations in MLLMs at both the existence and attribute levels. ODE employs a graph-based structure to represent real-world object concepts, their attributes, and the distributional associations between them. This structure facilitates the extraction of concept combinations based on diverse distributional criteria, generating varied samples for structured queries that evaluate hallucinations in both generative and discriminative tasks. Through the generation of new samples, dynamic concept combinations, and varied distribution frequencies, ODE mitigates the risk of data contamination and broadens the scope of evaluation. This protocol is applicable to both general and specialized scenarios, including those with limited data. Experimental results demonstrate the effectiveness of our protocol, revealing that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated samples, which indicates potential data contamination. Furthermore, these generated samples aid in analyzing hallucination patterns and fine-tuning models, offering an effective approach to mitigating hallucinations in MLLMs. Our code are available at https://github.com/Iridescent-y/ODE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ac7cc880f897626ee2d2d5a5c40180d551f4e0f8.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Hallucination poses a persistent challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are generally static, which may overlook the potential risk of data contamination. To address this issue, we propose ODE, an openset, dynamic protocol designed to evaluate object hallucinations in MLLMs at both the existence and attribute levels. ODE employs a graph-based structure to represent real-world object concepts, their attributes, and the distributional associations between them. This structure facilitates the extraction of concept combinations based on diverse distributional criteria, generating varied samples for structured queries that evaluate hallucinations in both generative and discriminative tasks. Through the generation of new samples, dynamic concept combinations, and varied distribution frequencies, ODE mitigates the risk of data contamination and broadens the scope of evaluation. This protocol is applicable to both general and specialized scenarios, including those with limited data. Experimental results demonstrate the effectiveness of our protocol, revealing that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated samples, which indicates potential data contamination. Furthermore, these generated samples aid in analyzing hallucination patterns and fine-tuning models, offering an effective approach to mitigating hallucinations in MLLMs. Our code are available at https://github.com/Iridescent-y/ODE.",
        "keywords": []
    }
}