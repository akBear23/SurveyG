\subsection*{Early Characterization and Taxonomies in LLMs}

The advent of Large Language Models (LLMs) marked a significant leap in generative AI, but simultaneously brought to the forefront the pervasive challenge of "hallucination"â€”the generation of content that is unfaithful to source material, factually incorrect, or logically inconsistent. Early research was thus fundamentally driven by the need to empirically observe, define, and categorize these phenomena to establish a foundational understanding for subsequent evaluation and mitigation strategies.

Initial empirical observations of unfaithful content were predominantly rooted in specific Natural Language Generation (NLG) tasks, particularly abstractive summarization. A seminal work by \cite{maynez2020h3q} provided a large-scale human evaluation of "faithfulness" in neural summarization. This study systematically characterized hallucinations into "intrinsic" (misrepresenting information present in the source) and "extrinsic" (adding information not directly inferable from the source) types. Crucially, \cite{maynez2020h3q} highlighted that traditional automatic metrics like ROUGE correlated poorly with human judgments of faithfulness, underscoring the limitations of surface-level evaluation and the necessity for deeper semantic understanding. Building on this, \cite{dong20223yz} further refined the understanding of faithfulness, challenging the strict assumption that all out-of-article content is undesirable. Their work demonstrated that gold-standard summaries often contain factually correct entities not explicitly present in the source, requiring external world knowledge. This introduced a critical dual perspective: faithfulness to the source document versus faithfulness to external world knowledge, thereby significantly shaping the early research agenda by clarifying these distinct dimensions of hallucination. These early efforts in summarization laid the groundwork for identifying content unsupported by a given context and distinguishing between adherence to input and general factual correctness.

Moving beyond document-level analysis, the need for finer-grained detection methods applicable to free-form text generation became apparent. While not directly a hallucination detection method, the work by \cite{chen2022gkm} on proposition-level segmentation and textual entailment recognition offered a foundational conceptual framework for analyzing the truthfulness of individual meaning units within sentences. By enabling the segmentation of sentences into distinct propositions and classifying their entailment relations with respect to a reference document, \cite{chen2022gkm} provided tools that could underpin more precise, token- or phrase-level identification of unsupported content. This approach represented a significant step towards dissecting generated text at a granular level, addressing the limitations of coarser-grained methods that might miss subtle inconsistencies, and paving the way for more detailed analyses of where and how hallucinations manifest within generated outputs.

As LLMs gained prominence, the scope of hallucination expanded, necessitating structured frameworks for categorization that transcended task-specific observations. Comprehensive surveys like \cite{zhang2023k1j} and \cite{ye2023yom} emerged as foundational works, synthesizing the burgeoning literature and proposing early LLM-centric taxonomies. \cite{zhang2023k1j} categorized LLM hallucinations into input-conflicting, context-conflicting, and fact-conflicting types, emphasizing the particular challenges posed by fact-conflicting errors due to the absence of an authoritative knowledge source. This survey provided a clear analytical framework for understanding the multifaceted nature of LLM failures. Concurrently, \cite{ye2023yom} offered a detailed taxonomy that spanned various text generation tasks, from machine translation to dialogue systems, and identified initial hypothesized origins related to data collection, knowledge gaps, and the optimization process. These meta-analyses were instrumental in consolidating diverse empirical findings into a common language, providing structured frameworks for understanding different types of hallucinations, and distinguishing between faithfulness to source and factual correctness in general knowledge. While these early taxonomies provided critical initial classifications, they often remained descriptive, and the sheer diversity and subtlety of LLM-generated errors hinted at a problem far more complex than simple factual deviations.

In conclusion, the early characterization of hallucination in LLMs evolved from empirical observations in specific NLG tasks, particularly abstractive summarization, to the development of sophisticated, LLM-centric taxonomies and meta-analyses. This progression, from identifying unfaithful content at a document level and refining the concept of faithfulness, to enabling finer-grained analysis and categorizing diverse error types, was instrumental in defining the problem space. These foundational works established the crucial distinction between faithfulness to source and factual correctness, which became a cornerstone for subsequent research. However, the initial frameworks also revealed that many LLM errors were not merely simple factual deviations but deeper failures of reasoning and consistency, suggesting that a more pervasive and nuanced understanding of hallucination was required, a challenge we explore in the subsequent section.