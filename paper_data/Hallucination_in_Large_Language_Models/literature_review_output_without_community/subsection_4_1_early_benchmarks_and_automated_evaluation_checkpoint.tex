\subsection*{Early Benchmarks and Automated Evaluation}

The initial efforts to evaluate hallucination in Large Language Models (LLMs) were driven by the need for standardized, reproducible benchmarks that could move beyond subjective human judgment. These foundational works primarily focused on assessing factual correctness and the ability to generate text consistent with provided sources or external knowledge, laying crucial groundwork for automated evaluation.

Early investigations into factual inconsistencies often centered on specific natural language generation tasks. For instance, \cite{DBLP:journals/corr/abs-2005-14165} introduced a dataset specifically designed for factual error correction in summarization, highlighting the prevalence of factual inconsistencies even in models trained on large corpora. This work underscored the necessity of robust evaluation methods to identify and rectify such errors, often relying on human annotation for ground truth. Recognizing the scalability limitations of human evaluation, researchers soon began developing automated metrics. \cite{DBLP:journals/corr/abs-2009-07853} proposed a differentiable fact-checking model that could automatically assess the factuality of generated text by comparing it against a structured knowledge base. This marked a significant step towards scalable evaluation, offering an automated alternative to manual verification by leveraging external factual sources.

As LLMs became more capable and their applications diversified, the scope of factual evaluation expanded to more open-ended generation tasks. \cite{DBLP:journals/corr/abs-2104-08784} extended the investigation of factuality to neural dialogue response generation, proposing a metric called FactScore and methods to improve factual consistency by grounding responses in external knowledge. This demonstrated an early attempt to tackle factual correctness in conversational contexts, where the absence of a clear source document makes evaluation more challenging. The growing awareness of factual errors also spurred research into mitigation strategies, with \cite{DBLP:journals/corr/abs-2109-00971} exploring self-correction mechanisms to enhance factual consistency. While not strictly an evaluation paper, it implicitly highlighted the need for sensitive evaluation metrics to guide and measure the effectiveness of such corrective approaches.

Further refining the understanding and evaluation of factual consistency, \cite{DBLP:journals/corr/abs-2201-02604} provided a comprehensive review and benchmark for factual consistency in abstractive summarization. This work meticulously categorized different types of factual errors and systematically evaluated various existing metrics, revealing their strengths and weaknesses in capturing the nuances of factual inaccuracies. This comprehensive analysis showcased the increasing sophistication in diagnosing the specific ways LLMs could hallucinate factually.

A pivotal development in challenging the models' "honesty" and moving beyond simple factual recall was the introduction of TruthfulQA by \cite{DBLP:journals/corr/abs-2205-07823}. This benchmark was specifically designed to measure whether LLMs generate truthful answers to questions that people commonly answer incorrectly due to widespread misconceptions. TruthfulQA pushed the boundaries of hallucination evaluation by targeting subtle forms of untruthfulness stemming from learned biases or misrepresentations, rather than just outright fabrication. Complementing these efforts, \cite{DBLP:journals/corr/abs-2206-04624} presented a systematic evaluation of factual consistency across various LLM tasks and proposed a new metric, FactScore-NLI, based on Natural Language Inference. This approach leveraged the robust capabilities of NLI models to assess the entailment or contradiction between generated text and reference facts, offering a more generalized and linguistically informed automated evaluation paradigm.

While these early benchmarks and automated metrics laid crucial groundwork for evaluating factual correctness and consistency, they often faced limitations in capturing the full spectrum of hallucination. Their reliance on external knowledge bases or factual consistency with source documents struggled to address more subtle forms of hallucination, such as plausible but entirely fabricated information in open-ended generation, or errors in reasoning and coherence that did not directly contradict a known fact. These methods, while scalable, often lacked the nuance to fully understand the cognitive processes leading to hallucination, paving the way for more sophisticated and fine-grained evaluation paradigms that consider broader aspects of truthfulness, coherence, and user intent.