\subsection{Multimodal Mitigation Strategies}

Hallucinations in multimodal models (MLLMs), encompassing Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs), present a complex challenge due to the intricate interplay and potential misalignments between diverse modalities \cite{lan20240yz, bai2024tkm, sahoo2024hcb}. These errors often stem from a "modality gap," where differences in data distribution or semantics between modalities lead to biased understanding, dataset toxicity, and inherited biases from underlying Large Language Models (LLMs) \cite{lan20240yz, bai2024tkm}. Addressing these issues necessitates specialized mitigation strategies that go beyond unimodal approaches, focusing on improving cross-modal grounding, balancing modality priors, and ensuring factual consistency in integrated outputs.

A prominent category of multimodal mitigation strategies involves training-free external guidance and verification mechanisms, which leverage cross-modal consistency checks to steer model generation without modifying internal model weights. These methods intervene during inference or use external tools to validate outputs. For instance, \cite{kim2024ozf} introduced \textbf{Counterfactual Inception}, a training-free method that prompts LMMs to engage in counterfactual thinking. The model generates "counterfactual keywords" (e.g., non-existent objects) based on visual input, which are filtered by a Plausibility Verification Process (PVP) using CLIP-based semantic alignment. These refined keywords then instruct the LMM to *avoid* generating such content, thereby enhancing visual grounding and reducing object, attribute, and relational hallucinations. This approach is fundamentally multimodal as it relies on the semantic interplay between generated text and visual content for self-correction. Similarly, \cite{park20247cm} leveraged Text-to-Image (T2I) diffusion models for hallucination visualization. By generating visual representations of an MLLM's textual output and comparing them with the original input, this method provides an external visual verification mechanism to guide contrastive decoding, effectively using an external generative model to identify and correct cross-modal inconsistencies. Complementary to this, \cite{wang2023hallucination} proposed a training-free image-grounded guidance method that employs a "negative image" (e.g., a blurred version of the input) to suppress non-grounded content in VLMs, using a "visual-grounding score" to quantify hallucination severity. While these external guidance methods offer flexibility and avoid costly retraining, their effectiveness is often contingent on the fidelity of the external verification models, and they do not fundamentally alter the MLLM's internal representation learning.

Beyond external guidance, more sophisticated strategies delve into the internal workings of multimodal models, often involving fine-tuning or specialized decoding, to address issues like 'visual amnesia' and modality imbalance at a deeper level. A significant challenge unique to interactive multimodal settings is "multimodal hallucination snowballing," where an MLLM's previously generated hallucination can mislead subsequent responses, even when ground visual information is available \cite{zhong2024mfi}. To combat this, \cite{zhong2024mfi} introduced \textbf{Residual Visual Decoding (RVD)}, a training-free decoding method that "residual connects" visual information with the current user instruction. RVD revises the MLLM's output distribution to emphasize direct visual evidence, providing models with more robust access to visual information during generation and reducing the propagation of self-generated errors in conversational contexts. For improving foundational visual grounding, \cite{jiang2022reg} proposed \textbf{Visual Cluster Grounding} for image captioning, which implicitly links generated words to informative regions in the image, dynamically focusing on discriminative parts or full object content to reduce object hallucinations and language bias.

Recent advancements have also focused on directly manipulating the decoding process or fine-tuning models with hallucination-targeted objectives. \textbf{Hallucination-Induced Optimization (HIO)} \cite{chen20247jb} introduces a novel paradigm where an "Evil LVLM" is intentionally trained using a *reversed* Bradley-Terry model to *prioritize* hallucinatory content. This "Evil LVLM" then serves as a strong contrastive signal during inference, amplifying the logit differences between hallucinatory and correct tokens to steer the original LVLM towards more factual outputs. This approach offers a more precise way to induce specific hallucinatory tokens for contrastive decoding compared to generic visual uncertainty methods. Similarly, \textbf{Hallucination-targeted Direct Preference Optimization (HDPO)} \cite{fu2024yqj} fine-tunes MLLMs by constructing specific preference pair data designed to address three distinct causes of hallucinations: Visual Distracted Hallucination (VDH), Long Context Hallucination (LCH), and Multimodal Conflict Hallucination (MCH). For VDH, negative samples are generated by amplifying irrelevant visual information; for LCH, negative examples are created by prompting the MLLM to continue truncated captions, often leading to deviation; and for MCH, conflicting textual information is introduced to train the model to prioritize visual grounding. HDPO's strength lies in its ability to jointly address multiple types of MLLM hallucinations through targeted data construction, offering a more comprehensive fine-tuning strategy compared to general DPO methods.

Deeper architectural interventions aim to resolve representational issues within the MLLM. Methods like \textbf{Memory-Space Visual Retracing (MemVR)} \cite{liu2024hallucination} allow models to re-examine and leverage visual features more effectively within intermediate layers, combating 'visual amnesia' by re-injecting visual tokens based on uncertainty. This ensures that crucial visual details are not forgotten during the generation process. Crucially, these internal interventions often incorporate causal mechanisms to balance modality priors, preventing one modality from dominating or suppressing information from another \cite{liu2024hallucination, zhou2024lvp}. For instance, \cite{zhou2024lvp}'s \textbf{CAUSAL MM} framework applies structural causal modeling to MLLMs, treating visual and language priors as confounding factors and using back-door adjustment and counterfactual reasoning to isolate and mitigate modality biases. Similarly, \textbf{Visual Amplification Fusion (VAF)} \cite{yin2025s2b} enhances attention to visual signals specifically within the MLLM's middle layers, arguing that language bias often stems from *insufficient* visual attention rather than an overemphasis on language. These intrinsic methods represent a deeper, mechanistic understanding of MLLM behavior, allowing for more precise and less intrusive corrections by ensuring more robust cross-modal integration.

While much of the research on multimodal hallucination mitigation has focused on LVLMs, the principles extend to other modalities, albeit with fewer dedicated studies \cite{sahoo2024hcb}. For Large Audio-Language Models (LALMs), object hallucination—where models generate or affirm the presence of non-existent sounds or objects—is a significant concern \cite{kuan20249pm}. \cite{kuan20249pm} demonstrated that carefully crafted prompt engineering can significantly improve LALM performance on discriminative audio tasks and reduce object hallucination, highlighting that even simple interventions can be effective when the underlying issue is query understanding rather than audio processing. Mitigation strategies for LALMs also include leveraging latent diffusion models or retrieval-based methods to ensure consistency between audio and text, as indicated by comprehensive surveys \cite{sahoo2024hcb}. In the realm of Audio-Visual Large Language Models (AV-LLMs), \cite{sungbin2024r2g} highlighted "cross-modal driven hallucinations," where models misinterpret information due to subtle relationships or over-reliance on one modality (e.g., video-driven audio hallucination or audio-driven video hallucination). Their work demonstrated that even simple training methods, such as Low-Rank Adaptation (LoRA) fine-tuning with enhanced feature alignment, can improve AV-LLM robustness against these complex inter-modal inconsistencies. General mitigation strategies for Video-Language Models (VLLMs) often involve temporal dependency modeling to ensure consistency across dynamic sequences, a critical challenge given the sequential nature of video data \cite{sahoo2024hcb}.

In conclusion, multimodal mitigation strategies have evolved from flexible, training-free external guidance and verification to sophisticated internal architectural interventions and causal reasoning frameworks, alongside targeted fine-tuning approaches. External methods like Counterfactual Inception \cite{kim2024ozf} and T2I visualization \cite{park20247cm} offer quick, adaptable solutions but rely on the robustness of external components. Decoding-time interventions like HIO \cite{chen20247jb} and fine-tuning approaches like HDPO \cite{fu2024yqj} represent a more direct engagement with the model's generation process, offering deeper control but requiring additional training or preference data. Intrinsic methods such as MemVR \cite{liu2024hallucination}, VAF \cite{yin2025s2b}, and CAUSAL MM \cite{zhou2024lvp} aim for fundamental improvements in cross-modal integration and modality balancing, offering robust solutions at the cost of increased model complexity or intrusiveness. While LVLMs have seen the most dedicated research, emerging work in LALMs \cite{kuan20249pm} and AV-LLMs \cite{sungbin2024r2g} indicates a growing focus on modality-specific challenges.

Despite significant progress, several challenges persist. The scalability and real-time applicability of complex causal interventions like CAUSAL MM \cite{zhou2024lvp} in dynamic multimodal streams, such as live video or audio, remain critical areas for research. A key open question is developing robust methods for arbitrating conflicting information presented by different modalities (e.g., an image showing a blue object while accompanying text describes a a red one), requiring dynamic weighting and conflict resolution mechanisms. Furthermore, the robustness of external verification methods, such as T2I-based approaches \cite{park20247cm}, is inherently tied to the fidelity and potential hallucination tendencies of the underlying generative models themselves. Ensuring temporal and causal consistency in long video or audio sequences, moving beyond single-frame object grounding, also poses a significant hurdle. Finally, effectively balancing the trade-offs between hallucination reduction, maintaining content quality, and preserving inference speed across diverse multimodal tasks (e.g., VQA, captioning, dialogue, video, audio) and model architectures remains a crucial area for future research \cite{yin2025s2b, sahoo2024hcb, lan20240yz, bai2024tkm}.