\subsection*{Decoding-Time Interventions and Contrastive Methods}

Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) frequently generate "hallucinations"â€”plausible but factually incorrect or ungrounded information. Addressing this critical challenge, decoding-time interventions offer an efficient, training-free paradigm for the primary model to steer generation towards more factual and grounded outputs by manipulating the probability distribution of generated tokens during inference. These methods operate by introducing a "negative" signal or bias that discourages the generation of ungrounded content, without requiring extensive fine-tuning of the main model itself.

One prominent approach in this domain is \textbf{Visual Contrastive Decoding (VCD)}, as proposed by \cite{park20247cm}. VCD mitigates hallucination, particularly in LVLMs, by penalizing tokens that are disproportionately favored when the visual input is subtly distorted or perturbed. The core mechanism involves comparing the logit distribution generated from the original input with that from a slightly altered, "negative" input (e.g., an image with minor noise or a masked object). By identifying tokens whose probabilities increase significantly under these subtle perturbations, VCD infers their sensitivity to ungrounded visual cues and suppresses them. This method leverages the model's internal representations to identify and counteract potential hallucinatory tendencies without requiring any additional training of the main LVLM. VCD has demonstrated effectiveness in reducing attribute and object hallucinations by steering the model towards more robust and visually grounded descriptions, often outperforming standard decoding strategies in terms of factual consistency. Its strength lies in its training-free nature and generalizability across different LVLMs, though its efficacy can be sensitive to the choice and strength of the perturbation.

Further enhancing the grounding aspect, other methods integrate external modalities to provide a strong, verifiable signal for contrastive decoding. For instance, approaches like the \textbf{Image-Grounded Guidance} proposed by \cite{zhao2024ge8} for LVLMs leverage pre-trained external vision models to establish a factual baseline. This method generates a reference caption for an input image using a robust image captioning model. During the LVLM's decoding process, the generated tokens are continuously compared against this reference. If the LVLM generates tokens corresponding to objects or attributes not present in the reference caption, these tokens are penalized by adjusting their logits downwards. This effectively guides the LVLM to generate text that is consistent with the external visual understanding, mitigating object and attribute hallucinations. While this approach is training-free for the main LVLM, it introduces computational overhead due to the inference required by the external image captioning model. However, it offers a powerful mechanism for ensuring strong visual grounding, particularly for object-centric hallucinations, by explicitly incorporating external, verifiable visual information into the decoding process.

Complementing these sophisticated contrastive techniques are simpler, yet often effective, \textbf{logit-based interventions}. Works such as \cite{leng2023ohr} demonstrate that direct manipulation of token logits can counter specific linguistic biases or common hallucination patterns. These methods involve applying predefined biases or penalties to certain token probabilities based on external knowledge, semantic categories, or simple heuristics. For example, if a model frequently hallucinates specific entities in a given context, their logits can be slightly suppressed. \cite{leng2023ohr} specifically proposes a logit-based calibration method that adjusts token probabilities by considering their frequency in factual vs. non-factual contexts, effectively down-weighting tokens associated with higher hallucination rates. These interventions are computationally light and offer a direct way to address specific, recurring types of hallucinations or linguistic tendencies that lead to ungrounded outputs. However, their generalizability is often limited, as they require prior identification of problematic tokens or patterns and may not adapt well to novel hallucination types.

In synthesis, decoding-time interventions offer a versatile and efficient suite of strategies. VCD (\cite{park20247cm}) provides a general, training-free mechanism to enhance robustness against subtle input variations, making it effective for reducing attribute and object hallucinations by promoting consistency. Image-Grounded Guidance (\cite{zhao2024ge8}) offers a more explicit form of external grounding for multimodal models, leveraging an auxiliary vision model to provide a strong factual anchor, albeit with increased inference cost. Logit-based calibration (\cite{leng2023ohr}) represents the most lightweight approach, directly adjusting token probabilities based on observed biases, offering immediate impact for known issues but requiring careful design to avoid over-suppression or limiting creativity.

Despite their efficiency and training-free nature for the primary model, challenges remain. A key difficulty lies in defining universally effective "negative" signals or biases that reliably identify and suppress hallucinations without inadvertently stifling creativity or factual nuance. The computational overhead of integrating external models for guidance, as seen in image-grounded approaches, can also be a practical limitation. Furthermore, ensuring that these interventions do not introduce new biases or degenerate the quality of non-hallucinatory content requires careful calibration. Future directions may involve developing more adaptive and dynamic intervention strengths, perhaps guided by real-time uncertainty estimation within the LLM, or exploring hybrid approaches that combine the general robustness of VCD with the explicit grounding of external knowledge and the targeted precision of logit-based adjustments. The goal is to create a robust, yet flexible, decoding framework that can dynamically balance factual accuracy with fluency and coherence across diverse generation tasks.