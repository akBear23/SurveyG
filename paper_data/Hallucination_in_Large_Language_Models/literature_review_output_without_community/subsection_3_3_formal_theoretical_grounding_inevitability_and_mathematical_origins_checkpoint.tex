\subsection{Formal Theoretical Grounding: Inevitability and Mathematical Origins}

The understanding of hallucination in Large Language Models (LLMs) has undergone a profound paradigm shift, transitioning from an empirically driven problem-solving endeavor to a rigorous theoretical exploration of its fundamental limits. While early research primarily focused on identifying practical sources of hallucination and developing engineering-centric mitigation strategies \cite{xu2024n76}, a groundbreaking line of inquiry has emerged to investigate whether hallucination is merely a transient engineering challenge or an innate, unavoidable characteristic of these powerful models. This theoretical turn seeks to establish a formal grounding for hallucination, moving beyond observational data to mathematical proofs of its inevitability.

A pivotal contribution in this area is presented by \cite{xu2024n76}, which fundamentally redefines the discourse around LLM hallucination. The paper introduces a "formal world" where LLMs are abstracted as total computable functions and ground truth is similarly defined as a computable function. This abstraction allows for the first formal definition of hallucination (Definition 4), characterizing it as an inconsistency between the LLM's output and the ground truth for a given input. This rigorous formalization provides a robust framework for theoretical analysis, independent of specific model architectures or training algorithms.

The core innovation of \cite{xu2024n76} lies in its application of advanced concepts from learning theory, specifically Cantor's diagonalization argument, to prove the inherent inevitability of hallucination. This mathematical technique, traditionally used to demonstrate the existence of uncomputable numbers or functions, is ingeniously adapted to show that no computable LLM can perfectly learn all computable functions representing ground truths. The authors present Theorem 1, which demonstrates that for any computably enumerable set of LLMs, all states will inevitably hallucinate on some inputs. This is further extended by Theorem 2, proving that such hallucination will occur on *infinitely many* inputs. Crucially, Theorem 3 generalizes these findings to establish that hallucination is inevitable for *any individual computable LLM*, both on some inputs and on infinitely many inputs. These proofs establish a theoretical upper bound on the capabilities of LLMs, demonstrating that they cannot serve as universal problem solvers without generating factually incorrect information.

This theoretical insight fundamentally reshapes the understanding of LLM capabilities and limitations. Unlike previous empirical efforts that aimed to *reduce* or *eliminate* hallucination through better data, architectures, or prompting techniques, \cite{xu2024n76} posits that complete eradication is mathematically impossible for any computable LLM. While parallel work by \cite{kalai2023statistical} provides a statistical lower bound on hallucination rates for calibrated LLMs, \cite{xu2024n76}'s findings are more general, applying to all computable LLMs and proving an absolute inevitability rather than a statistical likelihood. This distinction highlights the profound nature of the theoretical limits identified by the diagonalization argument.

Consequently, hallucination is reframed not as a transient engineering problem to be fixed, but as an innate, fundamental characteristic inherent to the very nature of computable functions that LLMs embody. This profound theoretical grounding guides future research away from the elusive goal of achieving "hallucination-free" LLMs and towards more realistic and robust strategies for detection, mitigation, and responsible deployment. It underscores the necessity for LLMs to be used as specialized tools, often requiring external verification and human oversight, rather than as infallible general problem solvers. The mathematical origins of hallucination thus compel a shift in focus towards managing its unavoidable presence, fostering a more realistic and effective research agenda for the future of LLM development.