\subsection*{Training-Based Approaches and Automated Data Generation}

Training-based approaches, encompassing fine-tuning and unlearning, represent a direct and potent strategy for mitigating hallucinations in large language models (LLMs) and multimodal large language models (MLLMs) \cite{sahoo2024hcb, zhang2023k1j, liu2024p39}. A central impediment to their scalability and effectiveness, however, is the prohibitive cost and scarcity of high-quality, labeled data, particularly for diverse and nuanced hallucination types \cite{cao2023ecl, li2025qzg}. Recent research has thus heavily focused on innovative automated data generation techniques to circumvent this data bottleneck, enabling more targeted and efficient model interventions.

One direct approach to address data scarcity for hallucination detection and mitigation is the automated generation of datasets by leveraging existing knowledge. AutoHall \cite{cao2023ecl} proposes a three-step pipeline to automatically construct model-specific hallucination datasets from existing fact-checking resources. By prompting an LLM to generate references for claims, classifying their support, and then flagging contradictions with ground truth, AutoHall efficiently creates labeled hallucinatory examples. This method eliminates laborious manual annotation, making it scalable for continuous model updates and specific hallucination patterns. While effective for text-based factuality, AutoHall's reliance on pre-existing fact-checking resources may limit the diversity of generated hallucinations and risks inheriting their topical biases, potentially failing to uncover novel or subtle hallucination types that are not yet documented.

Building on the principle of generating adversarial examples, several methods leverage auxiliary models or controlled processes to create dispreferred, hallucinatory content for training. Hallucination-Induced Optimization (HIO) \cite{chen20247jb} exemplifies this by training an "Evil LVLM" specifically to generate adversarial, hallucinated examples given an image and a prompt. This "Evil LVLM" is optimized using a Contrary Bradley-Terry Model (CBTM) to *prioritize* hallucinatory content, effectively amplifying a diverse set of potential visual and factual inconsistencies. These meticulously crafted hallucinatory outputs then serve as negative examples to train a "Good LVLM" via contrastive decoding (as discussed further in Section 6.1), thereby enhancing its robustness. A similar concept is explored in Induce-then-Contrast Decoding (ICD) \cite{zhang202396g} for LLMs, which constructs a "factually weak LLM" by fine-tuning it on non-factual samples generated by converting factual samples into untruthful ones. During inference, the log probabilities of these induced hallucinations from the weak model are subtracted from the original model's predictions, penalizing untruthful tokens. Further extending this, \textit{VHTest} \cite{huang20247wn} introduces an adversarial generation paradigm for visual hallucination (VH) in MLLMs. It systematically creates new, diverse, and uncontaminated VH images using text-to-image models (e.g., DALLÂ·E-3), guided by MLLM-generated descriptions of hallucination modes. This approach allows for the construction of robust benchmarks and subsequent fine-tuning to mitigate specific VH types like object existence, shape, and size. While these generative approaches offer greater diversity and novelty in adversarial examples compared to AutoHall, they introduce their own challenges, such as the computational cost of training auxiliary "evil" or "weak" models and the risk that the generated "bad" data might still be stereotypical or lack the subtle nuances of real-world hallucinations, potentially introducing new biases rather than creating truly generalizable improvements \cite{yin2024iau}.

The broader paradigm of preference optimization, including Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), has proven highly effective in aligning LLMs with human preferences. A critical component of these methods is the availability of high-quality preference pairs, especially dispreferred (negative) responses. Preference Optimization in VLLM (POVID) \cite{zhou2024wbi} addresses this by automating the creation of dispreferred responses using an AI model. This method is particularly effective in reducing object hallucination in VLLMs by providing abundant, automatically generated examples of incorrect or misleading descriptions, allowing the model to learn preferred, factually accurate outputs more efficiently. Extending this, Hallucination-targeted Direct Preference Optimization (HDPO) \cite{fu2024yqj} specifically constructs preference pair data designed to target three distinct causes of MLLM hallucinations: visual distracted hallucination, long context hallucination, and multimodal conflict hallucination. HDPO's innovation lies in its novel data construction strategies, such as generating negative samples by preserving only low-attention visual tokens or by prompting the MLLM with conflicting information, thereby guiding the model to learn robust alignment against diverse hallucination types. Further advancing judge-free self-improvement, Deng et al. \cite{deng202405j} propose a framework that generates both positive and negative response candidates by introducing a "hallucination ratio" during decoding, blending conditional and unconditional token distributions. These generated pairs are then verified by a lightweight, objective verifier (e.g., a CLIP model) to ensure data quality, significantly reducing the computational costs and biases associated with MLLM-as-judge approaches in traditional RLHF/DPO pipelines. A critical consideration for these preference optimization methods is the reliability of the AI model used to generate dispreferred responses or act as a verifier; if the AI itself is prone to biases or errors, it could inadvertently reinforce undesirable behaviors or generate suboptimal training signals.

Addressing a specific, yet globally relevant, hallucination challenge, Multilingual Hallucination Removal (MHR) \cite{qu20240f7} tackles the problem of significantly more severe hallucination in Large Vision-Language Models (LVLMs) when queried in non-English languages. MHR proposes a two-stage framework, with the second stage focusing on hallucination-enhanced preference optimization. Crucially, it introduces a novel *cross-lingual alignment method* to automatically generate multilingual hallucination-aware data pairs. This method leverages the LVLM itself to generate multiple responses in various non-English languages, which are then aligned with existing English hallucination/non-hallucination answers using semantic distance metrics. This scalable approach creates multilingual hallucination-aware datasets, significantly reducing manual effort and enabling DPO-based fine-tuning to favor non-hallucinating responses across languages. While MHR demonstrates substantial improvements, particularly in reducing "unknown" responses and increasing accuracy across diverse languages, it acknowledges that some instruction-following issues persist for low-resource languages, indicating limitations tied to the foundational multilingual capabilities of the base LLM.

Collectively, these training-based approaches underscore a significant shift towards more automated and data-efficient methods for mitigating hallucinations. By innovatively generating synthetic, adversarial, or dispreferred data, methods like AutoHall, HIO, ICD, POVID, HDPO, VHTest, and MHR circumvent the traditional data annotation bottleneck, making fine-tuning and preference optimization strategies more practical and scalable \cite{sahoo2024hcb}. The integration of lightweight verifiers, as seen in Deng et al. \cite{deng202405j}, further enhances efficiency and reduces reliance on expensive human or LLM-based judgments. However, challenges persist in ensuring the generalizability and diversity of automatically generated adversarial examples, precisely defining and identifying all forms of hallucination for targeted unlearning, and managing the computational overhead associated with training auxiliary models or complex unlearning processes. Furthermore, the theoretical inevitability of hallucination in computable LLMs \cite{xu2024n76, li2025qzg} suggests that even the most sophisticated training-based approaches may only reduce, but not entirely eliminate, the problem. Future research will likely explore hybrid approaches that combine these automated data generation techniques with more advanced unlearning algorithms, investigate methods for dynamically adapting data generation strategies to evolving hallucination types, and further refine judge-free verification mechanisms to build even more robust and trustworthy AI systems within these inherent limitations.