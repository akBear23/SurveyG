\subsection*{Remaining Challenges and Open Questions}

Despite significant advancements in characterizing, evaluating, and mitigating hallucination in large language models (LLMs) and multimodal large language models (MLLMs), several critical unresolved issues persist, defining promising avenues for future research in trustworthy AI. The dynamic nature of these models and their expanding capabilities mean that hallucination remains a moving target, necessitating continuous innovation.

One fundamental challenge lies in the **scalability of fine-grained annotation and evaluation**. While efforts like \cite{ji20243j6} and \cite{gu202414e} have introduced analytical, sentence-level annotation datasets (ANAH and ANAH-v2) and iterative self-training frameworks to scale this process, the sheer diversity of tasks, domains, and hallucination types makes truly comprehensive, human-quality annotation prohibitively expensive and time-consuming. This bottleneck hinders the development of robust, generalizable evaluation benchmarks that can capture the nuances of complex reasoning, as exemplified by the need for rationale verification in \cite{oh2024xa3} (ERBench) and path-based evaluation in graph computation tasks \cite{tang2024a1j} (GraphArena). The challenge extends beyond mere data quantity to ensuring the *quality, diversity, and contextual richness* of annotated data across an ever-expanding problem space.

Another pressing issue is the development of **truly real-time, adaptive, and seamlessly integrated mitigation strategies**. Current approaches have made strides towards proactive prevention and self-correction. For instance, \cite{manakul20236ex} (SelfCheckGPT) and \cite{yang20251dw} (MetaQA) offer zero-resource, black-box detection methods, yet these often incur computational overhead or rely on sampling, which can introduce latency or fail to capture dynamic shifts in model uncertainty. While \cite{tjandra2024umq} proposes label-free abstention using semantic entropy, the challenge remains in making such mechanisms adaptively responsive to subtle changes in user intent or context without sacrificing generation quality or speed. Similarly, advanced Retrieval-Augmented Generation (RAG) techniques like \cite{lv2024k5x}'s Coarse-to-Fine Highlighting (COFT) and \cite{ding20244yr}'s Rowen improve context relevance, but the seamless integration of such dynamic knowledge retrieval and synthesis into the core generation process, without introducing new latency or compromising creative outputs, remains an open problem. Furthermore, while \cite{hakim2024d4u} introduced "semantic guardrails" for safety-critical domains, generalizing such hard constraints to open-ended, creative, or rapidly evolving tasks without stifling utility is a complex balancing act.

The field also grapples with the complex task of **bridging the gap between theoretical inevitability and practical error reduction**. The unified theoretical framework proposed by \cite{li2025qzg} (Loki's Dance of Illusions) suggests that some forms of hallucination might be mathematically inherent to LLM architectures or their training paradigms. If certain types of hallucination are indeed inevitable, the critical question shifts from absolute elimination to understanding the *acceptable error rate* and designing systems that can gracefully handle or transparently communicate these inherent limitations. This necessitates further research into robust uncertainty quantification and calibrated abstention mechanisms that are both accurate and user-friendly, allowing LLMs to "know what they don't know" effectively.

Beyond these challenges, several **open questions** guide the next wave of innovation. The rapid expansion into multimodal AI has unveiled **novel hallucination types** that require dedicated investigation. While object hallucination in vision-language models \cite{liu2024sn3, lan20240yz} and audio-language models \cite{kuan20249pm} has been identified, and video-specific temporal inconsistencies are being explored \cite{wang2024rta}, the full spectrum of cross-modal fabrication and misinterpretation, especially in complex reasoning or creative multimodal generation, is yet to be fully mapped. Moreover, the emergence of adversarial attacks that induce hallucinations \cite{wang2025jen} suggests that new, engineered forms of hallucination will continue to challenge detection and mitigation efforts.

Another critical open question concerns the **long-term impact of multimodal interactions and cascading hallucinations**. While \cite{qiu2024zyc} has begun to address long-context multimodal hallucination, the cumulative effect of minor inconsistencies over extended dialogues or multi-turn reasoning in complex multimodal environments remains poorly understood. The concept of "multimodal hallucination snowballing" \cite{zhong2024mfi} highlights the potential for initial errors to propagate and amplify, leading to increasingly unreliable outputs. Developing methods to track, predict, and mitigate these cascading effects across modalities and over time is crucial for building truly robust conversational and interactive AI systems.

Finally, there is a pressing need for **more unified and generalizable solutions that perform robustly across diverse tasks and domains**. Many current mitigation strategies are task-specific (e.g., RAG for factual QA, visual grounding for LVLMs). The development of a single, overarching framework that can effectively address hallucination across diverse applications—from summarization and dialogue to code generation and creative writing, and across various modalities—while maintaining high performance and adaptability, remains an elusive goal. While some efforts, like \cite{chang2024u3t}'s unified LVLM mitigation framework, attempt to generalize within a multimodal context, achieving true cross-domain and cross-task robustness without extensive, domain-specific fine-tuning is a significant hurdle. The pursuit of a holistic "information quality" metric, as conceptualized by \cite{rejeleene2024okw}, could pave the way for more generalizable evaluation and mitigation, but its practical implementation across diverse scenarios is still an open area of research. Addressing these challenges and open questions will be paramount in guiding the next wave of innovation towards building inherently trustworthy and transparent AI systems.