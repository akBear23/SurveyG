\subsection*{Empirical Causes: Data, Training, and Inference Biases}

Large Language Models (LLMs) are susceptible to generating "hallucinations"—plausible but factually incorrect or unfaithful content—due to a complex interplay of empirical factors spanning their entire lifecycle, from data acquisition to training and inference. Understanding these practical causes is crucial for developing targeted interventions throughout the model development pipeline. A foundational observation, particularly in abstractive summarization, highlighted that standard likelihood training objectives and approximate decoding strategies inherently prioritize fluency and coherence over strict factual accuracy, leading to both intrinsic (misrepresenting source information) and extrinsic (adding ungrounded information) hallucinations \cite{maynez2020h3q}. This initial insight underscores biases embedded during training and exacerbated during generation.

A significant category of empirical causes stems from **data collection and preparation issues**. The immense pre-training corpora, while enabling powerful generalization, often contain noise, inconsistencies, and factual decay that models inadvertently learn \cite{li2024qrj}. For instance, empirical studies reveal that the lower the frequency of specific knowledge in pre-training data, the higher the propensity for LLMs to hallucinate when queried on that topic \cite{li2024qrj}. This suggests that data imbalance and under-representation of certain facts directly contribute to factual errors. Furthermore, LLMs learn superficial statistical patterns from their training data rather than robust logical reasoning. McKenna et al. (2023) identified two key biases in pre-trained models: an **attestation bias**, where models over-rely on propositional memory, affirming entailment if a hypothesis is likely attested in their training data regardless of the premise; and a **relative frequency bias**, where models tend to affirm entailment if the premise predicate is less frequent than the hypothesis predicate \cite{mckenna2023pzc}. These biases demonstrate how statistical regularities in the data, rather than semantic understanding, can lead to incorrect inferences, with specific named entities often acting as "indices" to trigger memorized, potentially irrelevant, propositional knowledge \cite{mckenna2023pzc}.

During **training**, models can develop biases that manifest as hallucinations. The predominant optimization objectives, typically focused on next-token prediction, do not explicitly penalize factual inaccuracies. This encourages models to "over-generalize" or invent plausible but unverified information to maintain fluency, thereby internalizing spurious correlations present in the data \cite{li2024qrj}. Supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) aim to align models, but their effectiveness can be highly dependent on the quality of instructions and the balanced complexity of the fine-tuning data. Suboptimal SFT can exacerbate hallucinations, and even RLHF's benefits can be domain-specific, indicating that the training process itself can introduce or fail to mitigate biases effectively \cite{li2024qrj}. In multimodal contexts, such as with Vision-Language Models (VLMs), training challenges can lead to "uncontrollable global visual uncertainty," where the model struggles to consistently ground its textual descriptions in the visual input, resulting in visual hallucinations \cite{chen20247jb}. This highlights that even with rich multimodal data, the training process might not sufficiently enforce cross-modal consistency.

**Inference-time biases** further contribute significantly to hallucination, even in well-trained models. The decoding process, as noted by \cite{maynez2020h3q}, often prioritizes fluency over factual accuracy. LLMs frequently struggle with complex reasoning tasks, leading to erroneous outputs. Empirical studies show that a "lack of logical reasoning capabilities is the primary contributor to Fact-Conflicting Hallucination (FCH)," particularly when dealing with temporal concepts and out-of-distribution knowledge \cite{li2024osp}. Benchmarks evaluating LLM rationales reveal that models can generate plausible-sounding but logically flawed reasoning paths \cite{oh2024xa3}. Furthermore, the order in which an LLM generates its reasoning and answer significantly impacts consistency, with generating an answer first often leading to subsequent fabrication of justifications, highlighting a fragile reasoning process at inference time \cite{xie20247zk}. When tasked with algorithmic reasoning on complex structures like real-world graphs, LLMs often fail to produce correct solutions, indicating a struggle with systematic, multi-step inference that can lead to hallucinated steps or conclusions \cite{tang2024a1j}.

Another prevalent inference-time bias is the model's overconfidence or lack of uncertainty awareness. LLMs tend to generate definitive answers even when highly uncertain, leading to confident hallucinations \cite{li2024qrj}. This necessitates proactive abstention mechanisms, which allow models to signal uncertainty and refuse to answer when confidence is low \cite{tjandra2024umq}. The empirical observation that models can produce "never event" errors in safety-critical domains further underscores this lack of inherent self-correction and the need for "semantic guardrails" to prevent catastrophic hallucinations during deployment \cite{hakim2024d4u}. For LVLMs, an inference-time bias can be their failure to adequately ground generated text in specific visual regions without explicit guidance, leading to object hallucinations that require training-free, image-grounded interventions \cite{zhao2024ge8}.

Even with Retrieval-Augmented Generation (RAG) systems designed to mitigate hallucinations by providing external knowledge, LLMs exhibit several inference-time biases in context utilization. Empirical benchmarks reveal that LLMs struggle with **noise robustness**, often confusing similar information when relevant and noisy documents are present; **negative rejection**, frequently failing to abstain from answering when no relevant information is available; **information integration**, demonstrating a significant lack of ability to synthesize facts from multiple documents; and **counterfactual robustness**, tending to trust and prioritize factually incorrect retrieved information even when possessing correct internal knowledge or being warned \cite{chen2023h04}. Moreover, LLMs can be receptive to newly generated counterfactual passages, indicating a vulnerability in their utility judgment of retrieved evidence \cite{zhang2024o58}. These challenges necessitate more intelligent, uncertainty-aware retrieval and context highlighting mechanisms \cite{niu2024v97, su2024gnz, lv2024k5x}.

In conclusion, empirical causes of hallucination are multifaceted, spanning the entire LLM development pipeline. Data quality issues, especially imbalances and superficial statistical patterns, lead to models learning incorrect associations and biases. Training objectives that prioritize fluency over factuality, coupled with challenges in enforcing cross-modal consistency, contribute to over-generalization and learned biases. During inference, models exhibit biases towards fluency, struggle with complex logical and algorithmic reasoning, often lack uncertainty awareness, and can inefficiently or incorrectly utilize external knowledge. Addressing these empirical causes requires a holistic approach, from meticulous data curation and refined training objectives to advanced inference-time controls and uncertainty quantification.