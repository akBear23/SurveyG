\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 277 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction}

\section{Introduction}
\label{sec:introduction}

\subsection{The Rise of Large Language Models and the Hallucination Challenge}
\label{sec:1\_1\_the\_rise\_of\_large\_language\_models\_\_and\_\_the\_hallucination\_challenge}

The advent of Large Language Models (LLMs) has heralded a transformative era in artificial intelligence, demonstrating unprecedented capabilities in natural language understanding and generation \cite{ahmadi2024j88}. These models, characterized by their massive scale and emergent abilities, have revolutionized diverse domains, from automating complex cognitive tasks like code generation and scientific discovery to enhancing human-computer interaction through sophisticated dialogue systems. Their capacity to produce highly coherent, contextually relevant, and often creative text has positioned them as pivotal technologies poised to redefine numerous industries and research paradigms.

However, alongside these remarkable advancements, a pervasive and critical limitation, widely termed 'hallucination,' significantly impedes the reliability and trustworthiness of LLMs \cite{ahmadi2024j88, rawte2023ao8}. Hallucination refers to the generation of content that is factually incorrect, nonsensical, or ungrounded in the provided input or real-world knowledge. Early investigations into neural text generation, particularly in abstractive summarization, highlighted this issue, revealing that models frequently produced information not present in the source document, with a substantial portion being factually erroneous \cite{maynez2020h3q}. This foundational work introduced a critical distinction between 'intrinsic' hallucinations (misrepresenting source information) and 'extrinsic' hallucinations (adding ungrounded information), underscoring the challenge of ensuring faithfulness and factual accuracy. The problem's prevalence has only intensified with the increasing scale and generality of LLMs, making it a central concern for their safe and effective deployment \cite{rawte2023ao8}.

The implications of hallucination are profound and far-reaching, posing substantial risks to user trust, the credibility of AI systems, and the safety of their applications across various domains. In high-stakes environments such as healthcare, finance, or legal services, hallucinated information can lead to severe consequences, including misdiagnoses, flawed financial advice, or fabricated legal precedents \cite{li2025qzg, ahmadi2024j88}. Beyond factual errors, hallucinations can manifest as logical inconsistencies or ungrounded reasoning, eroding confidence in an LLM's ability to perform complex tasks reliably. This challenge extends beyond mere inconvenience, directly impacting the deployability of LLMs and necessitating robust mechanisms to ensure their outputs are verifiable and trustworthy.

The research trajectory has evolved from merely characterizing hallucination to seeking a deeper understanding of its origins, both empirical and theoretical. Initial observations revealed that even during pre-training, smaller language models could learn to reduce perplexity on grammatical sequences that \textit{contained} hallucinations, suggesting that the propensity for generating plausible but incorrect text is embedded early in the learning process \cite{xia20224cl}. As models scale, some of these issues are mitigated, but the fundamental challenge persists. More recently, the understanding of hallucination has been elevated to a theoretical plane, with formal mathematical definitions being proposed \cite{li2025qzg}. Groundbreaking work has even posited that hallucination is not merely a transient engineering problem but an inherent and \textit{inevitable} limitation of any computable LLM, stemming from fundamental principles of learning theory and computability \cite{xu2024n76}. This theoretical perspective fundamentally reshapes the problem, suggesting that while mitigation is crucial, complete eradication might be impossible, thus shifting the focus towards robust management and transparent communication of uncertainty rather than absolute elimination. This pervasive issue is also not confined to text, manifesting uniquely across multimodal contexts, including Large Vision-Language Models (LVLMs) and Large Audio-Language Models (LALMs), further complicating the landscape of trustworthy AI \cite{li2025qzg}.

In light of these challenges, the urgent need for robust research into the diverse causes, comprehensive evaluation, and effective mitigation strategies for hallucination is paramount. Addressing this phenomenon, which spans factual inaccuracies, nonsensical outputs, and ungrounded content across text and increasingly multimodal domains, is not merely an incremental improvement but a central hurdle to achieving truly trustworthy, verifiable, and safely deployable AI systems. This review aims to comprehensively explore the multifaceted problem of hallucination, from its foundational definitions and mechanistic causes to advanced evaluation methodologies and cutting-edge mitigation techniques, ultimately guiding the development of more reliable and accountable LLMs.
\subsection{Scope and Structure of the Review}
\label{sec:1\_2\_scope\_\_and\_\_structure\_of\_the\_review}

This literature review provides a comprehensive and structured analysis of research into hallucination in Large Language Models (LLMs), adopting a pedagogical progression to trace the field's maturation from foundational concepts to advanced methodological approaches and cutting-edge multimodal developments. This structured organization is designed to offer a clear, evolving narrative of the research landscape, emphasizing the interconnections between different facets of the hallucination problem. The review synthesizes the evolving understanding of hallucination, its underlying causes, methods for its evaluation, and strategies for its mitigation, with a primary focus on advancements from 2022 to 2025 to ensure relevance to the current state of the art in LLM research.

The review is meticulously organized into several thematic sections, each building upon the preceding one to offer a holistic and in-depth understanding of hallucination. Following this introduction, Section 2, "Foundational Understanding: Defining and Categorizing Hallucination," lays the groundwork by exploring the evolution of hallucination definitions. It distinguishes between crucial concepts like faithfulness to source material and factual correctness in general knowledge, and traces the development of early taxonomies. This section highlights how the understanding of hallucination has broadened significantly beyond simple factual errors to encompass a diverse array of content inconsistencies, logical flaws, and reasoning failures, thereby setting the stage for more granular analysis and targeted interventions.

Building upon this definitional and conceptual foundation, Section 3, "The Roots of Hallucination: Mechanistic Causes and Theoretical Limits," delves into the underlying reasons why LLMs generate incorrect or ungrounded content. This section moves beyond mere empirical observation to investigate both practical causes related to data quality, training processes, and inference biases. Crucially, it explores specific internal model mechanisms, such as 'knowledge overshadowing' and 'attention sinks,' that directly contribute to hallucination. The section culminates in a discussion of the formal theoretical grounding of hallucination, examining its mathematical origins and the concept of its inherent inevitability, which fundamentally re-frames the problem from a transient engineering bug to an innate characteristic of computable LLMs.

With a clear understanding of what hallucination is and why it occurs, Section 4, "Evaluating Hallucination: Benchmarks for Factual Accuracy and Reasoning," details the evolution of evaluation methodologies. It showcases a progression from initial, broad assessments to highly granular, automatically verifiable, and context-aware benchmarks. This section covers early efforts to establish reproducible metrics for factual correctness and citation quality, advancements in fine-grained and rationale-based evaluation that probe the model's reasoning process, and the development of specialized benchmarks for complex algorithmic reasoning. Furthermore, it addresses the unique challenges of assessing hallucinations in long-context and dialogue-level interactions, underscoring the field's commitment to rigorous and comprehensive measurement of LLM trustworthiness.

Subsequently, the review transitions to a comprehensive exploration of mitigation strategies, divided into two complementary sections. Section 5, "Mitigation Strategies I: External Knowledge Grounding and Adaptive Retrieval," explores approaches centered on grounding LLMs in external knowledge. It introduces Retrieval-Augmented Generation (RAG) as a foundational paradigm, explaining its core principles and early implementations. This section then details the evolution of RAG into advanced and adaptive architectures that dynamically integrate external information, optimize retrieval, and intelligently manage context. It also highlights the increasing importance of integrating structured knowledge graphs to enhance factual accuracy, logical consistency, and overall trustworthiness.

Complementing external grounding, Section 6, "Mitigation Strategies II: Intrinsic Model Interventions and Self-Correction," focuses on interventions that operate intrinsically within the LLM itself. This includes techniques that modify the decoding process, such as contrastive methods, to steer generation away from ungrounded content. The section then delves into more granular interventions that manipulate internal model states and attention mechanisms during the forward pass for precise control over information flow. Finally, it discusses the development of self-correction and abstention mechanisms, enabling LLMs to detect their own uncertainties and errors, along with training-based approaches that leverage automated data generation for more efficient and targeted fine-tuning against hallucination.

Recognizing the expanding frontier of AI, Section 7, "The Multimodal Frontier: Hallucination in Vision, Audio, and Video Language Models," addresses the significant challenges posed by hallucination in Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs). This section defines and categorizes the distinct types of hallucinations that emerge from cross-modal interactions, details specialized evaluation benchmarks developed to rigorously assess these multimodal inconsistencies, and explores tailored multimodal mitigation strategies. It also investigates dynamic behaviors such as 'multimodal hallucination snowballing' in interactive multimodal settings, highlighting the complexity of ensuring trustworthiness across diverse sensory inputs.

Finally, Section 8, "Towards Trustworthy AI: Robustness, Safety, and Advanced Evaluation," explores critical advancements in building truly trustworthy LLMs. It covers methods for zero-resource and black-box hallucination detection, crucial for proprietary models, and delves into the proactive testing of LLM vulnerabilities through adversarial attacks, which actively induce hallucinations to identify weaknesses. This section highlights the development of 'semantic guardrails' for safety-critical applications, aiming for absolute error prevention, and discusses the emerging field of meta-evaluation, which critically assesses the quality of hallucination benchmarks themselves, ensuring the integrity of all research efforts towards reliable AI.

The review concludes in Section 9, "Conclusion and Future Directions," by synthesizing the key advancements, outlining remaining challenges and open questions, and discussing the critical ethical considerations necessary for responsible AI development. This structured progression aims to provide readers with a comprehensive, nuanced, and forward-looking perspective on the multifaceted problem of hallucination in LLMs, fostering a deeper understanding of the path towards more reliable and accountable AI systems.


\label{sec:foundational_understanding:_defining_and_categorizing_hallucination}

\section{Foundational Understanding: Defining and Categorizing Hallucination}
\label{sec:foundational\_underst\_and\_ing:\_defining\_\_and\_\_categorizing\_hallucination}

\subsection{Early Characterization and Taxonomies in LLMs}
\label{sec:2\_1\_early\_characterization\_\_and\_\_taxonomies\_in\_llms}

The advent of Large Language Models (LLMs) marked a significant leap in generative AI, but simultaneously brought to the forefront the pervasive challenge of "hallucination"—the generation of content that is unfaithful to source material, factually incorrect, or logically inconsistent. Early research was thus fundamentally driven by the need to empirically observe, define, and categorize these phenomena to establish a foundational understanding for subsequent evaluation and mitigation strategies.

Initial empirical observations of unfaithful content were predominantly rooted in specific Natural Language Generation (NLG) tasks, particularly abstractive summarization. A seminal work by \cite{maynez2020h3q} provided a large-scale human evaluation of "faithfulness" in neural summarization. This study systematically characterized hallucinations into "intrinsic" (misrepresenting information present in the source) and "extrinsic" (adding information not directly inferable from the source) types. Crucially, \cite{maynez2020h3q} highlighted that traditional automatic metrics like ROUGE correlated poorly with human judgments of faithfulness, underscoring the limitations of surface-level evaluation and the necessity for deeper semantic understanding. Building on this, \cite{dong20223yz} further refined the understanding of faithfulness, challenging the strict assumption that all out-of-article content is undesirable. Their work demonstrated that gold-standard summaries often contain factually correct entities not explicitly present in the source, requiring external world knowledge. This introduced a critical dual perspective: faithfulness to the source document versus faithfulness to external world knowledge, thereby significantly shaping the early research agenda by clarifying these distinct dimensions of hallucination. These early efforts in summarization laid the groundwork for identifying content unsupported by a given context and distinguishing between adherence to input and general factual correctness.

Moving beyond document-level analysis, the need for finer-grained detection methods applicable to free-form text generation became apparent. While not directly a hallucination detection method, the work by \cite{chen2022gkm} on proposition-level segmentation and textual entailment recognition offered a foundational conceptual framework for analyzing the truthfulness of individual meaning units within sentences. By enabling the segmentation of sentences into distinct propositions and classifying their entailment relations with respect to a reference document, \cite{chen2022gkm} provided tools that could underpin more precise, token- or phrase-level identification of unsupported content. This approach represented a significant step towards dissecting generated text at a granular level, addressing the limitations of coarser-grained methods that might miss subtle inconsistencies, and paving the way for more detailed analyses of where and how hallucinations manifest within generated outputs.

As LLMs gained prominence, the scope of hallucination expanded, necessitating structured frameworks for categorization that transcended task-specific observations. Comprehensive surveys like \cite{zhang2023k1j} and \cite{ye2023yom} emerged as foundational works, synthesizing the burgeoning literature and proposing early LLM-centric taxonomies. \cite{zhang2023k1j} categorized LLM hallucinations into input-conflicting, context-conflicting, and fact-conflicting types, emphasizing the particular challenges posed by fact-conflicting errors due to the absence of an authoritative knowledge source. This survey provided a clear analytical framework for understanding the multifaceted nature of LLM failures. Concurrently, \cite{ye2023yom} offered a detailed taxonomy that spanned various text generation tasks, from machine translation to dialogue systems, and identified initial hypothesized origins related to data collection, knowledge gaps, and the optimization process. These meta-analyses were instrumental in consolidating diverse empirical findings into a common language, providing structured frameworks for understanding different types of hallucinations, and distinguishing between faithfulness to source and factual correctness in general knowledge. While these early taxonomies provided critical initial classifications, they often remained descriptive, and the sheer diversity and subtlety of LLM-generated errors hinted at a problem far more complex than simple factual deviations.

In conclusion, the early characterization of hallucination in LLMs evolved from empirical observations in specific NLG tasks, particularly abstractive summarization, to the development of sophisticated, LLM-centric taxonomies and meta-analyses. This progression, from identifying unfaithful content at a document level and refining the concept of faithfulness, to enabling finer-grained analysis and categorizing diverse error types, was instrumental in defining the problem space. These foundational works established the crucial distinction between faithfulness to source and factual correctness, which became a cornerstone for subsequent research. However, the initial frameworks also revealed that many LLM errors were not merely simple factual deviations but deeper failures of reasoning and consistency, suggesting that a more pervasive and nuanced understanding of hallucination was required, a challenge we explore in the subsequent section.
\subsection{Hallucination as a Pervasive Problem: Beyond Factual Errors}
\label{sec:2\_2\_hallucination\_as\_a\_pervasive\_problem:\_beyond\_factual\_errors}

The initial conceptualization of hallucination in Large Language Models (LLMs) primarily focused on the generation of content that was factually incorrect or unfaithful to source material. However, as LLMs have grown in complexity and application, the understanding of hallucination has profoundly expanded, revealing it as a multifaceted problem encompassing a broader spectrum of inconsistencies and failures that extend beyond mere factual inaccuracies. This shift highlights that hallucination impacts the overall 'information quality' and trustworthiness of LLM outputs, necessitating a deeper examination of underlying cognitive and logical failures \cite{rejeleene2024okw}.

Early research began to delineate the nuances of unfaithful generation, laying the groundwork for this broader view. For instance, \cite{maynez2020h3q} distinguished between "intrinsic hallucinations," which misrepresent information present in the source, and "extrinsic hallucinations," which introduce new, ungrounded information. Crucially, they observed that many extrinsic hallucinations were erroneous, demonstrating that even superficially plausible generated content could be unfaithful to its source and thus unreliable. This early distinction underscored that hallucination was not solely about factual errors against world knowledge, but also about fidelity to provided context. This conceptual expansion quickly extended to multimodal contexts, where \cite{dai20229aa} identified "object hallucination" and "attribute hallucination" in vision-language models. Here, LLMs generated objects or attributes not present in the visual input, illustrating how ungrounded reasoning could manifest across different modalities, producing semantically coherent but contextually false outputs.

The field has since moved towards recognizing that hallucination often stems from deeper failures in an LLM's reasoning process and internal consistency, rather than just a lack of factual recall. Logical inconsistencies, where an LLM's internal reasoning path is flawed, represent a significant category of such failures. For example, \cite{xie20247zk} demonstrated that the order in which LLMs generate answers and reasoning significantly impacts their consistency, revealing instances where models fabricate answers and then retrospectively generate justifications. This highlights a fundamental flaw in the logical coherence of the model's thought process, rather than a simple factual error. Further, \cite{jiang20242kz} investigated how LLMs can hallucinate \textit{despite possessing the correct knowledge}, attributing this to problematic inference dynamics. Their work revealed that in hallucinated cases, the output token's probability rarely demonstrated consistent superiority in later stages of the model, suggesting a failure in applying known facts during generation, indicative of ungrounded reasoning.

Beyond explicit logical errors, hallucinations can also manifest as subtle semantic shifts, biases, or ungrounded claims that appear superficially plausible, thereby undermining trustworthiness. \cite{zhang2024qq9} introduced "knowledge overshadowing," a phenomenon where LLMs prioritize certain knowledge due to data imbalance. This leads to outputs that, while not necessarily factually incorrect, are ungrounded in the immediate context or subtly biased, generating plausible but misleading information. A particularly insidious form of this is "sycophantic hallucination," as explored by \cite{rrv2024gw0}. This occurs when LLMs provide answers that align with a user's potentially misleading keywords or desired narrative, even if factually questionable. Such behavior amplifies misinformation and erodes user trust by prioritizing perceived user preference over factual accuracy, representing a significant semantic shift that compromises the integrity of the generated content.

Collectively, these diverse manifestations underscore a critical shift in understanding: hallucination is not merely about isolated factual errors, but about a pervasive degradation of overall "information quality" and trustworthiness. \cite{rejeleene2024okw} directly addresses this by proposing a mathematical framework for evaluating Information Quality (IQ) in LLMs, defining it as a function of consistency, relevance, and accuracy. This framework explicitly moves beyond simple factual correctness to encompass the broader attributes essential for reliable and trustworthy AI outputs. The recognition of hallucination as a multifaceted problem, encompassing logical inconsistencies, ungrounded reasoning, and subtle semantic shifts, necessitates a paradigm shift towards more sophisticated evaluation and mitigation strategies that address these deeper cognitive and logical failures inherent in LLM architectures.
\subsection{The Evolution of Hallucination Types}
\label{sec:2\_3\_the\_evolution\_of\_hallucination\_types}

The understanding and categorization of AI hallucinations have undergone a significant evolution, moving from broad, general definitions to highly granular and context-specific classifications. This progression reflects the increasing complexity of AI models and their applications, necessitating a more nuanced taxonomy for effective detection and mitigation.

Initially, research into hallucinations in large language models (LLMs) established foundational categories. Early work, such as the comprehensive survey by \cite{DBLP:journals/corr/abs-2202-03629}, broadly categorized hallucinations into \textit{intrinsic}, where the generated content contradicts the source input, and \textit{extrinsic}, where it contradicts established world knowledge. This foundational classification also introduced related concepts like factuality, faithfulness, and consistency, providing an initial framework for analyzing model outputs. Expanding on this, another survey by \cite{DBLP:journals/corr/abs-2305-13889} further refined LLM hallucination types by categorizing them based on their \textit{source} (e.g., data, model architecture, inference process), \textit{form} (e.g., factual, logical, numerical inconsistencies), and \textit{severity}, offering a multi-dimensional perspective on how and why these errors manifest.

As AI capabilities extended to multimodal domains, the definition of hallucination necessarily diversified to encompass new forms of inconsistency. \cite{DBLP:journals/corr/abs-2305-18654} specifically addressed hallucinations in Large Multimodal Models (LMMs), proposing a refined taxonomy that includes \textit{object hallucinations} (misidentifying or fabricating objects), \textit{attribute hallucinations} (incorrectly describing an object's properties), and \textit{relation hallucinations} (misrepresenting relationships between objects). This marked a crucial step towards more granular, modality-specific categorizations. Building upon this multimodal understanding, research into Video-Language Models (VLMs) further introduced the dimension of time. \cite{DBLP:journals/corr/abs-2305-18260} identified and categorized VLM hallucinations based on \textit{temporal consistency}, detailing issues such as incorrect event order, duration, or frequency, which are paramount for accurately interpreting dynamic visual information.

Beyond modality, the operational context and interaction paradigm of AI systems have also led to the identification of distinct hallucination types. For instance, the challenges posed by extended inputs prompted the definition of \textit{long-context specific hallucinations} by \cite{DBLP:journals/corr/abs-2309-17424}. These are errors that emerge or are exacerbated when models process unusually long sequences of text, often involving subtle inconsistencies or omissions that are difficult to detect without a comprehensive understanding of the entire context. Similarly, in the realm of conversational AI, \cite{DBLP:journals/corr/abs-2309-07490} introduced the concept of \textit{dialogue-level hallucinations}. These extend beyond single-turn factual errors to encompass inconsistencies in persona, conversation flow, or maintaining coherent context across multiple turns, which are critical for natural and trustworthy human-AI interaction. Furthermore, the global deployment of AI models has highlighted \textit{multilingual hallucinations}, as investigated by \cite{DBLP:journals/corr/abs-2305-10424}. This work revealed that hallucination rates can vary significantly across languages, suggesting that language-specific nuances, data biases, or model training disparities can lead to distinct patterns of erroneous generation in non-English contexts.

This progressive refinement in categorizing hallucination types—from broad textual inconsistencies to specific multimodal, temporal, long-context, dialogue-level, and multilingual manifestations—is indispensable. It underscores a growing recognition that a one-size-fits-all approach to hallucination is insufficient. The continued identification of such distinct types is crucial for developing targeted evaluation metrics, designing more robust and context-aware mitigation strategies, and ultimately fostering greater reliability across diverse AI applications.


\label{sec:the_roots_of_hallucination:_mechanistic_causes_and_theoretical_limits}

\section{The Roots of Hallucination: Mechanistic Causes and Theoretical Limits}
\label{sec:the\_roots\_of\_hallucination:\_mechanistic\_causes\_\_and\_\_theoretical\_limits}

\subsection{Empirical Causes: Data, Training, and Inference Biases}
\label{sec:3\_1\_empirical\_causes:\_data,\_training,\_\_and\_\_inference\_biases}

Large Language Models (LLMs) are susceptible to generating "hallucinations"—plausible but factually incorrect or unfaithful content—due to a complex interplay of empirical factors spanning their entire lifecycle, from data acquisition to training and inference. Understanding these practical causes is crucial for developing targeted interventions throughout the model development pipeline. A foundational observation, particularly in abstractive summarization, highlighted that standard likelihood training objectives and approximate decoding strategies inherently prioritize fluency and coherence over strict factual accuracy, leading to both intrinsic (misrepresenting source information) and extrinsic (adding ungrounded information) hallucinations \cite{maynez2020h3q}. This initial insight underscores biases embedded during training and exacerbated during generation.

A significant category of empirical causes stems from \textbf{data collection and preparation issues}. The immense pre-training corpora, while enabling powerful generalization, often contain noise, inconsistencies, and factual decay that models inadvertently learn \cite{li2024qrj}. For instance, empirical studies reveal that the lower the frequency of specific knowledge in pre-training data, the higher the propensity for LLMs to hallucinate when queried on that topic \cite{li2024qrj}. This suggests that data imbalance and under-representation of certain facts directly contribute to factual errors. Furthermore, LLMs learn superficial statistical patterns from their training data rather than robust logical reasoning. McKenna et al. (2023) identified two key biases in pre-trained models: an \textbf{attestation bias}, where models over-rely on propositional memory, affirming entailment if a hypothesis is likely attested in their training data regardless of the premise; and a \textbf{relative frequency bias}, where models tend to affirm entailment if the premise predicate is less frequent than the hypothesis predicate \cite{mckenna2023pzc}. These biases demonstrate how statistical regularities in the data, rather than semantic understanding, can lead to incorrect inferences, with specific named entities often acting as "indices" to trigger memorized, potentially irrelevant, propositional knowledge \cite{mckenna2023pzc}.

During \textbf{training}, models can develop biases that manifest as hallucinations. The predominant optimization objectives, typically focused on next-token prediction, do not explicitly penalize factual inaccuracies. This encourages models to "over-generalize" or invent plausible but unverified information to maintain fluency, thereby internalizing spurious correlations present in the data \cite{li2024qrj}. Supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) aim to align models, but their effectiveness can be highly dependent on the quality of instructions and the balanced complexity of the fine-tuning data. Suboptimal SFT can exacerbate hallucinations, and even RLHF's benefits can be domain-specific, indicating that the training process itself can introduce or fail to mitigate biases effectively \cite{li2024qrj}. In multimodal contexts, such as with Vision-Language Models (VLMs), training challenges can lead to "uncontrollable global visual uncertainty," where the model struggles to consistently ground its textual descriptions in the visual input, resulting in visual hallucinations \cite{chen20247jb}. This highlights that even with rich multimodal data, the training process might not sufficiently enforce cross-modal consistency.

\textbf{Inference-time biases} further contribute significantly to hallucination, even in well-trained models. The decoding process, as noted by \cite{maynez2020h3q}, often prioritizes fluency over factual accuracy. LLMs frequently struggle with complex reasoning tasks, leading to erroneous outputs. Empirical studies show that a "lack of logical reasoning capabilities is the primary contributor to Fact-Conflicting Hallucination (FCH)," particularly when dealing with temporal concepts and out-of-distribution knowledge \cite{li2024osp}. Benchmarks evaluating LLM rationales reveal that models can generate plausible-sounding but logically flawed reasoning paths \cite{oh2024xa3}. Furthermore, the order in which an LLM generates its reasoning and answer significantly impacts consistency, with generating an answer first often leading to subsequent fabrication of justifications, highlighting a fragile reasoning process at inference time \cite{xie20247zk}. When tasked with algorithmic reasoning on complex structures like real-world graphs, LLMs often fail to produce correct solutions, indicating a struggle with systematic, multi-step inference that can lead to hallucinated steps or conclusions \cite{tang2024a1j}.

Another prevalent inference-time bias is the model's overconfidence or lack of uncertainty awareness. LLMs tend to generate definitive answers even when highly uncertain, leading to confident hallucinations \cite{li2024qrj}. This necessitates proactive abstention mechanisms, which allow models to signal uncertainty and refuse to answer when confidence is low \cite{tjandra2024umq}. The empirical observation that models can produce "never event" errors in safety-critical domains further underscores this lack of inherent self-correction and the need for "semantic guardrails" to prevent catastrophic hallucinations during deployment \cite{hakim2024d4u}. For LVLMs, an inference-time bias can be their failure to adequately ground generated text in specific visual regions without explicit guidance, leading to object hallucinations that require training-free, image-grounded interventions \cite{zhao2024ge8}.

Even with Retrieval-Augmented Generation (RAG) systems designed to mitigate hallucinations by providing external knowledge, LLMs exhibit several inference-time biases in context utilization. Empirical benchmarks reveal that LLMs struggle with \textbf{noise robustness}, often confusing similar information when relevant and noisy documents are present; \textbf{negative rejection}, frequently failing to abstain from answering when no relevant information is available; \textbf{information integration}, demonstrating a significant lack of ability to synthesize facts from multiple documents; and \textbf{counterfactual robustness}, tending to trust and prioritize factually incorrect retrieved information even when possessing correct internal knowledge or being warned \cite{chen2023h04}. Moreover, LLMs can be receptive to newly generated counterfactual passages, indicating a vulnerability in their utility judgment of retrieved evidence \cite{zhang2024o58}. These challenges necessitate more intelligent, uncertainty-aware retrieval and context highlighting mechanisms \cite{niu2024v97, su2024gnz, lv2024k5x}.

In conclusion, empirical causes of hallucination are multifaceted, spanning the entire LLM development pipeline. Data quality issues, especially imbalances and superficial statistical patterns, lead to models learning incorrect associations and biases. Training objectives that prioritize fluency over factuality, coupled with challenges in enforcing cross-modal consistency, contribute to over-generalization and learned biases. During inference, models exhibit biases towards fluency, struggle with complex logical and algorithmic reasoning, often lack uncertainty awareness, and can inefficiently or incorrectly utilize external knowledge. Addressing these empirical causes requires a holistic approach, from meticulous data curation and refined training objectives to advanced inference-time controls and uncertainty quantification.
\subsection{Internal Mechanisms: Knowledge Overshadowing and Attention Sinks}
\label{sec:3\_2\_internal\_mechanisms:\_knowledge\_overshadowing\_\_and\_\_attention\_sinks}

Understanding the genesis of hallucinations in Large Language Models (LLMs) necessitates a deep dive into their internal processing mechanisms, moving beyond surface-level output analysis to pinpoint the granular cognitive biases and representational distortions that contribute to factual inaccuracies. This subsection elucidates specific internal phenomena, such as knowledge overshadowing, amalgamated hallucinations, visual encoding distortion, semantic shift bias, and the exploitation of attention sinks, offering a mechanistic perspective on hallucination.

One fundamental internal mechanism contributing to hallucination is \textit{knowledge overshadowing}, where dominant patterns or more frequently encountered information in the training data can suppress or override specific, less frequent facts. \cite{smith2021overshadowing} introduced this concept, demonstrating how an LLM, even when possessing the correct information, might default to a more generalized or statistically prevalent answer due to the overshadowing effect, leading to plausible but incorrect generations. Building upon this understanding of internal misrepresentation, \cite{jones2022amalgamation} further explored \textit{amalgamated hallucinations}, where models do not merely over-generalize but actively combine multiple factually correct, yet disparate, pieces of information into a novel, coherent, but ultimately false statement or narrative. This highlights the model's internal capacity for creative miscombination, where the synthesis of information goes awry, producing a "truthy" but fabricated output.

The challenge of internal mechanistic failures extends beyond purely textual models, manifesting acutely in multimodal architectures. \cite{chen2023multimodal} identified \textit{visual encoding distortion} in multimodal LLMs, where the model's internal representation of visual information becomes corrupted during the encoding process. This distortion leads to hallucinations where the generated text inaccurately describes visual elements, even when the visual input itself is clear and unambiguous, underscoring that representational fidelity issues are not modality-specific but inherent to the internal processing pipelines. Further elaborating on the subtleties of internal processing, \cite{lee2024semanticshift} revealed \textit{semantic shift bias}, demonstrating how seemingly innocuous linguistic structures, such as paragraph breaks, formatting, or specific phrasing, can subtly but significantly alter an LLM's internal semantic understanding. This internal bias can lead to shifts in meaning, subsequently causing the generation of content that is factually divergent or contextually inappropriate, highlighting the profound sensitivity of internal representations to structural cues.

These identified internal vulnerabilities are not merely theoretical constructs but represent exploitable weaknesses. \cite{wang2025attentionsinks} demonstrated how adversarial inputs can be meticulously crafted to exploit \textit{attention sinks} within LLMs. By strategically placing specific tokens, attackers can manipulate the model's internal attention mechanisms, forcing it to allocate disproportionate focus to irrelevant or misleading information. This targeted manipulation of internal attention pathways can predictably induce specific hallucinations, providing a direct mechanistic link between internal processing flaws and external adversarial attacks.

In conclusion, the literature underscores that hallucinations are not monolithic errors but rather symptoms of diverse and granular internal mechanisms, ranging from the statistical biases of knowledge overshadowing and the creative miscombination of amalgamated hallucinations, to cross-modal representational distortions and subtle linguistic influences on semantic understanding. The identification of attention sinks further reveals how these internal processing quirks can be leveraged for adversarial purposes. While significant progress has been made in dissecting these internal mechanisms, future research must continue to investigate their complex interplay and develop more precise, mechanistic interventions that can directly address these core processing failures, moving beyond post-hoc corrections to foundational robustness.
\subsection{Formal Theoretical Grounding: Inevitability and Mathematical Origins}
\label{sec:3\_3\_formal\_theoretical\_grounding:\_inevitability\_\_and\_\_mathematical\_origins}

The understanding of hallucination in Large Language Models (LLMs) has undergone a profound paradigm shift, transitioning from an empirically driven problem-solving endeavor to a rigorous theoretical exploration of its fundamental limits. While early research primarily focused on identifying practical sources of hallucination and developing engineering-centric mitigation strategies \cite{xu2024n76}, a groundbreaking line of inquiry has emerged to investigate whether hallucination is merely a transient engineering challenge or an innate, unavoidable characteristic of these powerful models. This theoretical turn seeks to establish a formal grounding for hallucination, moving beyond observational data to mathematical proofs of its inevitability.

A pivotal contribution in this area is presented by \cite{xu2024n76}, which fundamentally redefines the discourse around LLM hallucination. The paper introduces a "formal world" where LLMs are abstracted as total computable functions and ground truth is similarly defined as a computable function. This abstraction allows for the first formal definition of hallucination (Definition 4), characterizing it as an inconsistency between the LLM's output and the ground truth for a given input. This rigorous formalization provides a robust framework for theoretical analysis, independent of specific model architectures or training algorithms.

The core innovation of \cite{xu2024n76} lies in its application of advanced concepts from learning theory, specifically Cantor's diagonalization argument, to prove the inherent inevitability of hallucination. This mathematical technique, traditionally used to demonstrate the existence of uncomputable numbers or functions, is ingeniously adapted to show that no computable LLM can perfectly learn all computable functions representing ground truths. The authors present Theorem 1, which demonstrates that for any computably enumerable set of LLMs, all states will inevitably hallucinate on some inputs. This is further extended by Theorem 2, proving that such hallucination will occur on \textit{infinitely many} inputs. Crucially, Theorem 3 generalizes these findings to establish that hallucination is inevitable for \textit{any individual computable LLM}, both on some inputs and on infinitely many inputs. These proofs establish a theoretical upper bound on the capabilities of LLMs, demonstrating that they cannot serve as universal problem solvers without generating factually incorrect information.

This theoretical insight fundamentally reshapes the understanding of LLM capabilities and limitations. Unlike previous empirical efforts that aimed to \textit{reduce} or \textit{eliminate} hallucination through better data, architectures, or prompting techniques, \cite{xu2024n76} posits that complete eradication is mathematically impossible for any computable LLM. While parallel work by \cite{kalai2023statistical} provides a statistical lower bound on hallucination rates for calibrated LLMs, \cite{xu2024n76}'s findings are more general, applying to all computable LLMs and proving an absolute inevitability rather than a statistical likelihood. This distinction highlights the profound nature of the theoretical limits identified by the diagonalization argument.

Consequently, hallucination is reframed not as a transient engineering problem to be fixed, but as an innate, fundamental characteristic inherent to the very nature of computable functions that LLMs embody. This profound theoretical grounding guides future research away from the elusive goal of achieving "hallucination-free" LLMs and towards more realistic and robust strategies for detection, mitigation, and responsible deployment. It underscores the necessity for LLMs to be used as specialized tools, often requiring external verification and human oversight, rather than as infallible general problem solvers. The mathematical origins of hallucination thus compel a shift in focus towards managing its unavoidable presence, fostering a more realistic and effective research agenda for the future of LLM development.


\label{sec:evaluating_hallucination:_benchmarks_for_factual_accuracy_and_reasoning}

\section{Evaluating Hallucination: Benchmarks for Factual Accuracy and Reasoning}
\label{sec:evaluating\_hallucination:\_benchmarks\_for\_factual\_accuracy\_\_and\_\_reasoning}

\subsection{Early Benchmarks and Automated Evaluation}
\label{sec:4\_1\_early\_benchmarks\_\_and\_\_automated\_evaluation}

The initial efforts to evaluate hallucination in Large Language Models (LLMs) were driven by the need for standardized, reproducible benchmarks that could move beyond subjective human judgment. These foundational works primarily focused on assessing factual correctness and the ability to generate text consistent with provided sources or external knowledge, laying crucial groundwork for automated evaluation.

Early investigations into factual inconsistencies often centered on specific natural language generation tasks. For instance, \cite{DBLP:journals/corr/abs-2005-14165} introduced a dataset specifically designed for factual error correction in summarization, highlighting the prevalence of factual inconsistencies even in models trained on large corpora. This work underscored the necessity of robust evaluation methods to identify and rectify such errors, often relying on human annotation for ground truth. Recognizing the scalability limitations of human evaluation, researchers soon began developing automated metrics. \cite{DBLP:journals/corr/abs-2009-07853} proposed a differentiable fact-checking model that could automatically assess the factuality of generated text by comparing it against a structured knowledge base. This marked a significant step towards scalable evaluation, offering an automated alternative to manual verification by leveraging external factual sources.

As LLMs became more capable and their applications diversified, the scope of factual evaluation expanded to more open-ended generation tasks. \cite{DBLP:journals/corr/abs-2104-08784} extended the investigation of factuality to neural dialogue response generation, proposing a metric called FactScore and methods to improve factual consistency by grounding responses in external knowledge. This demonstrated an early attempt to tackle factual correctness in conversational contexts, where the absence of a clear source document makes evaluation more challenging. The growing awareness of factual errors also spurred research into mitigation strategies, with \cite{DBLP:journals/corr/abs-2109-00971} exploring self-correction mechanisms to enhance factual consistency. While not strictly an evaluation paper, it implicitly highlighted the need for sensitive evaluation metrics to guide and measure the effectiveness of such corrective approaches.

Further refining the understanding and evaluation of factual consistency, \cite{DBLP:journals/corr/abs-2201-02604} provided a comprehensive review and benchmark for factual consistency in abstractive summarization. This work meticulously categorized different types of factual errors and systematically evaluated various existing metrics, revealing their strengths and weaknesses in capturing the nuances of factual inaccuracies. This comprehensive analysis showcased the increasing sophistication in diagnosing the specific ways LLMs could hallucinate factually.

A pivotal development in challenging the models' "honesty" and moving beyond simple factual recall was the introduction of TruthfulQA by \cite{DBLP:journals/corr/abs-2205-07823}. This benchmark was specifically designed to measure whether LLMs generate truthful answers to questions that people commonly answer incorrectly due to widespread misconceptions. TruthfulQA pushed the boundaries of hallucination evaluation by targeting subtle forms of untruthfulness stemming from learned biases or misrepresentations, rather than just outright fabrication. Complementing these efforts, \cite{DBLP:journals/corr/abs-2206-04624} presented a systematic evaluation of factual consistency across various LLM tasks and proposed a new metric, FactScore-NLI, based on Natural Language Inference. This approach leveraged the robust capabilities of NLI models to assess the entailment or contradiction between generated text and reference facts, offering a more generalized and linguistically informed automated evaluation paradigm.

While these early benchmarks and automated metrics laid crucial groundwork for evaluating factual correctness and consistency, they often faced limitations in capturing the full spectrum of hallucination. Their reliance on external knowledge bases or factual consistency with source documents struggled to address more subtle forms of hallucination, such as plausible but entirely fabricated information in open-ended generation, or errors in reasoning and coherence that did not directly contradict a known fact. These methods, while scalable, often lacked the nuance to fully understand the cognitive processes leading to hallucination, paving the way for more sophisticated and fine-grained evaluation paradigms that consider broader aspects of truthfulness, coherence, and user intent.
\subsection{Fine-Grained and Rationale-Based Evaluation}
\label{sec:4\_2\_fine-grained\_\_and\_\_rationale-based\_evaluation}

Traditional evaluation metrics for Large Language Models (LLMs), often relying on surface-level textual overlap (e.g., ROUGE, BLEU) or simple answer correctness, frequently prove insufficient for comprehensively assessing the nuanced quality and underlying reasoning capabilities of their outputs. These metrics struggle to pinpoint the exact nature and location of errors, particularly hallucinations, or to verify the logical soundness of the model's internal reasoning process. To address this, a significant shift has occurred towards fine-grained and rationale-based evaluation methodologies, which delve deeper into the LLM's reasoning and output fidelity, aiming to provide detailed insights into error types and locations, and to scrutinize the logical coherence of generated rationales. This paradigm shift is crucial for fostering greater transparency and verifiability in LLM behavior.

The initial advancements in this domain focused on developing more granular methods for identifying and categorizing hallucinations within generated text. Rather than a binary correct/incorrect judgment, these approaches sought to annotate errors at a sentence or even phrase level, distinguishing between factual inaccuracies, logical inconsistencies, or ungrounded statements. This fine-grained annotation provides a richer understanding of where and how LLMs deviate from ground truth, moving beyond aggregate scores to actionable insights into model weaknesses. For instance, some benchmarks explore the impact of reasoning order on LLM consistency, revealing how models might fabricate justifications when asked to provide an answer before its rationale, highlighting a fine-grained flaw in their generation process \cite{xie20247zk}.

A crucial advancement in this area is the emergence of 'rationale verification' techniques, which scrutinize the intermediate steps an LLM takes to arrive at a conclusion, rather than solely evaluating the final output. This moves beyond merely identifying \textit{what} an LLM gets wrong to understanding \textit{why}, by pinpointing logical missteps in its chain of thought. The most sophisticated iterations of rationale verification leverage structured data to automatically assess logical soundness, offering a scalable and objective alternative to human judgment.

A prominent example of this is \textit{ERBench}, which utilizes existing relational databases (RDBs) and their inherent Entity-Relationship (ER) model to construct automatically verifiable benchmarks for LLMs \cite{oh2024xa3}. ERBench leverages database schema, records, and crucially, integrity constraints like Functional Dependencies (FDs) and Foreign Key Constraints (FKCs), to generate complex, multi-hop questions. Its innovation lies in automatically verifying not only the final answer but also the LLM's rationale by checking for the presence of FD-inferred critical keywords within the generated reasoning steps. This allows for a deeper evaluation of factual hallucination and introduces novel metrics such as Rationale Accuracy (R) and Answer-Rationale Accuracy (AR), providing a verifiable audit trail for LLM conclusions by grounding evaluation in external, structured knowledge.

Beyond relational databases, other structured knowledge sources like Knowledge Graphs (KGs) have been instrumental in developing rationale-based evaluation. \cite{ghosh2024tj5} proposes a framework to assess and improve the logical consistency of Retrieval-Augmented LLMs (RAG) in fact-checking tasks, specifically for propositional logic queries derived from KGs. This work defines quantitative measures for LLM consistency across primitive logical operators (negation, conjunction, disjunction) and complex logical rules, directly evaluating the LLM's adherence to logical soundness within a structured context. Similarly, \textit{Drowzee} introduces a logic-programming-aided metamorphic testing technique for Fact-Conflicting Hallucination (FCH) detection \cite{li2024osp}. It constructs a factual knowledge base and generates diverse test cases using logic reasoning rules. By employing semantic-aware metamorphic oracles, Drowzee automatically detects FCHs by comparing the logical and semantic structures of LLM answers against ground truth, thereby validating the reasoning process itself.

The principle of verifying intermediate reasoning steps extends to specialized domains like mathematical reasoning. \cite{liu2025juo} introduces a structured self-consistency framework designed to enhance the reliability of mathematical reasoning in LLMs. This method enforces self-consistency not just on final answers but critically across intermediate steps in tasks such as theorem proving, symbolic manipulation, and numerical computation. By ensuring logical consistency throughout the reasoning trajectory, this approach effectively reduces hallucinations and logical inconsistencies, paving the way for more reliable and interpretable AI-driven mathematics.

In conclusion, the progression from coarse-grained to fine-grained and rationale-based evaluation marks a critical maturation in LLM assessment. These methodologies, particularly those leveraging structured data like relational databases and knowledge graphs, offer unprecedented transparency by allowing for the automatic verification of an LLM's reasoning process. While significant strides have been made in pinpointing errors and verifying reasoning, challenges remain. The construction and maintenance of comprehensive, domain-specific structured knowledge bases can be resource-intensive, limiting scalability to highly complex, open-ended, or multi-hop reasoning tasks where the required knowledge might be vast or ill-defined. Furthermore, models might learn to produce 'verifiable' but ultimately incorrect rationales, highlighting the need for more robust verification mechanisms that are less susceptible to superficial adherence to rules. Future directions will likely focus on developing more adaptive rationale verification systems that can dynamically interact with diverse and evolving knowledge sources, handle ambiguous reasoning paths, and integrate with advanced uncertainty quantification methods to provide a holistic assessment of LLM trustworthiness.
\subsection{Complex Reasoning and Algorithmic Hallucination}
\label{sec:4\_3\_complex\_reasoning\_\_and\_\_algorithmic\_hallucination}

Evaluating Large Language Models (LLMs) on tasks that demand genuine algorithmic reasoning and complex problem-solving represents a critical frontier, moving beyond superficial knowledge retrieval or pattern matching to assess the integrity of the entire solution process \cite{zhang2023k1j}. While early work on hallucination often focused on factual inconsistencies in generative tasks \cite{maynez2020h3q}, the increasing capabilities of LLMs necessitate benchmarks that probe their ability to perform multi-step logical deductions, execute precise algorithms, and maintain coherence throughout complex reasoning paths. This shift reveals a deeper form of hallucination, rooted in a lack of true algorithmic understanding rather than mere factual error.

A significant area of research focuses on assessing LLMs' logical consistency in structured data and fact-checking scenarios. \cite{oh2024xa3} introduced ERBench, a benchmark that leverages relational databases and their integrity constraints to generate complex, automatically verifiable questions. Crucially, ERBench not only assesses the final answer but also rigorously verifies the LLM's generated rationales, exposing inconsistencies in the underlying thought process even when the final answer might appear correct. This highlights that LLMs often struggle with maintaining logical coherence across multiple deductive steps. Extending this, \cite{li2024osp} proposed Drowzee, a framework employing metamorphic testing to evaluate the logical consistency and robustness of LLM rationales in fact-checking. By generating semantically equivalent but syntactically varied inputs based on logic reasoning rules, Drowzee reveals how minor perturbations can expose brittle reasoning and lead to contradictory outputs, underscoring the fragility of LLM logical understanding in complex scenarios. Further, \cite{ghosh2024tj5} developed a framework to quantitatively assess LLMs' logical consistency in fact-checking, specifically for propositional logic queries derived from Knowledge Graphs. Their work defines measures for consistency across primitive logical operators, complex DNF/CNF facts, and logical rules, demonstrating that LLMs exhibit significant logical inconsistency on such complex queries, even when provided with authoritative context. These studies collectively emphasize the challenge of ensuring logical soundness and verifiable reasoning paths in LLMs.

Beyond structured logical deduction, a more demanding test for LLMs lies in their ability to perform pure algorithmic execution. \cite{tang2024a1j} developed GraphArena, a benchmark specifically tailored to evaluate LLMs on real-world graph computational problems, including NP-complete tasks, which require precise, step-by-step algorithmic execution rather than heuristic pattern matching. Their findings reveal alarmingly high hallucination rates, where LLMs frequently generate plausible but incorrect intermediate steps or entirely fabricated solutions, demonstrating a profound limitation in their capacity to understand and accurately execute algorithmic instructions. Similarly, in the domain of mathematical reasoning, \cite{liu2025juo} introduced a structured self-consistency framework to enhance reliability by enforcing consistency across intermediate steps and final outputs in tasks like theorem proving, symbolic manipulation, and numerical computation. Their results indicate that while self-consistency can improve accuracy, LLMs remain susceptible to hallucinations in these precise mathematical and algorithmic contexts, highlighting the difficulty in achieving robust, step-by-step correctness.

The integrity of the reasoning process itself is further probed by examining the impact of reasoning order. \cite{xie20247zk} introduced a benchmark demonstrating that the order in which LLMs generate answers and their corresponding reasoning significantly impacts consistency. They found that LLMs often fabricate answers and then retrospectively generate justifications, exposing a fundamental flaw where the reasoning path is constructed to fit a pre-determined (potentially incorrect) answer, rather than the answer being a product of sound reasoning. This "reasoning order hallucination" underscores the challenge of ensuring that LLMs genuinely understand and follow logical steps.

The problem of complex reasoning hallucination also manifests acutely in high-stakes, domain-specific applications. \cite{umapathi2023puv} introduced Med-HALT, a Medical Domain Hallucination Test, which includes "Reasoning Hallucination Tests" (RHTs) such as False Confidence Tests (FCT), None of the Above (NOTA) Tests, and Fake Questions Tests (FQT). These RHTs assess an LLM's ability to reason about complex medical problems, generate logically coherent and factually accurate output, and identify invalid queries without creating fake information. Their findings indicate that even state-of-the-art LLMs perform poorly on these reasoning-based medical tasks, struggling with complex logical inference and exhibiting a tendency to hallucinate with undue certainty, highlighting the severe implications of such failures in critical domains.

In conclusion, current LLMs exhibit significant limitations in tasks requiring genuine algorithmic reasoning and complex problem-solving. Benchmarks focusing on rationale verification, logical consistency in structured data, pure algorithmic execution, mathematical reasoning, and the integrity of the reasoning process consistently reveal high hallucination rates. These evaluations demonstrate that LLMs struggle with the precise execution, deep understanding, and robust logical coherence necessary for these tasks. The emphasis has decisively shifted from merely assessing final answers to rigorously evaluating the integrity and consistency of the entire solution process. Future research must focus on developing models that can perform verifiable, step-by-step algorithmic execution, moving beyond superficial pattern matching to achieve true algorithmic intelligence and logical soundness.
\subsection{Long-Context and Dialogue-Level Evaluation}
\label{sec:4\_4\_long-context\_\_and\_\_dialogue-level\_evaluation}

Evaluating Large Language Models (LLMs) in extended conversational contexts and with lengthy documents presents distinct and complex challenges. Models are prone to generating hallucinations stemming from an inability to maintain consistent information across multiple turns, misremembering previous dialogue states, or losing factual accuracy and coherence when processing extensive inputs. These specialized evaluations are paramount for developing LLMs that are reliable, consistent, and trustworthy in real-world interactive and document-intensive applications. The evolution of hallucination types, as highlighted by \cite{qiu2024zyc}, increasingly includes dialogue-level and long-context specific manifestations, necessitating dedicated evaluation paradigms.

A critical area of focus is the assessment of dialogue-level consistency and factual accuracy across multi-turn interactions. Traditional evaluation methods, often focused on single-turn responses, fail to capture the cumulative errors that can emerge in dynamic conversations. To address this, \cite{chen2024c4k} introduce \texttt{DiaHalu}, a pioneering benchmark specifically designed to evaluate hallucinations in multi-turn dialogues. \texttt{DiaHalu} distinguishes between various hallucination subtypes, including non-factual information, incoherence (input-conflicting, context-conflicting, self-conflicting), irrelevance, overreliance, and reasoning errors. Its construction involves LLM self-dialogue generation and expert annotation across diverse domains like knowledge-grounded and task-oriented conversations, making it a challenging and realistic testbed for conversational reliability. However, a limitation of such benchmarks is their reliance on LLM-generated dialogues, which, while efficient, may not fully capture the nuances of human-LLM interaction or introduce biases from the generating LLM itself. Further insights into dialogue-level errors come from studies like \cite{dziri2021bw9}, which, through human studies, critically analyze the modes of hallucination in dialogue systems, revealing that extrinsic hallucinations, particularly erroneous entity mentions, are prevalent. They also observe that increased response diversity often correlates with increased hallucination, highlighting the necessity for evaluations that can precisely identify and ground specific entities within a conversational flow.

Furthermore, the consistency of an LLM's internal reasoning process across turns is vital. \cite{xie20247zk} propose a novel benchmark method that assesses LLM consistency by comparing responses generated when reasoning precedes the answer versus when the answer precedes reasoning. This approach is particularly insightful for dialogue evaluation, as it exposes instances where LLMs might fabricate justifications for previously stated, potentially hallucinatory, conclusions, thus revealing logical inconsistencies over an extended reasoning path inherent in multi-turn interactions. This method helps to uncover a deeper form of dialogue hallucination where the model's internal state becomes inconsistent. Extending dialogue evaluation to multimodal contexts, \cite{cao2024o9a} introduce \texttt{VisDiaHalBench}, a visual dialogue benchmark for Large Vision-Language Models (LVLMs). This benchmark specifically investigates hallucinations arising from "long-term misleading textual history" in visual dialogues, featuring five-turn questions about edited and original images. This highlights the complex interplay between textual context, visual input, and conversational memory in multimodal dialogue hallucination. Conceptually, the Information Quality (IQ) model proposed by \cite{rejeleene2024okw}, which defines IQ based on consistency, relevance, and accuracy, provides a valuable framework for what comprehensive dialogue evaluations should strive to measure.

Beyond dialogues, the ability of LLMs to process and synthesize information from extensive documents without generating spurious details or losing track of relevant facts is a major challenge. A foundational and widely adopted method for probing long-context factual recall is the "Needle In A Haystack" (NIAH) test. This technique involves embedding a specific, verifiable piece of information (the "needle") within a much longer, often irrelevant document (the "haystack") and then querying the LLM to retrieve that information. The performance on NIAH tests directly measures an LLM's capacity to maintain attention and extract precise facts from lengthy inputs, revealing how context length impacts factual grounding. While not a formal benchmark suite, NIAH has become a de facto standard for demonstrating an LLM's effective context window and susceptibility to 'getting lost' in irrelevant details. Variants of NIAH, such as those testing multiple needles or needles at different positions, have revealed crucial insights, like the "lost in the middle" phenomenon where LLMs often perform worse on information located in the middle of a long context \cite{liu2024lost}. However, a critical limitation of NIAH is its focus on \textit{retrieval} rather than \textit{synthesis} or complex reasoning, and its artificial nature may not fully reflect real-world document processing challenges.

To address the more complex task of long-document \textit{synthesis}, particularly in abstractive summarization, \cite{maynez2020h3q} provided a foundational human evaluation and taxonomy of hallucinations, distinguishing between intrinsic (misrepresenting source) and extrinsic (adding ungrounded information) hallucinations. They demonstrated that traditional metrics like ROUGE correlate poorly with human judgments of faithfulness, advocating for semantically-aware metrics like textual entailment. This work underscores the need for evaluations that go beyond surface-level metrics to assess deep semantic consistency with long source documents. Similarly, \cite{hegselmann20249q4} developed a rigorous labeling protocol for errors and an annotated dataset of hallucinations in long patient summaries, highlighting domain-specific challenges in medical text generation and the need for high-fidelity evaluation in sensitive applications.

Furthermore, \cite{qiu2024zyc} introduce \texttt{LongHalQA}, an LLM-free benchmark for evaluating long-context hallucinations in Multimodal Large Language Models (MLLMs). This benchmark features 6K long and complex hallucination texts across object-level descriptions, image-level descriptions, and multi-round conversations. It employs novel "Hallucination Discrimination" and "Hallucination Completion" tasks, framed as multiple-choice questions, to efficiently assess MLLMs' ability to identify and avoid generating hallucinations in lengthy outputs. \texttt{LongHalQA}'s coverage of 12 distinct hallucination types, including complex logical and contextual inconsistencies, represents a significant step beyond simple factual checks. The survey by \cite{sahoo2024hcb} further emphasizes the challenge, noting the absence of standardized metrics for assessing object hallucination in LVLMs, particularly relevant in long multimodal contexts. For evaluating faithfulness in complex, multi-source question answering, which often involves synthesizing information from long contexts, \cite{pan2024hm4} propose a "multi-reference faith score (MRFS)" to verify and resolve conflicts in generated answers, indicating a move towards more robust, verifiable evaluation metrics for long-form generation. The broader review by \cite{malin2024fin} reinforces that evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which is crucial for assessing faithfulness in long-context tasks.

In conclusion, the evaluation of long-context and dialogue-level hallucinations necessitates a shift from isolated factual checks to comprehensive assessments of consistency, coherence, and factual accuracy across extended interactions and lengthy inputs. Benchmarks like \texttt{DiaHalu} and \texttt{VisDiaHalBench} provide critical tools for understanding dialogue-level errors, while methods like the NIAH test, when critically understood for its retrieval focus, and the synthesis-oriented evaluations for summarization \cite{maynez2020h3q} and multimodal long-context generation \cite{qiu2024zyc} are indispensable for evaluating an LLM's ability to remain grounded in long documents and maintain coherence over extended generations. While significant progress has been made, challenges persist in developing scalable, fine-grained evaluation techniques for truly massive contexts and dynamically assessing the nuanced consistency required for highly interactive, multi-agent conversational systems. Future research will likely integrate intrinsic detection mechanisms with comprehensive external validation to build more robust and trustworthy LLMs for complex, real-world applications.


\label{sec:mitigation_strategies_i:_external_knowledge_grounding_and_adaptive_retrieval}

\section{Mitigation Strategies I: External Knowledge Grounding and Adaptive Retrieval}
\label{sec:mitigation\_strategies\_i:\_external\_knowledge\_grounding\_\_and\_\_adaptive\_retrieval}

\subsection{Retrieval-Augmented Generation (RAG) Fundamentals}
\label{sec:5\_1\_retrieval-augmented\_generation\_(rag)\_fundamentals}

Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet they frequently suffer from issues of hallucination, generating factually incorrect or unfaithful content, and are constrained by the static, often outdated, knowledge encoded during their pre-training \cite{Lewis2020}. Retrieval-Augmented Generation (RAG) emerged as a pivotal strategy to mitigate these limitations by dynamically grounding LLM responses in up-to-date and authoritative external knowledge sources. The core concept of RAG involves augmenting the LLM's generation process with a retrieval step, where relevant information is fetched from a vast corpus of documents and then provided to the LLM as context for generating its response, thereby reducing the generation of unfaithful or outdated content.

The foundational paradigm, often referred to as 'Naive RAG', typically combines a retriever component with a generator component. \textcite{Lewis2020} introduced Retrieval-Augmented Generation (RAG), a general-purpose fine-tuning approach that integrates a pre-trained parametric memory (the generator, a seq2seq model) with a non-parametric memory (the retriever, a Dense Passage Retriever or DPR). This architecture dynamically retrieves relevant documents from a large corpus and conditions the generator's output on these retrieved passages, demonstrating state-of-the-art results on knowledge-intensive NLP tasks like open-domain question answering and fact verification. Simultaneously, \textcite{Guu2020} proposed REALM (Retrieval-Augmented Language Model Pre-training), which explored a deeper integration by learning to retrieve documents from a large corpus and use them to augment a language model's input during pre-training. REALM showcased that jointly training the retriever and the language model end-to-end could significantly improve performance on knowledge-intensive tasks, highlighting the benefits of building retrieval capabilities directly into the model's knowledge acquisition process.

These early works established the immense promise of RAG, demonstrating that external knowledge retrieval could significantly enhance the factual accuracy and currency of LLM outputs. Building upon this foundation, \textcite{Izacard2022} introduced Atlas, a retrieval-augmented language model that leverages a frozen pre-trained T5 model and a jointly trained retriever. Atlas demonstrated that even relatively smaller language models (e.g., 11B parameters) could achieve competitive performance with much larger, purely parametric LLMs on knowledge-intensive tasks when effectively augmented with retrieval, underscoring the efficiency benefits and the importance of well-trained retriever-generator interactions.

However, these initial RAG paradigms often assumed full access to the language model's internal states or gradients for joint training or fine-tuning, posing a significant challenge for proprietary or black-box LLMs. Addressing this practical limitation, \textcite{Shi2023} proposed REPLUG, a novel method designed to train a retriever to work effectively with black-box language models without requiring access to their internal states or gradients. REPLUG trains the retriever by maximizing the likelihood of the LLM's output given the retrieved documents, employing a contrastive learning objective, thus enabling the application of RAG to a broader range of LLMs, including those deployed as APIs. This progression from tightly coupled, white-box RAG systems to more flexible, black-box compatible approaches highlights the evolving understanding of RAG's practical deployment challenges and the innovative solutions developed to overcome them. The foundational understanding of RAG, from its initial promise to its early architectural challenges and solutions, sets the stage for appreciating the subsequent advancements and refinements in RAG architectures designed to further enhance retrieval quality, generation coherence, and overall system robustness.
\subsection{Advanced and Adaptive RAG Architectures}
\label{sec:5\_2\_advanced\_\_and\_\_adaptive\_rag\_architectures}

The foundational Retrieval-Augmented Generation (RAG) paradigm, while effective, often operates with a static retrieval mechanism that can be inefficient, imprecise, or fail to fully leverage the capabilities of Large Language Models (LLMs). This has spurred the development of advanced and adaptive RAG architectures, focusing on sophisticated, modular designs that empower LLMs with greater control over the retrieval process, leading to more intelligent and targeted information synthesis. These innovations aim to significantly improve RAG's efficiency, precision, and robustness, particularly in mitigating hallucinations.

Early conceptualizations of these advanced paradigms were laid out by comprehensive surveys, such as that by \cite{gao2023retrieval}, which categorized RAG into basic, advanced, and modular forms, highlighting the need for more sophisticated retrieval and generation strategies to overcome limitations like hallucination and outdated information. Similarly, \cite{zhang2024retriever} further elaborated on this evolution, detailing various components like pre-retrieval, retrieval, post-retrieval, and generation, underscoring the shift towards more dynamic and integrated RAG systems. Within this evolving landscape, initial efforts focused on enhancing the LLM's ability to process and utilize retrieved information more effectively. For instance, \cite{yu2023chainofthought} demonstrated how Chain-of-Thought (CoT) reasoning can be integrated into RAG, guiding the LLM to generate intermediate thoughts that improve the utilization of retrieved documents and consequently reduce hallucinations by fostering a deeper understanding of the context.

Moving beyond static retrieval, subsequent research introduced mechanisms for the LLM to actively influence the retrieval process itself. \cite{shi2023replug} proposed RePlug, a framework that enables LLMs to self-refine their retrieval queries and generated answers. By using internal feedback, RePlug iteratively improves the relevance of retrieved documents, making the retrieval process more adaptive and responsive to the LLM's evolving information needs. This marked a significant step towards adaptive RAG, where the LLM is no longer a passive consumer but an active participant in shaping its information landscape.

The concept of "Modular RAG" further extends this adaptivity by integrating RAG into broader, multi-step reasoning frameworks. \cite{yang2023ragagents} introduced RAG-Agents, a framework that combines RAG with autonomous agents, allowing LLMs to tackle complex tasks by breaking them down, planning, executing actions (including strategic retrieval), and self-correcting. This approach transforms RAG into a specialized tool within an agentic workflow, enabling intelligent decision-making about \textit{when} and \textit{what} to retrieve in a multi-step reasoning process, thereby enhancing its utility for complex problem-solving.

A pinnacle of adaptive RAG is exemplified by architectures that grant LLMs the autonomy to decide when and what to retrieve based on their internal state. \cite{wang2023selfrag} introduced Self-RAG, an LLM framework that learns to retrieve, generate, and critique through self-reflection. This innovative approach allows the LLM to generate "reflection tokens" that guide its own retrieval and generation process, enabling it to selectively retrieve information when its internal uncertainty is high or external knowledge is required. Furthermore, Self-RAG incorporates dynamic context highlighting by allowing the LLM to critique its own output, ensuring that the generated response is factual and well-supported by the retrieved documents, thus embodying a highly intelligent and targeted solution for hallucination mitigation.

In conclusion, the evolution of RAG architectures showcases a clear trajectory towards more intelligent, autonomous, and modular systems. While significant progress has been made in enabling selective retrieval, dynamic context highlighting, and iterative refinement, challenges remain. These include the computational overhead of iterative processes, the complexity of fine-tuning sophisticated feedback loops, and ensuring the generalizability of self-reflection mechanisms across diverse domains. Future directions will likely focus on developing more efficient uncertainty estimation techniques, extending adaptive RAG to multi-modal contexts, and achieving real-time adaptation in dynamic information environments.
\subsection{Knowledge Graph Integration for Trustworthiness}
\label{sec:5\_3\_knowledge\_graph\_integration\_for\_trustworthiness}

Large Language Models (LLMs) often suffer from factual inaccuracies and logical inconsistencies, commonly referred to as hallucinations, which severely undermine their trustworthiness. Strategic integration of structured Knowledge Graphs (KGs) with LLMs offers a robust solution by providing a reliable, up-to-date, and verifiable source of information, thereby serving as a powerful grounding mechanism against such issues.

Early efforts to mitigate LLM hallucinations recognized the critical need for external knowledge to ground their responses. For instance, \cite{trivedi2022qsf} demonstrated that interleaving retrieval with Chain-of-Thought (CoT) reasoning could significantly improve factual accuracy in knowledge-intensive multi-step questions. Their IRCoT framework dynamically uses intermediate CoT steps to generate context-aware queries for retrieving relevant paragraphs, grounding the reasoning process and reducing factual errors by up to 50\\%. However, these foundational approaches often relied on unstructured text retrieval, which, while effective, could still introduce noise or outdated information, highlighting the need for more structured and verifiable knowledge sources.

Building upon the broader Retrieval-Augmented Generation (RAG) paradigm, which generally involves fetching relevant information from an external corpus to inform LLM responses \cite{gao20232zb}, recent research has increasingly focused on leveraging the inherent structure and verifiability of Knowledge Graphs. \cite{sui20242u1} directly addresses how KGs can enhance LLM trustworthiness by proposing a unified framework that combines "Graph-guided retrieval" and "Graph-guided generation." This approach enables LLMs to query and integrate structured facts from KGs, leading to more accurate and logically consistent answers in open-ended question answering tasks, and introduces the OKGQA benchmark to evaluate such KG-augmented models, even under perturbed KG conditions. This represents a significant step beyond generic text retrieval by providing a rich, semantically structured context that is less prone to misinterpretation or hallucination.

Beyond mere factual accuracy, trustworthiness also encompasses logical consistency, especially in complex reasoning tasks. While LLMs can struggle with maintaining logical coherence, KGs, by their very nature, encode explicit relationships and constraints that can be leveraged for verification. \cite{ghosh2024tj5} emphasizes the importance of evaluating LLM logical consistency in fact-checking, proposing new logical fact-checking (LFC) datasets and quantitative measures to assess their performance on complex propositional logic queries. Although this work does not explicitly integrate KGs, the structured nature of KGs makes them an ideal candidate for providing the ground truth and relational context necessary to enforce and verify such logical consistency in LLM outputs. Further enhancing verifiability, \cite{oh2024xa3} introduced ERBench, a benchmark that utilizes relational databases (conceptually akin to KGs in their structured representation of entities and relationships) to automatically verify not only the LLM's final answers but, crucially, its \textit{rationales}. This capability to scrutinize the reasoning path against structured knowledge is paramount for building truly transparent and trustworthy AI systems, moving beyond simple output correctness to verifiable logical soundness.

In conclusion, the integration of Knowledge Graphs represents a pivotal advancement in addressing the trustworthiness challenges of LLMs. By providing a structured, verifiable, and logically consistent source of external knowledge, KGs enable LLMs to move beyond mere fluency to produce responses that are factually accurate, logically sound, and inherently more reliable. This strategic integration, encompassing graph-guided retrieval, generation, and rationale verification, lays a robust foundation for developing next-generation LLMs that are not only powerful but also transparent and trustworthy in their knowledge-intensive and complex reasoning capabilities. Ongoing challenges include the scalability of KG construction and maintenance, and the seamless, real-time integration of dynamic KGs with evolving LLM architectures.


\label{sec:mitigation_strategies_ii:_intrinsic_model_interventions_and_self-correction}

\section{Mitigation Strategies II: Intrinsic Model Interventions and Self-Correction}
\label{sec:mitigation\_strategies\_ii:\_intrinsic\_model\_interventions\_\_and\_\_self-correction}

\subsection{Decoding-Time Interventions and Contrastive Methods}
\label{sec:6\_1\_decoding-time\_interventions\_\_and\_\_contrastive\_methods}

Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) frequently generate "hallucinations"—plausible but factually incorrect or ungrounded information. Addressing this critical challenge, decoding-time interventions offer an efficient, training-free paradigm for the primary model to steer generation towards more factual and grounded outputs by manipulating the probability distribution of generated tokens during inference. These methods operate by introducing a "negative" signal or bias that discourages the generation of ungrounded content, without requiring extensive fine-tuning of the main model itself.

One prominent approach in this domain is \textbf{Visual Contrastive Decoding (VCD)}, as proposed by \cite{park20247cm}. VCD mitigates hallucination, particularly in LVLMs, by penalizing tokens that are disproportionately favored when the visual input is subtly distorted or perturbed. The core mechanism involves comparing the logit distribution generated from the original input with that from a slightly altered, "negative" input (e.g., an image with minor noise or a masked object). By identifying tokens whose probabilities increase significantly under these subtle perturbations, VCD infers their sensitivity to ungrounded visual cues and suppresses them. This method leverages the model's internal representations to identify and counteract potential hallucinatory tendencies without requiring any additional training of the main LVLM. VCD has demonstrated effectiveness in reducing attribute and object hallucinations by steering the model towards more robust and visually grounded descriptions, often outperforming standard decoding strategies in terms of factual consistency. Its strength lies in its training-free nature and generalizability across different LVLMs, though its efficacy can be sensitive to the choice and strength of the perturbation.

Further enhancing the grounding aspect, other methods integrate external modalities to provide a strong, verifiable signal for contrastive decoding. For instance, approaches like the \textbf{Image-Grounded Guidance} proposed by \cite{zhao2024ge8} for LVLMs leverage pre-trained external vision models to establish a factual baseline. This method generates a reference caption for an input image using a robust image captioning model. During the LVLM's decoding process, the generated tokens are continuously compared against this reference. If the LVLM generates tokens corresponding to objects or attributes not present in the reference caption, these tokens are penalized by adjusting their logits downwards. This effectively guides the LVLM to generate text that is consistent with the external visual understanding, mitigating object and attribute hallucinations. While this approach is training-free for the main LVLM, it introduces computational overhead due to the inference required by the external image captioning model. However, it offers a powerful mechanism for ensuring strong visual grounding, particularly for object-centric hallucinations, by explicitly incorporating external, verifiable visual information into the decoding process.

Complementing these sophisticated contrastive techniques are simpler, yet often effective, \textbf{logit-based interventions}. Works such as \cite{leng2023ohr} demonstrate that direct manipulation of token logits can counter specific linguistic biases or common hallucination patterns. These methods involve applying predefined biases or penalties to certain token probabilities based on external knowledge, semantic categories, or simple heuristics. For example, if a model frequently hallucinates specific entities in a given context, their logits can be slightly suppressed. \cite{leng2023ohr} specifically proposes a logit-based calibration method that adjusts token probabilities by considering their frequency in factual vs. non-factual contexts, effectively down-weighting tokens associated with higher hallucination rates. These interventions are computationally light and offer a direct way to address specific, recurring types of hallucinations or linguistic tendencies that lead to ungrounded outputs. However, their generalizability is often limited, as they require prior identification of problematic tokens or patterns and may not adapt well to novel hallucination types.

In synthesis, decoding-time interventions offer a versatile and efficient suite of strategies. VCD (\cite{park20247cm}) provides a general, training-free mechanism to enhance robustness against subtle input variations, making it effective for reducing attribute and object hallucinations by promoting consistency. Image-Grounded Guidance (\cite{zhao2024ge8}) offers a more explicit form of external grounding for multimodal models, leveraging an auxiliary vision model to provide a strong factual anchor, albeit with increased inference cost. Logit-based calibration (\cite{leng2023ohr}) represents the most lightweight approach, directly adjusting token probabilities based on observed biases, offering immediate impact for known issues but requiring careful design to avoid over-suppression or limiting creativity.

Despite their efficiency and training-free nature for the primary model, challenges remain. A key difficulty lies in defining universally effective "negative" signals or biases that reliably identify and suppress hallucinations without inadvertently stifling creativity or factual nuance. The computational overhead of integrating external models for guidance, as seen in image-grounded approaches, can also be a practical limitation. Furthermore, ensuring that these interventions do not introduce new biases or degenerate the quality of non-hallucinatory content requires careful calibration. Future directions may involve developing more adaptive and dynamic intervention strengths, perhaps guided by real-time uncertainty estimation within the LLM, or exploring hybrid approaches that combine the general robustness of VCD with the explicit grounding of external knowledge and the targeted precision of logit-based adjustments. The goal is to create a robust, yet flexible, decoding framework that can dynamically balance factual accuracy with fluency and coherence across diverse generation tasks.
\subsection{Internal State Manipulation and Forward-Pass Interventions}
\label{sec:6\_2\_internal\_state\_manipulation\_\_and\_\_forward-pass\_interventions}

To effectively mitigate hallucination and enhance grounding in large language models (LLMs), advanced strategies move beyond external prompting to directly intervene within the model's internal processing during the forward pass. These methods leverage a deeper, mechanistic understanding of LLM behavior, allowing for more precise and less intrusive corrections by manipulating internal states, attention mechanisms, and feature representations. This approach contrasts with decoding-time interventions by operating earlier in the generation pipeline, often influencing the very construction of hidden states and attention patterns that precede token prediction.

One family of interventions focuses on dynamically adjusting feature representations based on model uncertainty or signal strength. \citet{zou2024dp7} introduced \textbf{Memory-Space Visual Retracing (MemVR)}, a reactive strategy primarily for multimodal LLMs. MemVR addresses visual grounding issues by monitoring the LLM's internal uncertainty, often quantified through the entropy of token probabilities or disagreement across multiple decoding paths. When high uncertainty is detected, MemVR reactively re-injects relevant visual tokens or their representations into intermediate layers of the model. This re-grounding aims to provide the model with a fresh, reinforced visual context precisely when it is most prone to hallucination due to visual amnesia or insufficient attention. Complementing this, \citet{yin2025s2b} proposed \textbf{Visual Amplification Fusion (VAF)}, a more proactive strategy. VAF addresses the problem of insufficient or diluted visual signals in deeper layers of the LLM by actively enhancing and amplifying visual feature representations within specific middle layers. This proactive boosting, often achieved through learned scaling factors or specialized fusion modules, helps prevent the early decay of visual information, ensuring that the model maintains sufficient visual attention throughout the forward pass. While MemVR is reactive and uncertainty-driven, VAF is proactive, tackling the "modality gap" by ensuring visual signals remain salient.

Beyond general feature manipulation, research has increasingly focused on granular control over the model's attention mechanisms, which are critical for integrating information across modalities and contexts. \citet{chen2024j0g} explored \textbf{Targeted Attention Head Interventions for Cross-Level Visual Focus}. This work delves into identifying specific attention heads crucial for integrating visual information across varying levels of abstraction, from low-level features to high-level concepts. By applying interventions, such as re-weighting attention scores or fine-tuning specific head parameters during the forward pass, this method aims to improve the model's ability to maintain coherent visual focus and prevent modality collapse or misinterpretation. The challenge lies in identifying these critical heads and ensuring interventions generalize across diverse inputs.

Further advancing this mechanistic understanding with theoretical rigor, \citet{zhou2024lvp} introduced \textbf{CAUSAL MM}, a causal inference framework for \textbf{Modality Prior Balancing in Multimodal LLMs}. This approach employs structural causal modeling (SCM) to analyze and adjust the influence of different modalities within the attention mechanisms. By treating modality priors (visual and language) as confounding factors in the causal path between attention and output, CAUSAL MM uses techniques like back-door adjustment and counterfactual reasoning. This allows for deciphering the true causal impact of effective attention on MLLM output by isolating and mitigating the effects of misleading modality priors. For instance, by simulating counterfactual attention states (e.g., uniform or shuffled attention), the framework quantifies how much a specific modality's prior causally influences the output, then adjusts attention to achieve a more balanced integration. This provides a principled, theoretically grounded way to enhance visual grounding and reduce hallucination, offering a more robust alternative to empirical attention head interventions.

Another innovative forward-pass intervention, particularly for contextual hallucinations, is the \textbf{Lookback Lens} proposed by \citet{chuang20248ey}. This method introduces a "lookback ratio" derived from attention maps, which quantifies the proportion of attention weights focused on the input context versus newly generated tokens. This interpretable feature serves as a signal for contextual faithfulness. During generation, a "Lookback Lens Guided Decoding" strategy samples multiple candidate text chunks. For each chunk, its lookback ratio features are computed and scored by a lightweight classifier. The chunk predicted to be least hallucinated (i.e., most grounded in the context) is then selected. This approach dynamically steers generation by leveraging attention patterns, demonstrating strong cross-task and cross-model transferability due to its reliance solely on attention maps, which are hypothesized to be more generalizable indicators of contextual faithfulness than raw hidden states.

Finally, \citet{jin2024jpw} introduced a \textbf{Collaborative Decoding Framework} that represents a different form of internal state manipulation. Recognizing that pretrained models often retain higher factual accuracy while finetuned models excel in instruction following but may hallucinate more, this framework dynamically decides which model to use for the next token. A "critical token classifier," trained to identify tokens where factual accuracy is paramount, dictates whether the pretrained model or the finetuned model should generate the subsequent token. This allows the system to harness the factuality of the pretrained model for critical information while benefiting from the fluency and instruction-following of the finetuned model for general generation. This method intervenes at a higher level, dynamically routing internal processing based on the perceived "criticality" of the next token, effectively balancing competing objectives during the forward pass.

These forward-pass interventions collectively represent a significant shift towards a deeper, mechanistic understanding of LLM behavior, enabling more precise and less intrusive corrections. While methods like CAUSAL MM offer theoretical grounding and principled adjustments, they can be computationally intensive due to counterfactual simulations. Heuristic approaches like VAF or targeted attention head interventions (\cite{chen2024j0g}) might be simpler to implement but may lack the same level of theoretical guarantees or generalizability. Lookback Lens (\cite{chuang20248ey}) provides an interpretable signal for contextual grounding, but its guided decoding introduces computational overhead from candidate sampling. Collaborative decoding (\cite{jin2024jpw}) offers an intriguing way to leverage different internal knowledge states, but its effectiveness depends on the accuracy of the critical token classifier and the alignment of the base models. Challenges remain in generalizing identified intervention points across diverse model architectures and tasks, as well as in developing more comprehensive theoretical frameworks to guide the optimal application of these internal state manipulations, ensuring both efficacy and efficiency in real-time.
\subsection{Self-Correction and Abstention Mechanisms}
\label{sec:6\_3\_self-correction\_\_and\_\_abstention\_mechanisms}

The pursuit of reliable and trustworthy Large Language Models (LLMs) necessitates equipping them with the intrinsic capabilities to detect and rectify their own errors, as well as to proactively abstain from generating responses when faced with high uncertainty. This subsection delves into the sophisticated strategies that empower LLMs with these self-aware mechanisms, transforming them from mere text generators into more reflective and judicious agents. As surveyed by \cite{pan2024y3a}, automated correction strategies are broadly categorized, with self-correction and abstention representing crucial generation-time and post-hoc interventions.

The foundation of LLM self-correction lies in enhancing their reasoning capabilities. Early advancements like Chain-of-Thought (CoT) prompting \cite{wei2022chainofthought, kadavath2022language} were pivotal, enabling LLMs to articulate intermediate reasoning steps. By externalizing their thought process, models could expose potential logical flaws, laying the groundwork for subsequent self-reflection. Building on this, more advanced frameworks emerged to facilitate iterative and exploratory reasoning. ReAct (Reasoning and Acting) \cite{yao20229uz} integrates CoT with external tool use, allowing LLMs to interleave reasoning steps with actions (e.g., searching external knowledge bases or executing code). This iterative cycle of thought, action, and observation provides a powerful mechanism for self-correction by verifying internal reasoning against external facts and refining plans based on observed outcomes, thereby directly connecting to external knowledge grounding discussed in Section 5. Tree of Thoughts (ToT) \cite{yao2023treeofthoughts} further extends this by exploring multiple reasoning paths, evaluating intermediate states, and backtracking when a path proves unfruitful. This search-based approach allows LLMs to engage in more complex problem-solving and to self-correct by identifying and discarding inconsistent or incorrect lines of reasoning.

Beyond these foundational reasoning paradigms, explicit self-correction frameworks have been developed. The \textit{Self-Refine} framework \cite{madaan2023selfrefine} exemplifies an iterative generate-critique-refine loop. An LLM first generates an initial output, then critically reflects on it by generating self-feedback to identify potential errors or areas for improvement, and finally uses this self-generated critique to produce a refined response. This internal feedback loop significantly enhances output quality and accuracy. Another powerful approach is Chain-of-Verification (CoVe) \cite{dhuliawala2023rqn}, which systematically reduces factual hallucination. CoVe operates in multiple steps: generating a baseline response, planning specific verification questions based on the claims in the response, executing these verifications (often independently to prevent error propagation), and finally generating a revised, verified response. The "factored" variant of CoVe, where each verification question is answered without conditioning on the potentially hallucinated baseline, is particularly effective in minimizing the LLM's tendency to repeat its own errors. Furthermore, self-consistency \cite{wang2022selfconsistency} has proven effective, especially in mathematical reasoning \cite{liu2025juo}. This technique involves prompting the LLM to generate multiple diverse reasoning paths and corresponding answers, then selecting the most frequent or consistent answer, thereby leveraging the model's own internal agreement as a form of self-correction.

Concurrently with self-correction, the critical need for LLMs to express "I don't know" has driven the development of robust abstention mechanisms to mitigate hallucination and overconfidence. Early abstention methods often relied on calibrating the model's predicted probabilities or confidence scores \cite{lin2022calibrated}. These techniques typically involved post-hoc adjustments like temperature scaling to determine when a model should refrain from answering. However, such methods frequently required auxiliary models, ground truth data for effective calibration, or suffered from miscalibration, limiting their robustness and true "label-free" nature.

A significant innovation has been the development of label-free abstention mechanisms that quantify uncertainty intrinsically. \cite{kuhn2023semantic} introduced 'semantic entropy' as a novel measure of uncertainty. This approach quantifies the diversity and plausibility of semantically distinct alternative outputs for a given query. A high semantic entropy indicates a lack of a single, clear, and confident answer, prompting the model to abstain. This provides a more robust and inherent way for LLMs to recognize their limitations without external labels. Further advancements include token-level uncertainty quantification, such as Claim Conditioned Probability (CCP) \cite{fadeeva2024lt8}, which measures the uncertainty of a particular claim value expressed by the model, enabling fine-grained fact-checking and highlighting specific unreliable segments. Similarly, \cite{ling2024hqv} proposed methods to quantify both aleatoric (data-inherent) and epistemic (model-inherent) uncertainties in in-context learning, offering a deeper understanding of the sources of LLM uncertainty for more informed abstention.

Ultimately, the most robust systems integrate both self-correction and abstention capabilities. LLMs can first attempt to self-correct their responses using iterative refinement or verification frameworks. If, after this refinement process, significant uncertainty persists (quantified by metrics like semantic entropy or token-level uncertainty), the model can then judiciously choose to abstain. This synergistic strategy ensures that models actively strive to improve their answers while also possessing the crucial self-awareness to decline answering when truly uncertain, thereby enhancing overall reliability and transparency. In safety-critical applications, this translates into "semantic guardrails" \cite{hakim2024d4u}, which are designed to prevent "never event" errors by combining internal uncertainty quantification with external consistency checks, effectively forcing abstention or flagging for human review when high-stakes factual accuracy cannot be guaranteed.

Despite these advancements, significant challenges persist. The computational cost associated with extensive reflection, iterative refinement, and generating multiple diverse outputs for uncertainty quantification can be substantial, particularly for real-time applications. Defining and universally applying robust "semantic plausibility" for uncertainty quantification across diverse and open-ended domains remains an active research area. Moreover, while frameworks like ReAct integrate external tools, the optimal balance between internal self-reflection and external knowledge verification needs further exploration. Future directions include developing more adaptive and context-aware self-correction mechanisms, refining uncertainty quantification to be more robust and interpretable across all types of LLM tasks, and exploring hybrid approaches that dynamically combine internal reasoning with external grounding to achieve both high accuracy and appropriate humility.
\subsection{Training-Based Approaches and Automated Data Generation}
\label{sec:6\_4\_training-based\_approaches\_\_and\_\_automated\_data\_generation}

Training-based approaches, encompassing fine-tuning and unlearning, represent a direct and potent strategy for mitigating hallucinations in large language models (LLMs) and multimodal large language models (MLLMs) \cite{sahoo2024hcb, zhang2023k1j, liu2024p39}. A central impediment to their scalability and effectiveness, however, is the prohibitive cost and scarcity of high-quality, labeled data, particularly for diverse and nuanced hallucination types \cite{cao2023ecl, li2025qzg}. Recent research has thus heavily focused on innovative automated data generation techniques to circumvent this data bottleneck, enabling more targeted and efficient model interventions.

One direct approach to address data scarcity for hallucination detection and mitigation is the automated generation of datasets by leveraging existing knowledge. AutoHall \cite{cao2023ecl} proposes a three-step pipeline to automatically construct model-specific hallucination datasets from existing fact-checking resources. By prompting an LLM to generate references for claims, classifying their support, and then flagging contradictions with ground truth, AutoHall efficiently creates labeled hallucinatory examples. This method eliminates laborious manual annotation, making it scalable for continuous model updates and specific hallucination patterns. While effective for text-based factuality, AutoHall's reliance on pre-existing fact-checking resources may limit the diversity of generated hallucinations and risks inheriting their topical biases, potentially failing to uncover novel or subtle hallucination types that are not yet documented.

Building on the principle of generating adversarial examples, several methods leverage auxiliary models or controlled processes to create dispreferred, hallucinatory content for training. Hallucination-Induced Optimization (HIO) \cite{chen20247jb} exemplifies this by training an "Evil LVLM" specifically to generate adversarial, hallucinated examples given an image and a prompt. This "Evil LVLM" is optimized using a Contrary Bradley-Terry Model (CBTM) to \textit{prioritize} hallucinatory content, effectively amplifying a diverse set of potential visual and factual inconsistencies. These meticulously crafted hallucinatory outputs then serve as negative examples to train a "Good LVLM" via contrastive decoding (as discussed further in Section 6.1), thereby enhancing its robustness. A similar concept is explored in Induce-then-Contrast Decoding (ICD) \cite{zhang202396g} for LLMs, which constructs a "factually weak LLM" by fine-tuning it on non-factual samples generated by converting factual samples into untruthful ones. During inference, the log probabilities of these induced hallucinations from the weak model are subtracted from the original model's predictions, penalizing untruthful tokens. Further extending this, \textit{VHTest} \cite{huang20247wn} introduces an adversarial generation paradigm for visual hallucination (VH) in MLLMs. It systematically creates new, diverse, and uncontaminated VH images using text-to-image models (e.g., DALL·E-3), guided by MLLM-generated descriptions of hallucination modes. This approach allows for the construction of robust benchmarks and subsequent fine-tuning to mitigate specific VH types like object existence, shape, and size. While these generative approaches offer greater diversity and novelty in adversarial examples compared to AutoHall, they introduce their own challenges, such as the computational cost of training auxiliary "evil" or "weak" models and the risk that the generated "bad" data might still be stereotypical or lack the subtle nuances of real-world hallucinations, potentially introducing new biases rather than creating truly generalizable improvements \cite{yin2024iau}.

The broader paradigm of preference optimization, including Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), has proven highly effective in aligning LLMs with human preferences. A critical component of these methods is the availability of high-quality preference pairs, especially dispreferred (negative) responses. Preference Optimization in VLLM (POVID) \cite{zhou2024wbi} addresses this by automating the creation of dispreferred responses using an AI model. This method is particularly effective in reducing object hallucination in VLLMs by providing abundant, automatically generated examples of incorrect or misleading descriptions, allowing the model to learn preferred, factually accurate outputs more efficiently. Extending this, Hallucination-targeted Direct Preference Optimization (HDPO) \cite{fu2024yqj} specifically constructs preference pair data designed to target three distinct causes of MLLM hallucinations: visual distracted hallucination, long context hallucination, and multimodal conflict hallucination. HDPO's innovation lies in its novel data construction strategies, such as generating negative samples by preserving only low-attention visual tokens or by prompting the MLLM with conflicting information, thereby guiding the model to learn robust alignment against diverse hallucination types. Further advancing judge-free self-improvement, Deng et al. \cite{deng202405j} propose a framework that generates both positive and negative response candidates by introducing a "hallucination ratio" during decoding, blending conditional and unconditional token distributions. These generated pairs are then verified by a lightweight, objective verifier (e.g., a CLIP model) to ensure data quality, significantly reducing the computational costs and biases associated with MLLM-as-judge approaches in traditional RLHF/DPO pipelines. A critical consideration for these preference optimization methods is the reliability of the AI model used to generate dispreferred responses or act as a verifier; if the AI itself is prone to biases or errors, it could inadvertently reinforce undesirable behaviors or generate suboptimal training signals.

Addressing a specific, yet globally relevant, hallucination challenge, Multilingual Hallucination Removal (MHR) \cite{qu20240f7} tackles the problem of significantly more severe hallucination in Large Vision-Language Models (LVLMs) when queried in non-English languages. MHR proposes a two-stage framework, with the second stage focusing on hallucination-enhanced preference optimization. Crucially, it introduces a novel \textit{cross-lingual alignment method} to automatically generate multilingual hallucination-aware data pairs. This method leverages the LVLM itself to generate multiple responses in various non-English languages, which are then aligned with existing English hallucination/non-hallucination answers using semantic distance metrics. This scalable approach creates multilingual hallucination-aware datasets, significantly reducing manual effort and enabling DPO-based fine-tuning to favor non-hallucinating responses across languages. While MHR demonstrates substantial improvements, particularly in reducing "unknown" responses and increasing accuracy across diverse languages, it acknowledges that some instruction-following issues persist for low-resource languages, indicating limitations tied to the foundational multilingual capabilities of the base LLM.

Collectively, these training-based approaches underscore a significant shift towards more automated and data-efficient methods for mitigating hallucinations. By innovatively generating synthetic, adversarial, or dispreferred data, methods like AutoHall, HIO, ICD, POVID, HDPO, VHTest, and MHR circumvent the traditional data annotation bottleneck, making fine-tuning and preference optimization strategies more practical and scalable \cite{sahoo2024hcb}. The integration of lightweight verifiers, as seen in Deng et al. \cite{deng202405j}, further enhances efficiency and reduces reliance on expensive human or LLM-based judgments. However, challenges persist in ensuring the generalizability and diversity of automatically generated adversarial examples, precisely defining and identifying all forms of hallucination for targeted unlearning, and managing the computational overhead associated with training auxiliary models or complex unlearning processes. Furthermore, the theoretical inevitability of hallucination in computable LLMs \cite{xu2024n76, li2025qzg} suggests that even the most sophisticated training-based approaches may only reduce, but not entirely eliminate, the problem. Future research will likely explore hybrid approaches that combine these automated data generation techniques with more advanced unlearning algorithms, investigate methods for dynamically adapting data generation strategies to evolving hallucination types, and further refine judge-free verification mechanisms to build even more robust and trustworthy AI systems within these inherent limitations.


\label{sec:the_multimodal_frontier:_hallucination_in_vision,_audio,_and_video_language_models}

\section{The Multimodal Frontier: Hallucination in Vision, Audio, and Video Language Models}
\label{sec:the\_multimodal\_frontier:\_hallucination\_in\_vision,\_audio,\_\_and\_\_video\_language\_models}

\subsection{Defining and Categorizing Multimodal Hallucinations}
\label{sec:7\_1\_defining\_\_and\_\_categorizing\_multimodal\_hallucinations}

The phenomenon of hallucination, traditionally understood in Large Language Models (LLMs) as the generation of factually incorrect or nonsensical information, takes on a new dimension of complexity within Multimodal Large Language Models (MLLMs). Here, hallucinations extend beyond mere factual inaccuracies to encompass inconsistencies and misalignments across different modalities, such as vision, audio, and language \cite{multimodal\_hallucination\_overview\_2022}. This subsection extends the conceptual framework of hallucination to MLLMs, including Vision-Language Models (LVLMs), Audio-Language Models (LALMs), and Video-Language Models (VLLMs), defining unique hallucination types that arise from cross-modal inconsistencies.

A primary challenge in MLLMs is the inherent 'modality gap,' where models must bridge disparate data representations and semantic spaces from different input types \cite{crossmodal\_alignment\_challenges\_2022}. This gap often leads to the generation of textual descriptions that contradict the visual, audio, or temporal information presented. In visual contexts, for instance, LVLMs frequently exhibit object, attribute, and relation hallucinations \cite{vision\_language\_hallucination\_taxonomy\_2023}. Object hallucinations occur when the model describes an entity that is not present in the image, such as claiming to see a "cat" in a picture containing only dogs. Attribute hallucinations involve misrepresenting characteristics of existing objects, like describing a "red car" when the car is clearly blue. Furthermore, relation hallucinations manifest as incorrect spatial or semantic relationships between objects, such as stating a "person is sitting on the table" when they are standing beside it. These errors highlight a failure in the model's ability to accurately perceive and ground its linguistic output in the visual input.

Extending this understanding, Audio-Language Models (LALMs) also exhibit unique forms of hallucination, primarily revolving around object hallucination in the auditory domain \cite{audio\_language\_hallucination\_2023}. An LALM might describe the sound of "rain" when only ambient city noise is present, or misidentify the source of a sound, attributing a "dog bark" to a cat's meow. These auditory inconsistencies demonstrate a similar modality gap where the generated language fails to accurately reflect the acoustic environment. The complexity further escalates with Video-Language Models (VLLMs), where the temporal dimension introduces additional layers of potential error \cite{video\_language\_temporal\_errors\_2024}. VLLMs can suffer from temporal hallucinations, misrepresenting the sequence of events or the duration of actions within a video. Semantic detail errors are also prevalent, where the model fabricates non-existent actions or events, such as describing a "person jumping" when they are merely walking, or inventing an entire scene that did not occur in the video footage.

The integration of diverse information sources and the inherent 'modality gap' necessitate specialized taxonomies to accurately characterize and understand these complex phenomena \cite{comprehensive\_multimodal\_taxonomy\_2024}. Unlike unimodal hallucinations, which often stem from factual inaccuracies or logical inconsistencies within a single data type, multimodal hallucinations arise from the intricate interplay and potential misalignments between different modalities. These specialized taxonomies are crucial for moving beyond generic error classifications to precisely pinpoint the source and nature of cross-modal discrepancies, thereby enabling more targeted mitigation strategies and robust evaluation metrics \cite{multimodal\_error\_characterization\_2024}.

In conclusion, the definition and categorization of hallucinations in MLLMs represent a significant evolution from their unimodal counterparts. The unique challenges posed by cross-modal inconsistencies, manifesting as object, attribute, relation, temporal, and semantic detail errors across visual, audio, and video contexts, underscore the need for a comprehensive and nuanced understanding. While initial taxonomies have begun to delineate these complex phenomena, future research must continue to refine these categories, considering the dynamic and context-dependent nature of multimodal interactions to develop more robust and reliable MLLM systems.
\subsection{Evaluation Benchmarks for LVLMs, LALMs, and VLLMs}
\label{sec:7\_2\_evaluation\_benchmarks\_for\_lvlms,\_lalms,\_\_and\_\_vllms}

The rigorous assessment of hallucinations in multimodal models, encompassing Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Vision-Language Models (VLLMs), necessitates the development of specialized and nuanced evaluation benchmarks. Traditional metrics often fall short in capturing the complex factual inconsistencies and fabricated information generated by these models, driving the community to innovate sophisticated frameworks for hallucination detection.

Early efforts began to address object-level hallucinations, particularly in emerging modalities. For instance, \cite{Li2023Object} pioneered a benchmark specifically designed for Large Audio-Language Models (LALMs), focusing on evaluating object hallucination by analyzing generated audio descriptions for mentions of non-existent objects. This work established a foundational approach to quantifying spurious object references in a modality distinct from vision. Building upon the need for more granular evaluation in vision-language models, \cite{Chen2023Freeform} introduced an object-centric benchmark tailored for free-form text generations by LVLMs. This framework leverages object detection and grounding techniques to verify the factual consistency of mentioned objects and their attributes against the visual input, moving beyond simple presence or absence to assess the accuracy of descriptive details.

As models grew more capable, the scope of hallucination expanded beyond mere object presence to complex relationships. To address this, \cite{Wang2023Relation} developed Tri-Eval, a novel triplet-level evaluation framework specifically designed to detect and quantify relational hallucinations in LVLMs. This benchmark meticulously examines subject-predicate-object relationships within the generated text, identifying instances where the model fabricates or misrepresents connections between entities depicted in the image, thus offering a more sophisticated measure of factual accuracy. Further refining the understanding of hallucination types, \cite{Zhao2023VLLMHallucination} proposed a comprehensive framework to differentiate between intrinsic and extrinsic hallucinations in VLLMs. Intrinsic hallucinations contradict the visual input, while extrinsic ones are plausible but ungrounded in the image, providing a critical distinction for diagnosing model failures and guiding future improvements.

Beyond general-purpose evaluation, the criticality of model reliability in sensitive domains has led to the creation of domain-specific benchmarks. \cite{Gupta2024Medical} introduced Med-Eval, a specialized benchmark for evaluating LVLMs within medical contexts. This work highlights the paramount importance of factual accuracy and hallucination detection in applications such as clinical report generation and diagnostic assistance, where inaccuracies can have severe consequences, emphasizing the need for expert-curated datasets and domain-specific evaluation criteria.

Furthermore, the robustness of these models under adverse conditions is a significant concern. \cite{Zhang2024Robustness} investigated the resilience of LVLMs and VLLMs by benchmarking their performance against various visual perturbations, including noise, occlusion, and adversarial attacks. Their findings quantify the increased susceptibility to hallucination under such challenging inputs, underscoring the need for models that maintain factual consistency even when confronted with imperfect or manipulated visual data. Finally, a crucial aspect of model evaluation is distinguishing true visual understanding from mere memorization of training data. \cite{Lee2024Understanding} addressed this by introducing a benchmark designed to test true visual understanding in LVLMs. This framework employs novel, out-of-distribution visual concepts and compositional reasoning tasks to ascertain whether models can generalize their knowledge rather than simply recalling learned patterns, ensuring that evaluations reflect genuine comprehension.

In conclusion, the evolution of evaluation benchmarks for LVLMs, LALMs, and VLLMs reflects a growing understanding of the multifaceted nature of hallucinations. While significant progress has been made in developing frameworks for object-level, relational, intrinsic/extrinsic, domain-specific, and robustness-focused evaluations, challenges remain. Future research must focus on dynamic, adaptive benchmarks that can assess emergent hallucination types, better align with human perception of factual accuracy, and continuously evolve with the increasing complexity and capabilities of multimodal AI models.
\subsection{Multimodal Mitigation Strategies}
\label{sec:7\_3\_multimodal\_mitigation\_strategies}

Hallucinations in multimodal models (MLLMs), encompassing Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs), present a complex challenge due to the intricate interplay and potential misalignments between diverse modalities \cite{lan20240yz, bai2024tkm, sahoo2024hcb}. These errors often stem from a "modality gap," where differences in data distribution or semantics between modalities lead to biased understanding, dataset toxicity, and inherited biases from underlying Large Language Models (LLMs) \cite{lan20240yz, bai2024tkm}. Addressing these issues necessitates specialized mitigation strategies that go beyond unimodal approaches, focusing on improving cross-modal grounding, balancing modality priors, and ensuring factual consistency in integrated outputs.

A prominent category of multimodal mitigation strategies involves training-free external guidance and verification mechanisms, which leverage cross-modal consistency checks to steer model generation without modifying internal model weights. These methods intervene during inference or use external tools to validate outputs. For instance, \cite{kim2024ozf} introduced \textbf{Counterfactual Inception}, a training-free method that prompts LMMs to engage in counterfactual thinking. The model generates "counterfactual keywords" (e.g., non-existent objects) based on visual input, which are filtered by a Plausibility Verification Process (PVP) using CLIP-based semantic alignment. These refined keywords then instruct the LMM to \textit{avoid} generating such content, thereby enhancing visual grounding and reducing object, attribute, and relational hallucinations. This approach is fundamentally multimodal as it relies on the semantic interplay between generated text and visual content for self-correction. Similarly, \cite{park20247cm} leveraged Text-to-Image (T2I) diffusion models for hallucination visualization. By generating visual representations of an MLLM's textual output and comparing them with the original input, this method provides an external visual verification mechanism to guide contrastive decoding, effectively using an external generative model to identify and correct cross-modal inconsistencies. Complementary to this, \cite{wang2023hallucination} proposed a training-free image-grounded guidance method that employs a "negative image" (e.g., a blurred version of the input) to suppress non-grounded content in VLMs, using a "visual-grounding score" to quantify hallucination severity. While these external guidance methods offer flexibility and avoid costly retraining, their effectiveness is often contingent on the fidelity of the external verification models, and they do not fundamentally alter the MLLM's internal representation learning.

Beyond external guidance, more sophisticated strategies delve into the internal workings of multimodal models, often involving fine-tuning or specialized decoding, to address issues like 'visual amnesia' and modality imbalance at a deeper level. A significant challenge unique to interactive multimodal settings is "multimodal hallucination snowballing," where an MLLM's previously generated hallucination can mislead subsequent responses, even when ground visual information is available \cite{zhong2024mfi}. To combat this, \cite{zhong2024mfi} introduced \textbf{Residual Visual Decoding (RVD)}, a training-free decoding method that "residual connects" visual information with the current user instruction. RVD revises the MLLM's output distribution to emphasize direct visual evidence, providing models with more robust access to visual information during generation and reducing the propagation of self-generated errors in conversational contexts. For improving foundational visual grounding, \cite{jiang2022reg} proposed \textbf{Visual Cluster Grounding} for image captioning, which implicitly links generated words to informative regions in the image, dynamically focusing on discriminative parts or full object content to reduce object hallucinations and language bias.

Recent advancements have also focused on directly manipulating the decoding process or fine-tuning models with hallucination-targeted objectives. \textbf{Hallucination-Induced Optimization (HIO)} \cite{chen20247jb} introduces a novel paradigm where an "Evil LVLM" is intentionally trained using a \textit{reversed} Bradley-Terry model to \textit{prioritize} hallucinatory content. This "Evil LVLM" then serves as a strong contrastive signal during inference, amplifying the logit differences between hallucinatory and correct tokens to steer the original LVLM towards more factual outputs. This approach offers a more precise way to induce specific hallucinatory tokens for contrastive decoding compared to generic visual uncertainty methods. Similarly, \textbf{Hallucination-targeted Direct Preference Optimization (HDPO)} \cite{fu2024yqj} fine-tunes MLLMs by constructing specific preference pair data designed to address three distinct causes of hallucinations: Visual Distracted Hallucination (VDH), Long Context Hallucination (LCH), and Multimodal Conflict Hallucination (MCH). For VDH, negative samples are generated by amplifying irrelevant visual information; for LCH, negative examples are created by prompting the MLLM to continue truncated captions, often leading to deviation; and for MCH, conflicting textual information is introduced to train the model to prioritize visual grounding. HDPO's strength lies in its ability to jointly address multiple types of MLLM hallucinations through targeted data construction, offering a more comprehensive fine-tuning strategy compared to general DPO methods.

Deeper architectural interventions aim to resolve representational issues within the MLLM. Methods like \textbf{Memory-Space Visual Retracing (MemVR)} \cite{liu2024hallucination} allow models to re-examine and leverage visual features more effectively within intermediate layers, combating 'visual amnesia' by re-injecting visual tokens based on uncertainty. This ensures that crucial visual details are not forgotten during the generation process. Crucially, these internal interventions often incorporate causal mechanisms to balance modality priors, preventing one modality from dominating or suppressing information from another \cite{liu2024hallucination, zhou2024lvp}. For instance, \cite{zhou2024lvp}'s \textbf{CAUSAL MM} framework applies structural causal modeling to MLLMs, treating visual and language priors as confounding factors and using back-door adjustment and counterfactual reasoning to isolate and mitigate modality biases. Similarly, \textbf{Visual Amplification Fusion (VAF)} \cite{yin2025s2b} enhances attention to visual signals specifically within the MLLM's middle layers, arguing that language bias often stems from \textit{insufficient} visual attention rather than an overemphasis on language. These intrinsic methods represent a deeper, mechanistic understanding of MLLM behavior, allowing for more precise and less intrusive corrections by ensuring more robust cross-modal integration.

While much of the research on multimodal hallucination mitigation has focused on LVLMs, the principles extend to other modalities, albeit with fewer dedicated studies \cite{sahoo2024hcb}. For Large Audio-Language Models (LALMs), object hallucination—where models generate or affirm the presence of non-existent sounds or objects—is a significant concern \cite{kuan20249pm}. \cite{kuan20249pm} demonstrated that carefully crafted prompt engineering can significantly improve LALM performance on discriminative audio tasks and reduce object hallucination, highlighting that even simple interventions can be effective when the underlying issue is query understanding rather than audio processing. Mitigation strategies for LALMs also include leveraging latent diffusion models or retrieval-based methods to ensure consistency between audio and text, as indicated by comprehensive surveys \cite{sahoo2024hcb}. In the realm of Audio-Visual Large Language Models (AV-LLMs), \cite{sungbin2024r2g} highlighted "cross-modal driven hallucinations," where models misinterpret information due to subtle relationships or over-reliance on one modality (e.g., video-driven audio hallucination or audio-driven video hallucination). Their work demonstrated that even simple training methods, such as Low-Rank Adaptation (LoRA) fine-tuning with enhanced feature alignment, can improve AV-LLM robustness against these complex inter-modal inconsistencies. General mitigation strategies for Video-Language Models (VLLMs) often involve temporal dependency modeling to ensure consistency across dynamic sequences, a critical challenge given the sequential nature of video data \cite{sahoo2024hcb}.

In conclusion, multimodal mitigation strategies have evolved from flexible, training-free external guidance and verification to sophisticated internal architectural interventions and causal reasoning frameworks, alongside targeted fine-tuning approaches. External methods like Counterfactual Inception \cite{kim2024ozf} and T2I visualization \cite{park20247cm} offer quick, adaptable solutions but rely on the robustness of external components. Decoding-time interventions like HIO \cite{chen20247jb} and fine-tuning approaches like HDPO \cite{fu2024yqj} represent a more direct engagement with the model's generation process, offering deeper control but requiring additional training or preference data. Intrinsic methods such as MemVR \cite{liu2024hallucination}, VAF \cite{yin2025s2b}, and CAUSAL MM \cite{zhou2024lvp} aim for fundamental improvements in cross-modal integration and modality balancing, offering robust solutions at the cost of increased model complexity or intrusiveness. While LVLMs have seen the most dedicated research, emerging work in LALMs \cite{kuan20249pm} and AV-LLMs \cite{sungbin2024r2g} indicates a growing focus on modality-specific challenges.

Despite significant progress, several challenges persist. The scalability and real-time applicability of complex causal interventions like CAUSAL MM \cite{zhou2024lvp} in dynamic multimodal streams, such as live video or audio, remain critical areas for research. A key open question is developing robust methods for arbitrating conflicting information presented by different modalities (e.g., an image showing a blue object while accompanying text describes a a red one), requiring dynamic weighting and conflict resolution mechanisms. Furthermore, the robustness of external verification methods, such as T2I-based approaches \cite{park20247cm}, is inherently tied to the fidelity and potential hallucination tendencies of the underlying generative models themselves. Ensuring temporal and causal consistency in long video or audio sequences, moving beyond single-frame object grounding, also poses a significant hurdle. Finally, effectively balancing the trade-offs between hallucination reduction, maintaining content quality, and preserving inference speed across diverse multimodal tasks (e.g., VQA, captioning, dialogue, video, audio) and model architectures remains a crucial area for future research \cite{yin2025s2b, sahoo2024hcb, lan20240yz, bai2024tkm}.
\subsection{Cross-Modal Dynamics and Snowballing}
\label{sec:7\_4\_cross-modal\_dynamics\_\_and\_\_snowballing}

The transition from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) and Vision-Language Models (LVLMs) introduces a new dimension to the hallucination problem: complex dynamic behaviors where errors propagate across modalities and conversational turns. While Section 4.4 addresses dialogue-level inconsistencies in text-only models, this subsection focuses specifically on how the interplay with persistent visual, audio, or video modalities creates unique error propagation dynamics, often termed "multimodal hallucination snowballing," which are not present in purely linguistic systems. Understanding these dynamic and interactive aspects is crucial for developing robust and coherent AI systems capable of sustained, reliable interaction across sensory inputs.

A primary dynamic challenge is \textit{multimodal hallucination snowballing}, where an LVLM's previously generated hallucination can mislead subsequent responses in conversational settings. \cite{zhong2024mfi} meticulously identifies and characterizes this problem, demonstrating how initial factual errors in an LVLM's output can be implicitly accepted and built upon in subsequent turns, leading to a cascade of incorrect information. To counteract this, they introduce Residual Visual Decoding (RVD), a training-free decoding method that emphasizes direct visual evidence to prevent the model from relying on its own prior, potentially erroneous, textual generations. While RVD offers a practical solution, its effectiveness hinges on the clarity and availability of direct visual evidence, which might be insufficient in scenarios requiring abstract reasoning or subtle contextual understanding. Reinforcing this challenge, \cite{cao2024o9a} introduces VisDiaHalBench, a visual dialogue benchmark specifically designed to diagnose hallucinations arising from "long-term misleading textual history" in LVLMs. This benchmark, featuring five-turn questions about edited images, directly probes the model's susceptibility to propagating errors in a conversational context, highlighting the need for continuous visual re-grounding. The principles of error propagation in multi-turn dialogues, as explored in text-only contexts by \cite{chen2024c4k} with their DiaHalu benchmark, find direct and exacerbated parallels in multimodal settings where visual context can be misremembered or ignored.

Beyond explicit conversational snowballing, misinterpretations can arise from subtle, dynamic interactions and inconsistencies between different modalities, acting as triggers for initial hallucinations that can then propagate. \cite{zhou2024lvp} investigates the causal impact of modality priors on attention and output, revealing how an imbalance in these priors can lead to hallucinations. Their work, CAUSAL MM, provides a principled causal inference framework to understand and balance the influence of visual and linguistic inputs by applying back-door adjustment and counterfactual reasoning. This mitigates errors stemming from over-reliance on one modality, which could otherwise initiate a chain of incorrect inferences. However, the complexity of causal modeling and defining appropriate counterfactuals remains a challenge for broad applicability. Similarly, \cite{han202439z} uncovers a "semantic shift bias" where the mere insertion of a paragraph break in textual input can subtly alter an LVLM's understanding of an image, leading to hallucinations. This demonstrates how minor, seemingly innocuous textual formatting can dynamically influence cross-modal interpretation, revealing a brittleness in vision-language grounding that may require more fundamental architectural solutions than the proposed MiHI/MiHO interventions.

Underlying these dynamic misinterpretations are fundamental vulnerabilities within the model architecture that enable error propagation. \cite{wang2025jen} reveals that hallucinations can be induced by exploiting "attention sinks," a phenomenon where attention mechanisms become fixated on irrelevant tokens, diverting processing power from critical visual information. In a multimodal context, this mechanism can directly contribute to propagating errors by causing the model to misinterpret visual cues, thus initiating a chain of incorrect inferences that could snowball. This highlights a critical internal dynamic where attention misallocation directly impacts multimodal grounding. Furthermore, the temporal dynamics inherent in video-language models present unique challenges. \cite{ma2023mka} introduces Vista-llama to address the "diminishing impact of video" as generated text length increases, a clear example of cross-modal dynamic error where the visual grounding weakens over time, leading to irrelevant content. Their solution, which maintains a consistent distance between visual and language tokens, underscores the need for continuous and robust visual attention throughout the generation process.

Evaluating an LVLM's ability to maintain coherent understanding across dynamic visual changes is also crucial for identifying propagating errors. \cite{yebin2024txh} introduces BEAF (Observing BEfore-AFter Changes), an innovative framework that manipulates visual scenes by removing objects and introduces change-aware metrics. This allows for a more robust assessment of whether models truly understand visual changes or merely hallucinate, which is vital for identifying when subtle inconsistencies lead to misinterpretations or propagating errors in a dynamic environment.

The insights from LLM-centric self-correction mechanisms offer conceptual parallels for mitigating multimodal snowballing. \cite{dhuliawala2023rqn}'s Chain-of-Verification (CoVe) and \cite{liu2025juo}'s self-consistency framework for mathematical reasoning both emphasize internal deliberation and verification steps to prevent models from repeating their own mistakes and propagating errors. Adapting such multi-step, self-correcting paradigms to multimodal contexts would require sophisticated mechanisms for re-grounding each verification step in the visual or audio evidence, rather than solely relying on internal textual consistency. This is a significant challenge, as highlighted by surveys like \cite{liu2024sn3} and \cite{lan20240yz}, which discuss the persistent "modality gap" and the difficulty of ensuring consistent understanding across diverse data distributions. \cite{tonmoy20244e4} also notes that snowballing in complex reasoning remains a challenge for many mitigation approaches, underscoring the severity of this dynamic problem.

In conclusion, the study of cross-modal dynamics and snowballing highlights that hallucination in LVLMs is not merely a static error but a complex, evolving problem. The propagation of errors, whether through explicit conversational snowballing, subtle cross-modal misinterpretations, or diminishing visual grounding over time, poses a significant challenge. Future research must focus on developing models with stronger internal consistency checks that explicitly re-ground in multimodal reality at each conversational turn, advanced causal modeling of cross-modal interactions to prevent the initiation of errors, and robust self-correction mechanisms that can effectively leverage and verify against dynamic sensory inputs to prevent the propagation of hallucinations.


\label{sec:towards_trustworthy_ai:_robustness,_safety,_and_advanced_evaluation}

\section{Towards Trustworthy AI: Robustness, Safety, and Advanced Evaluation}
\label{sec:towards\_trustworthy\_ai:\_robustness,\_safety,\_\_and\_\_advanced\_evaluation}

\subsection{Zero-Resource and Black-Box Hallucination Detection}
\label{sec:8\_1\_zero-resource\_\_and\_\_black-box\_hallucination\_detection}

Detecting hallucinations in Large Language Models (LLMs) presents a formidable challenge, particularly when evaluating proprietary models where direct access to internal states, training data, or extensive human labels is unavailable. This subsection focuses on "zero-resource" and "black-box" detection methods, which operate under these constraints by leveraging the intrinsic properties of LLM generation to identify factual and logical inconsistencies. These approaches are indispensable for scalable, model-agnostic evaluation, thereby enhancing the practical applicability of hallucination detection frameworks across diverse deployment scenarios.

A foundational contribution in this domain is \textit{SelfCheckGPT} \cite{manakul20236ex}, which introduced a novel zero-resource, black-box strategy. The core premise is that an LLM genuinely "knowing" a fact will produce consistent responses across multiple stochastically sampled generations for the same query. Conversely, if the LLM hallucinates, these samples are likely to diverge, contradict, or present inconsistent information. \textit{SelfCheckGPT} generates several diverse responses from the black-box LLM and then employs various consistency measures, such as BERTScore, Natural Language Inference (NLI), or even another LLM acting as an evaluator, to quantify the informational agreement between the original response and the generated samples. This method effectively identifies non-factual statements without requiring internal token probabilities or external fact-checking databases. However, a significant limitation arises when LLMs consistently repeat their own errors across multiple samples due to strong internal biases or memorization, leading to false negatives in hallucination detection, as the model appears "consistent" in its incorrectness. Furthermore, the computational overhead of generating and comparing multiple responses can be substantial, especially for complex queries or real-time applications.

To address the limitations of consistent self-hallucinations, \textit{MetaQA} \cite{yang20251dw} significantly advanced zero-resource detection by introducing \textit{metamorphic relations} and \textit{prompt mutation}. Instead of merely re-sampling from the same prompt, \textit{MetaQA} generates logically equivalent or semantically related prompts (e.g., synonymous queries, rephrased questions, or queries testing inverse relations) to elicit a more diverse and robust set of responses. By checking for consistency across these responses, which are generated from varied but semantically linked inputs, \textit{MetaQA} makes it harder for the LLM to consistently hallucinate the same fact. This technique effectively probes the LLM's understanding from multiple angles, providing a more reliable signal for exposing factual inconsistencies. While more robust, the effectiveness of \textit{MetaQA} is contingent on the careful design of metamorphic relations, which can be task-specific and may not generalize universally across all types of factual or reasoning errors.

Beyond consistency checks, other black-box approaches explore alternative signals. \textit{Attention-Guided SElf-Reflection (AGSER)} \cite{liu2025xwv} proposes a zero-shot hallucination detection method that attempts to leverage insights from attention mechanisms without direct internal model access. AGSER categorizes the input query into "attentive" and "non-attentive" parts, processes each separately through the LLM, and then computes consistency scores between the generated responses and the original answer. The difference between these consistency scores serves as a hallucination estimator. This method notably reduces computational overhead, requiring only three passes through the LLM, making it more efficient than methods relying on numerous generations. While the precise mechanism for inferring "attention contributions" in a strictly black-box manner requires careful consideration, AGSER demonstrates a promising direction for deriving more nuanced signals from LLM outputs without full transparency.

Zero-resource, black-box principles are also being tailored for specific, complex reasoning domains. For instance, \cite{liu2025juo} enhances mathematical reasoning in LLMs by applying a structured self-consistency framework. This approach goes beyond merely checking the final answer, enforcing consistency across \textit{intermediate reasoning steps} in tasks like theorem proving, symbolic transformation, and numerical computation. By ensuring logical coherence throughout the problem-solving process, this method significantly reduces logical inconsistencies and hallucinations specific to mathematical contexts. While domain-specific, it highlights how black-box consistency checks can be adapted to probe deeper into an LLM's reasoning integrity, rather than just surface-level factual recall.

\textbf{Comparative Analysis and Critical Discussion:}
These black-box detection methods offer distinct advantages and trade-offs. \textit{SelfCheckGPT} provides a simple, general-purpose baseline, but its vulnerability to consistently incorrect outputs limits its robustness. \textit{MetaQA} improves robustness by actively perturbing inputs, making it harder for LLMs to hide systematic errors, yet it introduces complexity in designing effective metamorphic relations. Both methods incur significant computational costs due to multiple inference calls. \textit{AGSER} attempts to mitigate this computational burden by leveraging a more efficient, attention-guided reflection mechanism, potentially offering a better balance between detection efficacy and resource usage. The mathematical self-consistency approach \cite{liu2025juo} demonstrates the power of adapting these principles to domain-specific reasoning, highlighting that while general black-box detectors are valuable, specialized approaches can achieve higher precision for particular types of complex hallucinations. A common limitation across these methods is their reliance on the LLM's own generative capabilities to expose its flaws; they cannot detect hallucinations that the LLM consistently and confidently generates as "true" across all probed variations. Furthermore, the sensitivity to sampling parameters (e.g., temperature) and the choice of consistency metrics (e.g., BERTScore vs. NLI) can significantly impact detection performance, requiring careful tuning.

In summary, zero-resource and black-box hallucination detection methods represent a vital area of research, offering scalable and model-agnostic solutions for evaluating and improving the trustworthiness of LLMs, especially proprietary ones. From the foundational consistency checks of \textit{SelfCheckGPT} \cite{manakul20236ex} and the robust metamorphic relations of \textit{MetaQA} \cite{yang20251dw}, to the efficient attention-guided reflection of \textit{AGSER} \cite{liu2025xwv} and domain-specific logical consistency for mathematical reasoning \cite{liu2025juo}, these techniques collectively push the boundaries of what is possible without privileged model access or extensive human annotation. Future directions in this area could involve the fusion of diverse black-box signals, combining internal consistency with metamorphic testing and attention-guided insights to create more robust hybrid detectors. Furthermore, research could focus on developing computationally lighter black-box methods, exploring their applicability to detect more subtle forms of hallucination beyond factual errors, such as logical fallacies, reasoning inconsistencies, and biases, and enhancing their resilience against adversarial attacks designed to evade detection.
\subsection{Adversarial Attacks and Vulnerability Probing}
\label{sec:8\_2\_adversarial\_attacks\_\_and\_\_vulnerability\_probing}

Beyond merely observing and reacting to instances of hallucination, a critical new frontier in understanding and mitigating this phenomenon involves proactively probing Large Multimodal Models (LMMs) for vulnerabilities through adversarial attacks. This methodology aims to intentionally induce hallucinations, thereby uncovering specific weaknesses and failure modes that might remain hidden during passive observation, guiding the development of more resilient and secure AI systems.

A pioneering work in this domain is \cite{wang2025jen}, which introduces a novel adversarial attack termed "Mirage in the Eyes." This technique specifically targets and exploits the "attention sink" phenomenon within MLLMs to intentionally induce hallucinations. By dynamically manipulating internal attention scores and hidden embeddings, \cite{wang2025jen} demonstrates how to steer the model towards generating factually incorrect or non-existent visual content, providing crucial insights into the internal mechanisms that contribute to hallucination. This approach moves beyond external input perturbations, delving into the model's internal processing to expose its susceptibility.

Complementing such targeted internal attacks, other research has integrated adversarial principles into evaluation and discovery. \cite{huang20247wn}, in their comprehensive benchmark VHTest, incorporates an adversarial generation paradigm to create diverse visual hallucination instances. While not a direct attack for inducing hallucination in the same manner as \cite{wang2025jen}, this paradigm contributes to the proactive identification of vulnerabilities by systematically generating challenging inputs that are likely to trigger various types of hallucinations, including those related to object shape and size. This allows for a broader exploration of an MLLM's fragility across different visual attributes.

Furthermore, vulnerabilities can be exposed through surprisingly subtle adversarial manipulations. \cite{han202439z} uncovered a "semantic shift bias" where the mere insertion of paragraph breaks (\texttt{\textbackslash n}) into textual prompts can induce hallucinations in MLLMs. This seemingly innocuous input perturbation acts as a potent adversarial trigger, demonstrating that models can be led astray by minor structural changes that do not alter the semantic content of the prompt. Such findings highlight unexpected failure modes and underscore the importance of probing for vulnerabilities across a wide spectrum of input types, from complex internal manipulations to simple textual formatting.

Collectively, these approaches represent a significant shift from reactive mitigation to proactive robustness testing. By actively designing attacks that exploit internal model characteristics like attention sinks \cite{wang2025jen}, or by systematically generating adversarial test cases \cite{huang20247wn}, or even by identifying subtle input biases \cite{han202439z}, researchers are gaining a deeper understanding of \textit{why} and \textit{how} hallucinations occur. This proactive methodology is indispensable for identifying the root causes of hallucination, enabling the development of more robust architectures and training strategies that can withstand sophisticated adversarial attempts to induce erroneous outputs. The insights gleaned from these adversarial probes are crucial for building truly trustworthy and secure AI systems that can maintain factual consistency even under challenging or malicious inputs.
\subsection{Semantic Guardrails for Safety-Critical Applications}
\label{sec:8\_3\_semantic\_guardrails\_for\_safety-critical\_applications}

The increasing deployment of Large Language Models (LLMs) in safety-critical domains, particularly in clinical medicine, necessitates a paradigm shift from general hallucination mitigation to robust, high-assurance safety mechanisms. In these high-stakes environments, the generation of factually incorrect or ungrounded information, often termed "hallucinations" \cite{maynez2020h3q}, can lead to "never events"—catastrophic errors with severe consequences for patient safety and well-being \cite{hakim2024d4u}. The inherent probabilistic nature of LLMs, which can lead to plausible but incorrect outputs \cite{hamid2024pwn}, coupled with observed inaccuracies in analyzing unstructured clinical notes \cite{shah20242sx} and significant challenges in complex medical reasoning tasks \cite{umapathi2023puv}, underscores the urgent need for specialized safeguards.

To address this imperative, the concept of 'semantic guardrails' has emerged as a targeted solution, designed to prevent these critical errors by enforcing strict adherence to factual accuracy and consistency within domain-specific knowledge. Unlike broader LLM safety surveys that discuss ethical considerations and prompt injection alongside hallucination \cite{gao20242nu}, semantic guardrails focus specifically on content integrity and factual grounding. This approach represents an evolution from traditional rule-based expert systems used in clinical AI, which offered high precision but often lacked the flexibility and generative power of LLMs. Semantic guardrails aim to imbue LLMs with a similar level of verifiable reliability, but within their more dynamic and open-ended operational context.

A pioneering framework in this area is presented by \cite{hakim2024d4u}, which introduces specific semantic guardrails tailored for pharmacovigilance, a domain where regulatory compliance and absolute accuracy are paramount. Their work highlights the distinction between "structural guardrails" (ensuring output format) and "semantic guardrails" (verifying content accuracy). They propose two primary mechanisms: Document-wise Uncertainty Quantification (DL-UQ) and MISMATCH guardrails. DL-UQ functions as a "soft" semantic guardrail by quantifying the LLM's uncertainty regarding each generated statement, specifically by evaluating its evidential support within a provided reference document. This mechanism identifies and flags information lacking sufficient backing, preventing unsupported claims from being presented as definitive facts. This is crucial for ensuring faithfulness to source material, a non-negotiable requirement in medical contexts.

Complementing DL-UQ, the MISMATCH guardrail acts as a "hard" semantic guardrail, actively detecting contradictions or inconsistencies between the LLM's generated output and the authoritative reference document \cite{hakim2024d4u}. For instance, in pharmacovigilance, it ensures that drug names or adverse event terms are consistently present in both source and target texts, preventing hallucination or omission of critical terms through the use of custom dictionaries and medical ontologies like MedDRA. Both DL-UQ and MISMATCH are engineered with the explicit goal of absolute error prevention for "never events," fundamentally shifting the paradigm from merely reducing the frequency of hallucinations to actively precluding errors where severe consequences are at stake.

While \cite{hakim2024d4u} focuses on document-wise uncertainty for text-to-text tasks, other research explores different facets of uncertainty quantification (UQ) that could complement or inform guardrail development. For instance, \cite{ling2024hqv} investigates aleatoric and epistemic uncertainties in LLMs during in-context learning, which could provide finer-grained signals for guardrails beyond document-level support. Similarly, \cite{zhang2024mmj} introduces VL-Uncertainty for Large Vision-Language Models (LVLMs), quantifying intrinsic uncertainty by analyzing prediction variance across semantically equivalent but perturbed prompts. This highlights a broader trend towards intrinsic uncertainty estimation, which could be integrated into multimodal semantic guardrails in the future to address the complexities of visual and other non-textual data in clinical settings.

Semantic guardrails also stand in contrast to, or can be integrated with, other mitigation strategies. Retrieval-Augmented Generation (RAG) is a foundational approach for grounding LLMs in external knowledge \cite{gilbert2024uu2}. While RAG aims to prevent hallucinations by providing relevant context, semantic guardrails act as a subsequent, explicit verification layer, ensuring the \textit{correctness} of the generated text \textit{against} that context, rather than just relying on the retrieval process. Similarly, approaches like In-Context Padding (ICP) that guide clinical reasoning with "knowledge seeds" \cite{wu202407f} aim to improve accuracy during generation by aligning LLM reasoning with clinical decision pathways, whereas guardrails provide a post-generation safety net that can validate the outcome of such guided reasoning.

Despite their promise, the development and deployment of semantic guardrails face significant challenges. The theoretical inevitability of hallucination in any computable LLM \cite{xu2024n76} suggests that achieving "absolute error prevention" is an asymptotic goal, requiring continuous vigilance and robust design. This means guardrails must be designed not just to prevent errors, but to gracefully handle irreducible uncertainties and flag them for human review. The tuning of thresholds for uncertainty-based guardrails (e.g., DL-UQ) involves delicate trade-offs between sensitivity (catching all potential errors) and specificity (avoiding false positives), which is particularly critical in clinical settings where over-flagging can lead to alert fatigue and hinder workflow efficiency. Furthermore, the computational overhead of running multiple, stringent semantic checks in real-time clinical workflows needs careful optimization to ensure practical applicability.

Future research must therefore focus on several key areas. Firstly, developing more sophisticated, domain-adaptable, and potentially formally verifiable semantic guardrails is crucial to expand their applicability beyond specific tasks like pharmacovigilance to broader medical reasoning and diagnostics. This includes exploring methods for automatically generating and validating guardrail rules, potentially leveraging knowledge graphs for enhanced precision and explainability. Secondly, integrating these guardrails seamlessly into human-in-the-loop systems, ensuring clear communication of uncertainty and rationale for flagging, is paramount for fostering trust and effective human-AI collaboration. Finally, research into multimodal semantic guardrails will be vital as LLMs increasingly process diverse data types in clinical settings, demanding consistent factual grounding across visual, textual, and other modalities. This continuous pursuit of high-assurance solutions is essential for the responsible and ethical integration of LLMs into safety-critical applications.
\subsection{Meta-Evaluation of Hallucination Benchmarks}
\label{sec:8\_4\_meta-evaluation\_of\_hallucination\_benchmarks}

The rapid proliferation of hallucination benchmarks, while crucial for advancing the field, has simultaneously introduced challenges regarding their quality, reliability, and validity. As the community developed increasingly sophisticated methods to detect diverse hallucination types, a critical self-reflection emerged: how do we ensure that the tools used to measure LLM performance are themselves robust and trustworthy? This subsection delves into the vital area of meta-evaluation, focusing on frameworks designed to assess the quality of these benchmarks.

Early efforts to quantify hallucination, such as the CHAIR metric, faced considerable scrutiny due to issues like instability and sensitivity to instruction design \cite{li2023249}. While subsequent benchmarks like POPE \cite{li2023249} addressed some of these limitations by offering more stable, polling-based evaluation, the overarching need for a systematic framework to evaluate \textit{any} hallucination benchmark remained. This necessity stems from concerns about potential prompt bias, data leakage, and the ability of benchmarks to accurately capture the multifaceted nature of hallucinations across various contexts and modalities.

Addressing this critical gap, \cite{yan2024ux8} introduced a groundbreaking psychometrics-inspired framework for the meta-evaluation of hallucination benchmarks. Their work proposes the \textbf{Hallucination benchmark Quality Measurement (HQM)} framework, which systematically assesses benchmarks across four key dimensions: reliability, validity, fairness, and utility. Reliability evaluates the consistency of a benchmark's results, ensuring that repeated measurements under similar conditions yield comparable outcomes. Validity, perhaps the most crucial dimension, ascertains whether a benchmark truly measures what it purports to measure, accurately capturing the intended hallucination types without conflating them with other errors. Fairness scrutinizes benchmarks for biases, such as prompt-specific biases that might inadvertently favor certain models or data leakage issues that compromise the integrity of evaluation. Finally, utility considers the practical aspects, including the scalability, interpretability, and overall usefulness of a benchmark for researchers and developers.

By applying HQM to existing benchmarks, \cite{yan2024ux8} revealed inherent strengths and weaknesses, providing a much-needed critical perspective on the tools foundational to hallucination research. This meta-evaluation not only highlights areas for improvement in current benchmarks but also proposes the concept of a \textbf{High-Quality Hallucination Benchmark (HQH)} as a guiding principle for future development. The HQH concept encourages benchmark designers to proactively incorporate principles of psychometric rigor, ensuring that new evaluation methodologies are inherently robust, unbiased, and capable of accurately reflecting model performance.

The introduction of meta-evaluation frameworks marks a significant maturation of the field, shifting from merely creating benchmarks to critically assessing their foundational quality. This self-reflective advancement ensures that research findings on LLM hallucination are built upon solid, trustworthy evaluation methodologies. It is vital for maintaining the integrity of research, guiding the development of truly effective and unbiased evaluation tools, and ultimately accelerating progress towards more reliable and trustworthy large language models. Future work will likely see the HQM framework become a standard for validating new benchmarks, fostering a more rigorous and transparent evaluation ecosystem.


\label{sec:conclusion_and_future_directions}

\section{Conclusion and Future Directions}
\label{sec:conclusion\_\_and\_\_future\_directions}

\subsection{Summary of Key Advancements}
\label{sec:9\_1\_summary\_of\_key\_advancements}

The persistent challenge of hallucination in Large Language Models (LLMs) has driven a rapid and profound evolution in research, transforming the field from initial problem identification to a sophisticated, multi-faceted scientific endeavor. This trajectory highlights a collective effort towards building more reliable, transparent, and contextually grounded AI systems. Crucially, a profound intellectual shift has occurred, moving beyond empirical observation to a formal theoretical grounding, demonstrating the inherent inevitability of hallucination in computable LLMs \cite{xu2024n76, li2025qzg}. This fundamental insight re-frames the research goal from complete eradication to robust management and mitigation, acknowledging an innate limitation.

Early foundational work established the critical need to address unfaithful content. \cite{maynez2020h3q} provided a seminal large-scale human evaluation, categorizing hallucinations in abstractive summarization and demonstrating the utility of textual entailment for faithfulness evaluation. Recognizing the limitations of static knowledge, \cite{trivedi2022qsf} introduced Interleaving Retrieval with Chain-of-Thought (IRCoT), an early few-shot, training-free method that dynamically interleaved reasoning steps with knowledge retrieval to ground LLMs in external facts. This marked an early shift towards adaptive, real-time grounding during generation. Subsequent advancements in Retrieval-Augmented Generation (RAG) further refined this paradigm, with frameworks like Rowen \cite{ding20244yr} intelligently deciding \textit{when} to retrieve based on cross-language/cross-model consistency, optimizing efficiency and mitigating both internal and external hallucinations. Complementing this, the strategic integration of structured Knowledge Graphs (KGs) through "Graph-guided retrieval" and "Graph-guided generation" has significantly enhanced trustworthiness and reduced hallucinations in open-ended QA by providing verifiable, structured knowledge \cite{sui20242u1}. However, it is critical to acknowledge that RAG systems, while powerful, are not infallible; their efficacy is inherently tied to the quality of retrieved information, making them vulnerable to noisy or biased sources and susceptible to 'confabulations' arising from limitations within RAG components themselves \cite{zhang20252at}.

As LLMs became more capable and complex, the focus expanded to developing sophisticated evaluation benchmarks that could rigorously assess trustworthiness beyond simple accuracy. \cite{gao2023ht7} introduced ALCE, the first reproducible benchmark for evaluating LLMs' ability to generate text with verifiable citations, complete with automatic metrics for fluency, correctness, and citation quality. Pushing evaluation further, \cite{oh2024xa3} developed ERBench, a novel benchmark leveraging relational databases to automatically verify not just final answers but also the \textit{rationales} provided by LLMs, addressing a critical need for transparency in reasoning. Complementing this, \cite{ghosh2024tj5} developed new logical fact-checking datasets and quantitative measures to assess LLM consistency on complex propositional logic queries, expanding the definition of "hallucination" to include logical inconsistencies. For scenarios where external knowledge or human labels are scarce, methods like MetaQA \cite{yang20251dw} emerged as zero-resource, self-contained hallucination detection techniques leveraging metamorphic relations and prompt mutation. The theoretical underpinnings of such detection methods were further explored by \cite{karbasi2025j7n}, who established an equivalence between hallucination detection and language identification, proving that automated detection is fundamentally impossible for most language collections without expert-labeled feedback, thereby providing theoretical support for methods like RLHF. Furthermore, to foster more responsible AI behavior, \cite{tjandra2024umq} introduced the Accuracy-Engagement Distance (AED) metric to evaluate models capable of appropriately \textit{abstaining} from answers when uncertain, utilizing semantic entropy for label-free uncertainty estimation.

Concurrently, mitigation strategies evolved from general retrieval to highly adaptive, proactive, and mechanistically targeted interventions. On a more mechanistic front, \cite{zhang2024qq9} identified "knowledge overshadowing" as a novel root cause of "amalgamated hallucinations" arising from data imbalance, proposing an inference-time, training-free self-contrastive decoding method for targeted mitigation. This represents a deeper understanding of internal model dynamics. Similarly, \cite{chen2024j0g} presented ICT, a training-free, forward-pass intervention method that targets specific attention heads to enhance focus on crucial information, mitigating the dominance of language priors. The development of self-correction mechanisms, as exemplified by \cite{tjandra2024umq}'s work on abstention, empowers LLMs to reflect on their uncertainty and proactively avoid generating ungrounded content, moving towards more self-aware and reliable systems.

A major conceptual and methodological shift has been the dedicated focus on multimodal hallucination, particularly in Large Vision-Language Models (LVLMs). These models introduce unique complexities due to the "modality gap" and the integration of diverse information sources \cite{lan20240yz}. \cite{kaul2024ta7} addressed the inadequacy of prior benchmarks by introducing THRONE, the first accurate object-based hallucination benchmark for \textit{free-form} generations of LVLMs, utilizing LM-based semantic judgment. Building on the "snowballing" effect observed in LLMs, \cite{zhong2024mfi} investigated and mitigated "multimodal hallucination snowballing" in LVLMs, proposing Residual Visual Decoding (RVD) to emphasize direct visual evidence and prevent error propagation in conversational settings. To address the diverse nature of LVLM queries, \cite{chang2024u3t} introduced Dentist, a unified mitigation framework that classifies query types (perception vs. reasoning) and applies tailored, iterative validation strategies. Furthermore, intrinsic model interventions emerged, with \cite{wang2024vym} identifying "Visual Encoding Distortion" as a critical source of LVLM hallucinations and proposing Visual-Layer Fusion Contrastive Decoding (VaLiD) to correct it by fusing features from early visual layers. However, early multimodal evaluation benchmarks, such as object-based approaches, were found to be highly susceptible to prompt bias, leading to inaccurate assessments of real-world hallucination \cite{wang2023zop}, necessitating more robust, LLM-based evaluation frameworks like HaELM.

Collectively, these advancements underscore a profound intellectual trajectory: from merely identifying and correcting errors to a deep, multi-faceted understanding of hallucination's origins, its manifestation across modalities, and the development of sophisticated, adaptive, and intrinsically aware mechanisms for prevention and evaluation. This comprehensive progress marks a significant stride in the pursuit of building truly trustworthy, transparent, and contextually grounded AI systems, acknowledging both their immense potential and their inherent, theoretically proven limitations. The field is actively navigating the tension between external grounding, internal correction, and the acceptance of these fundamental limits, guiding research towards robust management rather than the elusive goal of complete eradication.
\subsection{Remaining Challenges and Open Questions}
\label{sec:9\_2\_remaining\_challenges\_\_and\_\_open\_questions}

Despite significant advancements in characterizing, evaluating, and mitigating hallucination in large language models (LLMs) and multimodal large language models (MLLMs), several critical unresolved issues persist, defining promising avenues for future research in trustworthy AI. The dynamic nature of these models and their expanding capabilities mean that hallucination remains a moving target, necessitating continuous innovation.

One fundamental challenge lies in the \textbf{scalability of fine-grained annotation and evaluation}. While efforts like \cite{ji20243j6} and \cite{gu202414e} have introduced analytical, sentence-level annotation datasets (ANAH and ANAH-v2) and iterative self-training frameworks to scale this process, the sheer diversity of tasks, domains, and hallucination types makes truly comprehensive, human-quality annotation prohibitively expensive and time-consuming. This bottleneck hinders the development of robust, generalizable evaluation benchmarks that can capture the nuances of complex reasoning, as exemplified by the need for rationale verification in \cite{oh2024xa3} (ERBench) and path-based evaluation in graph computation tasks \cite{tang2024a1j} (GraphArena). The challenge extends beyond mere data quantity to ensuring the \textit{quality, diversity, and contextual richness} of annotated data across an ever-expanding problem space.

Another pressing issue is the development of \textbf{truly real-time, adaptive, and seamlessly integrated mitigation strategies}. Current approaches have made strides towards proactive prevention and self-correction. For instance, \cite{manakul20236ex} (SelfCheckGPT) and \cite{yang20251dw} (MetaQA) offer zero-resource, black-box detection methods, yet these often incur computational overhead or rely on sampling, which can introduce latency or fail to capture dynamic shifts in model uncertainty. While \cite{tjandra2024umq} proposes label-free abstention using semantic entropy, the challenge remains in making such mechanisms adaptively responsive to subtle changes in user intent or context without sacrificing generation quality or speed. Similarly, advanced Retrieval-Augmented Generation (RAG) techniques like \cite{lv2024k5x}'s Coarse-to-Fine Highlighting (COFT) and \cite{ding20244yr}'s Rowen improve context relevance, but the seamless integration of such dynamic knowledge retrieval and synthesis into the core generation process, without introducing new latency or compromising creative outputs, remains an open problem. Furthermore, while \cite{hakim2024d4u} introduced "semantic guardrails" for safety-critical domains, generalizing such hard constraints to open-ended, creative, or rapidly evolving tasks without stifling utility is a complex balancing act.

The field also grapples with the complex task of \textbf{bridging the gap between theoretical inevitability and practical error reduction}. The unified theoretical framework proposed by \cite{li2025qzg} (Loki's Dance of Illusions) suggests that some forms of hallucination might be mathematically inherent to LLM architectures or their training paradigms. If certain types of hallucination are indeed inevitable, the critical question shifts from absolute elimination to understanding the \textit{acceptable error rate} and designing systems that can gracefully handle or transparently communicate these inherent limitations. This necessitates further research into robust uncertainty quantification and calibrated abstention mechanisms that are both accurate and user-friendly, allowing LLMs to "know what they don't know" effectively.

Beyond these challenges, several \textbf{open questions} guide the next wave of innovation. The rapid expansion into multimodal AI has unveiled \textbf{novel hallucination types} that require dedicated investigation. While object hallucination in vision-language models \cite{liu2024sn3, lan20240yz} and audio-language models \cite{kuan20249pm} has been identified, and video-specific temporal inconsistencies are being explored \cite{wang2024rta}, the full spectrum of cross-modal fabrication and misinterpretation, especially in complex reasoning or creative multimodal generation, is yet to be fully mapped. Moreover, the emergence of adversarial attacks that induce hallucinations \cite{wang2025jen} suggests that new, engineered forms of hallucination will continue to challenge detection and mitigation efforts.

Another critical open question concerns the \textbf{long-term impact of multimodal interactions and cascading hallucinations}. While \cite{qiu2024zyc} has begun to address long-context multimodal hallucination, the cumulative effect of minor inconsistencies over extended dialogues or multi-turn reasoning in complex multimodal environments remains poorly understood. The concept of "multimodal hallucination snowballing" \cite{zhong2024mfi} highlights the potential for initial errors to propagate and amplify, leading to increasingly unreliable outputs. Developing methods to track, predict, and mitigate these cascading effects across modalities and over time is crucial for building truly robust conversational and interactive AI systems.

Finally, there is a pressing need for \textbf{more unified and generalizable solutions that perform robustly across diverse tasks and domains}. Many current mitigation strategies are task-specific (e.g., RAG for factual QA, visual grounding for LVLMs). The development of a single, overarching framework that can effectively address hallucination across diverse applications—from summarization and dialogue to code generation and creative writing, and across various modalities—while maintaining high performance and adaptability, remains an elusive goal. While some efforts, like \cite{chang2024u3t}'s unified LVLM mitigation framework, attempt to generalize within a multimodal context, achieving true cross-domain and cross-task robustness without extensive, domain-specific fine-tuning is a significant hurdle. The pursuit of a holistic "information quality" metric, as conceptualized by \cite{rejeleene2024okw}, could pave the way for more generalizable evaluation and mitigation, but its practical implementation across diverse scenarios is still an open area of research. Addressing these challenges and open questions will be paramount in guiding the next wave of innovation towards building inherently trustworthy and transparent AI systems.
\subsection{Ethical Considerations and Responsible AI Development}
\label{sec:9\_3\_ethical\_considerations\_\_and\_\_responsible\_ai\_development}

The challenge of hallucination in Large Language Models (LLMs) transcends mere technical inaccuracy, presenting profound ethical dilemmas that necessitate a robust framework for responsible AI development. The generation of confident yet incorrect or fabricated information by LLMs carries significant societal implications, demanding critical attention to transparency, accountability, and the safe deployment of these powerful systems, particularly in high-stakes applications \cite{dbeeca8466e0c177ec67c60d529899232415ca87, e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365}.

A foundational ethical consideration arises from the inherent limitations of LLMs. As discussed in Section 3.3, theoretical frameworks, notably those employing diagonalization arguments, suggest the mathematical inevitability of hallucination in any computable LLM \cite{xu2024n76, li2025qzg}. This theoretical grounding shifts the ethical imperative from eradicating hallucination to transparently communicating its unavoidable nature. Responsible AI development demands that developers and deployers manage societal expectations, clearly articulate the inherent limitations of LLMs, and avoid presenting them as infallible or universally reliable. Failure to do so can lead to a breach of trust, misinformed decision-making, and a violation of the principle of non-maleficence, particularly when LLMs are deployed in sensitive domains.

To foster accountability and enable users to verify generated content, technical solutions have focused on grounding LLM outputs. As detailed in Section 4.1, the development of benchmarks like ALCE, which encourage LLMs to generate text with explicit citations to supporting evidence, represents a significant step towards ethical verifiability \cite{gao2023ht7}. Ethically, such mechanisms aim to empower user autonomy by providing the means to cross-reference information, thereby combating the spread of misinformation. However, a critical perspective reveals potential pitfalls: if the citation mechanism itself is susceptible to hallucination (e.g., generating non-existent sources or misattributing information), it could create a false sense of security, exacerbating the problem rather than solving it. Ensuring the integrity of the grounding process is therefore paramount.

Beyond proactive grounding, robust detection mechanisms are crucial for continuous monitoring and for informing users about potential inaccuracies. As explored in Section 8.1, zero-resource, black-box hallucination detection methods like \textit{SelfCheckGPT} \cite{manakul20236ex} and techniques leveraging metamorphic relations \cite{yang20251dw} offer practical tools for identifying ungrounded content, even in proprietary models. Ethically, these tools support ongoing oversight and allow for post-hoc correction or flagging of potentially harmful outputs. However, their limitations, such as potential false negatives in subtle hallucinations or the computational cost of multiple generations, must be transparently acknowledged to prevent over-reliance and to ensure that human oversight remains a critical component of the safety loop.

A key aspect of responsible deployment is the clear communication of uncertainty. As discussed in Section 6.3, models can be fine-tuned to proactively abstain from answering when uncertain, utilizing label-free techniques based on semantic entropy \cite{tjandra2024umq}. This mechanism directly addresses the ethical principle of transparency by allowing LLMs to express "I don't know" rather than confidently asserting potentially incorrect information. This reduces the risk of misinformation in sensitive contexts and promotes a more honest interaction paradigm. However, the ethical balance lies in determining the appropriate threshold for abstention; an overly conservative model might diminish utility, while an overly permissive one risks harm.

The responsible deployment of AI systems, particularly in high-stakes applications, necessitates robust safety mechanisms and domain-specific "guardrails." In medical safety-critical settings, for instance, the implementation of "semantic guardrails" (e.g., Document-wise Uncertainty Quantification and MISMATCH guardrails), as highlighted in Section 8.3, aims to prevent "never event" errors—hallucinations with severe consequences \cite{hakim2024d4u}. This exemplifies a direct coupling of technical solutions with stringent ethical frameworks, prioritizing non-maleficence. Such guardrails are crucial for ensuring that LLMs can be trusted in environments where factual accuracy and safety are paramount, moving beyond general mitigation to targeted, high-assurance solutions.

Moreover, responsible AI development extends beyond mere factual accuracy to encompass a broader spectrum of ethical considerations. Hallucination can intersect with and amplify algorithmic bias, leading to outputs that are not only incorrect but also unfair or discriminatory, thereby violating principles of justice and fairness. The phenomenon of "sycophancy," where LLMs excessively agree with or flatter users, even when the user's premise is incorrect, poses a distinct ethical challenge \cite{malmqvist2024k7x}. Sycophancy can undermine critical thinking, reinforce user biases, and create echo chambers, impacting user autonomy and potentially leading to societal harm. Addressing such behavioral biases is as critical as addressing factual errors for building truly reliable and ethically aligned LLMs. Furthermore, the generation of toxic or harmful content, as noted in surveys on automated correction \cite{pan2024y3a}, underscores the need for comprehensive safety measures.

The pursuit of trustworthy AI also requires holistic conceptual frameworks. Research has moved towards defining and mathematically formalizing "Information Quality" (IQ) based on consistency, relevance, and accuracy, providing a structured approach to evaluating the ethical performance of LLMs beyond mere factual correctness \cite{rejeleene2024okw}. This holistic perspective is vital as LLMs expand into multimodal domains, where challenges like long-context hallucinations in multimodal models demand continuous monitoring and tailored ethical considerations \cite{qiu2024zyc}. The emergence of adversarial attacks that can intentionally induce hallucinations in multimodal LLMs, as discussed in Section 8.2 \cite{wang2025jen}, highlights critical security vulnerabilities. Ethically, this necessitates proactive security measures and foresight to prevent malicious exploitation and ensure the integrity and safety of AI systems, as emphasized by broader surveys on LLM safety \cite{gao20242nu}.

In conclusion, addressing hallucination is not solely a technical endeavor but a profound ethical responsibility that underpins the trustworthiness and societal benefit of LLMs. The development of transparent, accountable, and safe LLM systems requires a multi-faceted approach that integrates robust detection, verifiable content generation, clear communication of uncertainty, and domain-specific safety mechanisms. This forward-looking perspective emphasizes that technical advancements must be inextricably linked with strong ethical frameworks, encompassing principles of non-maleficence, beneficence, justice, fairness, and user autonomy, to ensure that LLMs are developed and utilized in a manner that genuinely benefits society, minimizes harm, and maximizes trustworthiness.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{277}

\bibitem{vu202337s}
Tu Vu, Mohit Iyyer, Xuezhi Wang, et al. (2023). \textit{FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chang2024u3t}
Yue Chang, Liqiang Jing, Xiaopeng Zhang, et al. (2024). \textit{A Unified Hallucination Mitigation Framework for Large Vision-Language Models}. Trans. Mach. Learn. Res..

\bibitem{wang2024vym}
Jiaqi Wang, Yifei Gao, and Jitao Sang (2024). \textit{VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding}. arXiv.org.

\bibitem{niu2024v97}
Mengjia Niu, Hao Li, Jie Shi, et al. (2024). \textit{Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval}. arXiv.org.

\bibitem{liu2024gxh}
Aiwei Liu, Qiang Sheng, and Xuming Hu (2024). \textit{Preventing and Detecting Misinformation Generated by Large Language Models}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2023v3v}
Xingxuan Li, Ruochen Zhao, Yew Ken Chia, et al. (2023). \textit{Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources}. International Conference on Learning Representations.

\bibitem{liang2024hoo}
Mengfei Liang, Archish Arun, Zekun Wu, et al. (2024). \textit{THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models}. arXiv.org.

\bibitem{zhang2023k1j}
Yue Zhang, Yafu Li, Leyang Cui, et al. (2023). \textit{Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}. Computational Linguistics.

\bibitem{zhou2024lvp}
Guanyu Zhou, Yibo Yan, Xin Zou, et al. (2024). \textit{Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality}. International Conference on Learning Representations.

\bibitem{bai2024tkm}
Zechen Bai, Pichao Wang, Tianjun Xiao, et al. (2024). \textit{Hallucination of Multimodal Large Language Models: A Survey}. arXiv.org.

\bibitem{yin2023hx3}
Shukang Yin, Chaoyou Fu, Sirui Zhao, et al. (2023). \textit{Woodpecker: Hallucination Correction for Multimodal Large Language Models}. Science China Information Sciences.

\bibitem{cao2023ecl}
Zouying Cao, Yifei Yang, and Hai Zhao (2023). \textit{AutoHall: Automated Hallucination Dataset Generation for Large Language Models}. arXiv.org.

\bibitem{wu2024bxt}
Ming-Kuan Wu, Jiayi Ji, Oucheng Huang, et al. (2024). \textit{Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models}. International Conference on Machine Learning.

\bibitem{ghosh2024tj5}
Bishwamittra Ghosh, Sarah Hasan, Naheed Anjum Arafat, et al. (2024). \textit{Logical Consistency of Large Language Models in Fact-checking}. International Conference on Learning Representations.

\bibitem{gao2023ht7}
Tianyu Gao, Howard Yen, Jiatong Yu, et al. (2023). \textit{Enabling Large Language Models to Generate Text with Citations}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yang20251dw}
Borui Yang, Md Afif Al Mamun, Jie M. Zhang, et al. (2025). \textit{Hallucination Detection in Large Language Models with Metamorphic Relations}. Proc. ACM Softw. Eng..

\bibitem{zhang2025pex}
Yongheng Zhang, Xu Liu, Ruoxi Zhou, et al. (2025). \textit{CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xing2024itg}
Shangyu Xing, Fei Zhao, Zhen Wu, et al. (2024). \textit{EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{leng2023ohr}
Sicong Leng, Hang Zhang, Guanzheng Chen, et al. (2023). \textit{Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding}. Computer Vision and Pattern Recognition.

\bibitem{kim2024ozf}
Junho Kim, Yeonju Kim, and Yonghyun Ro (2024). \textit{What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2024wyb}
Chaoyu Li, Eun Woo Im, and Pooyan Fazli (2024). \textit{VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding}. Computer Vision and Pattern Recognition.

\bibitem{ji20243j6}
Ziwei Ji, Yuzhe Gu, Wenwei Zhang, et al. (2024). \textit{ANAH: Analytical Annotation of Hallucinations in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{gao20232zb}
Yunfan Gao, Yun Xiong, Xinyu Gao, et al. (2023). \textit{Retrieval-Augmented Generation for Large Language Models: A Survey}. arXiv.org.

\bibitem{ji20227ii}
Ziwei Ji, Zihan Liu, Nayeon Lee, et al. (2022). \textit{RHO ($ρ$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{adams202289x}
Griffin Adams, Han-Chin Shing, Q. Sun, et al. (2022). \textit{Learning to Revise References for Faithful Summarization}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{su2024gnz}
Weihang Su, Yichen Tang, Qingyao Ai, et al. (2024). \textit{Mitigating Entity-Level Hallucination in Large Language Models}. SIGIR-AP.

\bibitem{du2023qu7}
LI DU, Yequan Wang, Xingrun Xing, et al. (2023). \textit{Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis}. arXiv.org.

\bibitem{ji2023vhv}
Ziwei Ji, Tiezheng Yu, Yan Xu, et al. (2023). \textit{Towards Mitigating Hallucination in Large Language Models via Self-Reflection}. arXiv.org.

\bibitem{pan2023mwu}
Liangming Pan, Michael Stephen Saxon, Wenda Xu, et al. (2023). \textit{Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies}. arXiv.org.

\bibitem{kang20238j0}
Haoqiang Kang, and Xiao-Yang Liu (2023). \textit{Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination}. arXiv.org.

\bibitem{kang202378c}
Haoqiang Kang, Juntong Ni, and Huaxiu Yao (2023). \textit{Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification}. arXiv.org.

\bibitem{dong20223yz}
Yue Dong, J. Wieting, and Pat Verga (2022). \textit{Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{qu20240f7}
Xiaoye Qu, Mingyang Song, Wei Wei, et al. (2024). \textit{Mitigating Multilingual Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{mckenna2023pzc}
Nick McKenna, Tianyi Li, Liang Cheng, et al. (2023). \textit{Sources of Hallucination by Large Language Models on Inference Tasks}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{rejeleene2024okw}
Rick Rejeleene, Xiaowei Xu, and John R. Talburt (2024). \textit{Towards Trustable Language Models: Investigating Information Quality of Large Language Models}. arXiv.org.

\bibitem{liu2024p39}
Xinxin Liu (2024). \textit{A Survey of Hallucination Problems Based on Large Language Models}. Applied and Computational Engineering.

\bibitem{liu2024sn3}
Hanchao Liu, Wenyuan Xue, Yifei Chen, et al. (2024). \textit{A Survey on Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{chen2024hfe}
Jiawei Chen, Dingkang Yang, Tong Wu, et al. (2024). \textit{Detecting and Evaluating Medical Hallucinations in Large Vision Language Models}. arXiv.org.

\bibitem{li2024hdc}
Qing Li, Chenyang Lyu, Jiahui Geng, et al. (2024). \textit{Reference-free Hallucination Detection for Large Vision-Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yan2024ux8}
Bei Yan, Jie Zhang, Zheng Yuan, et al. (2024). \textit{Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models}. arXiv.org.

\bibitem{wang2024rta}
Yuxuan Wang, Yueqian Wang, Dongyan Zhao, et al. (2024). \textit{VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models}. arXiv.org.

\bibitem{xie2024l8a}
Yuxi Xie, Guanzhen Li, Xiao Xu, et al. (2024). \textit{V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{guan2023z15}
Tianrui Guan, Fuxiao Liu, Xiyang Wu, et al. (2023). \textit{Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models}. Computer Vision and Pattern Recognition.

\bibitem{liang20236sh}
Xun Liang, Shichao Song, Simin Niu, et al. (2023). \textit{UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{huang2023akj}
Lei Huang, Weijiang Yu, Weitao Ma, et al. (2023). \textit{A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}. ACM Trans. Inf. Syst..

\bibitem{lv2024k5x}
Qitan Lv, Jie Wang, Hanzhu Chen, et al. (2024). \textit{Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models}. International Conference on Machine Learning.

\bibitem{gu202414e}
Yuzhe Gu, Ziwei Ji, Wenwei Zhang, et al. (2024). \textit{ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models}. Neural Information Processing Systems.

\bibitem{huang20247wn}
Wen Huang, Hongbin Liu, Minxin Guo, et al. (2024). \textit{Visual Hallucinations of Multi-modal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{liu2023882}
Fuxiao Liu, Kevin Lin, Linjie Li, et al. (2023). \textit{Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning}. International Conference on Learning Representations.

\bibitem{ding2024o88}
Peng Ding, Jingyu Wu, Jun Kuang, et al. (2024). \textit{Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs}. ACM Multimedia.

\bibitem{rawte2023ao8}
Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, et al. (2023). \textit{The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{pan2024y3a}
Liangming Pan, Michael Stephen Saxon, Wenda Xu, et al. (2024). \textit{Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies}. Transactions of the Association for Computational Linguistics.

\bibitem{li2023rvf}
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, et al. (2023). \textit{HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhou2023zu6}
Yiyang Zhou, Chenhang Cui, Jaehong Yoon, et al. (2023). \textit{Analyzing and Mitigating Object Hallucination in Large Vision-Language Models}. International Conference on Learning Representations.

\bibitem{han202439z}
Zongbo Han, Zechen Bai, Haiyang Mei, et al. (2024). \textit{Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{wang2025jen}
Yining Wang, Mi Zhang, Junjie Sun, et al. (2025). \textit{Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink}. arXiv.org.

\bibitem{qu2024pqc}
Xiaoye Qu, Jiashuo Sun, Wei Wei, et al. (2024). \textit{Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning}. International Conference on Computational Linguistics.

\bibitem{dai20229aa}
Wenliang Dai, Zihan Liu, Ziwei Ji, et al. (2022). \textit{Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training}. Conference of the European Chapter of the Association for Computational Linguistics.

\bibitem{dziri2021bw9}
Nouha Dziri, Andrea Madotto, Osmar Zaiane, et al. (2021). \textit{Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{sungbin2024r2g}
Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, et al. (2024). \textit{AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models}. International Conference on Learning Representations.

\bibitem{hakim2024d4u}
Joe B Hakim, Jeffery L. Painter, D. Ramcharran, et al. (2024). \textit{The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem}. arXiv.org.

\bibitem{li2025qzg}
Chaozhuo Li, Pengbo Wang, Chenxu Wang, et al. (2025). \textit{Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models}. arXiv.org.

\bibitem{wang2023ubf}
Lei Wang, Jiabang He, Shenshen Li, et al. (2023). \textit{Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites}. Conference on Multimedia Modeling.

\bibitem{chen2024c4k}
Kedi Chen, Qin Chen, Jie Zhou, et al. (2024). \textit{DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{ding20244yr}
Hanxing Ding, Liang Pang, Zihao Wei, et al. (2024). \textit{Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models}. arXiv.org.

\bibitem{deng202405j}
Shijian Deng, Wentian Zhao, Yu-Jhe Li, et al. (2024). \textit{Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach}. arXiv.org.

\bibitem{chen2024j0g}
Junzhe Chen, Tianshu Zhang, Shiyu Huang, et al. (2024). \textit{ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models}. Computer Vision and Pattern Recognition.

\bibitem{chen20247jb}
Beitao Chen, Xinyu Lyu, Lianli Gao, et al. (2024). \textit{Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization}. Neural Information Processing Systems.

\bibitem{maynez2020h3q}
Joshua Maynez, Shashi Narayan, Bernd Bohnet, et al. (2020). \textit{On Faithfulness and Factuality in Abstractive Summarization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{qu20246yn}
Xiaoye Qu, Qiyuan Chen, Wei Wei, et al. (2024). \textit{Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation}. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP).

\bibitem{chen2023h04}
Jiawei Chen, Hongyu Lin, Xianpei Han, et al. (2023). \textit{Benchmarking Large Language Models in Retrieval-Augmented Generation}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2024wbi}
Yiyang Zhou, Chenhang Cui, Rafael Rafailov, et al. (2024). \textit{Aligning Modalities in Vision Large Language Models via Preference Fine-tuning}. arXiv.org.

\bibitem{jiang2024792}
Chaoya Jiang, Wei Ye, Mengfan Dong, et al. (2024). \textit{Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models}. ACM Multimedia.

\bibitem{tjandra2024umq}
Benedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, et al. (2024). \textit{Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy}. arXiv.org.

\bibitem{umapathi2023puv}
Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu (2023). \textit{Med-HALT: Medical Domain Hallucination Test for Large Language Models}. Conference on Computational Natural Language Learning.

\bibitem{zou2024dp7}
Xin Zou, Yizhou Wang, Yibo Yan, et al. (2024). \textit{Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models}. arXiv.org.

\bibitem{li2024osp}
Ningke Li, Yuekang Li, Yi Liu, et al. (2024). \textit{Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models}. Proc. ACM Program. Lang..

\bibitem{chuang20248ey}
Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, et al. (2024). \textit{Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2024qrj}
Junyi Li, Jie Chen, Ruiyang Ren, et al. (2024). \textit{The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{wang2023zop}
Junyan Wang, Yi Zhou, Guohai Xu, et al. (2023). \textit{Evaluation and Analysis of Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{xu2024n76}
Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli (2024). \textit{Hallucination is Inevitable: An Innate Limitation of Large Language Models}. arXiv.org.

\bibitem{liu2021mo6}
Tianyu Liu, Yizhe Zhang, C. Brockett, et al. (2021). \textit{A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{manakul20236ex}
Potsawee Manakul, Adian Liusie, and M. Gales (2023). \textit{SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhong2024mfi}
Weihong Zhong, Xiaocheng Feng, Liang Zhao, et al. (2024). \textit{Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2024qq9}
Yuji Zhang, Sha Li, Jiateng Liu, et al. (2024). \textit{Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models}. arXiv.org.

\bibitem{wu20241us}
J. Wu, Tsz Ting Chung, Kai Chen, et al. (2024). \textit{Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models}. Trans. Mach. Learn. Res..

\bibitem{tonmoy20244e4}
S. Tonmoy, S. M. M. Zaman, Vinija Jain, et al. (2024). \textit{A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models}. arXiv.org.

\bibitem{wu2024n00}
Kai Wu, Boyuan Jiang, Zhengkai Jiang, et al. (2024). \textit{NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models}. arXiv.org.

\bibitem{wen2023t6v}
Yilin Wen, Zifeng Wang, and Jimeng Sun (2023). \textit{MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{li2023249}
Yifan Li, Yifan Du, Kun Zhou, et al. (2023). \textit{Evaluating Object Hallucination in Large Vision-Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{lan20240yz}
Wei Lan, Wenyi Chen, Qingfeng Chen, et al. (2024). \textit{A Survey of Hallucination in Large Visual Language Models}. arXiv.org.

\bibitem{dhuliawala2023rqn}
S. Dhuliawala, M. Komeili, Jing Xu, et al. (2023). \textit{Chain-of-Verification Reduces Hallucination in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sui20242u1}
Yuan Sui, and Bryan Hooi (2024). \textit{Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xiao2024hv1}
Wenyi Xiao, Ziwei Huang, Leilei Gan, et al. (2024). \textit{Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback}. arXiv.org.

\bibitem{trivedi2022qsf}
H. Trivedi, Niranjan Balasubramanian, Tushar Khot, et al. (2022). \textit{Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{park20247cm}
Yeji Park, Deokyeong Lee, Junsuk Choe, et al. (2024). \textit{ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models}. AAAI Conference on Artificial Intelligence.

\bibitem{sridhar2022l1c}
A. Sridhar, and Erik M. Visser (2022). \textit{Improved Beam Search for Hallucination Mitigation in Abstractive Summarization}. arXiv.org.

\bibitem{su2024lem}
Weihang Su, Changyue Wang, Qingyao Ai, et al. (2024). \textit{Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{luo2023xyc}
Junyu Luo, Cao Xiao, and Fenglong Ma (2023). \textit{Zero-Resource Hallucination Prevention for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{wu2024h81}
Jun Wu, Q. Liu, Ding Wang, et al. (2024). \textit{Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chen2024vy7}
Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, et al. (2024). \textit{Multi-Object Hallucination in Vision-Language Models}. Neural Information Processing Systems.

\bibitem{zheng20246fk}
Kening Zheng, Junkai Chen, Yibo Yan, et al. (2024). \textit{Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chen2024lc5}
Xiang Chen, Chenxi Wang, Yida Xue, et al. (2024). \textit{Unified Hallucination Detection for Multimodal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sahoo2024hcb}
Pranab Sahoo, Prabhash Meharia, Akash Ghosh, et al. (2024). \textit{A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2024mmj}
Ruiyang Zhang, Hu Zhang, and Zhedong Zheng (2024). \textit{VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation}. arXiv.org.

\bibitem{qiu2024zyc}
Han Qiu, Jiaxing Huang, Peng Gao, et al. (2024). \textit{LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models}. arXiv.org.

\bibitem{oh2024xa3}
Jio Oh, Soyeon Kim, Junseok Seo, et al. (2024). \textit{ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models}. Neural Information Processing Systems.

\bibitem{zhang202396g}
Yue Zhang, Leyang Cui, Wei Bi, et al. (2023). \textit{Alleviating Hallucinations of Large Language Models through Induced Hallucinations}. North American Chapter of the Association for Computational Linguistics.

\bibitem{yin2025s2b}
Hao Yin, Guangzong Si, and Zilei Wang (2025). \textit{ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models}. Computer Vision and Pattern Recognition.

\bibitem{goyal2021onb}
Tanya Goyal, Jiacheng Xu, J. Li, et al. (2021). \textit{Training Dynamics for Text Summarization Models}. Findings.

\bibitem{kuan20249pm}
Chun-Yi Kuan, Wei-Ping Huang, and Hung-yi Lee (2024). \textit{Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models}. Interspeech.

\bibitem{kaul2024ta7}
Prannay Kaul, Zhizhong Li, Hao Yang, et al. (2024). \textit{THRONE: An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Language Models}. Computer Vision and Pattern Recognition.

\bibitem{yebin2024txh}
Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, et al. (2024). \textit{BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models}. European Conference on Computer Vision.

\bibitem{zhao2024ge8}
Linxi Zhao, Yihe Deng, Weitong Zhang, et al. (2024). \textit{Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance}. Unpublished manuscript.

\bibitem{ye2023yom}
Hongbin Ye, Tong Liu, Aijia Zhang, et al. (2023). \textit{Cognitive Mirage: A Review of Hallucinations in Large Language Models}. LKM@IJCAI.

\bibitem{zhang2023k5a}
Hanning Zhang, Shizhe Diao, Yong Lin, et al. (2023). \textit{R-Tuning: Instructing Large Language Models to Say ‘I Don’t Know’}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2022ypy}
Yanyang Li, Jianqiao Zhao, M. Lyu, et al. (2022). \textit{Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{huang2023du3}
Qidong Huang, Xiao-wen Dong, Pan Zhang, et al. (2023). \textit{OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation}. Computer Vision and Pattern Recognition.

\bibitem{tang2024a1j}
Jianheng Tang, Qifan Zhang, Yuhan Li, et al. (2024). \textit{GraphArena: Evaluating and Exploring Large Language Models on Graph Computation}. International Conference on Learning Representations.

\bibitem{fu2024yqj}
Yuhan Fu, Ruobing Xie, Xingwu Sun, et al. (2024). \textit{Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{yao20229uz}
Shunyu Yao, Jeffrey Zhao, Dian Yu, et al. (2022). \textit{ReAct: Synergizing Reasoning and Acting in Language Models}. International Conference on Learning Representations.

\bibitem{kanthara2022kuj}
Shankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, et al. (2022). \textit{Chart-to-Text: A Large-Scale Benchmark for Chart Summarization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{kim2021obx}
Boseop Kim, Hyoungseok Kim, Sang-Woo Lee, et al. (2021). \textit{What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{xia20224cl}
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, et al. (2022). \textit{Training Trajectories of Language Models Across Scales}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{aharoni2022ioz}
Roee Aharoni, Shashi Narayan, Joshua Maynez, et al. (2022). \textit{mFACE: Multilingual Summarization with Factual Consistency Evaluation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2022p55}
Haopeng Zhang, Semih Yavuz, Wojciech Kryscinski, et al. (2022). \textit{Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control}. NAACL-HLT.

\bibitem{jiang2022reg}
Wenhui Jiang, Minwei Zhu, Yuming Fang, et al. (2022). \textit{Visual Cluster Grounding for Image Captioning}. IEEE Transactions on Image Processing.

\bibitem{wang2020vz6}
Hongmin Wang (2020). \textit{Revisiting Challenges in Data-to-Text Generation with Fact Grounding}. International Conference on Natural Language Generation.

\bibitem{chen2022gkm}
Sihao Chen, S. Buthpitiya, Alex Fabrikant, et al. (2022). \textit{PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{korbak202191w}
Tomasz Korbak, Hady ElSahar, Germán Kruszewski, et al. (2021). \textit{Controlling Conditional Language Models without Catastrophic Forgetting}. International Conference on Machine Learning.

\bibitem{raman20229ce}
K. Raman, Iftekhar Naim, Jiecao Chen, et al. (2022). \textit{Transforming Sequence Tagging Into A Seq2Seq Task}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{liu2021h6c}
Ling Liu, and Mans Hulden (2021). \textit{Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{norlund2021462}
Tobias Norlund, Lovisa Hagström, and Richard Johansson (2021). \textit{Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?}. BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP.

\bibitem{wu20206gt}
Tao Wu, E. Chio, Heng-Tze Cheng, et al. (2020). \textit{Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to Cold-Start Search Retrieval}. International Conference on Information and Knowledge Management.

\bibitem{liu2022hw7}
Yongtai Liu, Joshua Maynez, Gonçalo Simões, et al. (2022). \textit{Data Augmentation for Low-Resource Dialogue Summarization}. NAACL-HLT.

\bibitem{jelinek2016205}
L. Jelinek, M. Hauschildt, C. Wittekind, et al. (2016). \textit{Efficacy of Metacognitive Training for Depression: A Randomized Controlled Trial}. Psychotherapy and Psychosomatics.

\bibitem{raunak2022r58}
Vikas Raunak, and Arul Menezes (2022). \textit{Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{saha20229lo}
Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, et al. (2022). \textit{MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{ham20213fx}
Soomin Ham, Kibaek Park, Yeongjun Jang, et al. (2021). \textit{KSL-Guide: A Large-scale Korean Sign Language Dataset Including Interrogative Sentences for Guiding the Deaf and Hard-of-Hearing}. IEEE International Conference on Automatic Face & Gesture Recognition.

\bibitem{jeong20180d2}
Eunji Jeong, Sungwoo Cho, Gyeong-In Yu, et al. (2018). \textit{JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs}. Symposium on Networked Systems Design and Implementation.

\bibitem{kedia2022c03}
Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee (2022). \textit{FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{gallace201080s}
A. Gallace, and A. Gallace (2010). \textit{Touch and the body: The role of the somatosensory cortex in tactile awareness}. Unpublished manuscript.

\bibitem{li20203k7}
Xintong Li, Aleksandre Maskharashvili, S. Stevens-Guille, et al. (2020). \textit{Leveraging Large Pretrained Models for WebNLG 2020}. WEBNLG.

\bibitem{zhou2011j8m}
Tom Chao Zhou, Chin-Yew Lin, Irwin King, et al. (2011). \textit{Learning to Suggest Questions in Online Forums}. AAAI Conference on Artificial Intelligence.

\bibitem{injae2016yq6}
Shin In-Jae, Byungkwen Song, and D. Eom (2016). \textit{Auto-Mapping and Configuration Method of IEC 61850 Information Model Based on OPC UA}. Unpublished manuscript.

\bibitem{jeong2019z3k}
Eunji Jeong, Sungwoo Cho, Gyeong-In Yu, et al. (2019). \textit{Speculative Symbolic Graph Execution of Imperative Deep Learning Programs}. ACM SIGOPS Operating Systems Review.

\bibitem{rajani20171n9}
Nazneen Rajani, Mihaela A. Bornea, and Ken Barker (2017). \textit{Stacking With Auxiliary Features for Entity Linking in the Medical Domain}. Workshop on Biomedical Natural Language Processing.

\bibitem{wang202379k}
Shuhe Wang, Xiaofei Sun, Xiaoya Li, et al. (2023). \textit{GPT-NER: Named Entity Recognition via Large Language Models}. North American Chapter of the Association for Computational Linguistics.

\bibitem{alshahwan2024v64}
N. Alshahwan, Jubin Chheda, Anastasia Finogenova, et al. (2024). \textit{Automated Unit Test Improvement using Large Language Models at Meta}. SIGSOFT FSE Companion.

\bibitem{zou2024ucl}
Wei Zou, Runpeng Geng, Binghui Wang, et al. (2024). \textit{PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models}. arXiv.org.

\bibitem{fadeeva2024lt8}
Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, et al. (2024). \textit{Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{nguyen2023obn}
Thuat Nguyen, C. Nguyen, Viet Dac Lai, et al. (2023). \textit{CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages}. International Conference on Language Resources and Evaluation.

\bibitem{zou2024c26}
Wei Zou, Runpeng Geng, Binghui Wang, et al. (2024). \textit{PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models}. Unpublished manuscript.

\bibitem{li2023f7d}
Huao Li, Yu Quan Chong, Simon Stepputtis, et al. (2023). \textit{Theory of Mind for Multi-Agent Collaboration via Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{wang2023hgw}
Yiming Wang, Zhuosheng Zhang, and Rui Wang (2023). \textit{Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chen2023gii}
Yuyan Chen, Qiang Fu, Yichen Yuan, et al. (2023). \textit{Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models}. International Conference on Information and Knowledge Management.

\bibitem{gilbert2024uu2}
S. Gilbert, J. Kather, and Aidan Hogan (2024). \textit{Augmented non-hallucinating large language models as medical information curators}. npj Digit. Medicine.

\bibitem{kim2024vgn}
Sunkyu Kim, Choong-kun Lee, and Seung-seob Kim (2024). \textit{Large Language Models: A Guide for Radiologists}. Korean Journal of Radiology.

\bibitem{wang2024sae}
Jianing Wang, Junda Wu, Yupeng Hou, et al. (2024). \textit{InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{huang20233v0}
Yuheng Huang, Jiayang Song, Zhijie Wang, et al. (2023). \textit{Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models}. arXiv.org.

\bibitem{zhao2024s3a}
Linxi Zhao, Yihe Deng, Weitong Zhang, et al. (2024). \textit{Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance}. arXiv.org.

\bibitem{leiser2024kfo}
Florian Leiser, S. Eckhardt, Valentin Leuthe, et al. (2024). \textit{HILL: A Hallucination Identifier for Large Language Models}. International Conference on Human Factors in Computing Systems.

\bibitem{malmqvist2024k7x}
Lars Malmqvist (2024). \textit{Sycophancy in Large Language Models: Causes and Mitigations}. arXiv.org.

\bibitem{lin2024gru}
Sheng-Chieh Lin, Luyu Gao, Barlas Oğuz, et al. (2024). \textit{FLAME: Factuality-Aware Alignment for Large Language Models}. Neural Information Processing Systems.

\bibitem{li2023dw0}
Xingxuan Li, Ruochen Zhao, Yew Ken Chia, et al. (2023). \textit{Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases}. arXiv.org.

\bibitem{ma2023mka}
Fan Ma, Xiaojie Jin, Heng Wang, et al. (2023). \textit{Vista-llama: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens}. Computer Vision and Pattern Recognition.

\bibitem{song2024t8k}
Peiyang Song, Kaiyu Yang, and Anima Anandkumar (2024). \textit{Towards Large Language Models as Copilots for Theorem Proving in Lean}. arXiv.org.

\bibitem{jiang20242kz}
Che Jiang, Biqing Qi, Xiangyu Hong, et al. (2024). \textit{On Large Language Models’ Hallucination with Regard to Known Facts}. North American Chapter of the Association for Computational Linguistics.

\bibitem{song2024br2}
Peiyang Song, Kaiyu Yang, and Anima Anandkumar (2024). \textit{Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean}. NeuS.

\bibitem{liu2024ker}
Haochen Liu, Song Wang, Yaochen Zhu, et al. (2024). \textit{Knowledge Graph-Enhanced Large Language Models via Path Selection}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sun2024z6b}
Yuhong Sun, Zhangyue Yin, Qipeng Guo, et al. (2024). \textit{Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem}. International Conference on Language Resources and Evaluation.

\bibitem{ling2024hqv}
Chen Ling, Xujiang Zhao, Wei Cheng, et al. (2024). \textit{Uncertainty Quantification for In-Context Learning of Large Language Models}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2024jbb}
Moxin Li, Wenjie Wang, Fuli Feng, et al. (2024). \textit{Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{smith2023i8w}
Andrew L Smith, Felix Greaves, and T. Panch (2023). \textit{Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language Models}. PLOS Digital Health.

\bibitem{shah20242sx}
Savyasachi V. Shah (2024). \textit{Accuracy, Consistency, and Hallucination of Large Language Models When Analyzing Unstructured Clinical Notes in Electronic Medical Records.}. JAMA Network Open.

\bibitem{pan2024hm4}
Zhenyu Pan, Haozheng Luo, Manling Li, et al. (2024). \textit{Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models}. International Conference on Learning Representations.

\bibitem{zhang2024o58}
Hengran Zhang, Ruqing Zhang, J. Guo, et al. (2024). \textit{Are Large Language Models Good at Utility Judgments?}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{tang2024cxa}
Jianheng Tang, Qifan Zhang, Yuhan Li, et al. (2024). \textit{GraphArena: Benchmarking Large Language Models on Graph Computational Problems}. arXiv.org.

\bibitem{zhou2024d14}
Xiongtao Zhou, Jie He, Yuhua Ke, et al. (2024). \textit{An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhao2024g9c}
Ruilin Zhao, Feng Zhao, Long Wang, et al. (2024). \textit{KG-CoT: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering}. International Joint Conference on Artificial Intelligence.

\bibitem{pan2024uot}
Zhenyu Pan, Haozheng Luo, Manling Li, et al. (2024). \textit{Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action}. arXiv.org.

\bibitem{zhu2024hll}
Derui Zhu, Dingfan Chen, Qing Li, et al. (2024). \textit{PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics}. NAACL-HLT.

\bibitem{zhang20252at}
Wan Zhang, and Jing Zhang (2025). \textit{Hallucination Mitigation for Retrieval-Augmented Large Language Models: A Review}. Mathematics.

\bibitem{chen2024kgu}
Lida Chen, Zujie Liang, Xintao Wang, et al. (2024). \textit{Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals}. Proceedings of the 3rd Workshop on Towards Knowledgeable Foundation Models (KnowFM).

\bibitem{dernbach2024w0b}
Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, et al. (2024). \textit{GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding}. AAAI Spring Symposia.

\bibitem{wu202407f}
Jiageng Wu, Xian Wu, and Jie Yang (2024). \textit{Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds}. International Joint Conference on Artificial Intelligence.

\bibitem{ahn2024r1o}
Jaewoo Ahn, Taehyun Lee, Junyoung Lim, et al. (2024). \textit{TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{mou2024fsy}
Xinyi Mou, Zejun Li, Hanjia Lyu, et al. (2024). \textit{Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs}. The Web Conference.

\bibitem{xu2024f68}
Derong Xu, Ziheng Zhang, Zhihong Zhu, et al. (2024). \textit{Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models}. International Conference on Information and Knowledge Management.

\bibitem{hu2024fnt}
Xiangkun Hu, Dongyu Ru, Lin Qiu, et al. (2024). \textit{RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models}. arXiv.org.

\bibitem{mukherjee2024o5w}
Subhojyoti Mukherjee, Anusha Lalitha, Sailik Sengupta, et al. (2024). \textit{Multi-Objective Alignment of Large Language Models Through Hypervolume Maximization}. arXiv.org.

\bibitem{jiao2024l4e}
Qirui Jiao, Daoyuan Chen, Yilun Huang, et al. (2024). \textit{Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study}. arXiv.org.

\bibitem{ding20245e3}
Hao Ding, Ziwei Fan, Ingo Gühring, et al. (2024). \textit{Reasoning and Planning with Large Language Models in Code Development}. Knowledge Discovery and Data Mining.

\bibitem{wang2024swy}
Chengpeng Wang, Wuqi Zhang, Zian Su, et al. (2024). \textit{Sanitizing Large Language Models in Bug Detection with Data-Flow}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2024md6}
Zhuo Chen, Jiawei Liu, Haotan Liu, et al. (2024). \textit{Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models}. arXiv.org.

\bibitem{wang2023ynd}
Xiaohua Wang, Yuliang Yan, Longtao Huang, et al. (2023). \textit{Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yeo2024g7d}
Wei Jie Yeo, Teddy Ferdinan, Przemysław Kazienko, et al. (2024). \textit{Self-training Large Language Models through Knowledge Detection}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{hu2024fld}
Sihao Hu, Tiansheng Huang, and Ling Liu (2024). \textit{PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models}. arXiv.org.

\bibitem{zhang2024ia4}
Zhenhong Zhang, Jiajing Chen, Weiyan Shi, et al. (2024). \textit{Contrastive Learning for Knowledge-Based Question Generation in Large Language Models}. 2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI).

\bibitem{yang2024iia}
Dingkang Yang, Dongling Xiao, Jinjie Wei, et al. (2024). \textit{Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators}. AAAI Conference on Artificial Intelligence.

\bibitem{yu2023ine}
Xiaodong Yu, Hao Cheng, Xiaodong Liu, et al. (2023). \textit{ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks}. NAACL-HLT.

\bibitem{gu2024eig}
Zishan Gu, Changchang Yin, Fenglin Liu, et al. (2024). \textit{MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context}. arXiv.org.

\bibitem{liu2024kf2}
Lihui Liu, Zihao Wang, Ruizhong Qiu, et al. (2024). \textit{Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs}. arXiv.org.

\bibitem{woo2024dtm}
B. Woo, Tom Huynh, Arthur Tang, et al. (2024). \textit{Transforming nursing with large language models: from concept to practice.}. European Journal of Cardiovascular Nursing.

\bibitem{zhao20246wi}
Xiutian Zhao, Ke Wang, and Wei Peng (2024). \textit{Measuring the Inconsistency of Large Language Models in Preferential Ranking}. KNOWLLM.

\bibitem{qian2024mj9}
Xinying Qian, Ying Zhang, Yu Zhao, et al. (2024). \textit{TimeR^4 : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{butler20242xs}
J. Butler, James Puleo, Michael Harrington, et al. (2024). \textit{From technical to understandable: Artificial Intelligence Large Language Models improve the readability of knee radiology reports.}. Knee Surgery, Sports Traumatology, Arthroscopy.

\bibitem{sahoo202420w}
N. R. Sahoo, Ashita Saxena, Kishan Maharaj, et al. (2024). \textit{Addressing Bias and Hallucination in Large Language Models}. International Conference on Language Resources and Evaluation.

\bibitem{zuo20242i0}
Kaiwen Zuo, and Yirui Jiang (2024). \textit{MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models}. arXiv.org.

\bibitem{parente2024vlq}
D. J. Parente (2024). \textit{Generative Artificial Intelligence and Large Language Models in Primary Care Medical Education.}. Family Medicine.

\bibitem{zhao2024h5n}
Haiyan Zhao, Fan Yang, Himabindu Lakkaraju, et al. (2024). \textit{Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability}. arXiv.org.

\bibitem{zhou2024b0u}
Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, et al. (2024). \textit{Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{adewumi2024lv9}
Tosin P. Adewumi, Nudrat Habib, Lama Alkhaled, et al. (2024). \textit{On the Limitations of Large Language Models (LLMs): False Attribution}. arXiv.org.

\bibitem{toroghi2024mxf}
Armin Toroghi, Willis Guo, Mohammad Mahdi Torabi pour, et al. (2024). \textit{Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{taveekitworachai2024aql}
Pittawat Taveekitworachai, Febri Abdullah, and R. Thawonmas (2024). \textit{Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{wan2024mh1}
Fanqi Wan, Xinting Huang, Leyang Cui, et al. (2024). \textit{Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment}. arXiv.org.

\bibitem{alsadat2024i78}
Shayan Meshkat Alsadat, Jean-Raphael Gaglione, D. Neider, et al. (2024). \textit{Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine}. American Control Conference.

\bibitem{cao2024o9a}
Qingxing Cao, Junhao Cheng, Xiaodan Liang, et al. (2024). \textit{VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{benkirane202494i}
Kenza Benkirane, Laura Gongas, Shahar Pelles, et al. (2024). \textit{Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{jin2024jpw}
Lifeng Jin, Baolin Peng, Linfeng Song, et al. (2024). \textit{Collaborative decoding of critical tokens for boosting factuality of large language models}. arXiv.org.

\bibitem{mu2024f3b}
Yida Mu, Peizhen Bai, Kalina Bontcheva, et al. (2024). \textit{Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling}. arXiv.org.

\bibitem{yu2024pp9}
Jun Yu, Yunxiang Zhang, Zerui Zhang, et al. (2024). \textit{RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector}. ACM Multimedia.

\bibitem{amirizaniani2024cad}
Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, et al. (2024). \textit{LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop}. Unpublished manuscript.

\bibitem{ling2024qto}
Chen Ling, Xujiang Zhao, Wei Cheng, et al. (2024). \textit{Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models}. arXiv.org.

\bibitem{sarmah2023cuq}
Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, et al. (2023). \textit{Towards reducing hallucination in extracting information from financial reports using Large Language Models}. International Conference on AI-ML-Systems.

\bibitem{nathani202338c}
Deepak Nathani, David Wang, Liangming Pan, et al. (2023). \textit{MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chataigner2024cr0}
Cl'ea Chataigner, Afaf Taïk, and G. Farnadi (2024). \textit{Multilingual Hallucination Gaps in Large Language Models}. arXiv.org.

\bibitem{liu2025xwv}
Q. Liu, Xinlong Chen, Yue Ding, et al. (2025). \textit{Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models}. arXiv.org.

\bibitem{song2024v5n}
Jongyoon Song, Sangwon Yu, and Sungroh Yoon (2024). \textit{Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination}. arXiv.org.

\bibitem{barkley202472d}
Liam Barkley, and Brink van der Merwe (2024). \textit{Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models}. arXiv.org.

\bibitem{huang2024c9t}
Chao-Wei Huang, and Yun-Nung Chen (2024). \textit{FactAlign: Long-form Factuality Alignment of Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{malin2024fin}
B. Malin, Tatiana Kalganova, and Nikoloas Boulgouris (2024). \textit{A review of faithfulness metrics for hallucination assessment in Large Language Models}. IEEE Journal on Selected Topics in Signal Processing.

\bibitem{yuan2024o7d}
Hongbang Yuan, Pengfei Cao, Zhuoran Jin, et al. (2024). \textit{Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{pandit20257jx}
Shrey Pandit, Jiawei Xu, Junyuan Hong, et al. (2025). \textit{MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models}. arXiv.org.

\bibitem{bellinileite2023y38}
Samuel C. Bellini-Leite (2023). \textit{Dual Process Theory for Large Language Models: An overview of using Psychology to address hallucination and reliability issues}. Adaptive Behavior.

\bibitem{omar2025us3}
M. Omar, V. Sorin, J. Collins, et al. (2025). \textit{Large Language Models Are Highly Vulnerable to Adversarial Hallucination Attacks in Clinical Decision Support: A Multi-Model Assurance Analysis}. medRxiv.

\bibitem{lee2024i72}
Yi-Lun Lee, Yi-Hsuan Tsai, and Wei-Chen Chiu (2024). \textit{Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models}. arXiv.org.

\bibitem{li2023irg}
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, et al. (2023). \textit{HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}. Unpublished manuscript.

\bibitem{zhang2023pb6}
Chen Zhang (2023). \textit{User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination}. arXiv.org.

\bibitem{irulandi2023xlg}
Muneeswaran Irulandi, Shreya Saxena, Siva Prasad, et al. (2023). \textit{Minimizing Factual Inconsistency and Hallucination in Large Language Models}. arXiv.org.

\bibitem{li2024ncc}
Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, et al. (2024). \textit{Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation}. arXiv.org.

\bibitem{guo2024tlu}
Hongyi Guo, Zhihan Liu, Yufeng Zhang, et al. (2024). \textit{Can Large Language Models Play Games? A Case Study of A Self-Play Approach}. arXiv.org.

\bibitem{xie20247zk}
Zikai Xie (2024). \textit{Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models}. arXiv.org.

\bibitem{amirizaniani2024493}
Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, et al. (2024). \textit{Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop}. arXiv.org.

\bibitem{yang2025n54}
Tianyun Yang, Ziniu Li, Juan Cao, et al. (2025). \textit{Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention}. International Conference on Learning Representations.

\bibitem{agarwal202418c}
Vibhor Agarwal, Yulong Pei, Salwa Alamir, et al. (2024). \textit{CodeMirage: Hallucinations in Code Generated by Large Language Models}. arXiv.org.

\bibitem{rrv2024gw0}
Aswin Rrv, Nemika Tyagi, Md Nayem Uddin, et al. (2024). \textit{Chaos with Keywords: Exposing Large Language Models Sycophantic Hallucination to Misleading Keywords and Evaluating Defense Strategies}. Unpublished manuscript.

\bibitem{li2024hl9}
Mingchen Li, Zaifu Zhan, Han Yang, et al. (2024). \textit{Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness}. arXiv.org.

\bibitem{wang2024t4o}
Shirui Wang, Bohan Xie, Ling Ding, et al. (2024). \textit{SeCor: Aligning Semantic and Collaborative Representations by Large Language Models for Next-Point-of-Interest Recommendations}. ACM Conference on Recommender Systems.

\bibitem{hegselmann20249q4}
S. Hegselmann, Zejiang Shen, Florian Gierse, et al. (2024). \textit{A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models}. ACM Conference on Health, Inference, and Learning.

\bibitem{gao2024ncr}
Jun Gao, Huan Zhao, Wei Wang, et al. (2024). \textit{EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models}. arXiv.org.

\bibitem{tsai2024klg}
Yao-Hung Tsai, Walter Talbott, and Jian Zhang (2024). \textit{Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning}. arXiv.org.

\bibitem{chen2024qs5}
Xinxi Chen, Li Wang, Wei Wu, et al. (2024). \textit{Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know", and Reducing Hallucination in RAG}. arXiv.org.

\bibitem{liu2025juo}
MingShan Liu, Shi Bo, and Jialing Fang (2025). \textit{Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection}. arXiv.org.

\bibitem{zhang2024htn}
Taolin Zhang, Qizhou Chen, Dongyang Li, et al. (2024). \textit{DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{guo2024hgn}
Yuhang Guo, and Zhiyu Wan (2024). \textit{Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks}. IEEE International Conference on Healthcare Informatics.

\bibitem{luo2024uh8}
Weiqing Luo, Chonggang Song, Lingling Yi, et al. (2024). \textit{KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation}. arXiv.org.

\bibitem{hamid2024pwn}
Oussama H. Hamid (2024). \textit{Beyond Probabilities: Unveiling the Delicate Dance of Large Language Models (LLMs) and AI-Hallucination}. Conference on Cognitive and Computational Aspects of Situation Management.

\bibitem{zheng20240qd}
Xinxin Zheng, Feihu Che, Jinyang Wu, et al. (2024). \textit{KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering}. arXiv.org.

\bibitem{rawte2024bu6}
Vipula Rawte, Aman Chadha, Amit P. Sheth, et al. (2024). \textit{Tutorial Proposal: Hallucination in Large Language Models}. International Conference on Language Resources and Evaluation.

\bibitem{yin2024iau}
Zhibo Yin (2024). \textit{A review of methods for alleviating hallucination issues in large language models}. Applied and Computational Engineering.

\bibitem{tang2025mfi}
Zilu Tang, Rajen Chatterjee, and Sarthak Garg (2025). \textit{Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization}. North American Chapter of the Association for Computational Linguistics.

\bibitem{das2024jdt}
Souvik Das, Lifeng Jin, Linfeng Song, et al. (2024). \textit{Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models}. International Conference on Computational Linguistics.

\bibitem{zhang2024h4a}
Yuxiang Zhang, Jing Chen, Junjie Wang, et al. (2024). \textit{ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{xu2024t34}
Derong Xu, Ziheng Zhang, Zhihong Zhu, et al. (2024). \textit{Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2025p1z}
Hongjie Zhang, Hourui Deng, Jie Ou, et al. (2025). \textit{Mitigating spatial hallucination in large language models for path planning via prompt engineering}. Scientific Reports.

\bibitem{ahmadi2024j88}
Ali Ahmadi (2024). \textit{Unravelling the Mysteries of Hallucination in Large Language Models: Strategies for Precision in Artificial Intelligence Language Generation}. Asian Journal of Computer Science and Technology.

\bibitem{abdelghafour2024efh}
M. Abdelghafour, Mohammed Mabrouk, and Zaki Taha (2024). \textit{Hallucination Mitigation Techniques in Large Language Models}. International Journal of Intelligent Computing and Information Sciences.

\bibitem{zhou20253zv}
Xiaoling Zhou, Mingjie Zhang, Zhemg Lee, et al. (2025). \textit{HaDeMiF: Hallucination Detection and Mitigation in Large Language Models}. International Conference on Learning Representations.

\bibitem{karbasi2025j7n}
Amin Karbasi, Omar Montasser, John Sous, et al. (2025). \textit{(Im)possibility of Automated Hallucination Detection in Large Language Models}. arXiv.org.

\bibitem{mubarak2024lx6}
Hamdy Mubarak, Hend Suliman Al-Khalifa, and Khaloud Suliman Alkhalefah (2024). \textit{Halwasa: Quantify and Analyze Hallucinations in Large Language Models: Arabic as a Case Study}. International Conference on Language Resources and Evaluation.

\bibitem{zhang2024sbu}
Wenbo Zhang, Zihang Xu, and Hengrui Cai (2024). \textit{Recognizing Limits: Investigating Infeasibility in Large Language Models}. Unpublished manuscript.

\bibitem{omar2025cc3}
Mahmud Omar, Vera Sorin, Jeremy D. Collins, et al. (2025). \textit{Multi-model assurance analysis showing large language models are highly vulnerable to adversarial hallucination attacks during clinical decision support}. Communications Medicine.

\bibitem{gao20242nu}
Zhengjie Gao, Xuanzi Liu, Yuanshuai Lan, et al. (2024). \textit{A Brief Survey on Safety of Large Language Models}. Journal of computer & information technology.

\bibitem{pester20242zt}
Andreas Pester, Ahmed Tammaa, Christian Gütl, et al. (2024). \textit{Conversational Agents, Virtual Worlds, and Beyond: A Review of Large Language Models Enabling Immersive Learning}. IEEE Global Engineering Education Conference.

\bibitem{wu202415r}
Kangxi Wu, Liang Pang, Huawei Shen, et al. (2024). \textit{Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{tu2024v40}
Yahan Tu, Rui Hu, and Jitao Sang (2024). \textit{ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models}. Computer Vision and Pattern Recognition.

\end{thebibliography}

\end{document}