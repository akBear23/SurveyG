\subsection{Adversarial Attacks and Vulnerability Probing}

Beyond merely observing and reacting to instances of hallucination, a critical new frontier in understanding and mitigating this phenomenon involves proactively probing Large Multimodal Models (LMMs) for vulnerabilities through adversarial attacks. This methodology aims to intentionally induce hallucinations, thereby uncovering specific weaknesses and failure modes that might remain hidden during passive observation, guiding the development of more resilient and secure AI systems.

A pioneering work in this domain is \cite{wang2025jen}, which introduces a novel adversarial attack termed "Mirage in the Eyes." This technique specifically targets and exploits the "attention sink" phenomenon within MLLMs to intentionally induce hallucinations. By dynamically manipulating internal attention scores and hidden embeddings, \cite{wang2025jen} demonstrates how to steer the model towards generating factually incorrect or non-existent visual content, providing crucial insights into the internal mechanisms that contribute to hallucination. This approach moves beyond external input perturbations, delving into the model's internal processing to expose its susceptibility.

Complementing such targeted internal attacks, other research has integrated adversarial principles into evaluation and discovery. \cite{huang20247wn}, in their comprehensive benchmark VHTest, incorporates an adversarial generation paradigm to create diverse visual hallucination instances. While not a direct attack for inducing hallucination in the same manner as \cite{wang2025jen}, this paradigm contributes to the proactive identification of vulnerabilities by systematically generating challenging inputs that are likely to trigger various types of hallucinations, including those related to object shape and size. This allows for a broader exploration of an MLLM's fragility across different visual attributes.

Furthermore, vulnerabilities can be exposed through surprisingly subtle adversarial manipulations. \cite{han202439z} uncovered a "semantic shift bias" where the mere insertion of paragraph breaks (\texttt{\textbackslash n}) into textual prompts can induce hallucinations in MLLMs. This seemingly innocuous input perturbation acts as a potent adversarial trigger, demonstrating that models can be led astray by minor structural changes that do not alter the semantic content of the prompt. Such findings highlight unexpected failure modes and underscore the importance of probing for vulnerabilities across a wide spectrum of input types, from complex internal manipulations to simple textual formatting.

Collectively, these approaches represent a significant shift from reactive mitigation to proactive robustness testing. By actively designing attacks that exploit internal model characteristics like attention sinks \cite{wang2025jen}, or by systematically generating adversarial test cases \cite{huang20247wn}, or even by identifying subtle input biases \cite{han202439z}, researchers are gaining a deeper understanding of *why* and *how* hallucinations occur. This proactive methodology is indispensable for identifying the root causes of hallucination, enabling the development of more robust architectures and training strategies that can withstand sophisticated adversarial attempts to induce erroneous outputs. The insights gleaned from these adversarial probes are crucial for building truly trustworthy and secure AI systems that can maintain factual consistency even under challenging or malicious inputs.