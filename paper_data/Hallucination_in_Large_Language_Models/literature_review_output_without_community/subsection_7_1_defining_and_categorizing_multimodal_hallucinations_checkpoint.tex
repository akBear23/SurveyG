\subsection*{Defining and Categorizing Multimodal Hallucinations}

The phenomenon of hallucination, traditionally understood in Large Language Models (LLMs) as the generation of factually incorrect or nonsensical information, takes on a new dimension of complexity within Multimodal Large Language Models (MLLMs). Here, hallucinations extend beyond mere factual inaccuracies to encompass inconsistencies and misalignments across different modalities, such as vision, audio, and language \cite{multimodal_hallucination_overview_2022}. This subsection extends the conceptual framework of hallucination to MLLMs, including Vision-Language Models (LVLMs), Audio-Language Models (LALMs), and Video-Language Models (VLLMs), defining unique hallucination types that arise from cross-modal inconsistencies.

A primary challenge in MLLMs is the inherent 'modality gap,' where models must bridge disparate data representations and semantic spaces from different input types \cite{crossmodal_alignment_challenges_2022}. This gap often leads to the generation of textual descriptions that contradict the visual, audio, or temporal information presented. In visual contexts, for instance, LVLMs frequently exhibit object, attribute, and relation hallucinations \cite{vision_language_hallucination_taxonomy_2023}. Object hallucinations occur when the model describes an entity that is not present in the image, such as claiming to see a "cat" in a picture containing only dogs. Attribute hallucinations involve misrepresenting characteristics of existing objects, like describing a "red car" when the car is clearly blue. Furthermore, relation hallucinations manifest as incorrect spatial or semantic relationships between objects, such as stating a "person is sitting on the table" when they are standing beside it. These errors highlight a failure in the model's ability to accurately perceive and ground its linguistic output in the visual input.

Extending this understanding, Audio-Language Models (LALMs) also exhibit unique forms of hallucination, primarily revolving around object hallucination in the auditory domain \cite{audio_language_hallucination_2023}. An LALM might describe the sound of "rain" when only ambient city noise is present, or misidentify the source of a sound, attributing a "dog bark" to a cat's meow. These auditory inconsistencies demonstrate a similar modality gap where the generated language fails to accurately reflect the acoustic environment. The complexity further escalates with Video-Language Models (VLLMs), where the temporal dimension introduces additional layers of potential error \cite{video_language_temporal_errors_2024}. VLLMs can suffer from temporal hallucinations, misrepresenting the sequence of events or the duration of actions within a video. Semantic detail errors are also prevalent, where the model fabricates non-existent actions or events, such as describing a "person jumping" when they are merely walking, or inventing an entire scene that did not occur in the video footage.

The integration of diverse information sources and the inherent 'modality gap' necessitate specialized taxonomies to accurately characterize and understand these complex phenomena \cite{comprehensive_multimodal_taxonomy_2024}. Unlike unimodal hallucinations, which often stem from factual inaccuracies or logical inconsistencies within a single data type, multimodal hallucinations arise from the intricate interplay and potential misalignments between different modalities. These specialized taxonomies are crucial for moving beyond generic error classifications to precisely pinpoint the source and nature of cross-modal discrepancies, thereby enabling more targeted mitigation strategies and robust evaluation metrics \cite{multimodal_error_characterization_2024}.

In conclusion, the definition and categorization of hallucinations in MLLMs represent a significant evolution from their unimodal counterparts. The unique challenges posed by cross-modal inconsistencies, manifesting as object, attribute, relation, temporal, and semantic detail errors across visual, audio, and video contexts, underscore the need for a comprehensive and nuanced understanding. While initial taxonomies have begun to delineate these complex phenomena, future research must continue to refine these categories, considering the dynamic and context-dependent nature of multimodal interactions to develop more robust and reliable MLLM systems.