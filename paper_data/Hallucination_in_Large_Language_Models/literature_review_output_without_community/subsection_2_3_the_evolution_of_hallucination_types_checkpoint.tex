\subsection*{The Evolution of Hallucination Types}

The understanding and categorization of AI hallucinations have undergone a significant evolution, moving from broad, general definitions to highly granular and context-specific classifications. This progression reflects the increasing complexity of AI models and their applications, necessitating a more nuanced taxonomy for effective detection and mitigation.

Initially, research into hallucinations in large language models (LLMs) established foundational categories. Early work, such as the comprehensive survey by \cite{DBLP:journals/corr/abs-2202-03629}, broadly categorized hallucinations into *intrinsic*, where the generated content contradicts the source input, and *extrinsic*, where it contradicts established world knowledge. This foundational classification also introduced related concepts like factuality, faithfulness, and consistency, providing an initial framework for analyzing model outputs. Expanding on this, another survey by \cite{DBLP:journals/corr/abs-2305-13889} further refined LLM hallucination types by categorizing them based on their *source* (e.g., data, model architecture, inference process), *form* (e.g., factual, logical, numerical inconsistencies), and *severity*, offering a multi-dimensional perspective on how and why these errors manifest.

As AI capabilities extended to multimodal domains, the definition of hallucination necessarily diversified to encompass new forms of inconsistency. \cite{DBLP:journals/corr/abs-2305-18654} specifically addressed hallucinations in Large Multimodal Models (LMMs), proposing a refined taxonomy that includes *object hallucinations* (misidentifying or fabricating objects), *attribute hallucinations* (incorrectly describing an object's properties), and *relation hallucinations* (misrepresenting relationships between objects). This marked a crucial step towards more granular, modality-specific categorizations. Building upon this multimodal understanding, research into Video-Language Models (VLMs) further introduced the dimension of time. \cite{DBLP:journals/corr/abs-2305-18260} identified and categorized VLM hallucinations based on *temporal consistency*, detailing issues such as incorrect event order, duration, or frequency, which are paramount for accurately interpreting dynamic visual information.

Beyond modality, the operational context and interaction paradigm of AI systems have also led to the identification of distinct hallucination types. For instance, the challenges posed by extended inputs prompted the definition of *long-context specific hallucinations* by \cite{DBLP:journals/corr/abs-2309-17424}. These are errors that emerge or are exacerbated when models process unusually long sequences of text, often involving subtle inconsistencies or omissions that are difficult to detect without a comprehensive understanding of the entire context. Similarly, in the realm of conversational AI, \cite{DBLP:journals/corr/abs-2309-07490} introduced the concept of *dialogue-level hallucinations*. These extend beyond single-turn factual errors to encompass inconsistencies in persona, conversation flow, or maintaining coherent context across multiple turns, which are critical for natural and trustworthy human-AI interaction. Furthermore, the global deployment of AI models has highlighted *multilingual hallucinations*, as investigated by \cite{DBLP:journals/corr/abs-2305-10424}. This work revealed that hallucination rates can vary significantly across languages, suggesting that language-specific nuances, data biases, or model training disparities can lead to distinct patterns of erroneous generation in non-English contexts.

This progressive refinement in categorizing hallucination types—from broad textual inconsistencies to specific multimodal, temporal, long-context, dialogue-level, and multilingual manifestations—is indispensable. It underscores a growing recognition that a one-size-fits-all approach to hallucination is insufficient. The continued identification of such distinct types is crucial for developing targeted evaluation metrics, designing more robust and context-aware mitigation strategies, and ultimately fostering greater reliability across diverse AI applications.