\subsection{Internal State Manipulation and Forward-Pass Interventions}

To effectively mitigate hallucination and enhance grounding in large language models (LLMs), advanced strategies move beyond external prompting to directly intervene within the model's internal processing during the forward pass. These methods leverage a deeper, mechanistic understanding of LLM behavior, allowing for more precise and less intrusive corrections by manipulating internal states, attention mechanisms, and feature representations. This approach contrasts with decoding-time interventions by operating earlier in the generation pipeline, often influencing the very construction of hidden states and attention patterns that precede token prediction.

One family of interventions focuses on dynamically adjusting feature representations based on model uncertainty or signal strength. \citet{zou2024dp7} introduced **Memory-Space Visual Retracing (MemVR)**, a reactive strategy primarily for multimodal LLMs. MemVR addresses visual grounding issues by monitoring the LLM's internal uncertainty, often quantified through the entropy of token probabilities or disagreement across multiple decoding paths. When high uncertainty is detected, MemVR reactively re-injects relevant visual tokens or their representations into intermediate layers of the model. This re-grounding aims to provide the model with a fresh, reinforced visual context precisely when it is most prone to hallucination due to visual amnesia or insufficient attention. Complementing this, \citet{yin2025s2b} proposed **Visual Amplification Fusion (VAF)**, a more proactive strategy. VAF addresses the problem of insufficient or diluted visual signals in deeper layers of the LLM by actively enhancing and amplifying visual feature representations within specific middle layers. This proactive boosting, often achieved through learned scaling factors or specialized fusion modules, helps prevent the early decay of visual information, ensuring that the model maintains sufficient visual attention throughout the forward pass. While MemVR is reactive and uncertainty-driven, VAF is proactive, tackling the "modality gap" by ensuring visual signals remain salient.

Beyond general feature manipulation, research has increasingly focused on granular control over the model's attention mechanisms, which are critical for integrating information across modalities and contexts. \citet{chen2024j0g} explored **Targeted Attention Head Interventions for Cross-Level Visual Focus**. This work delves into identifying specific attention heads crucial for integrating visual information across varying levels of abstraction, from low-level features to high-level concepts. By applying interventions, such as re-weighting attention scores or fine-tuning specific head parameters during the forward pass, this method aims to improve the model's ability to maintain coherent visual focus and prevent modality collapse or misinterpretation. The challenge lies in identifying these critical heads and ensuring interventions generalize across diverse inputs.

Further advancing this mechanistic understanding with theoretical rigor, \citet{zhou2024lvp} introduced **CAUSAL MM**, a causal inference framework for **Modality Prior Balancing in Multimodal LLMs**. This approach employs structural causal modeling (SCM) to analyze and adjust the influence of different modalities within the attention mechanisms. By treating modality priors (visual and language) as confounding factors in the causal path between attention and output, CAUSAL MM uses techniques like back-door adjustment and counterfactual reasoning. This allows for deciphering the true causal impact of effective attention on MLLM output by isolating and mitigating the effects of misleading modality priors. For instance, by simulating counterfactual attention states (e.g., uniform or shuffled attention), the framework quantifies how much a specific modality's prior causally influences the output, then adjusts attention to achieve a more balanced integration. This provides a principled, theoretically grounded way to enhance visual grounding and reduce hallucination, offering a more robust alternative to empirical attention head interventions.

Another innovative forward-pass intervention, particularly for contextual hallucinations, is the **Lookback Lens** proposed by \citet{chuang20248ey}. This method introduces a "lookback ratio" derived from attention maps, which quantifies the proportion of attention weights focused on the input context versus newly generated tokens. This interpretable feature serves as a signal for contextual faithfulness. During generation, a "Lookback Lens Guided Decoding" strategy samples multiple candidate text chunks. For each chunk, its lookback ratio features are computed and scored by a lightweight classifier. The chunk predicted to be least hallucinated (i.e., most grounded in the context) is then selected. This approach dynamically steers generation by leveraging attention patterns, demonstrating strong cross-task and cross-model transferability due to its reliance solely on attention maps, which are hypothesized to be more generalizable indicators of contextual faithfulness than raw hidden states.

Finally, \citet{jin2024jpw} introduced a **Collaborative Decoding Framework** that represents a different form of internal state manipulation. Recognizing that pretrained models often retain higher factual accuracy while finetuned models excel in instruction following but may hallucinate more, this framework dynamically decides which model to use for the next token. A "critical token classifier," trained to identify tokens where factual accuracy is paramount, dictates whether the pretrained model or the finetuned model should generate the subsequent token. This allows the system to harness the factuality of the pretrained model for critical information while benefiting from the fluency and instruction-following of the finetuned model for general generation. This method intervenes at a higher level, dynamically routing internal processing based on the perceived "criticality" of the next token, effectively balancing competing objectives during the forward pass.

These forward-pass interventions collectively represent a significant shift towards a deeper, mechanistic understanding of LLM behavior, enabling more precise and less intrusive corrections. While methods like CAUSAL MM offer theoretical grounding and principled adjustments, they can be computationally intensive due to counterfactual simulations. Heuristic approaches like VAF or targeted attention head interventions (\cite{chen2024j0g}) might be simpler to implement but may lack the same level of theoretical guarantees or generalizability. Lookback Lens (\cite{chuang20248ey}) provides an interpretable signal for contextual grounding, but its guided decoding introduces computational overhead from candidate sampling. Collaborative decoding (\cite{jin2024jpw}) offers an intriguing way to leverage different internal knowledge states, but its effectiveness depends on the accuracy of the critical token classifier and the alignment of the base models. Challenges remain in generalizing identified intervention points across diverse model architectures and tasks, as well as in developing more comprehensive theoretical frameworks to guide the optimal application of these internal state manipulations, ensuring both efficacy and efficiency in real-time.