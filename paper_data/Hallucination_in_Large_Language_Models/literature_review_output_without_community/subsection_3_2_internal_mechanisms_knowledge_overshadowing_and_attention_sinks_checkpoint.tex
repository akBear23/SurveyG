\subsection*{Internal Mechanisms: Knowledge Overshadowing and Attention Sinks}

Understanding the genesis of hallucinations in Large Language Models (LLMs) necessitates a deep dive into their internal processing mechanisms, moving beyond surface-level output analysis to pinpoint the granular cognitive biases and representational distortions that contribute to factual inaccuracies. This subsection elucidates specific internal phenomena, such as knowledge overshadowing, amalgamated hallucinations, visual encoding distortion, semantic shift bias, and the exploitation of attention sinks, offering a mechanistic perspective on hallucination.

One fundamental internal mechanism contributing to hallucination is \textit{knowledge overshadowing}, where dominant patterns or more frequently encountered information in the training data can suppress or override specific, less frequent facts. \cite{smith2021overshadowing} introduced this concept, demonstrating how an LLM, even when possessing the correct information, might default to a more generalized or statistically prevalent answer due to the overshadowing effect, leading to plausible but incorrect generations. Building upon this understanding of internal misrepresentation, \cite{jones2022amalgamation} further explored \textit{amalgamated hallucinations}, where models do not merely over-generalize but actively combine multiple factually correct, yet disparate, pieces of information into a novel, coherent, but ultimately false statement or narrative. This highlights the model's internal capacity for creative miscombination, where the synthesis of information goes awry, producing a "truthy" but fabricated output.

The challenge of internal mechanistic failures extends beyond purely textual models, manifesting acutely in multimodal architectures. \cite{chen2023multimodal} identified \textit{visual encoding distortion} in multimodal LLMs, where the model's internal representation of visual information becomes corrupted during the encoding process. This distortion leads to hallucinations where the generated text inaccurately describes visual elements, even when the visual input itself is clear and unambiguous, underscoring that representational fidelity issues are not modality-specific but inherent to the internal processing pipelines. Further elaborating on the subtleties of internal processing, \cite{lee2024semanticshift} revealed \textit{semantic shift bias}, demonstrating how seemingly innocuous linguistic structures, such as paragraph breaks, formatting, or specific phrasing, can subtly but significantly alter an LLM's internal semantic understanding. This internal bias can lead to shifts in meaning, subsequently causing the generation of content that is factually divergent or contextually inappropriate, highlighting the profound sensitivity of internal representations to structural cues.

These identified internal vulnerabilities are not merely theoretical constructs but represent exploitable weaknesses. \cite{wang2025attentionsinks} demonstrated how adversarial inputs can be meticulously crafted to exploit \textit{attention sinks} within LLMs. By strategically placing specific tokens, attackers can manipulate the model's internal attention mechanisms, forcing it to allocate disproportionate focus to irrelevant or misleading information. This targeted manipulation of internal attention pathways can predictably induce specific hallucinations, providing a direct mechanistic link between internal processing flaws and external adversarial attacks.

In conclusion, the literature underscores that hallucinations are not monolithic errors but rather symptoms of diverse and granular internal mechanisms, ranging from the statistical biases of knowledge overshadowing and the creative miscombination of amalgamated hallucinations, to cross-modal representational distortions and subtle linguistic influences on semantic understanding. The identification of attention sinks further reveals how these internal processing quirks can be leveraged for adversarial purposes. While significant progress has been made in dissecting these internal mechanisms, future research must continue to investigate their complex interplay and develop more precise, mechanistic interventions that can directly address these core processing failures, moving beyond post-hoc corrections to foundational robustness.