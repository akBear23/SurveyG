\subsection{Cross-Modal Dynamics and Snowballing}

The transition from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) and Vision-Language Models (LVLMs) introduces a new dimension to the hallucination problem: complex dynamic behaviors where errors propagate across modalities and conversational turns. While Section 4.4 addresses dialogue-level inconsistencies in text-only models, this subsection focuses specifically on how the interplay with persistent visual, audio, or video modalities creates unique error propagation dynamics, often termed "multimodal hallucination snowballing," which are not present in purely linguistic systems. Understanding these dynamic and interactive aspects is crucial for developing robust and coherent AI systems capable of sustained, reliable interaction across sensory inputs.

A primary dynamic challenge is \textit{multimodal hallucination snowballing}, where an LVLM's previously generated hallucination can mislead subsequent responses in conversational settings. \cite{zhong2024mfi} meticulously identifies and characterizes this problem, demonstrating how initial factual errors in an LVLM's output can be implicitly accepted and built upon in subsequent turns, leading to a cascade of incorrect information. To counteract this, they introduce Residual Visual Decoding (RVD), a training-free decoding method that emphasizes direct visual evidence to prevent the model from relying on its own prior, potentially erroneous, textual generations. While RVD offers a practical solution, its effectiveness hinges on the clarity and availability of direct visual evidence, which might be insufficient in scenarios requiring abstract reasoning or subtle contextual understanding. Reinforcing this challenge, \cite{cao2024o9a} introduces VisDiaHalBench, a visual dialogue benchmark specifically designed to diagnose hallucinations arising from "long-term misleading textual history" in LVLMs. This benchmark, featuring five-turn questions about edited images, directly probes the model's susceptibility to propagating errors in a conversational context, highlighting the need for continuous visual re-grounding. The principles of error propagation in multi-turn dialogues, as explored in text-only contexts by \cite{chen2024c4k} with their DiaHalu benchmark, find direct and exacerbated parallels in multimodal settings where visual context can be misremembered or ignored.

Beyond explicit conversational snowballing, misinterpretations can arise from subtle, dynamic interactions and inconsistencies between different modalities, acting as triggers for initial hallucinations that can then propagate. \cite{zhou2024lvp} investigates the causal impact of modality priors on attention and output, revealing how an imbalance in these priors can lead to hallucinations. Their work, CAUSAL MM, provides a principled causal inference framework to understand and balance the influence of visual and linguistic inputs by applying back-door adjustment and counterfactual reasoning. This mitigates errors stemming from over-reliance on one modality, which could otherwise initiate a chain of incorrect inferences. However, the complexity of causal modeling and defining appropriate counterfactuals remains a challenge for broad applicability. Similarly, \cite{han202439z} uncovers a "semantic shift bias" where the mere insertion of a paragraph break in textual input can subtly alter an LVLM's understanding of an image, leading to hallucinations. This demonstrates how minor, seemingly innocuous textual formatting can dynamically influence cross-modal interpretation, revealing a brittleness in vision-language grounding that may require more fundamental architectural solutions than the proposed MiHI/MiHO interventions.

Underlying these dynamic misinterpretations are fundamental vulnerabilities within the model architecture that enable error propagation. \cite{wang2025jen} reveals that hallucinations can be induced by exploiting "attention sinks," a phenomenon where attention mechanisms become fixated on irrelevant tokens, diverting processing power from critical visual information. In a multimodal context, this mechanism can directly contribute to propagating errors by causing the model to misinterpret visual cues, thus initiating a chain of incorrect inferences that could snowball. This highlights a critical internal dynamic where attention misallocation directly impacts multimodal grounding. Furthermore, the temporal dynamics inherent in video-language models present unique challenges. \cite{ma2023mka} introduces Vista-llama to address the "diminishing impact of video" as generated text length increases, a clear example of cross-modal dynamic error where the visual grounding weakens over time, leading to irrelevant content. Their solution, which maintains a consistent distance between visual and language tokens, underscores the need for continuous and robust visual attention throughout the generation process.

Evaluating an LVLM's ability to maintain coherent understanding across dynamic visual changes is also crucial for identifying propagating errors. \cite{yebin2024txh} introduces BEAF (Observing BEfore-AFter Changes), an innovative framework that manipulates visual scenes by removing objects and introduces change-aware metrics. This allows for a more robust assessment of whether models truly understand visual changes or merely hallucinate, which is vital for identifying when subtle inconsistencies lead to misinterpretations or propagating errors in a dynamic environment.

The insights from LLM-centric self-correction mechanisms offer conceptual parallels for mitigating multimodal snowballing. \cite{dhuliawala2023rqn}'s Chain-of-Verification (CoVe) and \cite{liu2025juo}'s self-consistency framework for mathematical reasoning both emphasize internal deliberation and verification steps to prevent models from repeating their own mistakes and propagating errors. Adapting such multi-step, self-correcting paradigms to multimodal contexts would require sophisticated mechanisms for re-grounding each verification step in the visual or audio evidence, rather than solely relying on internal textual consistency. This is a significant challenge, as highlighted by surveys like \cite{liu2024sn3} and \cite{lan20240yz}, which discuss the persistent "modality gap" and the difficulty of ensuring consistent understanding across diverse data distributions. \cite{tonmoy20244e4} also notes that snowballing in complex reasoning remains a challenge for many mitigation approaches, underscoring the severity of this dynamic problem.

In conclusion, the study of cross-modal dynamics and snowballing highlights that hallucination in LVLMs is not merely a static error but a complex, evolving problem. The propagation of errors, whether through explicit conversational snowballing, subtle cross-modal misinterpretations, or diminishing visual grounding over time, poses a significant challenge. Future research must focus on developing models with stronger internal consistency checks that explicitly re-ground in multimodal reality at each conversational turn, advanced causal modeling of cross-modal interactions to prevent the initiation of errors, and robust self-correction mechanisms that can effectively leverage and verify against dynamic sensory inputs to prevent the propagation of hallucinations.