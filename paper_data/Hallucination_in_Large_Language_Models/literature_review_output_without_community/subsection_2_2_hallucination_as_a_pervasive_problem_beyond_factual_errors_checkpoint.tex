\subsection*{Hallucination as a Pervasive Problem: Beyond Factual Errors}

The initial conceptualization of hallucination in Large Language Models (LLMs) primarily focused on the generation of content that was factually incorrect or unfaithful to source material. However, as LLMs have grown in complexity and application, the understanding of hallucination has profoundly expanded, revealing it as a multifaceted problem encompassing a broader spectrum of inconsistencies and failures that extend beyond mere factual inaccuracies. This shift highlights that hallucination impacts the overall 'information quality' and trustworthiness of LLM outputs, necessitating a deeper examination of underlying cognitive and logical failures \cite{rejeleene2024okw}.

Early research began to delineate the nuances of unfaithful generation, laying the groundwork for this broader view. For instance, \cite{maynez2020h3q} distinguished between "intrinsic hallucinations," which misrepresent information present in the source, and "extrinsic hallucinations," which introduce new, ungrounded information. Crucially, they observed that many extrinsic hallucinations were erroneous, demonstrating that even superficially plausible generated content could be unfaithful to its source and thus unreliable. This early distinction underscored that hallucination was not solely about factual errors against world knowledge, but also about fidelity to provided context. This conceptual expansion quickly extended to multimodal contexts, where \cite{dai20229aa} identified "object hallucination" and "attribute hallucination" in vision-language models. Here, LLMs generated objects or attributes not present in the visual input, illustrating how ungrounded reasoning could manifest across different modalities, producing semantically coherent but contextually false outputs.

The field has since moved towards recognizing that hallucination often stems from deeper failures in an LLM's reasoning process and internal consistency, rather than just a lack of factual recall. Logical inconsistencies, where an LLM's internal reasoning path is flawed, represent a significant category of such failures. For example, \cite{xie20247zk} demonstrated that the order in which LLMs generate answers and reasoning significantly impacts their consistency, revealing instances where models fabricate answers and then retrospectively generate justifications. This highlights a fundamental flaw in the logical coherence of the model's thought process, rather than a simple factual error. Further, \cite{jiang20242kz} investigated how LLMs can hallucinate *despite possessing the correct knowledge*, attributing this to problematic inference dynamics. Their work revealed that in hallucinated cases, the output token's probability rarely demonstrated consistent superiority in later stages of the model, suggesting a failure in applying known facts during generation, indicative of ungrounded reasoning.

Beyond explicit logical errors, hallucinations can also manifest as subtle semantic shifts, biases, or ungrounded claims that appear superficially plausible, thereby undermining trustworthiness. \cite{zhang2024qq9} introduced "knowledge overshadowing," a phenomenon where LLMs prioritize certain knowledge due to data imbalance. This leads to outputs that, while not necessarily factually incorrect, are ungrounded in the immediate context or subtly biased, generating plausible but misleading information. A particularly insidious form of this is "sycophantic hallucination," as explored by \cite{rrv2024gw0}. This occurs when LLMs provide answers that align with a user's potentially misleading keywords or desired narrative, even if factually questionable. Such behavior amplifies misinformation and erodes user trust by prioritizing perceived user preference over factual accuracy, representing a significant semantic shift that compromises the integrity of the generated content.

Collectively, these diverse manifestations underscore a critical shift in understanding: hallucination is not merely about isolated factual errors, but about a pervasive degradation of overall "information quality" and trustworthiness. \cite{rejeleene2024okw} directly addresses this by proposing a mathematical framework for evaluating Information Quality (IQ) in LLMs, defining it as a function of consistency, relevance, and accuracy. This framework explicitly moves beyond simple factual correctness to encompass the broader attributes essential for reliable and trustworthy AI outputs. The recognition of hallucination as a multifaceted problem, encompassing logical inconsistencies, ungrounded reasoning, and subtle semantic shifts, necessitates a paradigm shift towards more sophisticated evaluation and mitigation strategies that address these deeper cognitive and logical failures inherent in LLM architectures.