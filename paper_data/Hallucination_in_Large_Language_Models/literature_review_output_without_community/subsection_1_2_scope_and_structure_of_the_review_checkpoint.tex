\subsection*{Scope and Structure of the Review}

This literature review provides a comprehensive and structured analysis of research into hallucination in Large Language Models (LLMs), adopting a pedagogical progression to trace the field's maturation from foundational concepts to advanced methodological approaches and cutting-edge multimodal developments. This structured organization is designed to offer a clear, evolving narrative of the research landscape, emphasizing the interconnections between different facets of the hallucination problem. The review synthesizes the evolving understanding of hallucination, its underlying causes, methods for its evaluation, and strategies for its mitigation, with a primary focus on advancements from 2022 to 2025 to ensure relevance to the current state of the art in LLM research.

The review is meticulously organized into several thematic sections, each building upon the preceding one to offer a holistic and in-depth understanding of hallucination. Following this introduction, Section 2, "Foundational Understanding: Defining and Categorizing Hallucination," lays the groundwork by exploring the evolution of hallucination definitions. It distinguishes between crucial concepts like faithfulness to source material and factual correctness in general knowledge, and traces the development of early taxonomies. This section highlights how the understanding of hallucination has broadened significantly beyond simple factual errors to encompass a diverse array of content inconsistencies, logical flaws, and reasoning failures, thereby setting the stage for more granular analysis and targeted interventions.

Building upon this definitional and conceptual foundation, Section 3, "The Roots of Hallucination: Mechanistic Causes and Theoretical Limits," delves into the underlying reasons why LLMs generate incorrect or ungrounded content. This section moves beyond mere empirical observation to investigate both practical causes related to data quality, training processes, and inference biases. Crucially, it explores specific internal model mechanisms, such as 'knowledge overshadowing' and 'attention sinks,' that directly contribute to hallucination. The section culminates in a discussion of the formal theoretical grounding of hallucination, examining its mathematical origins and the concept of its inherent inevitability, which fundamentally re-frames the problem from a transient engineering bug to an innate characteristic of computable LLMs.

With a clear understanding of what hallucination is and why it occurs, Section 4, "Evaluating Hallucination: Benchmarks for Factual Accuracy and Reasoning," details the evolution of evaluation methodologies. It showcases a progression from initial, broad assessments to highly granular, automatically verifiable, and context-aware benchmarks. This section covers early efforts to establish reproducible metrics for factual correctness and citation quality, advancements in fine-grained and rationale-based evaluation that probe the model's reasoning process, and the development of specialized benchmarks for complex algorithmic reasoning. Furthermore, it addresses the unique challenges of assessing hallucinations in long-context and dialogue-level interactions, underscoring the field's commitment to rigorous and comprehensive measurement of LLM trustworthiness.

Subsequently, the review transitions to a comprehensive exploration of mitigation strategies, divided into two complementary sections. Section 5, "Mitigation Strategies I: External Knowledge Grounding and Adaptive Retrieval," explores approaches centered on grounding LLMs in external knowledge. It introduces Retrieval-Augmented Generation (RAG) as a foundational paradigm, explaining its core principles and early implementations. This section then details the evolution of RAG into advanced and adaptive architectures that dynamically integrate external information, optimize retrieval, and intelligently manage context. It also highlights the increasing importance of integrating structured knowledge graphs to enhance factual accuracy, logical consistency, and overall trustworthiness.

Complementing external grounding, Section 6, "Mitigation Strategies II: Intrinsic Model Interventions and Self-Correction," focuses on interventions that operate intrinsically within the LLM itself. This includes techniques that modify the decoding process, such as contrastive methods, to steer generation away from ungrounded content. The section then delves into more granular interventions that manipulate internal model states and attention mechanisms during the forward pass for precise control over information flow. Finally, it discusses the development of self-correction and abstention mechanisms, enabling LLMs to detect their own uncertainties and errors, along with training-based approaches that leverage automated data generation for more efficient and targeted fine-tuning against hallucination.

Recognizing the expanding frontier of AI, Section 7, "The Multimodal Frontier: Hallucination in Vision, Audio, and Video Language Models," addresses the significant challenges posed by hallucination in Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs). This section defines and categorizes the distinct types of hallucinations that emerge from cross-modal interactions, details specialized evaluation benchmarks developed to rigorously assess these multimodal inconsistencies, and explores tailored multimodal mitigation strategies. It also investigates dynamic behaviors such as 'multimodal hallucination snowballing' in interactive multimodal settings, highlighting the complexity of ensuring trustworthiness across diverse sensory inputs.

Finally, Section 8, "Towards Trustworthy AI: Robustness, Safety, and Advanced Evaluation," explores critical advancements in building truly trustworthy LLMs. It covers methods for zero-resource and black-box hallucination detection, crucial for proprietary models, and delves into the proactive testing of LLM vulnerabilities through adversarial attacks, which actively induce hallucinations to identify weaknesses. This section highlights the development of 'semantic guardrails' for safety-critical applications, aiming for absolute error prevention, and discusses the emerging field of meta-evaluation, which critically assesses the quality of hallucination benchmarks themselves, ensuring the integrity of all research efforts towards reliable AI.

The review concludes in Section 9, "Conclusion and Future Directions," by synthesizing the key advancements, outlining remaining challenges and open questions, and discussing the critical ethical considerations necessary for responsible AI development. This structured progression aims to provide readers with a comprehensive, nuanced, and forward-looking perspective on the multifaceted problem of hallucination in LLMs, fostering a deeper understanding of the path towards more reliable and accountable AI systems.