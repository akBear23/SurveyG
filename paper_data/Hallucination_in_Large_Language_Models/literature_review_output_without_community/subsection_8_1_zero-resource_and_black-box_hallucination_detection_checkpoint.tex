\subsection*{Zero-Resource and Black-Box Hallucination Detection}

Detecting hallucinations in Large Language Models (LLMs) presents a formidable challenge, particularly when evaluating proprietary models where direct access to internal states, training data, or extensive human labels is unavailable. This subsection focuses on "zero-resource" and "black-box" detection methods, which operate under these constraints by leveraging the intrinsic properties of LLM generation to identify factual and logical inconsistencies. These approaches are indispensable for scalable, model-agnostic evaluation, thereby enhancing the practical applicability of hallucination detection frameworks across diverse deployment scenarios.

A foundational contribution in this domain is \textit{SelfCheckGPT} \cite{manakul20236ex}, which introduced a novel zero-resource, black-box strategy. The core premise is that an LLM genuinely "knowing" a fact will produce consistent responses across multiple stochastically sampled generations for the same query. Conversely, if the LLM hallucinates, these samples are likely to diverge, contradict, or present inconsistent information. \textit{SelfCheckGPT} generates several diverse responses from the black-box LLM and then employs various consistency measures, such as BERTScore, Natural Language Inference (NLI), or even another LLM acting as an evaluator, to quantify the informational agreement between the original response and the generated samples. This method effectively identifies non-factual statements without requiring internal token probabilities or external fact-checking databases. However, a significant limitation arises when LLMs consistently repeat their own errors across multiple samples due to strong internal biases or memorization, leading to false negatives in hallucination detection, as the model appears "consistent" in its incorrectness. Furthermore, the computational overhead of generating and comparing multiple responses can be substantial, especially for complex queries or real-time applications.

To address the limitations of consistent self-hallucinations, \textit{MetaQA} \cite{yang20251dw} significantly advanced zero-resource detection by introducing *metamorphic relations* and *prompt mutation*. Instead of merely re-sampling from the same prompt, \textit{MetaQA} generates logically equivalent or semantically related prompts (e.g., synonymous queries, rephrased questions, or queries testing inverse relations) to elicit a more diverse and robust set of responses. By checking for consistency across these responses, which are generated from varied but semantically linked inputs, \textit{MetaQA} makes it harder for the LLM to consistently hallucinate the same fact. This technique effectively probes the LLM's understanding from multiple angles, providing a more reliable signal for exposing factual inconsistencies. While more robust, the effectiveness of \textit{MetaQA} is contingent on the careful design of metamorphic relations, which can be task-specific and may not generalize universally across all types of factual or reasoning errors.

Beyond consistency checks, other black-box approaches explore alternative signals. \textit{Attention-Guided SElf-Reflection (AGSER)} \cite{liu2025xwv} proposes a zero-shot hallucination detection method that attempts to leverage insights from attention mechanisms without direct internal model access. AGSER categorizes the input query into "attentive" and "non-attentive" parts, processes each separately through the LLM, and then computes consistency scores between the generated responses and the original answer. The difference between these consistency scores serves as a hallucination estimator. This method notably reduces computational overhead, requiring only three passes through the LLM, making it more efficient than methods relying on numerous generations. While the precise mechanism for inferring "attention contributions" in a strictly black-box manner requires careful consideration, AGSER demonstrates a promising direction for deriving more nuanced signals from LLM outputs without full transparency.

Zero-resource, black-box principles are also being tailored for specific, complex reasoning domains. For instance, \cite{liu2025juo} enhances mathematical reasoning in LLMs by applying a structured self-consistency framework. This approach goes beyond merely checking the final answer, enforcing consistency across *intermediate reasoning steps* in tasks like theorem proving, symbolic transformation, and numerical computation. By ensuring logical coherence throughout the problem-solving process, this method significantly reduces logical inconsistencies and hallucinations specific to mathematical contexts. While domain-specific, it highlights how black-box consistency checks can be adapted to probe deeper into an LLM's reasoning integrity, rather than just surface-level factual recall.

\textbf{Comparative Analysis and Critical Discussion:}
These black-box detection methods offer distinct advantages and trade-offs. \textit{SelfCheckGPT} provides a simple, general-purpose baseline, but its vulnerability to consistently incorrect outputs limits its robustness. \textit{MetaQA} improves robustness by actively perturbing inputs, making it harder for LLMs to hide systematic errors, yet it introduces complexity in designing effective metamorphic relations. Both methods incur significant computational costs due to multiple inference calls. \textit{AGSER} attempts to mitigate this computational burden by leveraging a more efficient, attention-guided reflection mechanism, potentially offering a better balance between detection efficacy and resource usage. The mathematical self-consistency approach \cite{liu2025juo} demonstrates the power of adapting these principles to domain-specific reasoning, highlighting that while general black-box detectors are valuable, specialized approaches can achieve higher precision for particular types of complex hallucinations. A common limitation across these methods is their reliance on the LLM's own generative capabilities to expose its flaws; they cannot detect hallucinations that the LLM consistently and confidently generates as "true" across all probed variations. Furthermore, the sensitivity to sampling parameters (e.g., temperature) and the choice of consistency metrics (e.g., BERTScore vs. NLI) can significantly impact detection performance, requiring careful tuning.

In summary, zero-resource and black-box hallucination detection methods represent a vital area of research, offering scalable and model-agnostic solutions for evaluating and improving the trustworthiness of LLMs, especially proprietary ones. From the foundational consistency checks of \textit{SelfCheckGPT} \cite{manakul20236ex} and the robust metamorphic relations of \textit{MetaQA} \cite{yang20251dw}, to the efficient attention-guided reflection of \textit{AGSER} \cite{liu2025xwv} and domain-specific logical consistency for mathematical reasoning \cite{liu2025juo}, these techniques collectively push the boundaries of what is possible without privileged model access or extensive human annotation. Future directions in this area could involve the fusion of diverse black-box signals, combining internal consistency with metamorphic testing and attention-guided insights to create more robust hybrid detectors. Furthermore, research could focus on developing computationally lighter black-box methods, exploring their applicability to detect more subtle forms of hallucination beyond factual errors, such as logical fallacies, reasoning inconsistencies, and biases, and enhancing their resilience against adversarial attacks designed to evade detection.