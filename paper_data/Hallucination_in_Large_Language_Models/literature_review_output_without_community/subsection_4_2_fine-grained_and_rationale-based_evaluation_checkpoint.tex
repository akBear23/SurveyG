\subsection*{Fine-Grained and Rationale-Based Evaluation}

Traditional evaluation metrics for Large Language Models (LLMs), often relying on surface-level textual overlap (e.g., ROUGE, BLEU) or simple answer correctness, frequently prove insufficient for comprehensively assessing the nuanced quality and underlying reasoning capabilities of their outputs. These metrics struggle to pinpoint the exact nature and location of errors, particularly hallucinations, or to verify the logical soundness of the model's internal reasoning process. To address this, a significant shift has occurred towards fine-grained and rationale-based evaluation methodologies, which delve deeper into the LLM's reasoning and output fidelity, aiming to provide detailed insights into error types and locations, and to scrutinize the logical coherence of generated rationales. This paradigm shift is crucial for fostering greater transparency and verifiability in LLM behavior.

The initial advancements in this domain focused on developing more granular methods for identifying and categorizing hallucinations within generated text. Rather than a binary correct/incorrect judgment, these approaches sought to annotate errors at a sentence or even phrase level, distinguishing between factual inaccuracies, logical inconsistencies, or ungrounded statements. This fine-grained annotation provides a richer understanding of where and how LLMs deviate from ground truth, moving beyond aggregate scores to actionable insights into model weaknesses. For instance, some benchmarks explore the impact of reasoning order on LLM consistency, revealing how models might fabricate justifications when asked to provide an answer before its rationale, highlighting a fine-grained flaw in their generation process \cite{xie20247zk}.

A crucial advancement in this area is the emergence of 'rationale verification' techniques, which scrutinize the intermediate steps an LLM takes to arrive at a conclusion, rather than solely evaluating the final output. This moves beyond merely identifying *what* an LLM gets wrong to understanding *why*, by pinpointing logical missteps in its chain of thought. The most sophisticated iterations of rationale verification leverage structured data to automatically assess logical soundness, offering a scalable and objective alternative to human judgment.

A prominent example of this is \textit{ERBench}, which utilizes existing relational databases (RDBs) and their inherent Entity-Relationship (ER) model to construct automatically verifiable benchmarks for LLMs \cite{oh2024xa3}. ERBench leverages database schema, records, and crucially, integrity constraints like Functional Dependencies (FDs) and Foreign Key Constraints (FKCs), to generate complex, multi-hop questions. Its innovation lies in automatically verifying not only the final answer but also the LLM's rationale by checking for the presence of FD-inferred critical keywords within the generated reasoning steps. This allows for a deeper evaluation of factual hallucination and introduces novel metrics such as Rationale Accuracy (R) and Answer-Rationale Accuracy (AR), providing a verifiable audit trail for LLM conclusions by grounding evaluation in external, structured knowledge.

Beyond relational databases, other structured knowledge sources like Knowledge Graphs (KGs) have been instrumental in developing rationale-based evaluation. \cite{ghosh2024tj5} proposes a framework to assess and improve the logical consistency of Retrieval-Augmented LLMs (RAG) in fact-checking tasks, specifically for propositional logic queries derived from KGs. This work defines quantitative measures for LLM consistency across primitive logical operators (negation, conjunction, disjunction) and complex logical rules, directly evaluating the LLM's adherence to logical soundness within a structured context. Similarly, \textit{Drowzee} introduces a logic-programming-aided metamorphic testing technique for Fact-Conflicting Hallucination (FCH) detection \cite{li2024osp}. It constructs a factual knowledge base and generates diverse test cases using logic reasoning rules. By employing semantic-aware metamorphic oracles, Drowzee automatically detects FCHs by comparing the logical and semantic structures of LLM answers against ground truth, thereby validating the reasoning process itself.

The principle of verifying intermediate reasoning steps extends to specialized domains like mathematical reasoning. \cite{liu2025juo} introduces a structured self-consistency framework designed to enhance the reliability of mathematical reasoning in LLMs. This method enforces self-consistency not just on final answers but critically across intermediate steps in tasks such as theorem proving, symbolic manipulation, and numerical computation. By ensuring logical consistency throughout the reasoning trajectory, this approach effectively reduces hallucinations and logical inconsistencies, paving the way for more reliable and interpretable AI-driven mathematics.

In conclusion, the progression from coarse-grained to fine-grained and rationale-based evaluation marks a critical maturation in LLM assessment. These methodologies, particularly those leveraging structured data like relational databases and knowledge graphs, offer unprecedented transparency by allowing for the automatic verification of an LLM's reasoning process. While significant strides have been made in pinpointing errors and verifying reasoning, challenges remain. The construction and maintenance of comprehensive, domain-specific structured knowledge bases can be resource-intensive, limiting scalability to highly complex, open-ended, or multi-hop reasoning tasks where the required knowledge might be vast or ill-defined. Furthermore, models might learn to produce 'verifiable' but ultimately incorrect rationales, highlighting the need for more robust verification mechanisms that are less susceptible to superficial adherence to rules. Future directions will likely focus on developing more adaptive rationale verification systems that can dynamically interact with diverse and evolving knowledge sources, handle ambiguous reasoning paths, and integrate with advanced uncertainty quantification methods to provide a holistic assessment of LLM trustworthiness.