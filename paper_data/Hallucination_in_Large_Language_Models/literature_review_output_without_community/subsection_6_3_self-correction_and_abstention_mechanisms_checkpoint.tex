\subsection*{Self-Correction and Abstention Mechanisms}

The pursuit of reliable and trustworthy Large Language Models (LLMs) necessitates equipping them with the intrinsic capabilities to detect and rectify their own errors, as well as to proactively abstain from generating responses when faced with high uncertainty. This subsection delves into the sophisticated strategies that empower LLMs with these self-aware mechanisms, transforming them from mere text generators into more reflective and judicious agents. As surveyed by \cite{pan2024y3a}, automated correction strategies are broadly categorized, with self-correction and abstention representing crucial generation-time and post-hoc interventions.

The foundation of LLM self-correction lies in enhancing their reasoning capabilities. Early advancements like Chain-of-Thought (CoT) prompting \cite{wei2022chainofthought, kadavath2022language} were pivotal, enabling LLMs to articulate intermediate reasoning steps. By externalizing their thought process, models could expose potential logical flaws, laying the groundwork for subsequent self-reflection. Building on this, more advanced frameworks emerged to facilitate iterative and exploratory reasoning. ReAct (Reasoning and Acting) \cite{yao20229uz} integrates CoT with external tool use, allowing LLMs to interleave reasoning steps with actions (e.g., searching external knowledge bases or executing code). This iterative cycle of thought, action, and observation provides a powerful mechanism for self-correction by verifying internal reasoning against external facts and refining plans based on observed outcomes, thereby directly connecting to external knowledge grounding discussed in Section 5. Tree of Thoughts (ToT) \cite{yao2023treeofthoughts} further extends this by exploring multiple reasoning paths, evaluating intermediate states, and backtracking when a path proves unfruitful. This search-based approach allows LLMs to engage in more complex problem-solving and to self-correct by identifying and discarding inconsistent or incorrect lines of reasoning.

Beyond these foundational reasoning paradigms, explicit self-correction frameworks have been developed. The \textit{Self-Refine} framework \cite{madaan2023selfrefine} exemplifies an iterative generate-critique-refine loop. An LLM first generates an initial output, then critically reflects on it by generating self-feedback to identify potential errors or areas for improvement, and finally uses this self-generated critique to produce a refined response. This internal feedback loop significantly enhances output quality and accuracy. Another powerful approach is Chain-of-Verification (CoVe) \cite{dhuliawala2023rqn}, which systematically reduces factual hallucination. CoVe operates in multiple steps: generating a baseline response, planning specific verification questions based on the claims in the response, executing these verifications (often independently to prevent error propagation), and finally generating a revised, verified response. The "factored" variant of CoVe, where each verification question is answered without conditioning on the potentially hallucinated baseline, is particularly effective in minimizing the LLM's tendency to repeat its own errors. Furthermore, self-consistency \cite{wang2022selfconsistency} has proven effective, especially in mathematical reasoning \cite{liu2025juo}. This technique involves prompting the LLM to generate multiple diverse reasoning paths and corresponding answers, then selecting the most frequent or consistent answer, thereby leveraging the model's own internal agreement as a form of self-correction.

Concurrently with self-correction, the critical need for LLMs to express "I don't know" has driven the development of robust abstention mechanisms to mitigate hallucination and overconfidence. Early abstention methods often relied on calibrating the model's predicted probabilities or confidence scores \cite{lin2022calibrated}. These techniques typically involved post-hoc adjustments like temperature scaling to determine when a model should refrain from answering. However, such methods frequently required auxiliary models, ground truth data for effective calibration, or suffered from miscalibration, limiting their robustness and true "label-free" nature.

A significant innovation has been the development of label-free abstention mechanisms that quantify uncertainty intrinsically. \cite{kuhn2023semantic} introduced 'semantic entropy' as a novel measure of uncertainty. This approach quantifies the diversity and plausibility of semantically distinct alternative outputs for a given query. A high semantic entropy indicates a lack of a single, clear, and confident answer, prompting the model to abstain. This provides a more robust and inherent way for LLMs to recognize their limitations without external labels. Further advancements include token-level uncertainty quantification, such as Claim Conditioned Probability (CCP) \cite{fadeeva2024lt8}, which measures the uncertainty of a particular claim value expressed by the model, enabling fine-grained fact-checking and highlighting specific unreliable segments. Similarly, \cite{ling2024hqv} proposed methods to quantify both aleatoric (data-inherent) and epistemic (model-inherent) uncertainties in in-context learning, offering a deeper understanding of the sources of LLM uncertainty for more informed abstention.

Ultimately, the most robust systems integrate both self-correction and abstention capabilities. LLMs can first attempt to self-correct their responses using iterative refinement or verification frameworks. If, after this refinement process, significant uncertainty persists (quantified by metrics like semantic entropy or token-level uncertainty), the model can then judiciously choose to abstain. This synergistic strategy ensures that models actively strive to improve their answers while also possessing the crucial self-awareness to decline answering when truly uncertain, thereby enhancing overall reliability and transparency. In safety-critical applications, this translates into "semantic guardrails" \cite{hakim2024d4u}, which are designed to prevent "never event" errors by combining internal uncertainty quantification with external consistency checks, effectively forcing abstention or flagging for human review when high-stakes factual accuracy cannot be guaranteed.

Despite these advancements, significant challenges persist. The computational cost associated with extensive reflection, iterative refinement, and generating multiple diverse outputs for uncertainty quantification can be substantial, particularly for real-time applications. Defining and universally applying robust "semantic plausibility" for uncertainty quantification across diverse and open-ended domains remains an active research area. Moreover, while frameworks like ReAct integrate external tools, the optimal balance between internal self-reflection and external knowledge verification needs further exploration. Future directions include developing more adaptive and context-aware self-correction mechanisms, refining uncertainty quantification to be more robust and interpretable across all types of LLM tasks, and exploring hybrid approaches that dynamically combine internal reasoning with external grounding to achieve both high accuracy and appropriate humility.